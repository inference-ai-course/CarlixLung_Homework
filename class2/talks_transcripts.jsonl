{"title": "What is NLP (Natural Language Processing)?", "video_id": "fLvJ8VdHLA0", "text": "What is natural language processing? Well, you're doing it right now. You're listening to the words and the sentences that I'm forming and you are forming some sort of comprehension from it. And when we ask a computer to do that, that is NLP on natural language processing. My name is Martin Keen. I'm a master inventor at IBM and I've utilized NLP in a good number of my invention disclosures. NLP really has a really high utility value in all sorts of AI applications. Now, NLP starts with something called unstructured text. What is that? Well, that's just what you and I say. That's how we speak. So for example, some unstructured text is add eggs and milk to my shopping list. Now, you and I understand exactly what that means, but it is unstructured at least to a computer. So what we need to do is to have a structured representation of that same information that a computer can process. Now, that might look something a bit more like this, where we have a shopping list element. And then it has sub elements within it, like an item for eggs, and an item for milk. That is an example of something that is structured. Now, the job of natural language processing is to translate between these two things. So NLP sits right in the middle here, translating between unstructured and structured data. And when we go from unstructured here to structured this way, that's called NLU or natural language understanding. And when we go this way from structured to unstructured, that's called natural language generation or NLG. We're going to focus today primarily on going from unstructured to structured in natural language processing. Now, let's think of some use cases where NLP might be quite handy. First of all, we've got machine translation. Now, when we translate from one language to another, we need to understand the context of that sentence. It's not just a case of taking each individual word from, say, English and then translating it into another language. We need to understand the overall structure and context of what's being said. And my favorite example of this going horribly wrong is if you take the phrase, the spirit is willing, but the flesh is weak, and you translate that from English to Russian, and then you translate that Russian translation back into English, you're going to go from the spirit is willing but the flesh is weak to something a bit more like the vodka is good, but the meat is rotten, which is really not the intended context of that sentence whatsoever. So NLP can help with situations like that. Now, the second kind of use case that I like to mention relates to virtual assistance and also to things like chat bots. Now, a virtual assistant, that's something like Siri or Alexa on your phone that is taking human utterances and deriving a command to execute based upon that. And a chatbot is something similar except in written language and that's taking written language and then using it to traverse a decision tree in order to take an action. NLP is very helpful there. Another use case is for sentiment analysis. Now, this is taking some text, perhaps an email message or a product review and trying to derive the sentiment that it's expressed within it. So for example, is this product review a positive sentiment or a negative sentiment? Is it written as a serious statement or is it being sarcastic? We can use NLP to tell us. And then finally, another good example is spam detection. So this is a case of looking at a given email message and trying to derive, is this a really email message or is it spam? And we can look for pointers within the content of the message. So things like overused words or poor grammar or an inappropriate claim of urgency can all indicate that this is actually perhaps spam. So those are some of the things that NLP can provide but how does it work? Well, the thing with NLP is it's not like one algorithm. It's actually more like a bag of tools and you can apply these bag of tools to be able to resolve some of these use cases. Now, the input to NLP is some unstructured text so either some written text or spoken text that has been converted to a written text through a speech to text algorithm. Once we've got that, the first stage of NLP is called tokenization. This is about taking a string and breaking it down into chunks. So if we consider the unstructured text we've got here, add eggs and milk to my shopping list. That's eight words, that could be eight tokens. And from here on in, we are going to work one token at a time as we traverse through this. Now, the first stage once we've got things down into tokens that we can perform is called stemming. And this is all about deriving the word stem for a given token. So for example, running runs and ran, the word stem for all three of those is run. We're just kind of removing the prefix and the suffixes and normalizing the tense and we're getting to the word stem. But stemming doesn't work well for every token. For example, universal and university will, well, they don't really stem down to universe. For situations like that, there is another tool that we have available. And that is called lemmatization. And lemmatization takes a given token and learns its meaning through a dictionary definition. And from there, it can derive its root or its lemme. So take better, for example, better is derived from good. So the root or the lemme of better is good. The stem of better would be bet. So you can see that it is significant whether we use stemming or we use lemmatization for a given token. Now, next thing we can do is we can do a process called part of speech tagging. And what this is doing is for a given token, it's looking where that token is used within the context of a sentence. So take the word make, for example. If I say I'm going to make dinner, make is a verb. But if I ask you what make is your laptop, well, make is now now. So where that token is used in a sentence matters, part of speech tagging can help us derive that context. And then finally, another stage is named entity. Recognition. And what this is asking is for a given token, is there an entity associated with it? So, for example, a token of Arizona has an entity of a US state, whereas a token of Ralph has an entity of a person's name. And these are some of the tools that we can apply in this big bag of tools that we have for NLP in order to get from this unstructured human speech through to something structured that a computer can understand. And once we've done that, then we can apply that structured data to all sorts of AI applications. Now, there's obviously a lot more to it than this. And I've included some links in the description if you'd like to know more. But hopefully this made some sense and that you were able to process some of the natural language that I've shared today. Thanks for watching. If you have questions, please drop us a line below and if you want to see more videos like this in the future, please like and subscribe.", "segments": [{"start": 0.0, "end": 3.48, "text": "What is natural language processing?"}, {"start": 3.48, "end": 6.18, "text": "Well, you're doing it right now."}, {"start": 6.18, "end": 9.44, "text": "You're listening to the words and the sentences that I'm forming"}, {"start": 9.44, "end": 13.88, "text": "and you are forming some sort of comprehension from it."}, {"start": 13.88, "end": 16.88, "text": "And when we ask a computer to do that,"}, {"start": 16.88, "end": 21.400000000000002, "text": "that is NLP on natural language processing."}, {"start": 21.400000000000002, "end": 22.68, "text": "My name is Martin Keen."}, {"start": 22.68, "end": 25.0, "text": "I'm a master inventor at IBM"}, {"start": 25.0, "end": 28.0, "text": "and I've utilized NLP in a good number"}, {"start": 28.0, "end": 30.36, "text": "of my invention disclosures."}, {"start": 30.36, "end": 34.04, "text": "NLP really has a really high utility value"}, {"start": 34.04, "end": 37.76, "text": "in all sorts of AI applications."}, {"start": 37.76, "end": 42.4, "text": "Now, NLP starts with something called unstructured text."}, {"start": 42.4, "end": 43.480000000000004, "text": "What is that?"}, {"start": 43.480000000000004, "end": 46.68, "text": "Well, that's just what you and I say."}, {"start": 46.68, "end": 47.68, "text": "That's how we speak."}, {"start": 47.68, "end": 50.32, "text": "So for example, some unstructured text"}, {"start": 50.32, "end": 57.32, "text": "is add eggs and milk to my shopping list."}, {"start": 59.0, "end": 64.44, "text": "Now, you and I understand exactly what that means,"}, {"start": 64.44, "end": 71.68, "text": "but it is unstructured at least to a computer."}, {"start": 71.68, "end": 75.92, "text": "So what we need to do is to have a structured representation"}, {"start": 75.92, "end": 78.96000000000001, "text": "of that same information that a computer can process."}, {"start": 78.96000000000001, "end": 81.56, "text": "Now, that might look something a bit more like this,"}, {"start": 81.56, "end": 86.56, "text": "where we have a shopping list element."}, {"start": 86.72, "end": 89.16, "text": "And then it has sub elements within it,"}, {"start": 89.16, "end": 91.60000000000001, "text": "like an item for eggs,"}, {"start": 94.16, "end": 97.72, "text": "and an item for milk."}, {"start": 103.28, "end": 106.12, "text": "That is an example of something that is structured."}, {"start": 110.12, "end": 113.16, "text": "Now, the job of natural language processing"}, {"start": 113.16, "end": 116.36, "text": "is to translate between these two things."}, {"start": 116.36, "end": 120.76, "text": "So NLP sits right in the middle here,"}, {"start": 120.76, "end": 123.6, "text": "translating between unstructured and structured data."}, {"start": 123.6, "end": 126.84, "text": "And when we go from unstructured here to structured"}, {"start": 126.84, "end": 131.84, "text": "this way, that's called NLU or natural language understanding."}, {"start": 133.6, "end": 137.04, "text": "And when we go this way from structured to unstructured,"}, {"start": 137.04, "end": 142.04, "text": "that's called natural language generation or NLG."}, {"start": 142.84, "end": 146.24, "text": "We're going to focus today primarily on going from unstructured"}, {"start": 146.24, "end": 149.88, "text": "to structured in natural language processing."}, {"start": 149.88, "end": 151.48000000000002, "text": "Now, let's think of some use cases"}, {"start": 151.48000000000002, "end": 155.4, "text": "where NLP might be quite handy."}, {"start": 155.4, "end": 160.0, "text": "First of all, we've got machine translation."}, {"start": 161.96, "end": 166.96, "text": "Now, when we translate from one language to another,"}, {"start": 168.4, "end": 173.4, "text": "we need to understand the context of that sentence."}, {"start": 173.88, "end": 176.96, "text": "It's not just a case of taking each individual word"}, {"start": 176.96, "end": 180.04000000000002, "text": "from, say, English and then translating it into another language."}, {"start": 180.04000000000002, "end": 182.64000000000001, "text": "We need to understand the overall structure"}, {"start": 182.64000000000001, "end": 184.72, "text": "and context of what's being said."}, {"start": 184.72, "end": 188.52, "text": "And my favorite example of this going horribly wrong"}, {"start": 188.52, "end": 190.76, "text": "is if you take the phrase,"}, {"start": 190.76, "end": 194.08, "text": "the spirit is willing, but the flesh is weak,"}, {"start": 194.08, "end": 196.48000000000002, "text": "and you translate that from English to Russian,"}, {"start": 196.48000000000002, "end": 200.4, "text": "and then you translate that Russian translation back into English,"}, {"start": 200.4, "end": 202.88, "text": "you're going to go from the spirit is willing"}, {"start": 202.88, "end": 207.04, "text": "but the flesh is weak to something a bit more like the vodka"}, {"start": 207.04, "end": 209.4, "text": "is good, but the meat is rotten,"}, {"start": 209.4, "end": 212.84, "text": "which is really not the intended context"}, {"start": 212.84, "end": 214.48, "text": "of that sentence whatsoever."}, {"start": 214.48, "end": 218.51999999999998, "text": "So NLP can help with situations like that."}, {"start": 218.51999999999998, "end": 222.64, "text": "Now, the second kind of use case that I like to mention"}, {"start": 222.64, "end": 227.64, "text": "relates to virtual assistance and also to things like chat bots."}, {"start": 228.92, "end": 232.28, "text": "Now, a virtual assistant, that's something like Siri"}, {"start": 232.32, "end": 236.72, "text": "or Alexa on your phone that is taking human utterances"}, {"start": 236.72, "end": 241.0, "text": "and deriving a command to execute based upon that."}, {"start": 241.0, "end": 245.44, "text": "And a chatbot is something similar except in written language"}, {"start": 245.44, "end": 247.16, "text": "and that's taking written language"}, {"start": 247.16, "end": 250.44, "text": "and then using it to traverse a decision tree"}, {"start": 250.44, "end": 252.04, "text": "in order to take an action."}, {"start": 252.04, "end": 253.6, "text": "NLP is very helpful there."}, {"start": 254.8, "end": 259.16, "text": "Another use case is for sentiment analysis."}, {"start": 260.16, "end": 264.92, "text": "Now, this is taking some text, perhaps an email message"}, {"start": 264.92, "end": 268.8, "text": "or a product review and trying to derive the sentiment"}, {"start": 268.8, "end": 270.44, "text": "that it's expressed within it."}, {"start": 270.44, "end": 275.44000000000005, "text": "So for example, is this product review a positive sentiment"}, {"start": 276.6, "end": 277.8, "text": "or a negative sentiment?"}, {"start": 277.8, "end": 280.96000000000004, "text": "Is it written as a serious statement"}, {"start": 280.96000000000004, "end": 282.88, "text": "or is it being sarcastic?"}, {"start": 282.88, "end": 285.68, "text": "We can use NLP to tell us."}, {"start": 285.72, "end": 290.72, "text": "And then finally, another good example is spam detection."}, {"start": 291.6, "end": 294.6, "text": "So this is a case of looking at a given email message"}, {"start": 294.6, "end": 297.52, "text": "and trying to derive, is this a really email message"}, {"start": 297.52, "end": 298.6, "text": "or is it spam?"}, {"start": 298.6, "end": 301.76, "text": "And we can look for pointers within the content of the message."}, {"start": 301.76, "end": 306.2, "text": "So things like overused words or poor grammar"}, {"start": 306.2, "end": 309.96000000000004, "text": "or an inappropriate claim of urgency can all indicate"}, {"start": 309.96000000000004, "end": 312.16, "text": "that this is actually perhaps spam."}, {"start": 313.08000000000004, "end": 316.40000000000003, "text": "So those are some of the things that NLP can provide"}, {"start": 316.40000000000003, "end": 318.16, "text": "but how does it work?"}, {"start": 318.16, "end": 323.16, "text": "Well, the thing with NLP is it's not like one algorithm."}, {"start": 323.88000000000005, "end": 326.56, "text": "It's actually more like a bag of tools"}, {"start": 326.56, "end": 328.96000000000004, "text": "and you can apply these bag of tools"}, {"start": 328.96000000000004, "end": 332.36, "text": "to be able to resolve some of these use cases."}, {"start": 332.36, "end": 336.72, "text": "Now, the input to NLP is some unstructured text"}, {"start": 336.72, "end": 339.32000000000005, "text": "so either some written text"}, {"start": 339.32, "end": 341.52, "text": "or spoken text that has been converted"}, {"start": 341.52, "end": 345.36, "text": "to a written text through a speech to text algorithm."}, {"start": 345.36, "end": 350.36, "text": "Once we've got that, the first stage of NLP is called tokenization."}, {"start": 355.0, "end": 357.12, "text": "This is about taking a string"}, {"start": 357.12, "end": 360.24, "text": "and breaking it down into chunks."}, {"start": 360.24, "end": 364.24, "text": "So if we consider the unstructured text we've got here,"}, {"start": 364.24, "end": 368.76, "text": "add eggs and milk to my shopping list."}, {"start": 368.8, "end": 371.92, "text": "That's eight words, that could be eight tokens."}, {"start": 371.92, "end": 375.36, "text": "And from here on in, we are going to work one token"}, {"start": 375.36, "end": 377.8, "text": "at a time as we traverse through this."}, {"start": 378.8, "end": 381.12, "text": "Now, the first stage once we've got things down"}, {"start": 381.12, "end": 385.12, "text": "into tokens that we can perform is called stemming."}, {"start": 387.56, "end": 390.64, "text": "And this is all about deriving the word stem"}, {"start": 390.64, "end": 392.24, "text": "for a given token."}, {"start": 392.24, "end": 396.36, "text": "So for example, running runs and ran,"}, {"start": 396.36, "end": 399.52000000000004, "text": "the word stem for all three of those is run."}, {"start": 399.52000000000004, "end": 402.40000000000003, "text": "We're just kind of removing the prefix and the suffixes"}, {"start": 402.40000000000003, "end": 405.40000000000003, "text": "and normalizing the tense and we're getting to the word stem."}, {"start": 406.24, "end": 409.52000000000004, "text": "But stemming doesn't work well for every token."}, {"start": 409.52000000000004, "end": 414.52000000000004, "text": "For example, universal and university"}, {"start": 414.52000000000004, "end": 418.44, "text": "will, well, they don't really stem down to universe."}, {"start": 418.44, "end": 420.68, "text": "For situations like that, there is another tool"}, {"start": 420.68, "end": 422.56, "text": "that we have available."}, {"start": 422.6, "end": 426.6, "text": "And that is called lemmatization."}, {"start": 428.44, "end": 430.92, "text": "And lemmatization takes a given token"}, {"start": 430.92, "end": 435.12, "text": "and learns its meaning through a dictionary definition."}, {"start": 435.12, "end": 439.4, "text": "And from there, it can derive its root or its lemme."}, {"start": 439.4, "end": 442.52, "text": "So take better, for example,"}, {"start": 442.52, "end": 444.88, "text": "better is derived from good."}, {"start": 444.88, "end": 449.04, "text": "So the root or the lemme of better is good."}, {"start": 449.04, "end": 453.28000000000003, "text": "The stem of better would be bet."}, {"start": 453.28000000000003, "end": 455.92, "text": "So you can see that it is significant"}, {"start": 455.92, "end": 459.64000000000004, "text": "whether we use stemming or we use lemmatization"}, {"start": 459.64000000000004, "end": 461.52000000000004, "text": "for a given token."}, {"start": 463.08000000000004, "end": 466.28000000000003, "text": "Now, next thing we can do is we can do"}, {"start": 466.28000000000003, "end": 469.6, "text": "a process called part of speech tagging."}, {"start": 471.56, "end": 475.12, "text": "And what this is doing is for a given token,"}, {"start": 475.12, "end": 477.56, "text": "it's looking where that token is used"}, {"start": 477.56, "end": 479.68, "text": "within the context of a sentence."}, {"start": 480.96, "end": 484.76, "text": "So take the word make, for example."}, {"start": 484.76, "end": 488.96, "text": "If I say I'm going to make dinner,"}, {"start": 488.96, "end": 491.16, "text": "make is a verb."}, {"start": 491.16, "end": 494.32, "text": "But if I ask you what make is your laptop,"}, {"start": 494.32, "end": 496.32, "text": "well, make is now now."}, {"start": 496.32, "end": 499.28, "text": "So where that token is used in a sentence matters,"}, {"start": 499.28, "end": 502.52, "text": "part of speech tagging can help us derive that context."}, {"start": 502.52, "end": 507.52, "text": "And then finally, another stage is named entity."}, {"start": 507.84, "end": 508.84, "text": "Recognition."}, {"start": 510.16, "end": 513.4, "text": "And what this is asking is for a given token,"}, {"start": 513.4, "end": 516.2, "text": "is there an entity associated with it?"}, {"start": 516.2, "end": 519.04, "text": "So, for example, a token of Arizona"}, {"start": 519.04, "end": 522.0, "text": "has an entity of a US state,"}, {"start": 522.0, "end": 527.0, "text": "whereas a token of Ralph has an entity of a person's name."}, {"start": 527.92, "end": 531.44, "text": "And these are some of the tools that we can apply"}, {"start": 531.44, "end": 533.96, "text": "in this big bag of tools that we have for NLP"}, {"start": 534.0, "end": 538.2, "text": "in order to get from this unstructured human speech"}, {"start": 538.2, "end": 539.44, "text": "through to something structured"}, {"start": 539.44, "end": 541.6800000000001, "text": "that a computer can understand."}, {"start": 541.6800000000001, "end": 543.12, "text": "And once we've done that,"}, {"start": 543.12, "end": 545.48, "text": "then we can apply that structured data"}, {"start": 545.48, "end": 548.4000000000001, "text": "to all sorts of AI applications."}, {"start": 548.4000000000001, "end": 551.2800000000001, "text": "Now, there's obviously a lot more to it than this."}, {"start": 551.2800000000001, "end": 553.4000000000001, "text": "And I've included some links in the description"}, {"start": 553.4000000000001, "end": 554.9200000000001, "text": "if you'd like to know more."}, {"start": 554.9200000000001, "end": 557.84, "text": "But hopefully this made some sense"}, {"start": 557.84, "end": 561.6, "text": "and that you were able to process"}, {"start": 561.6, "end": 563.76, "text": "some of the natural language"}, {"start": 563.76, "end": 566.12, "text": "that I've shared today."}, {"start": 566.12, "end": 567.12, "text": "Thanks for watching."}, {"start": 568.8, "end": 571.2, "text": "If you have questions, please drop us a line below"}, {"start": 571.2, "end": 574.28, "text": "and if you want to see more videos like this in the future,"}, {"start": 574.28, "end": 576.12, "text": "please like and subscribe."}], "language": "en"}
{"title": "Natural Language Processing In 5 Minutes | What Is NLP And How Does It Work? | Simplilearn", "video_id": "CMrHM8a3hqw", "text": "Star Wars fans would be familiar with the golden, life-sized hospitality robot C3PO, while Star Wars might be set in a galaxy far, far away. The reality of having machines talk and respond to us in a human-like manner is already a reality, which keeps getting more and more realistic with every passing day. The people you ask for queries on websites, your smart assistants, even calls made over the internet, all of them have one thing in common. None of them are actually human. Now, you must be thinking, if they are not human, how do they manage to sound and seem so human-like? How do they respond to me so intelligently? And how are they so articulate? This, my friends, is the magic of natural language processing. What is NLP? Natural language processing, or NLP, refers to the branch of artificial intelligence that gives the machines the ability to read, understand, and derive meaning from human languages. NLP combines the field of linguistics and computer science to decipher language structure and guidelines, and to make models which can comprehend, break down, and separate significant details from text and speech. Every day, humans interact with each other through public social media, transferring fast quantities of freely available data to each other. This data is extremely useful in understanding human behavior and customer habits. To analyze and machine learning experts, utilize this data to give machines the ability to mimic human linguistic behavior. This helps save millions in terms of manpower and time, as you don't need to always have a person present at the other end of a phone. NLP is also a lot more widespread than you may realize. You use it every day in seemingly normal and insignificant situations. Don't know how to correctly spell a word? Auto-correct has you covered. Want to see if your article or thesis will get flagged for copyright violations? That's okay. A plagiarism checker will search through the web and find any cases of published documents which may match your work lined by line. While NLP seems really cool, yet a cutting edge and complicated technology concept. It is actually pretty easy to learn. You start off with a document or an article. To make your algorithm understand what is going on in it, you need to process it into a form which is easily comprehensible by the machine. This is no different than making a child learn to read for the first time. You start off by performing segmentation, which is to break the entire document down into its constituent sentences. You can do this by segmenting the article along its punctuations like full stops and comments. For the algorithm to understand these sentences, we get the words in a sentence and to explain them individually to our algorithm. So we break down our sentence into its constituent words and store them. This is called tokenizing where each word is called a token. We can make the learning process faster by getting rid of non-essential words which do not add much meaning to our statement and are just there to make our statement sound more cohesive. These words such as R and the are called stop words. Now that we have the basic form of our document, we need to explain it to our machine. We first start off by explaining that some words like skipping, skips, skipped are the same word with added prefixes and suffixes. This is called stemming. We also identify the base words for different word tense, mood, gender, etc. This is called limitization, stemming from the base word lemma. Now we explain the concept of nouns, verbs, articles, and other parts of speech to the machine by adding these tags to our words. This is called part of speech tagging. And we introduce our machine to pop culture references and everyday names by flagging names of movies, important personalities or locations, etc. that may occur in the document. This is called named into detagging. Once we have our base words and tags, we use a machine learning algorithm like naive bays to teach our model humans sentiment and speech. At the end of the day, most of the techniques used in NLP are simple grammar techniques that we have been taught in school. Here is a question for you, which of these NLP techniques is used to obtain words from sentences? A, stemming, B, tokenization, C, limitization, D, segmentation. Give it a thought and leave your answers in the comments section below. Three lucky winners will receive Amazon gift vouchers. With the increasing demand for automated language solutions, companies are looking for NLP experts to join them and are prepared to offer highly lucrative salaries as well. If you want to learn more about NLP, you can check out simply learn postgraduate program in AI and machine learning in collaboration with IBM. In this program, you will learn about frameworks like Keras and TensorFlow and get hands-on experience in deep learning to become a truly experienced AI engineer. That brings us to the end of this video on NLP. We hope you enjoyed this video. If you did, a thumbs up would be really appreciated. Here's your reminder to subscribe to our channel and to click on the bell icon for more on the latest technologies and trends. Thank you for watching and stay tuned for more from SimpliLearn.", "segments": [{"start": 0.0, "end": 6.2, "text": "Star Wars fans would be familiar with the golden, life-sized hospitality robot C3PO, while"}, {"start": 6.2, "end": 9.68, "text": "Star Wars might be set in a galaxy far, far away."}, {"start": 9.68, "end": 15.56, "text": "The reality of having machines talk and respond to us in a human-like manner is already a reality,"}, {"start": 15.56, "end": 19.240000000000002, "text": "which keeps getting more and more realistic with every passing day."}, {"start": 19.240000000000002, "end": 23.72, "text": "The people you ask for queries on websites, your smart assistants, even calls made over the"}, {"start": 23.72, "end": 26.92, "text": "internet, all of them have one thing in common."}, {"start": 26.92, "end": 28.48, "text": "None of them are actually human."}, {"start": 28.88, "end": 33.52, "text": "Now, you must be thinking, if they are not human, how do they manage to sound and seem"}, {"start": 33.52, "end": 34.92, "text": "so human-like?"}, {"start": 34.92, "end": 37.16, "text": "How do they respond to me so intelligently?"}, {"start": 37.16, "end": 39.32, "text": "And how are they so articulate?"}, {"start": 39.32, "end": 43.52, "text": "This, my friends, is the magic of natural language processing."}, {"start": 43.52, "end": 45.519999999999996, "text": "What is NLP?"}, {"start": 45.519999999999996, "end": 50.08, "text": "Natural language processing, or NLP, refers to the branch of artificial intelligence that"}, {"start": 50.08, "end": 56.0, "text": "gives the machines the ability to read, understand, and derive meaning from human languages."}, {"start": 56.0, "end": 60.64, "text": "NLP combines the field of linguistics and computer science to decipher language structure"}, {"start": 60.64, "end": 65.8, "text": "and guidelines, and to make models which can comprehend, break down, and separate significant"}, {"start": 65.8, "end": 68.12, "text": "details from text and speech."}, {"start": 68.12, "end": 72.88, "text": "Every day, humans interact with each other through public social media, transferring fast"}, {"start": 72.88, "end": 76.2, "text": "quantities of freely available data to each other."}, {"start": 76.2, "end": 81.56, "text": "This data is extremely useful in understanding human behavior and customer habits."}, {"start": 81.56, "end": 86.36, "text": "To analyze and machine learning experts, utilize this data to give machines the ability to"}, {"start": 86.36, "end": 89.16, "text": "mimic human linguistic behavior."}, {"start": 89.16, "end": 93.28, "text": "This helps save millions in terms of manpower and time, as you don't need to always have"}, {"start": 93.28, "end": 96.24000000000001, "text": "a person present at the other end of a phone."}, {"start": 96.24000000000001, "end": 99.96000000000001, "text": "NLP is also a lot more widespread than you may realize."}, {"start": 99.96000000000001, "end": 104.44, "text": "You use it every day in seemingly normal and insignificant situations."}, {"start": 104.44, "end": 106.68, "text": "Don't know how to correctly spell a word?"}, {"start": 106.68, "end": 108.80000000000001, "text": "Auto-correct has you covered."}, {"start": 108.8, "end": 112.88, "text": "Want to see if your article or thesis will get flagged for copyright violations?"}, {"start": 112.88, "end": 113.88, "text": "That's okay."}, {"start": 113.88, "end": 118.32, "text": "A plagiarism checker will search through the web and find any cases of published documents"}, {"start": 118.32, "end": 121.2, "text": "which may match your work lined by line."}, {"start": 121.2, "end": 126.52, "text": "While NLP seems really cool, yet a cutting edge and complicated technology concept."}, {"start": 126.52, "end": 128.6, "text": "It is actually pretty easy to learn."}, {"start": 128.6, "end": 131.0, "text": "You start off with a document or an article."}, {"start": 131.0, "end": 135.72, "text": "To make your algorithm understand what is going on in it, you need to process it into a form"}, {"start": 135.72, "end": 139.12, "text": "which is easily comprehensible by the machine."}, {"start": 139.12, "end": 144.04, "text": "This is no different than making a child learn to read for the first time."}, {"start": 144.04, "end": 148.64, "text": "You start off by performing segmentation, which is to break the entire document down into"}, {"start": 148.64, "end": 151.0, "text": "its constituent sentences."}, {"start": 151.0, "end": 156.32, "text": "You can do this by segmenting the article along its punctuations like full stops and comments."}, {"start": 156.32, "end": 160.92, "text": "For the algorithm to understand these sentences, we get the words in a sentence and to explain"}, {"start": 160.92, "end": 163.2, "text": "them individually to our algorithm."}, {"start": 163.20000000000002, "end": 167.62, "text": "So we break down our sentence into its constituent words and store them."}, {"start": 167.62, "end": 171.60000000000002, "text": "This is called tokenizing where each word is called a token."}, {"start": 171.60000000000002, "end": 176.12, "text": "We can make the learning process faster by getting rid of non-essential words which do not"}, {"start": 176.12, "end": 181.36, "text": "add much meaning to our statement and are just there to make our statement sound more cohesive."}, {"start": 181.36, "end": 186.24, "text": "These words such as R and the are called stop words."}, {"start": 186.24, "end": 190.72000000000003, "text": "Now that we have the basic form of our document, we need to explain it to our machine."}, {"start": 190.72, "end": 195.35999999999999, "text": "We first start off by explaining that some words like skipping, skips, skipped are the"}, {"start": 195.35999999999999, "end": 199.07999999999998, "text": "same word with added prefixes and suffixes."}, {"start": 199.07999999999998, "end": 200.72, "text": "This is called stemming."}, {"start": 200.72, "end": 205.92, "text": "We also identify the base words for different word tense, mood, gender, etc."}, {"start": 205.92, "end": 209.76, "text": "This is called limitization, stemming from the base word lemma."}, {"start": 209.76, "end": 214.96, "text": "Now we explain the concept of nouns, verbs, articles, and other parts of speech to the machine"}, {"start": 214.96, "end": 217.72, "text": "by adding these tags to our words."}, {"start": 217.72, "end": 220.6, "text": "This is called part of speech tagging."}, {"start": 220.6, "end": 224.56, "text": "And we introduce our machine to pop culture references and everyday names by flagging"}, {"start": 224.56, "end": 230.79999999999998, "text": "names of movies, important personalities or locations, etc. that may occur in the document."}, {"start": 230.79999999999998, "end": 233.24, "text": "This is called named into detagging."}, {"start": 233.24, "end": 238.4, "text": "Once we have our base words and tags, we use a machine learning algorithm like naive bays"}, {"start": 238.4, "end": 241.4, "text": "to teach our model humans sentiment and speech."}, {"start": 241.4, "end": 246.01999999999998, "text": "At the end of the day, most of the techniques used in NLP are simple grammar techniques that"}, {"start": 246.01999999999998, "end": 248.04, "text": "we have been taught in school."}, {"start": 248.04, "end": 252.67999999999998, "text": "Here is a question for you, which of these NLP techniques is used to obtain words from"}, {"start": 252.67999999999998, "end": 253.67999999999998, "text": "sentences?"}, {"start": 253.67999999999998, "end": 261.8, "text": "A, stemming, B, tokenization, C, limitization, D, segmentation."}, {"start": 261.8, "end": 265.28, "text": "Give it a thought and leave your answers in the comments section below."}, {"start": 265.28, "end": 268.56, "text": "Three lucky winners will receive Amazon gift vouchers."}, {"start": 268.56, "end": 273.24, "text": "With the increasing demand for automated language solutions, companies are looking for NLP experts"}, {"start": 273.24, "end": 277.76, "text": "to join them and are prepared to offer highly lucrative salaries as well."}, {"start": 277.76, "end": 282.0, "text": "If you want to learn more about NLP, you can check out simply learn postgraduate program"}, {"start": 282.0, "end": 285.56, "text": "in AI and machine learning in collaboration with IBM."}, {"start": 285.56, "end": 290.71999999999997, "text": "In this program, you will learn about frameworks like Keras and TensorFlow and get hands-on experience"}, {"start": 290.71999999999997, "end": 295.36, "text": "in deep learning to become a truly experienced AI engineer."}, {"start": 295.36, "end": 298.76, "text": "That brings us to the end of this video on NLP."}, {"start": 298.76, "end": 300.36, "text": "We hope you enjoyed this video."}, {"start": 300.36, "end": 303.64, "text": "If you did, a thumbs up would be really appreciated."}, {"start": 303.64, "end": 307.56, "text": "Here's your reminder to subscribe to our channel and to click on the bell icon for more"}, {"start": 307.56, "end": 309.84, "text": "on the latest technologies and trends."}, {"start": 309.84, "end": 313.4, "text": "Thank you for watching and stay tuned for more from SimpliLearn."}], "language": "en"}
{"title": "Natural Language Processing - Tokenization (NLP Zero to Hero - Part 1)", "video_id": "fNxaJsNG3-s", "text": "Hi, and welcome to this series on zero-to-hero for natural language processing using TensorFlow. If you're not an expert on AIRML, don't worry, we're taking the concepts of NLP and teaching them from first principles. In this first lesson, we'll talk about how to represent words in a way that a computer can process them, with a view to later training a neural network that can understand their meaning. This process is called tokenization, so let's take a look. Consider the word listen, as you can see here. It's made up of a sequence of letters. These letters can be represented by numbers using an encoding scheme. A popular one called ASCII has these letters represented by these numbers. This bunch of numbers can then represent the word listen. But, the word silent has the same letters and thus the same numbers, just in a different order. So it makes it hard for us to understand sentiment of a word just by the letters in it. So it might be easier instead of encoding letters to encode words. Consider the sentence I love my dog. So what would happen if we start encoding the words in this sentence instead of the letters in each words? So for example, the word I could be one. And then the sentence I love my dog could be one, two, three, four. Now if I take another sentence, for example, I love my cat, how would we encode it? Now we see I love my, has already been given one, two, three. So all I need to do is encode cat, I'll give that the number five. And now if we look at the two sentences, they are one, two, three, four and one, two, three, five, which I'll ready show some form of similarity between them. And it's a similarity you'd expect because they're both about loving a pet. Given this method of encoding sentences into numbers, now let's take a look at some code to achieve this for us. This process, as I mentioned before, is called tokenization and there's an API for that. We'll look at how to use it with Python. So here's your first look at some code to tokenize these sentences. Let's go through it line by line. First of all, we'll need to tokenize our APIs and we can get these from TensorFlow Keras like this. We can represent our sentences as a Python array of strings like this. It's simply the, I love my dog and I love my cat that we saw earlier. Now the fun begins. I can create an instance of a tokenizer object. The num words parameter is the maximum number of words to keep. So instead of, for example, just these two sentences, imagine if we had hundreds of books to tokenize, but we just want the most frequent 100 words in all of that. This would automatically do that for us when we do the next step. And that's to tell the tokenizer to go through all the text and then fit itself to them like this. The full list of words is available as the tokenizer's word index property. So we can take a look at it like this. And then simply print it out. The result will be this dictionary showing the key being the word and the value being the token for that word. So for example, my has a value of three. The tokenizer is also smart enough to catch some exceptions. So for example, if we updated our sentences to this by adding a third sentence, noting that dog here is followed by an exclamation mark. The nice thing is that the tokenizer is smart enough to spot this and not create a new token. It's just dog. And you can see the results here. There's no token for dog exclamation, but there is one for dog. And there's also a new token for the word you. If you want to try this out for yourself, I've put the code in a colab here. Take it for a spin and experiment. You've now seen how words can be tokenized and the tools intensive flow that handled that tokenization for you. Now that your words are represented by numbers like this, you'll next need to represent your sentences by sequences of numbers in the correct order. You'll then have data ready for processing by a neural network to understand or maybe even generate new text. You'll see the tools that you can use to manage this sequencing in the next episode. So don't forget to hit that subscribe button.", "segments": [{"start": 0.0, "end": 9.72, "text": "Hi, and welcome to this series on zero-to-hero for natural language processing using TensorFlow."}, {"start": 9.72, "end": 14.4, "text": "If you're not an expert on AIRML, don't worry, we're taking the concepts of NLP and"}, {"start": 14.4, "end": 17.12, "text": "teaching them from first principles."}, {"start": 17.12, "end": 21.38, "text": "In this first lesson, we'll talk about how to represent words in a way that a computer"}, {"start": 21.38, "end": 25.96, "text": "can process them, with a view to later training a neural network that can understand their"}, {"start": 25.96, "end": 27.400000000000002, "text": "meaning."}, {"start": 27.400000000000002, "end": 31.200000000000003, "text": "This process is called tokenization, so let's take a look."}, {"start": 31.200000000000003, "end": 33.68, "text": "Consider the word listen, as you can see here."}, {"start": 33.68, "end": 36.160000000000004, "text": "It's made up of a sequence of letters."}, {"start": 36.160000000000004, "end": 40.2, "text": "These letters can be represented by numbers using an encoding scheme."}, {"start": 40.2, "end": 45.56, "text": "A popular one called ASCII has these letters represented by these numbers."}, {"start": 45.56, "end": 48.68000000000001, "text": "This bunch of numbers can then represent the word listen."}, {"start": 48.68000000000001, "end": 54.88, "text": "But, the word silent has the same letters and thus the same numbers, just in a different"}, {"start": 54.88, "end": 55.88, "text": "order."}, {"start": 56.0, "end": 61.800000000000004, "text": "So it makes it hard for us to understand sentiment of a word just by the letters in it."}, {"start": 61.800000000000004, "end": 66.36, "text": "So it might be easier instead of encoding letters to encode words."}, {"start": 66.36, "end": 69.44, "text": "Consider the sentence I love my dog."}, {"start": 69.44, "end": 74.16, "text": "So what would happen if we start encoding the words in this sentence instead of the letters"}, {"start": 74.16, "end": 75.72, "text": "in each words?"}, {"start": 75.72, "end": 79.12, "text": "So for example, the word I could be one."}, {"start": 79.12, "end": 84.84, "text": "And then the sentence I love my dog could be one, two, three, four."}, {"start": 84.84, "end": 91.92, "text": "Now if I take another sentence, for example, I love my cat, how would we encode it?"}, {"start": 91.92, "end": 96.92, "text": "Now we see I love my, has already been given one, two, three."}, {"start": 96.92, "end": 102.52000000000001, "text": "So all I need to do is encode cat, I'll give that the number five."}, {"start": 102.52000000000001, "end": 109.12, "text": "And now if we look at the two sentences, they are one, two, three, four and one, two, three,"}, {"start": 109.12, "end": 113.60000000000001, "text": "five, which I'll ready show some form of similarity between them."}, {"start": 113.60000000000001, "end": 118.00000000000001, "text": "And it's a similarity you'd expect because they're both about loving a pet."}, {"start": 118.00000000000001, "end": 123.32000000000001, "text": "Given this method of encoding sentences into numbers, now let's take a look at some code"}, {"start": 123.32000000000001, "end": 125.4, "text": "to achieve this for us."}, {"start": 125.4, "end": 130.84, "text": "This process, as I mentioned before, is called tokenization and there's an API for that."}, {"start": 130.84, "end": 133.16, "text": "We'll look at how to use it with Python."}, {"start": 133.16, "end": 137.24, "text": "So here's your first look at some code to tokenize these sentences."}, {"start": 137.24, "end": 139.88, "text": "Let's go through it line by line."}, {"start": 139.88, "end": 143.92, "text": "First of all, we'll need to tokenize our APIs and we can get these from TensorFlow Keras"}, {"start": 143.92, "end": 145.96, "text": "like this."}, {"start": 145.96, "end": 150.51999999999998, "text": "We can represent our sentences as a Python array of strings like this."}, {"start": 150.51999999999998, "end": 155.48, "text": "It's simply the, I love my dog and I love my cat that we saw earlier."}, {"start": 155.48, "end": 156.92, "text": "Now the fun begins."}, {"start": 156.92, "end": 160.44, "text": "I can create an instance of a tokenizer object."}, {"start": 160.44, "end": 164.48, "text": "The num words parameter is the maximum number of words to keep."}, {"start": 164.48, "end": 168.76, "text": "So instead of, for example, just these two sentences, imagine if we had hundreds of books"}, {"start": 168.76, "end": 174.84, "text": "to tokenize, but we just want the most frequent 100 words in all of that."}, {"start": 174.84, "end": 179.35999999999999, "text": "This would automatically do that for us when we do the next step."}, {"start": 179.35999999999999, "end": 183.84, "text": "And that's to tell the tokenizer to go through all the text and then fit itself to them like"}, {"start": 183.84, "end": 185.92, "text": "this."}, {"start": 185.92, "end": 190.92, "text": "The full list of words is available as the tokenizer's word index property."}, {"start": 190.92, "end": 193.32, "text": "So we can take a look at it like this."}, {"start": 193.32, "end": 195.44, "text": "And then simply print it out."}, {"start": 195.44, "end": 199.88, "text": "The result will be this dictionary showing the key being the word and the value being"}, {"start": 199.88, "end": 201.72, "text": "the token for that word."}, {"start": 201.72, "end": 205.24, "text": "So for example, my has a value of three."}, {"start": 205.24, "end": 209.16, "text": "The tokenizer is also smart enough to catch some exceptions."}, {"start": 209.16, "end": 215.24, "text": "So for example, if we updated our sentences to this by adding a third sentence, noting that"}, {"start": 215.24, "end": 218.6, "text": "dog here is followed by an exclamation mark."}, {"start": 218.6, "end": 224.16, "text": "The nice thing is that the tokenizer is smart enough to spot this and not create a new token."}, {"start": 224.16, "end": 225.96, "text": "It's just dog."}, {"start": 225.96, "end": 227.64, "text": "And you can see the results here."}, {"start": 227.64, "end": 231.52, "text": "There's no token for dog exclamation, but there is one for dog."}, {"start": 231.52, "end": 235.0, "text": "And there's also a new token for the word you."}, {"start": 235.0, "end": 238.72, "text": "If you want to try this out for yourself, I've put the code in a colab here."}, {"start": 238.72, "end": 240.96, "text": "Take it for a spin and experiment."}, {"start": 240.96, "end": 245.0, "text": "You've now seen how words can be tokenized and the tools intensive flow that handled that"}, {"start": 245.0, "end": 247.07999999999998, "text": "tokenization for you."}, {"start": 247.07999999999998, "end": 251.24, "text": "Now that your words are represented by numbers like this, you'll next need to represent"}, {"start": 251.24, "end": 255.96, "text": "your sentences by sequences of numbers in the correct order."}, {"start": 255.96, "end": 260.96000000000004, "text": "You'll then have data ready for processing by a neural network to understand or maybe even"}, {"start": 260.96000000000004, "end": 262.76, "text": "generate new text."}, {"start": 262.76, "end": 267.2, "text": "You'll see the tools that you can use to manage this sequencing in the next episode."}, {"start": 267.2, "end": 268.68, "text": "So don't forget to hit that subscribe button."}], "language": "en"}
{"title": "Neuro Linguistic Programming | Ram Verma | TEDxFORESchool", "video_id": "syvKeNKww_Y", "text": "I never thought of having this subject as my career. In fact, I created a big career out of nothing. And it is called turning the tails into heads. I think this is the topic we are dealing with today. My dear friends, a big personality of this world has set in love and life either we grow or we decay. Every moment, either we are growing or we are decaying. We are having five lives at moment. Your health is there. Your social contacts are there. Your financial life is there. Your spirituality, your relationships and your mental and emotional power is also there. In fact, each moment you need to grow in your life. That is why you are here. You completed your schools, then graduation. Now your post, I mean MBAs, each moment you are growing towards a better orbit. Because we human beings are never satisfied with the status quo. We never want to get stagnant. We always want to grow in life and the life is such a highway where there is no you turn. You need to go ahead every time. And the best thing in life is that when you have wonderful tools and things that makes that life powerful or wonderful or comfortable to. The aim of my talk to is to make you aware of the things that you can do in your life. I will get you a chance also where you can transform little bit also today. And we do not know which part of your life get empowered in a moment also. Life is like a river also. We never enter the same river. It is always changed. Every day we are facing lot many new thoughts, new emotions and we are creating new abilities. For our career, for our family, for our health, for our money and for our relationships also. But the problem occurs when we do not get the things that we plan for and we get the things we do not need at all. We never thought of getting anger. We never thought of getting anxiety. We are never planning for our feeling victim at times. We never plan for our failures. We never plan for our broken relationships then why do we get it? These are the certain tales in our life. And we are planning a lot for our success, good relationships, we are planning to be happy, we are planning to be delightful. But sometimes it seems that we are not getting success. This is the big question when I started my career or learning that technology that I will reveal today. After all why some people are getting their desired results in life and why some are not. While they are in the same environment, we are having the same neurology. Means our body structure or mind structure. But where is the problem? Why some students are getting good marks while others are not? While some people are getting wonderful health, while some are not. While some people are enjoying the good relationships, while some are not. It was a very, very interesting question for me when I started my career as a coach. I never had a formal education of being a human trainer. But since I started my journey, I have been helping and empowering a lot of many people. Since we are looking for always change, change is always constant. But the problem is why don't we get the required change? And once again, it depends upon our communication. In college, in schools, in society, we are talking about the communication of one type. So we are talking to people now. It is beyond that. According to them, there are two communications that you are having at any time. There are communications rather. One, that you are talking to yourself. When you communicate to yourself, it defines whether you will be happy or you will be sad. Whether you will be depressed or you will be delighted. Whether you will be angry, anxiety, frustration, or flexibility. Whether you will be confused or conflicted. Whether you will feel guilty, more victim. This is the communication that you are making to yourself. And this communication is the key to your success and happiness also. Once you define your communication to yourself, it defines completely your, you know, I mean life. The quality of your communication is the quality of your life. And the major part of that communication holds when you are talking to yourself. Number two, when you are talking to your people, this is the second level of communication. That defines how wonderful you will be in your society. What would be the net work or net worth as far as people and finances are concerned. So there should be a technique which deals with both the communication. One, how you are talking to yourself at times and then two, how you are talking to people. So we should be the master of communication. And since we need to heal every time our body is, we need to heal our relationship. We need to heal our money. We are, you know, looking for the ways how to heal our, you know, health. So let me tell you, in fact, your ability to heal yourself is the sign of life that you are enjoying. The better ability of healing is there, the better, better quality of your life is there. Now question arises, when we want to change, why it doesn't the change occur. We are taught in many ways how to avoid the procrastination, how to go for the goals, how to enjoy good relationship, how to enjoy better money, how to create better goals in life and outcomes. Then what is to observe when the thinking is there? Now let me understand, let me make you understand three basic things that you might use in your life every day, every moment rather. Even now, there one, first is your awareness of the things. We need to get aware, aware of the things that we need, our goal, our health. And this awareness is the part of your mind. So you need, you make lot of goals, you need to make lot of targets in life. But this is in your awareness. The second one that we need to understand today, and we need to be taught in that way also that the second part is called our permission. And that comes from your emotions, the sensations, and metaphorically we call it the heart. When we are making a plan that is in our awareness, then there should be a permission from your heart. And permission comes when there is no ecological problem, and there is no internal objection. And this sensation should be strong enough to take you to the next level of life, where you need to get up and go for your job. I mean, outcome. Now question arises, if this emotion is missing, whatever the level of your awareness is, it is just useless. Because unless you have this emotional, you know, a leverage, you can't take action. So one is awareness, and the second is permission from your heart. And the third one is the most important, once again, the most important, the ability of your physiology. Means, your body should be enough able to take that assignment, that you want to complete. Now question arises, what stops us? Why we can't have these wonderful emotions that we need to have? There are three things once again. You are stopping yourself because of three things. Now, I have been helping people from all walks of life. I have been helping people who are suffering from, you know, cancer. I have been helping people who are having a lot of big corporates in this country. I have been helping people from the different industries called Bollywood or Esports also. I have seen, there are three points where you can see that you are stopping yourself. One, you are passed. There might be some past references that you might be holding. There might be some fears. There is four Bias, obstusive compulsive. Some handlers are there. So in case you are not healing, you are passed and you are not creating the distance from those obese OCDs or obstructive compulsive behaviors, you will feel yourself that you are stopping. Two, the third, the second one is the present. At present situation, you might be having a lot of problems. The carrier problem, the money problem, the relationship problem, the financial problems. problems. At that time, your mind may perceive that the problem is bigger than what who or what who I am. This is called your inner self-image transformation. I'll help you do it or transform it in the session itself. And the third one is the most important knowingly or unknowingly, your mind has perceived that the future is no more there or the future is uncertain. And these three things are there in your mind, one, two or all three. It leaves an imprint on your subconscious mind that might be stopping you from gaining good health, relationship, money, and health also. Now, here comes my job. I have been helping people to remove those imprints that are stopping them and they are lying deep in their subconscious mind. And here, I'm using a technique called sub-conscious re-imprinting. And my technique is an LP Neuralinguistic Programming. Two decades back, I started that journey when I was, I wanted to help some of the children who were not having good, you know, learning ability. At that time, I was planning to help them but the problem was that there was no strain or at that time. I got only one book and the book was so complicated that I could not understand it in one, in the very first go. After 10 readings, I came to know that I'm still not getting it well, but at that time, I go to believe my dear friends that if I can not understand it even after 10 readings, there might not be many people who will be reading it after 10 readings. But if I keep reading and understanding the techniques and that the working of the mind, I may be unique in this country. And I read this book more than 80 times. And at that time, though there was no trainer, I started helping people. Since then, I have been helping people around the country. Recently, I helped a woman who sustained many injuries in 26 Alevin Hotel Taj Mumbai. And since then, she has been having four be of sound. Her life was like hell. And in no time, less than half an hour, I removed all the imprints where the fear was lying. I helped two small children. One of them is known as Wonder Girl in, on the Google, if you search and one is known as the Google Girl. They both are of Alevin years old. And one of them is, Alevin year old child is studying in class 10. And she remembers or memorized everything, whatever you give to her. A small child is there. So, this way I have been helping people. I helped a big corporate person here and I brought him into confidence zone where he once again controlled his life and corporate having more than 1700 people. So, this way I have been helping people. This is my journey. But what you can do with it? My message to you is very simple. You have all the tools and all the resources with you. At any time, that you need to do in your life. Even now, some of them, I mean some of you will be cursing the environment, that you have not been in a good environment. My family is not okay. My parents were not okay. My, I mean education was not okay. I couldn't learn particular language at the right time. I was not giving the opportunity. So, you might be cursing your environment. Some of you might be having challenges of responses. That my behavior, this particular behavior is not good enough, but I do not know how to respond to the, I mean world. Some of you might be talking about your capabilities. And most of the schools and colleges are working here only. They are either giving behavioral training or the capability training because they think that while having all these things, we can create the good environment and our success. But beyond that, there are certain things that you need to go through. I need two, three, four people. I mean from the young, I mean management students, those who are ready to come here and see a particular change. It will be only two to three minute exercise. Three for students, please do come. At least four, two boys and two girls will do. Come. It is simple. And rest of you can do this exercise while sitting on your seat. And I will make you do a simple script other imprints of success, imprints of wellness, imprints of good identity. What you need to do, you need to come ahead and you, what you need to do, you should, I mean just stand over here like you face me, you face me, right, good and you make a line. Okay, just look at me, look at me, right. What you need to do, you need to come ahead, come ahead, come ahead, come ahead, come ahead, just good, good and okay, here. So, what you need to do, we are creating a wonderful exercise for you. It is called a neurological alignment. Remember, your mileage, the mileage of your card doesn't depend upon the quality of your, you know, only on the quality of the parts, but it depends upon the alignment, these parts are in, yes or no. So, better alignment you have with yourself, the better mileage or the better success you will get. We think that the, when we are the master of our behavior and capabilities, we will get the result. No, it's not like that. The theory is different now. Dr. Gregory Batson has found a model that I want to do it before you. Now, I'll make them do some, a small exercise. In first round, they will create an audit where they are and in the next journey, they will create the transformation. What you need to do, you can enjoy the things with me, right. So, let us start. What you need to do, I'll keep, I'll make you go little bit backward, only six steps. So, take your steps, very small, like this, then like this, then like this with me, right. I'll take you to a particular thing and I'll create an imprint for your brain, in your brain, in your mind so that you are completely aligned with your purpose in life and you create a new identity and values also. The belief that you need, you may create them, right. Since we are having a small session, you need to be very alert and comfortable also, right. So, just put your shoulders up and be comfortable. And remember, imagine that you are standing in the first level of your alignment and it is called the level of environment. So, my question is just close your eyes in case you are comfortable. Just think of your environment where you are, your school, your college, your family or your company, where you are working. Think of this, where you are enjoying your career or your education or your family, it is your environment usually. Sometimes, you may think of your relatives also, where you pass your time or with your friends. Okay, taking this audit of your environment, go back, just one step, it's small one. And now think of the responses that you are creating. My question is simple, think of an ideal day, how you get up early in the morning and respond to this world. How you get are, I mean, get fresh, come to your college, study, utilize your time or sometimes you don't utilize your times, what activities you are doing and what is your responses in life. How you respond to this world, lot of many challenges are there, are there. So, you must be responding to them. So, this is the level of behavior. Thinking or taking the audit of this level, just step back once again, right, good. And this is the third level called your capabilities. Think of your capability. Think of how much or how much of your capability is being utilized by you. What are the abilities or capabilities that you have been using or utilizing to take care of your health, career or whatever it is. Okay, the answer may be may not be there, don't worry. Just go one step back. Okay, when you are here, think of your beliefs. This is the most important thing. Think of your beliefs. What do you think about yourself? What do you think about your future? What is your thought about your relationships? What do you think about this world? What do you think about people around you? What do you think about your money, success, teachers in the whole world? So, there might be some, I mean some level of thinking. You may say that question is the answer is coming or the answer is not coming, no problem. While taking the audit of your thoughts or your beliefs, now just step back. Good. And here the next level is there and this level is known as the values. What is important to me? Ask this question. Ask this question. What is important to me? As far as I am a student, as far as I am a MBA student or as far as I am a human being, what is important to me? You might get answer once again or you might not get answer, don't worry. Just go, I step back. This is the next level. The most wonderful level once again is called the level of identity. Who I am, may I move on? I help you to find out this question. In case my name is there in the dictionary, how people will define it? Who I am? I'm a successful person. I'm a delightful one. You remember your name is the social identity. Your religion? It is just religious identity. Your gender is gender identity. Who you are beyond it? You might get, you might not get. Okay. Do one thing. Go step back once again, finally. All right. I'll request you all to put your shoulders up now. Have a long breath and those who are involved with me just enjoy this wonderful session here. Putting your shoulders up, have a long breath and think, what is my purpose in life? What is my purpose in life? Fine. In case you have lot of money and time and blessings, what would you like to do? Not only for yourself, but your family. Good. For your society. Think for a second, you are leaving this body here and going into sky, meeting your ancestors and God and God as, whomever you believe in, I'll just make a conversation with them. What would you like to achieve when you are on this earth for a particular period of time? You want to help people? You want to transform the lives of birds and animals? You want to open more schools, good hospitals, old age. What would you like to do in case you have all the liberty, freedom and choice to make here? Think. Keep putting your spine and shoulders up. Good. Think it. Think that there is a timeline in front of you. There is a golden future timeline where you can see yourself growing up to hundreds of age. See what you are doing. There are many things to do. You need to open many schools, hospitals, old age. You need to transform the life of animals, birds and plants, trees. What would you like to do? Go beyond your limits. Just think of the world as your family now. Right. Put your shoulders up. Have a long breath. Think of this purpose. And with this purpose, keeping your shoulders up, have a step forward. Put your shoulders up once again. Think of that purpose. You are a transformer. Create a new identity here. Who you are now? Thinking of that purpose, keeping that purpose in your mind, making those pictures bright, beautiful, bolder and closer to your mental screen. Thinking of the activity that you are involved throughout your life. Think of the identity that you want to create. Who you are now? A transformer, a change maker, a candlelight. Create an identity. Think for a second that your identity card is being designed today. And below your name, what three four words should be there? Write them today. Because these imprints once they are settled in your mind, you are going to enjoy that identity. With that wonderful purpose and identity, take it forward and have a step ahead now. Think of your purpose now. Think of your identity now. And now, design what is important to me, creating good relationship? What is important to me? Having wonderful health? What is important to me? Reading good books? What is important to me now? So that I am having this identity and having this wonderful purpose. Enjoying good health? Doing exercises every day? Find it. What is important to me? Good. Now taking that purpose on your mental screen, thinking of the identity that you have developed, making a list what is important to me in life, have a step ahead now. Good. And now think of the beliefs that you need to adopt in your life about yourself. Create a sound. Yes, I can do it. Listen to this words again. I mean this sound again and again. Let the sound be around your ears in big volume. Good. Keep your shoulders up. Have a long breath and keep listening. Yes, I can do it. Think of the world now. The world is a wonderful place to live in. Think of the people. They are ready to cooperate or help you. Think of the situation. They are helpful. There is always head for you. Even the tail is there. It is helping you to reach the head. Create that belief and having this purpose in your mind, the identity, the values and believe now have one step ahead. Come step ahead. Right. And think of the capability now. Think of the capability that you want to adopt now. What should be the level of your capabilities now? Abilities. You want to read. I mean you want to communicate people with the difference. You want to make relationships. You want to go ahead with the purpose, purposes and the targets. Find which capability that you need to do with that purpose. And now having that purpose in your mind, identity, beliefs, values and capabilities, come ahead. One more. Yes. And it is your behavior. Now design in your mental screen. Design the ideal day. How you are getting up. What is the response on your face? How you are using your tonality? How you are using your physiology, means your body? How you are getting up? What are the behaviors that you need to adopt so that you become a wonderful manager tomorrow? You are not only managing or cooperating. You are managing your anger. Your frustrations, your feeling victim. You are beyond it now. So find what behaviors you need to adopt. And come ahead. Taking each and everything. Come ahead one more step. Yes. And think of the environment, the same environment and how it is helping you. The same environment where you see the challenges, now you see the opportunity because you have developed your purpose because you have developed your identity because you have taken care of your values and beliefs and capability and behavior also. Just go through these seven steps. What is my purpose? Remember once again all of you in case you want to change your life beyond how why is important? Why I am doing? Why I am standing here before you? Why are you sitting here? Why we are doing whatever we are doing? Why comes from the purpose? Once you create the purpose, you create the why. Once you create the why, it creates the sensation in your body and this sensation gives you the permission for your body to utilize all the abilities in life. Now you can open your eyes. Clap for these four people. Thank you very much. What you need to do, just create this and I mean journey once again. And if in case you create your purpose today, that purpose will take you to the next level of life. Remember the healthiest and the happiest people have one thing in common and that is called purpose. This is why they keep their body very thin and slim. They are always happy and healthy and they are laughing a lot. They create wonderful relationship because they have a wonderful purpose in life. So your purpose should be beyond you, where your society, where your family, where your country, where your the whole universe is involved. Thank you very much. Thanks a lot. So what you need to do, you need to create the imprints of your life, the imprints of your purpose, the imprints of your belief, the imprints of your identity. Remember, my name is Ram, it is my, I mean social identity, I am Hindu or Muslim, it is my, I mean religious identity, I belong to this political party or this political party, it is my political identity. But beyond that, who I am, I am, I am Hunkonus Kebath. The question remains always and what we need to do, we need to enhance or increase or expand our identity. We don't, we don't need to shrink it rather we need to enhance it. So my, my message to you is just remember keep developing good imprints on your subconscious mind, the imprints of your health, the imprints of your success, the imprints of your happiness and create the distance from the negative past painful memories, create the distance from the untransformed self-image, create that distance from the frustrations, the depression, the anxiety. And remember, the more imprints you have for your success, the more successful you will be. Let me finish my talk with a wonderful prayer that I make for yourself. May you enjoy your success all the time, may the wonderful rays of sun, also on your wonderful faces and gives you new energy. May your family keep expanding day after day, may your crops remain green, may your business keep growing and may you be safe in the palms of Almighty, Almighty, every time. Thank you.", "segments": [{"start": 0.0, "end": 23.0, "text": "I never thought of having this subject as my career."}, {"start": 23.0, "end": 27.16, "text": "In fact, I created a big career out of nothing."}, {"start": 27.16, "end": 30.16, "text": "And it is called turning the tails into heads."}, {"start": 30.16, "end": 34.16, "text": "I think this is the topic we are dealing with today."}, {"start": 34.16, "end": 40.16, "text": "My dear friends, a big personality of this world has set in love and life either we grow"}, {"start": 40.16, "end": 41.16, "text": "or we decay."}, {"start": 41.16, "end": 44.16, "text": "Every moment, either we are growing or we are decaying."}, {"start": 44.16, "end": 46.16, "text": "We are having five lives at moment."}, {"start": 46.16, "end": 47.16, "text": "Your health is there."}, {"start": 47.16, "end": 49.16, "text": "Your social contacts are there."}, {"start": 49.16, "end": 51.16, "text": "Your financial life is there."}, {"start": 51.16, "end": 56.16, "text": "Your spirituality, your relationships and your mental and emotional power is also there."}, {"start": 56.160000000000004, "end": 59.160000000000004, "text": "In fact, each moment you need to grow in your life."}, {"start": 59.160000000000004, "end": 61.160000000000004, "text": "That is why you are here."}, {"start": 61.160000000000004, "end": 63.160000000000004, "text": "You completed your schools, then graduation."}, {"start": 63.160000000000004, "end": 70.16, "text": "Now your post, I mean MBAs, each moment you are growing towards a better orbit."}, {"start": 70.16, "end": 75.16, "text": "Because we human beings are never satisfied with the status quo."}, {"start": 75.16, "end": 78.16, "text": "We never want to get stagnant."}, {"start": 78.16, "end": 84.16, "text": "We always want to grow in life and the life is such a highway where there is no you turn."}, {"start": 84.16, "end": 87.16, "text": "You need to go ahead every time."}, {"start": 87.16, "end": 98.16, "text": "And the best thing in life is that when you have wonderful tools and things that makes that life powerful or wonderful or comfortable to."}, {"start": 98.16, "end": 105.16, "text": "The aim of my talk to is to make you aware of the things that you can do in your life."}, {"start": 105.16, "end": 111.16, "text": "I will get you a chance also where you can transform little bit also today."}, {"start": 111.16, "end": 117.16, "text": "And we do not know which part of your life get empowered in a moment also."}, {"start": 117.16, "end": 120.16, "text": "Life is like a river also."}, {"start": 120.16, "end": 122.16, "text": "We never enter the same river."}, {"start": 122.16, "end": 124.16, "text": "It is always changed."}, {"start": 124.16, "end": 131.16, "text": "Every day we are facing lot many new thoughts, new emotions and we are creating new abilities."}, {"start": 131.16, "end": 137.16, "text": "For our career, for our family, for our health, for our money and for our relationships also."}, {"start": 137.16, "end": 146.16, "text": "But the problem occurs when we do not get the things that we plan for and we get the things we do not need at all."}, {"start": 146.16, "end": 148.16, "text": "We never thought of getting anger."}, {"start": 148.16, "end": 151.16, "text": "We never thought of getting anxiety."}, {"start": 151.16, "end": 156.16, "text": "We are never planning for our feeling victim at times."}, {"start": 156.16, "end": 159.16, "text": "We never plan for our failures."}, {"start": 159.16, "end": 163.16, "text": "We never plan for our broken relationships then why do we get it?"}, {"start": 163.16, "end": 167.16, "text": "These are the certain tales in our life."}, {"start": 167.16, "end": 176.16, "text": "And we are planning a lot for our success, good relationships, we are planning to be happy, we are planning to be delightful."}, {"start": 176.16, "end": 182.16, "text": "But sometimes it seems that we are not getting success."}, {"start": 182.16, "end": 189.16, "text": "This is the big question when I started my career or learning that technology that I will reveal today."}, {"start": 189.16, "end": 194.16, "text": "After all why some people are getting their desired results in life and why some are not."}, {"start": 194.16, "end": 199.16, "text": "While they are in the same environment, we are having the same neurology."}, {"start": 199.16, "end": 202.16, "text": "Means our body structure or mind structure."}, {"start": 202.16, "end": 204.16, "text": "But where is the problem?"}, {"start": 204.16, "end": 207.16, "text": "Why some students are getting good marks while others are not?"}, {"start": 207.16, "end": 210.16, "text": "While some people are getting wonderful health, while some are not."}, {"start": 210.16, "end": 214.16, "text": "While some people are enjoying the good relationships, while some are not."}, {"start": 214.16, "end": 220.16, "text": "It was a very, very interesting question for me when I started my career as a coach."}, {"start": 220.16, "end": 225.16, "text": "I never had a formal education of being a human trainer."}, {"start": 225.16, "end": 232.16, "text": "But since I started my journey, I have been helping and empowering a lot of many people."}, {"start": 232.16, "end": 237.16, "text": "Since we are looking for always change, change is always constant."}, {"start": 237.16, "end": 242.16, "text": "But the problem is why don't we get the required change?"}, {"start": 242.16, "end": 247.16, "text": "And once again, it depends upon our communication."}, {"start": 247.16, "end": 252.16, "text": "In college, in schools, in society, we are talking about the communication of one type."}, {"start": 252.16, "end": 254.16, "text": "So we are talking to people now."}, {"start": 254.16, "end": 255.16, "text": "It is beyond that."}, {"start": 255.16, "end": 259.15999999999997, "text": "According to them, there are two communications that you are having at any time."}, {"start": 259.15999999999997, "end": 260.15999999999997, "text": "There are communications rather."}, {"start": 260.15999999999997, "end": 263.15999999999997, "text": "One, that you are talking to yourself."}, {"start": 263.15999999999997, "end": 268.15999999999997, "text": "When you communicate to yourself, it defines whether you will be happy or you will be sad."}, {"start": 268.16, "end": 271.16, "text": "Whether you will be depressed or you will be delighted."}, {"start": 271.16, "end": 274.16, "text": "Whether you will be angry, anxiety, frustration, or flexibility."}, {"start": 274.16, "end": 276.16, "text": "Whether you will be confused or conflicted."}, {"start": 276.16, "end": 278.16, "text": "Whether you will feel guilty, more victim."}, {"start": 278.16, "end": 281.16, "text": "This is the communication that you are making to yourself."}, {"start": 281.16, "end": 285.16, "text": "And this communication is the key to your success and happiness also."}, {"start": 285.16, "end": 291.16, "text": "Once you define your communication to yourself, it defines completely your, you know, I mean life."}, {"start": 291.16, "end": 295.16, "text": "The quality of your communication is the quality of your life."}, {"start": 295.16, "end": 300.16, "text": "And the major part of that communication holds when you are talking to yourself."}, {"start": 300.16, "end": 304.16, "text": "Number two, when you are talking to your people, this is the second level of communication."}, {"start": 304.16, "end": 307.16, "text": "That defines how wonderful you will be in your society."}, {"start": 307.16, "end": 312.16, "text": "What would be the net work or net worth as far as people and finances are concerned."}, {"start": 312.16, "end": 316.16, "text": "So there should be a technique which deals with both the communication."}, {"start": 316.16, "end": 321.16, "text": "One, how you are talking to yourself at times and then two, how you are talking to people."}, {"start": 321.16, "end": 324.16, "text": "So we should be the master of communication."}, {"start": 324.16, "end": 328.16, "text": "And since we need to heal every time our body is, we need to heal our relationship."}, {"start": 328.16, "end": 330.16, "text": "We need to heal our money."}, {"start": 330.16, "end": 337.16, "text": "We are, you know, looking for the ways how to heal our, you know, health."}, {"start": 337.16, "end": 345.16, "text": "So let me tell you, in fact, your ability to heal yourself is the sign of life that you are enjoying."}, {"start": 345.16, "end": 352.16, "text": "The better ability of healing is there, the better, better quality of your life is there."}, {"start": 353.16, "end": 359.16, "text": "Now question arises, when we want to change, why it doesn't the change occur."}, {"start": 359.16, "end": 370.16, "text": "We are taught in many ways how to avoid the procrastination, how to go for the goals, how to enjoy good relationship, how to enjoy better money, how to create better goals in life and outcomes."}, {"start": 370.16, "end": 373.16, "text": "Then what is to observe when the thinking is there?"}, {"start": 373.16, "end": 381.16, "text": "Now let me understand, let me make you understand three basic things that you might use in your life every day, every moment rather."}, {"start": 382.16, "end": 387.16, "text": "Even now, there one, first is your awareness of the things."}, {"start": 387.16, "end": 392.16, "text": "We need to get aware, aware of the things that we need, our goal, our health."}, {"start": 392.16, "end": 395.16, "text": "And this awareness is the part of your mind."}, {"start": 395.16, "end": 400.16, "text": "So you need, you make lot of goals, you need to make lot of targets in life."}, {"start": 400.16, "end": 403.16, "text": "But this is in your awareness."}, {"start": 403.16, "end": 409.16, "text": "The second one that we need to understand today, and we need to be taught in that way also that"}, {"start": 409.16, "end": 412.16, "text": "the second part is called our permission."}, {"start": 412.16, "end": 418.16, "text": "And that comes from your emotions, the sensations, and metaphorically we call it the heart."}, {"start": 418.16, "end": 425.16, "text": "When we are making a plan that is in our awareness, then there should be a permission from your heart."}, {"start": 425.16, "end": 430.16, "text": "And permission comes when there is no ecological problem, and there is no internal objection."}, {"start": 430.16, "end": 438.16, "text": "And this sensation should be strong enough to take you to the next level of life, where you need to get up and go for your job."}, {"start": 438.16, "end": 440.16, "text": "I mean, outcome."}, {"start": 440.16, "end": 449.16, "text": "Now question arises, if this emotion is missing, whatever the level of your awareness is, it is just useless."}, {"start": 449.16, "end": 458.16, "text": "Because unless you have this emotional, you know, a leverage, you can't take action."}, {"start": 458.16, "end": 462.16, "text": "So one is awareness, and the second is permission from your heart."}, {"start": 462.16, "end": 469.16, "text": "And the third one is the most important, once again, the most important, the ability of your physiology."}, {"start": 469.16, "end": 479.16, "text": "Means, your body should be enough able to take that assignment, that you want to complete."}, {"start": 479.16, "end": 485.16, "text": "Now question arises, what stops us? Why we can't have these wonderful emotions that we need to have?"}, {"start": 485.16, "end": 487.16, "text": "There are three things once again."}, {"start": 487.16, "end": 490.16, "text": "You are stopping yourself because of three things."}, {"start": 490.16, "end": 493.16, "text": "Now, I have been helping people from all walks of life."}, {"start": 493.16, "end": 496.16, "text": "I have been helping people who are suffering from, you know, cancer."}, {"start": 496.16, "end": 500.16, "text": "I have been helping people who are having a lot of big corporates in this country."}, {"start": 500.16, "end": 505.16, "text": "I have been helping people from the different industries called Bollywood or Esports also."}, {"start": 505.16, "end": 510.16, "text": "I have seen, there are three points where you can see that you are stopping yourself."}, {"start": 510.16, "end": 512.1600000000001, "text": "One, you are passed."}, {"start": 512.1600000000001, "end": 515.1600000000001, "text": "There might be some past references that you might be holding."}, {"start": 515.1600000000001, "end": 516.1600000000001, "text": "There might be some fears."}, {"start": 516.16, "end": 518.16, "text": "There is four Bias, obstusive compulsive."}, {"start": 518.16, "end": 520.16, "text": "Some handlers are there."}, {"start": 520.16, "end": 525.16, "text": "So in case you are not healing, you are passed and you are not creating the distance from those obese OCDs"}, {"start": 525.16, "end": 531.16, "text": "or obstructive compulsive behaviors, you will feel yourself that you are stopping."}, {"start": 531.16, "end": 536.16, "text": "Two, the third, the second one is the present."}, {"start": 536.16, "end": 539.16, "text": "At present situation, you might be having a lot of problems."}, {"start": 539.16, "end": 544.16, "text": "The carrier problem, the money problem, the relationship problem, the financial problems."}, {"start": 544.16, "end": 549.26, "text": "problems. At that time, your mind may perceive that the problem is bigger than what who or"}, {"start": 549.26, "end": 555.1999999999999, "text": "what who I am. This is called your inner self-image transformation. I'll help you do it or"}, {"start": 555.1999999999999, "end": 562.9399999999999, "text": "transform it in the session itself. And the third one is the most important knowingly or"}, {"start": 562.9399999999999, "end": 569.14, "text": "unknowingly, your mind has perceived that the future is no more there or the future is uncertain."}, {"start": 569.14, "end": 575.96, "text": "And these three things are there in your mind, one, two or all three. It leaves an imprint on your"}, {"start": 575.96, "end": 582.4399999999999, "text": "subconscious mind that might be stopping you from gaining good health, relationship, money,"}, {"start": 582.4399999999999, "end": 593.8, "text": "and health also. Now, here comes my job. I have been helping people to remove those imprints that are"}, {"start": 593.8000000000001, "end": 600.2400000000001, "text": "stopping them and they are lying deep in their subconscious mind. And here, I'm using a"}, {"start": 600.2400000000001, "end": 607.48, "text": "technique called sub-conscious re-imprinting. And my technique is an LP Neuralinguistic Programming."}, {"start": 607.48, "end": 614.48, "text": "Two decades back, I started that journey when I was, I wanted to help some of the children who"}, {"start": 614.48, "end": 621.2400000000001, "text": "were not having good, you know, learning ability. At that time, I was planning to help them but"}, {"start": 621.32, "end": 628.0, "text": "the problem was that there was no strain or at that time. I got only one book and the book was so"}, {"start": 628.0, "end": 632.84, "text": "complicated that I could not understand it in one, in the very first go. After 10 readings,"}, {"start": 632.84, "end": 639.12, "text": "I came to know that I'm still not getting it well, but at that time, I go to believe my dear friends"}, {"start": 639.12, "end": 645.96, "text": "that if I can not understand it even after 10 readings, there might not be many people who will"}, {"start": 645.96, "end": 652.2, "text": "be reading it after 10 readings. But if I keep reading and understanding the techniques and"}, {"start": 652.2, "end": 659.52, "text": "that the working of the mind, I may be unique in this country. And I read this book more than 80"}, {"start": 659.52, "end": 664.6800000000001, "text": "times. And at that time, though there was no trainer, I started helping people. Since then,"}, {"start": 664.6800000000001, "end": 671.12, "text": "I have been helping people around the country. Recently, I helped a woman who sustained"}, {"start": 671.12, "end": 677.88, "text": "many injuries in 26 Alevin Hotel Taj Mumbai. And since then, she has been having four"}, {"start": 677.88, "end": 684.76, "text": "be of sound. Her life was like hell. And in no time, less than half an hour, I removed all the"}, {"start": 684.76, "end": 692.0, "text": "imprints where the fear was lying. I helped two small children. One of them is known as Wonder"}, {"start": 692.0, "end": 696.92, "text": "Girl in, on the Google, if you search and one is known as the Google Girl. They both are of"}, {"start": 696.92, "end": 704.68, "text": "Alevin years old. And one of them is, Alevin year old child is studying in class 10. And she"}, {"start": 704.68, "end": 710.0, "text": "remembers or memorized everything, whatever you give to her. A small child is there. So, this"}, {"start": 710.0, "end": 715.28, "text": "way I have been helping people. I helped a big corporate person here and I brought him into"}, {"start": 715.28, "end": 722.0, "text": "confidence zone where he once again controlled his life and corporate having more than 1700"}, {"start": 722.08, "end": 728.6, "text": "people. So, this way I have been helping people. This is my journey. But what you can do with it?"}, {"start": 728.6, "end": 737.36, "text": "My message to you is very simple. You have all the tools and all the resources with you. At any"}, {"start": 737.36, "end": 744.6, "text": "time, that you need to do in your life. Even now, some of them, I mean some of you will be cursing"}, {"start": 744.6, "end": 750.08, "text": "the environment, that you have not been in a good environment. My family is not okay. My"}, {"start": 750.08, "end": 755.32, "text": "parents were not okay. My, I mean education was not okay. I couldn't learn particular language"}, {"start": 755.32, "end": 760.84, "text": "at the right time. I was not giving the opportunity. So, you might be cursing your environment. Some"}, {"start": 760.84, "end": 765.44, "text": "of you might be having challenges of responses. That my behavior, this particular behavior is not"}, {"start": 765.44, "end": 772.44, "text": "good enough, but I do not know how to respond to the, I mean world. Some of you might be talking"}, {"start": 772.44, "end": 778.4000000000001, "text": "about your capabilities. And most of the schools and colleges are working here only. They are either"}, {"start": 778.4, "end": 784.64, "text": "giving behavioral training or the capability training because they think that while having all"}, {"start": 784.64, "end": 790.8, "text": "these things, we can create the good environment and our success. But beyond that, there are certain"}, {"start": 790.8, "end": 798.68, "text": "things that you need to go through. I need two, three, four people. I mean from the young, I mean"}, {"start": 798.68, "end": 804.52, "text": "management students, those who are ready to come here and see a particular change. It will be"}, {"start": 804.52, "end": 811.8, "text": "only two to three minute exercise. Three for students, please do come. At least four, two boys"}, {"start": 811.8, "end": 817.88, "text": "and two girls will do. Come. It is simple. And rest of you can do this exercise while sitting on"}, {"start": 817.88, "end": 825.64, "text": "your seat. And I will make you do a simple script other imprints of success, imprints of wellness,"}, {"start": 825.64, "end": 832.72, "text": "imprints of good identity. What you need to do, you need to come ahead and you, what you need to do,"}, {"start": 832.72, "end": 841.96, "text": "you should, I mean just stand over here like you face me, you face me, right, good and you make a line."}, {"start": 841.96, "end": 848.28, "text": "Okay, just look at me, look at me, right. What you need to do, you need to come ahead, come ahead,"}, {"start": 848.28, "end": 858.0400000000001, "text": "come ahead, come ahead, come ahead, just good, good and okay, here. So, what you need to do, we are"}, {"start": 858.04, "end": 864.56, "text": "creating a wonderful exercise for you. It is called a neurological alignment. Remember, your"}, {"start": 864.56, "end": 870.92, "text": "mileage, the mileage of your card doesn't depend upon the quality of your, you know, only on the"}, {"start": 870.92, "end": 880.36, "text": "quality of the parts, but it depends upon the alignment, these parts are in, yes or no. So,"}, {"start": 880.36, "end": 884.8399999999999, "text": "better alignment you have with yourself, the better mileage or the better success you will get."}, {"start": 884.84, "end": 891.5600000000001, "text": "We think that the, when we are the master of our behavior and capabilities, we will get the result."}, {"start": 891.5600000000001, "end": 898.6, "text": "No, it's not like that. The theory is different now. Dr. Gregory Batson has found a model that I want"}, {"start": 898.6, "end": 907.76, "text": "to do it before you. Now, I'll make them do some, a small exercise. In first round, they will"}, {"start": 907.76, "end": 916.4, "text": "create an audit where they are and in the next journey, they will create the transformation. What you"}, {"start": 916.4, "end": 924.64, "text": "need to do, you can enjoy the things with me, right. So, let us start. What you need to do, I'll keep,"}, {"start": 924.64, "end": 932.64, "text": "I'll make you go little bit backward, only six steps. So, take your steps, very small, like this,"}, {"start": 933.12, "end": 938.72, "text": "then like this, then like this with me, right. I'll take you to a particular thing and I'll create"}, {"start": 938.72, "end": 944.88, "text": "an imprint for your brain, in your brain, in your mind so that you are completely aligned with your"}, {"start": 944.88, "end": 950.4, "text": "purpose in life and you create a new identity and values also. The belief that you need,"}, {"start": 951.68, "end": 958.88, "text": "you may create them, right. Since we are having a small session, you need to be very alert and"}, {"start": 958.88, "end": 965.12, "text": "comfortable also, right. So, just put your shoulders up and be comfortable. And remember,"}, {"start": 965.12, "end": 970.16, "text": "imagine that you are standing in the first level of your alignment and it is called the level of"}, {"start": 970.16, "end": 975.92, "text": "environment. So, my question is just close your eyes in case you are comfortable. Just think of your"}, {"start": 975.92, "end": 980.56, "text": "environment where you are, your school, your college, your family or your company, where you are"}, {"start": 980.56, "end": 986.48, "text": "working. Think of this, where you are enjoying your career or your education or your family,"}, {"start": 986.48, "end": 991.76, "text": "it is your environment usually. Sometimes, you may think of your relatives also, where you pass"}, {"start": 991.76, "end": 999.04, "text": "your time or with your friends. Okay, taking this audit of your environment, go back, just one step,"}, {"start": 999.04, "end": 1006.0, "text": "it's small one. And now think of the responses that you are creating. My question is simple,"}, {"start": 1006.0, "end": 1011.04, "text": "think of an ideal day, how you get up early in the morning and respond to this world."}, {"start": 1012.0000000000001, "end": 1021.1200000000001, "text": "How you get are, I mean, get fresh, come to your college, study, utilize your time or sometimes"}, {"start": 1021.1200000000001, "end": 1026.0, "text": "you don't utilize your times, what activities you are doing and what is your responses in life."}, {"start": 1026.72, "end": 1031.6000000000001, "text": "How you respond to this world, lot of many challenges are there, are there. So, you must be"}, {"start": 1031.6000000000001, "end": 1037.92, "text": "responding to them. So, this is the level of behavior. Thinking or taking the audit of this"}, {"start": 1037.92, "end": 1047.76, "text": "level, just step back once again, right, good. And this is the third level called your capabilities."}, {"start": 1047.76, "end": 1055.8400000000001, "text": "Think of your capability. Think of how much or how much of your capability is being utilized by you."}, {"start": 1057.2, "end": 1064.0, "text": "What are the abilities or capabilities that you have been using or utilizing to take care of your"}, {"start": 1064.0, "end": 1072.8, "text": "health, career or whatever it is. Okay, the answer may be may not be there, don't worry."}, {"start": 1074.16, "end": 1084.8, "text": "Just go one step back. Okay, when you are here, think of your beliefs. This is the most important"}, {"start": 1084.8, "end": 1092.32, "text": "thing. Think of your beliefs. What do you think about yourself? What do you think about your future?"}, {"start": 1092.32, "end": 1099.28, "text": "What is your thought about your relationships? What do you think about this world? What do you"}, {"start": 1099.28, "end": 1108.32, "text": "think about people around you? What do you think about your money, success, teachers in the whole world?"}, {"start": 1108.32, "end": 1113.52, "text": "So, there might be some, I mean some level of thinking. You may say that question is the answer is"}, {"start": 1113.52, "end": 1119.9199999999998, "text": "coming or the answer is not coming, no problem. While taking the audit of your thoughts or your beliefs,"}, {"start": 1120.48, "end": 1128.5600000000002, "text": "now just step back. Good. And here the next level is there and this level is known as the values."}, {"start": 1128.5600000000002, "end": 1133.3600000000001, "text": "What is important to me? Ask this question. Ask this question. What is important to me?"}, {"start": 1134.88, "end": 1139.76, "text": "As far as I am a student, as far as I am a MBA student or as far as I am a human being,"}, {"start": 1139.76, "end": 1147.8400000000001, "text": "what is important to me? You might get answer once again or you might not get answer, don't worry."}, {"start": 1147.84, "end": 1156.0, "text": "Just go, I step back. This is the next level. The most wonderful level once again is called the"}, {"start": 1156.0, "end": 1166.08, "text": "level of identity. Who I am, may I move on? I help you to find out this question. In case my name is"}, {"start": 1166.08, "end": 1174.72, "text": "there in the dictionary, how people will define it? Who I am? I'm a successful person. I'm a delightful"}, {"start": 1174.72, "end": 1181.76, "text": "one. You remember your name is the social identity. Your religion? It is just religious identity."}, {"start": 1181.76, "end": 1186.4, "text": "Your gender is gender identity. Who you are beyond it? You might get, you might not get."}, {"start": 1187.44, "end": 1192.56, "text": "Okay. Do one thing. Go step back once again, finally."}, {"start": 1194.88, "end": 1199.52, "text": "All right. I'll request you all to put your shoulders up now."}, {"start": 1199.76, "end": 1208.24, "text": "Have a long breath and those who are involved with me just enjoy this wonderful session here. Putting"}, {"start": 1208.24, "end": 1217.76, "text": "your shoulders up, have a long breath and think, what is my purpose in life? What is my purpose in life?"}, {"start": 1217.76, "end": 1226.24, "text": "Fine. In case you have lot of money and time and blessings, what would you like to do? Not only for"}, {"start": 1226.24, "end": 1238.0, "text": "yourself, but your family. Good. For your society. Think for a second, you are leaving this"}, {"start": 1238.0, "end": 1245.44, "text": "body here and going into sky, meeting your ancestors and God and God as, whomever you believe in,"}, {"start": 1246.24, "end": 1252.72, "text": "I'll just make a conversation with them. What would you like to achieve when you are on this earth"}, {"start": 1252.8, "end": 1260.64, "text": "for a particular period of time? You want to help people? You want to transform the lives of birds and"}, {"start": 1260.64, "end": 1270.56, "text": "animals? You want to open more schools, good hospitals, old age. What would you like to do in case"}, {"start": 1271.52, "end": 1280.0, "text": "you have all the liberty, freedom and choice to make here? Think. Keep putting your spine and shoulders"}, {"start": 1280.0, "end": 1293.52, "text": "up. Good. Think it. Think that there is a timeline in front of you. There is a golden future timeline"}, {"start": 1293.52, "end": 1299.28, "text": "where you can see yourself growing up to hundreds of age. See what you are doing."}, {"start": 1302.48, "end": 1309.44, "text": "There are many things to do. You need to open many schools, hospitals, old age. You need to transform"}, {"start": 1309.44, "end": 1317.04, "text": "the life of animals, birds and plants, trees. What would you like to do? Go beyond your limits."}, {"start": 1317.04, "end": 1327.2, "text": "Just think of the world as your family now. Right. Put your shoulders up. Have a long breath."}, {"start": 1327.8400000000001, "end": 1334.72, "text": "Think of this purpose. And with this purpose, keeping your shoulders up, have a step forward."}, {"start": 1334.72, "end": 1344.0, "text": "Put your shoulders up once again. Think of that purpose. You are a transformer. Create a new identity"}, {"start": 1344.0, "end": 1351.3600000000001, "text": "here. Who you are now? Thinking of that purpose, keeping that purpose in your mind, making those"}, {"start": 1351.3600000000001, "end": 1358.48, "text": "pictures bright, beautiful, bolder and closer to your mental screen. Thinking of the activity that"}, {"start": 1358.48, "end": 1366.96, "text": "you are involved throughout your life. Think of the identity that you want to create. Who you are"}, {"start": 1366.96, "end": 1378.56, "text": "now? A transformer, a change maker, a candlelight. Create an identity. Think for a second that your"}, {"start": 1378.56, "end": 1385.6, "text": "identity card is being designed today. And below your name, what three four words should be there?"}, {"start": 1385.6000000000001, "end": 1391.6000000000001, "text": "Write them today. Because these imprints once they are settled in your mind, you are going to enjoy"}, {"start": 1391.6000000000001, "end": 1399.3600000000001, "text": "that identity. With that wonderful purpose and identity, take it forward and have a step ahead now."}, {"start": 1401.1200000000001, "end": 1408.3200000000002, "text": "Think of your purpose now. Think of your identity now. And now, design what is important to me,"}, {"start": 1408.56, "end": 1416.32, "text": "creating good relationship? What is important to me? Having wonderful health? What is important to me?"}, {"start": 1416.32, "end": 1422.24, "text": "Reading good books? What is important to me now? So that I am having this identity and having"}, {"start": 1422.24, "end": 1432.8, "text": "this wonderful purpose. Enjoying good health? Doing exercises every day? Find it. What is important to me?"}, {"start": 1438.48, "end": 1447.9199999999998, "text": "Good. Now taking that purpose on your mental screen, thinking of the identity that you have"}, {"start": 1447.9199999999998, "end": 1456.24, "text": "developed, making a list what is important to me in life, have a step ahead now. Good. And now"}, {"start": 1456.24, "end": 1462.3999999999999, "text": "think of the beliefs that you need to adopt in your life about yourself. Create a sound. Yes,"}, {"start": 1462.3999999999999, "end": 1467.76, "text": "I can do it. Listen to this words again. I mean this sound again and again. Let the sound be"}, {"start": 1468.72, "end": 1475.2, "text": "around your ears in big volume. Good. Keep your shoulders up. Have a long breath and keep"}, {"start": 1475.2, "end": 1479.9199999999998, "text": "listening. Yes, I can do it. Think of the world now. The world is a wonderful place to live in."}, {"start": 1480.48, "end": 1485.28, "text": "Think of the people. They are ready to cooperate or help you. Think of the situation. They are"}, {"start": 1485.28, "end": 1491.52, "text": "helpful. There is always head for you. Even the tail is there. It is helping you to reach the head."}, {"start": 1491.52, "end": 1500.08, "text": "Create that belief and having this purpose in your mind, the identity, the values and believe"}, {"start": 1500.08, "end": 1506.48, "text": "now have one step ahead. Come step ahead. Right. And think of the capability now. Think of the"}, {"start": 1506.48, "end": 1513.52, "text": "capability that you want to adopt now. What should be the level of your capabilities now? Abilities."}, {"start": 1513.52, "end": 1518.4, "text": "You want to read. I mean you want to communicate people with the difference. You want to make relationships."}, {"start": 1519.2, "end": 1525.2800000000002, "text": "You want to go ahead with the purpose, purposes and the targets. Find which capability that you need"}, {"start": 1525.2800000000002, "end": 1533.2800000000002, "text": "to do with that purpose. And now having that purpose in your mind, identity, beliefs, values and"}, {"start": 1533.2800000000002, "end": 1541.8400000000001, "text": "capabilities, come ahead. One more. Yes. And it is your behavior. Now design in your mental screen."}, {"start": 1542.4, "end": 1551.2, "text": "Design the ideal day. How you are getting up. What is the response on your face? How you are using"}, {"start": 1551.2, "end": 1556.64, "text": "your tonality? How you are using your physiology, means your body? How you are getting up?"}, {"start": 1557.5200000000002, "end": 1562.2400000000002, "text": "What are the behaviors that you need to adopt so that you become a wonderful manager tomorrow?"}, {"start": 1562.96, "end": 1566.8000000000002, "text": "You are not only managing or cooperating. You are managing your anger. Your frustrations,"}, {"start": 1567.36, "end": 1573.12, "text": "your feeling victim. You are beyond it now. So find what behaviors you need to adopt."}, {"start": 1575.6, "end": 1580.48, "text": "And come ahead. Taking each and everything. Come ahead one more step."}, {"start": 1581.52, "end": 1586.08, "text": "Yes. And think of the environment, the same environment and how it is helping you."}, {"start": 1588.6399999999999, "end": 1593.44, "text": "The same environment where you see the challenges, now you see the opportunity because you have"}, {"start": 1593.44, "end": 1599.92, "text": "developed your purpose because you have developed your identity because you have taken care of"}, {"start": 1599.92, "end": 1606.3200000000002, "text": "your values and beliefs and capability and behavior also. Just go through these seven steps."}, {"start": 1607.52, "end": 1613.68, "text": "What is my purpose? Remember once again all of you in case you want to change your life"}, {"start": 1614.16, "end": 1620.0, "text": "beyond how why is important? Why I am doing? Why I am standing here before you? Why are you sitting"}, {"start": 1620.72, "end": 1629.36, "text": "here? Why we are doing whatever we are doing? Why comes from the purpose? Once you create the purpose,"}, {"start": 1629.36, "end": 1635.52, "text": "you create the why. Once you create the why, it creates the sensation in your body and this sensation"}, {"start": 1635.52, "end": 1642.96, "text": "gives you the permission for your body to utilize all the abilities in life. Now you can open your eyes."}, {"start": 1643.92, "end": 1650.88, "text": "Clap for these four people. Thank you very much. What you need to do, just create this and"}, {"start": 1652.16, "end": 1658.8, "text": "I mean journey once again. And if in case you create your purpose today, that purpose will take"}, {"start": 1658.8, "end": 1664.56, "text": "you to the next level of life. Remember the healthiest and the happiest people have one thing in common"}, {"start": 1664.56, "end": 1670.72, "text": "and that is called purpose. This is why they keep their body very thin and slim. They are always happy"}, {"start": 1670.72, "end": 1675.2, "text": "and healthy and they are laughing a lot. They create wonderful relationship because they have a"}, {"start": 1675.2, "end": 1680.4, "text": "wonderful purpose in life. So your purpose should be beyond you, where your society, where your family,"}, {"start": 1680.4, "end": 1684.4, "text": "where your country, where your the whole universe is involved. Thank you very much."}, {"start": 1685.3600000000001, "end": 1691.28, "text": "Thanks a lot. So what you need to do, you need to create the imprints of your life,"}, {"start": 1691.28, "end": 1695.84, "text": "the imprints of your purpose, the imprints of your belief, the imprints of your identity. Remember,"}, {"start": 1696.64, "end": 1703.0400000000002, "text": "my name is Ram, it is my, I mean social identity, I am Hindu or Muslim, it is my, I mean religious"}, {"start": 1703.0400000000002, "end": 1709.0400000000002, "text": "identity, I belong to this political party or this political party, it is my political identity."}, {"start": 1709.0400000000002, "end": 1716.5600000000002, "text": "But beyond that, who I am, I am, I am Hunkonus Kebath. The question remains always and what we need to"}, {"start": 1716.5600000000002, "end": 1722.5600000000002, "text": "do, we need to enhance or increase or expand our identity. We don't, we don't need to shrink it"}, {"start": 1722.56, "end": 1732.48, "text": "rather we need to enhance it. So my, my message to you is just remember keep developing good"}, {"start": 1732.48, "end": 1738.24, "text": "imprints on your subconscious mind, the imprints of your health, the imprints of your success, the"}, {"start": 1738.24, "end": 1745.52, "text": "imprints of your happiness and create the distance from the negative past painful memories,"}, {"start": 1746.08, "end": 1753.6, "text": "create the distance from the untransformed self-image, create that distance from the frustrations,"}, {"start": 1753.6, "end": 1760.56, "text": "the depression, the anxiety. And remember, the more imprints you have for your success, the more"}, {"start": 1760.56, "end": 1767.52, "text": "successful you will be. Let me finish my talk with a wonderful prayer that I make for yourself."}, {"start": 1767.52, "end": 1771.76, "text": "May you enjoy your success all the time, may the wonderful rays of sun,"}, {"start": 1772.4, "end": 1778.16, "text": "also on your wonderful faces and gives you new energy. May your family keep expanding day after day,"}, {"start": 1779.12, "end": 1785.84, "text": "may your crops remain green, may your business keep growing and may you be safe in the palms of"}, {"start": 1785.8400000000001, "end": 1794.0800000000002, "text": "Almighty, Almighty, every time. Thank you."}], "language": "en"}
{"title": "5 Best NLP Techniques To Overcome Self Limiting Beliefs (STOP THE ANXIETY CYCLE)", "video_id": "S3IE9s4tEGQ", "text": "War years as you can tell I'm excited that you are with me today. Why? Because today is the day where I'm going to share with you what I believe to be the five most powerful techniques to counter your self-limiting ideas. It's an exciting day. Now we all have a personal model of reality warriors, okay? We don't see reality the same way from person to person. That reality changes depending on the thoughts that we entertain the most, the thoughts that we become emotional towards and the thoughts that we act on, okay? If you're listening to this video, it's highly likely that you've got certain thinking patterns going on in your head that are affecting your physical body and really are distorting your view over what reality is, right? You are creating an environment, this planet that is much scarier than what it really is. Therefore you experience anxiety. So I want to help you today to change that model of reality by entertaining self-limiting ideas less and entertaining self-empowering ideas more, okay? Five techniques powerful to start this off I want you to think about. One particular negative idea about yourself, a limiting idea, a viewpoint that really doesn't serve you for the better, okay? I'll give you a moment think about it. Cool. I'm going to work with the one that a lot of anxiety suffers believe. I'll never overcome my physical symptoms of anxiety. So I'm going to use that one as an example. Technique number one is to consciously slow down the tempo and the speed of that thought. So it goes like this. I'll never overcome my symptoms of anxiety. I'll never overcome my symptoms of anxiety and so on. What you want to do is you want to keep telling yourself that same thought in that tempo, in that speed until you create disinterest until you are bored of that idea. And I want you to comment below and let me know how you felt after each and every one of these techniques because I want you to implement them here with me right now, okay? So slow down the tempo, slow down the speed because what's going on right now in your head is that there's an enormous amount of internal chatter and it's not slow. It's definitely fast. So it becomes very easy to live a reactive life. Oh, I think this and I react. I think this I become emotional but the moment I slow it down I can look back on the same idea and I can begin to see past that idea. It no longer has the same threatening tone that it had before. It no longer has the same threatening speed as it did before. Technique number two is to create boredom behind the thought, something like this. I'll never overcome my symptoms of anxiety. I'll never overcome my symptoms of anxiety. I'll never overcome my symptoms of anxiety. And so on. You know, doesn't it get boring after a while? The same idea is the same thoughts flowing through your head that you're entertaining all the time that are ruining your life, right? So I just want to make this real. Think about the negative thought that you've been so excited towards. You know, you're excited towards it, you're entertaining it, you're giving it plenty of attention, you are fueling it. So instead of fueling it, let's create boredom around the thought and watch what starts to happen. Comment below, okay? Let me know how you view the thought after you've created boredom around it. And again, don't do it once, don't do it twice. Every one of these techniques do them multiple times until you're fed up, okay? Technique number three is change the pitch. We're talking about a higher pitch, we're talking about a lower pitch. So when we listen to these thoughts in our head, it's always the same pitch, it's always the same voice, it's always the same way. But if we can manipulate these things, if we can change what we call an NLP, these submodalities, all of a sudden I can go back and look at it differently. So I'm about to sound like Mickey Mouse, I apologize, but it goes something like this, change the pitch. I'll never overcome my physical symptoms of anxiety. I'll never overcome my physical symptoms of anxiety. I'll never overcome my physical symptoms of anxiety. Okay, you know, you kind of look back and you go, that's the same thought that's been ruining my life and causing me all this sensitivity and anxiousness. Come on now, right? So again, comment below, how did you respond? How did you view the thought after changing the pitch? Number four is add an echo, okay? Oh, oh, oh, never, never, overcome, overcome, overcome. My, my, my symptoms, symptoms, of, of, of, anxiety, anxiety, anxiety. Okay, you know, it's just, it's, it's not, it feels like these thoughts have been like a river that have been flowing so naturally for so long. And when you implement the echo, okay, what happens is you start to place logs in between that river. Again, it can be very difficult to entertain something that, you know, you've manipulated in this way. So number four is adding the echo to it, okay? Number five is putting it into past tense, past tense because what we want to do is any kind of characteristic, any personality trait, any kind of way of thinking, any way of acting, any of these bad emotions, what we need to do is we need to start convincing ourselves that these are things we used to do. We don't do them anymore, okay? Or at least we don't do them as consistently and strongly as we did before. So technique number five is putting it into past tense, meaning I used to believe that I would never overcome my physical symptoms of anxiety. I used to believe that I would never overcome my physical symptoms of anxiety. I used to believe that I would never overcome my physical symptoms of anxiety. You literally start to believe that these are things that you used to think about, that you used to entertain. It's a beautiful moment. It's a beautiful moment when you got that epiphany, where you go, my goodness, what a waste of time these thoughts have been, what a waste of time the way I've been viewing myself and so forth. So again, write these down, do them all the time every single day. And I want to know, comment below, which one of these five are the most powerful to you after you've implemented them? Is it number one and changing the tempo and the speed? Is it number two and creating boredom? Is it number three and changing the pitch? Is it number four and the echo? Or is it number five putting it into past tense, powerful, powerful, powerful? I love each and every one of you. Remember, you are more than anxiety. I'll see you in the next video. Make sure to like and subscribe. And if you have any other questions on the end of the anxiety program or the life mastery program head on over to this website right here.", "segments": [{"start": 0.0, "end": 16.64, "text": "War years as you can tell I'm excited that you are with me today. Why? Because today is the day"}, {"start": 16.64, "end": 23.28, "text": "where I'm going to share with you what I believe to be the five most powerful techniques to counter"}, {"start": 23.28, "end": 33.52, "text": "your self-limiting ideas. It's an exciting day. Now we all have a personal model of reality warriors,"}, {"start": 33.52, "end": 41.44, "text": "okay? We don't see reality the same way from person to person. That reality changes depending on"}, {"start": 41.44, "end": 46.96, "text": "the thoughts that we entertain the most, the thoughts that we become emotional towards and the"}, {"start": 46.96, "end": 54.4, "text": "thoughts that we act on, okay? If you're listening to this video, it's highly likely that you've got"}, {"start": 54.4, "end": 60.88, "text": "certain thinking patterns going on in your head that are affecting your physical body and really"}, {"start": 60.88, "end": 69.28, "text": "are distorting your view over what reality is, right? You are creating an environment, this planet"}, {"start": 69.28, "end": 76.72, "text": "that is much scarier than what it really is. Therefore you experience anxiety. So I want to help"}, {"start": 76.72, "end": 87.68, "text": "you today to change that model of reality by entertaining self-limiting ideas less and entertaining"}, {"start": 87.68, "end": 97.68, "text": "self-empowering ideas more, okay? Five techniques powerful to start this off I want you to think about."}, {"start": 98.32000000000001, "end": 107.36000000000001, "text": "One particular negative idea about yourself, a limiting idea, a viewpoint that really doesn't serve"}, {"start": 107.36000000000001, "end": 114.72, "text": "you for the better, okay? I'll give you a moment think about it. Cool. I'm going to work with the one"}, {"start": 115.76, "end": 123.68, "text": "that a lot of anxiety suffers believe. I'll never overcome my physical symptoms of anxiety. So I'm"}, {"start": 123.68, "end": 130.72, "text": "going to use that one as an example. Technique number one is to consciously slow down the tempo"}, {"start": 130.72, "end": 146.08, "text": "and the speed of that thought. So it goes like this. I'll never overcome my symptoms of anxiety. I'll"}, {"start": 146.56, "end": 159.52, "text": "never overcome my symptoms of anxiety and so on. What you want to do is you want to keep telling yourself"}, {"start": 159.52, "end": 170.64000000000001, "text": "that same thought in that tempo, in that speed until you create disinterest until you are bored of"}, {"start": 170.64000000000001, "end": 177.52, "text": "that idea. And I want you to comment below and let me know how you felt after each and every one of"}, {"start": 177.52, "end": 185.04000000000002, "text": "these techniques because I want you to implement them here with me right now, okay? So slow down the"}, {"start": 185.04000000000002, "end": 193.20000000000002, "text": "tempo, slow down the speed because what's going on right now in your head is that there's an enormous"}, {"start": 193.20000000000002, "end": 202.08, "text": "amount of internal chatter and it's not slow. It's definitely fast. So it becomes very easy to live"}, {"start": 202.08, "end": 209.12, "text": "a reactive life. Oh, I think this and I react. I think this I become emotional but the moment I slow it"}, {"start": 209.12, "end": 220.24, "text": "down I can look back on the same idea and I can begin to see past that idea. It no longer has the same"}, {"start": 220.24, "end": 228.56, "text": "threatening tone that it had before. It no longer has the same threatening speed as it did before."}, {"start": 229.68, "end": 235.84, "text": "Technique number two is to create boredom behind the thought, something like this."}, {"start": 238.4, "end": 247.92000000000002, "text": "I'll never overcome my symptoms of anxiety. I'll never overcome my symptoms of anxiety. I'll never"}, {"start": 247.92000000000002, "end": 257.84000000000003, "text": "overcome my symptoms of anxiety. And so on. You know, doesn't it get boring after a while? The same"}, {"start": 257.84000000000003, "end": 261.92, "text": "idea is the same thoughts flowing through your head that you're entertaining all the time that are"}, {"start": 261.92, "end": 268.72, "text": "ruining your life, right? So I just want to make this real. Think about the negative thought that"}, {"start": 268.72, "end": 274.72, "text": "you've been so excited towards. You know, you're excited towards it, you're entertaining it, you're"}, {"start": 274.72, "end": 282.56, "text": "giving it plenty of attention, you are fueling it. So instead of fueling it, let's create boredom"}, {"start": 282.56, "end": 290.96000000000004, "text": "around the thought and watch what starts to happen. Comment below, okay? Let me know how you view the"}, {"start": 290.96000000000004, "end": 298.8, "text": "thought after you've created boredom around it. And again, don't do it once, don't do it twice. Every one"}, {"start": 298.8, "end": 306.96000000000004, "text": "of these techniques do them multiple times until you're fed up, okay? Technique number three is"}, {"start": 306.96000000000004, "end": 314.8, "text": "change the pitch. We're talking about a higher pitch, we're talking about a lower pitch. So when we"}, {"start": 314.8, "end": 320.0, "text": "listen to these thoughts in our head, it's always the same pitch, it's always the same voice,"}, {"start": 320.0, "end": 327.2, "text": "it's always the same way. But if we can manipulate these things, if we can change what we call an NLP,"}, {"start": 327.2, "end": 333.84, "text": "these submodalities, all of a sudden I can go back and look at it differently. So I'm about to"}, {"start": 333.84, "end": 338.64, "text": "sound like Mickey Mouse, I apologize, but it goes something like this, change the pitch."}, {"start": 340.8, "end": 349.28, "text": "I'll never overcome my physical symptoms of anxiety. I'll never overcome my physical symptoms of"}, {"start": 349.28000000000003, "end": 362.88000000000005, "text": "anxiety. I'll never overcome my physical symptoms of anxiety. Okay, you know, you kind of look back"}, {"start": 362.88000000000005, "end": 370.08000000000004, "text": "and you go, that's the same thought that's been ruining my life and causing me all this sensitivity"}, {"start": 370.08, "end": 379.59999999999997, "text": "and anxiousness. Come on now, right? So again, comment below, how did you respond? How did you view"}, {"start": 379.59999999999997, "end": 390.88, "text": "the thought after changing the pitch? Number four is add an echo, okay? Oh, oh, oh, never, never,"}, {"start": 391.68, "end": 401.76, "text": "overcome, overcome, overcome. My, my, my symptoms, symptoms, of, of, of, anxiety, anxiety, anxiety."}, {"start": 404.08, "end": 412.4, "text": "Okay, you know, it's just, it's, it's not, it feels like these thoughts have been like a river that"}, {"start": 412.4, "end": 419.76, "text": "have been flowing so naturally for so long. And when you implement the echo, okay, what happens is you"}, {"start": 419.76, "end": 427.59999999999997, "text": "start to place logs in between that river. Again, it can be very difficult to entertain something"}, {"start": 427.59999999999997, "end": 434.8, "text": "that, you know, you've manipulated in this way. So number four is adding the echo to it, okay?"}, {"start": 435.92, "end": 446.96, "text": "Number five is putting it into past tense, past tense because what we want to do is any kind of"}, {"start": 446.96000000000004, "end": 454.24, "text": "characteristic, any personality trait, any kind of way of thinking, any way of acting, any of these"}, {"start": 454.24, "end": 460.32000000000005, "text": "bad emotions, what we need to do is we need to start convincing ourselves that these are things we"}, {"start": 460.32000000000005, "end": 467.6, "text": "used to do. We don't do them anymore, okay? Or at least we don't do them as consistently and strongly"}, {"start": 467.6, "end": 475.44000000000005, "text": "as we did before. So technique number five is putting it into past tense, meaning I used to believe"}, {"start": 475.44, "end": 482.16, "text": "that I would never overcome my physical symptoms of anxiety. I used to believe that I would never"}, {"start": 482.16, "end": 489.04, "text": "overcome my physical symptoms of anxiety. I used to believe that I would never overcome my physical"}, {"start": 489.04, "end": 496.0, "text": "symptoms of anxiety. You literally start to believe that these are things that you used to think"}, {"start": 496.0, "end": 502.64, "text": "about, that you used to entertain. It's a beautiful moment. It's a beautiful moment when you got that"}, {"start": 502.64, "end": 508.68, "text": "epiphany, where you go, my goodness, what a waste of time these thoughts have been, what"}, {"start": 508.68, "end": 512.8, "text": "a waste of time the way I've been viewing myself and so forth."}, {"start": 512.8, "end": 517.24, "text": "So again, write these down, do them all the time every single day."}, {"start": 517.24, "end": 523.3199999999999, "text": "And I want to know, comment below, which one of these five are the most powerful to you"}, {"start": 523.3199999999999, "end": 525.48, "text": "after you've implemented them?"}, {"start": 525.48, "end": 529.52, "text": "Is it number one and changing the tempo and the speed?"}, {"start": 529.52, "end": 532.76, "text": "Is it number two and creating boredom?"}, {"start": 532.76, "end": 535.76, "text": "Is it number three and changing the pitch?"}, {"start": 535.76, "end": 538.16, "text": "Is it number four and the echo?"}, {"start": 538.16, "end": 545.64, "text": "Or is it number five putting it into past tense, powerful, powerful, powerful?"}, {"start": 545.64, "end": 548.36, "text": "I love each and every one of you."}, {"start": 548.36, "end": 552.68, "text": "Remember, you are more than anxiety."}, {"start": 552.68, "end": 554.48, "text": "I'll see you in the next video."}, {"start": 554.48, "end": 556.76, "text": "Make sure to like and subscribe."}, {"start": 556.76, "end": 560.96, "text": "And if you have any other questions on the end of the anxiety program or the life mastery"}, {"start": 560.96, "end": 567.96, "text": "program head on over to this website right here."}], "language": "en"}
{"title": "Natural Language Processing: Crash Course Computer Science #36", "video_id": "fOvTtapxa9c", "text": "Hi, I'm Carrie Ann, and welcome to Crash Course Computer Science. Last episode we talked about computer vision, giving computers the ability to see and understand visual information. Today, we're going to talk about how to give computers the ability to understand language. You might argue they've always had this capability. Back in episodes 9 and 12, we talked about machine language instructions, as well as higher level programming languages. While they certainly meet the definition of a language, they also tend to have small vocabularies and follow highly structured conventions. Code will only compile and run if there's 100% free of spelling and syntactic errors. Of course, this is quite different from human languages, what are called natural languages, containing large diverse vocabularies, words with several different meanings, speakers with different accents and all sorts of interesting wordplay. People also make linguistic faux pas when writing and speaking, like slurring words together, leaving out key details so things are ambiguous and mispronouncing things. But for the most part, humans can roll right through these challenges. The skillful use of language is a major part of what makes us human, and for this reason, the desire for computers to understand and speak our language has been around since they were first conceived. This led to the creation of natural language processing, or NLP, an interdisciplinary field combining computer science and linguistics. There's an essentially infinite number of ways to arrange words in a sentence. We can't give computers a dictionary of all possible sentences to help them understand what humans are blabbing on about. So an early and fundamental NLP problem was deconstructing sentences into bite-sized pieces, which could be more easily processed. In school, you learned about nine fundamental types of English words. Nouns, pronouns, articles, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections. These are all called parts of speech. There are all sorts of sub-categories, too, like singular verses plural nouns and superlative verses comparative adverbs, but we're not going to get into that. Knowing a word's type is definitely useful, but unfortunately, there are a lot of words that have multiple meanings, like rows and leaves, which can be used as nouns or verbs. A digital dictionary alone isn't enough to resolve this ambiguity, so computers also need to know some grammar. For this, phrase structure rules were developed, which encapsulate the grammar of a language. For example, in English, there's a rule that says a sentence can be comprised of a noun phrase followed by a verb phrase. Noun phrases can be an article like the, followed by a noun, or they can be an adjective followed by a noun. And you can make rules like this for an entire language. Then using these rules is fairly easy to construct what's called a parse tree, which not only tags every word with a likely part of speech, but also reveals how the sentence is constructed. We now know, for example, that the noun focus of this sentence is the mongles, and we know it's about them doing the action of rising from something. In this case, leaves. These smaller chunks of data allow computers to more easily access process and respond to information. Equivalent processes are happening every time you do a voice search, like where's the nearest pizza? The computer can recognize this is a where question. Knows do you want the noun pizza, and the dimension you care about is nearest. The same process applies to what is the biggest giraffe, or who's sang thriller. By treating language almost like Lego, computers can be quite adept at natural language tasks. They can answer questions and also process commands, like set an alarm for 220, or play teaspoons or on Spotify. But as you've probably experienced, they fail when you start getting too fancy, and they can no longer parse the sentence correctly, or capture your intent. Hey Siri, me thinks the mongles doth roam too much. Or think he on this most gentle mid-summer's day? I'm not sure I got that. I should also know that phrase structure rules and similar methods that codify language can be used by computers to generate natural language text. This works particularly well when data is stored in a web of semantic information, where entities are linked to one another in meaningful relationships, providing all the ingredients you need to craft informational sentences. Thriller was released in 1983 and sung by Michael Jackson. Google's version of this is called Knowledge Graph. At the end of 2016, it contained roughly 70 billion facts about and relationships between different entities. These two processes, parsing and generating text, are fundamental components of natural language chatbots, computer programs that chat with you. Early chatbots were primarily rule-based, where experts would encode hundreds of rules mapping what a user might say to how a program should reply. Obviously, this was unwieldy to maintain and limited the possible sophistication. A famous early example was Eliza, created in the mid-1960s at MIT. This was a chatbot that took on the role of a therapist and used basic syntactic rules to identify content in written exchanges, which he would turn around and ask the user about. Sometimes it felt very much like human-human communication, but other times it would make simple and even comical mistakes. Chatbots and more advanced dialogue systems have come a long way in the last 50 years and can be quite convincing today. Modern approaches are based on machine learning, where gigabytes of real human-to-human chats are used to train chatbots. Today, the technology is finding using customer service applications, where there's already heaps of example conversations to learn from. People have also been getting chatbots to talk with one another, and in a Facebook experiment, chatbots even started to involve their own language. This experiment got a bunch of scary-sounding press, but it was just the computer's crafting a simplified protocol to negotiate with one another. It wasn't evil, it was efficient, but what about if something is spoken? How does a computer get words from the sound? That's the domain of speech recognition, which has been the focus of research for many decades. Bell Labs debuted the first speech recognition system in 1952, nicknamed Audrey, the automatic digit recognizer. It could recognize all 10 numerical digits if you set them slowly enough. Five, nine, seven. The project didn't go anywhere because it was much faster to enter telephone numbers with the finger. Ten years later, at the 1962 World's Fair, IBM demonstrated a shoebox-sized machine capable of recognising 16 words. To boost research in the area, DARPA kicked off an ambitious five-year funding initiative in 1971, which led to the development of Harpy at Carnegie Mellon University. Harpy was the first system to recognise over a thousand words. But, on computers of the era, transcription was often ten or more times slower than the rate of natural speech. Fortunately, thanks to huge advances in computing performance in the 80s and 90s, continuous real-time speech recognition became practical. There was simultaneous innovation in the algorithms for processing natural language, moving from handcrafted rules to machine learning techniques that could learn automatically from existing data sets of human language. Today, the speech recognition systems with the best accuracy are using deep neural networks, which we touched on in episode 34. To get a sense of how these techniques work, let's look at some speech, specifically the acoustic signal. Let's start by looking at vowel sounds, like R and E. These are the waveforms of those two sounds, as captured by a computer's microphone. As we discussed in episode 21 on files and file formats, this signal is the magnitude of displacement of a diaphragm inside of a microphone, as sound waves cause it to oscillate. In this view of sound data, the horizontal axis is time, and the vertical axis is the magnitude of displacement or amplitude. Although we can see there are differences between the waveforms, it's not super obvious what you would point to and say, aha, this is definitely an E sound. To really make this pop out, we need to view the data in a totally different way. A spectrogram. In this view of the data, we still have time along the horizontal axis, but now instead of amplitude on the vertical axis, we plot the magnitude of the different frequencies that make up each sound. The brighter the color, the louder that frequency component. This conversion from waveform to frequencies is done with a very cool algorithm called a fast Fourier transform. If you've ever stared at a stereo system's EQ visualizer, it's pretty much the same thing. A spectrogram is plotting that information over time. You might have noticed that the signals have a sort of ribbed pattern to them. That's all the resonances of my vocal tract. To make different sounds, I squeeze my vocal chords mountain tongue into different shapes, which amplifies or dampens different resonances. We can see this in the signal, with areas that are brighter and areas that are darker. If we work our way out from the bottom labeling where we see peaks in the spectrum, what are called formants, we can see the two sounds have quite different arrangements, and this is true for all vowel sounds. It's exactly this type of information that lets computers recognise spoken vowels, and indeed whole words. Let's see a more complicated example, like when I say, she was happy. We can see our e sound here and our sound here. We can also see a bunch of other distinctive sounds, like the sh sound is she, the wattens in was and so on. These sound pieces that make up words are called phonemes. Speech recognition software knows what all these phonemes look like. In English, there are roughly 44, so it mostly boils down to fancy pattern matching. Then you have to separate words from one another, figure out when sentences begin and end, and ultimately you end up with speech converted into text, allowing for techniques like we discussed at the beginning of the episode. Because people say words in slightly different ways, due to things like accents and mispronunciations, transcription accuracy is greatly improved when combined with a language model, which contains statistics about sequences of words. For example, she was most likely to be followed by an adjective like happy. It's uncommon for she was to be followed immediately by a noun, so if the speech recogniser was unsure between happy and harpy, it would pick happy, since the language model would report that as a more likely choice. Finally, we need to talk about speech synthesis, that is, giving computers the ability to output speech. This is very much like speech recognition, but in reverse. You can take a sentence of text and break it down into its phonetic components, and then play those sounds back-to-back out of the computer speaker. You can hear this changing of phonemes very clearly with older speech synthesis technologies like this 1937 hand-operated machine from Bell Labs. By the 1980s, this had improved a lot, but that discontinuous and awkward blending of phonemes still created that signature robotic sound. Thriller was released in 1983 and sung by Michael Jackson. Today, synthesised computer voices like Siri, Cortana and Alexa have gotten much better, but they're still not quite human, but we're so so close, and it's likely to be a solved problem pretty soon, especially because we're now seeing an explosion of voice user interfaces on our phones in our cars and homes and maybe soon plugs right into our ears. This ubiquity is creating a positive feedback loop, where people are using voice interaction more often, which in turn is giving companies like Google, Amazon and Microsoft more data to train their systems on, which is enabling better accuracy, which is leading to people using voice more, which is enabling even better accuracy, and the loop continues. Many predict that speech technologies will become as common a form of interaction as screens, keyboards, trackpads and other physical input output devices that we use today. That's particularly good news for robots who don't want to have to walk around with keyboards in order to communicate with humans, but we'll talk more about them next week. See you then. Crash Course Computer Science is produced in association with PBS Digital Studios. At their channel, you can check out our playlist of shows like Eons Physics Girl and It's Okay To Be Smart. This episode was filmed at the Chad and Stacey Emmergott studio in Indianapolis, and it was made with the help of all these nice people and our wonderful graphics team, Thought Cafe. Thanks for the random access memories, I'll see you next time.", "segments": [{"start": 0.0, "end": 6.0600000000000005, "text": "Hi, I'm Carrie Ann, and welcome to Crash Course Computer Science."}, {"start": 6.0600000000000005, "end": 10.28, "text": "Last episode we talked about computer vision, giving computers the ability to see and understand"}, {"start": 10.28, "end": 11.28, "text": "visual information."}, {"start": 11.28, "end": 15.72, "text": "Today, we're going to talk about how to give computers the ability to understand language."}, {"start": 15.72, "end": 18.080000000000002, "text": "You might argue they've always had this capability."}, {"start": 18.080000000000002, "end": 22.28, "text": "Back in episodes 9 and 12, we talked about machine language instructions, as well as higher level"}, {"start": 22.28, "end": 23.6, "text": "programming languages."}, {"start": 23.6, "end": 27.96, "text": "While they certainly meet the definition of a language, they also tend to have small vocabularies"}, {"start": 27.96, "end": 30.44, "text": "and follow highly structured conventions."}, {"start": 30.44, "end": 35.160000000000004, "text": "Code will only compile and run if there's 100% free of spelling and syntactic errors."}, {"start": 35.160000000000004, "end": 39.120000000000005, "text": "Of course, this is quite different from human languages, what are called natural languages,"}, {"start": 39.120000000000005, "end": 43.260000000000005, "text": "containing large diverse vocabularies, words with several different meanings, speakers with"}, {"start": 43.260000000000005, "end": 46.44, "text": "different accents and all sorts of interesting wordplay."}, {"start": 46.44, "end": 51.519999999999996, "text": "People also make linguistic faux pas when writing and speaking, like slurring words together,"}, {"start": 51.519999999999996, "end": 55.2, "text": "leaving out key details so things are ambiguous and mispronouncing things."}, {"start": 55.2, "end": 58.36, "text": "But for the most part, humans can roll right through these challenges."}, {"start": 58.36, "end": 62.800000000000004, "text": "The skillful use of language is a major part of what makes us human, and for this reason,"}, {"start": 62.800000000000004, "end": 66.68, "text": "the desire for computers to understand and speak our language has been around since they"}, {"start": 66.68, "end": 68.16, "text": "were first conceived."}, {"start": 68.16, "end": 73.56, "text": "This led to the creation of natural language processing, or NLP, an interdisciplinary field combining"}, {"start": 73.56, "end": 75.80000000000001, "text": "computer science and linguistics."}, {"start": 76.8, "end": 88.64, "text": "There's an essentially infinite number of ways to arrange words in a sentence."}, {"start": 88.64, "end": 92.75999999999999, "text": "We can't give computers a dictionary of all possible sentences to help them understand what"}, {"start": 92.75999999999999, "end": 94.47999999999999, "text": "humans are blabbing on about."}, {"start": 94.47999999999999, "end": 99.75999999999999, "text": "So an early and fundamental NLP problem was deconstructing sentences into bite-sized pieces, which"}, {"start": 99.75999999999999, "end": 101.56, "text": "could be more easily processed."}, {"start": 101.56, "end": 104.92, "text": "In school, you learned about nine fundamental types of English words."}, {"start": 104.92, "end": 109.8, "text": "Nouns, pronouns, articles, verbs, adjectives, adverbs, prepositions, conjunctions, and"}, {"start": 109.8, "end": 110.8, "text": "interjections."}, {"start": 110.8, "end": 112.6, "text": "These are all called parts of speech."}, {"start": 112.6, "end": 116.6, "text": "There are all sorts of sub-categories, too, like singular verses plural nouns and superlative"}, {"start": 116.6, "end": 119.92, "text": "verses comparative adverbs, but we're not going to get into that."}, {"start": 119.92, "end": 123.48, "text": "Knowing a word's type is definitely useful, but unfortunately, there are a lot of words"}, {"start": 123.48, "end": 128.6, "text": "that have multiple meanings, like rows and leaves, which can be used as nouns or verbs."}, {"start": 128.6, "end": 133.12, "text": "A digital dictionary alone isn't enough to resolve this ambiguity, so computers also need"}, {"start": 133.12, "end": 134.12, "text": "to know some grammar."}, {"start": 134.12, "end": 138.68, "text": "For this, phrase structure rules were developed, which encapsulate the grammar of a language."}, {"start": 138.68, "end": 142.36, "text": "For example, in English, there's a rule that says a sentence can be comprised of a noun"}, {"start": 142.36, "end": 144.52, "text": "phrase followed by a verb phrase."}, {"start": 144.52, "end": 149.20000000000002, "text": "Noun phrases can be an article like the, followed by a noun, or they can be an adjective followed"}, {"start": 149.20000000000002, "end": 150.20000000000002, "text": "by a noun."}, {"start": 150.20000000000002, "end": 152.84, "text": "And you can make rules like this for an entire language."}, {"start": 152.84, "end": 157.24, "text": "Then using these rules is fairly easy to construct what's called a parse tree, which not only tags"}, {"start": 157.24, "end": 161.68, "text": "every word with a likely part of speech, but also reveals how the sentence is constructed."}, {"start": 161.68, "end": 166.48000000000002, "text": "We now know, for example, that the noun focus of this sentence is the mongles, and we know"}, {"start": 166.48000000000002, "end": 169.68, "text": "it's about them doing the action of rising from something."}, {"start": 169.68, "end": 171.28, "text": "In this case, leaves."}, {"start": 171.28, "end": 175.36, "text": "These smaller chunks of data allow computers to more easily access process and respond to"}, {"start": 175.36, "end": 176.36, "text": "information."}, {"start": 176.36, "end": 180.28, "text": "Equivalent processes are happening every time you do a voice search, like where's the nearest"}, {"start": 180.28, "end": 181.28, "text": "pizza?"}, {"start": 181.28, "end": 183.84, "text": "The computer can recognize this is a where question."}, {"start": 183.84, "end": 187.68, "text": "Knows do you want the noun pizza, and the dimension you care about is nearest."}, {"start": 187.68, "end": 192.08, "text": "The same process applies to what is the biggest giraffe, or who's sang thriller."}, {"start": 192.08, "end": 197.16, "text": "By treating language almost like Lego, computers can be quite adept at natural language tasks."}, {"start": 197.16, "end": 202.16, "text": "They can answer questions and also process commands, like set an alarm for 220, or play"}, {"start": 202.16, "end": 203.92000000000002, "text": "teaspoons or on Spotify."}, {"start": 203.92000000000002, "end": 207.76000000000002, "text": "But as you've probably experienced, they fail when you start getting too fancy, and they"}, {"start": 207.76000000000002, "end": 211.68, "text": "can no longer parse the sentence correctly, or capture your intent."}, {"start": 211.68, "end": 215.60000000000002, "text": "Hey Siri, me thinks the mongles doth roam too much."}, {"start": 215.6, "end": 219.04, "text": "Or think he on this most gentle mid-summer's day?"}, {"start": 219.04, "end": 220.35999999999999, "text": "I'm not sure I got that."}, {"start": 220.35999999999999, "end": 224.44, "text": "I should also know that phrase structure rules and similar methods that codify language"}, {"start": 224.44, "end": 227.84, "text": "can be used by computers to generate natural language text."}, {"start": 227.84, "end": 232.68, "text": "This works particularly well when data is stored in a web of semantic information, where entities"}, {"start": 232.68, "end": 236.68, "text": "are linked to one another in meaningful relationships, providing all the ingredients"}, {"start": 236.68, "end": 239.6, "text": "you need to craft informational sentences."}, {"start": 239.6, "end": 243.07999999999998, "text": "Thriller was released in 1983 and sung by Michael Jackson."}, {"start": 243.08, "end": 245.56, "text": "Google's version of this is called Knowledge Graph."}, {"start": 245.56, "end": 250.92000000000002, "text": "At the end of 2016, it contained roughly 70 billion facts about and relationships between"}, {"start": 250.92000000000002, "end": 252.20000000000002, "text": "different entities."}, {"start": 252.20000000000002, "end": 257.08000000000004, "text": "These two processes, parsing and generating text, are fundamental components of natural language"}, {"start": 257.08000000000004, "end": 259.76, "text": "chatbots, computer programs that chat with you."}, {"start": 259.76, "end": 263.8, "text": "Early chatbots were primarily rule-based, where experts would encode hundreds of rules mapping"}, {"start": 263.8, "end": 266.88, "text": "what a user might say to how a program should reply."}, {"start": 266.88, "end": 270.96000000000004, "text": "Obviously, this was unwieldy to maintain and limited the possible sophistication."}, {"start": 271.0, "end": 275.84, "text": "A famous early example was Eliza, created in the mid-1960s at MIT."}, {"start": 275.84, "end": 279.91999999999996, "text": "This was a chatbot that took on the role of a therapist and used basic syntactic rules"}, {"start": 279.91999999999996, "end": 284.91999999999996, "text": "to identify content in written exchanges, which he would turn around and ask the user about."}, {"start": 284.91999999999996, "end": 289.08, "text": "Sometimes it felt very much like human-human communication, but other times it would make"}, {"start": 289.08, "end": 291.32, "text": "simple and even comical mistakes."}, {"start": 291.32, "end": 295.28, "text": "Chatbots and more advanced dialogue systems have come a long way in the last 50 years and"}, {"start": 295.28, "end": 297.32, "text": "can be quite convincing today."}, {"start": 297.32, "end": 301.32, "text": "Modern approaches are based on machine learning, where gigabytes of real human-to-human chats"}, {"start": 301.32, "end": 303.0, "text": "are used to train chatbots."}, {"start": 303.0, "end": 307.36, "text": "Today, the technology is finding using customer service applications, where there's already"}, {"start": 307.36, "end": 310.0, "text": "heaps of example conversations to learn from."}, {"start": 310.0, "end": 314.4, "text": "People have also been getting chatbots to talk with one another, and in a Facebook experiment,"}, {"start": 314.4, "end": 317.15999999999997, "text": "chatbots even started to involve their own language."}, {"start": 317.15999999999997, "end": 321.56, "text": "This experiment got a bunch of scary-sounding press, but it was just the computer's crafting"}, {"start": 321.56, "end": 324.64, "text": "a simplified protocol to negotiate with one another."}, {"start": 324.64, "end": 328.44, "text": "It wasn't evil, it was efficient, but what about if something is spoken?"}, {"start": 328.44, "end": 330.96, "text": "How does a computer get words from the sound?"}, {"start": 330.96, "end": 334.36, "text": "That's the domain of speech recognition, which has been the focus of research for many"}, {"start": 334.36, "end": 335.36, "text": "decades."}, {"start": 335.36, "end": 341.24, "text": "Bell Labs debuted the first speech recognition system in 1952, nicknamed Audrey, the automatic"}, {"start": 341.24, "end": 342.8, "text": "digit recognizer."}, {"start": 342.8, "end": 346.91999999999996, "text": "It could recognize all 10 numerical digits if you set them slowly enough."}, {"start": 346.91999999999996, "end": 350.44, "text": "Five, nine, seven."}, {"start": 350.44, "end": 354.2, "text": "The project didn't go anywhere because it was much faster to enter telephone numbers with"}, {"start": 354.2, "end": 355.2, "text": "the finger."}, {"start": 355.2, "end": 360.64, "text": "Ten years later, at the 1962 World's Fair, IBM demonstrated a shoebox-sized machine capable"}, {"start": 360.64, "end": 362.52, "text": "of recognising 16 words."}, {"start": 362.52, "end": 366.52, "text": "To boost research in the area, DARPA kicked off an ambitious five-year funding initiative"}, {"start": 366.52, "end": 372.08, "text": "in 1971, which led to the development of Harpy at Carnegie Mellon University."}, {"start": 372.08, "end": 374.88, "text": "Harpy was the first system to recognise over a thousand words."}, {"start": 374.88, "end": 379.03999999999996, "text": "But, on computers of the era, transcription was often ten or more times slower than the"}, {"start": 379.03999999999996, "end": 380.68, "text": "rate of natural speech."}, {"start": 380.68, "end": 384.15999999999997, "text": "Fortunately, thanks to huge advances in computing performance in the 80s and 90s,"}, {"start": 384.16, "end": 387.8, "text": "continuous real-time speech recognition became practical."}, {"start": 387.8, "end": 392.12, "text": "There was simultaneous innovation in the algorithms for processing natural language, moving from"}, {"start": 392.12, "end": 396.40000000000003, "text": "handcrafted rules to machine learning techniques that could learn automatically from existing"}, {"start": 396.40000000000003, "end": 398.40000000000003, "text": "data sets of human language."}, {"start": 398.40000000000003, "end": 403.52000000000004, "text": "Today, the speech recognition systems with the best accuracy are using deep neural networks,"}, {"start": 403.52000000000004, "end": 405.56, "text": "which we touched on in episode 34."}, {"start": 405.56, "end": 409.68, "text": "To get a sense of how these techniques work, let's look at some speech, specifically the"}, {"start": 409.68, "end": 410.68, "text": "acoustic signal."}, {"start": 410.68, "end": 414.2, "text": "Let's start by looking at vowel sounds, like R and E."}, {"start": 414.2, "end": 418.36, "text": "These are the waveforms of those two sounds, as captured by a computer's microphone."}, {"start": 418.36, "end": 422.92, "text": "As we discussed in episode 21 on files and file formats, this signal is the magnitude of"}, {"start": 422.92, "end": 427.84000000000003, "text": "displacement of a diaphragm inside of a microphone, as sound waves cause it to oscillate."}, {"start": 427.84000000000003, "end": 432.96000000000004, "text": "In this view of sound data, the horizontal axis is time, and the vertical axis is the magnitude"}, {"start": 432.96000000000004, "end": 434.84000000000003, "text": "of displacement or amplitude."}, {"start": 434.84000000000003, "end": 438.36, "text": "Although we can see there are differences between the waveforms, it's not super obvious what"}, {"start": 438.36, "end": 442.32, "text": "you would point to and say, aha, this is definitely an E sound."}, {"start": 442.32, "end": 446.36, "text": "To really make this pop out, we need to view the data in a totally different way."}, {"start": 446.36, "end": 447.36, "text": "A spectrogram."}, {"start": 447.36, "end": 451.64, "text": "In this view of the data, we still have time along the horizontal axis, but now instead of"}, {"start": 451.64, "end": 455.88, "text": "amplitude on the vertical axis, we plot the magnitude of the different frequencies that"}, {"start": 455.88, "end": 457.52000000000004, "text": "make up each sound."}, {"start": 457.52000000000004, "end": 460.64, "text": "The brighter the color, the louder that frequency component."}, {"start": 460.64, "end": 464.6, "text": "This conversion from waveform to frequencies is done with a very cool algorithm called"}, {"start": 464.6, "end": 466.6, "text": "a fast Fourier transform."}, {"start": 466.6, "end": 470.8, "text": "If you've ever stared at a stereo system's EQ visualizer, it's pretty much the same"}, {"start": 470.8, "end": 471.8, "text": "thing."}, {"start": 471.8, "end": 474.32000000000005, "text": "A spectrogram is plotting that information over time."}, {"start": 474.32000000000005, "end": 477.64000000000004, "text": "You might have noticed that the signals have a sort of ribbed pattern to them."}, {"start": 477.64000000000004, "end": 480.0, "text": "That's all the resonances of my vocal tract."}, {"start": 480.0, "end": 484.08000000000004, "text": "To make different sounds, I squeeze my vocal chords mountain tongue into different shapes,"}, {"start": 484.08000000000004, "end": 486.8, "text": "which amplifies or dampens different resonances."}, {"start": 486.8, "end": 490.88, "text": "We can see this in the signal, with areas that are brighter and areas that are darker."}, {"start": 490.88, "end": 494.36, "text": "If we work our way out from the bottom labeling where we see peaks in the spectrum, what are"}, {"start": 494.36, "end": 498.48, "text": "called formants, we can see the two sounds have quite different arrangements, and this"}, {"start": 498.48, "end": 500.32, "text": "is true for all vowel sounds."}, {"start": 500.32, "end": 504.84000000000003, "text": "It's exactly this type of information that lets computers recognise spoken vowels, and indeed"}, {"start": 504.84000000000003, "end": 505.84000000000003, "text": "whole words."}, {"start": 505.84000000000003, "end": 511.12, "text": "Let's see a more complicated example, like when I say, she was happy."}, {"start": 511.12, "end": 514.36, "text": "We can see our e sound here and our sound here."}, {"start": 514.36, "end": 519.4, "text": "We can also see a bunch of other distinctive sounds, like the sh sound is she, the wattens"}, {"start": 519.4, "end": 520.84, "text": "in was and so on."}, {"start": 520.84, "end": 523.5600000000001, "text": "These sound pieces that make up words are called phonemes."}, {"start": 523.5600000000001, "end": 526.96, "text": "Speech recognition software knows what all these phonemes look like."}, {"start": 526.96, "end": 531.1600000000001, "text": "In English, there are roughly 44, so it mostly boils down to fancy pattern matching."}, {"start": 531.1600000000001, "end": 535.72, "text": "Then you have to separate words from one another, figure out when sentences begin and end,"}, {"start": 535.72, "end": 540.5200000000001, "text": "and ultimately you end up with speech converted into text, allowing for techniques like we discussed"}, {"start": 540.5200000000001, "end": 542.08, "text": "at the beginning of the episode."}, {"start": 542.08, "end": 546.8800000000001, "text": "Because people say words in slightly different ways, due to things like accents and mispronunciations,"}, {"start": 546.8800000000001, "end": 551.44, "text": "transcription accuracy is greatly improved when combined with a language model, which contains"}, {"start": 551.44, "end": 553.6, "text": "statistics about sequences of words."}, {"start": 553.6, "end": 558.2, "text": "For example, she was most likely to be followed by an adjective like happy."}, {"start": 558.2, "end": 562.6400000000001, "text": "It's uncommon for she was to be followed immediately by a noun, so if the speech recogniser"}, {"start": 562.6400000000001, "end": 567.6800000000001, "text": "was unsure between happy and harpy, it would pick happy, since the language model would report"}, {"start": 567.6800000000001, "end": 569.48, "text": "that as a more likely choice."}, {"start": 569.48, "end": 574.48, "text": "Finally, we need to talk about speech synthesis, that is, giving computers the ability to output"}, {"start": 574.48, "end": 575.48, "text": "speech."}, {"start": 575.48, "end": 578.32, "text": "This is very much like speech recognition, but in reverse."}, {"start": 578.32, "end": 582.5200000000001, "text": "You can take a sentence of text and break it down into its phonetic components, and then"}, {"start": 582.5200000000001, "end": 585.48, "text": "play those sounds back-to-back out of the computer speaker."}, {"start": 585.48, "end": 589.9200000000001, "text": "You can hear this changing of phonemes very clearly with older speech synthesis technologies"}, {"start": 589.9200000000001, "end": 593.6400000000001, "text": "like this 1937 hand-operated machine from Bell Labs."}, {"start": 593.64, "end": 615.4, "text": "By the 1980s, this had improved a lot, but that discontinuous and awkward blending of"}, {"start": 615.4, "end": 618.64, "text": "phonemes still created that signature robotic sound."}, {"start": 618.64, "end": 622.96, "text": "Thriller was released in 1983 and sung by Michael Jackson."}, {"start": 622.96, "end": 627.9200000000001, "text": "Today, synthesised computer voices like Siri, Cortana and Alexa have gotten much better,"}, {"start": 627.9200000000001, "end": 632.32, "text": "but they're still not quite human, but we're so so close, and it's likely to be a solved"}, {"start": 632.32, "end": 637.0, "text": "problem pretty soon, especially because we're now seeing an explosion of voice user interfaces"}, {"start": 637.0, "end": 641.32, "text": "on our phones in our cars and homes and maybe soon plugs right into our ears."}, {"start": 641.32, "end": 645.88, "text": "This ubiquity is creating a positive feedback loop, where people are using voice interaction"}, {"start": 645.88, "end": 650.8000000000001, "text": "more often, which in turn is giving companies like Google, Amazon and Microsoft more data to"}, {"start": 650.8000000000001, "end": 655.36, "text": "train their systems on, which is enabling better accuracy, which is leading to people using"}, {"start": 655.36, "end": 659.72, "text": "voice more, which is enabling even better accuracy, and the loop continues."}, {"start": 659.72, "end": 663.72, "text": "Many predict that speech technologies will become as common a form of interaction as screens,"}, {"start": 663.72, "end": 667.72, "text": "keyboards, trackpads and other physical input output devices that we use today."}, {"start": 667.72, "end": 671.96, "text": "That's particularly good news for robots who don't want to have to walk around with keyboards"}, {"start": 671.96, "end": 676.44, "text": "in order to communicate with humans, but we'll talk more about them next week."}, {"start": 676.44, "end": 677.44, "text": "See you then."}, {"start": 677.44, "end": 681.6400000000001, "text": "Crash Course Computer Science is produced in association with PBS Digital Studios."}, {"start": 681.6400000000001, "end": 685.6400000000001, "text": "At their channel, you can check out our playlist of shows like Eons Physics Girl and It's Okay"}, {"start": 685.6400000000001, "end": 686.6400000000001, "text": "To Be Smart."}, {"start": 686.6400000000001, "end": 690.8800000000001, "text": "This episode was filmed at the Chad and Stacey Emmergott studio in Indianapolis, and it was"}, {"start": 690.8800000000001, "end": 695.72, "text": "made with the help of all these nice people and our wonderful graphics team, Thought Cafe."}, {"start": 695.72, "end": 698.5200000000001, "text": "Thanks for the random access memories, I'll see you next time."}], "language": "en"}
{"title": "Natural Language Processing In 10 Minutes | NLP Tutorial For Beginners | NLP Training | Simplilearn", "video_id": "6I-Alfkr5K4", "text": "Hey everyone, in this video we will be talking about NLP in the next 10 minutes. So what can you expect from this video? What's in it for you? First, we will give you a basic understanding of NLP. We will tell you why we need NLP and we will talk about what NLP exactly is. Next, we look at NLP pipelines. NLP pipelines cover the basic processes involved in text passing in NLP, which means converting text to a form suitable for machines to understand as humans would understand it. And finally, we'll talk about some real-world applications of NLP, which are all around us and a lot more common than you might have realized. So let's get started. So what exactly is NLP? Before we talk about NLP, let's talk about how humans talk and interact with each other. Humans communicate with each other mostly via speech or text. To convey your intent to someone, you may directly talk to them or send them a message. The human way of communicating is known as natural language. Around the globe, many languages are spoken and messages are shared virtually every single day. All these conversations, feedbacks, and messages are data in themselves. This data is extremely valuable as it can give us customer information and insight into human sentiment. However, this data is not useful to computers as it is not in a form that can be understood by machines. Machines communicate using ones and zeros and not via words. They cannot understand English, French or Spanish, only binary. And this is where NLP comes into the picture. NLP stands for natural language processing. Natural language processing is a branch of artificial intelligence that deals with the interactions between humans and computers. Using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a manner that is valuable and to build systems that can make sense of text and perform tasks like translation, grammar checking, or topic classification. NLP basically is the task of processing written forms of languages and making a computer understand them. Companies are increasingly using NLP-equipped tools to gain insights from data into automated routine tasks. A sentiment analyzer, for instance, can help brands detect emotions in text, such as negative comments on social media. NLP at the end of the day is nothing more than accumulation of artificial intelligence, computer science, and the human language. The next topic that we'll be looking at are NLP pipelines. So what exactly are pipelines? A pipeline is a set of data processing elements which are connected in series, whether output of one element is the input of the next one. It basically represents the various steps which have to be taken in a computation and the order in which they occur. In this pipeline, we are going to be parsing our data. Parsing means converting our data from one form to another. In this case, we are parsing our input text data to a file which can be understood by a computer. The text is first broken into segments, and the segments are then converted into even smaller tokens. The word stems of all of these tokens are found followed by finding the root words of these stems in a process known as lemmatization. We will then recognize which part of speech the word belongs to, which means if the word is a verb, a noun, or a pronoun. And finally, we will separate the instances of everyday popular entities from our words to better understand what the sentence is trying to convey. The final file that we will get will be a file which will be understood by a computer. Segmentation. The first process of a pipeline is segmentation. Now what exactly is segmentation? Sentence segmentation or text segmentation is basically dividing the given text into logically decipherable units of information. We divide the sentence into its constituent sub-sentences, usually along the punctuations like full stops or commas, or along line breaks and page components for HTML files. Dividing a document into its constituent sentences allows us to process it without losing its essence and the necessary information that it contains. In this case, let's consider a sentence. The lemonade quenched her thirst, but not her longing. After sentence segmentation, we are going to divide the sentence into two. The first sentence is going to be the lemonade quenched her thirst and the second one is going to be but not her longing. The next step of our pipeline is called tokenization. Tokenization is nothing but the process of dividing a sentence into its constituent words. The sentence that is given to us will be separated and all the words in the sentence will be stored separately. This is done so we can understand the syntactic and semantic information contained in each sentence. Thus we decipher the relevance of her sentence by analyzing it word by word, thereby making sure that no loss of information occurs. The computer does not understand punctuations and special characters. Hence we can remove any punctuations and special characters which may occur. Let's take a part of our previously segmented sentence. Over here, let's consider the lemonade quenched her thirst. After tokenization, we are going to separate every single word in the sentence. So after tokenization, we are going to get five different tokens. The lemonade, quenched, her and thirst. All of these are going to be treated as separate entities. After tokenization, we perform stemming. Stemming is a process of obtaining the word stems of a word. Word stems are also known as the base form of a word and we can create new words by attaching a fixes to them in a process known as inflection. Stemming is a process of recognizing the word stems of individual words. This is done by removing a fixes such as ing, s, ed etc. For example consider our sentence jump. Jump is the word stem of various different words like jumping, jumped and jumps. If we remove all of these fixes, we will get our basic word stem which is jump. This is basically what we want at the end of stemming. The next process in our pipeline is called Lematization. Lematization is the process of figuring out the root form or root word which is nothing but the most basic form, also known as the lemma of each word in the sentence. Lematization is very similar to stemming where we remove word of fixes to get the base form of a word. The difference is that the root word is always a word which is present in the dictionary. But the root stem may not be so. Lematization uses a knowledge base called word net. Let's consider three different words, went, going and gone. At the end of the day, all of these words have originated from a single word which is go. In this case, go is our lemma. All the other words which are derived from go can be traced back to it. The next part of our pipeline is called part of speech tagging. Part of speech tagging is a process of converting a sentence to different forms. It can be a list of words or a list of tuples. The tag in case of is a part of speech tag and signifies whether the word is a noun, adjective, verb and so on. We are basically splitting our verbs into the grammatical components. To understand the meaning of any sentence or to extract relationships and to build a knowledge graph, part of speech tagging is a very important step. As the same word can have different part of speeches in different sentences. For example, let's consider this sentence, give me your answer. In this sentence, answer is a noun. But if we consider another sentence, answer the question. Over here, answer will be a verb. Using part of speech tagging, we can take our different tokens and find the different part of speech that it belongs to. In this case, is a determiner, lemonade is a noun, quenched is a verb, her is a pronoun and thirst is a noun. The final step in our NLP pipeline that we are looking at here is nothing but named entity recognition. Named entity recognition, also known as named entity identification, entity chunking and entity extraction, is a subtask of information extraction that seeks to locate and classify named entities which are mentioned in unstructured text into predefined categories. Extracting the main entities in a text helps us sort unstructured data and detect important information which is crucial if you have to deal with large data sets. The subcategories that we are considering are person, whether the name entity is a person, whether it's a quantity like kilograms, a location, an organization, the name of a movie, or whether it's a monetary value like dollars or euros. So far, we've looked at what NLP is and how we can perform national language processing. But what are some applications of it and where is it used in the real world? One of the applications of NLP is in chatbots. Chatbots can help you solve issues while performing natural language generation. In other words, they can hold a conversation in plain English. A chatbot is nothing but a software application which can be used to conduct an online chat conversation either through text or speech in place of providing direct contact with a live human agent. You might have seen those talk to one of our agents section on websites. Those are usually chatbots. A lot of companies also use WhatsApp chatbots to make the process seem less mechanical. Another application of NLP is speech recognition. Probably the most popular example of NLP in action are virtual assistants like Google Assist, Siri and Alexa. Natural language processing understands and translates the human language like here Siri, whereas the nearest gas station into numbers making it easy for machines to understand. They recognize when you are talking, converting speech to text and understand what you requested. Over the years, virtual assistants have become streamlined enough to be able to emulate human speech patterns almost flawlessly. Another application of NLP is auto correction. Auto correction also known as text replacement, replace as your type or simply auto correct is an automatic data validation function commonly found in word processors and text editing interfaces for smart phones and tablet computers. It acts as a spell check and corrects any spellings or grammar mistakes which may arise as you are typing. Some language checks software like Grammily, Paperator, Reverso and others can even check how unique and engaging your articles are and all of this is done using NLP. Now this brings us to the end of this video on NLP in 10 minutes. We hope that the video was useful to your journey to learning NLP. To learn more about natural language processing and related topics, you can check out the SimplyLearn website which is linked in the description below. To keep learning with fun interactive videos, do subscribe to the SimplyLearn channel. Thank you for watching and keep learning.", "segments": [{"start": 0.0, "end": 13.280000000000001, "text": "Hey everyone, in this video we will be talking about NLP in the next 10 minutes."}, {"start": 13.280000000000001, "end": 15.68, "text": "So what can you expect from this video?"}, {"start": 15.68, "end": 17.28, "text": "What's in it for you?"}, {"start": 17.28, "end": 20.48, "text": "First, we will give you a basic understanding of NLP."}, {"start": 20.48, "end": 25.76, "text": "We will tell you why we need NLP and we will talk about what NLP exactly is."}, {"start": 25.84, "end": 28.0, "text": "Next, we look at NLP pipelines."}, {"start": 28.0, "end": 33.28, "text": "NLP pipelines cover the basic processes involved in text passing in NLP,"}, {"start": 33.28, "end": 37.68, "text": "which means converting text to a form suitable for machines to understand"}, {"start": 37.68, "end": 39.68, "text": "as humans would understand it."}, {"start": 39.68, "end": 43.760000000000005, "text": "And finally, we'll talk about some real-world applications of NLP,"}, {"start": 43.760000000000005, "end": 47.6, "text": "which are all around us and a lot more common than you might have realized."}, {"start": 47.6, "end": 49.92, "text": "So let's get started."}, {"start": 49.92, "end": 52.72, "text": "So what exactly is NLP?"}, {"start": 52.72, "end": 58.64, "text": "Before we talk about NLP, let's talk about how humans talk and interact with each other."}, {"start": 58.64, "end": 63.6, "text": "Humans communicate with each other mostly via speech or text."}, {"start": 63.6, "end": 68.8, "text": "To convey your intent to someone, you may directly talk to them or send them a message."}, {"start": 68.8, "end": 72.8, "text": "The human way of communicating is known as natural language."}, {"start": 72.8, "end": 78.96000000000001, "text": "Around the globe, many languages are spoken and messages are shared virtually every single day."}, {"start": 78.96000000000001, "end": 83.2, "text": "All these conversations, feedbacks, and messages are data in themselves."}, {"start": 84.08000000000001, "end": 90.16000000000001, "text": "This data is extremely valuable as it can give us customer information and insight into human sentiment."}, {"start": 90.16000000000001, "end": 96.80000000000001, "text": "However, this data is not useful to computers as it is not in a form that can be understood by machines."}, {"start": 96.80000000000001, "end": 100.72000000000001, "text": "Machines communicate using ones and zeros and not via words."}, {"start": 100.72000000000001, "end": 104.56, "text": "They cannot understand English, French or Spanish, only binary."}, {"start": 105.52000000000001, "end": 108.72000000000001, "text": "And this is where NLP comes into the picture."}, {"start": 108.8, "end": 111.84, "text": "NLP stands for natural language processing."}, {"start": 112.4, "end": 118.24, "text": "Natural language processing is a branch of artificial intelligence that deals with the interactions"}, {"start": 118.24, "end": 122.16, "text": "between humans and computers. Using the natural language."}, {"start": 122.88, "end": 129.84, "text": "The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages"}, {"start": 129.84, "end": 135.12, "text": "in a manner that is valuable and to build systems that can make sense of text and perform tasks"}, {"start": 135.12, "end": 138.96, "text": "like translation, grammar checking, or topic classification."}, {"start": 139.6, "end": 145.44, "text": "NLP basically is the task of processing written forms of languages and making a computer understand them."}, {"start": 146.0, "end": 151.44, "text": "Companies are increasingly using NLP-equipped tools to gain insights from data into automated"}, {"start": 151.44, "end": 157.6, "text": "routine tasks. A sentiment analyzer, for instance, can help brands detect emotions in text,"}, {"start": 157.6, "end": 159.68, "text": "such as negative comments on social media."}, {"start": 159.76000000000002, "end": 167.12, "text": "NLP at the end of the day is nothing more than accumulation of artificial intelligence,"}, {"start": 167.12, "end": 173.28, "text": "computer science, and the human language. The next topic that we'll be looking at are NLP"}, {"start": 173.28, "end": 181.20000000000002, "text": "pipelines. So what exactly are pipelines? A pipeline is a set of data processing elements which are"}, {"start": 181.20000000000002, "end": 187.20000000000002, "text": "connected in series, whether output of one element is the input of the next one. It basically represents"}, {"start": 187.20000000000002, "end": 191.92000000000002, "text": "the various steps which have to be taken in a computation and the order in which they occur."}, {"start": 193.52, "end": 199.44000000000003, "text": "In this pipeline, we are going to be parsing our data. Parsing means converting our data from one"}, {"start": 199.44000000000003, "end": 207.04000000000002, "text": "form to another. In this case, we are parsing our input text data to a file which can be understood"}, {"start": 207.04000000000002, "end": 213.84000000000003, "text": "by a computer. The text is first broken into segments, and the segments are then converted into even"}, {"start": 213.84, "end": 222.4, "text": "smaller tokens. The word stems of all of these tokens are found followed by finding the root words"}, {"start": 222.4, "end": 229.36, "text": "of these stems in a process known as lemmatization. We will then recognize which part of speech the word"}, {"start": 229.36, "end": 236.88, "text": "belongs to, which means if the word is a verb, a noun, or a pronoun. And finally, we will separate"}, {"start": 236.88, "end": 242.96, "text": "the instances of everyday popular entities from our words to better understand what the sentence is"}, {"start": 242.96, "end": 248.08, "text": "trying to convey. The final file that we will get will be a file which will be understood by a"}, {"start": 248.08, "end": 256.16, "text": "computer. Segmentation. The first process of a pipeline is segmentation. Now what exactly is segmentation?"}, {"start": 256.88, "end": 263.6, "text": "Sentence segmentation or text segmentation is basically dividing the given text into logically"}, {"start": 263.6, "end": 270.40000000000003, "text": "decipherable units of information. We divide the sentence into its constituent sub-sentences,"}, {"start": 270.4, "end": 276.56, "text": "usually along the punctuations like full stops or commas, or along line breaks and page"}, {"start": 276.56, "end": 284.23999999999995, "text": "components for HTML files. Dividing a document into its constituent sentences allows us to process it"}, {"start": 284.23999999999995, "end": 289.91999999999996, "text": "without losing its essence and the necessary information that it contains. In this case,"}, {"start": 289.91999999999996, "end": 296.15999999999997, "text": "let's consider a sentence. The lemonade quenched her thirst, but not her longing. After sentence"}, {"start": 296.24, "end": 301.36, "text": "segmentation, we are going to divide the sentence into two. The first sentence is going to be the"}, {"start": 301.36, "end": 308.0, "text": "lemonade quenched her thirst and the second one is going to be but not her longing. The next step of"}, {"start": 308.0, "end": 314.8, "text": "our pipeline is called tokenization. Tokenization is nothing but the process of dividing a sentence"}, {"start": 314.8, "end": 321.76000000000005, "text": "into its constituent words. The sentence that is given to us will be separated and all the words in"}, {"start": 321.84, "end": 328.32, "text": "the sentence will be stored separately. This is done so we can understand the syntactic and semantic"}, {"start": 328.32, "end": 334.96, "text": "information contained in each sentence. Thus we decipher the relevance of her sentence by analyzing"}, {"start": 334.96, "end": 341.44, "text": "it word by word, thereby making sure that no loss of information occurs. The computer does not"}, {"start": 341.44, "end": 346.88, "text": "understand punctuations and special characters. Hence we can remove any punctuations and special"}, {"start": 346.96, "end": 353.12, "text": "characters which may occur. Let's take a part of our previously segmented sentence. Over here,"}, {"start": 353.12, "end": 359.04, "text": "let's consider the lemonade quenched her thirst. After tokenization, we are going to separate every"}, {"start": 359.04, "end": 365.76, "text": "single word in the sentence. So after tokenization, we are going to get five different tokens. The"}, {"start": 366.48, "end": 373.12, "text": "lemonade, quenched, her and thirst. All of these are going to be treated as separate entities."}, {"start": 373.68, "end": 379.76, "text": "After tokenization, we perform stemming. Stemming is a process of obtaining the word stems of a word."}, {"start": 380.56, "end": 386.0, "text": "Word stems are also known as the base form of a word and we can create new words by attaching"}, {"start": 386.0, "end": 391.68, "text": "a fixes to them in a process known as inflection. Stemming is a process of recognizing the word stems"}, {"start": 391.68, "end": 399.52, "text": "of individual words. This is done by removing a fixes such as ing, s, ed etc. For example consider"}, {"start": 399.52, "end": 406.15999999999997, "text": "our sentence jump. Jump is the word stem of various different words like jumping,"}, {"start": 407.12, "end": 414.64, "text": "jumped and jumps. If we remove all of these fixes, we will get our basic word stem which is jump."}, {"start": 415.2, "end": 420.79999999999995, "text": "This is basically what we want at the end of stemming. The next process in our pipeline is called"}, {"start": 420.79999999999995, "end": 427.28, "text": "Lematization. Lematization is the process of figuring out the root form or root word which is"}, {"start": 427.28000000000003, "end": 432.88000000000005, "text": "nothing but the most basic form, also known as the lemma of each word in the sentence."}, {"start": 433.68, "end": 439.92, "text": "Lematization is very similar to stemming where we remove word of fixes to get the base form of a word."}, {"start": 439.92, "end": 444.24, "text": "The difference is that the root word is always a word which is present in the dictionary."}, {"start": 444.8, "end": 450.64000000000004, "text": "But the root stem may not be so. Lematization uses a knowledge base called word net."}, {"start": 451.28, "end": 457.12, "text": "Let's consider three different words, went, going and gone. At the end of the day,"}, {"start": 457.12, "end": 464.8, "text": "all of these words have originated from a single word which is go. In this case, go is our lemma."}, {"start": 464.8, "end": 468.96, "text": "All the other words which are derived from go can be traced back to it."}, {"start": 469.76, "end": 475.76, "text": "The next part of our pipeline is called part of speech tagging. Part of speech tagging is a process"}, {"start": 475.76, "end": 481.03999999999996, "text": "of converting a sentence to different forms. It can be a list of words or a list of tuples."}, {"start": 482.08, "end": 487.44, "text": "The tag in case of is a part of speech tag and signifies whether the word is a noun,"}, {"start": 487.44, "end": 493.68, "text": "adjective, verb and so on. We are basically splitting our verbs into the grammatical components."}, {"start": 494.32, "end": 499.28, "text": "To understand the meaning of any sentence or to extract relationships and to build a knowledge"}, {"start": 499.28, "end": 504.96, "text": "graph, part of speech tagging is a very important step. As the same word can have different"}, {"start": 504.96000000000004, "end": 511.68000000000006, "text": "part of speeches in different sentences. For example, let's consider this sentence, give me"}, {"start": 511.68000000000006, "end": 517.6, "text": "your answer. In this sentence, answer is a noun. But if we consider another sentence,"}, {"start": 518.32, "end": 524.5600000000001, "text": "answer the question. Over here, answer will be a verb. Using part of speech tagging, we can take"}, {"start": 524.5600000000001, "end": 530.72, "text": "our different tokens and find the different part of speech that it belongs to. In this case,"}, {"start": 531.12, "end": 538.5600000000001, "text": "is a determiner, lemonade is a noun, quenched is a verb, her is a pronoun and thirst is a noun."}, {"start": 539.12, "end": 545.6800000000001, "text": "The final step in our NLP pipeline that we are looking at here is nothing but named entity recognition."}, {"start": 546.4, "end": 554.88, "text": "Named entity recognition, also known as named entity identification, entity chunking and entity extraction,"}, {"start": 554.88, "end": 561.68, "text": "is a subtask of information extraction that seeks to locate and classify named entities"}, {"start": 561.68, "end": 567.6, "text": "which are mentioned in unstructured text into predefined categories. Extracting the main entities"}, {"start": 567.6, "end": 573.4399999999999, "text": "in a text helps us sort unstructured data and detect important information which is crucial if"}, {"start": 573.4399999999999, "end": 580.8, "text": "you have to deal with large data sets. The subcategories that we are considering are person, whether the"}, {"start": 580.8000000000001, "end": 591.0400000000001, "text": "name entity is a person, whether it's a quantity like kilograms, a location, an organization,"}, {"start": 592.5600000000001, "end": 600.72, "text": "the name of a movie, or whether it's a monetary value like dollars or euros. So far, we've looked at"}, {"start": 600.72, "end": 607.44, "text": "what NLP is and how we can perform national language processing. But what are some applications of it"}, {"start": 607.5200000000001, "end": 614.8000000000001, "text": "and where is it used in the real world? One of the applications of NLP is in chatbots."}, {"start": 614.8000000000001, "end": 620.8000000000001, "text": "Chatbots can help you solve issues while performing natural language generation. In other words,"}, {"start": 620.8000000000001, "end": 626.72, "text": "they can hold a conversation in plain English. A chatbot is nothing but a software application"}, {"start": 626.72, "end": 632.4000000000001, "text": "which can be used to conduct an online chat conversation either through text or speech"}, {"start": 632.48, "end": 637.6, "text": "in place of providing direct contact with a live human agent. You might have seen those"}, {"start": 637.6, "end": 644.0799999999999, "text": "talk to one of our agents section on websites. Those are usually chatbots. A lot of companies"}, {"start": 644.0799999999999, "end": 651.04, "text": "also use WhatsApp chatbots to make the process seem less mechanical. Another application of NLP"}, {"start": 651.04, "end": 658.16, "text": "is speech recognition. Probably the most popular example of NLP in action are virtual assistants"}, {"start": 658.16, "end": 664.4, "text": "like Google Assist, Siri and Alexa. Natural language processing understands and translates the"}, {"start": 664.4, "end": 670.88, "text": "human language like here Siri, whereas the nearest gas station into numbers making it easy for machines"}, {"start": 670.88, "end": 676.9599999999999, "text": "to understand. They recognize when you are talking, converting speech to text and understand what"}, {"start": 676.9599999999999, "end": 683.04, "text": "you requested. Over the years, virtual assistants have become streamlined enough to be able to emulate"}, {"start": 683.04, "end": 691.12, "text": "human speech patterns almost flawlessly. Another application of NLP is auto correction. Auto correction"}, {"start": 691.12, "end": 698.0, "text": "also known as text replacement, replace as your type or simply auto correct is an automatic data"}, {"start": 698.0, "end": 704.48, "text": "validation function commonly found in word processors and text editing interfaces for smart phones"}, {"start": 704.48, "end": 711.04, "text": "and tablet computers. It acts as a spell check and corrects any spellings or grammar mistakes which"}, {"start": 711.04, "end": 717.92, "text": "may arise as you are typing. Some language checks software like Grammily, Paperator, Reverso"}, {"start": 717.92, "end": 724.64, "text": "and others can even check how unique and engaging your articles are and all of this is done using NLP."}, {"start": 725.4399999999999, "end": 730.64, "text": "Now this brings us to the end of this video on NLP in 10 minutes. We hope that the video was useful"}, {"start": 730.64, "end": 736.7199999999999, "text": "to your journey to learning NLP. To learn more about natural language processing and related topics,"}, {"start": 736.72, "end": 741.0400000000001, "text": "you can check out the SimplyLearn website which is linked in the description below."}, {"start": 741.0400000000001, "end": 746.0, "text": "To keep learning with fun interactive videos, do subscribe to the SimplyLearn channel."}, {"start": 746.0, "end": 748.1600000000001, "text": "Thank you for watching and keep learning."}], "language": "en"}
{"title": "Introduction | NLP Tutorial For Beginners In Python  -  Season 1 Episode 1", "video_id": "R-AG4-qZs1A", "text": "I am super excited to announce the beginning of natural language processing in Python Tutorial playlist today. Previously I had uploaded machine learning and deep learning Tutorial playlist along with end to end projects on my youtube channel which received more than 2 million views and not only people were able to understand complex topics using very simple and intuitive explanations but they were able to practice on end to end projects and they got a lot of them got a job as well I have received so many testimonials. After the success of those two series when I conducted a survey on my channel regarding which series I should work next and overwhelming majority of you suggested NLP. There will be four main highlights of this entire playlist. Number one is very easy intuitive understanding of complex topics. You can go to youtube and search for what is convolution neural network and you will find my video where I used visual representation to explain very complex topic in such a way that even a high school student can understand it easily. By watching that video you will get a glimpse on what kind of presentations you can expect from this particular Tutorial series. Highlight number two will be a lot of hands-on coding and exercises. Highlight number three would be end to end projects so we will take a real industry problem and build an end to end application in NLP along with deployment into the cloud and the fourth highlight will be expert talks. I know a lot of friends who have who are working as a data scientist or NLP engineer in the industry both in US and in India and I want to invite them to discuss different topics or how NLP is used in the industry so that you are not just learning academic topics and just practicing on some dummy toy examples but you also get a feel of what goes on in the industry. I want to say my special thanks to the authors of practical natural language processing book because some of the content in this playlist is going to be influenced by this book. I have read this book. It's an amazing NLP book. It has a lot of practical tips on how to build NLP system end to end to solve various industry use cases. Two authors of this book have been on core basics YouTube channel Anuj Gupta who is a head of machine learning at Vahan and Boodi Satwa who is a Facebook AI researcher working in California here in USA. These guys are experts. They know what they're talking about and I absolutely recommend this book to anyone who is interested in NLP. This is not a sponsored video by the way. This is my own genuine feedback. The link of the book is in video description below. Now what exactly is NLP? I'm going to show you some real-life use cases where NLP is impacting you and by looking at those use cases we will understand what is NLP. The first use case is Gmail. When you're typing any sentence in your Gmail you will notice that it tries to autocomplete. See here it says if it changes in the future and this autocompletion is done using NLP. The other standard use case is spam filters. If these emails didn't have spam filters then you will be so much worried and you will get so much headache by this bombarding of all kind of commercial spam but luckily using NLP you can filter them and you can take them out of your inbox and in NLP they use these keywords. The spam emails will have things like hurry up the offer ends and you want some price and all of that bullshit so based on some of those terms and using some machine learning based classification model you can filter the spam messages. The other use case is language translation where you can use Google translate. You can translate a sentence in one language to another language with very high accuracy. This wasn't possible many years back but nowadays these translators are pretty good. The other standard use case is customer service chat board so nowadays if you are using any service let's say bank and you go to their chat service you type in a message and many times there is no human on the other hand. The chat board can interpret your language the question you're asking and it can derive intent out of it and it can respond to your question on its own and sometimes when it doesn't work well then they connect it to human beings so chat boards is becoming a big use case in NLP. The other standard one is the voice assistance such as Amazon Alexa and Google Assistant. I have a Google Pixel phone here and I'm going to show you I'm just going to ask what appointments do I have tomorrow and it will show me so it actually showed me what kind of appointment do I have. So these voice assistance are becoming very powerful if you have Amazon Alexa or Google Assistant at home. They can even tell you okay you have a meeting at 9 o'clock and there is a lot of traffic so instead of leaving at 8.30 you should leave at 8.15. It can do that kind of assistance. Google search uses NLP or to answer your question and they use this special language model called BUDD. So before BUDD if you type a discussion in Google saying can you get medicine for someone pharmacy? See before that it wasn't giving you the right answer but now it gives you a precise answer and this is again possible because of the use of natural language processing. Automated news generation for news companies is another use case. I work for Bloomberg and in Bloomberg we automated the news stories to predict market events. I'm going to link this Bloomberg's article in the video description below but basically the picture here is off a Bloomberg terminal and all the news stories that you are seeing they were not written by some human editor basically they were all generated by computer using NLP using some AI techniques. They detect some signals and they can auto write these stories and that's pretty cool. There are many other use cases as well but to summarize NLP is a field in computer science and AI that gives machines and ability to understand human language better and to assist it in language related tasks. If you look at computer traditionally they are designed to work on numbers and they were pretty good at working at tasks which are related to numbers but now using NLP you can help computer assist you. In language related tasks the use cases that we saw like question and answer the the language translation things like that and in this tutorial series we are going to use Python of course as a programming language and we will be using spicy jensen analytic these are like different libraries that allows you to do NLP in Python. We'll also use scikit learn for our machine learning problems and then tensor flow and PyTouch for deep learning related problems in NLP and hugging phase 2. Now I know there is a lot to learn what we'll do is we'll take a very practical approach we'll take a concept or a problem and then whatever library can solve the problem in a better way we'll try to use it so don't worry that you're going to run or so many things like say one or eight different libraries in the end these are all tools what you care about is here is the given problem how do you come up with a solution and for coming up with that solution in a better way whatever tool you need to use you need to use it's syntax you can google it and you can figure things out easily. Now talking about career opportunities in the field of NLP you have three roles that you can select from one is data scientist specializing in NLP the second one is NLP engineer which is basically a machine learning engineer solving NLP problems and the third one is NLP researcher all of these roles are very high paying so if you have expertise in the field of NLP and if you choose any of these roles in US for example you can make anywhere from $100,000 per year to $650,000 per year now all of this depends on your experience the the company that you're working for the location and all those factors but I'm just giving you a broader range if you're in India you can make anywhere from 10 lakh rupees a year to one crore rupees a year yes people make that kind of crazy money when they work for big tech company and they are solving cool problems that's all we had for today in the next video we are going to talk about why NLP is booming right now in terms of video upload schedule I will try to upload one video every week but my schedule is very crazy nowadays so if there is a delay please be with me but I'm committed and I will try my best to wrap up this series as soon as I can thank you", "segments": [{"start": 0.0, "end": 6.0, "text": "I am super excited to announce the beginning of natural language processing in Python Tutorial"}, {"start": 6.0, "end": 11.28, "text": "playlist today. Previously I had uploaded machine learning and deep learning Tutorial playlist"}, {"start": 11.28, "end": 17.36, "text": "along with end to end projects on my youtube channel which received more than 2 million views"}, {"start": 17.36, "end": 24.88, "text": "and not only people were able to understand complex topics using very simple and intuitive explanations"}, {"start": 24.959999999999997, "end": 31.439999999999998, "text": "but they were able to practice on end to end projects and they got a lot of them got a job as well"}, {"start": 31.439999999999998, "end": 37.76, "text": "I have received so many testimonials. After the success of those two series when I conducted a survey"}, {"start": 37.76, "end": 44.16, "text": "on my channel regarding which series I should work next and overwhelming majority of you suggested"}, {"start": 44.16, "end": 50.879999999999995, "text": "NLP. There will be four main highlights of this entire playlist. Number one is very easy"}, {"start": 50.88, "end": 56.400000000000006, "text": "intuitive understanding of complex topics. You can go to youtube and search for what is"}, {"start": 56.400000000000006, "end": 63.92, "text": "convolution neural network and you will find my video where I used visual representation to explain"}, {"start": 63.92, "end": 69.12, "text": "very complex topic in such a way that even a high school student can understand it easily."}, {"start": 69.12, "end": 75.2, "text": "By watching that video you will get a glimpse on what kind of presentations you can expect from"}, {"start": 75.28, "end": 82.24000000000001, "text": "this particular Tutorial series. Highlight number two will be a lot of hands-on coding and exercises."}, {"start": 82.88, "end": 90.0, "text": "Highlight number three would be end to end projects so we will take a real industry problem and build"}, {"start": 90.0, "end": 98.32000000000001, "text": "an end to end application in NLP along with deployment into the cloud and the fourth highlight"}, {"start": 98.32000000000001, "end": 105.36000000000001, "text": "will be expert talks. I know a lot of friends who have who are working as a data scientist or NLP"}, {"start": 105.36000000000001, "end": 113.76, "text": "engineer in the industry both in US and in India and I want to invite them to discuss different topics"}, {"start": 113.76, "end": 120.56, "text": "or how NLP is used in the industry so that you are not just learning academic topics and just"}, {"start": 120.56, "end": 127.2, "text": "practicing on some dummy toy examples but you also get a feel of what goes on in the industry."}, {"start": 127.2, "end": 131.76, "text": "I want to say my special thanks to the authors of practical natural language processing"}, {"start": 132.48, "end": 137.52, "text": "book because some of the content in this playlist is going to be influenced by this book. I have read"}, {"start": 137.52, "end": 144.16, "text": "this book. It's an amazing NLP book. It has a lot of practical tips on how to build NLP system"}, {"start": 144.16, "end": 150.32, "text": "end to end to solve various industry use cases. Two authors of this book have been on core"}, {"start": 150.4, "end": 156.88, "text": "basics YouTube channel Anuj Gupta who is a head of machine learning at Vahan and Boodi Satwa who is"}, {"start": 156.88, "end": 164.48, "text": "a Facebook AI researcher working in California here in USA. These guys are experts. They know what"}, {"start": 164.48, "end": 171.28, "text": "they're talking about and I absolutely recommend this book to anyone who is interested in NLP."}, {"start": 171.28, "end": 176.32, "text": "This is not a sponsored video by the way. This is my own genuine feedback. The link of the book is"}, {"start": 176.32, "end": 184.0, "text": "in video description below. Now what exactly is NLP? I'm going to show you some real-life use cases"}, {"start": 184.0, "end": 190.72, "text": "where NLP is impacting you and by looking at those use cases we will understand what is NLP."}, {"start": 191.28, "end": 198.24, "text": "The first use case is Gmail. When you're typing any sentence in your Gmail you will notice that it tries"}, {"start": 198.24, "end": 205.84, "text": "to autocomplete. See here it says if it changes in the future and this autocompletion is done using NLP."}, {"start": 205.84, "end": 213.28, "text": "The other standard use case is spam filters. If these emails didn't have spam filters then you will"}, {"start": 213.28, "end": 221.44, "text": "be so much worried and you will get so much headache by this bombarding of all kind of commercial spam"}, {"start": 221.44, "end": 230.56, "text": "but luckily using NLP you can filter them and you can take them out of your inbox and in NLP they use"}, {"start": 230.64000000000001, "end": 239.04, "text": "these keywords. The spam emails will have things like hurry up the offer ends and you want some"}, {"start": 239.04, "end": 246.16, "text": "price and all of that bullshit so based on some of those terms and using some machine learning"}, {"start": 246.16, "end": 253.92000000000002, "text": "based classification model you can filter the spam messages. The other use case is language translation"}, {"start": 254.00000000000003, "end": 264.32, "text": "where you can use Google translate. You can translate a sentence in one language to another language with"}, {"start": 264.32, "end": 270.16, "text": "very high accuracy. This wasn't possible many years back but nowadays these translators are"}, {"start": 270.16, "end": 277.6, "text": "pretty good. The other standard use case is customer service chat board so nowadays if you are using"}, {"start": 277.6, "end": 283.92, "text": "any service let's say bank and you go to their chat service you type in a message and many times"}, {"start": 284.64000000000004, "end": 291.92, "text": "there is no human on the other hand. The chat board can interpret your language the question you're"}, {"start": 291.92, "end": 299.52000000000004, "text": "asking and it can derive intent out of it and it can respond to your question on its own and sometimes"}, {"start": 299.52000000000004, "end": 305.76000000000005, "text": "when it doesn't work well then they connect it to human beings so chat boards is becoming a big use case"}, {"start": 305.76, "end": 315.76, "text": "in NLP. The other standard one is the voice assistance such as Amazon Alexa and Google Assistant."}, {"start": 315.76, "end": 322.48, "text": "I have a Google Pixel phone here and I'm going to show you I'm just going to ask what appointments do"}, {"start": 322.48, "end": 328.8, "text": "I have tomorrow and it will show me so it actually showed me what kind of appointment do I have."}, {"start": 328.8, "end": 333.84, "text": "So these voice assistance are becoming very powerful if you have Amazon Alexa or Google"}, {"start": 333.84000000000003, "end": 340.16, "text": "Assistant at home. They can even tell you okay you have a meeting at 9 o'clock and there is a lot of"}, {"start": 340.16, "end": 346.72, "text": "traffic so instead of leaving at 8.30 you should leave at 8.15. It can do that kind of assistance."}, {"start": 347.76000000000005, "end": 355.52000000000004, "text": "Google search uses NLP or to answer your question and they use this special language model called"}, {"start": 355.52000000000004, "end": 362.16, "text": "BUDD. So before BUDD if you type a discussion in Google saying can you get medicine for someone"}, {"start": 362.16, "end": 368.88000000000005, "text": "pharmacy? See before that it wasn't giving you the right answer but now it gives you a precise"}, {"start": 368.88000000000005, "end": 375.04, "text": "answer and this is again possible because of the use of natural language processing. Automated"}, {"start": 375.04, "end": 382.32000000000005, "text": "news generation for news companies is another use case. I work for Bloomberg and in Bloomberg we"}, {"start": 382.32000000000005, "end": 388.48, "text": "automated the news stories to predict market events. I'm going to link this Bloomberg's article in"}, {"start": 388.48, "end": 395.20000000000005, "text": "the video description below but basically the picture here is off a Bloomberg terminal and all"}, {"start": 395.20000000000005, "end": 400.88, "text": "the news stories that you are seeing they were not written by some human editor basically they were"}, {"start": 400.88, "end": 409.76, "text": "all generated by computer using NLP using some AI techniques. They detect some signals and they can"}, {"start": 409.76, "end": 416.96000000000004, "text": "auto write these stories and that's pretty cool. There are many other use cases as well but to summarize"}, {"start": 417.68000000000006, "end": 423.76000000000005, "text": "NLP is a field in computer science and AI that gives machines and ability to understand"}, {"start": 423.76000000000005, "end": 430.00000000000006, "text": "human language better and to assist it in language related tasks. If you look at computer"}, {"start": 430.00000000000006, "end": 436.00000000000006, "text": "traditionally they are designed to work on numbers and they were pretty good at working at tasks which"}, {"start": 436.00000000000006, "end": 445.84000000000003, "text": "are related to numbers but now using NLP you can help computer assist you. In language related"}, {"start": 445.84000000000003, "end": 452.88000000000005, "text": "tasks the use cases that we saw like question and answer the the language translation things like"}, {"start": 452.88000000000005, "end": 460.08000000000004, "text": "that and in this tutorial series we are going to use Python of course as a programming language"}, {"start": 460.08000000000004, "end": 467.20000000000005, "text": "and we will be using spicy jensen analytic these are like different libraries that allows you to do"}, {"start": 467.20000000000005, "end": 475.44000000000005, "text": "NLP in Python. We'll also use scikit learn for our machine learning problems and then tensor"}, {"start": 475.44, "end": 483.84, "text": "flow and PyTouch for deep learning related problems in NLP and hugging phase 2. Now I know there is a lot"}, {"start": 483.84, "end": 490.72, "text": "to learn what we'll do is we'll take a very practical approach we'll take a concept or a problem"}, {"start": 490.72, "end": 496.56, "text": "and then whatever library can solve the problem in a better way we'll try to use it so don't worry"}, {"start": 496.56, "end": 501.12, "text": "that you're going to run or so many things like say one or eight different libraries in the end"}, {"start": 501.12, "end": 507.68, "text": "these are all tools what you care about is here is the given problem how do you come up with a"}, {"start": 507.68, "end": 512.88, "text": "solution and for coming up with that solution in a better way whatever tool you need to use you need"}, {"start": 512.88, "end": 518.64, "text": "to use it's syntax you can google it and you can figure things out easily. Now talking about career"}, {"start": 518.64, "end": 525.76, "text": "opportunities in the field of NLP you have three roles that you can select from one is data scientist"}, {"start": 525.76, "end": 531.36, "text": "specializing in NLP the second one is NLP engineer which is basically a machine learning engineer"}, {"start": 531.36, "end": 539.52, "text": "solving NLP problems and the third one is NLP researcher all of these roles are very high paying so"}, {"start": 539.52, "end": 546.24, "text": "if you have expertise in the field of NLP and if you choose any of these roles in US for example you can"}, {"start": 546.24, "end": 556.16, "text": "make anywhere from $100,000 per year to $650,000 per year now all of this depends on your experience"}, {"start": 556.96, "end": 562.96, "text": "the the company that you're working for the location and all those factors but I'm just giving you"}, {"start": 562.96, "end": 568.88, "text": "a broader range if you're in India you can make anywhere from 10 lakh rupees a year to one"}, {"start": 568.88, "end": 575.12, "text": "crore rupees a year yes people make that kind of crazy money when they work for big tech company and"}, {"start": 575.2, "end": 580.5600000000001, "text": "they are solving cool problems that's all we had for today in the next video we are going to talk"}, {"start": 580.5600000000001, "end": 587.36, "text": "about why NLP is booming right now in terms of video upload schedule I will try to upload one video"}, {"start": 587.36, "end": 593.44, "text": "every week but my schedule is very crazy nowadays so if there is a delay please be with me but I'm"}, {"start": 593.44, "end": 604.8800000000001, "text": "committed and I will try my best to wrap up this series as soon as I can thank you"}], "language": "en"}
{"title": "Training a model to recognize sentiment in text (NLP Zero to Hero - Part 3)", "video_id": "Y_hzMnRXjhI", "text": "Hi, and welcome back to this series on zero-to-hero with TensorFlow, where we're looking at natural language processing. In the last couple of episodes, you saw how to tokenize text into numeric values and how to use tools in TensorFlow to regularize and pad that text. Now that we've gotten the pre-processing out of the way, we can next look at how to build a classifier to recognize sentiment in text. We'll start by using a data set of headlines where the headline has been categorized as sarcastic or not. We'll train a classifier on this, and it can then tell us afterwards if a new piece of text looks like it might be sarcastic. We'll use Rishav Mizra's data set from Kaggle, and you can find details on it here. The data is nice and simple. The is sarcastic field is one, if it's sarky, and zero otherwise. There's a headline where the text will train on, and then there's a URL to the article if you're interested in reading it. But we're not going to use this just the headline text. The data is stored in JSON format like this, pretty straightforward. We'll have to convert it to Python format for training, so it will look like this. Every JSON element becomes a Python list element, and it's all encapsulated in square brackets. Python has a JSON toolkit that can achieve this, and here's the complete code. We'll go through it step by step. First of all, we'll import the JSON library. Then, we can load in the sarcasm JSON file using the JSON library. We can then create lists for the labels, headlines, and article URLs. And when we iterate through the JSON, we can load the requisite values into our Python lists. Now that we have three lists, one with our labels, one with the text, and one with the URLs, we can start doing our familiar pre-processing on the text. Here's the code. By calling tokenizer.fit on text with the headline, we'll create tokens for every word in the corpus. And then we'll see them in the word index. You can see an example of some of the words here. So, Underwood has been tokenized to 24127, and Schilling's ball, what is that anyway, to 23055. So now, we can turn our sentences into sequences of tokens, and pad them to the same length with this code. If we want to inspect them, we can simply print them out. Here you can see one tokenized sentence, and the shape of the entire corpus. That's 26,709 sequences each with 40 tokens. Now there's a problem here. We don't have a split in the data for training and testing. We just have a list of 26,709 sequences. Fortunately, Python makes it super easy for us to slice this up. Let's take a look at that next. So, we have a bunch of sentences in a list, and a bunch of labels in a list. To slice them into training and test sets is actually pretty easy. If we pick a training size, say, 20,000, we can cut it up with code like this. So the training sentences will be the first 20,000 sliced by this syntax, and the testing sentences will be the remaining slices like this. And we can do the same for the labels to get a training and a test set. But there's a bit of a problem. Remember earlier, we used the tokenizer to create a word index of every word in the set? That was all very good, but if we really want to test its effectiveness, we have to ensure that the neural net only sees the training data and that it never sees the test data. So we have to rewrite our code to ensure that the tokenizer is just fit to the training data. Let's take a look at how to do that now. Here's the new code to create our training and test sets. Let's look at it line by line. We'll first instantiate a tokenizer like before, but now we'll fit the tokenizer on just the training sentences that we split out earlier instead of the entire corpus. And now instead of one overall set of sequences, we can now create a set of training sequences and pad them, and then do exactly the same thing for the test sequences is really that easy. But you might be wondering at this point, we've turned our sentences into numbers with the numbers being tokens representing words, but how do we get meaning from that? How do we determine if something is sarcastic just from the numbers? Well, here's where the context of embeddings come in. Let's consider the most basic of sentiments. Something is good, or something is bad. We often see these as being opposites, so we can plot them as having opposite directions like this. So then what happens with a word like meh? It's not particularly good, and it's not particularly bad, probably a little more bad than good, so you might plot it a bit like this. Or the phrase not bad, which is usually meant to plot something as having a little bit of goodness, but not necessarily very good, so it might look like this. Now if we plot this on an x and y axis, we can start to determine the good or bad sentiment as coordinates in the x and y. Good is 1,0, meh is minus 0.4, 0.7, etc. By looking at the direction of the vector, we can start to determine the meaning of the word. So what if you extend that into multiple dimensions instead of just two? What if words that are labeled with sentiment like sarcastic and not sarcastic are plotted in these multiple dimensions? And then as we train, we try to learn what the direction in these multi-dimensional spaces should look like. Words that only appear in the sarcastic sentences will have a strong component in the sarcastic direction, and others will have one in the not sarcastic direction. As we load more and more sentences into the network for training, these directions can change. And when we have a fully trained network and give it a set of words, it could look up the vectors for these words, sum them up and thus give us an idea for the sentiment. This concept is known as embedding. So going back to this diagram, consider what would have happened if I said something was not bad a bit meh. If we were to sum up the vectors, we'd have something that's 0.7 on y and 0.1 on x, so its sentiment could be considered slightly on the good side of neutral. So now let's take a look at coding this. Here's my neural network code. The top layer is an embedding where the direction of each word will be learned epoch by epoch. After that, we pull with a global average pooling, namely adding up the vectors as I demonstrated earlier. This is then fed into a common or garden deep neural network. Training is now as simple as model.fit using the training data and labels and specifying the testing padded and labels for the validation data. At this URL, you can try it out for yourself. And here you can see the results that I got training it for just 30 epochs. While it was able to fit the training data to 99% accuracy, more importantly with the test data, that is words that the network has never seen, it still got 81% to 82% accuracy, which is pretty good. So how do we use this to establish sentiment for new sentences? Here's the code. Let's create a couple of sentences that we want to classify. The first one looks a little bit sarcastic and the second one's quite plain and boring. We'll use the tokenizer that we created earlier to convert them into sequences. This way the words will have the same tokens as the training set. We'll then pad those sequences to be the same dimensions as those in the training set and use the same padding type. And we can then predict on the padded set. The results are like this. The first sentence gives me .91, which is very close to 1, indicating that there's a very high probability of sarcasm. The second is 5 times 10 to the minus 6, indicating an extremely low chance of sarcasm. It does seem to be working. All of this code is runnable in a colab at this URL, so give it a try for yourself. You've now built your first text classification model to understand sentiment in text. Give it a try for yourself and let us know what kind of classifiers you built. I hope you've enjoyed this short series and there's more on the way. So don't forget to hit that subscribe button and get the latest and greatest in AI videos right here on the TensorFlow Channel.", "segments": [{"start": 0.0, "end": 10.0, "text": "Hi, and welcome back to this series on zero-to-hero with TensorFlow, where we're looking at natural language processing."}, {"start": 10.0, "end": 15.0, "text": "In the last couple of episodes, you saw how to tokenize text into numeric values"}, {"start": 15.0, "end": 19.0, "text": "and how to use tools in TensorFlow to regularize and pad that text."}, {"start": 19.0, "end": 27.0, "text": "Now that we've gotten the pre-processing out of the way, we can next look at how to build a classifier to recognize sentiment in text."}, {"start": 27.0, "end": 33.0, "text": "We'll start by using a data set of headlines where the headline has been categorized as sarcastic or not."}, {"start": 33.0, "end": 41.0, "text": "We'll train a classifier on this, and it can then tell us afterwards if a new piece of text looks like it might be sarcastic."}, {"start": 41.0, "end": 46.0, "text": "We'll use Rishav Mizra's data set from Kaggle, and you can find details on it here."}, {"start": 46.0, "end": 52.0, "text": "The data is nice and simple. The is sarcastic field is one, if it's sarky, and zero otherwise."}, {"start": 52.0, "end": 58.0, "text": "There's a headline where the text will train on, and then there's a URL to the article if you're interested in reading it."}, {"start": 58.0, "end": 61.0, "text": "But we're not going to use this just the headline text."}, {"start": 61.0, "end": 65.0, "text": "The data is stored in JSON format like this, pretty straightforward."}, {"start": 65.0, "end": 70.0, "text": "We'll have to convert it to Python format for training, so it will look like this."}, {"start": 70.0, "end": 76.0, "text": "Every JSON element becomes a Python list element, and it's all encapsulated in square brackets."}, {"start": 76.0, "end": 83.0, "text": "Python has a JSON toolkit that can achieve this, and here's the complete code. We'll go through it step by step."}, {"start": 83.0, "end": 90.0, "text": "First of all, we'll import the JSON library. Then, we can load in the sarcasm JSON file using the JSON library."}, {"start": 90.0, "end": 95.0, "text": "We can then create lists for the labels, headlines, and article URLs."}, {"start": 95.0, "end": 101.0, "text": "And when we iterate through the JSON, we can load the requisite values into our Python lists."}, {"start": 101.0, "end": 106.0, "text": "Now that we have three lists, one with our labels, one with the text, and one with the URLs,"}, {"start": 106.0, "end": 111.0, "text": "we can start doing our familiar pre-processing on the text. Here's the code."}, {"start": 111.0, "end": 118.0, "text": "By calling tokenizer.fit on text with the headline, we'll create tokens for every word in the corpus."}, {"start": 118.0, "end": 123.0, "text": "And then we'll see them in the word index. You can see an example of some of the words here."}, {"start": 123.0, "end": 132.0, "text": "So, Underwood has been tokenized to 24127, and Schilling's ball, what is that anyway, to 23055."}, {"start": 132.0, "end": 139.0, "text": "So now, we can turn our sentences into sequences of tokens, and pad them to the same length with this code."}, {"start": 139.0, "end": 143.0, "text": "If we want to inspect them, we can simply print them out."}, {"start": 143.0, "end": 152.0, "text": "Here you can see one tokenized sentence, and the shape of the entire corpus. That's 26,709 sequences each with 40 tokens."}, {"start": 153.0, "end": 157.0, "text": "Now there's a problem here. We don't have a split in the data for training and testing."}, {"start": 157.0, "end": 165.0, "text": "We just have a list of 26,709 sequences. Fortunately, Python makes it super easy for us to slice this up."}, {"start": 165.0, "end": 168.0, "text": "Let's take a look at that next."}, {"start": 168.0, "end": 173.0, "text": "So, we have a bunch of sentences in a list, and a bunch of labels in a list."}, {"start": 173.0, "end": 176.0, "text": "To slice them into training and test sets is actually pretty easy."}, {"start": 176.0, "end": 181.0, "text": "If we pick a training size, say, 20,000, we can cut it up with code like this."}, {"start": 181.0, "end": 190.0, "text": "So the training sentences will be the first 20,000 sliced by this syntax, and the testing sentences will be the remaining slices like this."}, {"start": 190.0, "end": 194.0, "text": "And we can do the same for the labels to get a training and a test set."}, {"start": 194.0, "end": 201.0, "text": "But there's a bit of a problem. Remember earlier, we used the tokenizer to create a word index of every word in the set?"}, {"start": 201.0, "end": 211.0, "text": "That was all very good, but if we really want to test its effectiveness, we have to ensure that the neural net only sees the training data and that it never sees the test data."}, {"start": 211.0, "end": 217.0, "text": "So we have to rewrite our code to ensure that the tokenizer is just fit to the training data."}, {"start": 217.0, "end": 220.0, "text": "Let's take a look at how to do that now."}, {"start": 220.0, "end": 224.0, "text": "Here's the new code to create our training and test sets. Let's look at it line by line."}, {"start": 225.0, "end": 236.0, "text": "We'll first instantiate a tokenizer like before, but now we'll fit the tokenizer on just the training sentences that we split out earlier instead of the entire corpus."}, {"start": 236.0, "end": 248.0, "text": "And now instead of one overall set of sequences, we can now create a set of training sequences and pad them, and then do exactly the same thing for the test sequences is really that easy."}, {"start": 248.0, "end": 258.0, "text": "But you might be wondering at this point, we've turned our sentences into numbers with the numbers being tokens representing words, but how do we get meaning from that?"}, {"start": 258.0, "end": 266.0, "text": "How do we determine if something is sarcastic just from the numbers? Well, here's where the context of embeddings come in."}, {"start": 266.0, "end": 277.0, "text": "Let's consider the most basic of sentiments. Something is good, or something is bad. We often see these as being opposites, so we can plot them as having opposite directions like this."}, {"start": 277.0, "end": 287.0, "text": "So then what happens with a word like meh? It's not particularly good, and it's not particularly bad, probably a little more bad than good, so you might plot it a bit like this."}, {"start": 287.0, "end": 296.0, "text": "Or the phrase not bad, which is usually meant to plot something as having a little bit of goodness, but not necessarily very good, so it might look like this."}, {"start": 296.0, "end": 305.0, "text": "Now if we plot this on an x and y axis, we can start to determine the good or bad sentiment as coordinates in the x and y."}, {"start": 305.0, "end": 317.0, "text": "Good is 1,0, meh is minus 0.4, 0.7, etc. By looking at the direction of the vector, we can start to determine the meaning of the word."}, {"start": 317.0, "end": 329.0, "text": "So what if you extend that into multiple dimensions instead of just two? What if words that are labeled with sentiment like sarcastic and not sarcastic are plotted in these multiple dimensions?"}, {"start": 329.0, "end": 336.0, "text": "And then as we train, we try to learn what the direction in these multi-dimensional spaces should look like."}, {"start": 336.0, "end": 345.0, "text": "Words that only appear in the sarcastic sentences will have a strong component in the sarcastic direction, and others will have one in the not sarcastic direction."}, {"start": 345.0, "end": 350.0, "text": "As we load more and more sentences into the network for training, these directions can change."}, {"start": 350.0, "end": 360.0, "text": "And when we have a fully trained network and give it a set of words, it could look up the vectors for these words, sum them up and thus give us an idea for the sentiment."}, {"start": 360.0, "end": 363.0, "text": "This concept is known as embedding."}, {"start": 363.0, "end": 370.0, "text": "So going back to this diagram, consider what would have happened if I said something was not bad a bit meh."}, {"start": 370.0, "end": 381.0, "text": "If we were to sum up the vectors, we'd have something that's 0.7 on y and 0.1 on x, so its sentiment could be considered slightly on the good side of neutral."}, {"start": 381.0, "end": 391.0, "text": "So now let's take a look at coding this. Here's my neural network code. The top layer is an embedding where the direction of each word will be learned epoch by epoch."}, {"start": 391.0, "end": 398.0, "text": "After that, we pull with a global average pooling, namely adding up the vectors as I demonstrated earlier."}, {"start": 398.0, "end": 411.0, "text": "This is then fed into a common or garden deep neural network. Training is now as simple as model.fit using the training data and labels and specifying the testing padded and labels for the validation data."}, {"start": 411.0, "end": 418.0, "text": "At this URL, you can try it out for yourself. And here you can see the results that I got training it for just 30 epochs."}, {"start": 418.0, "end": 431.0, "text": "While it was able to fit the training data to 99% accuracy, more importantly with the test data, that is words that the network has never seen, it still got 81% to 82% accuracy, which is pretty good."}, {"start": 431.0, "end": 437.0, "text": "So how do we use this to establish sentiment for new sentences? Here's the code."}, {"start": 437.0, "end": 445.0, "text": "Let's create a couple of sentences that we want to classify. The first one looks a little bit sarcastic and the second one's quite plain and boring."}, {"start": 446.0, "end": 454.0, "text": "We'll use the tokenizer that we created earlier to convert them into sequences. This way the words will have the same tokens as the training set."}, {"start": 454.0, "end": 463.0, "text": "We'll then pad those sequences to be the same dimensions as those in the training set and use the same padding type. And we can then predict on the padded set."}, {"start": 463.0, "end": 472.0, "text": "The results are like this. The first sentence gives me .91, which is very close to 1, indicating that there's a very high probability of sarcasm."}, {"start": 472.0, "end": 485.0, "text": "The second is 5 times 10 to the minus 6, indicating an extremely low chance of sarcasm. It does seem to be working. All of this code is runnable in a colab at this URL, so give it a try for yourself."}, {"start": 485.0, "end": 495.0, "text": "You've now built your first text classification model to understand sentiment in text. Give it a try for yourself and let us know what kind of classifiers you built."}, {"start": 495.0, "end": 504.0, "text": "I hope you've enjoyed this short series and there's more on the way. So don't forget to hit that subscribe button and get the latest and greatest in AI videos right here on the TensorFlow Channel."}], "language": "en"}
{"title": "Training a model to recognize sentiment in text (NLP Zero to Hero - Part 3)", "video_id": "Y_hzMnRXjhI", "text": "Hi, and welcome back to this series on Zero to Hero with TensorFlow, where we're looking at natural language processing. In the last couple of episodes, you saw how to tokenize text into numeric values and how to use tools and TensorFlow to regularize and pad that text. Now that we've gotten the pre-processing out of the way, we can next look at how to build a classifier to recognize sentiment in text. We'll start by using a data set of headlines, where the headline has been categorized as sarcastic or not. We'll train a classifier on this, and it can then tell us afterwards if a new piece of text looks like it might be sarcastic. We'll use Rishav Mizra's data set from Kaggle, and you can find details on it here. The data is nice and simple. The is sarcastic field is one, if it's sarky, and zero otherwise. There's a headline, where the text will train on, and then there's a URL to the article, if you're interested in reading it. But we're not going to use this, just the headline text. The data is stored in JSON format like this, pretty straightforward. We'll have to convert it to Python format for training, so it will look like this. Every JSON element becomes a Python list element, and it's all encapsulated in square brackets. Python has a JSON toolkit that can achieve this, and here's the complete code. We'll go through it step by step. First of all, we'll import the JSON library. Then, we can load in the sarcasm JSON file using the JSON library. We can then create lists for the labels, headlines, and article URLs. And when we iterate through the JSON, we can load the requisite values into our Python list. Now that we have three lists, one with our labels, one with the text, and one with the URLs, we can start doing our familiar pre-processing on the text. Here's the code. By calling tokenizer.fit on text with the headline, we'll create tokens for every word in the corpus, and then we'll see them in the word index. You can see an example of some of the words here, so Underwood has been tokenized to 24127 and Schilling's ball, what is that anyway, to 23055. So now, we can turn our sentences into sequences of tokens, and pad them to the same length with this code. If we want to inspect them, we can simply print them out. Here you can see one tokenized sentence and the shape of the entire corpus. That's 26,709 sequences each with 40 tokens. Now there's a problem here. We don't have a split in the data for training and testing. We just have a list of 26,709 sequences. Fortunately, Python makes it super easy for us to slice this up. Let's take a look at that next. So, we have a bunch of sentences in a list, and a bunch of labels in a list. To slice them into training and test sets is actually pretty easy. If we pick a training size, say, 20,000, we can cut it up with code like this. So the training sentences will be the first 20,000 sliced by this syntax, and the testing sentences will be the remaining sliced like this. And we can do the same for the labels to get a training and a test set. But there's a bit of a problem. Remember earlier, we used the tokenizer to create a word index of every word in the set? That was all very good, but if we really want to test its effectiveness, we have to ensure that the neural net only sees the training data and that it never sees the test data. So we have to rewrite our code to ensure that the tokenizer is just fit to the training data. Let's take a look at how to do that now. Here's the new code to create our training and test sets. Let's look at it line by line. We'll first instantiate a tokenizer like before, but now we'll fit the tokenizer on just the training sentences that we split out earlier instead of the entire corpus. And now instead of one overall set of sequences, we can now create a set of training sequences and pad them. And then do exactly the same thing for the test sequences. It's really that easy. But you might be wondering at this point, we've turned our sentences into numbers with the numbers being tokens representing words. But how do we get meaning from that? How do we determine if something is sarcastic just from the numbers? Well here's where the context of embeddings come in. Let's consider the most basic of sentiments. Something is good or something is bad. We often see these as being opposites so we can plot them as having opposite directions like this. So then what happens with a word like meh? It's not particularly good and it's not particularly bad, probably a little more bad than good. So you might plot it a bit like this. Or the phrase not bad, which is usually meant to plot something as having a little bit of goodness but not necessarily very good. So it might look like this. Now if we plot this on an x and y axis, we can start to determine the good or bad sentiment as coordinates in the x and y. Good is 1,0, meh is minus 0.4, 0.7, etc. By looking at the direction of the vector, we can start to determine the meaning of the word. So what if you extend that into multiple dimensions instead of just two? What if words that are labeled with sentiment like sarcastic and not sarcastic are plotted in these multiple dimensions? And then as we train, we try to learn what the direction in these multi-dimensional spaces should look like. Words that only appear in the sarcastic sentences will have a strong component in the sarcastic direction and others will have one in the not sarcastic direction. As we load more and more sentences into the network for training, these directions can change. And when we have a fully trained network and give it a set of words, it could look up the vectors for these words, sum them up, and thus give us an idea for the sentiment. This concept is known as embedding. So going back to this diagram, consider what would have happened if I said something was not bad a bit meh. If we were to sum up the vectors, we'd have something that's 0.7 on y and 0.1 on x. So its sentiment could be considered slightly on the good side of neutral. So now let's take a look at coding this. Here's my neural network code. The top layer is an embedding where the direction of each word will be learned epoch by epoch. After that, we pool with a global average pooling, namely adding up the vectors as I demonstrated earlier. This is then fed into a common or garden deep neural network. Training is now as simple as model.fit using the training data and labels and specifying the testing padded and labels for the validation data. At this URL, you can try it out for yourself. And here you can see the results that I got training it for just 30 epochs. While it was able to fit the training data to 99% accuracy, more importantly with the test data, that is words that the network has never seen, it's still got 81 to 82% accuracy, which is pretty good. So how do we use this to establish sentiment for new sentences? Here's the code. Let's create a couple of sentences that we want to classify. The first one looks a little bit sarcastic, and the second one's quite plain and boring. We'll use the tokenizer that we created earlier to convert them into sequences. This way the words will have the same tokens as the training set. We'll then pad those sequences to be the same dimensions as those in the training set, and use the same padding type. And we can then predict on the padded set. The results are like this. The first sentence gives me .91, which is very close to 1, indicating that there's a very high probability of sarcasm. The second is 5 times 10 to the minus 6, indicating an extremely low chance of sarcasm. It does seem to be working. All of this code is runnable in a collab at this URL, so give it a try for yourself. You've now built your first text classification model to understand sentiment in text. Give it a try for yourself, and let us know what kind of classifiers you build. I hope you've enjoyed this short series, and there's more on the way. So don't forget to hit that subscribe button, and get the latest and greatest in AI videos right here on the TensorFlow Channel.", "segments": [{"start": 0.0, "end": 10.0, "text": "Hi, and welcome back to this series on Zero to Hero with TensorFlow, where we're looking at natural language processing."}, {"start": 10.0, "end": 19.0, "text": "In the last couple of episodes, you saw how to tokenize text into numeric values and how to use tools and TensorFlow to regularize and pad that text."}, {"start": 19.0, "end": 27.0, "text": "Now that we've gotten the pre-processing out of the way, we can next look at how to build a classifier to recognize sentiment in text."}, {"start": 27.0, "end": 33.0, "text": "We'll start by using a data set of headlines, where the headline has been categorized as sarcastic or not."}, {"start": 33.0, "end": 41.0, "text": "We'll train a classifier on this, and it can then tell us afterwards if a new piece of text looks like it might be sarcastic."}, {"start": 41.0, "end": 46.0, "text": "We'll use Rishav Mizra's data set from Kaggle, and you can find details on it here."}, {"start": 46.0, "end": 52.0, "text": "The data is nice and simple. The is sarcastic field is one, if it's sarky, and zero otherwise."}, {"start": 52.0, "end": 58.0, "text": "There's a headline, where the text will train on, and then there's a URL to the article, if you're interested in reading it."}, {"start": 58.0, "end": 61.0, "text": "But we're not going to use this, just the headline text."}, {"start": 61.0, "end": 70.0, "text": "The data is stored in JSON format like this, pretty straightforward. We'll have to convert it to Python format for training, so it will look like this."}, {"start": 70.0, "end": 76.0, "text": "Every JSON element becomes a Python list element, and it's all encapsulated in square brackets."}, {"start": 76.0, "end": 83.0, "text": "Python has a JSON toolkit that can achieve this, and here's the complete code. We'll go through it step by step."}, {"start": 83.0, "end": 90.0, "text": "First of all, we'll import the JSON library. Then, we can load in the sarcasm JSON file using the JSON library."}, {"start": 90.0, "end": 95.0, "text": "We can then create lists for the labels, headlines, and article URLs."}, {"start": 95.0, "end": 101.0, "text": "And when we iterate through the JSON, we can load the requisite values into our Python list."}, {"start": 101.0, "end": 109.0, "text": "Now that we have three lists, one with our labels, one with the text, and one with the URLs, we can start doing our familiar pre-processing on the text."}, {"start": 109.0, "end": 111.0, "text": "Here's the code."}, {"start": 111.0, "end": 120.0, "text": "By calling tokenizer.fit on text with the headline, we'll create tokens for every word in the corpus, and then we'll see them in the word index."}, {"start": 120.0, "end": 131.0, "text": "You can see an example of some of the words here, so Underwood has been tokenized to 24127 and Schilling's ball, what is that anyway, to 23055."}, {"start": 131.0, "end": 139.0, "text": "So now, we can turn our sentences into sequences of tokens, and pad them to the same length with this code."}, {"start": 139.0, "end": 143.0, "text": "If we want to inspect them, we can simply print them out."}, {"start": 143.0, "end": 153.0, "text": "Here you can see one tokenized sentence and the shape of the entire corpus. That's 26,709 sequences each with 40 tokens."}, {"start": 153.0, "end": 161.0, "text": "Now there's a problem here. We don't have a split in the data for training and testing. We just have a list of 26,709 sequences."}, {"start": 161.0, "end": 168.0, "text": "Fortunately, Python makes it super easy for us to slice this up. Let's take a look at that next."}, {"start": 168.0, "end": 173.0, "text": "So, we have a bunch of sentences in a list, and a bunch of labels in a list."}, {"start": 173.0, "end": 182.0, "text": "To slice them into training and test sets is actually pretty easy. If we pick a training size, say, 20,000, we can cut it up with code like this."}, {"start": 182.0, "end": 191.0, "text": "So the training sentences will be the first 20,000 sliced by this syntax, and the testing sentences will be the remaining sliced like this."}, {"start": 191.0, "end": 195.0, "text": "And we can do the same for the labels to get a training and a test set."}, {"start": 195.0, "end": 201.0, "text": "But there's a bit of a problem. Remember earlier, we used the tokenizer to create a word index of every word in the set?"}, {"start": 201.0, "end": 212.0, "text": "That was all very good, but if we really want to test its effectiveness, we have to ensure that the neural net only sees the training data and that it never sees the test data."}, {"start": 212.0, "end": 218.0, "text": "So we have to rewrite our code to ensure that the tokenizer is just fit to the training data."}, {"start": 218.0, "end": 220.0, "text": "Let's take a look at how to do that now."}, {"start": 220.0, "end": 225.0, "text": "Here's the new code to create our training and test sets. Let's look at it line by line."}, {"start": 225.0, "end": 236.0, "text": "We'll first instantiate a tokenizer like before, but now we'll fit the tokenizer on just the training sentences that we split out earlier instead of the entire corpus."}, {"start": 236.0, "end": 243.0, "text": "And now instead of one overall set of sequences, we can now create a set of training sequences and pad them."}, {"start": 243.0, "end": 248.0, "text": "And then do exactly the same thing for the test sequences. It's really that easy."}, {"start": 248.0, "end": 256.0, "text": "But you might be wondering at this point, we've turned our sentences into numbers with the numbers being tokens representing words."}, {"start": 256.0, "end": 263.0, "text": "But how do we get meaning from that? How do we determine if something is sarcastic just from the numbers?"}, {"start": 263.0, "end": 266.0, "text": "Well here's where the context of embeddings come in."}, {"start": 266.0, "end": 271.0, "text": "Let's consider the most basic of sentiments. Something is good or something is bad."}, {"start": 271.0, "end": 277.0, "text": "We often see these as being opposites so we can plot them as having opposite directions like this."}, {"start": 277.0, "end": 284.0, "text": "So then what happens with a word like meh? It's not particularly good and it's not particularly bad, probably a little more bad than good."}, {"start": 284.0, "end": 287.0, "text": "So you might plot it a bit like this."}, {"start": 287.0, "end": 294.0, "text": "Or the phrase not bad, which is usually meant to plot something as having a little bit of goodness but not necessarily very good."}, {"start": 294.0, "end": 305.0, "text": "So it might look like this. Now if we plot this on an x and y axis, we can start to determine the good or bad sentiment as coordinates in the x and y."}, {"start": 305.0, "end": 317.0, "text": "Good is 1,0, meh is minus 0.4, 0.7, etc. By looking at the direction of the vector, we can start to determine the meaning of the word."}, {"start": 317.0, "end": 321.0, "text": "So what if you extend that into multiple dimensions instead of just two?"}, {"start": 321.0, "end": 329.0, "text": "What if words that are labeled with sentiment like sarcastic and not sarcastic are plotted in these multiple dimensions?"}, {"start": 329.0, "end": 336.0, "text": "And then as we train, we try to learn what the direction in these multi-dimensional spaces should look like."}, {"start": 336.0, "end": 345.0, "text": "Words that only appear in the sarcastic sentences will have a strong component in the sarcastic direction and others will have one in the not sarcastic direction."}, {"start": 345.0, "end": 350.0, "text": "As we load more and more sentences into the network for training, these directions can change."}, {"start": 350.0, "end": 360.0, "text": "And when we have a fully trained network and give it a set of words, it could look up the vectors for these words, sum them up, and thus give us an idea for the sentiment."}, {"start": 360.0, "end": 363.0, "text": "This concept is known as embedding."}, {"start": 363.0, "end": 370.0, "text": "So going back to this diagram, consider what would have happened if I said something was not bad a bit meh."}, {"start": 370.0, "end": 376.0, "text": "If we were to sum up the vectors, we'd have something that's 0.7 on y and 0.1 on x."}, {"start": 376.0, "end": 381.0, "text": "So its sentiment could be considered slightly on the good side of neutral."}, {"start": 381.0, "end": 386.0, "text": "So now let's take a look at coding this. Here's my neural network code."}, {"start": 386.0, "end": 392.0, "text": "The top layer is an embedding where the direction of each word will be learned epoch by epoch."}, {"start": 392.0, "end": 398.0, "text": "After that, we pool with a global average pooling, namely adding up the vectors as I demonstrated earlier."}, {"start": 398.0, "end": 402.0, "text": "This is then fed into a common or garden deep neural network."}, {"start": 402.0, "end": 411.0, "text": "Training is now as simple as model.fit using the training data and labels and specifying the testing padded and labels for the validation data."}, {"start": 411.0, "end": 418.0, "text": "At this URL, you can try it out for yourself. And here you can see the results that I got training it for just 30 epochs."}, {"start": 418.0, "end": 425.0, "text": "While it was able to fit the training data to 99% accuracy, more importantly with the test data,"}, {"start": 425.0, "end": 432.0, "text": "that is words that the network has never seen, it's still got 81 to 82% accuracy, which is pretty good."}, {"start": 432.0, "end": 437.0, "text": "So how do we use this to establish sentiment for new sentences? Here's the code."}, {"start": 437.0, "end": 441.0, "text": "Let's create a couple of sentences that we want to classify."}, {"start": 441.0, "end": 445.0, "text": "The first one looks a little bit sarcastic, and the second one's quite plain and boring."}, {"start": 445.0, "end": 450.0, "text": "We'll use the tokenizer that we created earlier to convert them into sequences."}, {"start": 450.0, "end": 454.0, "text": "This way the words will have the same tokens as the training set."}, {"start": 454.0, "end": 460.0, "text": "We'll then pad those sequences to be the same dimensions as those in the training set, and use the same padding type."}, {"start": 460.0, "end": 463.0, "text": "And we can then predict on the padded set."}, {"start": 463.0, "end": 472.0, "text": "The results are like this. The first sentence gives me .91, which is very close to 1, indicating that there's a very high probability of sarcasm."}, {"start": 472.0, "end": 480.0, "text": "The second is 5 times 10 to the minus 6, indicating an extremely low chance of sarcasm. It does seem to be working."}, {"start": 480.0, "end": 485.0, "text": "All of this code is runnable in a collab at this URL, so give it a try for yourself."}, {"start": 485.0, "end": 491.0, "text": "You've now built your first text classification model to understand sentiment in text."}, {"start": 491.0, "end": 495.0, "text": "Give it a try for yourself, and let us know what kind of classifiers you build."}, {"start": 495.0, "end": 498.0, "text": "I hope you've enjoyed this short series, and there's more on the way."}, {"start": 498.0, "end": 504.0, "text": "So don't forget to hit that subscribe button, and get the latest and greatest in AI videos right here on the TensorFlow Channel."}], "language": "en"}
