arX1v:2305.09281vl [cs.CL] 16 May 2023

On the Origins of Bias in NLP through the Lens of the Jim Code

Fatma Elsafoury! and Gavin Abercrombie”

' school of Physics, Engineering, and Computing, the University of The West of Scotland, UK
HUW NLP lab, Heriot-Watt University, UK

Abstract

In this paper, we trace the biases in current nat-
ural language processing (NLP) models back
to their origins in racism, sexism, and homo-
phobia over the last 500 years. We review lit-
erature from critical race theory, gender stud-
ies, data ethics, and digital humanities studies,
and summarize the origins of bias in NLP mod-
els from these social science perspective. We
show how the causes of the biases in the NLP
pipeline are rooted in social issues. Finally,
we argue that the only way to fix the bias and
unfairness in NLP is by addressing the social
problems that caused them in the first place
and by incorporating social sciences and so-
cial scientists in efforts to mitigate bias in NLP
models. We provide actionable recommenda-
tions for the NLP research community to do
So.

1 Introduction

In Race After Technology, Benjamin (2019) coins
the term “The New Jim Code’, which she describes
as:

“The employment of new technologies
that reflect and reproduce existing in-
equities but that are promoted and per-
ceived as more objective or progressive
than discriminatory systems of a previ-
ous era.”

While the Jim Code is a spin on, “Jim Crow”,
a derogatory epithet for African-Americans, the
same concept can be generalized to the bias and
unfairness in artificial intelligence (AI) systems
against all marginalised groups. It is crucial to
study bias and fairness in machine learning (ML)
and natural language processing (NLP) models to
understand how existing social biases and stereo-
typing are being encoded in the data used to train
them, as well as to compare (1) the fairness of
the decisions made by NLP models due to biases

in the datasets, with (2) biased choices made by
the developers of those models as a result of unin-
tended bias or to maximize profit. Studying bias
and unfairness in NLP models is one way to pierce
a hole in the black box and shed a little light on the
limitations of widely used models. However, it is
not possible to understand the roots of algorithmic
bias, without incorporating relevant studies from
social sciences, critical race theory, gender studies,
LGBTQ studies, and digital humanities studies, as
recommended by Benjamin (2019).

In this paper, we study the various origins of
bias in NLP models from two perspectives: (1) the
NLP pipeline perspective, where we review the
sources of bias in models from the NLP literature;
and (2) the Jim Code perspective, in which we re-
view the origins of bias from the literature on social
science, critical race theory, gender, LGBTQ, and
digital studies. We argue that, in fact, the sources
of bias found in the NLP pipeline are rooted in
those uncovered in the social sciences. Then, we
discuss how the lack of inclusion of social sciences
in attempts at eliminating social issues like bias in
NLP models has resulted in problematic quantita-
tive measures of bias (Blodgett et al., 2021) and
superficial mitigation techniques (Gonen and Gold-
berg, 2019). Finally, we propose recommendations
to NLP researchers to mitigate biases and improve
the fairness of the models that they develop by ad-
dressing their underlying social causes.

2 Background: History of discrimination

In Western societies, the biases and inequalities to-
wards marginalised groups based on ethnicity, sex,
class, religion, sexual orientation, age, or disabil-
ity that we see today are direct results of centuries
of racism, sexism, and homophobia, as has been
discussed by many scholars.

In The Myth of Race: The Troubling Persistence
of an Unscientific Idea, Sussman (2014a) reviews
the history of 500 years of racism in Western Eu-


rope to answer the question of why the invalid
concept of race still prevails. He argues that the
ideology of race developed from multiple histori-
cal events and movements ranging from the Span-
ish Inquisition to social Darwinism, eugenics, and
modern IQ tests, starting as early as the fifteenth
century, when the Catholic Church in Spain perse-
cuted the Jewish population for “impurity of blood”
(Sussman, 2014a).

He goes on to explain that some Enlightenment
scholars like David Hume and Immanuel Kant be-
lieved that, based on skin colours, there are more
than one race of humans, and that white men are
the most civilized people (Sussman, 2014b). In the
nineteenth century, drawing from evolution theory,
social Darwinists like Herbert Spencer argued that
helping the poor and the weak was an interference
with natural selection, coining the term “survival of
the fittest”. This led to sterilization and ultimately
the extermination camps of the eugenics movement
(Sussman, 2014b).

Moving to the 1970s, Sussman (2014c) shows
that Arthur Jensen, a professor of Educational Psy-
chology at the University of California, argued that
Black people are intellectually inferior to white peo-
ple. This argument was reasserted in the 1990s with
the publication of Richard Herrnstein and Charles
Murray’s The Bell Curve.

Sussman (2014d) goes on to show that in the
2000s, racism took on a disguise of “Culturism’’, as
coined by the anthropologist Franz-Boas to explain
the difference in human behaviour and social orga-
nizations. Culturism paved the way to modern-day
anti-immigration agendas with immigrants, like
Arabs or Muslims, not claimed to be genetically
inferior to Europeans, but to have a cultural burden
that prevents them from integrating in the West.

Homophobia is intertwined with racism, as ar-
gued by Morris (2010) in their research on the his-
tory of the LGBTQ community social movement.
Morris explains that homosexuality and transgen-
der identity were accepted in many ancient soci-
eties like those of ancient Greece, Native Ameri-
cans, North Africa, and the Pacific Islands. These
accepting cultures oppose the Western culture of
heterosexuality and binary genders, who regarded
homosexuality and transgender as foreign, savage,
and evidence of inferior races. When Europeans
started colonization campaigns, they imposed their
moral codes and persecuted LGBTQ communities.
The first known case of punishing homosexuality

by death was in North America in 1566. Later, in
the era of sexology studies in 1882 and 1897, Euro-
pean doctors and scientists labelled homosexuality
as degenerate and abnormal, and as recently as the
1980s and 1990s, AIDS was widely rationalised as
being god’s punishment for gay people.

As argued by Criado Perez (2019) in Invisible
Women: Data Bias in a World Designed for Men,
Sexism can be tracked back to the fourth century
B.C. when Aristotle articulated that the male form
is the default form as an inarguable fact. This
concept still persists today, as we can see in the
one-size-fits-men approach to designing suppos-
edly gender-neutral products like piano keyboards
and smartphones. Overall, as Manne (2017) de-
scribes it in Down Girl: The Logic of Misogyny,
sexism consists of “assumptions, beliefs, theories,
stereotypes, and broader cultural narratives that
... make rational people more inclined to support
and participate in patriarchal social arrangements”.

Marsginalization has been studied in social sci-
ences by many scholars in critical race theory
(Benjamin, 2019), gender studies (McIntosh, 2001;
Davis, 1982), and LGBTQ studies (Fausto-Sterling,
2008). However, negative stereotyping, stigma, and
unintended bias continue against marginalised peo-
ple based on ethnicity, religion, disability, sexual
orientation, or gender. These stigmas and unin-
tended bias have led to different forms of discrim-
ination from education, job opportunities, health
case, housing, incarceration, and others, as Nordell
(2021) details in The End of Bias.

They can also have negative impact on the
cognitive ability, and mental and physical health
of the people who carry their load. As Steele
(2011) shows in Whistling Vivaldi: How Stereo-
types Affect Us and What We Can Do, based
on experiments in behavioural psychology, carry-
ing stigma made women underperform in maths
tests, and African-American students underperform
in academia. Hence, stereotypes become self-
fulfilling prophecies, eventually leading to their
perpetuation and the continuation of the prejudice
and discrimination.

In the age of knowledge, computing, and big
data, prejudice and discrimination have found
their way to machine learning models. These mod-
els that are now dictating every aspect of our lives
from online advertising, to employment and judi-
cial systems that rely on black box models and
discriminate against marginalised groups, while


benefitting privileged elites, as O’ Neil (2017) ex-
plains in Weapons of Maths Destruction. One of
the most well-known examples of discriminative
decisions made by a machine learning models is
the COMPAS algorithm, a risk assessment tool that
measures the likelihood that a criminal becomes a
recidivist, a term used in legal systems to describe
a criminal who reoffends. Despite Northpoint, the
company that produced the COMPAS tool not shar-
ing how the model measures the recidivism scores,
the algorithm was deployed by the state of New
York in 2010. In 2016, ProPublica found that Black
defendants are more likely than white defendants
to be incorrectly judged to be at a higher risk of
recidivism while the latter were more likely than
Black defendants to be incorrectly flagged as low
risk (Larson et al., 2016).

One example of algorithmic gender discrimi-
nation is the CV screening model used by Ama-
zon, which, according to a Reuters report in 2018,
favoured CVs of male over female candidates
even when both had the same skills and qualifi-
cations (Dastin, 2018). Similar examples of algo-
rithmic discrimination can be found against the
LGBTQ community (Tomasev et al., 2021), older
people (Stypinska, 2022), Muslims (Samuel, 2021),
and people with disabilities (Binns and Kirkham,
2021).

3. Bias and fairness: Definitions

The term bias is defined and used in many ways
(Olteanu et al., 2019). The normative definition
of bias, in cognitive science is: “behaving accord-
ing to some cognitive priors and presumed reali-
ties that might not be true at all” (Garrido-Mufioz
et al., 2021). The statistical definition of bias is
“systematic distortion in the sampled data that com-
promises its representatives” (Olteanu et al., 2019).

In NLP, while bias and fairness have been de-
scribed in several ways, the statistical definition is
most dominant (Elsafoury et al., 2022a; Caliskan
et al., 2017; Garg et al., 2018; Nangia et al., 2020;
Nadeem et al., 2021). In the last two years or so,
there has been a trend to distinguish two types of
bias in NLP systems: intrinsic bias and extrinsic
bias (Cao et al., 2022; Kaneko et al., 2022; Steed
et al., 2022). Intrinsic bias is used to describe the
biased representations of pre-trained models. As
far as we know, there is no formal definition of
intrinsic bias in the literature. However, from the
research done to study bias in word embeddings

(Elsafoury et al., 2022a), we can infer the following
definition: Intrinsic bias is stereotypical represen-
tations of certain groups of people learned during
pre-training. For example, when a model asso-
ciates women with certain jobs like caregivers and
men with doctors (Caliskan et al., 2017). This type
of bias exists in both static (Caliskan et al., 2017;
Garg et al., 2018) and contextual word embeddings
(Nangia et al., 2020; Nadeem et al., 2021).

On the other hand, Extrinsic bias, also known as
model fairness, has many formal definitions built
on those from literature on the fairness of exam
testing from the 1960s, 70s and 80s (Hutchinson
and Mitchell, 2019). The most recent fairness def-
initions are broadly categorized into two groups:
Individual fairness, which is defined as “An algo-
rithm is fair if it gives similar predictions to similar
individuals” (Kusner et al., 2017).

For a given model Y : X > Y with features
X, sensitive attributes A, prediction Y, and two
individuals 7 and 7, and if individuals i and j are
similar. The model achieves individual fairness if

Y(X', A’) © Y(X!, A?) (1)

The second type of fairness definition is Group
fairness, which can be defined as “An algorithm
is fair if the model prediction Y and sensitive at-
tribute A are independent” (Caton and Haas, 2020;
Kusner et al., 2017). Based on group fairness, the
model is fair if

Y(X|A=0) =Y(X|A=1) (2)

Group fairness is the most common definition
used in NLP. There are different ways to mea-
sure it, like equality of odds (Baldini et al., 2022).
However, other metrics have been proposed in the
NLP literature to measure individual fairness like
counterfactual fairness methods (Prabhakaran et al.,
2019).

4 Origins of bias

While much literature proposes methods to mea-
sure bias and fairness in NLP models, there are far
fewer papers that discuss their origins. Those that
do so tend to neglect literature from social science
or the critical race theory that has examined topics
directly related to bias like racism, sexism, or ho-
mophobia. This short-sightedness has, so far, led to
cosmetic changes in the proposed NLP models to
fix the problem of bias rather than fixing the racist,
sexist, homophobic status quo (Benjamin, 2019).


NLP pipeline
Sources of
Bias

oO Jim Code
Origins
of bias

k of tivi
accountability Beck ot creativity

Lack of context

Research Design

Bg

Overampflication

The nurse came to the room, ...she.... is nice.
The doctor came to the room, ...he.. Is nice.

BE Oe

Lack of public

Lack of diversity ‘Awareness

Figure |: The origins of bias in supervised NLP models

In this section, we review the different origins of
bias in NLP systems from the Jim Code perspective
of social science, using tools like critical race the-
ory, digital studies, gender studies, LGBTQ studies,
and internet and data activism. Then we review the
sources of bias from a purely NLP perspective,
while trying to connect these two strands to gain
a more profound understanding of the origins of
bias. Figure 1 shows an overview summary of the
origins of bias from the different perspectives, and
how the biases in the NLP pipeline originate in the
Jim Code.

4.1 The Jim Code perspective

As previously described, Jim Code is a term that
refers to the new forms of systematic discrimina-
tion found in new technologies that build on older
discriminatory systems. This is one of the main
origins of bias and unfairness that we find in most
NLP systems. This can be broken down into the
following sources of bias:

1. Lack of context: In More than a Glitch, Brous-
sard (2023) explains that, like computers, the data
used to train NLP and ML models are produced
without a specific human context. A similar point
is made by Benjamin (2019), who discusses how
social and historical contexts are not taken into
consideration when data is collected to train NLP
models. But it is not only the data. With the NLP
models being developed in isolation from social
science perspectives, how these systems impact the
people of different identity groups gets overlooked.
For example, models output decisions on who is
eligible to get a loan or get a job without consider-
ation of the fact that this might increase the wealth

gap between marginalised and privileged groups.

Moreover, it is because of the lack of context that
researchers in NLP do not think about the harmful
ways that their proposed systems could be used.
For example, when models are used to detect race
from last names and zip codes, their developers
have probably failed to consider how they will be
employed by certain businesses to illegally collect
information on ethnicity (Benjamin, 2019). Even
greater harm is caused when a model categorises
people as criminals or terrorists due to their inferred
ethnicity.

2. Lack of creativity: Because of the lack of con-
text, many developers of ML and NLP models tend
to build their systems on top of existing racist, sex-
ist, homophobic, ageist, and ableist systems. An
example is when recommender systems used “cul-
tural segregation” to infer information about a per-
son’s ethnicity to personalise their recommenda-
tions, using ethnicity as a proxy for individuality
(Benjamin, 2019). Hence, those systems perpetuate
the racist view that people who belong to a specific
group must have similar preferences. Researchers
need to be more creative and find other ways to
recommend content that do not rely on social bias
shortcuts.

3. Lack of accountability: There is a lack of ac-
countability that allows tech companies to maxi-
mize profits and get away with creating oppressive
systems that are not just “glitches” as explained
by critical race and digital humanities studies ac-
tivists (Broussard, 2023; Nobel, 2018; Benjamin,
2019). A lack of accountability enables companies
to sell their systems as black boxes without ex-


plaining how their models make decisions (O’ Neil,
2017). We also see that in the scientific community,
where big tech companies publish papers emphasis-
ing their models’ excellent results without sharing
those models or the data that were used to train
them, precluding reproducibility.

Moreover, when, the Justice League, a group of
Al ethicists and activists, launched the Safe Face
pledge to ensure that computer vision models don’t
discriminate between people based on their skin
colour, no major tech company was willing to sign
it (Benjamin, 2019). With the lack of accountability
and legislation, big tech companies, which are one
of the main drivers of the field, have no reason to
revise and change the way they build their ML and
NLP systems, or to include the social and historical
context into their research in a way that profoundly
changes the systems instead of just covering it up
and fixing the “glitches”.

4. Lack of diversity: The majority of ML and
NLP technologies are developed in companies or
research institutes in Western societies and by re-
searchers who are mostly white, able-bodied, het-
erosexual men. They develop and test systems that
work well for them, without considering how func-
tional these systems are for people from different
backgrounds. Examples are facial recognition sys-
tems that only work with people with light skin
(Benjamin, 2019; Broussard, 2023) and CV recom-
mendation systems that favour applicants with male
names (Dastin, 2018). There is also a lack of diver-
sity when it comes to the targeted customers of the
systems. Since most of these technologies are ex-
pensive to buy, the developers of these systems fo-
cus on the customers who can afford it and who are
also predominantly white, able-bodied, heterosex-
ual men (Benjamin, 2019). This lack of diversity,
in addition to the lack of social and historical con-
texts, leads to the development of discriminatory
systems.

5. Lack of public awareness: In addition to the
previously discussed origins of bias in NLP, another
factor that allows the biases to spread is the lack of
public awareness. This is a result of using mathe-
matical and statistical terminology and jargon that
most non-specialists can’t understand. This lack of
understanding of how ML and NLP models work
and their limitations led people to over-trust AI sys-
tems and leads to ““Technochauvinism’, described
by Broussard (2023) as:

“the kind of bias that considers compu-
tational solutions to be superior to all
other solutions. Embedded in this bias
is a priori assumption that computers
are better than humans which is actually
a Claim that the people who make and
program computers are better than other
humans.”

The lack of public awareness and Technochauvin-
ism are the reasons why banks, schools, hospitals,
universities, and other institutions that are supposed
to deal with people and society and make social
decisions adopting NLP systems that are poorly
understood, with the false notion that they are unbi-
ased, and their decisions are faultless and objective
(Benjamin, 2019; Broussard, 2023).

4.2 The NLP pipeline perspective

We now turn to the sources of bias in the NLP
pipeline described in the literature. Shah et al.
(2020) introduce four sources of bias in the NLP
pipeline that might impact the model’s fairness.
Hovy and Prabhumoye (2021) also discuss these,
adding a fifth source related to the overarching de-
sign of NLP research projects.

Here, we outline these pipeline sources of bias
and also show how they, in fact, originate in the
Jim Code perspective.

1. Research design: According to Hovy and Prab-
humoye (2021), research design bias is manifested
in the skewness of NLP research towards Indo-
European languages, especially English. This skew
leads to a self-fulfilling prophecy, since most of the
research focuses on text in English, more data in
English becomes available, which in turn makes it
easier for NLP researchers to work on English text.
This has further ramifications as Hovy and Prabhu-
moye (2021) also question whether, if English was
not the “default” language, the n — gram would
have been the focus of NLP models. The authors ar-
gue that the lack of diversity in the makeup of NLP
research groups, is one of the reasons behind the
linguistic and cultural skewness in NLP research.

In addition to these skews, there are further sources
of bias reflected in research design that originate
from the Jim Code perspective. Lack of social
context is clearly manifested in NLP research de-
sign. For example, NLP researchers deal with lan-
guage as a number of word occurrences and co-
occurence probabilities rather than dealing with


language as a diverse social component that re-
flects societal relationships and biases (Holmes,
2013). Another example, is lack of historical con-
text, with most of the data that NLP models are
trained on generated by white middle-class men,
resulting in speech recognition models not recog-
nizing African American dialects (Benjamin, 2019;
Tatman, 2017) and hate speech detection models
falsely flagging African American dialect as hate-
ful (Sap et al., 2019). Lack of creativity is also
reflected in research design. For example, with
NLP models relying on the n — gram models
and words co-occurrences, they incorporate biases
such that they associate gendered words,“woman”
and “man’’, with certain jobs, “nurse” and “doctor”
(Caliskan et al., 2017). As Hovy and Prabhumoye
(2021) contend, lack of diversity is also reflected in
the research design bias, as evident in the skewness
towards Indo-European languages. Because of the
lack of accountability and the lack of public aware-
ness, NLP research design bias has been going on
for decades, largely unnoticed and unconsidered.

2. Selection bias: Selection bias is a result of non-
representative observations in the datasets used to
train NLP models (Shah et al., 2020; Hovy and
Prabhumoye, 2021). This bias could manifest when
a model is trained on text data that has been gen-
erated by one group of people, but is subsequently
deployed in the real world and used by more di-
verse groups. For example, the syntactic parsers
and part-of-speech taggers that were trained on
data generated by white middle-aged men, which
then impacted the accuracy of these models when
tested on text generated by different groups of peo-
ple (Shah et al., 2020). Another example in hate
speech detection models, where the models were
trained on data with over-representation of terms
associated with marginalised identity groups with
the positive class (hateful) resulting in the models
falsely labelling content as hateful just because it
includes mentions of those identities (Sap et al.,
2019; Dixon et al., 2018).

Selection bias is also a result of lack of con-
text, since the NLP researchers used datasets
with over-representation of one group and under-
representation of many other groups due to their
lack of social and historical context of who gener-
ated that data and which identity groups are under-
represented in the chosen data. Lack of diversity
is also a prominent reason behind selection bias in
NLP, as most of the researchers come from non-

marginalised backgrounds (Michael et al., 2022)
with blind spots for the under-represented groups
of people. Finally, lack of creativity is another
reason behind selection bias. As NLP researchers
build their models on biased systems that generated
biased data, instead of being more creative and us-
ing more diverse representative data that work for
everyone.

3. Label bias: Label bias, also known as annotator
bias, is a result of a mismatch between the anno-
tators and the authors of the data. There are many
reasons behind label bias. It can result from spam-
ming annotators who are uninterested in the task
and assign labels randomly to get the task done, as
can happen on crowdsourcing platforms. It also
happens due to confusion or ill-designed annota-
tion tasks. Another reason is due to the individual
annotator’s perception and interpretation of the task
or the label (Hovy and Prabhumoye, 2021). More-
over, there could be a mismatch between the au-
thors’ and annotators’ linguistic and social norms.
For example, annotators are prone to mislabel con-
tent as hateful for including the N-word, despite its
often benign in-group use by African Americans.
Finally, labels might carry the annotators societal
perspectives and social biases (Sap et al., 2019).

On the other hand, we can argue that some of these
biases result from unfairness in the crowdsourcing
systems. Since the pay that annotators receive is
often extremely low they are incentivised to com-
plete as many tasks as they can as fast as possible to
make ends meet, which in turn impacts the quality
of the labels (Fort et al., 2011). Moreover, Miceli
et al. (2022) argue that the bias in the labels is not
only due to the biased perceptions of the annotators,
but also due to a certain format the annotators have
to follow for their annotation tasks and if that for-
mat falls short on diversity, the annotators lack the
means to communicate that to the designers of the
task. An example is when an annotator is presented
with a binary gender choice even if the data con-
tains information about non-binary or transgender
people. Hence, label bias could be seen as a result
of the lack of context. As the NLP researchers who
mismatch the demographics of their data’s authors
and annotators do that due to lack of social context
of the author of the data. Label bias is also a result
of the lack of accountability, as big tech and NLP
research groups hire annotators with unfair pay in
addition to the lack of means for those annotators
to communicate problems in the annotation task


with the task designer due to power dynamics.

4. Representation bias: Representation bias, also
known as intrinsic bias or semantic bias, describes
the societal stereotypes that language models en-
code during pre-training. The bias exists in the
training dataset that then gets encoded in the lan-
guage models static (Caliskan et al., 2017; El-
safoury et al., 2022a; Garg et al., 2018), or con-
textual (Nangia et al., 2020; Nadeem et al., 2021).
Hovy and Prabhumoye (2021) argue that one of
the main reasons behind representation bias is the
objective function that trains the language mod-
els. As these objective functions aim to predict the
most probable next term given the previous context,
which in turn makes these models reflect our biased
societies in the data.

Again, representation bias is a result of the lack of
social and historical context, which is why NLP
researchers tend to use biased data to train these
language models. It is also a result of lack of cre-
ativity as instead of using objective function that
aim to reproduce the biased word that we live in,
NLP researchers could have used different objec-
tive functions that optimize fairness and equality in
addition to performance.

5. Model overampflication bias: According to
Shah et al. (2020), overampflication bias happens
because, during training, the models rely on small
differences between sensitive attributes regarding
an objective function and amplify these differ-
ences to be more pronounced in the predicted out-
come. For example, in the imSitu image captioning
dataset, 58% of the captions involving a person
in a kitchen mention women, resulting in models
trained on such data predicting people depicted
in kitchens to be women 63% of the time (Shah
et al., 2020). For the task of hate speech detec-
tion, overampflication bias could happen because
certain identity groups could exist within different
semantic contexts, for example, when an identity
group like “Muslims” co-occurs with the word “ter-
rorism’’. Even if the sentence does not contain any
hate, e.g. “Anyone could be a terrorist not just mus-
lims’’, the model will learn to pick this information
up about Muslims and amplify them, leading to
these models predicting future sentences that con-
tain the word “Muslim” as hateful. According to
(Hovy and Prabhumoye, 2021), one of the sources
of overampflication bias is the choice of objective
function used in training the model. Since these ob-

jective functions mainly aim to improve precision,
the models tend to exploit spurious correlations or
statistical irregularities in the data to achieve high
performance by that metric.

Overamplification bias is again a result of the lack
of social and historical context, which results in
using data that has an over-representation of certain
identities in a certain social or semantic context.
These over-representations are then picked up by
the models during training. Another reason is the
lack of creativity that results in choosing objective
functions that exacerbate the differences found in
the datasets between different identity groups and
prioritising overall performance over fairness.

5 Discussion

It is clear that the sources of bias that we find in
the NLP pipeline do not come out of nowhere, but
have their origins in those that have been outlined
in the social science, critical race theory and digi-
tal humanities studies—the Jim Code perspective.
Despite this, the bias metrics that have been pro-
posed in the NLP literature measure only pipeline
bias, which has led to limitations in the currently
proposed methods to measure and mitigate bias.
In this section, we outline these limitations and
recommend measures to mitigate them.

5.1 Limitations of bias research in NLP

The lack of scrutiny of the social background be-
hind biases, has led approaches to bias measure-
ment to incorporate the same methods that intro-
duced bias in the first place. For example, crowd-
sourcing the data used in measuring bias in lan-
guage models (Nangia et al., 2020; Nadeem et al.,
2021) reintroduces label bias into the metric that
is supposed to measure bias. Moreover, studies
that propose bias metrics in NLP don’t incorporate
the social science literature on bias and fairness,
which results in a lack of articulation of what these
metrics actually measure, and ambiguities and un-
stated assumptions, as discussed in (Blodgett et al.,
2021).

This results in limitations to the current bias met-
rics proposed and used in the NLP literature. One
of these is that different bias metrics produce dif-
ferent bias scores, which makes it difficult to come
to any conclusion on how biased the different NLP
models are (Elsafoury et al., 2022b). There is also
the limitation that current bias metrics claim to mea-
sure the existence of bias and not its absence, mean-


ing that lower bias scores do not necessarily mean
the absence of bias (May et al., 2019), leading to
lack of conclusive information about the NLP mod-
els. Another consequence of the lack of understand-
ing what the bias metrics in NLP actually measure,
is that most of the research done on investigating
the impact of social bias in NLP models on the
downstream tasks could not find an impact on the
performance of the downstream tasks (Goldfarb-
Tarrant et al., 2021; Elsafoury et al., 2022a) or the
fairness of the downstream tasks (Kaneko et al.,
2022; Cao et al., 2022).

Similarly, one of the main limitations of the pro-
posed methods to measure individual fairness met-
rics is that the motivation behind the proposed met-
rics and what the metrics actually measure are not
disclosed. For example, Prabhakaran et al. (2019);
Czarnowska et al. (2021); Qian et al. (2022) pro-
pose metrics to measure individual fairness using
counterfactuals without explaining the intuition be-
hind their proposed methods and how these metrics
meet the criteria for individual fairness.

As for group fairness metrics, they are all based
on statistical measures that have come in for crit-
icism. For example, Hedden (2021) argues that
group fairness metrics are based on criteria that
cannot be satisfied unless the models make perfect
predictions or that the base rates are equal across
all the identity groups in the datase. Base rate here
refers to the class of probability that is uncondi-
tioned on the featural evidence (Bar-Hillel, 1980).
Hedden (2021) goes on to ask if the statistical crite-
ria of fairness cannot be jointly satisfied except in
marginal cases, which criteria then are conditions
of fairness.

In the same direction of questioning the whole
notion of using statistical methods to measure
fairness, Broussard (2023) argues that some of
the founders of the field of statistics were white
supremacists, which resulted in skewed statisti-
cal methods and suggests that to measure fairness,
maybe we should use non-statistical methods. Ap-
proaching the bias and fairness problem in NLP
as a purely quantitative problem led the commu-
nity to develop quantitative methods to remove the
bias from NLP models like (Bolukbasi et al., 2016;
Liang et al., 2020; Schick et al., 2021) which re-
sulted in only a superficial fix of the problem while
the models are still biased (Kaneko et al., 2022;
Gonen and Goldberg, 2019). As shown above, Sec-
tion 4.2, bias and fairness in NLP models are the

results of deeper sources of bias, and removing the
NLP pipeline sources of bias would not lead to any
real change unless the more profound issues from
the social science perspective are addressed.

Similarly, current efforts to improve the model’s
fairness have relied on quantitative fairness mea-
sures that aim to achieve equality between different
identity groups, when equality does not necessarily
mean equity (Broussard, 2023). Achieving equality
would mean that the NLP models give similar per-
formances to different groups of people. However,
in some cases, fairness or equity would require
treating people of certain backgrounds differently.
For example, Dias Oliva et al. (2021) demonstrate
that Facebook’s hate speech detection models re-
strict the use of certain words considered offensive
without taking into consideration the context in
which they are being used. This leads to the censor-
ing of some of the comments written by members
of the LGBTQ community, who claim some of
these restricted words as self-expression. In this
case, equality did not lead to equity.

5.2 How to mitigate those limitations?

Addressing the Jim Code sources of bias, is not a
simple task. However, by doing so, we can take
steps towards developing more effective ways to
make NLP systems more inclusive, fairer and safer
for everyone. Here, we outline actionable recom-
mendations for the NLP community:

1. Lack of context can be addressed by incorporat-
ing social sciences as part of the effort of mitigating
bias in NLP models. This is only possible through:

(a) Interdisciplinary research where scientists
with backgrounds in fields such as critical race
theory, gender studies and digital humanities
studies are included in NLP project teams, so
they can point out the social impact of the
choices made by the NLP researchers.

(b

wm

It can also be addressed by further integration
of the teaching of data and machine learning
ethics into NLP curricula, whereby students
gain an understanding of the societal implica-
tions of the choices they make. Currently, they
are typically only exposed to minimal and to-
kenistic treatment of the topics of bias and
fairness in NLP models, which is insufficient
to understand the origins of bias from a social
science perspective. This should also include
training in AI auditing, enabling students to as-


sess the limitations and societal impact of the
NLP systems they develop (Broussard, 2023).

2. Lack of creativity is a direct result of lack of
context. We can address the lack of creativity by:

(a)

(b)

(c)

Raising awareness of the social and historical
context and the social impact of development
choices among NLP researchers. This will
encourage more creative methods to achieve
their goals, instead of the reproduction of op-
pressive systems in shiny new packaging. On-
line competition and code sharing platforms
could be a place to start, for example, cre-
ating shared tasks in which participants de-
velop new NLP models that do not rely on
n — grams or objective functions that do not
amplify societal biases.

Another way to encourage NLP researchers
to re-investigate NLP fundamentals, is spe-
cialized conferences and workshops on re-
imagining NLP models with an emphasis on
fairness and impact on society. This effort is
already underway with conferences like ACM
Conference on Fairness, Accountability, and
Transparency (ACM FAccT) *. The outcomes
of these endeavours should be open for audit-
ing, evaluation and reproducibility. One way
to achieve that, without the controversy of
open-source, is for NLP conferences to adopt
the ACM artifact evaluation measures’ and
give reproducibility badges to published pa-
pers. This could be developed further to give
social responsibility badges to the papers that
were audited by a special responsible NLP
committee.

Specialized interdisciplinary seminars in ma-
jor NLP conferences could encourage NLP
researchers to collaborate with social scien-
tists. For example, organizing events like the
Dagstuhl seminars* that invite social scien-
tists to discuss their work on bias and fair-
ness which might lead to the exchange and
development of ideas between social and NLP
researchers.

3. Lack of diversity can be addressed with:

“*https://facctconference.org/

"https://www.acm.org/publications/
policies/artifact-—review-—badging

*https://www.dagstuhl.de/en/seminars/
dagstuhl-—seminars

(a) Greater diversity on research teams working

(b

(c

(d

4.

on NLP problems. A more diverse perspective
will be introduced to the research to make sure
that the proposed solution and new systems
are inclusive and work for everyone.

) NLP conferences play a great role in promot-
ing diversity in NLP research by incorporat-
ing shared tasks that encourage researchers
to work on low-resourced languages. For ex-
ample, the shared tasks* on Arabic, Persian,
Korean, and others.

) Incorporating more diversity workshops in
NLP conferences that allow researchers from
different backgrounds to publish their work,
e.g. the WiNLP workshop},

) This effort can go further by creating shared
tasks that test the impact of NLP systems on
different groups of people.

Lack of accountability The suggested mea-

sures should be enforced with:

(a) State level regulation to make sure that re-

(b

5.

search is not conducted in a way that may
harm society, which is only possible by hold-
ing universities and big tech companies ac-
countable for the systems they produce. One
step taken in this direction is the EU AI Act!
which is a legislative proposal that assigns AI
applications to three risk categories:

“First, applications and systems that
create an unacceptable risk, such as
government-run social scoring of the
type used in China, are banned. Sec-
ond, high-risk applications, such as
a CV-scanning tool that ranks job ap-
plicants, are subject to specific legal
requirements. Lastly, applications not
explicitly banned or listed as high-risk
are largely left unregulated.”

wm

There should also be an AI regulation team
that works for governments and employs AI
auditing teams and social scientists to approve
newly developed NLP systems before they are
released to the public. (Broussard, 2023)

Lack of awareness and Technochauvinism

The suggested regulations, can only happen by
democratically electing people who are willing to

Shttp://nlpprogress.com/
Thttps://www.winlp.org/

"nttps://artificialintelligenceact.eu/



put these regulations in place. This comes with rais-
ing the awareness of the limitations of the current
ML and NLP systems. It is important that the pub-
lic is aware that the likely doomsday scenario is not
an AI system that outsmarts humans and controls
them, but one that behaves like a Stochastic Parrot
(Bender et al., 2021) that keeps reproducing our
discriminative systems on a wider scale under the
mask of objectivity (O’ Neil, 2017; Benjamin, 2019;
Broussard, 2023; Nobel, 2018). NLP researchers
can help to raise public awareness through:

(a) Journalism is an important resource to inform
the public of the limitations and ethical is-
sues in the current AI systems. Muckraking
journalists in ProPublica, and The New York
Times investigate AI technologies, sharing
their investigations with the public (Brous-
sard, 2023). For example, the journalist’s in-
vestigation of the COMPAS system and its
unfairness was published by ProPublica. NLP
researchers should be encouraged to accept
interview invitations from journalists to share
their worries about the limitations of the cur-
rent NLP systems.

(b) Published Books for non-specialists is another
way to raise public awareness on issues related
to discrimination in AI systems. Especially
books that are targeted at non-specialists. For
example, books like Race after Technology,
More than a Glitch, and Algorithms of Oppres-
sion. NLP researchers can participate in those
efforts by writing about current NLP systems
and their limitations for non-specialists.

(c) Talks: NLP researchers should be encouraged
to share their views on AI in non-academic
venues. For example, participating in doc-
umentaries like Coded Bias** could bring
awareness to the public.

(d) Museums of technology, and arts could also
raise public awareness of the limitations and
potential dangers of AI. For example, in 2022,
the Modern Museum of Arts, had an exhibi-
tion called “Systems”'", showing how AI sys-
tems work, their inequalities, and how much
natural resources are used to build them. NLP
researchers and universities can help organize
exhibitions on the limitations of NLP systems.

“https://www.imdb.com/title/
tt11394170/
hetps://www.moma.org/collection/

works/401279?sov_referrer=theme&theme_
id=5472

(e) Social media awareness campaigns could be a
way to reach more people, especially younger
people. Currently, individual NLP researchers
share on social media their views and worries
about NLP systems. However, an organized
campaign can be more effective.

6 Conclusion

In this paper, we have reviewed the literature on
historic forms of sexism, racism, and other types of
discrimination that are being reproduced in the new
age of technology on a larger scale and under the
cover of supposed objectivity in NLP models. We
reviewed the origins of bias from the NLP literature
in addition to the social science, critical race theory,
and digital humanities studies literature. We argue
that the sources of bias in NLP originate in those
identified in the social sciences, and that they are
direct results of the sources of bias from the “Jim
Code” perspective. We also demonstrate that ne-
glecting the social science literature in attempting
to build unbiased and fair NLP models has led to
unreliable bias metrics and ineffective debiasing
methods. We argue that the way forward is to in-
corporate knowledge from social sciences and fur-
ther collaborate with social scientists to make sure
that these goals are achieved effectively without
negative impacts on society and its diverse groups.
Finally, we share a list of actionable suggestions
and recommendations with the NLP community on
how to mitigate the discussed Jim code origins of
bias in NLP research.

7 Ethical Statement

Our critical review on the origins of bias in NLP
systems should not produce any direct negative im-
pacts or harms. However, our work does not come
without risks. One of these could be in discour-
aging quantitative research on bias and fairness
in NLP by making such work seem daunting, re-
quiring collaborations and more effort than other
research disciplines in NLP. However, our aim is
rather to encourage researchers to be more cau-
tious and take a more inclusive approach to their
research, incorporating social scientists and their
knowledge into efforts at understanding bias in
NLP.


References

Ioana Baldini, Dennis Wei, Karthikeyan Natesan Ra-
mamurthy, Moninder Singh, and Mikhail Yurochkin.
2022. Your fairness may vary: Pretrained language
model fairness in toxic text classification. In Find-
ings of the Association for Computational Linguis-
tics: ACL 2022, pages 2245-2262, Dublin, Ireland.
Association for Computational Linguistics.

Maya Bar-Hillel. 1980. The base-rate fallacy in prob-
ability judgments. Acta Psychologica, 44(3):211-
233.

Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency,
FAccT ’21, page 610-623, New York, NY, USA. As-
sociation for Computing Machinery.

Ruha Benjamin. 2019. Race after Technology: Aboli-
tionist Tools for the New Jim Code. Polity.

Reuben Binns and Reuben Kirkham. 2021. How could
equality and data protection law shape AI fairness
for people with disabilities? ACM Trans. Access.
Comput., 14(3).

Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
Robert Sim, and Hanna M. Wallach. 2021. Stereo-
typing norwegian salmon: An inventory of pitfalls
in fairness benchmark datasets. In Proceedings of
the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACLITJCNLP 2021, (Volume 1: Long Papers), Vir-
tual Event, August 1-6, 2021, pages 1004-1015. As-
sociation for Computational Linguistics.

Tolga Bolukbasi, Kai-Wei Chang, James Zou,
Venkatesh Saligrama, and Adam Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In Pro-
ceedings of the 30th International Conference on
Neural Information Processing Systems, NIPS’ 16,
page 4356-4364, Red Hook, NY, USA. Curran
Associates Inc.

Meredith Broussard. 2023. More than a glitch: Con-
fronting race, gender, and ability bias in tech. MIT
Press.

Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183-186.

Yang Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul
Gupta, Varun Kumar, Jwala Dhamala, and Aram
Galstyan. 2022. On the intrinsic and extrinsic fair-
ness evaluation metrics for contextualized language
representations. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 561-570,

Dublin, Ireland. Association for Computational Lin-
guistics.

Simon Caton and Christian Haas. 2020. Fairness
in machine learning: A survey. arXiv preprint
arXiv:2010.04053.

Caroline Criado Perez. 2019. Invisible women: Data
bias in a world designed for men. Abrams.

Paula Czarnowska, Yogarshi Vyas, and Kashif Shah.
2021. Quantifying social biases in NLP: A gener-
alization and empirical comparison of extrinsic fair-
ness metrics. Transactions of the Association for
Computational Linguistics, 9:1249-1267.

Jeddery Dastin. 2018. Amazon scraps secret AI recruit-
ing tool that showed bias against women.

Angela Davis. 1982.
activist perspective.
10(4):5.

Women, race and class: An
Women’s Studies Quarterly,

Thiago Dias Oliva, Dennys Marcelo Antonialli, and
Alessandra Gomes. 2021. Fighting hate speech, si-
lencing drag queens? Artificial intelligence in con-
tent moderation and risks to lgbtq voices online. Sex-
uality & Culture, 25(2):700-732.

Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,
and Lucy Vasserman. 2018. Measuring and mitigat-
ing unintended bias in text classification. In Pro-
ceedings of the 2018 AAAI/ACM Conference on AI,
Ethics, and Society, ATES °18, page 67-73, New
York, NY, USA. Association for Computing Machin-
ery.

Fatma Elsafoury, Steve R. Wilson, Stamos Katsigian-
nis, and Naeem Ramzan. 2022a. SOS: Systematic
offensive stereotyping bias in word embeddings. In
Proceedings of the 29th International Conference
on Computational Linguistics, pages 1263-1274,
Gyeongju, Republic of Korea. International Com-
mittee on Computational Linguistics.

Fatma Elsafoury, Steven R. Wilson, and Naeem
Ramzan. 2022b. A comparative study on word em-
beddings and social NLP tasks. In Proceedings of
the Tenth International Workshop on Natural Lan-
guage Processing for Social Media, pages 55-64,
Seattle, Washington. Association for Computational
Linguistics.

Anne Fausto-Sterling. 2008. Myths of gender: Biologi-
cal theories about women and men. Basic Books.

Karén Fort, Gilles Adda, and K. Bretonnel Cohen.
2011. Last words: Amazon Mechanical Turk: Gold
mine or coal mine? Computational Linguistics,
37(2):413-420.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2018. Word embeddings quantify
100 years of gender and ethnic stereotypes. Pro-
ceedings of the National Academy of Sciences,
115(16):E3635—E3644.


Ismael Garrido-Mufioz, Arturo Montejo-Réez, Fer-
nando Martinez-Santiago, and L. Alfonso Urefia-
Lopez. 2021. A survey on bias in deep nlp. Applied
Sciences, 11(7).

Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-
cardo Mufioz Sanchez, Mugdha Pandya, and Adam
Lopez. 2021. Intrinsic bias metrics do not correlate
with application bias. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1926-1940, Online. Associa-
tion for Computational Linguistics.

Hila Gonen and Yoav Goldberg. 2019. Lipstick on a
pig: Debiasing methods cover up systematic gender
biases in word embeddings but do not remove them.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume I (Long and Short Papers), pages 609-614,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.

Brian Hedden. 2021.
gorithmic fairness.
49(2):209-231.

On statistical criteria of al-
Philosophy & Public Affairs,

Janet Holmes. 2013. An Introduction to Sociolinguis-
tics. Routledge.

Dirk Hovy and Shrimai Prabhumoye. 2021. Five
sources of bias in natural language processing. Lan-
guage and Linguistics Compass, 15(8):e12432.

Ben Hutchinson and Margaret Mitchell. 2019. 50
years of test (un)fairness: Lessons for machine learn-
ing. In Proceedings of the Conference on Fairness,
Accountability, and Transparency, FAT* ’19, page
49-58, New York, NY, USA. Association for Com-
puting Machinery.

Masahiro Kaneko, Danushka Bollegala, and Naoaki
Okazaki. 2022. Debiasing isn’t enough! -— on
the effectiveness of debiasing MLMs and their so-
cial biases in downstream tasks. In Proceedings
of the 29th International Conference on Computa-
tional Linguistics, pages 1299-1310, Gyeongju, Re-
public of Korea. International Committee on Com-
putational Linguistics.

Matt J Kusner, Joshua Loftus, Chris Russell, and Ri-
cardo Silva. 2017. Counterfactual fairness. Ad-
vances in neural information processing systems, 30.

Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia
Angwin. 2016. How we analyzed the compas recidi-
vism algorithm.

Paul Pu Liang, Irene Mengze Li, Emily Zheng,
Yao Chong Lim, Ruslan Salakhutdinov, and Louis-
Philippe Morency. 2020. Towards debiasing sen-
tence representations. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 5502-5515, Online. Association
for Computational Linguistics.

Kate Manne. 2017. Down Girl: The Logic of Misogyny.
Oxford University Press.

Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measur-
ing social biases in sentence encoders. In Long
and Short Papers, NAACL HLT 2019 - 2019 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies - Proceedings of the Con-
ference, pages 622-628. Association for Computa-
tional Linguistics (ACL).

Peggy McIntosh. 2001. White privilege and male priv-
ilege: A personal account of coming to see corre-
spondences through work in women’s studies (1988).
Race, Class, and Gender: An Anthology, pages 95—
105.

Milagros Miceli, Julian Posada, and Tianling Yang.
2022. Studying up machine learning data: Why talk
about bias when we mean power? Proc. ACM Hum.-
Comput. Interact., 6(GROUP).

Julian Michael, Ari Holtzman, Alicia Parrish, Aaron
Mueller, Alex Wang, Angelica Chen, Divyam
Madaan, Nikita Nangia, Richard Yuanzhe Pang, Ja-
son Phang, et al. 2022. What do nlp researchers
believe? results of the nlp community metasurvey.
arXiv preprint arXiv:2208.12852.

Bonnie J. Morris. 2010. History of lesbian, gay, bisex-
ual and transgender social movements. PsycEXTRA
Dataset.

Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
StereoSet: Measuring stereotypical bias in pre-
trained language models. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume I: Long Papers), pages 5356-5371, Online. As-
sociation for Computational Linguistics.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1953-1967, Online. As-
sociation for Computational Linguistics.

Safiya Umoja Nobel. 2018. Algorithms of Oppression:
How Search Engines Reinforce Racism. New York
University Press.

Jessica Nordell. 2021. The End of Bias. Granta publi-
cations.

Alexandra Olteanu, Carlos Castillo, Fernando Diaz,
and Emre Kiciman. 2019. Social data:  Bi-
ases, methodological pitfalls, and ethical boundaries.
Frontiers in Big Data, 2:13.

Cathy O’Neil. 2017. Weapons of Math Destruction:
How Big Data Increases Inequality and Threatens
Democracy. Crown.


Vinodkumar Prabhakaran, Ben Hutchinson, and Mar-
garet Mitchell. 2019. Perturbation sensitivity analy-
sis to detect unintended model biases. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 5740-5745, Hong
Kong, China. Association for Computational Lin-
guistics.

Rebecca Qian, Candace Ross, Jude Fernandes,
Eric Michael Smith, Douwe Kiela, and Adina
Williams. 2022. Perturbation augmentation for
fairer NLP. In Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 9496-9521, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.

Sigal Samuel. 2021. Als Islamophobia problem.

Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,
and Noah A. Smith. 2019. The risk of racial bias
in hate speech detection. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 1668-1678, Florence,
Italy. Association for Computational Linguistics.

Timo Schick, Sahana Udupa, and Hinrich Schiitze.
2021. Self-diagnosis and self-debiasing: A proposal
for reducing corpus-based bias in nlp. Transactions
of the Association for Computational Linguistics,
9:1408-1424.

Deven Santosh Shah, H. Andrew Schwartz, and Dirk
Hovy. 2020. Predictive biases in natural language
processing models: A conceptual framework and
overview. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 5248-5264, Online. Association for Computa-
tional Linguistics.

Ryan Steed, Swetasudha Panda, Ari Kobren, and
Michael Wick. 2022. Upstream Mitigation Is Not
All You Need: Testing the Bias Transfer Hypothesis
in Pre-Trained Language Models. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume I: Long Papers),
pages 3524-3542, Dublin, Ireland. Association for
Computational Linguistics.

Claude M Steele. 2011. Whistling Vivaldi: How stereo-
types affect us and what we can do. WW Norton &
Company.

Justyna Stypinska. 2022. Ai ageism: a critical roadmap
for studying age discrimination and exclusion in dig-
italized societies. AJ & society, pages 1-13.

Robert Wald Sussman. 2014a. Early Racism in West-
ern Europe, pages 11-42. Harvard University Press.

Robert Wald Sussman. 2014b. Eugenics and the Nazis,
pages 107-145. Harvard University Press.

Robert Wald Sussman. 2014c. The Pioneer Fund,
1970s—1990s, pages 235-248. Harvard University
Press.

Robert Wald Sussman. 2014d. The Pioneer Fund in
the Twenty-First Century, pages 249-272. Harvard
University Press.

Rachael Tatman. 2017. Gender and dialect bias in
YouTube’s automatic captions. In Proceedings of
the First ACL Workshop on Ethics in Natural Lan-
guage Processing, pages 53-59, Valencia, Spain. As-
sociation for Computational Linguistics.

Nenad Tomasev, Kevin R McKee, Jackie Kay, and
Shakir Mohamed. 2021. Fairness for unobserved
characteristics: Insights from technological impacts
on queer communities. In Proceedings of the 2021
AAAI/ACM Conference on AI, Ethics, and Society,
pages 254-265.
