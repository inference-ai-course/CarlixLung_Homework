arXiv:2306.15766v1 [cs.CL] 27 Jun 2023

Large Language Models as Annotators: Enhancing Generalization of NLP
Models at Minimal Cost

Parikshit Bansal
Microsoft Research India
parikshitb52@gmail.com

Abstract

State-of-the-art supervised NLP models
achieve high accuracy but are also susceptible
to failures on inputs from low-data regimes,
such as domains that are not represented in
training data. As an approximation to collect-
ing ground-truth labels for the specific domain,
we study the use of large language models
(LLMs) for annotating inputs and improving
the generalization of NLP models. Specifically,
given a budget for LLM annotations, we
present an algorithm for sampling the most
informative inputs to annotate and retrain
the NLP model. We find that popular active
learning strategies such as uncertainty-based
sampling do not work well. Instead, we
propose a sampling strategy based on the
difference in prediction scores between the
base model and the finetuned NLP model,
utilizing the fact that most NLP models are
finetuned from a base model. Experiments
with classification (semantic similarity) and
ranking (semantic search) tasks show that our
sampling strategy leads to significant gains
in accuracy for both the training and target
domains.

1 Introduction

A common limitation of supervised NLP models is
that they fail to generalize in ow data regimes, cor-
responding to inputs from subgroups or domains
that have limited labelled data in the training set.
These generalisation errors occur due to distribu-
tion shifts between the new inputs and training data,
that render some of the correlations learnt by the
model as invalid (Wang et al., 2022). For instance,
models may learn spurious correlations with sen-
sitive attributes like gender (Sun et al., 2019) or
may over-emphasize lexical patterns (Gururangan
et al., 2018); or in some cases, inputs may exhibit a
new concept that has not been seen in the training
data (Gama et al., 2014).

As a motivating example, consider the task of
determining semantic similarity between a pair of

Amit Sharma
Microsoft Research India
amshar@microsoft.com

sentences (Reimers and Gurevych, 2019). This task
forms the basis of information retrieval and recom-
mendation systems such as similar question recom-
mendation on online forums (Wang et al., 2018)
or product recommendation on e-commerce web-
sites (He and McAuley, 2016). In such systems,
it is common to encounter new unseen domains
during deployment. For instance, introduction of
a new item category or users from a new demo-
graphic may cause failures for a deployed model
due to a shift in distribution of inputs in the sys-
tem compared to the training data. Unlabelled data
is readily available for such distribution shift (..e.,
new questions posted by users or items from a new
category), but labelling the data requires consid-
erable human effort. In other cases, failures may
occur due to hard-to-learn semantic patterns found
in a small minority of the training data (see the
example pair containing lexically similar questions
on oxygen and glucose in Figure 1).

A common solution in all these cases is to col-
lect more labelled data distinct from the distribu-
tion of training data, but labelling (or annotating)
data is an expensive and manual process. To ad-
dress this issue, prior work suggests using large
language models (LLMs, (Ouyang et al., 2022;
Brown et al., 2020)) to annotate data. LLMs like
GPT-3 obtain promising accuracy for annotating
data for a variety of NLP tasks including senti-
ment classification (Ding et al., 2022), keyword
relevance (Choi et al., 2023; Gilardi et al., 2023)
and question answering (Gilardi et al., 2023). How-
ever, LLM-based annotations can be noisy and due
to efficiency reasons, we cannot deploy LLM mod-
els directly.

In this paper, we take the natural next step and
ask whether annotations from LLMs can be used
to enhance generalization of existing NLP models.
Given a corpus of unlabelled data, we find that a
naive application of LLMs (annotating inputs at
random) provides only marginal gains on total ac-


curacy and in some cases, can worsen accuracy for
low-data groups. To optimize the sampling, we
formulate the problem of sampling inputs for anno-
tation as an active learning problem (Zhang et al.,
2022). However, we find that the popular sampling
strategy based on model uncertainty (Lewis, 1995)
is also not optimal for LLM-based annotation.

Using experiments on classification (semantic
similarity) and ranking (semantic search) tasks, we
propose an alternative strategy for sampling inputs.
For cost-efficient sampling of new unlabeled in-
puts for LLM annotations, an intuitive solution is
to annotate only those inputs that the NLP model
is expected to be incorrect on, i.e., inputs where
the NLP model’s prediction and the ground truth
label would differ. In the absence of GT labels
for new inputs, we propose a metric, Conditional
Informativeness, that approximates this intuition.
We utilize the fact that state-of-the-art supervised
NLP models are often finetuned from a base model
such as BERT (Vaswani et al., 2017) that provides
an initial embedding for the input. For a given
input and an NLP task, Conditional Informative-
ness measures the deviation between the prediction
score from the base model and the score from the
NLP model finetuned using the available labelled
data for the task. We argue that the inputs which
have the max deviation between the two scores are
the ones likely to be incorrectly predicted by the
finetuned model and hence the most informative
ones for finetuning over the base model.

Our sampling metric provides a practical way to
improve generalization of NLP models for a task
(see Figure | for an illustration). Given a budget for
LLM annotation (i.e., number of queries), we select
the inputs having the maximum Conditional Infor-
mativeness for LLM annotation and then retrain the
NLP model using this additional training data. Our
algorithm shows significant improvements in target
domain and total accuracy, on the Quora dataset
for the semantic similarity task, and on Amazon
and Wikipedia datasets for the semantic search task.
Our algorithm also provides higher gains than the
uncertainty-based sampling from active learning
literature. This may be because of the error distri-
bution of LLM annotations: only for inputs with
high deviation, the LLM-based annotations may be
expected to be more accurate than the base model.

To summarize, we make the following contribu-
tions.
1) The Conditional Informativeness metric for sam-

pling inputs for LLM-based annotation that outper-
forms commonly used active learning approaches.
2) Experiments on semantic similarity and search
tasks that show LLM annotations can significantly
improve both in-domain and target domain accu-
racy.

2 Related Work

LLMs for data augmentation. A popular frame-
work for improving a NLP model’s generalization
has been to generate new data using LLMs and test
the model’s output using a human-in-the-loop, i.e.
LLMs are used in partnership with human partic-
ipants for data generation and testing/debugging
models (Ribeiro and Lundberg, 2022; Wang et al.,
2021). In recent work, (He et al., 2023b) utilise the
same strategy for training an NLP model: they use
GPT-3 for data generation over under-represented
groups, which are then annotated by users before
including in training set.

However, with more capable LLMs like Chat-
GPT, LLMs are now capable of not just generat-
ing data, but also annotating it (while faithfully
following annotation instructions). Recent work
(Gilardi et al., 2023; He et al., 2023a; Ding et al.,
2022) has looked at the annotation accuracy for
LLMs and found them to be at par with crowd-
worker annotators. Combining generation and an-
notation, parallel to us, (Whitehouse et al., 2023)
explore the utility of both input and labels gener-
ated from LLMs for crosslingual common sense
reasoning tasks. Similarly, for the task of building
a sentence embedding using contrastive learning,
(Cheng et al., 2023) use LLMs to both generate
novel input pairs and then score their similarity.

Motivated by real-world applications from infor-
mation retrieval, we focus our attention on the un-
supervised domain adaptation(Ramponi and Plank,
2020) (UDA) setting where unlabelled inputs are
easily available. UDA methods assume a source
labeled domain and a target unlabeled domain with
the goal of adapting to the target domain (while
also performing well on the source domain). For
instance, (Saad-Falcon et al., 2023) motivate the
passage reranking task where a large number of un-
labelled passages are available. They use LLMs to
generate synthetic queries for a given passage and
then use such augmented data to train a downstream
model. Given the potential of LLM-annotated data
for training downstream classifiers and the asso-
ciated costs of querying them, we study how to


1. Condition Informativeness Computation

Semantic Similarity

Lexical Similarity

2. Input Sampling

4. Model training

Sampling

A Conditional
Informativeness

Accuracy

5 Uncertainty

| Random
. il

What is structure

A for glucose? Ol
Whatis structure
for oxygen?

How can! make
friends?

How do! make
friends?

Low Semantic Similarity High Semantic Similarity
High Lexical Similarity | High Lexical Similarity

Input Prompt

Output all pair of sentences
asking the same question. o

Pair1 : What is structure for glucose?, = nt mp Pair2
What is structure for oxygen? cp

Pair2 : How can | make friends?, LLM

How do | make friends?

Answer:

3. LLM Annotation
LLM Output

Figure 1: Enhanced Generalization using LLM Annotations. Illustration of our algorithm using the duplicate
question detection task. We propose a sampling strategy based on deviation of an NLP model’s similarity score
from the base model, called (base model)-conditional informativeness. Inputs are sampled using this strategy (Step
2), annotated using an LLM (Step 3) and then added to the training set of the NLP model. Our sampling strategy
performs significantly better than random or active learning-based strategies.

efficiently utilise these annotations to train a more
generalizable NLP model; specifically, which in-
puts to annotate for maximum benefit?

Semantic similarity with limited labeled data.
(Chen et al., 2023) present a comprehensive survey
of data augmentation techniques for limited label
data settings in NLP. AugSBERT (Thakur et al.,
2020) present an augmentation strategy that uses a
bigger (oracle) cross-encoder model for generating
(pseudo-)labels for unlabeled inputs. These inputs
are then utilised to train a smaller and efficient
NLP model. Such an oracle, however, is limited
by the training data whereas LLMs are known to
have zero-shot capabilities that generalize to new
domains(Hou et al., 2023).

In addition to augmentation, unsupervised do-
main adaptation methods have also been pro-
posed. Apart from the main task learning loss,
(Ramesh Kashyap et al., 2021) propose an addi-
tional loss which minimizes the divergence be-
tween source and target domain representations.
Recent work UDApter (Malik et al., 2023) com-
bines UDA methods with adapters for efficient
domain adaptation. However, domain matching
techniques work only under a restrictive set of
assumptions (Li et al., 2020). Instead, we aim
to approximate the ground-truth labels through

LLMs, thereby converting the unsupervised prob-
lem into a simpler, supervised learning problem.
(Dua et al., 2022) investigates the failure modes
of Open-domain question answering when faced
with distribution shifts. In addition they propose a
few-shot data augmentation method for improving
generalisation of these models. The augmentations
uses LLMs to generate question for a given pas-
sage.

Active Learning. Choosing which inputs to an-
notate has been classically studied as an active
learning problem (Settles, 2009).In active learn-
ing setup, we are given a small set of L labeled
inputs, along with a large pool of U unlabeled in-
puts. We are also specified a budget B, which
denotes the number of inputs from the unlabeled
data that can be annotated by an oracle/human. Ac-
tive learning explores how to best sample B inputs
from the unlabeled pool to maximize the general-
ization accuracy of the final model that is trained
on the original L + (annotated) B samples. Ac-
tive Learning uses two primary criterion for sam-
ple selection : Informativeness and Representa-
tiveness (Zhang et al., 2022). The most popular
informativeness technique is uncertainty sampling
(Lewis, 1995; Schréder et al., 2021) and for rep-
resentativeness is diversity/density. As an appli-


cation, recent work (Margatina et al., 2023) uses
active learning in an in-context learning setting for
LLMs and shows that similarity based sampling
(instead of uncertainty and diversity) are most ef-
fective for in-context learning. In this paper, we
focus on LLM-based annotations and evaluate the
uncertainty-based informativeness sampling tech-
nique. Based on our experiments, we also propose
a new informativeness criterion.

3 Conditional informativeness criterion
for sampling LLM annotations

3.1 Background: Building NLP classifiers
using base models

Given a domain of sentences, 1, and a task 7 :
X — {0,1} we consider learning a classifier func-
tion f : X — {0,1} which follows the task
ie. f(x) = T(x) Va © &. The function
aims to learn features which are predictive of the
output label and their mapping to the output la-
bel. A subset of the domain ¥ is denoted by
X = {X0,£1,22,.--,2|x|} C ¥. The output
label of x; is T(x;) and is denoted by t;. A set of
examples can be represented as

D = {(ai, ti) 7 € [XI] (1)
Unlabeled examples lack the task label ¢;.

Semantic Similarity. As an example, consider
the semantic similarity task (Cer et al., 2017). In-
puts for semantic similarity come from ¥ x Y
where ¥ and ¥Y are a pair of domain of sentences.
The domains can be the same or different for sym-
metric and asymmetric similarity respectively. For
a given input (2;, y;), the task output is 1 if a pair
are semantically similar, and 0 if they are not. The
classifier for semantic similarity is hence defined
as f : X x Y > {0,1}. We denote a training set
as :

D={((i,y)ti):4€(XI}

Further details on semantic similarity are in
Supp. E.

Finetuning on Base model. NLP models are usu-
ally finetuned on top of some pretrained text mod-
els (e.g., we use MSMARCO-DistilBERT-V4 for se-
mantic similarity) called Base model. The base
model adheres to an approximation of the task
based on the pretraining dataset and provides ini-
tial embedding for the input. We call these features
defined by the base model as pretrained features.

3.2 A domain adaptation case study: Which
inputs to annotate?

To evaluate different input sampling techniques for
LLM annotations, we consider the semantic similar-
ity task of duplicate question detection. We train bi-
encoders (SBERT (Reimers and Gurevych, 2019))
on the Quora Questions Pair dataset (Wang et al.,
2018),using MSMARCO-DistilBERT-Vv4 as the base
model. To simulate a challenging target domain,
we remove 60% of “extreme” examples from the
training dataset. These are examples where the
base model either obtains the lowest mean squared
error w.r.t. the ground truth labels or obtains the
highest mean squared error. That is, half of the
examples (30%) are the easy examples the base
model is (most) correct on and the remaining half
are the hard examples where the base model is
(most) incorrect on. Further, we remove labels
from the target domain. Hence from the original
data we have 40% “source” labeled examples and
60% “target” unlabeled examples. For accuracy
evaluation on both source and target domains, we
create analogous domains over the test set too.

We consider an active learning setup where se-
lected inputs from target domain can be annotated
by an LLM and augmented in the training set.
After augmentation, the model is trained on the
source domain + augmented dataset. We consider
two popular active-sampling approaches in liter-
ature: Random and Uncertainty-based sampling.
Apart from these, we include two additional sam-
pling techniques based on our knowledge of the
target domain: base-consistent-sample and base-
inconsistent-sample. These are designed to capture
the easy and hard examples that constitute the tar-
get domain. Given labeled data L, unlabeled inputs
U and a budget for annotation as B we have :

* random-sampling. We randomly select B
inputs out of the U unlabeled inputs for anno-
tations.

uncertainty-sampling. We first finetune
the base model on the given labeled data D
and then select the B (budget) most uncertain
(according to the finetuned model) unlabeled
inputs (out of U).

base-consistent-sampling. We choose top B
examples having lowest (MSE) error on base
model predictions with GT labels.

¢ base-inconsistent-sampling. We choose top


B examples having highest (MSE) error on
base model predictions with GT labels.

These B inputs are then annotated and included for
final training on L + B.

AUC under different sampling strategies. Us-
ing gpt-3.5-turbo as the annotater LLM, we report
AUC (area-under-(ro)curve) in Table 1. We set a
budget B of 10% of the dataset for annotation. For
details on prompts used, see Sec 4.1.

Looking at the AUC metric for the complete
target domain, we observe that random-sampling
and uncertain-sampling lead to similar improve-
ments compared to the training set. Compared to
these active learning techniques, base-inconsistent-
sampling leads to almost twice the AUC improve-
ment. That is, annotations with LLM are best un-
der base-inconsistent-sampling. Remarkably, with
only 10% of the examples annotated, AUC with
base-inconsistent-sampling is even higher than the
setting where we augment the full target domain
(100% of examples). In contrast, base-consistent-
sampling hurts generalization. Even though base-
consistent-sampling was designed to sample exam-
ples with low base model error, it obtains worse
AUC than base-inconsistent-sampling on test exam-
ples with low base model error. Results on using
Ground Truth (GT) labels for annotations (instead
of LLM annotations) are in Supp. Table 11.

Implications. The above results indicate that for
LLM annotations, uncertainty-sampling may not
be the best technique. To understand these results,
note that the original model finetuned on training
set (first row in Table 1, with no augmentation from
target domain) has high generalization (AUC) for
low base error inputs while generalizing poorly
for high base error inputs. Annotating with base-
consistent-sampling is thus a waste of budget as
the base and simple finetuned model are already
good on the low base error inputs. Moreover since
LLM-annotations are not perfect, augmenting with
base-consistent-sampling introduces noise into the
model, when the model already has a high accu-
racy.

On the other hand, high base error examples,
which are targeted by base-inconsistent-sampling,
do have substantial room for AUC improvement
when considering the original finetuned model.
This indicates that LLM annotations should focus
only on base-inconsistent-sampling inputs, as such
annotations may be the most informative.

3.3. Conditional Informativeness metric

Based on the experiments above, we find that when
annotated with LLMs, high base error, or base-
inconsistent examples are the most informative for
training. But base-inconsistent-sampling, as de-
scribed above, is not practical since it requires
knowledge of the ground truth labels of inputs.
Hence in this section, we develop an approxi-
mate metric for quantifying the degree of base-
inconsistency of unlabeled inputs.

We use a metric which measures deviation of the
finetuned NLP model from the base model, and call
it Conditional Informativeness, since it depends on
the base model in addition to the finetuned model.
For a input x; we define it as

a(f, fo) = Dev( (i), fo(w)) (3)

where fo is the base model, f is the finetuned
model and Dev is a measure of deviation. We use
simple squared error in our work.The intuition is
that during the finetuning process with the goal of
minimizing error, a model is more likely to deviate
from the base model’s score on an input if the base
model has high error on that input. Here we assume
that the finetuned model’s score deviation captures
this notion of base error, which can be generalized
to the unlabelled inputs.

We present qualitative examples from our metric
on Quora dataset in Table 2. These inputs were
selected by our Conditional Informativeness metric
as having high deviation. While for the first pair
of examples the lexical similarity (base semantic)
is of the pair is low, their semantic meaning (du-
plicate question semantics) is the same, while for
the second pair, while the lexical similarity is high,
their semantic similarity is low. When doing LLM
annotations, inputs like these would be the most
informative for training.

The formulation above defines Conditional Infor-
mativeness based on deviation of individual input
semantic similarity scores. But we can also define
Conditional Informativeness using deviation at a
domain level. For example, for a multi-domain
dataset with domain information for each input, the
metric can be averaged over the entire domain to
find the most suitable domains for LLM annota-
tions.


Data Complete Test High Base Error Low Base Error
Initial Train Set 86.824 + 0.038 59.335 +0.139 99.068 + 0.048
+ 100% (complete target domain) | 87.544+ 0.035 65.785 +0.121 98.164 + 0.044
+ Random-sampling 10% 87.052 + 0.151 60.551 40.701 98.805 + 0.058
+ Uncertain-sampling 10% 87.620 + 0.029 61.594 +0.433 99.081 + 0.024
+ Base-consistent-sampling 10% 86.763 + 0.149 59.986 + 0.340 98.833 + 0.024
+ Base-inconsistent-sampling 10% | 88.108 + 0.062 65.538 +0.175 98.861 + 0.046

Table 1: AUC for Quora duplicate questions task, before and after including LLM-based annotations using four
different sampling techniques: random, uncertainty, base-consistent and base-inconsistent. AUC is evaluated on the
full test set, the test subset with high base model error and the test subset with low base model error. Sampling just
10% of the data for annotation using base-inconsistent-sampling is better than annotating with the complete (100%)

target dataset.

Pair Similarity
Base _ Finetuned
What is a good diet plan for a commuter that wants to gain weight ? Low High
What food should I eat to gain weight ?
How can you determine the structure for glucose ? ‘
5 a. High Low
How can you determine the structure for oxygen ?

Table 2: Quora test examples having high Conditional
Informativeness , i.e. finetuned predictions are different
from base model predictions. Base model captures lexi-
cal similarity while finetuned captures target semantics.

4 EAGLE: Enhanced Generalization
using LLM Annotations

Based on the Conditional Informativeness metric,
we now present the EAGLE algorithm for enhanc-
ing generalization of NLP models using LLM an-
notations. As in Section 3, we consider an active
learning setup where we are given some labeled
examples L and a pool of unlabeled inputs U along
with a budget B of annotating unlabeled inputs
(using LLMs). In addition to standard classifica-
tion tasks, our algorithm can also work for other
tasks such as ranking. We first present the gen-
eral algorithm and then present instantiations of it
for a classification task (semantic similarity) and a
ranking task (semantic search).

4.1 The EAGLE Algorithm

Step 1: Computing Conditional Informativeness
As the first step, we finetune our base model on the
labeled data L to get a finetuned model f i.e.,

f =argmin Ew, ¢er\L(f (xi), ti] @&

Using f, we compute the Conditional Informa-
tiveness score z;(f, fo) where fo is the base model,
for each unlabeled input x; € U ie.

z= {alf, fo): zi € US (5)

Step 2: Sampling inputs using Conditional In-
formativeness The next step involves sampling
appropriate inputs for LLM annotations. We either
choose to do an input-wise Conditional Informa-
tiveness sampling, or if the data is domain anno-
tated, we can do domain level annotations. For
input-wise sampling we select the top B samples
Le.

(6)

For domain level annotations, we can obtain the
domain-level Conditional Informativeness metric
(by averaging the metric over inputs belonging to
the domain). In this case, the budget B is uniformly
distributed over inputs in selected domains.

U sampled = 1e; 1a top(z, By}

Step 3: Annotating sampled inputs using LLM
Given a sampled set of unlabelled input Usampied,
we use LLM annotations for these inputs to get
an annotated set as EE coop tet We denote LLM
annotations function by 7’ : ¥ — {0,1}, and
hence the LLM annotation for input x; as ti; €
{0,1}. The augmented dataset made from U is

hence L’
L.ampled = {(2:, ti) 1G U} (7)

Step 4: Finetuning classifier on augmented la-
belled data Finally we finetune the base model
on the augmented dataset L + L/, ampled USing Eq 4.

4.2 Application: Semantic Similarity

We present how our algorithm can be used for
the semantic similarity task described in Section
3. Step 1 follows from the main algorithm. The
Conditional Informativeness computation follows
Eq 3, with the only caveat being that the classifier
function now takes two inputs :


Sampling is done in the same way with the algo-
rithm selecting top B inputs having highest z;.

LLM Annotation Details Consider a set of un-
labeled examples U consisting of pairs (2;, y;) to
be annotated by the LLM. We construct a prompt
which consists of set of pairs (x;, y;) of sentences.
For cost-efficiency we consider 10 pairs in each
prompt for our experiments. The prompt asks the
LLM to output all the pairs which are semantically
similiar (with semantics defined appropriately in-
side the prompt). All pairs outputted by LLM as
similar are considered similar while rest are not.
See Table 3 for example annotation outputs on
Quora dataset.

Pair Deviation GT LLM |
Why does Cuba tolerate the presence of Guantanamo Bay Naval Base ? Low 0 1 |
What is the deal with Guantanamo Bay ? Why isn’t it closed yet ?

What are the best headhunters in Mexico ? Low 1 0 |
Who are the best headhunters in Mexico ?

What is third pricing model ? .

What is a pricing model ? High 0 iu |
How was trading performed in Ancient India ?

Is there proof of ancient Indians trading overseas ? If yes , High 1 1

then what did they trade in and with what countries ?

Table 3: LLM (gpt-3.5-turbo) annotations for some low
and high deviation examples. LLM is able to correctly
guess the high deviation ones while is incorrect on the
low deviation ones. LLM annotation accuracy is agnos-
tic of the deviation. See Supp A for prompts used.

4.3 Application: Semantic Search

While semantic similarity is a fundamental task,
real world applications often rely on semantic
search, In such applications, the X is called set of
all queries denoted as X = {X0,%1,2,.--,2|x|};
while Y is the set of labels denoted as Y =
{Yo Y1;Y2;---5Yjy|}. These search for an optimal
semantic match for a sentence x € X from the set
Y, Le.

In practice since we don’t have the true semantics
T (e.g., relevance to query), we use some approxi-
mation of semantics for argmax. We denote a set
of examples by:

Dsearch = {((2i, ¥), Ti) + 4 € [|X|]}

where

(10)

T; = {tig 3 € (IY II} (11)
Unlabeled samples lack T; information. Following
Eqn. 3, Conditional Informativeness on set X is
defined as

y =9(2i, fo)

12
2(f, fo) =Dev(f (xi, y), fo(ai, y)) )

where g(.) finds the nearest y; € Y for x; accord-
ing to base embedding function fp (Eqn 9).

LLM Annotation Details The unlabeled set U
now consisting of pairs (7;,Y) to be annotated
by LLMs. Querying semantic similarity for each
query, label pair {(a;, yj) : yj € Y} is very expen-
sive. Hence we first create a filtered set of labels
from a semantic similarity model (in our case fine-
tuned model) f. With slight abuse of notation, we
consider an extension of the function g in Eq 9
as g(x, f, K) where g now outputs a set of top K
labels for each query. Our filtered set is hence
Y’ = g(x, f, K) where f is the finetuned model.
The set Y’ hence consists of top K ranked labels
for a query according to the finetuned model f.
The rest of the labels (which weren’t in the top Iv
ranking of the finetuned model) i.e. Y/Y’ have
their semantic similarity set to 0. We query the
LLM for semantic similarity of labels in the fil-
tered set Y’, where |Y’| = K. Hence this helps
us reduce the complexity of searching through the
whole label space by restricting the search space us-
ing the finetuned model f. We take / = 10 for all
experiments. For each pair {(«;, yj) : yj € Y’} we
can then query LLM similar to semantic similarity
setup above.

L' = {((@i,¥), Tj): (@i,Y¥)€U} (13)
We empirically observe that it is better to provide
one prompt for each query along with its top kK
filtered labels. The labels should be ordered by
their semantic similarity score according to the
model f in the prompt. Example prompts used
in our experiments can be found in Supp. A. The
filtering step in semantic search can use any good
similarity model. In our experiments, we utilise
our finetuned model f for the filtering step.

5 Experiments

We evaluate EAGLE algorithm on two tasks: 1) se-
mantic similarity, a fundamental task; 2) semantic
search, a real-world task motivated by information
retrieval applications. We assume that in addition
to some labeled examples, we are also given a large
pool of unlabeled inputs. For semantic similarity
we consider generalisation in limited labeled data
setting, while for semantic search we evaluate gen-
eralisation to unlabeled target domains. In limited
labeled data setting, both the labeled and unlabeled
inputs follow the same distribution while when


Gain in AUC

T T T T T
2.5 7.5 10.0 12.5 15.0 17.5

Quantiles —>

0.0 5.0

(a) LLM-annotated

Gain in AUC

0.8 4 \ {
} |
0.67
0.44 ;
{
0.0 2.5 5.0 75 10.0 12.5 15.0 17.5

Quantiles —

(b) GT-annotated

Figure 2: Gain in AUC on including LLM-annotated and GT-based augmentations on Quora dataset. The orange
line is gain in AUC with uncertainty based sampling (shaded region shows std err.). We divide the unlabeled data
into 20 quantiles, based on the Conditional Informativeness metric. Conditional Informativeness increases from left
to right. For LLMs, uncertainty is not a good method for sampling and Conditional Informativeness based sampling
is better, while for GT-based augmentations uncertainty based sampling is better.

Data Test AUC
Initial Train set 85.780 + 0.002
+ Random LLM 86.001 + 0.139
+ Uncertainty LLM 86.058 + 0.089
+ Conditional Informativeness LLM | 86.432 + 0.106
+ Random GT 86.677 + 0.068
+ Uncertainty GT 87.125 + 0.135
+ Conditional Informativeness GT 87.445 + 0.091

Table 4: AUC for different sampling techniques for
Quora semantic similarity task. We sample 10% of unla-
beled data. For LLM-based annotations, the best method
is to sample using our Conditional Informativeness sam-
pling while for GT based annotation, both uncertain and
Conditional Informativeness based sampling are good.

adapting to unlabeled target domain (the unlabeled
input), there is a distribution shift in inputs of the
labeled and unlabeled examples. We show how our
Conditional Informativeness based sampling inputs
helps to improve generalization in both of these set-
tings. We do our experiments on embedding-based
semantic similarity as defined below.

Embedding-based Semantic Similarity/Search
The search argmax operation (Eq 9) over the com-
plete query set |X| is quadratic (i.e. |X| x |Y]|). For
efficient computation, we use embeddings based se-
mantic similarity (SBERT (Reimers and Gurevych,
2019)), where both queries and labels are seperately
embedded into a N dimensional unit norm space.
The dot product between the embedded represen-
tations of sentences gives the semantic similarity.
Hence the goal is to learn a embedding function

h: XU > RN s.t. h(x;)Th(y) gives the seman-
tic similarity between the functions.

Sampling methods. In addition to Conditional
Informativeness, we consider random-sampling
and an active learning uncertainty-based sampling
algorithm. As an oracle, we also consider ground-
truth labels for the same inputs sampled by each of
these sampling algorithms.

Implementation details. We consider the base

model as MSMARCO-DistilBERT-v4 for both tasks.

For LLM-based annotations we use GPT-3.5-
Turbo. See Supp. A for prompts used in the

experiments. We tried open source models such as

TogetherComputer/RedPajama-INCITE-7B-Base
(along with the Chat and Instruct version) or

MosaicML/mpt-7b-chat but did not obtain good

annotation accuracy. All results are reported for 3

seeds. Other training details are in Supp. C.

5.1 Semantic Similarity

Setup We conduct experiments on the Quora
Question Pairs (Wang et al., 2018) dataset, which
consists of pairs of questions. The task is to label
each pair as a duplicate or not, i.e., whether the
questions have the same intent or not. We subsam-
ple 38400 training pairs from the train set. We
consider a setup where 10% of the Quora dataset
is labeled by ground truth, while rest of the 90%
forms the unlabeled pool of data. We present test
AUC (Area-Under-ROC) numbers as the evaluation
metric.


Wikipedia Amazon
USA Total Books Total
Initial Train set 12.530 + 0.034 19.048 + 0.019 | 17.226+0.008 24.904 + 0.076
+ Target LLM Random 40% 13.188 + 0.073 19.232 + 0.024 | 17.959 +0.075 25.065 + 0.049
+ Target LLM Conditional Informativeness (bottom 40%) | 13.089 + 0.079 19.209 + 0.034 | 18.021 + 0.033 25.110 + 0.038
+ Target LLM Conditional Informativeness (middle 40%) | 13.166+ 0.021 19.228+ 0.022 | 18.123+0.060 25.216 + 0.012
+ Target LLM Conditional Informativeness (top 40%) 13.372 + 0.058 19.363 + 0.023 | 18.351 + 0.028 25.271 + 0.030
+ Target GT Random 40% 13.893 + 0.047 19.430 + 0.052 | 18.375 +0.068 25.329 + 0.051
+ Target GT Conditional Informativeness (bottom 40%) 13.911 40.032 19.395+ 0.013 | 18.455+0.054 25.271 + 0.020
+ Target GT Conditional Informativeness (middle 40%) 13.973 + 0.070 19.327 + 0.023 | 18.400+ 0.027 25.213 + 0.075
+ Target GT Conditional Informativeness (top 40%) 13.878+ 0.015 19.414+ 0.015 | 18.613 + 0.043 25.285 + 0.057

Table 5: P@1 for test target domain (USA in Wikipedia and Books in Amazon) and complete test set. For LLM-
based annotation, top 40% samples according to our Conditional Informativeness are optimal for total accuracy
(while also being optimal for target domains accuarcy). For GT based annotations, Random Sampling is best for
total accuracy. Best target domain accuracy method for GT is inconclusive.

Comparison with Random and Uncertainty
Sampling We follow the Algorithm from 4.1 for
semantic similarity. Using the model finetuned on
labeled data, we sample 10% of unlabeled data for
annotation, according to various sampling strate-
gies (namely random, uncertainty and Conditional
Informativeness). For details on how LLM annota-
tions are done see Sec 4.2. We also present results
on annotations with ground truth labels i.e. t, = t;
(Sec 4.2). In Table 4, we show that for LLM-
based annotations, Conditional Informativeness-
based sampling achieves significantly better test
AUC than random and uncertainty sampling. In
comparison, for annotating with GT labels, both un-
certainty and Conditional Informativeness- based
sampling yield high AUC.

Evaluating Conditional Informativeness-based
Quantiles To find out why uncertainty-based
sampling did not work for LLM annotations, we
divide the data into 20 quantiles, each having 5%
of unlabeled data based on Conditional Informa-
tiveness metric. Figure 2 shows the gain in AUC
on including these samples (LLM or GT annotated)
with the training data. As a comparison, the or-
ange line in the plot signifies accuracy on sampling
5% from uncertainty metric (shaded portion is std
error). For LLM annotations, we observe that un-
certainty is not a good technique for sampling and
Conditional Informativeness-based sampling is bet-
ter, while for GT-based augmentations uncertainty-
based sampling provides better gains than Condi-
tional Informativeness-based sampling.

5.2 Semantic Search

Next, we evaluate the utility of Conditional In-
formativeness sampling for generalisation to un-
labeled target domains in semantic search tasks.

Datasets We consider two  recommen-
dation datasets for semantic search : 1)
LF-WikiSeeAlsoTitles-320K (Bhatia et al.,
2016) (i.e., Wikipedia) considers a recommen-
dation/retrieval setting. The train set consists of
Wikipedia page titles (queries X) along with a
large set of page titles (labels Y). For Wikipedia, a
label y; is semantically similar to a query x; if the
label is likely to occur in the SeeAlso section of the
query article’s wiki-page. As described in Section
4.3 for the semantic search task, the set of labels
remains fixed to Y. The task is to learn embed-
dings which follow the semantics above. For each
article X we also parse its category information,
which we use as it domain label. If for an article
Xj, it’s categorical information contains "USA" or
"America" it belongs to the domain USA, otherwise
not. 2) LF-AmazonTitles-131K (Bhatia et al.,
2016) (i.e., Amazon) considers recommendations
in e-commerce AlsoBought product setting. Given
a query product (X) the labels correspond to
possible products a user might buy (Y). Here too
we consider categorical information for all query
products X. We construct two domains in Amazon.
All products in "Books" category are in the Books
domain, while all products in the "Kitchen and
Dining" category form the Kitchen domain.

Setup For Wikipedia we consider the USA do-
main as our target unlabeled domain, and the rest
of the dataset as our labeled data. Similarly for
Amazon we construct two versions of the dataset,
one where we Books domain as the target unlabeled
domain and another where we consider Kitchen as
the target unlabeled domain. We use Precision@ |
(P@1) metric for evaluation, i.e. the fraction of
queries whose top ranked label is semantically sim-


ilar (or relevant) to the query, i.e,

Precision@1 = Ez,ex|T (xi, g(%i, f))]

For the GT annotation oracle, we annotate the top
ke sampled labels (using the finetuned model /)
with ground truth information i.e. for a query 2;,
tij = tij Vy; € Y’ and tij =O0Vy; € Y’. See
Sec 4.3 for notation (Eq 11,13). Note that for all
labels which are not ranked in top i by the fine-
tuned model have their semantic similarity set to 0,
even if they were relevant in GT. For other details
refer to Supp. C.

Results We present test P@1 for the target do-
mains (USA domain in Wikipedia and Books do-
main in Amazon) and the complete source + target
domain test sets in Table 5. We find that when aug-
menting with LLM based annotations, selecting
inputs which are in the top 40% inputs according
to our Conditional Informativeness are optimal for
total accuracy (while also being optimal for tar-
get domains accuarcy). For GT based annotations,
Random Sampling is best for total accuracy, though
results are not significant.

Using Domain-knowledge for Qualitative mea-
sure of Conditional Informativeness On the
Amazon recommendation task, consider domain
adaptation to Books or Kitchen domains. For Book
recommendations using only book titles (e.g., say
The Kite Runner for A Thousand Splendid
Suns) the Conditional Informativeness would be
high for encoder based models (assuming that en-
coder doesn’t have the necessary domain knowl-
edge for book recommendations, i.e., the two books
share the same author). That is, it would re-
quire more world knowledge than for domains like
Kicthen, (e.g., Kaiser Bakeware Muffin Pan
for Nordic Ware Brownie Pan) which are more
likely to be consistent with the base model’s seman-
tics (in this case lexical similarity).

For the domain Kitchen, we can see in Table 6
that including LLM-based annotations for domain
Kitchen does not provide any gains compared to
the base model. In comparison, for other domains
like Books, LLM annotations lead to better gener-
alisation than both base and training set finetuned
models. Refer to Supp. B for a plot showing how
LLMs are not better than finetuned/base model for
Amazon(Kitchen) domain, whereas for Wiki(USA)
and Amazon(Books) LLMs are significantly better
(Fig 3). For accuracy improvements on Kitchen do-
main, techniques utilising regularisation to base

Finetuning Dataset Wikipeda(USA) Amazon(Books) Amazon(Kitchen)

Conditional Informativeness | 9.57 9.48 8.62

Base Model 12.33 16.78 32.91
Training set 12.54 17:37 31.53
Training set+Target LLM 12.95 18.28 32.91

Table 6: Domain averaged Conditional Informativeness
(x100) scores for the target domains (USA in Wikipedia
and Books/Kitchen in Amazon) are shown in the top row.
The next three rows present P@1 numbers for target
domains. LLM annotations help for USA and Books
target domain; but for the Kitchen target domain, the
Base model has the same accuracy as LLM-augmented
model.

model may be suitable and LLMs may not be
needed.

6 Conclusion

We showed how LLMs can be used for annotations
and how sampling of inputs plays an important role
in improving an NLP model’s generalization. To
this end, we presented a novel sampling algorithm
for input selection that performs better than the
popular technique of uncertainty-based sampling.

As future work, we would like to test whether
the Conditional Informativeness metric applies to
other NLP tasks beyond semantic similarity. For
the semantic search setting, given the generative ca-
pabilities of LLMs, an interesting future direction
is to use LLMs to generate labels for queries while
restricting the generated label set to our target label
set.

References

K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal,
Y. Prabhu, and M. Varma. 2016. The extreme classi-
fication repository: Multi-label datasets and code.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing

systems, 33:1877-1901.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv: 1708.00055.

Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal,
and Diyi Yang. 2023. An empirical survey of data
augmentation for limited data learning in nlp. Trans-
actions of the Association for Computational Linguis-
tics, 11:191-211.


Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang
Li, and Xipeng Qiu. 2023. Improving contrastive
learning of sentence embeddings from ai feedback.
arXiv preprint arXiv:2305.01918.

Jonathan H Choi, Kristin E Hickman, Amy Monahan,
and Daniel Schwarcz. 2023. Chatgpt goes to law
school. Available at SSRN.

Kunal Dahiya, Nilesh Gupta, Deepak Saini, Akshay
Soni, Yajun Wang, Kushal Dave, Jian Jiao, Prasenjit
Dey, Amit Singh, Deepesh Hada, et al. 2023. Ngame:
Negative mining-aware mini-batching for extreme
classification. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data
Mining, pages 258-266.

Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,
Shafiq Joty, and Boyang Li. 2022. Is gpt-3 a good
data annotator? arXiv preprint arXiv:2212.10450.

Dheeru Dua, Emma Strubell, Sameer Singh, and
Pat Verga. 2022. To adapt or to annotate: Chal-
lenges and interventions for domain adaptation in
open-domain question answering. arXiv preprint
arXiv:2212.10381.

Joio Gama, Indrundefined Zliobaitundefined, Al-
bert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. 2014. A survey on concept drift adapta-
tion. ACM Comput. Surv., 46(4).

Fabrizio Gilardi, Meysam Alizadeh, and Maél Kubli.
2023. Chatgpt outperforms crowd-workers for text-
annotation tasks. arXiv preprint arXiv:2303.15056.

Suchin Gururangan, Swabha Swayamdipta, Omer Levy,
Roy Schwartz, Samuel R Bowman, and Noah A
Smith. 2018. Annotation artifacts in natural language
inference data. arXiv preprint arXiv: 1803.02324.

Ruining He and Julian McAuley. 2016. Ups and downs:
Modeling the visual evolution of fashion trends with
one-class collaborative filtering. In proceedings of
the 25th international conference on world wide web,

pages 507-517.

Xingwei He, Zhenghao Lin, Yeyun Gong, A Jin, Hang
Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,
Weizhu Chen, et al. 2023a. Annollm: Making large
language models to be better crowdsourced annota-
tors. arXiv preprint arXiv:2303.16854.

Zexue He, Marco Tulio Ribeiro, and Fereshte
Khani. 2023b. Targeted data generation: Find-
ing and fixing model weaknesses. arXiv preprint
arXiv:2305. 17804.

Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,
Ruobing Xie, Julian McAuley, and Wayne Xin
Zhao. 2023. Large language models are zero-shot
rankers for recommender systems. arXiv preprint
arXiv:2305.08845.

David D Lewis. 1995. A sequential algorithm for train-
ing text classifiers: Corrigendum and additional data.
In Acm Sigir Forum, volume 29, pages 13-19. ACM
New York, NY, USA.

Bo Li, Yezhen Wang, Tong Che, Shanghang Zhang,
Sicheng Zhao, Pengfei Xu, Wei Zhou, Yoshua Ben-
gio, and Kurt Keutzer. 2020. Rethinking distribu-
tional matching based domain adaptation. arXiv
preprint arXiv:2006.13352.

Bhavitvya Malik, Abhinav Ramesh Kashyap, Min- Yen
Kan, and Soujanya Poria. 2023. Udapter—efficient
domain adaptation using adapters. arXiv preprint
arXiv:2302.03194.

Katerina Margatina, Timo Schick, Nikolaos Aletras, and
Jane Dwivedi-Yu. 2023. Active learning principles
for in-context learning with large language models.
arXiv preprint arXiv:2305.14264.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems, 35:27730-27744.

Abhinav Ramesh Kashyap, Devamanyu Hazarika, Min-
Yen Kan, and Roger Zimmermann. 2021. Domain
divergences: A survey and empirical analysis. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1830-1849, Online. Association for Computa-
tional Linguistics.

Alan Ramponi and Barbara Plank. 2020. Neural unsu-
pervised domain adaptation in nlp—a survey. arXiv
preprint arXiv:2006.00632.

Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
arXiv preprint arXiv: 1908. 10084.

Marco Tulio Ribeiro and Scott Lundberg. 2022. Adap-
tive testing and debugging of NLP models. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 3253-3267, Dublin, Ireland. Associa-
tion for Computational Linguistics.

Jon Saad-Falcon, Omar Khattab, Keshav Santhanam,
Radu Florian, Martin Franz, Salim Roukos, Avirup
Sil, Md Arafat Sultan, and Christopher Potts. 2023.
Udapdr: Unsupervised domain adaptation via lm
prompting and distillation of rerankers. arXiv
preprint arXiv:2303.00807.

Christopher Schréder, Andreas Niekler, and Martin
Potthast. 2021. Revisiting uncertainty-based query
strategies for active learning with transformers. arXiv
preprint arXiv:2107.05687.

Burr Settles. 2009. Active learning literature survey.


Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Eliza-
beth Belding, Kai-Wei Chang, and William Yang
Wang. 2019. Mitigating gender bias in natural lan-
guage processing: Literature review. arXiv preprint
arXiv: 1906.08976.

Nandan Thakur, Nils Reimers, Johannes Daxenberger,
and Iryna Gurevych. 2020. Augmented sbert: Data
augmentation method for improving bi-encoders for
pairwise sentence scoring tasks. arXiv preprint
arXiv:2010.08240.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP, pages
353-355, Brussels, Belgium. Association for Com-
putational Linguistics.

Jindong Wang, Cuiling Lan, Chang Liu, Yidong
Ouyang, Tao Qin, Wang Lu, Yigiang Chen, Wenjun
Zeng, and Philip Yu. 2022. Generalizing to unseen
domains: A survey on domain generalization. JEEE
Transactions on Knowledge and Data Engineering.

Shuohang Wang, Yang Liu, Yichong Xu, Chenguang
Zhu, and Michael Zeng. 2021. Want to reduce
labeling cost? gpt-3 can help. arXiv preprint
arXiv:2108. 13487.

Chenxi Whitehouse, Monojit Choudhury, and Al-
ham Fikri Aji. 2023. Llm-powered data augmen-
tation for enhanced crosslingual performance. arXiv
preprint arXiv:2305. 14288.

Zhisong Zhang, Emma Strubell, and Eduard Hovy. 2022.
A survey of active learning for natural language pro-
cessing. arXiv preprint arXiv:2210.10109.


A LLM Prompting

A.1 Semantic Similarity

Prompts used : Systems and Question prompt
See Table 7

A.2 Semantic Search : Top K sampling with
Finetuned for Recommendations

Prompts used : Systems and Question prompt
See Table 8, Table 9, Table 9.

B LLM Annotation Accuracy

For different Ground Truth Conditional
Informativeness-based quantiles (i.e. the
metric is computed over base model error with
the ground truth label instead of a deviation with
finetuned model) we show the accuracy of LLM
annotations as compared to the Finetuned and
Base models across different datasets. For Quora
(Fig 3a) we can see that LLMs augmentation
performance is always worse than finetuned
model and is only comparable on the highest
quantiles. Similarly for Wikipedia-USA (Fig 3b)
we can see that LLMs are better than finetuned
model for around 50% of higher quantiles while
for Amazon-Books (Fig 3c) LLMs are always
better than/comparable to finetuned model. For
Amazon-Kitchen (Fig 3d), LLMs are comparable
to finetuned and base model and hence don’t offer
much advantage. This analysis can be used by
practitioner to decide which domains to augment
with LLMs.

C_ Training Details

C.1 Semantic Similarity

We finetune using quora with learning rate of le-4
for 2 epochs for all experiments. We use a batch
size of 32 and have a linearly decay learning rate
scheduler. We use MSE Loss for training. The
initial quora dataset is subsampled by a factor of
10 (i.e. 38400 samples). We further subsample by
10 for active learning setup.

C.2. Semantic Search

Recommendation are trained using a recent state-
of-art algorithm (Dahiya et al., 2023). We also
subsample both the datasets by a factor of 10. We
finetune both models for 100 epochs.

D_ Ground Truth Annotations for
Hard/Easy Target Domain in Quora

See Table 11

E_ Formalising Semantic Similarity

Given a pair domains of sentences (short texts), V
and \y, the task of semantic similarity is concerned
with learning a function f : ¥ x Y — {0,1} which
for a given pair of sentence (x;,y;) from the set
X x Y outputs either | (or 0) to show that x; is
(or isn’t) semantically same as y;. For symmet-
ric tasks (like duplicate question detection) the set
&X and Y can be the same, but we consider the
general case. The semantics are broadly defined
and depend on the target task. For e.g., for task
of duplicate question detection (say on quora) a
good model f outputs whether for a pair of ques-
tion the answer for one of the questions, answers
the second question. For the task of say books rec-
ommendations, semantics might require capturing
the similarity in pairs of sentences in context of
the authors of the books, the books genre, their tar-
get audience etc. We hence mathematically denote
semantics as S: ¥ x Y > {0,1}.

We consider the subset of these domains as _X =
{x0,%1,--- 2) xj} C¥,Y = {yo,y1,--- yy} C
Y. The similarity between a pair (x;, y;) is hence
S(x;,y;) shortened as s;;. For semantic search
a set of examples can be sufficient represented by
subset of pair of indices Do C {| X|] x [|Y|] denoted
by

D = {(2i, yj, 813) (4,9) € Do}
where x;,y; are the pair of sentences and s;; is

the semantic similarity. Unlabeled data lacks s;;
information.

(14)


System Prompt

Question Prompt

You are an expert in judging the intended answer for short questions on quora. You
use the intended answer to find duplicate questions (i.e. questions having the same
intended answer and answering one of the questions will answer the other question
too. Given a pair of questions from one of these forums you are effectively able to
discern if they are duplicates of each other or not by reasoning about the intended
answer.

Given pairs of questions from (say) Quora, output the pair of questions that are
asking the same question (i.e. have the same intended answer). Reason about the
intended answer to solve this. Here is an example of output "Pairl".

Pairl: %s Pair2: %s Pair3: %s Pair4: %s Pair5: %s Pair6: %s Pair7: %s Pair8: %s
Pair9: %s Pair10: %s

Pairs having duplicates are :

Table 7: Prompts Used for Quora

System Prompt

Question Prompt

You are an expert on United States centric Wikipedia articles and article titles. You
are able to infer the content and context of an article accurately from the title of
the article alone. Also, given a reference article title, you are able to accurately
discern which articles should be in the ’SeeAlso’ section of the reference article’s
wikipedia page.

You are given the main article title and titles for possible "SeeAlso" articles. You
have to output which <SeeAlsoArticle> is most likely to be in the "SeeAlso"
section of the main article. The topics of the articles are around or relating to
United States somehow. You should infer the content of the articles from their
titles and output SeeAlso articles are closely related to the main article’s topic and
provide additional useful information to the reader. SeeAlso articles should also
help readers explore related areas and should have value to the reader. Output just
the article id (e.g. SeeAlsoArticle1) for the most likely article. Here is an example
of output "SeeAlsoArticlel".

MainArticleTitle: %s SeeAlsoArticlel: %s SeeAlsoArticle2: %s SeeAlsoArticle3:
%s SeeAlsoArticle4: %s SeeAlsoArticle5: %s SeeAlsoArticle6: %s SeeAlsoArti-
cle7: %s SeeAlsoArticle8: %s SeeAlsoArticle9: %s SeeAlsoArticle10: %s

Most likely <SeeAlsoArticle> to be in "SeeAlso" section is :

Table 8: Prompts Used for Wikipedia USA

System Prompt

Question Prompt

You are an expert on relevance (/similarity) between books sold on e-commerce
websites. Given a reference book, you are able to accurately discern the relevance
of other books to the reference book.

Given a product a customer has recently bought and a list of 10 possible products,
output (product id of) which product is most relevant for the customer. Relevant
products have similar/complementary use cases. Here is an example of output
"Product1".

Question BoughtProduct: %s Product1: %s Product2: %s Product3: %s Product4:
%s Product5: %s Product6: %s Product7: %s Product8: %s Product9: %s Prod-
uctl0: %s

Most relevant product is :

Table 9: Prompts Used for Amazon Books



System Prompt

Question Prompt

You are an expert on ’Kitchen and Dining’ products sold on e-commerce websites
and on judging their utility to a customer. Given a product bought from e-commerce
website, you are able to accurately discern the relevance of other products to the
customer.

Given a product a customer has recently bought and a list of 10 possible products,
output (product id of) which product is most relevant for the customer. Relevant
products have similar/complementary use cases. Here is an example of output
"Product1".

Question BoughtProduct: %s Product1: %s Product2: %s Product3: %s Product4:
%s Product5: %s Product6: %s Product7: Ys Product8: %s Product9: %s Prod-
uctl0: %s

Most relevant product is :

Table 10: Prompts Used for Amazon Kitchen

Quora Wikipedia (USA)
1.0
& & —@- LLM Acc. 0.9 4 2. -©- LLMP@1
0.94 ‘22... —®- Finetuned Acc. ve =®- Finetuned P@1
= 0. —®- Base Acc. 0.8 4 SS, —®- Base P@1
0.875 . “36
= be s a7 4 eA By
. “wa Ly . e. Ne
G 0.77 Wing Q e 4 064
; iia ts. on 2 ST .~e
Jd . © 7 ‘e-—
& 06 a 0.5 4 -*~ i
bs ~e-® \ Se a
xl % eS ea iL)
0.5 @ 0.4 4 “ey
0.4 5 ® 0.34 ~~ .
0 2 4 6 8 0 2 4 6 8
Quantiles — Quantiles —
(a) LLM vs Finetuned accuracy across quantiles on Quora (b) LLM vs Finetuned accuracy across quantiles on Wikipedia
Question Pairs USA category
Amazon (Books) Amazon (Kitchen)
0.94 bs -e- LLMP@1 094 &~ ~e- LLMP@1
i —®- Finetuned P@1 a i —®- Finetuned P@1
0.85 WN —®- Base P@1 0.8 | PN —®- Base P@1
%. 0.7 4 BN
= 06 | a
a 0.54 Sy
0.44 . Ss
0.34 ‘\ SS.
eL 1S.
0.2 4 “Ne
~é
00 O05 10 415 20 25 3.0 3.5 4.0
Quantiles — Quantiles —
(c) LLM vs Finetuned accuracy across quantiles on Amazon (d) LLM vs Finetuned accuracy across quantiles on Amazon

Books category

Kitchen category

Figure 3: LLM vs Finetuned accuracy across quantiles


Data base-inconsistent-sample base-consistent-sample Total
Training set 59.335 + 0.139 99.068 + 0.048 86.824 + 0.038
+ 100% (complete target domain) 78.134 + 0.187 98.229 + 0.084 90.740 + 0.063
+ Random 50% 74.432 + 0.204 98.496 + 0.063 89.893 + 0.123
+ Uncertain 50% 77.083 + 0.248 98.077 + 0.090 90.159 + 0.069
+ base-consistent-sample 50% 51.403 + 0.390 99.499 + 0.018 86.034 + 0.046
+ base-inconsistent-sample 50% 79.801 + 0.142 96.830 + 0.067 89.644 + 0.065
+ Random 16% 69.761 + 0.352 98.732 + 0.014 89.020 + 0.071
+ Uncertain 16% 70.102 + 0.164 98.522 + 0.047 88.735 + 0.060
+ base-consistent-sample 16% 56.082 + 0.350 99.266 + 0.040 86.561 + 0.103
+ base-inconsistent-sample 16% 76.093 + 0.325 97.709 + 0.065 89.482 + 0.098

Table 11: AUC for Quora with different Ground Truth (GT) Annotations.

