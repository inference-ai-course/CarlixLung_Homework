2401.07518v4 [cs.CL] 11 Oct 2025

arXiv

JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Survey of Natural Language Processing for
Education: Taxonomy, Systematic Review, and
Future Trends

Yunshi Lan*, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian and Aoying Zhou,

Abstract—Natural Language Processing (NLP) aims to analyze text or speech via techniques in the computer science field. It serves
applications in the domains of healthcare, commerce, education, and so on. Particularly, NLP has been widely applied to the education
domain and its applications have enormous potential to help teaching and learning. In this survey, we review recent advances in NLP
with a focus on solving problems relevant to the education domain. In detail, we begin with introducing the related background and the
real-world scenarios in education to which NLP techniques could contribute. Then, we present a taxonomy of NLP in the education
domain and highlight typical NLP applications including question answering, question construction, automated assessment, and error
correction. Next, we illustrate the task definition, challenges, and corresponding cutting-edge techniques based on the above
taxonomy. In particular, LLM-involved methods are included for discussion due to the wide usage of LLMs in diverse NLP applications.
After that, we showcase some off-the-shelf demonstrations in this domain, which are designed for educators or researchers. At last, we
conclude with five promising directions for future research, including generalization over subjects and languages, deployed LLM-based

systems for education, adaptive learning for teaching and learning, interpretability for education, and ethical consideration of NLP
techniques. We organize all relevant datasets and papers in the open-available Github Link for better

review https://github.com/LiXinyuan1015/NLP-for-Education.

Index Terms—Natural language processing, educational NLP, educational applications, question answering, question construction,

automated assessment, error correction, survey.

1. INTRODUCTION

Natural Language Processing (NLP) refers to the branch of
computer science and artificial intelligence, that empowers
computers the ability to understand text and spoken words
the way human beings can. It serves real-world scenarios.
For example, NLP can help in healthcare applications. A wide
range of NLP techniques are being utilized to anticipate
the treatment process of the patients. Question answer-
ing [1], [2] and dialogue generation [3] help conduct medical
diagnosis, administrative assistants for appointments, and
billing. Similarly, NLP can help in commerce applications.
There is a growing trend of incorporating NLP techniques
into analytics and customer management platforms to help
learn more about the relationship between the consumers
and items via sentiment analysis [4].

In the education domain, the widespread collaboration
of new technologies into the education domain revolution-
izes education in the future. NLP techniques can help edu-
cation in the following ways:

e Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Wein-
ing Qian and Aoying Zhou are with School of Data Science and Engi-
neering, East China Normal University.

Manuscript revised xxx.

This work was supported in part by National Natural Science Foundation
of China under Grants 62537001, in part by Guizhou Provincial Program
on Commercialization of Scientific and Technological Achievements (Qianke-
hezhongyindi [2025] No. 006), and in part by the Chenguang Program
of Shanghai Education Development Foundation and Shanghai Municipal
Education Commission under Grant 24CGA26.

(Corresponding author: Yunshi Lan.)

e NLP technologies provide automatic key information

extraction from unstructured text which facilitates high-
lighting the key points when teachers and students read
passages, tutorials, and textbooks.

NLP technologies offer intelligent tutoring and recom-
mendations. When students are learning a concept, a
system can help recommend relevant concepts, prac-
tices, and reading materials such that the students
can find proper reading materials as references, which
enables active learning and self-inspection for students.
NLP technologies help automatic problem-solving. For
example, in math or science courses, students may have
extensive questions about exercises after the classes.
To avoid repeated and overwhelming questions for
teachers, NLP technologies can conduct reasoning and
answer the questions automatically.

NLP technologies facilitate automatic smart content
generation when teachers have trouble in preparing
the teaching materials from a rush of information on
the Internet or textbooks. Additionally, given a passage
in the textbook as the context and a concept as the
correct answer, NLP techniques can quickly construct
a multiple-choice question associated with the test
question and the distractors. This could facilitate the
teachers to make quick quizzes for classes.

NLP techniques enable personalized instruction. Stu-
dents may have different background and it is vital to
track the learning trajectory and provide personalized
instruction to the students. Various NLP models trained
on different annotated data collected from different


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

scenarios could contribute to personalized education.

e NLP technologies enable Al-supported assessment and
correction. For example, it is time-consuming for teach-
ers to grade the essays and codes from students in
linguistics or coding classes, by analyzing the textual
contents of the assignments, NLP applications can fig-
ure out the errors in the assignment and return the
corresponding correction to the students.

The above techniques allow for more flexible and effective
learning for students, thus freeing up valuable time for
teachers.

We notice there are a few surveys related to NLP for
education. However, a line of surveys focuses on a certain
task. Specifically, Lan et al. [5] summarize the general ad-
vancement for knowledge base question answering tasks
but the domain is not restricted to education. Bryant et al. [6]
summarize the cutting-edge techniques of grammatical er-
ror correction, which is a typical task of educational NLP.
Zhang et al. [7] carefully review the technical progress of
math word problem solving tasks. Messer et al. [8] provide
a systematic review of the task of code programming. How-
ever, the above surveys focus solely on a specific task and do
not identify the significant distinction of these tasks applied
in the education domain. Recently, we found there are some
surveys conducting a specific discussion on large language
models for education [9], [10], which are advanced but cover
only partial NLP techniques.

To provide a comprehensive overview of NLP for educa-
tion from a technical point of view: (1) New taxonomy: we
present a novel taxonomy for educational NLP and identify
representative NLP tasks that can be applied for learning
and comprehension as well as writing and assessment in
education scenarios, and then discuss them in detail; (2)
Comprehensive review: we collect recent papers published
in well-recognized computer science venues and review
their methods for solving the challenges of corresponding
tasks. We further include LLM-based methods due to their
wide usage in NLP applications; (3) Rich resources: we
discuss the publicly-available datasets and demonstrations
with their various attributes, which makes it easy for readers
to get started for their research.

For the rest of the manuscript, we first present the taxon-
omy of NLP in the education domain, which highlights four
representative tasks and eight fine-grained sub-tasks in to-
tal. For each task, we review the datasets and techniques for
the identified tasks, pointing out the major challenges and
corresponding solutions. Then, we summarize the available
demonstrations. At last, we conclude the survey with future
directions.

2 TAXONOMY OF NLP IN EDUCATION DOMAIN

Systems built on the basis of NLP applications and equipped
with NLP techniques could perform as empowered AI tu-
toring, which enhances the teaching and learning experience
of students and teachers. With the systems, students are able
to get access to private tutoring and extra support outside
the classroom, which avoids increasing the workload of
teachers.

We first demonstrate an overview of NLP for education
in Figure 1, where the systematic frame of relevant NLP

2
Applications
Text Question Automated Error
Simplification Construction Assessment Correction
Processing and Analysis Techniques )
Knowledge Graph Information Information — Knowledge
Construction Extraction Retrieval Alignment
—t t t
x
Data Source
Textbook Syllabus Exams Curricula Materials

Ne a

FIGURE 1: An overview of NLP for education. The left
section displays the systematic frame of relevant NLP tech-
niques, including data sources, processing and analysis
techniques, and applications. The right section demonstrates
the outcomes of diverse techniques.

techniques and their corresponding outcomes are displayed.
The data sources are significant as the fundamental sup-
ports for subsequent analyses and applications. Generally,
we could obtain the data from textbooks, syllabus, exams,
and curricula materials, which may contain behavioral data
of students, exercise questions, guidance signals of error
correction, and so on. From the perspective of teaching
subjects, the data resources cover subjects like chemistry,
math, linguistics, computer science, and so on, which could
support applications in different subjects. Given the raw
data resources, we could conduct processing and analysis
(e.g., knowledge graph construction, information extraction,
information retrieval, knowledge alignment) to extract the
key information from massive data and organize them
in structured knowledge graphs (e.g., EduKG'), databases
(e.g., WordNet?) and lists, which could be applied to various
downstream applications. Upon the well-organized data,
NLP tasks are applied (e.g., question answering, question
construction, automated assessment, and error correction)
to form high-level applications to serve teachers and stu-
dents in real-world scenarios. Grammarly’, Gauthmath’,
CodeSubmit°?, AutoQG °, and EduChat’ are possible edu-
cational products and demonstrations developed based on
these NLP applications.

This survey mainly focuses on the application level. We
highlight the most representative NLP applications with the
taxonomy shown in Figure 3. Considering that NLP tech-
niques play significant roles in different phases of education,
on the one hand, NLP could serve as a teaching assistant
for learning and comprehension. When students learn the
curriculum, the following NLP tasks could simplify the
difficulties of learning.

¢ Question Answering (QA), as a typical application,
could not only provide tutorials for students but also

1. https: //edukg.cn/team

2. https: / /wordnet.princeton.edu/ documentation
3. https:/ /app.grammarly.com

4. https:/ /www.gauthmath.com

5. https: / /codesubmit.io

6. https: / /autoqg.net

7. https: //www.educhat.top /


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3

Fine-tune the contextual knowledge as language mod-

The subject-specific )A.data eling [11], [12] and augment data from different sub-

is limited for training

jects [13].
fametan The contexts may be pre- Incorporate the pre-processing steps of diagrams/tables/-
sented in diverse formats graphs as off-the-shelf toolkits [14], [15].

Answering

Collaborate with LLMs as the agent in TQA tasks and
integrate the image-to-caption modules and pre-processing
=— toolkits [16], [17], [18].

Zero-shot issues frequently

Formulate the math expression as an abstract tree and
re-organize the textual contexts with structures [19], [20],
[21], [22], [23], [24].

Some MWPs are complex in
logic

Math Word , Learn the MWPs solving with weak supervision [19], [25];
Problem oxprescions in Leo ae Distill the knowledge from a large pre-trained generic
Solving model [26].

Zero/few-shot complex Force LLMs to think step by step via CoT [27], [28], [29],

MWPs in real-world scenar- [30], [31], [32]; Treat LLMs as agent to call toolkits for

ios processing contexts in multi-modalities [33].

Setup a QA model as the discriminator [34]; Propose

The generated questions a feature of difficulty levels for learning based on the

ignore the difficulty stratifi- number of inference steps/ item response theory/ Bloom’s

cation Taxonomy [35], [36], [37]; Prompt LLMs to indicate diffi-

culty levels [38], [39]. J

Question The generated questions Setup a binary model as the discriminator to judge if it
Generation show weak relevance to the is related to certain keywords or concepts [40], [41], [42]

syllabus which are various for subjects.

The generated questions Incorporate a knowledge tracking model [43], [44] with

should be customized to

ardent pre-defined profiles of students.

NLP Appli-
cations for
Education

Employ dual attention and contrastive learning to ensure
the consistency and high relevance to the contexts [45],
[46], [47], [48], [49]. .

The produced distractors are
irrelevant to the questions

Distractor
Generation

The distractors lack strong Propose methods to avoid excessive similarity between
distractive effectiveness J |the generated distractors [50], [51], [52]. .
The singular evaluation of Introduce more diverse evaluation metrics [53], [54] and
the essays lacks comprehen- explainable prompts [55] considering specific objects and
Automated siveness scenarios. J
Essay Scor-
ms The sparse annotation of the Decouple the domain-dependent and domain-independent
essays in domain-specific features [56], [57], [58]
prompts e es a we
It is hard to identify good Collect comments/ number of lines/ number of operators
features and scoring algo- to train a model to evaluate the quality of the code [59],

rithms [60], [61], [62], [63] with supervision .
Code Scoring Implicitly encode the code

; nei Encode a code snippet and compare with the correct
snippet via distributed
erento ones [64], [65], [66], [67], [68], [69], [70] .

GEC with LLMs encounters Conduct ensemble of the edits produced via LLMs [71],
Grammatical over-correction issue [72], [73].

Error Correc-
tion Distorted results of GEC Construct multilingual data to handle mix-language

systems in low resources text [74].

The structural information of Utilize tree structure [75], [76], [77]/ AST data [?] to

code has been ignored model the structural characteristics of code.
Code Error CEC systems have resource Incorporating grammar constraints of programming
Correction overhead issue languages [78], [79].
CEC systems encounter Evaluate on a wider range of execution scenarios [80];
overfitting issue propose overfitting detection systems [81], [82]

FIGURE 2: The detailed techniques of NLP applications. The hierarchical structure is arranged with: NLP applications >
Challenges — Solutions.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

——
Question
Answering

——__
1. Textbook Question Answering

2. Math Word Problem Solving
)

natural science/

Learning medicine/math etc.

and
Comprehension

natural science/ Question 1 .Question Generation

Construction

“| linguistics etc. 2. Distractor Construction
NLP for

Education

_| computer science/| | Automated
linguistics etc. Assessment
LY

1. Automated Essay Scoring

Writing 2. Automated Code Scoring
and
Assessment

(~ >)
computer science/
| — linguistics etc.

X

Error 1. Grammatical Error Correction

Correction |]
i

2. Code Error Correction

FIGURE 3: The taxonomy of NLP for Education. The NLP
applications associated with the well-defined tasks and ap-
plicable subjects are displayed.

save time for overloaded teachers by providing feed-
back on questions. Based on the different goals of ques-
tion answering, textbook question answering and math
word problem solving are presented as the two typical
subcategories of question answering. These two tasks
both aim to generate answers from the given contexts
automatically. They can be widespread in the education
of science subjects.

e Question Construction (QC) is to generate questions as
well as the distractors given the correct answers, which
helps the teachers create a wide range of test questions
using pre-existing learning resources. Generally, a good
test question may consist of a question and multiple
distractors. Therefore, question generation and distrac-
tor construction are two key sub-tasks to be reviewed.

On the other hand, NLP improves writing and assessment
for students. When given constructive criticism and writing
prompts, students are likely to go deep into the topic.
However, teachers sometimes have difficulties responding
thoughtfully to students due to time limitations. But they
will benefit from the following tasks.

e Automated Assessment (AA) aims to grade assign-
ments for students automatically. Language grading is
a representative scenario of AA, which includes auto-
mated essay scoring for natural language learning and
automated code scoring for program language learn-
ing. For essay scoring, grammar, spelling, and sentence
structure are key aspects that need to be considered. For
code scoring, the correctness of the code, style aspects,
code quality are metrics to be viewed.

¢ Error Correction (EC) for language are useful to im-
prove language learning via explicating the correct
edits. Still, taking language learning as an example,
grammatical error correction and code error correction
are canonical sub-tasks for EC. The methods are not
simply required to provide error-free outputs based on
the erroneous inputs, more detailed instructions should
be provided to shed light on the reasons or knowledge
points to the errors.

Please be aware that there are some other NLP tasks
that are related to education. For example, tasks like text
rewriting [83] and knowledge graph construction [84] can
also help teachers to prepare suitable teaching materials. In
this survey, we mainly focus on the highlighted tasks, which
are widely explored NLP tasks in the education domain.
Our discussion is carried out based on the above taxonomy.

3 REVIEW OF TECHNIQUES

To help readers catch up with cutting-edge research on these
highlighted tasks, in this section, we provide a compre-
hensive review of the evaluated resources and techniques
for these tasks, showing emphasis on the development of
techniques and solutions to the key challenges. The review
is arranged in a tree in Figure 2 for a quick lookup.

3.1 Question Answering

Even though QA has been explored as a popular NLP task
for decades, educational QA requires a system to answer
the domain-specific questions, related to natural science,
social science, language science and so on [111]. These
systems act as assistants to help answer difficult questions
such that the students could learn from them and boost
their performance. Question answering in different subjects
may be presented in different formats. In Figure 4, we
display two typical examples of question answering in the
education domain. To answer the displayed questions, the
systems need domain-specific knowledge like mathematics
and science with the given contexts like text and images.
The output of the questions can be presented as an entity or
a math expression. In this section, we will present cutting-
edge techniques for Textbook QA and MWP solving.

3.1.1 Textbook Question Answering

Textbook QA

Context: Two magnets are placed as

Math Word Problem Solving

Question: Beth bakes 4,2 dozen batches of
cookies in a week. If these cookies are
shared amongst 16 people equally, how
many cookies does each person consume?

S

Question: Will these magnets attract
or repel each other?

Options: (A) repel (B) attract 96

Solution: 4* 2 =8,12*8=96, ~=6
Answer: (A)

Answer: 6

FIGURE 4: The examples of question answering in educa-
tion domain. The red box denotes the inputs and the green
box denotes the required outputs for the tasks. The dis-
played examples are extracted from ScenceQA and GSM8K,
respectively.

Textbook Question Answering (TQA) is a task that re-
quires a system to comprehensively understand the multi-
modal information from the textbook curriculum, spreading
across text documents, images, and diagrams. We formulate
the general loss function of TQA as follows for simplicity:

Ita = —E:p,g,a)ep log Po(AlQ, P)

where P,@,A are contexts, questions and answers from
data collection D, respectively. P may be presented in
non-textual modalities. The major challenge of TOA is to
comprehend the multi-modal domain-specific contexts as
well as the questions, and then identify the key information
to the questions.

Datasets. Kembhavi ef al. [112] introduced TQA dataset,
which aims to evaluate a system that contains multi-modal
contexts and diverse topics in the scientific domain. Sim-
ilar datasets such as AI2D [113], ScunceQA [1] are other

8. https: / /huggingface.co/spaces/wenhu/Science-Leaderboard


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 i)
Dataset Science subject Source Context #Q Top 1 Top 2 Top 3
TQA4 sdhysiecl ncitce sciceee curricula ™aBe/diagram 26K | ota [85] SECON [06] ISAAC [1
GeoSQAA geography high school image/diagram 13K ed feat Fen rion ere
AI2DA science grade 1-6 image/diagram 5K es Pees A pater cannes ties
ScenceQAA natural/social/ ; grade 1-12 image/diagram/ 21K ; 96.2 [18] ; 94.9 [92] 94.4 [93]
language science science curriculum text Multimodal-T-SciQ [18] MC-CoT-F [92] Honeybee [93]
MedQAs medicine Pea Xt 40K | edPatMe2 “GPL FlansPalM
MedMCQAA medicine simulated exams text 200K a wee woe,
TheoremQAA meee si university exam text 800 aS ee Cwer. Sug OB
Dolphin-18K{ math grade school text 18K ho [97] 15.8/ Rove or) [97] TORE La
DRAW-1KA math algebra text 1K apr Ts cro it tT eet ea)
Mathes mats grade selioal om aah ihachineeP Td [101] Wile ese 107] Gener rcackart [103]
MathQAa math grade school text 37K EL ake 2) ae a wane “tne
ASDivA math grade school text 2K Self Refine [105] Toolfermer [106] “CPEs.
taMaBA mats grade selioal ext als noe ei non Careers a a cha (a
IconQAa math KZ text /image eS atenstre HOE rian ere agit
TasMWPA math grade school text/table 38K isa 1a [110] incre 7B tebe cle

TABLE 1: Comparison of TQA and MWP solving datasets. The first section includes datasets of TQA. The second section
includes datasets of MWP solving. “#Q” denotes the number of questions. We select these datasets because they are
constructed upon real-world textbook materials from different subjects. The references of the reported results or the
methods producing the results are shown at the right side or directly below the results, respectively. All datasets are
measured via the Answer AccuracyA except that Dolphin-18K is measured via BLEU-4/METEOR/ROUGE-L}.

large-scale TQA datasets with annotated lectures and ex-
planations covering diverse science topics including nat-
ural science, social science, and language science. Some
other datasets such as GeoSQA [88], MedQA [94], MedM-
CQA [114] cover specific subjects such as geography and
medicine. A recently released TheoremQA dataset further
covers textbook questions at the university level [2].
Methods. From the technical perspective, TQA can be
deemed as Visual Question Answering (VQA) in na-
ture [115], [116], [117]. Conventional VQA studies em-
ploy deep-learning models to encode the questions and
images [118], [119]. Then the multi-modal information is
fused together to comprehend the questions. While there are
significant similarities between VQA and TQA, we highlight
some specific obstacles and limitations involved due to the
distinct educational scenarios.

e The subject-specific QA data is limited for training.
Even though PLMs are frequently utilized as the backbone
model, they are short of the subject knowledge. To bridge
the gap, some studies [11], [12] fine-tune the contextual
knowledge in the dataset as language modeling, which
boosts the performance of TQA when only a small num-
ber of examples are available in the specific domains:

Lp retrain =

where Pm, P\m represent the masked tokens and un-
masked tokens respectively. Some other studies combine
diverse datasets in different subjects:

where D is augmented data set from different subjects. Py
is a VOA model trained with adversarial training to learn
subject-invariant features [13].

e The contexts may be presented in diverse formats. To
understand the diagrams and tables, graph-based pars-
ing methods are developed to extract the concepts from
diagrams [113]. Optical Character Recognition (OCR) is
employed to identify the answers from the charts, which
is further aligned with the questions [14], [15]. These
methods conduct pre-processing:

P=T(P).

Here, T is a set of off-the-shelf toolkits for calling. After
that, P will be deemed as the contexts of Q.

e Zero-shot issues frequently occur. TQA in education
has a higher requirement for reasoning under zero-shot
scenarios. Recently, LLMs have shown impressive zero-
shot capabilities in various NLP tasks, a number of stud-
ies [1], [16], [18] applied GPT-3 to solve the TQA. To
apply the full advantage of LLMs to image modality,
they use caption models to translate visual information
into language modality. The paradigm of TQA systems
becomes:

P = Image2Text(P); A= LLM(Q, P).

Liu et al. [17] extended LLMs to multi-modalities, which
results in a multi-modal model that connects a vision
encoder and an LLM for general-purpose visual and
language understanding:

A =MLLM(Q, P).


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

More recent studies [2], [32] develop compositional rea-
soning with LLMs as a planner that assembles a sequence
of tools (e.g., Hugging Face as image captioner, Github
as text detector, Web Search as bing search, and Python
as program verifier.) to execute and answer the textbook
questions. We can formulate their methods as:

A=LLM(Q,P,T).

The above methods could effectively solve part of
the challenges, but based on Table 2, there is still room
for improvement and we observe a severe performance
gap of TQA tasks between the mainstream and the non-
mainstream subjects such as geography and medicine,
which is caused by the imbalanced collection of QA data.
This could be solved by constructing larger and wider scale
of educational data.

3.1.2 Math Word Problem solving

Math Word Problem (MWP) solving is a typical category of
QA specifically in mathematics subjects, aiming to convert
a narrative description to an abstract expression:

LIywe = —E(g,z)ep log Po(E|Q).

Here, @ and EF are a question and its corresponding math
expression, which consists of a sequence of tokens and
symbols, respectively. This task is challenging because there
remains a wide semantic gap in parsing human-readable
words into machine-understandable logic.

Datasets. Dolphin-18K [120] is an early large-scale MWP
solving dataset, which contains over 18, 000 annotated math
word problems of elementary mathematics. DRAW-1K [121]
is introduced as a testbed for algebra word problem solvers.
Math23K [122] is a Chinese MWP solving dataset. Subse-
quently, MathQA [123] and ASDiv [124] further extend the
scale and diversity of the existing datasets, which makes it
particularly suited for usage in training deep learning mod-
els to solve MWP problems. Cobbe et al. [125] introduced a
more challenging dataset GSM8K, which is complicated in
high linguistic diversity. lconQA [126] extends MWP solv-
ing to abstract diagram understanding and comprehensive
cognitive reasoning by involving diagrams in problems.
Moreover, TABMWP [127] includes tables to enhance the
difficulty of the MWPs, which requires the table parsing
and understanding capabilities [127]. As we can see, there
is a set of MWP datasets that examine various reasoning
capabilities. With the development of this task, the evalu-
ated datasets become increasingly complicated in linguistic
diversity and multi-modalities.

Methods. MWP solving is a popular NLP application in
the education domain for granted since it can be utilized
as a teaching assistant in math. The investigation of MWP
solving can be traced back to the 1960s [128]. Early work
leverages manually crafted rules and schemas for pattern
matching, where human intervention is heavily relied on.
Subsequently, various strategies of feature engineering and
statistical learning are proposed to boost the performance.
With the development of techniques and the emergence
of large-scale datasets, deep learning-based methods have
been widely applied to MWP solving. The majority of the
methods follow the paradigm of machine translation [122],

6

which first encodes the narratives, then decodes the mathe-

matical expression token as the output sequence. Compared

with machine translation, there are some distinct solutions
that have been proposed due to the structured outputs of

MWPs.

« Some MWPs are complex in logic. To improve the gen-
eration of complex solution expressions, some methods
are proposed to formulate a mathematical expression
as an abstract tree [19], [20], [21], [22]. To improve the
understanding of the narratives, some studies focus on
improving the encoding of the narratives by re-organizing
the textual contexts with structure [23], [24], which may
increase computation complexity as a drawback. In this
case, graph networks are frequently involved in:

€ = Ps(E|Q) = GNN(Q).

Here, equations and questions can be represented as de-
pendency trees Q and abstract syntax trees €, respectively.
There is a lack of annotated expressions in the MWPs.
Due to the weak supervision caused by the lack of an-
notated expressions, multiple studies [19], [25] model the
MWP solving task as follows:

Imwe = —E(g,a)eD log S> P(A|E) Po(E|Q).
B

Also, some researchers [26] apply knowledge distillation
to MWP solving tasks, which learns a smaller model from
a large pre-trained generic model but may also include
some unexpected noise.

Zero/few-shot complex MWPs in real-world scenarios.
MWP solving usually encounters problems with com-
plex reasoning steps under zero-shot. Chain-of-Thought
(CoT) is a typical prompting strategy that is meant to
decompose a complicated MWP into sub-questions such
that LLMs could conduct reasoning step by step [27].
Follow-up methods improve it by designing more inter-
active prompting or constructing more efficient demon-
strations [28], [29], [30], [31], [32]. Similar to the TOA
tasks, researchers [33] leverage LLMs as an agent [129]
and delegate the computation to a program interpreter,
which can be conducted by the LLMs and executed by a
Python interpreter.

We observe the performance of MWP solving has a large
variance across different datasets in Table 2. With the in-
creasing complexity of the modalities and questions, the
tasks become more challenging. Hence, more advanced
and powerful methods should be explored to improve the
performance.

3.2 Question Construction

Question Construction (QC) aims to automatically construct
questions from a given context, which performs a significant
role in education [148]. Usually, Multiple Choice Questions
(MCQ) are common question types for a test quiz. The
construction of MCQ consists of two parts: Question Gen-
eration (QG) and Distractor Generation (DG). The former
generates questions based on a given context, while the
latter generates distractors to complement the correct ones.
In Figure 5, we display a typical example of QG and DG
in the education domain. In this section, we introduce the
relevant datasets and methods for QG and DG in detail.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Dataset Science subject Source Q&A type #Q Top 1 Top 2 Top 3
: : online science normal . 37.2/—/— [130] 36.7/—/— [130] 29.4/17.2/29.5 [131]
SciQa scence textbooks MC&extractive 13-7K EduQG [130] Leaf [132] DDS+ST [131]
a Sag! secondary school normal&cloze 39.8/—/34.0 [133] 36.5/—/31.6 [133] 24.7/15.1/23.8 [131]
RACEA lnngnistic English exams | MCé&extractive ‘00K BDG [133] GPT DDS+ST [131]
computing/ online education normal 24.7/8.7/27.4 [134] 19.8/6.4/23.1 [134] 0.4/3.0/6.5 [134]
Leeming  — smence, Pasinesss platform - Pos Attention [135] Seq2Seq [136] H&S [137]
humanities /math
: online education normal 25.6/12.7/27.6 [138] 25.1/12.7/26.1 [138] 20.2/10.8/22.0 [138]
KHANQA sdleice platform extractive Ug T5 [139] BART [140] UniLM [141]
history / geography / . my ee 4 ca 224 *
: : grades 6-12 normal 43.0/50.3/66.8 [142] 36.5/44.3/66.6 [142] 33.6/41.7/64.0 [142]
EduProbea ——_ economics/science/ textbooks extractive 208 TS [139] BART [140] MBART [143]
environmental studies
politics/physiology/
biology /business/ : normal&cloze - 33.7/45.8/— [41] 15.4/29.7/34.3 [144]
EduQGa law/sociology/ online textbooks MC&extractive 3K DCS [41] T5 [139] .
psychology /history
biology /physics/ ' normal 35.4/—/— [145] 34.5/—/— [145] 33.6/—/— [145]
MCQL+ chemistry website MC TALK LR+RF [145] LR+LM [145] RF [145]
math/health/ : : . an Za
: : online education normal . —/58.8/16.4 [45] 85.8/28.9/50.1 [146] —/27.8/36.6 [45]
‘etporey Mistomy/ geegraphy/ platform MC ok ChatGPT DQ-SIM [146] m5 [147]

TABLE 2: Comparison of QG and DG datasets. The format of the question is categorized as either normal (e.g., interrogative
sentence.) or cloze (e.g., fill-in-the-blank.), and the answer is categorized as either multiple-choice (MC) or extractive (parts
of the text are considered as the answer) [144]. The evaluation metrics are BLEU-4/METEOR/ROUGE-LA for QG and
MAP (Mean Average Precision)/GDR (Good Distractor Rate) / NDR (Non-sense Distractor Rate, lower is better){ for DG

Question Construction

Context: Mesophiles grow best in moderate
temperature, typically between 25°C and
40°C (77°F and 104°F). Mesophiles are often
found living in or on the bodies of humans
or other animals. The optimal growth
temperature of many pathogenic
mesophiles is 37°C (98°F), the normal human
body temperature. Mesophilic organisms
have important uses in food preparation,
including cheese, yogurt, beer, and wine.

Answer: mesophilic organisms

Question: What type of organism is
commonly used in preparation of foods
such as cheese and yogurt?

Distractor:

(B) protozoa

(C) gymnosperms
(D) viruses

FIGURE 5: The examples of Question Construction. The
displayed examples are extracted from SciQ.

3.2.1 Question Generation

The goal of QG is to automatically generate a question
from a given sentence or paragraph. The challenge lies in
identifying the key statement based on the context with or
without the utilization of the answer (namely answer-aware
QG or answer-agnostic QG [149]) and generating a question
based on the statement, which can be described as follows:

Loc =— 4(C,[A],Q)ED log Po(Q | C, [A])

Here C and Q are context and the generated question,
respectively. [A] is the answer which is optional to the input.

Datasets. OG and QA are dual tasks [150]. They both
require reasoning between questions and answers. As a
result, some MCQ datasets initially designed for QA tasks,
including SciQ [151] and RACE [152], are also leveraged
for research on QG [153], [154], [155], [156]. Besides, there
are some datasets particularly constructed for QG, such
as LearningQ [134], KHANQO [138], EduQG [144] and
EduProbe [142]. These datasets extend across various sub-
jects (e.g., science, medicine) and various education levels
(e.g., preliminary school, middle school, and university.),
the details of which are displayed in Table 2. Regard-
ing these datasets, LearningQ is collected from instructor-
crafted learning questions, which contain cognitively de-
manding documents for QG that require reasoning, but the

absence of answers limits its application to answer-aware
QG. KHANO and EduProbe provide (context, textual cue,
question) triples. EduQG is a high-quality MCQ dataset
generated by educational experts and questions are linked
to their cognitive complexity, which makes this dataset
move closer to reality.

Methods. Early methods for general QG primarily relied
on rule matching [157]. Du et al. [158] first applied the
Seq2Seq model with attention to automatic QG. Built upon
this framework, methods such as integrating linguistic fea-
tures [159], training a multi-modal QG model [160], lever-
aging multi-task learning [161] and reinforcement learn-
ing [162] have also been used to optimize the QG models.
With the advent of language models, the BERT model and
GPT-3 have been applied to QG [163], [164]. Besides, re-
garding answer information in the QG system, researchers
proposed to encode the answer together with the con-
text [165], utilize the position information of the answer
in the context [166], and achieve answer-aware QG, while
using summaries of the texts as input can serve as a strategy
for answer-agnostic QG as it can help the modeling of
topic salience, which denotes the importance of specific
words or phrases in the context [149]. Importantly, there are
some unique challenges when we adopt QG systems to the
education domain.

e The generated questions ignore the difficulty strati-
fication. Some general methods lack an effective way
to control the difficulty of the question, which is vital
for improving the efficiency of education. The difficulty-
controllable QG task can be described as:

Loc == E(C,A,Q,D)ED log Po(Q | Q, A, D)

Here D represents the difficulty levels, which can be
formulated in several ways: whether a well-trained QA
model can answer it [34], the number of inference
steps [35], and the educational paradigms like Item Re-
sponse Theory [36] or Bloom’s Taxonomy [37] that cat-
egorize learning objectives into cognitive, affective, and
psychomotor domains. Recently, Hwang et al. [167] ap-



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

plied few-shot prompting of GPT-4 to generate questions
aligned with Bloom’s Taxonomy [37]. Lee et al. [38] and
Maity et al. [39] also designed an effect prompt for the QG
system to cover a wide range of difficulty levels.

e The generated questions show weak relevance to the
syllabus. Aligning generated questions to the syllabus
can be beneficial for identifying the focus of the test quiz,
which can be achieved by including a selection step for
content in general QG methods. Multiple studies [40],
[41] train binary classifiers or ranking models to judge
whether the generated question is related to concepts or
pedagogical contents with the objective of maximizing
both relevance and diversity.

P3(K | Q, C) = Classifier(Q, C, Kx),

where Ix represents certain keywords or topics of the
syllabus. Besides, Xiao et al. [42] generated questions that
align with given topic keywords using the plug-and-play
language models [168].

e The generated questions should be customized to stu-
dents. QG systems for customized education focus on
generating personalized questions for students. Srivas-
tava et al. [43] developed a knowledge-tracking model
that uses students’ answer histories to predict their per-
formance on new questions, fine-tuning it with an autore-
gressive language model. Subsequently, Wang et al. [44]
introduced the few-shot knowledge tracking model to
predict question difficulty by incorporating sequences of
student states and questions.

Leg = —Evc,5,9,p)ep log Po(Q | C, 5, D)

Here D is the difficulty level of the question. S is
the state of the students, which is represented as a
temporally-evolving sequence of questions and their re-
sponses [43]. Particularly, given a student’s responses as a
(correct(Y’) /incorrect(N)) to m questions as q, the state s
can be formulated as s = {q1,41,.--Gm,@m}.-
Even though various methods have been proposed, ques-
tion generation is still a difficult task to ensure its align-
ment to the real world. More studies focusing on develop-
ing syllabus-guided and customized questions are highly-
anticipated.

3.2.2 Distractor Generation

As we mentioned above, DG is an associated component for
question construction, which aims to generate distractors
with the given context, question, and correct answer in a
MC. A good distractor should be relevant to the context,
grammatically coherent to the interrogative of the question,
and deceptive as a false answer [151]. The task is generally
formulated as follows:

Loc = —Ec,Q,4,0)eD log Po(O | C,Q, A)

Here, C, Q, A, and O are context, question, answer, and
distractors, respectively.

Datasets. The aforementioned MCQs for QG tasks such
as SciQ and RACE can also be leveraged for DG tasks.
Specifically, the distractors in SciQ are designed by crowd
workers with reference [151]. Instead of providing entities
and concepts as distractors, the distractors of RACE are

8

at the sentence level [169], which are collected from the
secondary school English exams. Alongside the datasets
mentioned before, there are other MCQ datasets especially
designed for DG. The data of MCQL [145] is crawled from
the web and provides distractors that are either phrases or
sentences, covering multi-subjects. Besides, Televic [146] is
collected through Televic Education’s platform with distrac-
tors manually crafted by experts across multiple languages
and domains.

Methods. Early studies for DG are rule-based and feature-
based, which rely on linguistic rules [170] and specific
features like word frequency [171] or diverse similarity
measurement between distractors and correct answers [145].
With the rapid development of deep learning, the construc-
tion of distractors can be divided into two lines of main-
stream: generation-based and ranking-based categories [46].
Generation-based methods automatically generate distrac-
tors token by token and ranking-based methods regard
DG as a ranking task for a pre-defined distractor candi-
date set. Hence, O could be either generated or selected
via generation-based or ranking-based models. Noting that
there are some key challenges of DG to make a good
distractor.

e The produced distractors are irrelevant to the ques-
tions. Some studies [45], [46], [47] focus on generating
distractors that are consistent with the context or the
answer. Gao et al. [48] proposed a hierarchical model
with dual attention for sentence and word relevance and
irrelevant information filtering to generate distractors that
are semantically consistent with and traceable in the con-
text. Ding et al. [49] propose a multi-modal DG model
integrating contrastive learning to ensure consistency and
have high relevance to the contexts.

e The distractors lack strong distractive effectiveness. A
good distractor should be misleading [50], with high
similarity to the correct answer and distinguishability
from other distractors. Taslimipoor et al. [51] proposed
a two-step approach using transfer learning to generate
correct answers and distractors simultaneously, followed
by clustering to classify and remove duplicates to avoid
excessive similarity among distractors. Qu et al. [52] pro-
posed an unsupervised DG framework using distillation
from LLMs and contrastive decoding to avoid excessive
similarity between generated distractors.

Our investigation reveals that DG, as a significant com-
ponent of QG, has not been fully explored. More discussion
on constructing DG datasets, improving evaluation strate-
gies, and generating “good” distractors should be learned.

3.3 Automated Assessment

Automated assessment is a widely-investigated task in the
education domain, which is helpful in reducing the burden
of teachers in various writing tasks. Regarding different
objects to be scored, different criterion is considered. For
instance, in Figure 6, scoring an essay in a language curricu-
lum requires the measurement of particular essay quality
such as persuasiveness and organization. For a piece of
code in the program curriculum, it is more important to
measure the correctness of the code, the completeness of
the comments, and quality issues. Therefore, we discuss the


JOURNAL OF ITeX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
Dataset Essay Types Source Language Exp #Q Top 1 Top 2 Top 3
SESE grades 12, ‘Bnelsh holistic TAA] —MTL-Vanilla (55].—_ROBERT
HSKA A/N the 3 Chinese holistic UE Onaies frogs ce :

TABLE 3: Comparison of AES datasets. The essay types contain: argumentative (A), response (R), narrative (N), comment
(C), suggestion (5) and letter (L). “Exp” denotes the type of scores annotated. “#Q” denotes the number of questions. We
display most datasets with holistic Score AccuracyA except that ICLE++ is displayed with prompt adhere /organization/-

cohesion Score Accuracyy.

Automated Essay Scoring Automated Code Scoring

Code:

Text: Laughter is the @CAPS1 Laughter is
# Match String

an incredible thing. Laughter can bring
families together, it can bring adversaries
together, but most importantly, it can
bring strangers together and start a
wonderful or terrible relationship.

t=r..”

match = re.match(t, string) ...
print(f"Full number: "
f"{match.group(1)}" )
Assessment:

* Persuasiveness: agree somewhat
* Organization: disagree somewhat

Assessment:
* Quality issue: f"{match.group(1)}” )

FIGURE 6: The examples of AES and ACS tasks. The
displayed examples are extracted from ASAP dataset and
paper [180], respectively.

techniques for Automated Essay Scoring (AES) and Auto-
mated Code Scoring (ACS) in the following subsections.

3.3.1 Automated Essay Scoring

AES aims to develop a system to score an essay automat-
ically, where the written text is given as the input and the
system summarizes its quality as a score:

(B,s)eDPo(s|f(E)), (1)

where s is a holistic score and f(£) is a set of features
derived from an essay E. The challenge of this task lies
in understanding the content of the essay by modeling the
different facets of the text.

Datasets. The Cambridge Learner Corpus-First Certificate
in English exam (CLC-FCE) [181] is a widely used dataset
for AES tasks. Usually, a dataset is annotated with the
prompt of the essay, which indicates the topic of the es-
say. Moreover, it provides both the holistic score and the
manually tagged linguistic error types such that it makes
possible to evaluate the instructional feedback of the de-
veloped AES systems. The Automated Student Assessment
Prize (ASAP) is a Kaggle competition released for AES. A
similar dataset ELLIPSE [182] contains essays from English
language learners of the diverse difficulty levels. Instead of
holistic scores, scores along different dimensions of essay
quality are needed. International Corpus of Learner English
(ICLE++) [174] is a dataset containing essays annotated with
the perspectives of structure organization, thesis clarity,
prompt adherence, and argument persuasiveness. There are
a few AES datasets, which are designed for other languages
such as Chinese [179]. We display the details of the above
datasets in Table 3.

Lars = —

Methods. The majority of the AES studies usually predict a
single score for an essay. In Equation (1), P9(-) can be for-
mulated as a regression task, a classification task, a ranking
task or their hybrid [108], [183]. A series of features such
as length-based features [184], lexical features [184], [185],
prompt-relevant features [186], readability features [184],
syntactic features [184], [187] and so on, are first extracted
from the essay, then off-the-shelf learning algorithms are
leveraged to fit the annotated scores [188]. Besides the hand-
crafted features, f(£’) can be modules automatically captur-
ing the document structure [176], [189]. In addition, there
are some focuses existing in the AES tasks for education.

e The singular evaluation of the essays lacks comprehen-
siveness. Multiple dimensions are considered in a few
studies such as cohesion, syntax, vocabulary, phraseology,
grammar, and conventions [53], [54], which are able to
construct more diverse evaluation metrics. To enrich the
evaluation of the AES task, researchers utilize LLMs to
provide a more comprehensive and explainable evalua-
tion of an essay [55]. With the trait-specific score, the AES
system becomes multi-tasking learning:

Lars = —Ecn,s)ep >. Po(s*|£(E)),

skes

where s* measures different aspects of the essays.

e The sparse annotation of the essays in domain-specific
prompts. Cross-prompt AES aims to rate unseen target
essays with annotated source prompts. Multiple stud-
ies [56], [57], [58] design neural networks decoupling the
domain-dependent and domain-independent features, to
facilitate the models to learn transitions (s — t) from
different domains: t = s x TT, where T is a matrix
modeling the transition from source prompts to target
prompts.

AES also faces the unsolved issues of object-oriented and

scenarios-oriented scoring. Specifically, the essay scores of

K12 learners, second-language learners and businessmen

are different. Maybe a more fine-grained scoring paradigm

or framework should be developed.

3.3.2 Automated Code Scoring

Similar to AES, ACS grades the score of a code snippet on
various dimensions. Compared with essay grading, code
grading is more complicated in understanding the long-
dependency inside a code snippet and its execution effect
could be a vital factor to judge.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Datasets. We notice that there are few publicly-available
ACS datasets due to the privacy issue. The source data con-
tain submissions in programming courses that can be col-
lected on the academy platforms such as JetBrains Academy,
MOOC, tutoring sessions and so on [180], [190], [191].

Methods. Code can be scored from different objectives,
which is vital in judging the quality of a code snippet.
Intuitively, by executing on a set of test cases, the execution
results are supposed to be the same as the excepted results.

{pass, fail} = Compiler(C, case),

where C’' is a piece of code snippet. In this way, many

dynamic analysis tools for detecting code quality issues are

developed, such as AutoStyle [192], WebTA [193], Refactor

Tutor [194] and Hyperstyle [190]. However, it is challenging

to score a code snippet without executing it.

e It is hard to identify good features and scoring al-
gorithms. The static features include complexity, com-
ments, number of lines, number of operators, number
of operands, and so on. The static analysis approaches
leverage these features to identify the quality of code [59].
Machine learning algorithms also have been applied to
ACS. Clustering, breadth-first search, and neural net-
work approaches are utilized to track the logic errors
in code [60], [61], [62]. Besides, Queiros et al. [63] pro-
posed hybrid approaches combining static and dynamic
approaches, which can be formulated as follows:

Lacs = —E(c,s)ep Pa(s|f(C)),

where C is the code snippet and s is the annotated score
for supervised training.

Implicitly encode the code snippet via distributed rep-
resentation. Instead of directly scoring a code snippet
based on the extracted features, there are a series of
studies focusing on evaluating the contextual embedding
of a code snippet, which benefits measuring the distance
between the evaluated snippet and the correct snippet
for ACS [64]. The embedding of variable and method
identifiers utilizes local and global context [65], Abstract
Syntax Trees (AST) [66], and AST paths [67], [68]. Later,
Feng et al. [69] and Kanade et al. [70] proposed CodeBERT
and CuBERT, respectively. They model the code beyond
simple tokenization:

EceDPo(Cm|C\m)-

Similar to language models, the pre-training aims to learn
the pattern of code language.

L PreTrain — —

Even though a great number of systems have been
developed for ACS, there still remains an unsolved question
to develop adaptive and fine-grained scoring paradigms for
ACS in various scenarios.

3.4 Error Correction

Compared with AA, EC requires systems to provide ex-
plicit edits and corresponding revisions to the errors. Sim-
ilarly, language learning, as an important educational sce-
nario, contains applications of Grammatical Error Correc-
tion (GEC) and Code Error Correction (CEC) for natural

8. https: //gigaom.com/2012/01/19/quixey-challenge/

Code Error Correction

Source Code:
Public Integer getMinElement (List myList){
if (myList.size() >= O){
return ListManager.getFirst(myList);}
return 0;}

Grammatical Error Correction

Source Sentence:
Travel by bus is expensive, bored
and annoying.

Target Code:
Public Integer getMinElement (List myList){
if (myList.size() >= 1)f
return ListManager.min(myList);}
return null;}

Target Sentence:
Travelling by bus is expensive,
boring and annoying.

FIGURE 7: The examples of GEC and CEC tasks. The
examples are extracted from BEA-2019 and Bugs2Fix, re-
spectively.

language learning and machine language learning. In terms
of the techniques of GEC and CEC tasks, the CEC differs
from GEC due to their different syntax. And we display two
typical examples of them in Figure 7.

3.4.1. Grammatical Error Correction

GEC tasks are designed to identify and correct grammatical
errors in text, including spelling errors caused by phonolog-
ical confusion and visual confusion, as well as grammatical
errors caused by incorrect use of grammatical rules.

Lesc = —E(x,y)ep log Pa(Y | X)

Here X and Y are the source sentence and target sentence,
respectively. For the GEC task, how to correctly identify and
correct grammar errors is difficult, and how to avoid over-
correction is challenging.

Datasets. Lang-8 is one of the most widely used datasets
for language error correction, covering 80 languages, but its
distribution of languages is very skewed, with Japanese and
English being its most prevalent languages [207]. CLANG-
8 [207] is a clean dataset based on Lang-8, fixing some
mismatches between data sources and targets in Lang-8.
CoNLL-2014 [231] and BEA-2019 [232] are both commonly
used English language error correction datasets that refine
different types of syntax errors. In addition, with the popu-
larity of GEC tasks, more and more non-English datasets
have been proposed and widely used by researchers.
SIGHAN [233] and CCTC [198] are Chinese GEC datasets in-
troduced to support more CGEC research. The FCGEC [201]
and FlaCGEC [204] datasets have added more linguistic
annotations, introduced more granular grammatical rules,
and covered 28 and 210 error types, respectively, presenting
new challenges to the GEC task. At the same time, there
are many GEC datasets for low-resource languages. There
are datasets GECCC [209] in Czech, RULEC-GEC [234] in
Russian, Falko-Merlin [235] in German, COWS-L2H [236] in
Spanish, UA-GEC [237] in Ukrainian and RONACC [238] in
Romanian.

Methods. Early methods to handle GEC tasks are
mostly achieved by manually defining rules or build-
ing classification-based models [239], [240]. With the
widespread use of neural networks, Seq2seq [241] model
has become the mainstream method to solve the task. It
treats grammatical error correction as a monolingual ma-
chine translation task, in which source sentences containing


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

Dataset Language Source Exp #Q Top 1 Top 2 Top 3
LANG-8 eo lengua ses) LANG-8 Website x 1M - - -
CoNLL-2014* En NUCLE [195] Y 58K weyomte ps emcee ESC 497]
CCTCs Zh Tatvcopeke, YK | CON Ia GECTOR POO} SpellGCN HOR)
FCGEC* Zh Chinese examinations ¥ 41K | yy oo lI. rot pa Ue ean tee
FlaCGEC« Zh HSK [203] YK pBcEC (s] Brrr GEGIO PHOT
Falko-MERLIN« Ge MET IkoEssayWhig “24K 15 [207] BIFT [208] ‘AG 09]
COWS:L2H ES Essays YK mn [210] Artificial [DU] MT pia}
RONACC RO TV and radioshows / 10K REP Ha aR Ceu pt 4 i Ane 5]
Defects4JA Java Open source programs = - 357 chika Reet oa TENURE enon aia
ManyBugs4 Cc Open source programs = - 185 sUGReveie _ neers is pans
IntroClasse c Student programs - 998 | Ceres looa) ——TrpautoRepsir [225] FLEXIREPAIR [223]
OuixBupen Java, Eytiion Ouixey Challenge - a0 Tiinensir an eee eaer role) ae ey
Bagel Java eae ~ 26M Aste Bee coaeis Bae PCa
CodeReviewt (9 languages! Github ~ (642K Ceild8 etter ee [228] Cidleltedleoree [229] cease
CadéReview: Newt Multi CodeReview, _ 15K 19.5 [230] 14.8 [230] -

* (16 languages) code reviews ChatGPT CodeReviewer [229]

TABLE 4: Comparison of GEC and CEC datasets. “Exp” denotes whether the annotation contains the explicit error types.
“#O” denotes the number of sentences in the dataset. The evaluation metric is Fo,;* score for GEC. For CEC, the metrics are
Exact Match} for Bugs2Fix and CodeReview-New, Number of Correct PatchesA for Defects4J, ManyBugs and QuixBugs,

Number of Plausible Patches for IntroClass, and BLEU-4t for CodeReview.

grammatical errors are tagged one by one to be translated
into the correct target sentences. The Seq2seq methods are
mainly built based on RNN, CNN, or Transformer frame-
work. BART [140], T5 [242], EBGEC [205], SynGEC [243]
and CSynGEC [244] are good models in practice. After that,
Seq2edit [245] methods have gradually emerged. It treats
the GEC task as a sequential labeling problem, marking the
text spans with appropriate error tags, leaving the rest of
the text unchanged, and generating one edit operation per
prediction. The commonly used efficient Seq2edit models
include PIE [246], EditNTS [247], LaserTagger [248], GEC-
ToR [200] and so on. Recently, the TemplateGEC [249] model
proposed by Li et al. integrates the two frameworks of
Seq2seq and Seq2edit, making use of their ability in error
detection and correction, alleviating the problem of over-
correction of seq2seq model to a certain extent, enhancing
the effectiveness and robustness of the model. While GEC
methods have achieved high performance, it still faces some
problems and challenges.

e GEC with LLMs encounters over-correction issue. Even
though LLMs are able to solve GEC tasks in nature, it has
an over-correction issue, which hinders its application in
the education domain since teachers are meant to correct
the sentences with minimum edits. Some studies change
the role of LLMs in GEC, utilizing them to improve the
performance of the smaller model [71]. Also, some studies
focus on addressing the over-correction issue in LLMs
directly. Wang et al. [72] used cross-inference to generate a
set of correction candidates and specifically train a model
for soft ensemble within the candidate set to avoid over-

correction. Omelianchuk et al. [73] used multiple models
for voting to prevent over-correction in the output of a
single model.

Y= Ensembley, ~ p,(y,|x)(Yi)-

e Distorted results of GEC systems in low resources.
For language teaching over the world, the development
of GEC systems for low-resource languages is underde-
veloped. Many studies focus on applying current GEC
methods to low-resource languages, such as Bangla [250],
Slovak [251], Turkish [252], Zarma [253] and so on. Chan et
al. [74] proposed a method for constructing multilingual
data to handle mixed-language text, which significantly
improves the performance of GEC systems.

Previous studies have not considered the GEC with visual
features, where hand-written essays are submitted by stu-
dents to be corrected. Besides, the evaluation criterion of
GEC following minimum edits should be reconsidered in
real-world since different corrections maybe acceptable.

3.4.2 Code Error Correction

Code Error Correction (CEC) fixes a buggy code snippet to
make it error-free and more coherent. It is a challenging task
as it requires the model to have a sufficient understanding
of the code in long-distance dependency and make proper
corrections to the erroneous code spans. It shares similar
principles as GEC tasks and we can formulate both of them
as neural machine translation [254], [255]:

Leec = —E(x,y)ep log Po(Y | X)



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Here, X and Y are the input and output code sequences.

Datasets. Defects4J [256], ManyBugs [257] and Intro-

Class [257] are some of the earlier datasets proposed for

CEC tasks, focusing on Java and C languages respectively,

but with small amounts of data. Lin et al. [258] proposed

the first multi-lingual parallel corpus of CEC benchmarks,
namely QuixBugs [258], which contains 40 buggy code

snippets translated into both Java and Python. Xia et al. [259]

combined the aforementioned Defects4J, ManyBugs, and

QuixBugs datasets into a single CEC evaluation dataset

to test the effect of PLM on the CEC task. Bugs2Fix [260]

is a dataset mined from GitHub bug-fixing commits, con-

taining 2.3M bug-fix pairs (BFPS). CodeReview [229] and

CodeReview-New [230] are also large-scale datasets of code

reviews collected from open-source projects, involving nine

of the most popular programming languages.

Methods. Existing techniques capture the structural and se-

mantic information of code by representing it as a sequence

of symbols or an Abstract Syntax Tree (AST) [261], which
involves some unique challenges for CEC tasks.

e The structural information of code has been ignored.
Both CuBERT [70] and CodeBERT [262] ignore the struc-
tural information of code. In response to this issue, Guo et
al. [75] proposed the GraphCodeBert model to represent
source code information based on AST data flow, so
as to reflect deeper code information. GraphCodeBERT
adapts Transformer and allows the model to learn the AST
data flow. The follow-up Unixcoder [76] improves Graph-
CodeBERT by converting an AST into a sequence struc-
ture. PLBART [77] incorporates the denoising strategies
to better understand program grammar and logical flows.
CodeT5 [227] uses the T5 model architecture and makes
use of the identifier tagging and comment information to
better model the structural characteristics of code.

e CEC systems have resource overhead issue. CEC sys-
tems’ high demands for memory and computing power
may disrupt the user’s workflow and decrease produc-
tivity [79]. CodePAD [78] designs an approach based on
pushdown automaton (PDA), which takes into account
the grammar constraints of programming languages and
reduces the unnecessary computational cost.

e CEC systems encounter overfitting issue Overfitting
refers to the issue where generated patches only pass the
developer-written test suite but fail to generalize to other
potential test cases [80]. Nilizadeh et al. [81] introduced
a method that identifies correct patches based on the
similarity of test case execution between the buggy and
patched programs, which reduces overfitting by evaluat-
ing patches across a broader range of execution scenarios.
Ye et al. [82] proposed an overfitting detection system,
which leverages static code features, such as variable
types, code structure, and contextual syntax information,
to classify patches as overfitting or correct.

The performance and memory load of CEC systems could

be improved more with the consideration of the informal

and rigorous teaching environment for coding teaching.

4 DEMONSTRATION SYSTEMS

To facilitate applications on education scenarios, in this
section, we present some off-the-shelf demo cases with

12

brief illustrations of their usage for each task mentioned
in Section 3. Information on these demos will be presented
in Table 5. We annotate demonstrations with “platform”,
“application”, “toolkit” and “extension”. Teachers and stu-
dents could utilize platforms and extensions featured with
interfaces and user instructions as AI helpers. Researchers
could utilize toolkits and applications featured with de-
velopment frameworks and flexible modules to implement
new methods quickly.

For QA, OpenVINO?’ is an open-source toolkit designed
for diverse automatic QA, including table, visual, interac-
tive, and LLMs-based QA. PrimeQA [263] is an open-source
public repository designed to democratize QA research and
simplify the replication of state-of-the-art methods, sup-
porting the training of multilingual QA models with core
functionalities like information retrieval, reading compre-
hension, and question generation. TableQAKit [264] is an
open-source toolkit designed for table-based QA, offering a
unified platform with various datasets, methods, and LLM
integrations. PiggyBack [265] is a user-friendly VQA plat-
form that simplifies the use of visual-language pre-trained
models with a browser-based interface and comprehensive
task support. Besides, other QA demos [266], [267], [268] are
also available for both educators and researchers.

For MWP, MWPToolkit [269] offers a holistic and exten-
sible framework that provides deep learning-based solvers,
popular datasets, and a modular architecture that supports
quick replication and innovation of MWP methods. Int-
Math’° is an online math platform that combines a math-
ematical computation engine with artificial intelligence to
parse and generates step-by-step natural language answers.
MWPRanker [270] is a tool to retrieve similar math word
problems based on expression tree similarity.

For QC, AutoQG [271] is a service that provides multilin-
gual question-and-answer generation, featuring a compre-
hensive toolkit for model fine-tuning, generation, and eval-
uation. SAQUET [272] proposes a toolkit to assess the struc-
tural and pedagogical quality of MCQs. AnswerQuest [273]
is a system that integrates QA and QG to create O&A items
for enhancing reading comprehension of multi-paragraph
documents. Besides, other QC demos [274] are also available
for both educators and researchers.

For AES and ACS, LinggleWrite [275] is a writing coach
tool that offers writing suggestions, proficiency assessments,
grammatical error detection, and corrective feedback. In-
telliMetric’’ is an online multilingual AES platform that
supports writing evaluation by delivering instant scores
with the accuracy and consistency of human experts. EX-
PATS [276] is an open-source framework for AES, enabling
rapid experimentation with various models, while inte-
grating visualization and interpretability tools. Qualified’
and CoderPad?’ are online platforms designed for coding
assessments, offering interactive environments to evaluate
coding skills.

9. https: / /docs.openvino.ai/2023.3 /notebooks_section_2_model_
demos.html

10. https://www.intmath.com/

11. https://www.intellimetric.com/direct/

12. https://www.qualified.io/

13. https://coderpad.io/


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
Task Demos
QA OpenVINO + PrimeQA f+ TableQAKit f PiggyBack * ASCENTo UKP-SQUAREo LOCALRQA *
MWP MWPToolkit + IntMath o MWPRanker *
QC AutoQG * SAQUET + AnswerQuest > Leaf *
AA LinggleWrite + IntelliMetrico EXPATS 7 Qualified © CoderPad ©
GEC  ALLECS «© Grammarly o =ERRANT + WAMP «x GECko+ * Effidit o
CEC  CodeGeeX A CodeHelp iFlyCode o MarsCode A

TABLE 5: Demos that can be applied in education domain. ©: platform, *: application, }: toolkit, A: extension. We regard a
platform as a tool that can be directly used by educators, while an application is a tool that requires additional setup and
deployment before it can be utilized. A toolkit is an extensible collection of tools, functions, and resources designed for
specific tasks. An extension is a tool that exists in the form of a plugin.

For GEC and CEC, ALLECS [277] and Grammarly'*
are web applications that provide online GEC service by
offering lightweight GEC systems optimized for users with
slow internet connections and comprehensive writing as-
sistance, respectively. Errant [278] is a toolkit that extracts
and classifies edits from parallel sentences, enabling error
type evaluation and dataset standardization. WAMP [279]
is a web-based annotation tool designed to efficiently create
annotated corpora for GEC, with features like customizable
error tags and file export for system evaluation. Other
demos [280], [281] can also be applied for both researchers
and educators. CodeGeeX [282] is an open-source and mullti-
lingual pre-trained model that enables syntax and function-
correct code generation and can be integrated as IDE exten-
sions. CodeHelp [283] is an LLM-powered assistant that pro-
vides real-time programming guidance and explanations,
helping students solve coding issues independently without
giving direct solutions. MarsCode [284] is an LLM-based
multi-agent framework for automated bug fixing.

5 CHALLENGES AND FUTURE TRENDS

In this section, we will discuss the challenges and future
trends when applying NLP technology in real educational
scenarios.

Generalization over subjects and languages. Even though
there is a range of datasets from question answering to
error correction, we also observe there is still a demand
for datasets and techniques fitting in more subjects and
languages. For example, for AES and GEC tasks, there is
a lack of AES and GEC datasets in non-English languages,
which is vital for language education in non-English coun-
tries. For CEC tasks, we found limited datasets for Python
language, which is a widely used programming language
today. Similarly, applications on different subjects are vital
in extending the NLP techniques to education. Even though
there is a series of datasets covering the subjects of grades
6-8 or K-12, for higher education, which includes some
advanced subjects like philosophy, and history, we still face
a shortage of data. In the future, a comprehensive dataset
is desired to cover subjects of different learning stages and
different tasks for both teaching and research.

Deployed LLM-based systems for education. Even though
an individual task can be well-formulated and solved via
specific techniques, there is a demand for integrated systems
for education. Baladn et al. [285] tuned open-source LLMs
for generating teacher responses in BEA 2023 Shared Task.

14. https://www.grammarly.com/

EduChat [286] is a more general LLM in the education do-
main, which is able to automatically assess essays, provide
emotional support, and conduct Socratic teaching with the
forms of chatting and question answering. In the future, an
integrated LLM-based tutoring system should be featured
with stronger capabilities like answering textbook ques-
tions, and automated quiz-making in the dimension of dif-
ferent subjects such as math, computer science, linguistics,
and so on. Regarding system deployment, although open-
source NLP technology demos, including toolkits, online
platforms, and applications, have been widely developed
(see Section 4). The limited hardware and resources make it
challenging to apply advanced NLP in education, prompt-
ing studies focused on improving feasibility in this area.
For example, studies [287], [288] of MWP solving have
shown that applying knowledge distillation can signifi-
cantly reduce model size and improve inference speed while
maintaining high solution accuracy. Further optimization is
needed considering the edge computing and efficiency.

Adaptive learning for teaching and learning. Even though
NLP applications can be applied to the education domain,
there are a few studies involving adaptive learning. In the
QG task, we notice some researchers [43], [44] attempt to
model the learning trajectories of students by incorporating
their historical answers to predict their performance on the
new questions, which are deemed as a significant feature
to determine the difficulty level of the next question. This
helps to build a more customized education system. Such
adaptive learning is in high demand to collaborate with
other tasks. For example, in EC tasks, when we correct the
sentences and codes, we can consider the learning trajectory
of the students to decide the way to correct them. If it
is a simple sentence with a misused word, we should
not substitute it with an obscure word. Besides modeling
the learning trajectories, the intervention of difficulty-level
control mechanisms should be integrated into the education
system. Even though there are multiple studies [34], [35],
[36] trying to model the difficulty level in the QG task, they
fail to align the level to the difficulty level of the syllabus.

Interpretability for education. Existing studies are able to
obtain good results on various tasks like question answer-
ing, question construction, automated assessment, and error
correction. However, there are limited techniques developed
with the perspective of interpretability. As shown in the
technical review of QA tasks, recent studies try to expli-
cate the process of thinking in solving TQA and MWPs
tasks [27], [289]. In other tasks, interpretability should also
be taken into account. It would be promising to show a


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

thinking path and make the construction of the questions
more rational. For AS and EC tasks, showing a fine-grained
explanation of the scoring and correction would not only
improve the tutoring experience but also benefit the meth-
ods as well as systems to diagnose their functions. Even
though we notice a few studies have started to develop
interpretable GEC systems [204], interpretability should be
further studied and explored with more attention.

Ethical considerations of NLP techniques. Applying NLP
techniques for education raises various ethical issues, such
as fairness, safety, privacy, and dependency. Li et al. [290]
systematically introduce the fairness of LMs. As noted by
Chinta et al. [291], racial bias in Al-driven essay scoring
systems has resulted in darker-skinned students receiving
lower scores than other students for essays of compara-
ble quality. Regarding privacy concerns, when students
use third-party generative AI tools for assignments or re-
flections, they may unintentionally expose sensitive aca-
demic data, risking privacy law violations. These tools
lack transparency around how data is stored, processed,
or reused [292]. To address this issue, researchers [293],
[294] have shown that federated learning protects student
privacy by training models locally on sensitive data and
sharing only model updates, enabling collaborative model
development without exposing raw data. For dependency,
Krupp et al. [295] indicate an overreliance on ChatGPT
among prospective physics teachers and students, affecting
their use of real-world contextualization and leading to
poorer results in complex tasks. Multiple education-related
studies also attempt to reduce educators’ or students’ over-
reliance on current techniques [283], [296].

6 CONCLUSIONS

This survey attempted to provide a comprehensive
overview of NLP in the education domain. We highlight
that NLP techniques can be applied to various procedures
of education. Our survey highlights four representative
tasks (i.e., question answering, question construction, au-
tomated assessment, and error correction) with their fine-
grained sub-tasks as the focus of this survey due to their
significant impact on teaching and learning. According to
the taxonomy, we go deep into these tasks and illustrate
the evolution of their techniques. Despite the strides made
in NLP applications in the education domain, there still
remains room for the improvement of diverse tasks. In the
last section, we suggest some future directions for NLP in
the education domain.

We would like to note that besides the highlighted tasks
in this survey, there are other studies that can contribute to
the educational scenarios. For example, knowledge graph
construction [84] aims to organize and extract the knowl-
edge points from the unstructured data sources to form a
knowledge graph, which is able to demonstrate the connec-
tion between the schema for various subjects and support
downstream applications. In summary, the demand for edu-
cational applications is increasing and educational NLP will
thrive to be a promising research area. We hope this survey
will give a comprehensive picture of educational NLP and
we encourage more contribution in this field.

14

REFERENCES

[1] P.Lu,S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord,
P. Clark, and A. Kalyan, “Learn to explain: Multimodal reasoning
via thought chains for science question answering,” in NeurIPS,
2022.

[2] |W. Chen, M. Yin, M. Ku, E. Wan, X. Ma, J. Xu, T. Xia, X. Wang,
and P. Lu, “Theoremqa: A theorem-driven question answering
dataset,” arXiv preprint arXiv:2305.12524, 2023.

[3] 5S. Wu, Y. Li, D. Zhang, Y. Zhou, and Z. Wu, “Diverse and infor-
mative dialogue generation with context-specific commonsense
knowledge awareness,” in ACL, 2020.

[4] J. Yuand J. Jiang, “Learning sentence embeddings with auxiliary
tasks for cross-domain sentiment classification,” in EMNLP, 2016.

[5] Y. Lan, G. He, J. Jiang, J. Jiang, W. X. Zhao, and J.-R. Wen,
“Complex knowledge base question answering: A survey,” IEEE
TKDE, 2023.

[6] C. Bryant, Z. Yuan, R. Muhammad, Q. Qorib, H. Cao, H. Ng, and
T. Briscoe, “Grammatical error correction: A survey of the state
of the art,” Computational Linguistics, 2023.

[7] D. Zhang, L. Wang, L. Zhang, B. T. Dai, and H. T. Shen, “The gap
of semantic parsing: A survey on automatic math word problem
solvers,” IEEE TPAMI, 2019.

[8] M. Messer, N. C. C. Brown, M. Kolling, and M. Shi, “Automated
grading and feedback tools for programming education: A sys-
tematic review,” TCE, 2023.

[9] Q.Li, L. Fu, W. Zhang, X. Chen, J. Yu, W. Xia, W. Zhang, R. Tang,
and Y. Yu, “Adapting large language models for education: Foun-
dational capabilities, potentials, and challenges,” arXiv preprint
arXiv:2401.08664, 2023.

[10] S. Wang, T. Xu, H. Li, C. Zhang, J. Liang, J. Tang, P. S. Yu, and

Q. Wen, “Large language models for education: A survey and

outlook,” arXiv preprint arXiv:2403.18105, 2024.

[11] O. Ram, Y. Kirstain, J. Berant, A. Globerson, and O. Levy, “Few-

shot question answering by pretraining span selection,” arXiv

preprint arXiv:2101.00438, 2021.

[12] W. Xu, X. Li, W. Zhang, M. Zhou, L. Bing, W. Lam, and

L. Si, “From clozing to comprehending: Retrofitting pre-trained

language model to pre-trained machine reader,” arXiv preprint

arXiv:2212.04755, 2022.

[13] S. Lee, D. Kim, and J. Park, “Domain-agnostic question-

answering with adversarial training,” 2019.

[14] J. Poco and J. Heer, “Reverse-engineering visualizations: Recover-

ing visual encodings from chart images,” Computer graphics forum,

2017.

[15] K. Kafle, B. Price, S. Cohen, and C. Kanan, “Dvqa: Understanding

data visualizations via question answering,” in CVPR, 2018.

[16] Y. Wang, X. Ma, and W. Chen, “Augmenting black-box llms with

medical textbooks for clinical question answering,” arXiv preprint

arXiv:2309.02233, 2023.

[17] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,”

arXiv preprint arXiv:2304.08485, 2023.

[18] L. Wang, Y. Hu, J. He, X. Xu, N. Liu, H. Liu, and H. T. Shen, “T-

sciq: Teaching multimodal chain-of-thought reasoning via large

language model signals for science question answering,” AAAI,

2024.

[19] L. Wang, Y. Wang, D. Cai, D. Zhang, and X. Liu, “Translating a

math word problem to a expression tree,” 2018.

[20] T.-R. Chiang and Y.-N. Chen, “Semantically-aligned equation

generation for solving and reasoning math word problems,”

arXiv preprint arXiv:1811.00720, 2019.

[21] Q. Liu, W. Guan, S. Li, and D. Kawahara, “Tree-structured

decoding for solving math word problems,” in EMNLP-IJCNLP,

2019.

[22] Z. Xie and S. Sun, “A goal-driven tree-structured neural model

for math word problems,” in IJCAI, 2019.

[23] J. Zhang, L. Wang, R. K.-W. Lee, Y. Bin, Y. Wang, J. Shao, and E.-P.

Lim, “Graph-to-tree learning for solving math word problems,”

in ACL, 2020.

[24] X. Lin, Z. Huang, H. Zhao, E. Chen, Q. Liu, H. Wang, and

S. Wang, “Hms: A hierarchical solver with dpendency-enhanced

understanding for math word problem,” AAAI, 2021.

[25] Y. Hong, Q. Li, D. Ciao, S. Huang, and S.-C. Zhu, “Learning

by fixing: Solving math word problems with weak supervision,”

AAAI, 2021.

[26] J. Zhang, R. K.-W. Lee, E.-P. Lim, W. Qin, L. Wang, J. Shao, and
Q. Sun, “Teacher-student networks with multiple decoders for
solving math word problem,” in IJCAI, 2020.



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]
[44]

[45]

[46]

[47]

[48]

[49]

[50]

J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,
E. Chi, Q. Le, and D. Zhou, “Chain-of-thought prompting elicits
reasoning in large language models,” NeurIPS, 2023.

L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-
P. Lim, “Plan-and-solve prompting: Improving zero-shot chain-
of-thought reasoning by large language models,” arXiv preprint
arXiv:2305.04091, 2023.

Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain
of thought prompting in large language models,” arXiv preprint
arXiv:2210.03493, 2022.

D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuur-
mans, O. Bousquet, Q. Le, and E. Chi, “Least-to-most prompting
enables complex reasoning in large language models,” arXiv
preprint arXiv:2205.10625, 2022.

C. Zheng, Z. Liu, E. Xie, Z. Li, and Y. Li, “Progressive-hint
prompting improves reasoning in large language models,” arXiv
preprint arXiv:2304.09797, 2023.

P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu,
S.-C. Zhu, and J. Gao, “Chameleon: Plug-and-play composi-
tional reasoning with large language models,” arXiv preprint
arXiv:2304.09842, 2023.

W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of
thoughts prompting: Disentangling computation from reasoning
for numerical reasoning tasks,” TMLR, 2023.

Y. Gao, L. Bing, W. Chen, M. R. Lyu, and I. King, “Difficulty
controllable generation of reading comprehension questions,”
arXiv preprint arXiv:1807.03586, 2018.

Y. Cheng, S. Li, B. Liu, R. Zhao, S. Li, C. Lin, and Y. Zheng,
“Guiding the growth: Difficulty-controllable question generation
through step-by-step rewriting,” arXiv preprint arXiv:2105.11698,
2021.

M. Uto, Y. Tomikawa, and A. Suzuki, “Difficulty-controllable
neural question generation for reading comprehension using
item response theory,” in BEA, 2023.

M. Forehand et al., “Bloom’s taxonomy: Original and revised,”
Emerging perspectives on learning, teaching, and technology, 2005.

U. Lee, H. Jung, Y. Jeon, Y. Sohn, W. Hwang, J. Moon, and H. Kim,
“Few-shot is enough: exploring chatgpt prompt engineering
method for automatic question generation in english education,”
Education and Information Technologies, 2024.

S. Maity, A. Deroy, and S. Sarkar, “Harnessing the power of
prompt-based techniques for generating school-level questions
using large language models,” in FIRE, 2024.

T. Steuer, A. Filighera, T. Meuser, and C. Rensing, “I do not
understand what i cannot define: Automatic question genera-
tion with pedagogically-driven content selection,” arXiv preprint
arXiv:2110.04123, 2021.

A. Hadifar, S. K. Bitew, J. Deleu, V. Hoste, C. Develder, and
T. Demeester, “Diverse content selection for educational question
generation,” in EACL, 2023.

C. Xiao, S. X. Xu, K. Zhang, Y. Wang, and L. Xia, “Evaluating
reading comprehension exercises generated by llms: A showcase
of chatgpt in education applications,” in BEA, 2023.

M. Srivastava and N. Goodman, “Question generation for adap-
tive education,” arXiv preprint arXiv:2106.04262, 2021.

Y. Wang and L. Li, “Difficulty-controlled question generation in
adaptive education for few-shot learning,” in ADMA, 2023.

S. K. Bitew, J. Deleu, C. Develder, and T. Demeester, “Distractor
generation for multiple-choice questions with predictive prompt-
ing and large language models,” arXiv preprint arXiv:2307.16338,
2023.

J. Wang, W. Rong, J. Bai, Z. Sun, Y. Ouyang, and Z. Xiong, “Multi-
source soft labeling and hard negative sampling for retrieval
distractor ranking,” IEEE TLT, 2023.

H.-C. Yu, Y-A. Shih, K.-M. Law, K.-Y. Hsieh, Y.-C. Cheng, H.-
C. Ho, Z.-A. Lin, W.-C. Hsu, and Y.-C. Fan, “Enhancing dis-
tractor generation for multiple-choice questions with retrieval
augmented pretraining and knowledge graph integration,” 2024.
Y. Gao, L. Bing, P. Li, I. King, and M. R. Lyu, “Generating
distractors for reading comprehension questions from real ex-
aminations,” in AAAI, 2019.

W. Ding, Y. Zhang, J. Wang, A. Jatowt, and Z. Yang, “Can we
learn question, answer, and distractors all from an image? a new
task for multiple-choice visual question answering,” in LREC-
COLING, 2024.

E. Alhazmi, Q. Z. Sheng, W. E. Zhang, M. Zaib, and A. Al-
hazmi, “Distractor generation in multiple-choice tasks: A sur-

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]
[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

15

vey of methods, datasets, and evaluation,” arXiv preprint
arXiv:2402.01512, 2024.

S. Taslimipoor, L. Benedetto, M. Felice, and P. Buttery, “Distractor
generation using generative and discriminative capabilities of
transformer-based models,” in LREC-COLING, 2024.

F. Qu, H. Sun, and Y. Wu, “Unsupervised distractor genera-
tion via large language model distilling and counterfactual con-
trastive decoding,” arXiv preprint arXiv:2406.01306, 2024.

I. Persing and V. Ng, “Modeling thesis clarity in student essays,”
in ACL, 2013.

, “Modeling prompt adherence in student essays,” in ACL,
2014.

S. Chen, Y. Lan, and Z. Yuan, “A multi-task automated assess-
ment system for essay scoring,” in AIED, 2024.

Z. Jiang, T. Gao, Y. Yin, M. Liu, H. Yu, Z. Cheng, and Q. Gu, “Im-
proving domain generalization for prompt-aware essay scoring
via disentangled representation learning,” in ACL, 2023.

Z. Jiang, M. Liu, Y. Yin, H. Yu, Z. Cheng, and Q. Gu, “Learning
from graph propagation via ordinal distillation for one-shot
automated essay scoring,” in WWW, 2021.

Y. Chen and X. Li, “PMAES: Prompt-mapping contrastive learn-
ing for cross-prompt automated essay scoring,” in ACL, 2023.

A. Lajis, H. M. Nasir, and N. A. Aziz, “Proposed assessment
framework based on bloom taxonomy cognitive competency:
Introduction to programming,” in ICSCA, 2018.

E. L. Glassman, J. Scott, R. Singh, P. J. Guo, and R. C. Miller,
“Overcode: Visualizing vriation in student solutions to program-
ming problems at scale,” TOCHI, 2015.

M. Talbot, K. Geldreich, J. Sommer, and P. Hubwieser, “Re-use
of programming patterns or problem solving? representation of
scratch programs by tgraphs to support static code analysis,” in
WiPSCE, 2020.

C. G. Von Wangenheim, J. C. Hauck, M. F. Demetrio, R. Pelle,
N. da Cruz Alves, H. Barbosa, and L. FE. Azevedo, “Codemaster—
automatic assessment and grading of app inventor and snap!
programs.” Informatics in Education, 2018.

R. A. P. Queirés and J. P. Leal, “Petcha: a programming exercises
teaching assistant,” in Proceedings of the 17th ACM annual con-
ference on Innovation and technology in computer science education,
2012.

D. Wang, E. Zhang, and X. Lu, “Automatic grading of student
code with similarity measurement,” in ECML-PKDD, 2023.

M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, “Suggesting
accurate method and class names,” in FSE, 2015.

J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, “A
novel neural source code representation based on abstract syntax
tree,” in ICSE, 2019.

U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “Code2vec:
Learning distributed representations of code,” PACMPL, 2019.

V. J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, and D. Bieber,
“Global relational models of source code,” in ICLR, 2019.

Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,
L. Shou, B. Qin, T. Liu, D. Jiang et al., “Codebert: A pre-trained
model for programming and natural languages,” arXiv preprint
arXiv:2002.08155, 2020.

A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, “Learning
and evaluating contextual embedding of source code,” in ICML,
2020.

Y. Li, S. Qin, H. Huang, Y. Li, L. Qin, X. Hu, W. Jiang, H.-T.
Zheng, and P. S. Yu, “Rethinking the roles of large language
models in chinese grammatical error correction,” arXiv preprint
arXiv:2402.11420, 2024.

Y. Wang, B. Wang, Y. Liu, D. Wu, and W. Che, “Lm-combiner:
A contextual rewriting model for chinese grammatical error
correction,” arXiv preprint arXiv:2403.17413, 2024.

K. Omelianchuk, A. Liubonko, O. Skurzhanskyi, A. Chernodub,
O. Korniienko, and I. Samokhin, “Pillars of grammatical er-
ror correction: Comprehensive inspection of contemporary ap-
proaches in the era of large language models,” arXiv preprint
arXiv:2404.14914, 2024.

K. W. H. Chan, C. Bryant, L. Nguyen, A. Caines, and Z. Yuan,
“Grammatical error correction for code-switched sentences by
learners of english,” arXiv preprint arXiv:2404.12489, 2024.

D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, F. Shengyu, M. Tufano, S. Deng, C. Clement,
D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, “Graph-



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[76]

[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]

[91]

[92]

[93]
[94]

[95]

[96]

[97]

[98]

[99]

[100]

[101]

codebert: Pre-training code representations with data flow,” arXiv
preprint arXiv:2009.08366, 2020.

D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, “Unix-
coder: Unified cross-modal pre-training for code representation,”
arXiv preprint arXiv:2203.03850.

W. Ahmad, S. Chakraborty, and B. e. a. Ray, “Unified pre-training
for program understanding and generation,” in NAACL, 2021.

Y. Dong, X. Jiang, Y. Liu, G. Li, and Z. Jin, “Codepad:
Sequence-based code generation with pushdown automaton,”
arXiv preprint arXiv:2211.00818, 2022.

A. Anand, A. Gupta, N. Yadav, and S. Bajaj, “A comprehensive
survey of ai-driven advancements and techniques in automated
program repair and code generation,” 2024.

Q. Zhang, C. Fang, Y. Ma, W. Sun, and Z. Chen, “A survey of
learning-based automated program repair,” 2023.

A. Nilizadeh, G. T. Leavens, X.-B. D. Le, C. S. Pasdreanu, and
D. R. Cok, “Exploring true test overfitting in dynamic automated
program repair using formal methods,” in ICST. IEEE, 2021.
H. Ye, J. Gu, M. Martinez, T. Durieux, and M. Monperrus,
“Automated classification of overfitting patches with statically
extracted code features,” IEEE TSE, 2022.

L. Cripwell, J. Legrand, and C. Gardent, “Context-aware docu-
ment simplification,” in ACL, 2023.

D. Wright, A. L. Gentile, N. Faux, and K. L. Beck, “Bioact:
Biomedical knowledge base construction using active learning,”
bioRxiv, 2022.

H. Abdulrahman Alawwad, A. Alhothali, U. Naseem,
A. Alkhathlan, and A. Jamal, “Enhancing textbook question
answering task with large language models and retrieval aug-
mented generation,” arXiv e-prints, 2024.

Y. Wang, B. Wei, J. Liu, Q. Lin, L. Zhang, and Y. Wu, “Spatial-
semantic collaborative graph network for textbook question an-
swering,” IEEE TCSVT, 2023.

J. M. Gomez-Perez and R. Ortega, “Isaaq—mastering textbook
questions with pre-trained transformers and bottom-up and top-
down attention,” arXiv preprint arXiv:2010.00562, 2020.

Z. Huang, Y. Shen, X. Li, Y. Wei, G. Cheng, L. Zhou, X. Dai,
and Y. Qu, “GeoSQA: A benchmark for scenario-based question
answering in the geography domain at high school level,” in
EMNLP-IJCNLP, 2019.

P. Clark, O. Etzioni, T. Khot, A. Sabharwal, O. Tafjord, P. Turney,
and D. Khashabi, “Combining retrieval, statistics, and inference
to answer elementary science questions,” AAAI, 2016.

Q. Chen, X. Zhu, Z.-H. Ling, S. Wei, H. Jiang, and D. Inkpen,
“Enhanced LSTM for natural language inference,” in ACL, 2017.
J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang,
and J. Zhou, “mplug-owl3: Towards long image-sequence under-
standing in multi-modal large language models,” 2024.

C. Tan, J. Wei, Z. Gao, L. Sun, S. Li, X. Yang, and S. Z. Li, “Boosting
the power of small multimodal reasoning models to match larger
models with self-consistency training,” ECCV, 2023.

J. Cha, W. Kang, J. Mun, and B. Roh, “Honeybee: Locality-
enhanced projector for multimodal Ilm,” in CVPR, 2024.

D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and
P. Szolovits, “What disease does this patient have? a large-scale
open domain question answering dataset from medical exams,”
arXiv preprint arXiv:2009.13081, 2020.

V. Liévin, A. G. Motzfeldt, I. R. Jensen, and O. Winther, “Varia-
tional open-domain question answering,” in ICML, 2023.

K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,
N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large
language models encode clinical knowledge,” arXiv preprint
arXiv:2212.13138, 2022.

W. Qin, X. Wang, Z. Hu, L. Wang, Y. Lan, and R. Hong, “Math
word problem generation via disentangled memory retrieval,”
TKDD, 2024.

Q. Zhou and D. Huang, “Towards generating math word prob-
lems from equations and topics,” in ICNLG, 2019.

B. Kim, K. S. Ki, S. Rhim, and G. Gweon, “EPT-X: An expression-
pointer transformer model that generates eXplanations for num-
bers,” in ACL, 2022.

K. S. Ki, D. Lee, B. Kim, and G. Gweon, “Generating equation by
utilizing operators : GEO model,” in COLING, 2020.

W. Tan, D. Chen, J. Xue, Z. Wang, and T. Chen, “Teaching-
inspired integrated prompting framework: A novel approach for
enhancing reasoning in large language models,” 2024.

[102]

[103]

[104]

[105]

[106]

[107]

[108]

[109]

[110]
[111]

[112]

[113]

[114]

[115]

[116]

[117]

[118]

[119]

[120]

[121]

[122]

[123]

[124]

[125]

[126]

[127]

16

W. Zhang, Y. Shen, Y. Ma, X. Cheng, Z. Tan, Q. Nong, and W. Lu,
“Multi-view reasoning: Consistent contrastive learning for math
word problem,” in EMNLP, 2022.

J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu,
“Generate & rank: A multi-task framework for math word prob-
lems,” in ACL, 2021.

J. Zhang and Y. Moshfeghi, “ELASTIC: Numerical reasoning with
adaptive symbolic compiler,” in NeurIPS, 2022.

S. Hu, C. Lu, and J. Clune, “Automated design of agentic sys-
tems,” arXiv preprint arXiv:2408.08435, 2024.

T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli,
E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Tool-
former: Language models can teach themselves to use tools,” in
NeurIPS, 2023.

Q. Zhong, K. Wang, Z. Xu, J. Liu, L. Ding, and B. Du, “Achieving
97% on gsm8k: Deeply understanding the problems makes Ilms
better solvers for math word problems,” 2024.

C. Wang, Z. Jiang, Y. Yin, Z. Cheng, S. Ge, and Q. Gu, “Aggre-
gating multiple heuristic signals as supervision for unsupervised
automated essay scoring,” in ACL, 2023.

S. Kim, A. Lee, J. Park, A. Chung, J. Oh, and J.-Y. Lee, “Towards
efficient visual-language alignment of the q-former for visual
reasoning tasks,” 2024.

M. Zheng, X. Feng, Q. Si, Q. She, Z. Lin, W. Jiang, and W. Wang,
“Multimodal table understanding,” in ACL, 2024.

T. G. Soares, A. Azhari, N. Rokhman, and E. Wonarko, “Educa-
tion question answering systems: a survey,” in IMECS, 2021.

A. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi, and
H. Hajishirzi, “Are you smarter than a sixth grader? textbook
question answering for multimodal machine comprehension,” in
CVPR, 2017.

A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and
A. Farhadi, “A diagram is worth a dozen images,” in ECCV, 2016.
A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A
large-scale multi-subject multi-choice dataset for medical domain
question answering,” in CHIL, 2022.

A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly
et al., “An image is worth 16x16 words: Transformers for image
recognition at scale,” arXiv preprint arXiv:2010.11929, 2020.

P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, and H. Li,
“Dynamic fusion with intra-and inter-modality attention flow for
visual question answering,” in CVPR, 2019.

F. Gao, Q. Ping, G. Thattai, A. Reganti, Y. N. Wu, and P. Natarajan,
“Transform-retrieve-generate: Natural language-centric outside-
knowledge visual question answering,” in CVPR, 2022.

S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,
and D. Parikh, “Vqa: Visual question answering,” in ICCV, 2015.
M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons:
A neural-based approach to answering questions about images,”
in ICCV, 2015.

D. Huang, S. Shi, C.-Y. Lin, J. Yin, and W.-Y. Ma, “How well
do computers solve math word problems? large-scale dataset
construction and evaluation,” in ACL, 2016.

S. Upadhyay and M.-W. Chang, “Annotating derivations: A new
evaluation strategy and dataset for algebra word problems,”
arXiv preprint arXiv:1609.07197, 2016.

Y. Wang, X. Liu, and S. Shi, “Deep neural solver for math word
problems,” in EMNLP, 2017.

A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi,
and H. Hajishirzi, “MathQA: Towards interpretable math word
problem solving with operation-based formalisms,” 2019.

S.-y. Miao, C.-C. Liang, and K.-Y. Su, “A diverse corpus for
evaluating and developing English math word problem solvers,”
2020.

K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and
J. Schulman, “Training verifiers to solve math word problems,”
arXiv preprint arXiv:2110.14168, 2021.

P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang,
and S.-C. Zhu, “Iconga: A new benchmark for abstract diagram
understanding and visual language reasoning,” 2021.

P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit,
P. Clark, and A. Kalyan, “Dynamic prompt learning via pol-
icy gradient for semi-structured mathematical reasoning,” arXiv
preprint arXiv:2209.14610, 2022.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[128]

[129]

[130]

[131]

[132]

[133]

[134]

[135]

[136]

[137]
[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

[147]

[148]

[149]

[150]

[151]

[152]

[153]

D. Zhang, L. Wang, L. Zhang, B. T. Dai, and H. T. Shen, “The gap
of semantic parsing: A survey on automatic math word problem
solvers,” IEEE TPAMI, 2020.

Z. Gou, Z. Shao, Y. Gong, Y. Yang, M. Huang, N. Duan, W. Chen
et al., “Tora: A tool-integrated reasoning agent for mathematical
problem solving,” arXiv preprint arXiv:2309.17452, 2023.

S. Bulathwela, H. Muse, and E. Yilmaz, “Scalable educational
question generation with pre-trained language models,” in AIED.
Springer, 2023.

P. Zhu and C. Hauff, “Unsupervised domain adaptation for ques-
tion generation with domain data selection and self-training,” in
NAACL. ACL, 2022.

L. E. Lopez, D. K. Cruz, J. C. B. Cruz, and C. Cheng, “Simplifying
paragraph-level question generation via transformer language
models,” in PRICAI. Springer, 2021.

H.-L. Chung, Y.-H. Chan, and Y.-C. Fan, “A bert-based distrac-
tor generation scheme with multi-tasking and negative answer
training strategies,” arXiv preprint arXiv:2010.05384, 2020.

G. Chen, J. Yang, C. Hauff, and G.-J. Houben, “Learningg: A
large-scale dataset for educational question generation,” in AAAI,
2018.

X. Du, J. Shao, and C. Cardie, “Learning to ask: Neural
question generation for reading comprehension,” arXiv preprint
arXiv:1705.00106, 2017.

M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba,
“Addressing the rare word problem in neural machine transla-
tion,” arXiv preprint arXiv:1410.8206, 2014.

M. Heilman and N. A. Smith, “Good question! statistical ranking
for question generation,” in NAACL, 2010.

H. Gong, L. Pan, and H. Hu, “Khanq: A dataset for generating
deep questions in education,” in COLING, 2022.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer
learning with a unified text-to-text transformer,” J. Mach. Learn.
Res., 2020.

M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: De-
noising sequence-to-sequence pre-training for natural language
generation, translation, and comprehension,” arXiv preprint
arXiv:1910.13461, 2019.

L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,
M. Zhou, and H.-W. Hon, “Unified language model pre-training
for natural language understanding and generation,” NeurIPS,
2019.

S. Maity, A. Deroy, and S. Sarkar, “Harnessing the power of
prompt-based techniques for generating school-level questions
using large language models,” in FIRE, 2023.

Y. Liu, “Multilingual denoising pre-training for neural machine
translation,” arXiv preprint arXiv:2001.08210, 2020.

A. Hadifar, S. K. Bitew, J. Deleu, C. Develder, and T. Demeester,
“Eduqg: A multi-format multiple-choice dataset for the educa-
tional domain,” IEEE Access, 2023.

C. Liang, X. Yang, N. Dave, D. Wham, B. Pursel, and C. L.
Giles, “Distractor generation for multiple choice questions using
learning to rank,” in BEA, 2018.

S. K. Bitew, A. Hadifar, L. Sterckx, J. Deleu, C. Develder, and
T. Demeester, “Learning to reuse distractors to support multiple
choice question generation in education,” IEEE TLT, 2022.

L. Xue, “mt5: A massively multilingual pre-trained text-to-text
transformer,” arXiv preprint arXiv:2010.11934, 2020.

B. Das, M. Majumder, S. Phadikar, and A. A. Sekh, “Automatic
question generation and answer assessment: A survey,” Research
and Practice in Technology Enhanced Learning, 2021.

L. Dugan, E. Miltsakaki, S. Upadhyay, E. Ginsberg, H. Gonzalez,
D. Choi, C. Yuan, and C. Callison-Burch, “A feasibility study
of answer-agnostic question generation for education,” arXiv
preprint arXiv:2203.08685, 2022.

D. Tang, N. Duan, T. Qin, Z. Yan, and M. Zhou, “Question
answering and question generation as dual tasks,” arXiv preprint
arXiv:1706.02027, 2017.

J. Welbl, N. F. Liu, and M. Gardner, “Crowdsourcing multiple
choice science questions,” arXiv preprint arXiv:1707.06209, 2017.
G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “Race: Large-
scale reading comprehension dataset from examinations,” arXiv
preprint arXiv:1704.04683, 2017.

Y. Wu, J. Nouri, B. Megyesi, A. Henriksson, M. Duneld, and X. Li,
“Towards data-effective educational question generation with

[154]

[155]

[156]

[157]

[158]
[159]

[160]

[161]

[162]

[163]

[164]

[165]

[166]

[167]

[168]

[169]

[170]
[171]
[172]
[173]
[174]

[175]

[176]

[177]

[178]

[179]

[180]

17

prompt-based learning,” in Science and Information Conference,
2023.

X. Jia, W. Zhou, X. Sun, and Y. Wu, “Eqg-race: Examination-type
question generation,” in AAAI, 2021.

T. Steuer, A. Filighera, and T. Tregel, “Investigating educational
and noneducational answer selection for educational question
generation,” IEEE Access, 2022.

Z. Zhao, Y. Hou, D. Wang, M. Yu, C. Liu, and X. Ma, “Educational
question generation of children storybooks via question type
distribution learning and event-centric summarization,” arXiv
preprint arXiv:2203.14187, 2022.

H. Kunichika, T. Katayama, T. Hirashima, and A. Takeuchi,
“Automated question generation methods for intelligent english
learning systems and its evaluation,” in ICCE, 2004.

X. Du and C. Cardie, “Identifying where to focus in reading
comprehension for neural question generation,” in EMNLP, 2017.
A. Naeiji, “Question generation using sequence-to-sequence
model with semantic role labels,” 2022.

Z. Wang and R. Baraniuk, “Multiqg-ti: Towards question genera-
tion from multi-modal sources,” arXiv preprint arXiv:2307.04643,
2023.

W. Zhou, M. Zhang, and Y. Wu, “Multi-task learning with
language modeling for question generation,” arXiv preprint
arXiv:1908.11813, 2019.

Y. Chen, L. Wu, and M. J. Zaki, “Natural question generation with
reinforcement learning based graph-to-sequence model,” arXiv
preprint arXiv:1910.08832, 2019.

Y.-H. Chan and Y.-C. Fan, “A recurrent bert-based model for
question generation,” in MRQA, 2019.

Z. Wang, J. Valdez, D. Basu Mallick, and R. G. Baraniuk, “To-
wards human-like educational question generation with large
language models,” in AIED, 2022.

X. Yuan, T. Wang, C. Gulcehre, A. Sordoni, P. Bachman, S. Sub-
ramanian, S. Zhang, and A. Trischler, “Machine comprehen-
sion by text-to-text neural question generation,” arXiv preprint
arXiv:1705.02012, 2017.

X. Ma, Q. Zhu, Y. Zhou, and X. Li, “Improving question genera-
tion with sentence-level semantic matching and answer position
inferring,” in AAAI, 2020.

K. Hwang, K. Wang, M. Alomair, F.-S. Choa, and L. K. Chen,
“Towards automated multiple choice question generation and
evaluation: aligning with bloom’s taxonomy,” in AIED. Springer,
2024.

S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino,
J. Yosinski, and R. Liu, “Plug and play language models: A
simple approach to controlled text generation,” arXiv preprint
arXiv:1912.02164, 2019.

Z. Qiu, X. Wu, and W. Fan, “Automatic distractor generation
for multiple choice questions in standard tests,” arXiv preprint
arXiv:2011.13100, 2020.

R. Mitkov et al., “Computer-aided generation of multiple-choice
tests,” in BEA, 2003.

J. Brown, G. Frishkoff, and M. Eskenazi, “Automatic question
generation for vocabulary assessment,” in HLT/EMNLP, 2005.

V. J. Schmalz and A. Brutti, “Automatic assessment of english
cefr levels using bert embeddings,” in CLIC, 2021.

S. Li and V. Ng, “Conundrums in cross-prompt automated essay
scoring: Making sense of the state of the art,” in ACL, 2024.

, “ICLE++: Modeling fine-grained traits for holistic essay
scoring,” in NAACL, 2024.

R. Kumar, S. Mathias, S. Saha, and P. Bhattacharyya, “Many
hands make light work: Using essay traits to automatically score
essays,” in NAACL, 2022.

M. Uto, Y. Xie, and M. Ueno, “Neural automated essay scoring
incorporating handcrafted features,” in COLING, 2020.

H. Yang, Y. He, X. Bu, H. Xu, and W. Guo, “Automatic essay
evaluation technologies in chinese writing—a systematic litera-
ture review,” Applied Sciences, 2023.

J. Sun, T. Song, J. Song, and W. Peng, “Improving automated
essay scoring by prompt prediction and matching,” Entropy, 2022.
W. Yupei and H. Renfen, “A prompt-independent and inter-
pretable automated essay scoring method for Chinese second
language writing,” in CCL, 2021.

M. Tigina, A. Birillo, Y. Golubev, H. Keuning, N. Vyahhi, and
T. Bryksin, “Analyzing the quality of submissions in online
programming courses,” arXiv preprint arX1v:2301.11158, 2023.



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[181]

[182]

[183]

[184]

[185]

[186]
[187]
[188]

[189]

[190]

[191]

[192]
[193]
[194]

[195]

[196]

[197]

[198]

[199]

[200]

[201]

[202]

[203]

[204]

[205]

[206]

[207]

H. Yannakoudakis, T. Briscoe, and B. Medlock, “A new dataset
and method for automatically grading ESOL texts,” in ACL, 2011.
A. Franklin, Maggie, M. Benner, N. Rambis, P. Baffour, R. Hol-
brook, S. Crossley, and ulrichboser, “Feedback prize - english
language learning,” 2022, kaggle.

R. Yang, J. Cao, Z. Wen, Y. Wu, and X. He, “Enhancing automated
essay scoring performance via fine-tuning pre-trained language
models with combination of regression and ranking,” in EMNLP,
2020.

T. Zesch, M. Wojatzki, and D. Scholten-Akoun, “Task-
independent features for automated essay grading,” in BEA,
2015.

P. Phandi, K. M. A. Chai, and H. T. Ng, “Flexible domain
adaptation for automated essay scoring using correlated linear
regression,” in EMNLP, 2015.

A. Louis and D. Higgins, “Off-topic essay detection using short
prompt texts,” in BEA, 2010.

H. Chen and B. He, “Automated essay scoring by maximizing
human-machine agreement,” in EMNLP, 2013.

M. Yamaura, I. Fukuda, and M. Uto, “Neural automated essay
scoring considering logical structure,” in AIED, 2023.

A. Sharma, A. Kabra, and R. Kapoor, “Feature enhanced capsule
networks for robust automatic essay scoring,” in ECML-PKDD,
2021.

A. Birillo, I. Vlasov, A. Burylov, V. Selishchev, A. Goncharov,
E. Tikhomirova, N. Vyahhi, and T. Bryksin, “Hyperstyle: A tool
for assessing the code quality of solutions to programming as-
signments,” in CSE, 2022.

A. Birillo, E. Artser, Y. Golubev, M. Tigina, H. Keuning, N. Vyahhi,
and T. Bryksin, “Detecting code quality issues in pre-written
templates of programming tasks in online courses,” arXiv preprint
arX1v:2304.12376, 2023.

R. Roy Choudhury, H. Yin, and A. Fox, “Scale-driven automatic
hint generation for coding style,” in ITS, 2016.

L. C. Ureel II and C. Wallace, “Automated critique of early
programming antipatterns,” in SIGCSE, 2019.

H. Keuning, B. Heeren, and J. Jeuring, “A tutoring system to learn
code refactoring,” in SIGCSE, 2021.

D. Dahlmeier, H. Ng, and S. Wu, “Building a large annotated
corpus of learner english: The nus corpus of learner english,”
BEA, 2013.

M. R. Qorib and H. T. Ng, “System combination via qual-
ity estimation for grammatical error correction,” arXiv preprint
arXiv:2310.14947, 2023.

M. R. Qorib, S.-H. Na, and H. T. Ng, “Frustratingly easy system
combination for grammatical error correction,” in NAACL, 2022.
B. Wang, X. Duan, D. Wu, W. Che, Z. Chen, and G. Hu, “Ccte:
a cross-sentence chinese text correction dataset for native speak-
ers,” in COLING, 2022.

X. Cheng, W. Xu, K. Chen, S. Jiang, F. Wang, T. Wang, W. Chu,
and Y. Qi, “Spellgcn: Incorporating phonological and visual
similarities into language models for chinese spelling check,”
2020.

K. Omelianchuk, V. Atrasevych, A. Chernodub, and O. Skurzhan-
skyi, “Gector - grammatical error correction: Tag, not rewrite,”
2020.

L. Xu, J. Wu, J. Peng, J. Fu, and M. Cai, “Fegec: Fine-grained
corpus for chinese grammatical error correction,” arXiv preprint
arXiv:2210.12364, 2022.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” OpenAI
blog, 2018.

Z. Baoli, “The principles for building the “international corpus of
learner chinese”,” Applied Linguistics, 2011.

H. Du, Y. Zhao, Q. Tian, J. Wang, L. Wang, Y. Lan, and X. Lu,
“Flacgec: A chinese grammatical error correction dataset with
fine-grained linguistic annotation,” in CIKM, 2023.

M. Kaneko, S. Takase, A. Niwa, and N. Okazaki, “Interpretability
for language learners using example-based grammatical error
correction,” arXiv preprint arXiv:2203.07085.

Y. Shao, Z. Geng, Y. Liu, J. Dai, F. Yang, L. Zhe, H. Bao, and X. Qiu,
“Cpt: A pre-trained unbalanced transformer for both chinese
language understanding and generation.” arXiv: Computation and
Language, 2021.

S. Rothe, J. Mallinson, E. Malmi, S. Krause, and A. Severyn, “A
simple recipe for multilingual grammatical error correction,” in
ACL—IJCNLP, 2021.

[208]

[209]

[210]

[211]
[212]
[213]

[214]

[215]

[216]
[217]

[218]

[219]

[220]

[221]

[222]

[223]

[224]

[225]

[226]

[227]

[228]

[229]

[230]

[231]

[232]

[233]

18

H. Cao, L. Yuan, Y. Zhang, and H. T. Ng, “Unsupervised
grammatical error correction rivaling supervised methods,” in
EMNLP, Dec. 2023.

J. Naplava, M. Straka, J. Strakova, and A. Rosen, “Czech grammar
error correction with a large and diverse corpus,” TACL, 2022.

F. Stahlberg and S. Kumar, “Synthetic data generation for low-
resource grammatical error correction with tagged corruption
models,” in BEA, 2024.

S. Flachs, F. Stahlberg, and S. Kumar, “Data strategies for low-
resource grammatical error correction,” in BEA, 2021.

Y. Kementchedjhieva and A. Segaard, “Grammatical error correc-
tion through round-trip machine translation,” in EACL, 2023.

M. A. Niculescu, S. Ruseti, and M. Dascalu, “Rogpt2: Romanian
gpt2 for text generation,” in ICTAI. IEEE, 2021.

N. Kumar, P. Kumar, S. Tripathy, N. Samal, D. Gountia, P. Gatla,
and T. Singh, “Context-aware adversarial graph-based learning
for multilingual grammatical error correction,” ACM Transactions
on Asian and Low-Resource Language Information Processing, 2024.
X. Sun, T. Ge, S. Ma, J. Li, F Wei, and H. Wang, “A unified strategy
for multilingual grammatical error correction with pre-trained
cross-lingual language model,” arXiv preprint arXiv:2201.10707,
2022.

X. Yin, C. Ni, S. Wang, Z. Li, L. Zeng, and X. Yang, “Thinkrepair:
Self-directed automated program repair,” in SIGSOFT, 2024.

X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu, and C. Hu,
“Template-based neural program repair,” in ICSE. IEEE, 2023.
N. Jiang, T. Lutellier, Y. Lou, L. Tan, D. Goldwasser, and X. Zhang,
“Knod: Domain knowledge distilled tree decoder for automated
program repair,” in ICSE. IEEE, 2023.

R. Gharibi, M. H. Sadreddini, and S. M. Fakhrahmad, “T5apr:
Empowering automated program repair across languages
through checkpoint ensemble,” Journal of Systems and Software,
2024

C. S. Xia, Y. Wei, and L. Zhang, “Automated program repair in
the era of large pre-trained language models,” in ICSE. IEEE,
2023.

A. Afzal, M. Motwani, K. T. Stolee, Y. Brun, and C. Le Goues,
“Sosrepair: Expressive semantic search for real-world program
repair,” IEEE TSE, 2019.

M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan,
H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., “Eval-
uating large language models trained on code,” arXiv preprint
arXiv:2107.03374, 2021.

A. Koyuncu, T. F. Bissyandé, J. Klein, and Y. L. Traon, “Flexire-
pair: Transparent program repair with generic patches,” arXiv
preprint arXiv:2011.13280, 2020.

C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A
generic method for automatic software repair,” IEEE TSE, 2011.
Y. Qi, X. Mao, and Y. Lei, “Efficient automated program repair
through fault-recorded testing prioritization,” in IEEE ICSME.
IEEE, 2013.

L. Gong, M. Elhoushi, and A. Cheung, “Ast-t5: structure-
aware pretraining for code generation and understanding.” arXiv
preprint ARXIV.2401.03003.

Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-
aware unified pre-trained encoder-decoder models for code un-
derstanding and generation,” arXiv preprint arXiv:2109.00859,
2021.

J. Lu, L. Yu, X. Li, L. Yang, and C. Zuo, “Llama-reviewer:
Advancing code review automation with large language models
through parameter-efficient fine-tuning,” in ISSRE. IEEE, 2023.
Z. Li, S. Lu, D. Guo, N. Duan, S. Jannu, G. Jenks, D. Ma-
jumder, J. Green, A. Svyatkovskiy, S. Fu, and N. Sundaresan,
“Automating code review activities by large-scale pre-training,”
in ESEC/FSE, 2022.

Q. Guo, J. Cao, X. Xie, S. Liu, X. Li, B. Chen, and X. Peng, “Ex-
ploring the potential of chatgpt in automated code refinement:
An empirical study,” arXiv preprint arXiv:2309.08221, 2023.

H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H. Susanto,
and C. Bryant, “The conll-2014 shared task on grammatical error
correction,” in CoNLL, 2014.

C. Bryant, M. Felice, @. E. Andersen, and T. Briscoe, “The bea-
2019 shared task on grammatical error correction,” in BEA, 2019.
Y.-H. Tseng, L.-H. Lee, L.-P. Chang, and H.-H. Chen, “Intro-
duction to sighan 2015 bake-off for chinese spelling check,” in
SIGHAN, 2015.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[234]

[235]

[236]

[237]

[238]
[239]

[240]

[241]

[242]

[243]

[244]

[245]

[246]

[247]

[248]

[249]

[250]

[251]

[252]

[253]

[254]

[255]

[256]

[257]

[258]

[259]

A. Rozovskaya and D. Roth, “Grammar error correction in mor-
phologically rich languages: The case of russian,” TACL, 2019.

R. Grundkiewicz and M. Junczys-Dowmunt, “The wiked error
corpus: A corpus of corrective wikipedia edits and its application
to grammatical error correction,” in Advances in Natural Language
Processing: 9th International Conference on NLP, 2014.

S. Davidson, A. Yamada, P. Mira, A. Carando, C. Sdnchez-
Gutiérrez, and K. Sagae, “Developing nlp tools with a new
corpus of learner spanish,” LREC, 2020.

O. Syvokon and O. Nahorna, “Ua-gec: Grammatical error cor-
rection and fluency corpus for the ukrainian language,” arXiv
preprint arXiv:2103.16997, 2021.

T.-M. Cotet, S. Ruseti, and M. Dascalu, “Neural grammatical error
correction for romanian,” in ICTAI, 2020.

K. Knight and I. Chander, “Automated postediting of docu-
ments,” AAAI, 1994.

A. Rozovskaya, K.-W. Chang, M. Sammons, D. Roth, and
N. Habash, “The illinois-columbia system in the conll-2014
shared task,” in CoNLL, 2014.

I. Sutskever, O. Vinyals, and Q. Le, “Sequence to sequence
learning with neural networks,” arXiv, 2014.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, 5. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer
learning with a unified text-to-text transformer,” JMLR, 2020.

Y. Zhang, B. Zhang, Z. Li, Z. Bao, C. Li, and M. Zhang, “Syngec:
Syntax-enhanced grammatical error correction with a tailored
gec-oriented parser,” arXiv preprint arXiv:2210.12484, 2022.

Y. Zhang and Z. Li, “Csyngec: Incorporating constituent-based
syntax for grammatical error correction with a tailored gec-
oriented parser,” arXiv preprint arXiv:2211.08158, 2022.

F. Stahlberg and S. Kumar, “Seq2Edits: Sequence transduction
using span-level edit operations,” in EMNLP, 2020.

A. Awasthi, S. Sarawagi, R. Goyal, S. Ghosh, and V. Piratla,
“Parallel iterative edit models for local sequence transduction,”
2019.

Y. Dong, Z. Li, M. Rezagholizadeh, and J. C. K. Cheung, “Ed-
itNTS: An neural programmer-interpreter model for sentence
simplification through explicit editing,” 2019.

E. Malmi, S. Krause, S. Rothe, D. Mirylenka, and A. Severyn,
“Encode, tag, realize: High-precision text editing,” arXiv preprint
arXiv:1909.01187, 2019.

Y. Li, X. Liu, S. Wang, P. Gong, D. F. Wong, Y. Gao, H. Huang,
and M. Zhang, “TemplateGEC: Improving grammatical error
correction with detection template,” in ACL, A. Rogers, J. Boyd-
Graber, and N. Okazaki, Eds., 2023.

N. Hossain, M. H. Bijoy, S. Islam, and S. Shatabda, “Panini:
a transformer-based grammatical error correction method for
bangla,” Neural Comput. Appl., 2023.

M. Harahus, Z. Sokolova, M. Pleva, J. Juhar, D. Hladek, J. Stas,
and M. Kocttrova, “Evaluation of datasets focused on gram-
matical error correction using the t5 model in slovak,” in RA-
DIOELEKTRONIKA, 2024.

A. Gebesce and G. G. Sahin, “Gecturk web: An explainable online
platform for turkish grammatical error detection and correction,”
arXiv preprint arXiv:2410.12350, 2024.

M. K. Keita, C. Homan, S. A. Hamani, A. Bremang, M. Zampieri,
H. A. Alfari, E. A. Ibrahim, and D. Owusu, “Grammatical error
correction for low-resource languages: The case of zarma,” arXiv
preprint arXiv:2410.15539, 2024.

S. Han, Y. Wang, and X. Lu, “Errorclr: Semantic error classi-
fication, localization and repair for introductory programming
assignments,” in SIGIR, 2023.

N. Chen, Q. Sun, J. Wang, X. Li, and M. Gao, “Pass-tuning:
Towards structure-aware parameter-efficient tuning for code rep-
resentation learning,” in EMNLP, 2023.

R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of exist-
ing faults to enable controlled testing studies for java programs,”
in NeurIPS, 2014.

C. Le Goues, N. Holtschulte, E. K. Smith, Y. Brun, P. Devanbu,
S. Forrest, and W. Weimer, “The manybugs and introclass bench-
marks for automated repair of c programs,” IEEE TSE, 2015.

D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, “Quixbugs: a
multi-lingual program repair benchmark set based on the quixey
challenge,” in SPLASH Companion, 2017.

C. Xia, Y. Wei, and L. Zhang, “Practical program repair in
the era of large pre-trained language models,” arXiv preprint
arXiv:2210.14179, 2022.

[260]

[261]

[262]

[263]

[264]

[265]

[266]

[267]

[268]

[269]

[270]

[271]

[272]

[273]

[274]

[275]
[276]
[277]

[278]

[279]

[280]

[281]

[282]

[283]

19

M. Tufano, C. Watson, G. Bavota, M. Penta, M. White, and
D. Poshyvanyk, “An empirical study on learning bug-fixing
patches in the wild via neural machine translation,” TOSEM,
2018.

J.-R. Falleri, F Morandat, X. Blanc, M. Martinez, and M. Mon-
perrus, “Fine-grained and accurate source code differencing,” in
ASE, 2014.

Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,
B. Qin, T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained
model for programming and natural languages,” in EMNLP,
2020.

A. Sil, J. Sen, B. Iyer, M. Franz, K. Fadnis, M. Bornea, S. Rosenthal,
S. McCarley, R. Zhang, V. Kumar et al., “Primeqa: The prime
repository for state-of-the-art multilingual question answering
research and development,” arXiv preprint arXiv:2301.09715, 2023.
F. Lei, T. Luo, P. Yang, W. Liu, H. Liu, J. Lei, Y. Huang, Y. Wei,
S. He, J. Zhao et al., “Tableqakit: A comprehensive and prac-
tical toolkit for table-based question answering,” arXiv preprint
arXiv:2310.15075, 2023.

Z. Zhang, S. Luo, J. Chen, S. Lai, 5. Long, H. Chung, and
S. C. Han, “Piggyback: Pretrained visual question answering
environment for backing up non-deep learning professionals,”
in WSDM, 2023.

T.-P. Nguyen, S. Razniewski, and G. Weikum, “Inside ascent:
Exploring a deep commonsense knowledge base and its usage
in question answering,” arXiv preprint arXiv:2105.13662, 2021.

T. Baumgartner, K. Wang, R. Sachdeva, M. Eichler, G. Geigle,
C. Poth, H. Sterz, H. Puerto, L. F. Ribeiro, J. Pfeiffer et al., “Ukp-
square: An online platform for question answering research,”
arXiv preprint arXiv:2203.13693, 2022.

X. Yu, Y. Lu, and Z. Yu, “Localrqa: From generating data to
locally training, testing, and deploying retrieval-augmented qa
systems,” arXiv preprint arXiv:2403.00982, 2024.

Y. Lan, L. Wang, Q. Zhang, Y. Lan, B. T. Dai, Y. Wang, D. Zhang,
and E.-P. Lim, “Mwptoolkit: an open-source framework for deep
learning-based math word problem solvers,” in AAAI, 2022.

M. Goel, V. Venktesh, and V. Goyal, “Mwpranker: An expression
similarity based math word problem retriever,” in ECML-PKDD.
Springer, 2023.

A. Ushio, F. Alva-Manchego, and J. Camacho-Collados, “A prac-
tical toolkit for multilingual question and answer generation,”
arXiv preprint arXiv:2305.17416, 2023.

S. Moore, E. Costello, H. A. Nguyen, and J. Stamper, “An auto-
matic question usability evaluation toolkit,” 2024.

M. Roemmele, D. Sidhpura, S. DeNeefe, and L. Tsou, “An-
swerquest: A system for generating question-answer items from
multi-paragraph documents,” arXiv preprint arXiv:2103.03820,
2021.

K. Vachev, M. Hardalov, G. Karadzhov, G. Georgiev, I. Koychev,
and P. Nakov, “Leaf: Multiple-choice question generation,” in
ECIR. Springer, 2022.

C.-T. Tsai, J.-J. Chen, C.-Y. Yang, and J. S. Chang, “Lingglewrite:
a coaching system for essay writing,” in ACL, 2020.

H. Manabe and M. Hagiwara, “Expats: a toolkit for explainable
automated text scoring,” arXiv preprint arXiv:2104.03364, 2021.
M. R. Qorib, G. Moon, and H. T. Ng, “Allecs: A lightweight
language error correction system,” in EACL, 2023.

C. Bryant, M. Felice, and E. Briscoe, “Automatic annotation
and evaluation of error types for grammatical error correction.”
ACL, 2017.

G. Moon, M. R. Qorib, D. Dahlmeier, and H. T. Ng, “Wamp:
Writing, annotation, and marking platform,” in IJCNLP, 2023.

E. Calo, L. Jacqmin, T. Rosemplatt, M. Amblard, M. Couceiro,
and A. Kulkarni, “Gecko+: a grammatical and discourse error
correction tool,” in TALN 2021-28e Conférence sur le Traitement
Automatique des Langues Naturelles. ATALA, 2021.

S. Shi, E. Zhao, W. Bi, D. Cai, L. Cui, X. Huang, H. Jiang, D. Tang,
K. Song, L. Wang et al., “Effidit: An assistant for improving
writing efficiency,” in ACL, 2023.

Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang,
L. Shen, A. Wang, Y. Li et al., “Codegeex: A pre-trained model for
code generation with multilingual evaluations on humaneval-x,”
arXiv preprint arXiv:2303.17568, 2023.

B. Sheese, M. Liffiton, J. Savelka, and P. Denny, “Patterns of stu-
dent help-seeking when using a large language model-powered
programming assistant,” in ACE, 2024.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[284]

[285]

[286]

[287]

[288]

[289]

[290]

[291]

[292]

[293]

[294]

[295]

[296]

Y. Liu, P. Gao, X. Wang, J. Liu, Y. Shi, Z. Zhang, and C. Peng,
“Marscode agent: Ai-native automated bug fixing,” 2024.

A. Baladn, I. Sastre, L. Chiruzzo, and A. Ros, “Retuyt-inco at bea
2023 shared task: Tuning open-source Ilms for generating teacher
responses,” in BEA, 2023.

Y. Dan, Z. Lei, Y. Gu, Y. Li, J. Yin, J. Lin, L. Ye, Z. Tie,
Y. Zhou, Y. Wang et al., “Educhat: A large-scale language model-
based chatbot system for intelligent education,” arXiv preprint
arXiv:2308.02773, 2023.

W. Fan, J. Xiao, and Y. Cao, “A framework for math word
problem solving based on pre-training models and spatial op-
timization strategies,” in ChineseCSCW, 2022.

Q. Lin, B. Xu, Z. Huang, and R. Cai, “From large to tiny: Distilling
and refining mathematical expertise for math word problems
with weakly supervision,” in ICIC, 2024.

J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Learning to
compose neural networks for question answering,” arXiv preprint
arXiv:1601.01705, 2016.

Y. Li, M. Du, R. Song, X. Wang, and Y. Wang, “A survey on
fairness in large language models,” 2024.

S. V. Chinta, Z. Wang, Z. Yin, N. Hoang, M. Gonzalez, T. L. Quy,
and W. Zhang, “Fairaied: Navigating fairness, bias, and ethics
in educational ai applications,” arXiv preprint arXiv:2407.18745,
2024.

B. Y. Ng, J. Li, X. Tong, K. Ye, G. Yenne, V. Chandrasekaran, and
J. Li, “Analyzing security and privacy challenges in generative ai
usage guidelines for higher education,” 2025.

E. Latif and X. Zhai, “Privacy-preserved automated scoring using
federated learning for educational research,” 2025.

M. Khalil, R. Shakya, and Q. Liu, “Towards privacy-preserving
data-driven education: The potential of federated learning,” in
IEEE ICTCS, 2025.

L. Krupp, S. Steinert, M. Kiefer-Emmanouilidis, K. E. Avila,
P. Lukowicz, J. Kuhn, S. Kiichemann, and J. Karolus, “Challenges
and opportunities of moderating usage of large language models
in education,” 2023.

C. Zhai, S. Wibowo, and L. D. Li, “The effects of over-reliance on
ai dialogue systems on students’ cognitive abilities: a systematic
review,” Smart Learning Environments, 2024.

20
