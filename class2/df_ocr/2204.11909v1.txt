How can NLP Help Revitalize Endangered Languages?
A Case Study and Roadmap for the Cherokee Language

Shiyue Zhang, Benjamin E. Frey, and Mohit Bansal
UNC Chapel Hill

{shiyue, mbansal}@cs.unc.edu; benfrey@email.unc.edu

Abstract

More than 43% of the languages spoken in the
world are endangered, and language loss cur-
rently occurs at an accelerated rate because of
globalization and neocolonialism. Saving and
revitalizing endangered languages has become
very important for maintaining the cultural di-
versity on our planet. In this work, we focus on
discussing how NLP can help revitalize endan-
gered languages. We first suggest three princi-
ples that may help NLP practitioners to foster
mutual understanding and collaboration with
language communities, and we discuss three
ways in which NLP can potentially assist in
language education. We then take Cherokee,
a severely-endangered Native American lan-
guage, as a case study. After reviewing the lan-
guage’s history, linguistic features, and exist-
ing resources, we (in collaboration with Chero-
kee community members) arrive at a few mean-
ingful ways NLP practitioners can collaborate
with community partners. We suggest two ap-
proaches to enrich the Cherokee language’s re-
sources with machine-in-the-loop processing,
and discuss several NLP tools that people from
the Cherokee community have shown interest
in. We hope that our work serves not only to in-
form the NLP community about Cherokee, but
also to provide inspiration for future work on
endangered languages in general.!

1 Introduction

There are an estimated 6000 to 7000 spoken lan-
guages in the world, and at least 43% of them
are endangered.” Throughout history, languages
have naturally shifted and declined into dormancy.
The current speed of language loss, however, is
far beyond “natural”. Some linguists estimate
that between 50% and 90% of languages will be

‘Our code and data will be open-sourced at https://
github. com/ZhangShiyue/RevitalizeCherokee.

*http: //www.unesco. org/languages-atlas/en/
statistics. html

severely endangered or dead by the end of this cen-
tury (Austin and Sallabank, 2011). This acceler-
ation of language endangerment owes largely to
cultural, political, and economic marginalization
and the rise of global imperialism. Worldwide, in-
digenous people have suffered from colonization
or conquest and given up their mother tongues in
favor of another language. In order to achieve a
higher social status, indigenous people have had to
capitulate to colonizers’ linguistic norms. Follow-
ing Ladefoged (1992), we acknowledge that bur-
dens such as raw material survival outweigh the
more abstract concerns of maintaining a language.
In other words, we cannot blame or fault indige-
nous people for giving up their languages in order
to secure a better life under intense socioeconomic
pressures. As linguists and NLP researchers, we
have the responsibility to address these power im-
balances and create a society where space exists for
indigenous languages. Moreover, language loss is
memory loss, identity loss, culture loss, and knowl-
edge loss, and it even affects the health of indige-
nous people (Whalen et al., 2016).

Endangered languages are even more underrep-
resented in the NLP literature. Joshi et al. (2020)
point out that more than 88% of the world lan-
guages spoken by around 1.2 billion people are
left behind, i.e., they have been and are still ig-
nored in the aspect of language technologies. Blasi
et al. (2021) show that linguistic NLP tasks (e.g.,
morphology analysis) are more language inclusive
than user-facing NLP tasks (e.g., machine trans-
lation). In this information age, NLP techniques
are widely applied on the Internet. Much Internet
content that we are exposed to daily is processed
or even created by NLP techniques. Hence, the
lack of NLP technology support for endangered
languages reduces the degree to which users are ex-
posed to them. Unfortunately, this exacerbates the
problem of linguistic marginalization, as frequent
language exposure is critical to language acquisi-


tion. At worst, it can generate a downward spiral:
since fewer speakers create content using these lan-
guages, the scarcity of resources will in turn hin-
der the development of NLP technologies. On the
other hand, the majority of NLP research is biased
towards high-resource languages, neglects diverse
linguistic typologies (Joshi et al., 2020), and often
relies on the availability of large-scale data. Includ-
ing endangered languages can help diagnose NLP
models’ generalizability (Bender, 2011) and push
towards universal and data-efficient approaches.

In this work, we address three important steps
on the roadmap of NLP for language revitalization:
starting from “before NLP” to “NLP for language
education” to “language-specific NLP research”.
Before diving into NLP research, we first suggest
that NLP practitioners, who are often “outsiders”
of indigenous communities, become aware of three
important principles: understand and respect first,
decolonize research, and build a community. We
especially want to promote building a community.
Since few people are speaking, learning, or study-
ing an endangered language, the knowledge of
each individual, the collected resources, and the
developed models should be shared as widely and
sustainably as possible. Hence, we need a commu-
nity to support this (see Section 2).

Second, language revitalization is an attempt
to reverse the decline of a language (Tsunoda,
2013). Fundamentally, this requires an increase
in the number of active speakers to bring the lan-
guage back to day-to-day use (Austin and Salla-
bank, 2011). Due to the lack of inter-generation
transmission, language education in school or on-
line is important. We introduce three approaches
for applying NLP techniques in assisting language
education (Section 3): automated quiz generation,
automated assessment, and community-based lan-
guage learning. The last approach connects to our
previous point about building a community.

Next, we introduce the case study of Cherokee;
an endangered* Native American language with
only 2,000 fluent first-language speakers remain-
ing. We first review its history (Section 4.1) to
understand how social, political, and economic re-
pression have harmed the Cherokee people and
their language. Then, we discuss a few linguis-
tic distinctions of Cherokee (Section 4.2), includ-
ing polysynthesis, word order, etc., which can help

3UNESCO has identified the dialect of Cherokee in Ok-

lahoma is “definitely endangered”, and the one in North Car-
olina is “severely endangered”.

us design linguistically informed NLP models. In
Section 5, we review some existing high-quality
Cherokee resources and propose two methods to
enrich resources: community-based resource col-
lection (which also relates to our previous point of
building a community) and automatic data mining.
Lastly, based on conversations with some Chero-
kee speakers/researchers, we dive deep into sev-
eral NLP tools that seem advantageous for commu-
nity members and may be able to create new usage
domains for the language, and we point out the key
challenges of their development (Section 6).

In summary, we propose suggestions to NLP
practitioners, approaches of NLP-assisted lan-
guage education, and directions for Cherokee lan-
guage processing. We hope that our work can in-
crease awareness of Cherokee and encourage more
work on minority languages.

Last but not the least, the authors of this work
come from both the Cherokee community (Ben-
jamin E. Frey) and the NLP community (Shiyue
Zhang and Mohit Bansal). Prof. Benjamin E. Frey
is a proficient second-language Cherokee speaker
and a citizen of the Eastern Band of Cherokee Indi-
ans. He has been teaching Cherokee and contribut-
ing to Cherokee revitalization for more than 10
years. He initiated our collaboration and continues
bridging the gap between the Cherokee language
and language technologies. In addition, we have
been talking with some other Cherokee community
members, including David Montgomery and Eva
Marie Garroutte. Prof. Eva Marie Garroutte from
Boston College said: “As a citizen of the Cherokee
Nation, I am very concerned for the preservation
of my tribe’s endangered language and I am con-
vinced that Dr. Frey’s work represents the most
promising project known to me for advancing this
goal.” Though by no means the views of this pa-
per can represent the whole Cherokee community,
our proposals are strongly initiated/motivated by
Cherokee community members and grounded by
NLP practitioners.

2 Before Diving into NLP Research

We suggest NLP practitioners, who are often “out-
siders” of the indigenous communities, three gen-
eral principles to follow before conducting NLP re-
search on endangered indigenous languages.

Understand and Respect First. Meaningful ad-
vances in building speech and language technolo-
gies for under-resourced languages hinge upon be-


ing able to understand those languages’ speaker
communities and their needs. Although the ini-
tial temptation among NLP researchers might be
to dive in with questions about particular compu-
tational tools, that conversation cannot unfold un-
til the speaker communities’ more basic needs are
met: the need for respect, reciprocity, and under-
standing. It may be tempting to say “this is out-
side the scope of our current research,” yet these
kinds of behaviors and assumptions are the very be-
haviors that led to the disenfranchisement of these
groups. When we ignore someone’s common hu-
manity and assume that our need for control over
the narrative and the situation is greater than their
need to be seen and respected, we participate in
the same marginalizing and dehumanizing behav-
iors that led to the problem we are purporting to
address. Therefore, it is instrumental that we ad-
dress the cultural practices and social norms of en-
dangered language communities before assuming
we know how to position ourselves, them, and our
research within their communities.

Decolonize Research. Decolonizing research is
to place indigenous voices and needs in the center
of the research process (Smith, 1999; Datta, 2018;
Bird, 2020a). As NLP researchers, we are used
to certain methodologies. When it comes to ques-
tions about endangered languages, it is tempting
for us to formulate the new problems we encounter
as what we are familiar with. However, we should
always question ourselves: Is the formulation suit-
able for the language we conduct research on? Are
the methodologies we familiar with the only true
ways to solve the problems? Unquestioned focus
on typical methodologies can make us treat lan-
guages as commodities and start to play a “num-
ber game” (e.g., the size of the data) and forget the
real problem, language revitalization, we intend to
solve in the first place (Dobrin et al., 2007). At ev-
ery research step, it is critical to weigh the burden
we put upon the speakers against the benefit that
the research can bring back to their community. If
the research outcome conveys no new knowledge,
information, or benefit to the community, it is no
different from “taking” indigenous knowledge that
has occurred over the centuries. That is exactly
why the word “research” is sometimes the direst
(i.e., conjuring up bad memories) word in indige-
nous world’s vocabulary (Smith, 1999). Finally, it
is important to carefully deal with copyright and
data governance; meanwhile, we advocate open-

sourced and community-contributed works.

Build a Community. Fundamentally, we want
to work together with people from the indigenous
communities (Bird, 2020a, 2021). It is the most ef-
fective way to foster mutual understanding. We
should communicate with the indigenous people
and get to know their priorities. Common attitudes
need to be fostered, common interests need to be
found, and common goals need to be set up, before
performing the research. These all lead to a com-
munity. We would imagine that there is an online
community (a website) where native speakers can
share their knowledge and language learners can
find resources and learn the language together (see
Section 3). People can share resources and par-
ticipant in machine-in-the-loop resource collection
projects (see Section 5). NLP researchers can eval-
uate and share their models in this community. En-
tertaining language learning or resource collection
games can be launched. We hope the community
can support wide and sustainable collaborations be-
tween indigenous speakers, language learners, and
NLP practitioners. Compared to local communi-
ties of the speakers, this community will be greatly
supported by technologies. A few NLP commu-
nities, e.g., MasakhaneNLP (focusing on African
languages) and SIGEL (special interest group en-
dangered languages), have been built. Differently,
the community we promote here will support both
NLP research and language learning. Lastly, com-
pared to Telegram groups (we are in a few different
Telegram groups with Cherokee community mem-
bers), we want to build a more open community
that everyone can have access to.

3 NLP-Assisted Language Education

Since little inter-generation language transmission
is happening, language education is an essential
requirement of language revitalization. Computer-
assisted language learning has a long-standing his-
tory (Higgins, 1983) and two workshops, BEA*
and NLP4CALL-, are held for research on apply-
ing NLP for language education. Here, we discuss
three ways in which NLP can potentially assist lan-
guage education of endangered languages.

Automated Quiz Generation. The most direct
way, in which NLP can help, is automatically gen-
erating quizzes for language learners. Practicing

“https: //aclanthology.org/venues/bea/
https: //aclanthology.org/venues/nlp4cal1/


and producing the language in questions are crit-
ical to language acquisition (Gass and Mackey,
2013). Usually, language instructors manually
design the quizzes, which is tedious and time-
consuming; not to mention, there are not a lot
of instructors for endangered languages. How-
ever, given the available text of endangered lan-
guages, NLP can easily and automatically gener-
ate cloze questions. It can also help find distract-
ing wrong answers that happen in a similar con-
text and thus form multi-choice questions (Hill and
Simha, 2016; Susanti et al., 2018). To increase
playfulness, language learning games, e.g., cross-
word puzzles and flashcards, can also be automati-
cally generated (Rigutini et al., 2012; Xu and Inga-
son, 2021). Since these applications involve very
basic language processing steps, NLP techniques
can be reliably and easily applied.

Automated Assessment. Another widely stud-
ied topic is NLP-supported automatic assessment.
Though a lot of advanced assessments, e.g., gram-
mar error correction (Bryant et al., 2019), essay
grading (Chen et al., 2016), are difficult to be
applied for endangered languages, we argue that
some easier assessments are feasible. For exam-
ple, automatic error analysis and template-based
feedback can be provided for language learning
quizzes. Another challenging but feasible assess-
ment is to assess the readability or difficulty of lan-
guage learning materials to provide suitable learn-
ing plans for learners of different levels. Using
statistic and linguistic features, such as word fre-
quency, morphology or syntactic complexity, etc.,
readability and difficulty can be automatically pre-
dicted (Schwarm and Ostendorf, 2005; Vajjala and
Meurers, 2012). However, basic NLP tools, like
POS tagger, dependency parser, morphology ana-
lyzer, need to be developed before these applica-
tions can be realized. The development of these
tools requires small but highly-curated data (Blasi
et al., 2021).

Community-based Language Learning. Free
online language learning platforms that integrate
automated quiz generation and assessment have
been developed, e.g., Oahpa (Uibo et al., 2015).
Taking one step further, we believe that a more
effective approach of supporting endangered lan-
guage education is to build an online and collab-
orative language learning platform, following the
human computation technique (Von Ahn, 2008).

When using technologies to assist in language re-
vitalization, we often face a dilemma. On the one
hand, due to the endangerment, there is not a lot
of resources available and it is very expensive (in
terms of time, effort, and cost) to collect resources
from speakers. On the other hand, machines strug-
gle to reach “useable” and “helpful” performances
without a decent amount of training data. Human
computation aims at combining human and com-
puter to solve problems neither of them could solve
alone (Von Ahn, 2008; Garcia, 2013). The most
famous example is Wikipedia where Internet users
contribute their knowledge together, and incredi-
bly high-quality content has been created. Other
successful cases are Duolingo and Tatoeba. Both
are for language learners to translate web text and
rate each other’s translations. Then, the translated
text can serve as learning materials and training
data for NLP models. However, Tatoeba only has
an English interface, and mixes languages on the
same site, making it hard to find peer learners of
under-resourced languages. Though Duolingo has
language-specific sites, it supports 23 languages
so far. Therefore, how to make use of collabo-
rative language learning platforms for endangered
languages is a big challenge. Nonetheless, we be-
lieve that it is a promising path to take for teaching
endangered languages to the young generation in
this information age.

4 The Cherokee Language

Starting from this section, we illustrate the situa-
tion of endangered languages through the example
of Cherokee. We first review its history and lin-
guistics. In the NLP area, we hardly get to know
the languages and often let the model learn statis-
tical patterns automatically from the data. How-
ever, it is critical to have basic knowledge of the
language when contributing to its revitalization.

4.1 History of the Cherokee People and Their
Language

Tribal Sovereignty. Before encountering Euro-
peans, American Indians were already governing
themselves. By drafting treaties with indigenous
nations, the colonial powers implicitly recognized
their sovereignty. Those treaties are still valid to-
day, and tribal peoples are very much operating as
sovereign nations, separate from the US (NCAI,
2020). There are three federally recognized na-
tions of Cherokee people: Cherokee Nation of Ok-


lahoma (CN), United Keetoowah Band of Chero-
kee Indians (UKB), and Eastern Band of Cherokee
Indians (EBC]). Traditional Cherokee homeland
covered parts of what are now eight US states.°
EBCI is composed of those Cherokees who were
able to remain in their homeland. CN is largely
comprised of the descendants of those who were
forcibly removed to Indian Territory along the in-
famous Trail of Tears in 1838 (Perdue and Green,
2007), while the UKB is composed largely of those
whose ancestors chose to remove themselves west
of the Mississippi. Although the three nations are
politically independent, they all descend from the
same Cherokee people, and maintain common in-
terests, cultural elements, and language.

The Language and its Dialects. Cherokee is the
only surviving member of the Southern Iroquoian
language family, which have separated from the
Northern Iroquoian languages about 4,000 years
ago (Julian, 2010). James Mooney identified three
main dialects of Cherokee: the Overhill dialect,
the Underhill dialect (has died out), and the Mid-
dle, or Kituwah dialect. The Overhill dialect is
primarily spoken in Oklahoma, and the Middle di-
alect is predominantly spoken in North Carolina
today. Although according to UNESCO, both di-
alects are endangered, Cherokee is comparatively
well-reported among American Indian languages.
This is partially due to its writing system known
as the 85-character Cherokee syllabary. It was in-
vented in the early 1820s by Sequoyah (Britannica,
2021). The Cherokees have a newspaper written in
their own language: the Cherokee Phoenix. The
Phoenix, alongside the Cherokee New Testament,
formed cornerstones of the Cherokee language in
the 1800s on which many current language preser-
vations and archiving projects rest.

Language Endangerment. Cherokee was ro-
bustly spoken until around the 1930s. The pri-
mary factor being responsible is the US gov-
ernment’s “civilization” policy, which aimed to
remove American Indians’ cultural distinctions
(Spring, 2016). Federal boarding schools were
created on the model of military institutions by
Richard H. Pratt under the philosophy of “kill the
Indian, save the man” (Pratt, 2013). American In-
dian children were sent to residential schools to be
educated in how to live in ways more similar to

°North Carolina, South Carolina, Georgia, Kentucky, Ten-
nessee, Alabama, Virginia, and West Virginia.

their white contemporaries. School overseers cut
their hair, forced them to abandon their traditional
dress, and punished them for speaking their tradi-
tional languages. Beyond the trauma, when they
returned to communities, banks, post offices, fac-
tories, and grocery stores were all controlled non-
locally. People working in them either no longer
spoke Cherokee because they were not from Chero-
kee communities or because their employers were
not Cherokee speakers. This transition contributed
to the decline of the language in daily use, until the
first generation grew up with only English as the
language of the home around 1950s (Gulick, 1958;
Frey, 2013). Recently, the larger project of lan-
guage revitalization, of which this paper is a part,
endeavors to return the language to regular day-to-
day use in the Cherokee communities.

4.2 Cherokee Linguistics

Polysynthetic. Cherokee, like most American
Indian languages, is polysynthetic. This means
that words are primarily composed of a root whose
meaning is modified by multiple prefixes and suf-
fixes. The word F§, gega, can be divided up: g-,
-e-, -ga. The g- prefix indicates that the subject of
the verb is lst person singular while the -ga suf-
fix indicates that the action happens in the present
tense and the aspect is progressive. The verb root
-e- conveys the idea of motion. The simplest verb
form in Cherokee will contain at minimum a root, a
pronominal prefix, and a tense/aspect suffix. One
oft-noted aspect of Cherokee grammar is its classi-
ficatory system, wherein verbs with direct objects
must conjugate to indicate the physical shape of
the direct object. The verb “I have,” for instance,
could appear in any of the following ways: Agiha
(I have (solid)), Agineha (I have (liquid)), Agwvya
(I have (long & rigid)), Agina’a (I have (flexible)),
Agikaha (I have (animate)). Cherokee also has
pre-pronominal prefixes that can specify the geo-
graphical location of particular events, such as wi-
(translocative), which indicates that the action will
happen at a distance away from the speaker, and di-
(cislocative), which indicates the action will hap-
pen at a distance approaching the speaker.

Word Order. Word order in Cherokee is de-
pendent on the larger pragmatic context in which
the sentence appears, with new information or
timeframes occurring before the verb and old
or established information occurring post-verbally.
Subject-object agreement is handled largely via the


dual-argument pronominal prefixes. E.g., in “I
see it,” PWAGeb (tsigowatiha), the pronominal
prefix ¢si- indicates Ist person singular (“IT”) act-
ing on 3rd person singular (“it”). In DYAGTb
(agigowatiha), we change tsi- to agi-, which means
3rd person singular acting on Ist person singular.

Person & Number. Although English has only
two categories of number: singular and plural,
Cherokee has a third, dual category. Therefore, a
verb in Cherokee can be conjugated in first, sec-
ond, or third person and specified for either singu-
lar, dual, or plural subjects. Dual and plural pre-
fixes in the first person must then be further subdi-
vided by clusivity, yielding 1st-person dual inclu-
sive (you & I) or exclusive (she/he & I), 1 st-person
plural inclusive (all of us) or exclusive (they & I).
The second person can inflect for dual (you two)
or plural (you all). Cherokee does not have a third-
person dual form, and speakers usually use the plu-
ral form when referring to two third persons.

Verb-centric. Cherokee is very verb-centric,
and verbs comprise 75% of Cherokee (Feeling,
1975). Cherokee nouns are divided into root nouns
(have no verbal inflection attached to them) and
derived nouns (carry verbal morphology). Sim-
ilarly, Cherokee adjectives can be distinguished
from verbs in that their forms cannot carry the
tense/aspect morphology typical of actual verbs.
Thus, to say someone is skinny, OCU (ulesoda)
carries the pronominal prefix u-, indicating 3rd
person singular, while O'¢FU FRT (ulesoda gesv’i)
marks past tense by adding a separate copula (“to
be”) that carries the tense/aspect suffix -v 7.

Evidentiality. Cherokee is also marked by a sys-
tem of evidentiality (indicating whether one has
firsthand knowledge of past events, or if one is re-
porting on hearsay). E.g., one might say D&ed0ET
(agasgv i), “it was raining (and I have firsthand
knowledge of this)” vs. D&eadFT (agasge’i), “it
was raining (from what I understand).” Interest-
ingly, this phenomenon applies regardless of the
assumed truth of the statement in question.

Phoneme. Cherokee’s phoneme inventory is,
like other Iroquoian languages, almost completely
bereft of bilabial sounds. It entirely lacks the p or b
phonemes, along with f/v, 6/d, and any r sound. It
has six vowels: a, e, i, 0, u, and v, and are generally
pronounced with continental values, as in Spanish,
except for v. Consonant inventory is small, at only

13, and most will be familiar to English speakers.
The main exception is the voiceless alveolar frica-
tive 4, likely more familiar to Icelandic speakers.

5 Cherokee Language Resources

The availability of language resources is not only
important for language education but also de-
termines the development of NLP technologies.
Cherokee is categorized into “The Scraping-Bys”
by Joshi et al. (2020), which means it has some
amount of data but solid movements still need to
be taken to increase the awareness of the language.

Existing Resources Online. It is not easy to lo-
cate a lot of Cherokee resources on the Internet,
compared to other high-resource languages. Here,
we point to a few places where high-quality Chero-
kee resources for language learning or NLP model
training can be found: (1) Cherokee-English Dic-
tionary’ has online Cherokee-English dictionaries,
a transliteration tool, a grammar guide, and a few
Cherokee text or audio corpora; (2) Cherokee Na-
tion website® contains Cherokee online classes,
learning materials, fonts and keyboards, etc. (3)
UNC Cherokee Program website’ has UNC Chero-
kee class resources and pointers to external re-
sources; (4) Cherokee Language Github group!?
gathers a lot of Cherokee text and audio data,
as well as initial attempts for speech synthesis
and some other NLP tools. (5) The Cherokee
Phoenix! publishes all-Cherokee issues as well
as some bilingual articles with Cherokee audios. !*
(6) We released around 17K Cherokee-English par-
allel data (Zhang et al., 2020).!° In addition,
Cherokee Wikipedia is available but its content is
noisy. A Cherokee resource catalog can be built
up in the future for easier locating resources.

Community-based Resource Collection. Be-
sides existing resources, we suggest collaborative
resource collection, which can be integrated with
the community-based language learning platform
we introduced in Section 3. A simple feature of
this platform could be a dropbox where people who
are willing to contribute their resources can drop

"https: //www.cherokeedictionary .net

‘https: //language. cherokee .org

https: //cherokee.web.unc.edu

https: //github. com/CherokeeLanguage

"https: //www.cherokeephoenix.org

nttps://tinyurl.com/4nf9txkf

https: //github.com/ZhangShiyue/ChrEn/tree/
main/data/parallel_01172022


in the files they have.'* The back-end program
can support any kind of data processing based on
the contributor’s request and permission. Then, the
resources can be shared back with the community
as language learning and model training resources.
Second, for more complex data annotation tasks,
like POS tagging, dependency parsing, we suggest
setting up game with a purpose (GWAP) applica-
tions on this website. GWAP is introduced by Luis
Von Ahn (Von Ahn, 2006; Von Ahn and Dabbish,
2008) who is also the founder of Duolingo. One fa-
mous example is his ESP game (Von Ahn and Dab-
bish, 2004) which formulates the image recogni-
tion task as a game. Following this idea, NLP prac-
titioners can design diverse games on the platform
to increase the fun and engagement of language
learning and resource collection. In addition, this
platform will focus more on what kind of mate-
rials the Cherokee community members consider
important to preserve instead of what the NLP re-
searchers find most valuable.

Automatic Resource Mining. As NLP practi-
tioners, we should try to make the most use of
computers for collecting resources automatically.
A lot of automatic data mining methods have
been proposed to mine monolingual or bilingual
text from the noisy web or Wikipedia (Guo et al.,
2018; Artetxe and Schwenk, 2019; Schwenk et al.,
2019; Wenzek et al., 2020; Schwenk et al., 2021;
Arkhangelskiy, 2019). Though the mined text has
many errors or noises, previous works demonstrate
that neural NLP models are surprisingly good at
using noisy data for training. However, some ad-
ditional NLP components, like language identifier
and multilingual embeddings, need to be devel-
oped to support the data mining. For instance,
to mine Cherokee-English parallel text, we will
need to map English and Cherokee sentences to the
same representation space to compute their similar-
ity. However, existing tools of getting multilingual
sentence embeddings, like LASER,'> do not sup-
port Cherokee, and Cherokee is not related to or
sharing scripts with any supported languages. But,
given the existing Cherokee-English parallel data
(Zhang et al., 2020), we can re-train these tools and
have Cherokee being supported. Note that these
automatic text miners can start with both crawled
web text and OCR-processed text (Section. 6.2).

'4An example can be found at https: //cherokee.web.
unc .edu/submit-materials-to-database.
https: //github.com/facebookresearch/LASER

6 NLP Tools for Cherokee Language
Processing

Based on our conversation with a few Cherokee
speakers, they agree that some NLP tools are good
to have and hold the potentials to be useful in
Cherokee language revitalization. Thus, some ini-
tial attempts have been made by the Cherokee Lan-
guage Github group and us (Zhang et al., 2020,
2021). Hence, we dive deep into several specific
NLP tools for Cherokee language processing in
this section. And for any NLP tool we develop, we
want to evaluate it by the Cherokee speakers, and
we suggest open-sourcing it for free usage. Con-
necting to our “build a community” proposal, we
hope that NLP models can also be shared and used
widely and sustainably in the community.

6.1 Machine Translation.

Ideally, a good machine translation (MT) system
can automatically translate the big amount of En-
glish text to Cherokee; or it can assist human trans-
lators. Dr. David Montgomery, a citizen of Chero-
kee Nation and a Cherokee language learner, com-
mented on MT: “It would be a great service to
Cherokee language learners to have a translation
tool as well as an ability to draft a translation of
documents for first-language Cherokee speakers to
edit as part of their translation tasks. If these tools
can be made to work accurately, they would be
transformative for the Cherokee language.” Pre-
viously, we collected parallel text and developed
an MT online translation demo between Chero-
kee and English (Zhang et al., 2020, 2021). How-
ever, our system can translate fragments of the
source sentence but make major mistakes, which
is far from being practically useful. The first chal-
lenge of MT development is the lack of data. Au-
tomatic data mining can help enrich MT training
data (Section 5). But we still need high-quality
and diverse evaluation data because existing eval-
uation sets (Zhang et al., 2020) are from limited
domains (the majority is the Bible). Recently, Flo-
res101, an MT evaluation benchmark covering 101
languages, has been created (Goyal et al., 2021).
Though it has not yet covered Cherokee, we hope
it can happen in the future.

The second challenge is processing and produc-
ing Cherokee text. Cherokee has rich morphol-
ogy (see Section 4.2). One Cherokee word can be
translated into one English sentence. Intuitively,
we would think subword tokenization (Sennrich


Original Screenshot
OCR tools | WER CER | WER CER
Tesseract 0.355 0.230 | 0.151 0.063
Google Vision | 0.533 0.199 | 0.468 0.074

Table 1: OCR performance of two OCR tools on our
evaluation sets. WER: word error rate, CER: character
error rates. For both WER and CER, lower is better.

et al., 2016; Kudo, 2018) is helpful. However, pre-
viously, we (Zhang et al., 2020) showed that ap-
plying subword tokenization for English to Chero-
kee translation is harmful. We argue that it is be-
cause we processed Cherokee text in its syllabary
rather than in transliterated Latin script, however,
morphemes are easier to be learned from the lat-
ter. E.g., in GLObAR, tsaquadvsidisv (when I was
growing up), the prefix ¢s- marks relative clauses,
but G is tsa. We suspect that character-level gener-
ation (in Latin script) would work better for Chero-
kee. Additionally, Cherokee has flexible word or-
der that is often determined by whether the infor-
mation is new or old in relation to the larger dis-
course (Section 4.2). Thus, document-level trans-
lations are more reasonable than typical sentence-
level translations.

6.2 Optical Character Recognition.

The majority of Cherokee text is in the format of
manuscripts or books, so as many other endan-
gered languages (Joshi et al., 2020; Bustamante
et al., 2020). Though humans can read them, they
are not machine-readable, which restricts the flex-
ibility of their use, e.g., automatically creating lan-
guage learning quizzes. Optical character recog-
nition (OCR) (Smith, 2007) can help extract plain
text from PDFs or images. Fortunately, existing
OCR tools, like Tesseract-OCR!° and Google Vi-
sion OCR API'’, support Cherokee and have de-
cent accuracy. However, OCR accuracy is highly
influenced by image quality. If the image has
a noisy background or the text is surrounded by
colorful pictures (which often happens in children
books), the OCR accuracy will drop significantly.

To prove this, we create two evaluation sets
from Cherokee books (including Cherokee New
Testament, children books, Cherokee narratives):
(1) Original has 20 images, and each image is
one complete page from a book; (2) Screenshot is
obtained by manually conducting screenshots and

‘https: //github.com/tesseract-ocr/
https: //cloud.google.com/vision/docs/ocr

| audio to phonetic text | audio to syllabic text

0.64 0.21

WER

Table 2: The ASR results of finetuned XLSR-53 (Con-
neau et al., 2020) models. WER: word error rate.

cutting out text from the 20 images, i.e., remov-
ing background noises. For each image in two
sets, we manually annotate the corresponding text.
Table 1 shows the results of Tesseract-OCR and
Google Vision OCR API. Both OCR tools achieve
significantly lower error rates on the Screenshot
set than on the Original set, which demonstrates
the importance of cleaning the images. Tesseract-
OCR shows better performance than Google Vi-
sion OCR, especially it is better at detecting word
boundaries. Although ways to improve image
quality are available,'® an easy-to-use tool need to
be developed. OCR post-correction methods can
also be applied (Riyhwani et al., 2020).

6.3 Speech Recognition and Synthesis.

Automatic speech recognition (ASR) (Povey et al.,
2011) can help language documentation, though
indigenous community members may prefer unas-
sisted transcription (Prud’hommeaux et al., 2021).
Moreover, ASR holds the potential to automati-
cally transcript audio data and thus enrich text cor-
pus. A good amount of Cherokee audio data can
be found from the “Cherokee Voices, Cherokee
Sounds” radio, Cherokee Phoenix, and recorded
meetings. ASR can automatically transcript these
audios to produce valuable Cherokee text data.
Recently, models that are first pre-trained on au-
dio data and then finetuned on audio-text data
have shown great advantages in performing ASR
(Baevski et al., 2020). Especially, Conneau et al.
(2020) pretrain and finetune a model on 53 lan-
guages and release XLSR-53 (supports ASR for
53 languages). It shows reasonable generalizabil-
ity to unseen and low-resource languages. This
sheds light on developing ASR for endangered lan-
guages.

Hence, we test its performance for Cherokee
ASR. Using the audio-text data open-sourced!?
or shared privately by Michael Conrad, we build
two ASR models: (1) audio to phonetic text, (2)
audio to syllabic text. See more details in Ap-
pendix A.1. As shown in Table 2, we get surpris-

8https://tinyurl .com/29xnewu9
https: //github.com/CherokeeLanguage/
cherokee-audio-data


| Precision Recall Fl
Unigram LM 16.6 19.6 17.9
BPE 14.4 16.5 15.4
Morfessor 16.6 16.3 16.5

Table 3: The alignment between subwords and gold
morphemes.

ingly good performances, especially for the audio-
to-syllabic-text model.”° This is very promising,
especially when knowing the fact that more self-
training strategies can be applied, e.g., pretrain
the speech encoder with Cherokee audio data, and
more audio-text training data can be compiled.
Text-to-speech synthesis (TTS) is more difficult
to develop than ASR; nevertheless, following the
pretrain-then-finetune paradigm, TTS models for
extremely low-resource languages have been intro-
duced (Xu et al., 2020).

6.4 Tokenization and Morphology Parsing.

Tokenization is an essential pre-processing step of
most NLP models, and it is related to morphol-
ogy parsing. Subword tokenization has become
de facto (Sennrich et al., 2016; Kudo, 2018). It
segments a word into frequent subwords, and sub-
words are supposed to align with morphemes. Bet-
ter alignment with morphemes can lead to better
downstream performance (Bostrom and Durrett,
2020), while current subword tokenization meth-
ods struggle to perform well in morphologically
rich languages (Amrhein and Sennrich, 2021).
Here, we evaluate how well subword tokeniza-
tion can learn real morphemes for Cherokee. We
train two subword tokenizers,?! Unigram LM
(Kudo, 2018) and BPE (Sennrich et al., 2016), and
one morphology parser, Morfessor (Smit et al.,
2014), on our previous MT training set (Zhang
et al., 2020). Instead of using the original syl-
labic text, we transliterate text into Latin script to
make it easier to learn morphemes. We collect
gold (expert-labeled) morphemes of 372 Cherokee
words from Cherokee Narratives (Feeling, 2018).
Then, we use the pretrained tokenizers or parser to
tokenize these 372 words and evaluate the align-
ment between subwords and gold morphemes. As
shown in Table 3, subwords are poorly aligned
with gold morphemes. Nonetheless, Unigram LM
(Kudo, 2018) demonstrates better ability of induc-

°The same model finetuned on CommonVoice’s Turkish
data gets WER=0.35. https: //tinyurl.com/62eykh9m
2lWe use SentencePiece (Kudo and Richardson, 2018).

ing morphemes, which is consistent with the obser-
vation made by Bostrom and Durrett (2020). We
think better representation methods need to be in-
troduced for Cherokee, and the labeled data from
Feeling (2018) can provide supervision.

6.5 POS-Tagging and Dependency Parsing.

More basic NLP tools like POS tagger and de-
pendency parser are under-developed for Chero-
kee. These tools can not only support the devel-
opment of other NLP tools but also be used to
predict the readability of language learning mate-
rials (Section 3). Moreover, data for these tasks
can serve as language learning materials for under-
standing Cherokee linguistics. Though unsuper-
vised methods have been proposed (Stratos et al.,
2016; Kim et al., 2019), usually small but high-
quality labeled data, like Universal Dependencies
(Nivre et al., 2016), is needed (Blasi et al., 2021).
Therefore, data annotation by experts is required
and community-based data collection strategies
can be applied (Section 5). Moreover, the parallel
English data and English tagger/parser can assist
the annotation on the Cherokee side, which will
also produce English-Cherokee word/phrase-level
alignments as by-products. These alignments are
valuable Cherokee language education resources,
e.g., asking students when you have “structure X”
in English, what is the corresponding “structure Y”
in Cherokee?

7 Conclusion & Future Work

In this work, we discuss how NLP can help revital-
ize endangered languages. We first suggest gen-
eral principles to NLP practitioners and propose
ways of NLP-assisted language education. Espe-
cially, we promote building a (online) community
that support collaborative language learning, re-
source collection, and knowledge sharing. Second,
we conduct a case study for Cherokee (a severely-
endangered Native American language). After re-
viewing Cherokee history and linguistics, we pro-
pose two methods of enriching Cherokee resources
and discuss the developments of several NLP mod-
els that people from the Cherokee community are
interested in. We hope our work can encourage
future work to think and plan the path forward
for other endangered languages. In the future, we
hope to broaden our collaboration to even more
Cherokee community members and build meaning-
ful relationships with tribal governments, so that


we can develop more useful applications through
NLP techniques for supporting Cherokee revital-
ization.

8 Broader Impact Statement

The content of this paper is based on and inspired
by our practice in Cherokee Language Revitaliza-
tion. The conclusions and suggestions may or may
not generalize to other endangered languages. For
example, since Cherokee has its own syllabary and
can be written down, we are interested in speech
recognition for audio transcription. Even though
some methods can directly translate audio to text
of another language, we do not want to skip the
transcription step. However, for some oral lan-
guages, they may want to prioritize translation
over transcription to tackle the transcription bottle-
neck (Bird, 2020b). On the other hand, our posi-
tion is influenced by Crystal (2014), who thinks
using electronic technology is important for lan-
guage revitalization. Therefore, a lot of our pro-
posals, like “building an online community”, may
have an assumption that computers and the Internet
have been or can be widely accepted and used in
the indigenous community. However, it may not
be true in every indigenous community.

Acknowledgments

We thank the reviewers for their helpful comments.
We thank Archiki Prasad and Zhiyuan Tang for pro-
viding guidance on developing ASR models. We
thank Michael Conrad for providing Cherokee au-
dios and transcriptions. We thank David Mont-
gomery and Eva Marie Garroutte for providing
their statements. We thank the Kituwah Preserva-
tion and Education Program (KPEP), the Eastern
Band of Cherokee Indians, and the Cherokee Na-
tion. This work was supported by NSF-CAREER
Award 1846185, ONR Grant N00014-18-1-2871,
NSF-AI Engage Institute DRL-2112635, and a
Bloomberg Data Science Ph.D. Fellowship. The
views contained in this article are those of the au-
thors and not of the funding agency.

References

Chantal Amrhein and Rico Sennrich. 2021. How suit-
able are subword segmentation strategies for trans-
lating non-concatenative morphology? In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2021, pages 689-705.

Timofey Arkhangelskiy. 2019. Corpora of social me-
dia in minority uralic languages. In Proceedings of
the Fifth International Workshop on Computational
Linguistics for Uralic Languages, pages 125-140.

Mikel Artetxe and Holger Schwenk. 2019. Margin-
based parallel corpus mining with multilingual sen-
tence embeddings. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 3197-3203.

Peter K Austin and Julia Sallabank. 2011.
bridge handbook of endangered languages.
bridge University Press.

The Cam-
Cam-

Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. arXiv preprint arXiv: 2006. 11477.

Emily M Bender. 2011. On achieving and evaluating
language-independence in nlp. Linguistic Issues in
Language Technology, 6(3):1—26.

Steven Bird. 2020a. Decolonising speech and lan-
guage technology. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics,
pages 3504-3519, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.

Steven Bird. 2020b. Sparse transcription. Computa-
tional Linguistics, 46(4):713-744.

Steven Bird. 2021. Lt4all!? rethinking the agenda. In
EMNLP.

Damian Blasi, Antonios Anastasopoulos, and Gra-
ham Neubig. 2021. Systematic inequalities in lan-
guage technology performance across the world’s
languages. arXiv preprint arXiv:2110.06733.

Kaj Bostrom and Greg Durrett. 2020. Byte pair encod-
ing is suboptimal for language model pretraining. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: Findings,
pages 4617-4624.

Britannica. 2021. Sequoyah. Encyclopedia Britannica,
28 Jul.

Christopher Bryant, Mariano Felice, @istein E Ander-
sen, and Ted Briscoe. 2019. The bea-2019 shared
task on grammatical error correction. In Proceed-
ings of the Fourteenth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
52-75.

Gina Bustamante, Arturo Oncevay, and Roberto
Zariquiey. 2020. No data to crawl? monolingual
corpus creation from PDF files of truly low-resource
languages in Peru. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference, pages
2914-2923, Marseille, France. European Language
Resources Association.


Jing Chen, James H Fife, Isaac I Bejar, and André A
Rupp. 2016. Building e-rater® scoring models using
machine learning methods. ETS Research Report Se-
ries, 2016(1):1-12.

Alexis Conneau, Alexei Baevski, Ronan Collobert,
Abdelrahman Mohamed, and Michael Auli. 2020.
Unsupervised cross-lingual representation learn-
ing for speech recognition. arXiv preprint
arXiv:2006.13979.

David Crystal. 2014. Language Death. Canto Classics.
Cambridge University Press.

Ranjan Datta. 2018. Decolonizing both researcher and
research and its effectiveness in indigenous research.
Research Ethics, 14(2):1—24.

Lise M Dobrin, Peter K Austin, and David Nathan.
2007. Dying to be counted: The commodification
of endangered languages in documentary linguistics.
In Proceedings of the conference on language docu-
mentation and linguistic theory, pages 59-68. SOAS
London.

D. Feeling. 1975. Cherokee-English Dictionary.
Cherokee Nation of Oklahoma.

Durbin Feeling. 2018. Cherokee Narratives: A Linguis-
tic Study. University of Oklahoma Press.

Benjamin E Frey. 2013. Toward a general theory of lan-
guage shift: A case study in Wisconsin German and
North Carolina Cherokee. Ph.D. thesis, The Univer-
sity of Wisconsin-Madison.

Ignacio Garcia. 2013. Learning a language for free
while translating the web. does duolingo work? Jn-
ternational Journal of English Linguistics, 3(1):19.

Susan M Gass and Alison Mackey. 2013. The Rout-
ledge handbook of second language acquisition.
Routledge.

Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
ishnan, Marc’ Aurelio Ranzato, Francisco Guzman,
and Angela Fan. 2021. The FLORES-101 evalu-
ation benchmark for low-resource and multilingual
machine translation. CoRR, abs/2106.03193.

John Gulick. 1958. Language and passive resistance
among the eastern cherokees. Ethnohistory, 5(1):60—
81.

Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge,
Daniel Cer, Gustavo Hernandez Abrego, Keith
Stevens, Noah Constant, Yun-Hsuan Sung, Brian
Strope, et al. 2018. Effective parallel corpus mining
using bilingual sentence embeddings. In Proceed-
ings of the Third Conference on Machine Transla-
tion: Research Papers, pages 165-176.

John Higgins. 1983. Computer assisted language learn-
ing. Language Teaching, 16(2):102—114.

Jennifer Hill and Rahul Simha. 2016. Automatic gener-
ation of context-based fill-in-the-blank exercises us-
ing co-occurrence likelihoods and google n-grams.
In Proceedings of the 11th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 23-30.

Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the nlp
world. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 6282-6293.

Charles Julian. 2010. A history of the Iroquoian lan-
guages. Ph.D. thesis, University of Manitoba.

Yoon Kim, Chris Dyer, and Alexander M Rush. 2019.
Compound probabilistic context-free grammars for
grammar induction. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 2369-2385.

Taku Kudo. 2018. Subword regularization: Improving
neural network translation models with multiple sub-
word candidates. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 66—75.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 66—71.

Peter Ladefoged. 1992. Another view of endangered
languages. Language, 68(4):809-811.

NCAI. 2020. Tribal nations and the United States: An
introduction. National Congress of American Indi-
ans Washington, DC.

Joakim Nivre, Marie-Catherine De Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
vl: A multilingual treebank collection. In Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC’16), pages
1659-1666.

Theda Perdue and Michael D Green. 2007. The Chero-
kee nation and the trail of tears. Penguin.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The kaldi speech recognition
toolkit. In JEEE 2011 workshop on automatic speech
recognition and understanding, CONF. IEEE Signal
Processing Society.

Richard H Pratt. 2013. 35. The Advantages of Mingling
Indians with Whites. Harvard University Press.


Emily Prud’>hommeaux, Robbie Jimerson, Richard
Hatcher, and Karin Michelson. 2021. Automatic
speech recognition for supporting endangered lan-
guage documentation. Language Documentation &
Conservation, 15:491-513.

Leonardo Rigutini, Michelangelo Diligenti, Marco
Maggini, and Marco Gori. 2012. Automatic gener-
ation of crossword puzzles. International Journal
on Artificial Intelligence Tools, 21(03):1250014.

Shruti Rijhwani, Antonios Anastasopoulos, and Gra-
ham Neubig. 2020. Ocr post-correction for endan-
gered language texts. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 5931-5942.

Sarah E Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL’05), pages 523-530.

Holger Schwenk, Vishrav Chaudhary, Shuo Sun,
Hongyu Gong, and Francisco Guzman. 2021. Wiki-
matrix: Mining 135m parallel sentences in 1620 lan-
guage pairs from wikipedia. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 1351-1361.

Holger Schwenk, Guillaume Wenzek, Sergey Edunov,
Edouard Grave, and Armand Joulin. 2019. Ccmatrix:
Mining billions of high-quality parallel sentences on
the web. arXiv preprint arXiv: 1911.04944.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1715-1725.

Peter Smit, Sami Virpioja, Stig-Arne Grénroos, and
Mikko Kurimo. 2014. Morfessor 2.0: Toolkit for
statistical morphological segmentation. In Proceed-
ings of the Demonstrations at the 14th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 21-24, Gothenburg, Swe-
den. Association for Computational Linguistics.

Linda Tuhiwai Smith. 1999. Decolonizing Methodolo-
gies: Research and Indigenous Peoples. ERIC.

Ray Smith. 2007. An overview of the tesseract ocr en-
gine. In Ninth international conference on document
analysis and recognition (ICDAR 2007), volume 2,
pages 629-633. IEEE.

Joel Spring. 2016. Native americans: Deculturaliza-
tion, schooling, globalization and inequality. In
Deculturalization and the Struggle for Equality,
pages 40-59. Routledge.

Karl Stratos, Michael Collins, and Daniel Hsu. 2016.
Unsupervised part-of-speech tagging with anchor
hidden markov models. Transactions of the Associ-
ation for Computational Linguistics, 4:245—257.

Yuni Susanti, Takenobu Tokunaga, Hitoshi Nishikawa,
and Hiroyuki Obari. 2018. Automatic distractor
generation for multiple-choice english vocabulary
questions. Research and practice in technology en-
hanced learning, 13(1):1-16.

Tasaku Tsunoda. 2013. Language endangerment and
language revitalization. De Gruyter Mouton.

Hiroto Uchihara. 2013. Tone and accent in Oklahoma
Cherokee. State University of New York at Buffalo.

Heli Uibo, Jaak Pruulmann-Vengerfeldt, Jack Rueter,
and Sulev Iva. 2015. Oahpa! dpi! opiq! developing
free online programs for learning estonian and voro.
In Proceedings of the fourth workshop on NLP for
computer-assisted language learning, pages 51-64.

Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
Proceedings of the seventh workshop on building ed-
ucational applications using NLP, pages 163-173.

Luis Von Ahn. 2006. Games with a purpose. Computer,
39(6):92-94.

Luis Von Ahn. 2008. Human computation. In 2008
IEEE 24th international conference on data engi-
neering, pages 1—2. IEEE.

Luis Von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319-326.

Luis Von Ahn and Laura Dabbish. 2008. Designing
games with a purpose. Communications of the ACM,
51(8):58-67.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-
neau, Vishrav Chaudhary, Francisco Guzman, Ar-
mand Joulin, and Edouard Grave. 2020. Cenet: Ex-
tracting high quality monolingual datasets from web
crawl data. In Proceedings of the 12th Language
Resources and Evaluation Conference, pages 4003—
4012.

Douglas H Whalen, Margaret Moss, and Daryl Bald-
win. 2016. Healing through language: Positive
physical health effects of indigenous language use.
F'1000Research, 5(852):852.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
towicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Can-
wen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush.
2020. Transformers: State-of-the-art natural lan-
guage processing. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing: System Demonstrations, pages 38-45,
Online. Association for Computational Linguistics.


audio to phonetic text
train dev test

Total Duration 16h 6.3m 6.0m
Average Duration | 1.3s 1.4s 1.45

Statistics

audio to syllabic text
train dev test

Total Duration 3h 25.9m  -24.9m
Average Duration | 7.5s 7.68 7.38

Statistics

Table 4: The statistics of the data we use for developing
the ASR models. s/m/h stands for second/minute/hour.

Jin Xu, Xu Tan, Yi Ren, Tao Qin, Jian Li, Sheng Zhao,
and Tie-Yan Liu. 2020. Lrspeech: Extremely low-
resource speech synthesis and recognition. In Pro-
ceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Min-
ing, pages 2802-2812.

Xindan Xu and Anton Karl Ingason. 2021. Developing
flashcards for learning icelandic. In Proceedings of
the 10th Workshop on NLP for Computer Assisted
Language Learning, pages 55-61.

Shiyue Zhang, Benjamin Frey, and Mohit Bansal. 2020.
ChrEn: Cherokee-English machine translation for
endangered language revitalization. In Proceedings
of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 577—
595, Online. Association for Computational Linguis-
tics.

Shiyue Zhang, Benjamin Frey, and Mohit Bansal. 2021.
Chrentranslate: Cherokee-english machine transla-
tion demo with quality estimation and corrective
feedback. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing: System Demonstrations,
pages 272-279.

A Appendix

A.1 Data and Implementation Details of ASR

(1) audio to phonetic text: Given the audio, the
model outputs text showing its pronunciation, e.g.,
Su:dali (means “‘six”’). It follows Uchihara’s Mod-
ified Community Orthography (Uchihara, 2013).
(2) audio to syllabic text: Given the audio, the
model outputs text showing the Cherokee syl-
labary, e.g., PLE (means “six”).

We split our ASR data into training, develop-
ment, and testing sets. Table 4 lists the statistics.
It can be seen that we have more data for the audio
to syllabic text model, which probably causes its
good performance shown in Table 2.

We follow the ASR recipe provided by Hug-
gingface’s Transformers” (Wolf et al., 2020) to

~netps://github.com/huggingface/

finetune the pretrained XLSR-53 model (Conneau
et al., 2020). Specifically, we use learning rate=3e-
4, epoch=15, mask_time_prob=0.01. We run each
experiment for 3 times and report the average per-
formance on the testing set in Table 2.

transformers/tree/master/examples/pytorch/
speech-recognition
