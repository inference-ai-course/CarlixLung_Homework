arX1v:2510.09544v1 [cs.CL] 10 Oct 2025

Beyond Surface Reasoning: Unveiling the True
Long Chain-of-Thought Capacity of Diffusion
Large Language Models

Qiguang Chen!* Hanjing Li!* Libo Qin® Dengyun Peng! JinhaoLiu' Jiangyi Wang!
Chengyue Wu? Xie Chen? YantaoDu°® Wanxiang Che!’

| LARG, Research Center for Social Computing and Interactive Robotics, Harbin Institute of Technology,
? School of Computer Science and Engineering, Central South University,
3 The University of Hong Kong, + Shanghai Jiao Tong University, > ByteDance Seed (China)

Abstract:

Recently, Diffusion Large Language Models (DLLMs) have offered high throughput and effective sequential
reasoning, making them a competitive alternative to autoregressive LLMs (ALLMs). However, parallel decoding,
which enables simultaneous token updates, conflicts with the causal order often required for rigorous reasoning.
We first identify this conflict as the core Parallel-Sequential Contradiction (PSC). Behavioral analyses in both
simple and complex reasoning tasks show that DLLMs exhibit genuine parallelism only for directly decidable
outputs. As task difficulty increases, they revert to autoregressive-like behavior, a limitation exacerbated
by autoregressive prompting, which nearly doubles the number of decoding steps with remasking without
improving quality. Moreover, PSC restricts DLLMs’ self-reflection, reasoning depth, and exploratory breadth. To
further characterize PSC, we introduce three scaling dimensions for DLLMs: parallel, diffusion, and sequential.
Empirically, while parallel scaling yields consistent improvements, diffusion and sequential scaling are con-
strained by PSC. Based on these findings, we propose several practical mitigations, parallel-oriented prompting,
diffusion early stopping, and parallel scaling, to reduce PSC-induced ineffectiveness and inefficiencies.

* Equal Contribution
Corresponding Author

Date: Oct 11, 2025
& Contact: qgchen@irhit.edu.cn, car@irhit.edu.cn, lbqin@csu.edu.cn

1. Introduction

In recent years, diffusion large language models (DLLMs) have emerged as a novel generative paradigm,
attracting increasing research attention [14, 25]. Representative works such as LLaDA [17] and Dream [27]
adopt a two-stage mask-denoising training strategy combined with parallel decoding for masked token
prediction, effectively mitigating the “reversal curse” in traditional autoregressive large language models
(ALLMs). Mercury [12] and Fast-DLLM [23] further demonstrate the parallel efficiency of DLLMs, achieving
an impressive generation speed in code tasks.

Meanwhile, the rapid development of the Long Chain-of-Thought (Long CoT) [9, 3, 4] has spurred
increasing research on applying DLLMs to extended reasoning tasks [22, 21]. Zhao et al. [28] and Tang et al.


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

Parallel Decoding Sequential Reasoning

[MASK], [MASK]... [MASK] , [MASK] Stepl: First, let’s analysis ...
4
OES FES ALES gp crocessing) Step 2: Then, we should ...
Lt
[MASK] [MAS processing) Step 3: After that, we need ...

Clove (Gatural Tanguage processing) Step 4: Therefore, the

“I
ot
AT

afe Ineffective AR Strategy

Simple Parallel Sequential Prompting
Complex Sequential Constraint-Guided Prompting Y

Parallel-Encourage Prompting

ee

Figure 1: Overview of our work. Applying DLLMs to reasoning scenarios reveals an inherent contradiction
between parallel processing and sequential reasoning, leading to high entropy, superficial parallel decoding,
and limited Long CoT capabilities.

[19] employ diffusion-augmented SFT and GRPO to further improve reasoning [8]. Moreover, Trado [22]
exploits overlooked information in sampling trajectories, achieving substantial gains.

As shown in Figure 1 (a), DLLMs generate text in parallel, producing a few non-sequential words in a
single diffusion step. In sequential reasoning scenarios (Figure 1 (b)), the generation of step; requires the
completion of step;_;, leading to lower entropy [5, 1]. In contrast, Figure 1 (c) shows DLLMs to parallel-decode
by generating step;,; before step;, resulting in high entropy. Nevertheless, these parallel and sequential
processes are inherently contradictory: parallelism involves simultaneous processing, while sequential
reasoning requires ordered steps. To address this, we introduce the Parallel-Sequential Contradiction
(PSC), which explores the underlying mechanisms and practical implications of diffusion-based reasoning.

To investigate this issue systematically, as shown in Figure 1 (d, e), we focus on two central research
questions: (1) Do DLLMs truly perform parallel reasoning that avoids PSC? (2) What challenges do
DLLMs meet in Long CoT based on PSC? To address the first question, we analyze the decoding behavior
of DLLMs in both simple and complex reasoning scenarios. Our findings show that DLLMs fail to achieve
genuine parallel reasoning due to the PSC. They perform superficial parallel computation when outputs
can be directly produced, but revert to an autoregressive mode under higher reasoning demands. This
reliance on autoregression affects computational efficiency, which nearly doubles the computational cost with
low confidence remasking. Furthermore, while autoregressive prompting is effective in ALLMs, it conflicts
with DLLMs’ parallel decoding design, amplifying the PSC of DLLMs. In contrast, strategies that reduce
contradiction, such as conditional prompting or prompts that encourage parallel generation, effectively
enhance prompting performance.

To understand the second question, we examine the core capabilities of Long CoT in DLLMs. Our analysis
reveals that, when faced with PSC, DLLMs often demonstrate limited self-reflection, shallow reasoning depth,
and constrained exploratory behavior. Furthermore, we introduce three scaling dimensions for inference


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

time, specifically designed for DLLMs: parallel, diffusion, and sequential scaling. Our findings show that
both diffusion and sequential scaling are significantly constrained by PSC, while the parallel scaling law
remains unaffected due to its vertical relationship with PSC.

In summary, our key contributions are as follows:

¢ Identification of Parallel-Sequential Contradictions: To our knowledge, we first identify the Parallel-
Sequential Contradiction (PSC) in DLLMs for Long CoT. We demonstrate that PSC leads to superficial
parallel reasoning and reduced efficiency, requiring twice the decoding steps.

¢ Systematic Exploration of DLLM Reasoning Limitation: We conduct a systematic evaluation of DLLM
reasoning, identifying the degradation of three core Long CoT capabilities, confirming the ineffectiveness
of traditional autoregressive prompting methods, and demonstrating that diffusion scaling and sequential
scaling are upper-bounded by PSC limitations.

* Novel Mitigation Strategies: We propose novel strategies to mitigate these issues and enhance DLLM
reasoning. Our methods include parallel-encouraging prompting, diffusion early stopping, and parallel
scaling, which substantially alleviate the constraints imposed by PSC.

2. Parallel-Sequential Contradiction

2.1. Parallel Masked Diffusion Language Models

In Diffusion Large Language Models (DLLMs), inference reconstructs missing spans by predicting masked
tokens conditioned on a partially masked input. Its goal is modelling the conditional likelihood pg (x4,|x;) for

masked positions:
L

L , :
i=1
where L denotes the total number of tokens; / is the number of masked tokens, uniformly sampled from
{1,2,...,L}; xo is the complete original sequence. x; is the partially masked sequence obtained by replacing
those / positions in xo with mask tokens, which serves as the conditional input. The indicator 1[x}! € M]
equals 1 if position 7 is masked and 0 otherwise.

2.2. Sequential Long Chain-of-Thought Reasoning

Long Chain-of-Thought (Long CoT) allows LLMs to tackle complex problems by generating a sequence
of reasoning steps. This method solves a problem P by following an ordered series of steps S1,52,...,Sy,
leading to the final answer A. Formally, it can be defined as:

n+1

pe(A|P) = [| po(SilP, S<t). (2)

t=1

Here, S,1; = A, meaning the final answer is treated as the last step of the reasoning sequence. When
generating each step S;, the model computes the conditional probability based on the problem P and all
previously generated steps S <;.

2.3. Parallel-Sequential Contradiction

For tasks with high parallelism, downstream states typically yield predictable, high-probability outcomes,
resulting in low predictive entropy. In these cases, optimizing the conditional probability pg(S, | S1) is
efficient, making non-autoregressive or semi-parallel generation methods advantageous. In contrast, tasks


sy,

LANGUAGE ANALYSIS
REASONING GROUP.

>) Decoding Order

[ To solve the equation $x/2 - 2x + 1 = 0$, we start by rl (wy solve the given functional equation:

SO) +0) =f&+ “1
for all real numbe:
need to find all in
First, is anal}

examining the quadratic expression:

HB <15% Step
Hl 15-30% Step
HB 30-60% Step
[i 60-90% Step
|] >90% Step

nd y, and given that f(1) = 1, we
n such that f( n)=n.
the (patna Gop ease by substituting]

First, (We j check if it can be factored directly.) The
quadratic expression $x*2 - 2x + 1$ might BOW
perfect square trinomial. To verify this, we use the
method of completing the| square or checking the!
discriminant.
The diseriminant of a quadratic equation $ax’2 = 2bx
Pe Opie py $\Delta = b’2 - 4ac$. For our,

a B\edae 0-1

t+ 0)-
I

$$a = 1, \quad b = -2, \quad¢ = 135
The discriminant $\Delta = (-2)’2 - 4) = 4-4
O$. Since the discriminant is zero...

(ec) First answer generated
diffusion step in simple reasoning
tasks on d1.

onal equation:

=A if(2) +f) = (2 + 1)-2 \edot 1 -

(a) Decoding order in simple reasoning
tasks on Trado. fot fee
f+ D=fO)+x+2
Step 5: Substitute \(x = 1 \) J
fQ) + f@=fa + 1)-1\cdot1- 1

| HB <15% Step

\ | Hl 15-30% Step
| Hl 30-60% Step
| [i 60-90% Step
L >90% Step

To determine the number of bushels of wheat that
Charlotte|owns, we rt

functional equation:
4) + f(D) = Heeeee 4\cdot 1-1

Therefore, Charlotte (owns 4650 bushels of wheat. iu 1F1O

4 (f) First answer generated
(b) Decoding order in simple reasoning (c) Decoding order in complex reasoning (d) Decoding order in complex diffusion step in complex
tasks on dl. tasks on Trado. reasoning tasks on dl. reasoning tasks on dl.

Figure 2: Diffusion order analysis with d1 [28] and Trado [22], where later decoding orders are indicated
by shallower colors.

with strong sequential dependencies exhibit high entropy when predicting distant future states in parallel.
This uncertainty leads to significant predictive loss. To reduce this loss, the model is encouraged to break
down the generation process into a sequence of low-entropy, step-by-step predictions. As a result, parallel
generation conflicts with the model’s objective of identifying a low-loss, high-probability sequential path.
The formal proof is provided in Appendix A.

3. Do DLLMs truely perform parallel reasoning that avoids PSC?

3.1. Parallel-Sequential Contradictions cause superficial parallel reasoning.

To examine whether DLLMs can genuinely perform parallel reasoning, we analyze their decoding behavior
in both simple and complex scenarios. We have two key observations:

Parallel-Sequential Contradictions cause superficial parallel reasoning in simple scenarios. As shown
in Figure 2 (a, b), DLLMs demonstrate parallel reasoning in simple cases where the model can direct output
results without reasoning. For instance, when solving "x* + 2x +1 = 0", the model may simultaneously
generate the Quadratic Formula "A = b* — 4ac" and the final solution "x = —1" within a few diffusion steps.
Following this, DLLMs complete the remaining reasoning steps in parallel, demonstrating the ability to
leverage diffusion-based decoding to arrive at direct solutions without relying heavily on sequential reasoning.
To further explore this, we analyze the distribution of answers generated in the initial diffusion steps. As
shown in Figure 2 (€), over 47% of answers are produced within the first 30% of diffusion steps. This suggests
that in simple scenarios, DLLMs are capable of performing parallel reasoning, even though the underlying
thought process, such as applying the Quadratic Formula before deriving the result, is inherently sequential.

For complex reasoning, DLLMs converge toward autoregressive-like behavior to avoid PSC. To examine
DLLM behavior in complex reasoning tasks, we validate PSC where the model cannot directly output the
correct answer. Figure 2 (c, d) shows that DLLMs increasingly resemble autoregressive models. For example,
when addressing tasks beyond direct generation, the model defaults to an autoregressive process. This
suggests difficulty in sustaining parallel reasoning, which shifts to step-by-step processing. Figure 2 (f)


sy,
LANGUAGE ANALYSIS
a REASONING GROUP

@ LLaDA if LLaMA-3.1-8B @ Dream [fj Qwen2.5-Instruct — 100% Accuracy - -90% Accuracy — 100% Accuracy- -90% Accuracy
~ 50 Max Token Len / Diffusion ~ 30
50 ~ so. © ies als = Max Token Len / Diffusion-Step=cber-
40 = 40 & sy 40 Max Token Len / Di: sy 40 : — os y
3 z § 2
30 § 302 8 30 5 30
: Fg E
20 8 20 8
< 20 20
10 < w* 4 F
= w
0 = 0 + 10 Aa 10
100 100
300 300 BS or 0
250300 = 2
energ 150100 55 500 3 : 4 16 64 256 1024 4 16 64 256 1024
teq Token Le, 0 oe © Max Token Length Max Token Length
sth (b) Impact of the mismatch between diffusion steps and the multiple of max
(a) The impact of diffusion/decoding steps and generated token length. token length on model performance.

Figure 3: Diffusion speed analysis in Long-CoT-needed tasks with LLaDA-8B-Instruct [16] and Dream-7B-
Instruct [27] on BigGSM benchmark [3].

further confirms this observation: in complex tasks, answers emerge later in the diffusion steps, reflecting a
stronger reliance on ordered reasoning. These findings indicate that DLLMs face inherent PSC challenges in
balancing parallel generation with sequential reasoning, ultimately converging toward autoregressive-like
processing in complex scenarios.

3.2. Diffusion-Step Dilemma: Sacrificing Efficiency Under PSC

To investigate the reasoning efficiency of current DLLMs, we systematically categorize questions in BigGSM [3]
into different sampling lengths and diffusion steps (with low-confidence remasking). We evaluate two
representative DLLMs under exponentially increasing diffusion steps and max token lengths (ranging from 1
to 1024). See Appendix B for more details.

When complex reasoning, DLLMs require significantly more diffusion steps than ALLMs. As shown
in Figure 3 (a), achieving comparable length and accuracy to ALLMs demands over 25% more diffusion or
decoding steps with remasking. In extreme cases, DLLMs require up to twice the token length in diffusion
steps to match the performance and output length of autoregressive models when generating over 256 tokens.
It indicates that effective reasoning entails roughly double the diffusion steps relative to the answer length,
underscoring a notable efficiency challenge in reasoning tasks.

In reasoning scenarios, a large number of diffusion steps for autoregressive reasoning is unavoidable
for acceptable accuracy. Each generated token requires a sufficient number of diffusion iterations to allow
the model to reason effectively and produce high-quality outputs. As illustrated in Figure 3 (b), performance
sharply declines when diffusion steps fall below the target token length. For example, generating 80 tokens
with a maximum length of 128 but only 64 diffusion steps results in over a 10% accuracy drop; with 32 steps,
accuracy decreases by about 40%. This demonstrates that inadequate diffusion severely impairs reasoning, as
the model lacks enough refinement iterations. Thus, diffusion steps should at least match the planned token
length to maintain reasoning quality. Nonetheless, excessive diffusion can significantly reduce efficiency.

3.3. Rethinking the prompting strategies in DLLMs from PSC perspective

In general, traditional autoregressive inference methods are typically categorized into two types: pipeline-
guided approaches and condition-following approaches (see Appendix D for further details). In this section,
we will begin by reviewing the theoretical foundations and representative implementations of these two
categories. We will then examine their practical limitations and challenges. Furthermore, we introduce a
parallel-encouraging prompting to improve DLLM effectiveness.


o

Fo LA

LANGUAGE ANALYSIS
REASONING GROUP.

Model Name BigGSM (Acc.) GSMS8K (Acc.) Math-500 (Acc.) HumanEval (Pass@1) Average
Dream-7B-Instruct 41.15 (+0.00) 80.52 (+0.00) 37.00 (+0.00) 51.22 (+0.00) 52.47 (+0.00)
+ Zero-CoT 41.15 (—0.00) 77.26 (—3.26) 34.00 (—3.00) 48.78 (—2.44) 50.30 (—2.18)
+Plan-and-Solve 34.75 (—6.40) 78.85 (_1.67) 20.20 (_16.80) 48.78 (9.44) 45.65 (683)
+Least-to-Most 39.29 (—5.90) 77.03 (—3.49) 16.20 (—20.80) 43.29 (-7.93) 42.94 (—9.53)
+Complex-CoT 41.80 (40.65) 80.95 (40.43) 37.40 (40.40) 52.44 (41.22) 53.15 (+0.68)
+MARP 42.95 (+1.80) 80.52 (+0.00) 37.20 (+0.20) 51.22 (+0.00) 52.97 (+0.50)
+Diff-MARP 47.21 (+6.06) 82.64 (+2.21) 43.60 (46.60) 52.44 (41.22) 56.47 (+4.00)
LLaDA-8B-Instruct 48.03 (+0.00) 75.36 (+0.00) 34.80 (+0.00) 32.32 (+0.00) 47.63 (+0.00)
+ Zero-CoT 35.57 (—12.46) 73.46 (—1.90) 32.40 (—2.40) 28.05 (—4.27) 42.37 (—5.26)
+ Plan-and-Solve 31.64 (_1639) 72.33 (_3.03) 29.00 (_5.80) 27.44 (_4.8) 40.10 (_753)
+ Least-to-Most 34.75 (13.28) 73.31 (2.95) 30.80 (_4.00) 27.44 (_4.88) 41.58 (6.05)
+ MARP 48.20 (+0.17) 76.35 (+0.99) 34.40 (—0.40) 35.37 (—3.05) 48.58 (+0.95)
+Diff-MARP 55.74 (+7.71) 76.80 (41.44) 38.20 (+3.40) 38.41 (+6.09) 52.29 (+4.66)
+ Zero-CoT 36.39 (—5.41) TALSY/ (—3.11) 37.20 (—0.80) 35.98 (—0.61) 45.36 (—2.48)
+ Plan-and-Solve 30.16 (—11.54) 74.37 (—0.61) 34.40 (—3.60) 35.98 (—0.61) 43.73 (—4.12)
+ Least-to-Most 35.90 (_5.90) 73.69 (_1.29) 34.60 (_3.40) 31.71 (_488) 43.98 (_3.87)
ap Complex-CoT 50.16 (+8.41) 75.51 (+0.53) 39.40 (+1.40) 39.02 (42.43) 51.04 (43.19)
+ MARP 42.13 (+0.33) 74.37 (0.61) 38.20 (+0.20) 37.20 (+0.61) 47.98 (+0.13)
+Diff-MARP 54.49 (412.79) 76.50 (41.52) 42.80 (+4.80) 38.41 (+1.82) 53.08 (45.23)
LLaDOU-Math 42.13 (+0.00) 81.88 (+0.00) 45.80 (+0.00) 39.02 (+0.00) 52.21 (+0.00)
+ Zero-CoT 38.52(_3.61) 80. 95(_ 0.93) 45.80(_ 0.00) 37.80(_1 22) 90.77 (_1.44)
+Plan-and-Solve 40.82(_1 31) 81.12(_9,76) 43.20 (_2 60) 38.41(_9.61) 50.89 (_ 132)
ap Complex-CoT 43.77 (41.64) 83.70 (41 82) 45.80(+0.00) 42.07 (+3.05) 52.47 (40.26)
+ MARP 41.15(_0 98) 82.1849 30) 45.60 _0.20) 40.26 (41.24) 52.30(+0,09)
+Diff-MARP 54.26 (+12.13) 84.76 (+2.88) 49.00 (43.20) 40.85 (1.83) 57.22 (45.01)

Table 1: Performance comparison across 4 benchmarks. Bold marks the best baseline score per metric. For
each method, we report its most token-efficient variant. Here, “ ”: prompting strategies, “ ”: offline
strategies, “ ”: online strategies.

Sequential Reasoning Prompting will enlarge PSC’s negative impact for DLLMs. Sequential prompting
strategies, which facilitate sequential reasoning, have been shown to significantly improve the performance of
ALLMs on complex tasks. However, as indicated in purple rows of Table 1, we observed a notable decline in
performance as tasks required an increasing number of reasoning steps. We attribute this decline to the fact
that sequential reasoning prompts exacerbate the negative impact of PSC, thereby impairing the reasoning
performance in DLLMs.

Constraint-guided Reasoning Prompting enhances model performance by preventing the introduction
of additional PSC. By incorporating explicit constraints into the reasoning process, constraint-guided
prompting effectively narrows the model’s search space, thereby preventing the emergence of additional
PSC during the reasoning process in DLLMs. This focused approach results in more accurate and reliable
solutions. As shown blue rows of in Table 1, methods based on this principle, such as Complex-CoT [6]
and MARP [3], demonstrate superior reasoning capabilities in DLLMs compared to traditional sequential
prompting methods.


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

Parallel-encouraging Prompting reduces the sequential feature so that it further improves performance.
Parallel-encouraging prompting refers to the technique of presenting multiple related tasks or questions
simultaneously. This approach reduces the impact of PSC and minimizes the sequential features in the
prompting process. By encouraging the model to make connections across these tasks, as illustrated in
green rows of Table 1, it effectively fosters DLLMs’ performance, leading to more efficient reasoning and
information integration. Leveraging the parallel processing capabilities of DLLMs, this method has the
potential to significantly enhance performance, particularly in complex reasoning tasks, by promoting more
comprehensive and coherent solutions.

Takeaways

1. Due to PSC, DLLMs engage in superficial parallel reasoning and exhibit autoregressive behavior in

complex scenarios, which compromises their reasoning efficiency.
2. Sequential prompts prove ineffective for DLLMs, requiring PSC-free or -redcued approaches like
constraint-guided and parallel-encouraging prompts to guide their operation.

4. What challenges do DLLMs meet in Long CoT based on PSC?

Despite impressive empirical results, DLLMs’ genuine reasoning abilities and scalability under Parallel-
Sequential Contradictions remain open questions. We systematically evaluate Long CoT to assess these
fundamental capabilities and scaling strategies.

4.1. DLLMs do not have sufficient basic capabilities to support Long CoT.

Long CoT is the primary innovation in recent reasoning large language models, leveraging inference-time
scaling for self-exploration, self-reflection, and deep reasoning [4]. Evaluation details are in Appendix E.

Traditional reflection strategies are Ineffective for DLLMs. Long CoT models always employ a self-
reflection mechanism for iterative reasoning refinement. To assess its efficacy, we examine two LLM paradigms:
(1) Prompting Reflection and (2) Autoregressive Forcing Reflection As shown in Figure 4 (a, b), reflection
paradigms yield no significant differences from vanilla reasoning chains in semantic similarity, informative-
ness, or token-level entropy. Though the reflection process increases entropy and reduces informativeness, it
maintains over 0.95 semantic similarity to original reasoning chains. These findings suggest the reflection
mechanism offers only limited surface-level optimization. Figure 4 (c) further reveals a substantial token
repetition ratio compared to the original path, resulting in approximately 10% reflection-to-error responses.

Limited Efficacy of traditional exploration strategies for novel reasoning path generation. Exploration,
a fundamental competency for complex reasoning, involves a model’s ability to generate diverse and innovative
solutions. To assess this potential in DLLMs, we designed experiments utilizing two strategies: (1) Prompting
Exploration and (2) Autoregressive Forcing Exploration. Figure 5 (a, b) reveal that current exploration
strategies offer several improvements in the novel semantic of generated reasoning processes. However,
these improvements remain superficial, evidenced by a high similarity (> 0.84) between explored paths and
original results. Furthermore, as depicted in Figure 5 (c), while the new path and explore-to-correct ratios
are limited (~ 5%), they nonetheless indicate a positive, albeit constrained, effect.

DLLMs possess limited reasoning boundaries and, consequently, exhibit restricted deep reasoning
abilities. To examine the limitations of DLLMs on deep reasoning, we evaluate their capacity to sustain
reasoning across sufficient depths. Figure 6 (a) demonstrates that error steps are all less than 2, which
suggests that current DLLMs are unable to consistently sustain deep reasoning performance. Furthermore,


Be

LANGUAGE ANALYSIS
o REASONING GROUP.

O Direct Baseline A Prompting Reflection (PR) od Autoregressive Forcing Reflection (AFR) LD Repetition Rate (vs. baseline)
Reflect-to-Error Rate (vs. baseline)

Sim: 0.9702

Sim: 0.9715

Sim: 0.9769
257 Za S54
2 Sim:0°9577, > 2
£55 . 253
= 5
a &
5 5.3 ge 5.2
“ BY
io] io)
B 51 Diffusion Step=-128 5.1

Diffusion-Step= 256

(A) g. 0 . T T T T T T T T
go Pee, 15 7 87.3 750) 0 10 20 30 40 50 60 70 80 90
2 2 y gh F
mae, 84 sss™ ay” 87.1 io Proportion Rate

(a) The quality analysis of reflection path on (b) The quality analysis of reflection path on (c) The manual quality analysis of exploration
Dream-Instruct-7B on ROSCOE. LLaDA-Instruct-8B on ROSCOE. path on two DLLMs.

Figure 4: Self-reflection performance and rationale quality evaluation on DLLMs.

O Direct Baseline /\ Prompting Exploration (PE) *& Autoregressive Forcing Exploration (AFE) LD New Path Rate (vs. to baseline)
90.5 92.0 —- (J Explore-to-Correct Rate (vs. baseline)
S 90.4 ~ Di fusion Step
2 S 91.5 ceo? Sea
: 5 91.0 Si 913 EB
c im: 0.9132 N
5 702 5 SONS Kose Semone A Lada
B= Of “<A oe }
90.1 = 90.5 a ang +AFE
OD a > <<
4g 90.0 2 90.0 * Dream
G 89.9 § Y +PE
% 89.8 & 89.5 a A Dream
wae +AFE
89.7 89.0
87.8 88.0 88.2 88.4 88.6 87.6 87.7 87.8 87.9 0 3 10 15 20 25 30
Informativeness (T) Informativeness (1) Proportion Rate
(a) The quality analysis of exploration (b) The quality analysis of exploration (c) The manual quality analysis of reflection
path on Dream-Instruct-7B on ROSCOE. path on LLaDA-Instruct-8B on ROSCOE. path on two DLLMs.

Figure 5: Self-exploration performance and rationale quality evaluation on DLLMs.

following Chen et al. [3], we define the 90% correctness step count as the models’ completely feasible
reasoning boundary (CFRB), and the 10% correctness step count as the completely infeasible reasoning

boundary (CIRB). As shown in Figure 6 (b), current DLLMs display similar CFRB values but lower CIRB
values, indicating narrower feasible reasoning ranges.

4.2. Current DLLMs have three-directional but limited Inference-Time Scaling

Given their denoising characteristics, we investigate a fundamental question: Is there also Inference-
Time Scaling Law in DLLM under such contradictions? We examine this through three complementary
perspectives: Parallel Scaling, Diffusion Scaling, and Sequential Scaling. These experiments determine
whether DLLMs follow inference-time scaling laws and provide practical insights for optimizing reasoning
performance. Implementation details can be seen in Appendix F.

4.2.1. Parallel Scaling Law holds Despite PSC

For DLLMs, a key question is whether their unique diffusion generation mechanism supports efficient parallel
sampling and whether parallel sampling can effectively enhance reasoning performance.


Be

LANGUAGE ANALYSIS
LJ REASONING GROUP

i Qwen-7B-Instruct @ Correct sample @ Incorrect sample —- CFRB== PFRB

[J Dream-7B-Instruct

ga [i 8 e
i LLaMA3-8B-Instruct |. ‘ 5 ¢
LLaDA-8B-Instruct |.8 & ‘ °3 8
=. 5 é ef é
3's € ge g
Ee é 3
s =ane ‘ °
s zZ5 ; is88 3
B ae 4 d :
[aly oa x es! A
” : & es = ©
0 a 1 4 7 10 13 16 1 4 7 10 13 16 1 4 7 10 13 16 1 4 7 10 13 16
Large Language Model The number of The number of The number of The number of
(a) The first incorrect step planning step (B(p)) planning step (B(p)) planning step (B(p)) planning step (B(p))
position across 4 models on (b) The reasoning boundaries (c) The reasoning boundaries (d) The reasoning boundaries (e) The reasoning boundaries
BigGSM. of LLaMA on BigGSM. of LLaDA on BigGSM. of on Qwen BigGSM. of Dream on BigGSM.

Figure 6: Incorrect Step and Reasoning Boundaries Distribution of DLLMs on BigGSM.

( Output (__ Diffusion Sealing 70 80
overall } overall ] -@ Temperature=1.0 -@- Temperature=0.7
uvuan n\'ne the vuvuau \'ne the 68 +| » Temperature=0.5 —9- Temperature=0.3
Tis Tene 4} © Temperature=0.1
[\LASK] [\tASK] 66
» Parallel 64
Scaling gS S
u } j 4 62 4
problem into s}aller steps... problem into s} aller steps... Z 60 4 B
™ 58 4 a
Sequential Scaling 56 J
54 4
DLLM 52 4
50

35
1 2 3 4 5 6 7 8 9 10 1 4 16
Parallel Scaling Size Parallel Scaling Size (log scale)
Input

(a) Parallel Scaling Performance on LLaDA (b) Inference-time Scaling Law on
with different temperature. Parallel Scaling Performance.

Figure 7: Three primary scaling directions for

DLLMs proposed in our work. Figure 8: Performance analysis under Parallel Scaling.

Higher temperatures do not always yield more diverse and effective parallel sampling. The decoding
temperature controls generation randomness, with higher values typically increasing output diversity in
ALLM reasoning. We adjust the temperature during generation (0.1 to 1.0) to evaluate its impact on
parallel sampling. Model accuracy improves steadily with increasing Pass@k values across all temperature
settings before plateauing. As shown in Figure 8 (a), moderate temperatures (e.g., T = 0.5) achieve optimal
performance, while both lower and higher temperatures yield diminished performance gains. This indicates
that moderate temperature settings provide the optimal balance between generation diversity and output
reliability.

DLLM reasoning accuracy improves with increased parallel samples, following inference-time scaling
patterns. As shown in Figure 8 (b), when k increases from 1 to 32, accuracy demonstrates nearly linear
improvement on a logarithmic scale. This indicates that DLLMs effectively utilize parallel sampling to
enhance reasoning performance, as diverse outputs increase the probability of generating correct solutions.
This pattern aligns with inference-time scaling laws observed in other advanced language models, where
performance scales with computational effort during inference.


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

-~ 60 = 60
Sys | | -e- tap —— & pas
iy -@-Dream = 40 a_i
gs 30 g el
=! 50 >» 2 20 Ja m@- LLaDA
15 ee ee 9 _—y ©-Dream
< < A
0 6<-0-6 : : 1 1 40 OS : . .
1 4 16 64 256 10244096 1 8 64 512 4096
Diffusion Steps sS Max Token Length
gn ; > 30 ’ 3
(a) Diffusion scaling performance. Fs (d) Sequential Scaling Performance.
5
~#- Diffusion Scaling -A- +Early Stop 3 20 An a 300 §
=50 SS S
= as = 40 A
45 | a 10 4096 & B 200 5
£40 256 & & 35
5 §& 5 20 ~® Length 100 5H
3 35 4 0 6 oS < 4 Acc. 5
30 ——— 40961024 256 64 16 4 4 ¥ 0 : 9 oO
32 «64 affects Sen 1024 2048 Max Token Length C) 1 8 ae 512 4096
iffusion Steps iffusi
P (c) Diffusion and Sequential Scaling Performance on Ditiusion Step
(b) Diffusion scaling vs. early stopon Dream. LLaDA-Instruct-8B. (e) Diffusion scaling with natural sequential scaling.

Figure 9: Diffusion Scaling analysis of reasoning accuracy across difficulty levels on BigGSM.

4.2.2. Diffusion Scaling Law is Broken and Constrained by PSC

Diffusion Scaling of DLLMs ensures performance gains, with diffusion time positively correlated.
Model accuracy increases monotonically with the number of diffusion steps. We are the first to formalize
Diffusion Scaling in DLLMs, proposing a positive correlation between model performance and diffusion
iterations. To validate this claim, we benchmark two representative DLLMs, DREAM [27] and LLaDA [16],
under an exponential schedule of diffusion steps (1-1024). By tracking accuracy at each step, we observe
how DLLMs address reasoning tasks of varying complexity across the diffusion process. As shown in Figure 9
(a), performance consistently improves with deeper diffusion; however, the rate of improvement depends
on task difficulty: simpler problems gain substantially, while tasks beyond the model’s capacity yield only
limited benefits.

Diffusion Scaling is effective and exhibits an upper bound, beyond which an over-diffusion phenomenon
emerges due to PSC. Consistent with classical scaling laws, the benefits of diffusion scaling are inherently
capped. As shown in Figure 9 (a), increasing diffusion steps improves performance from 32 to 512 steps, after
which gains plateau. More importantly, excessive diffusion reduces accuracy: Figure 9 (b) shows a drop from
44.92% to 44.43%. This decline illustrates over-diffusion, where extended denoising introduces excessive
corrections that disrupt reasoning chains, akin to overfitting caused by training without early stopping.

Early stopping can effectively mitigate over-diffusion. To address over-diffusion, we propose a Diffusion
Early Stopping (DES) strategy that halts the process when generated tokens stabilize. The implementation
comprises three components: (1) Overlap Ratio Calculation: computed as the proportion of identical tokens
between consecutive steps. (2) Convergence Detection: potential convergence occurs when the overlap
ratio meets or exceeds a predefined threshold. (3) Activation Condition: early stopping triggers only after
three consecutive steps satisfy the threshold, preventing false positives from transient fluctuations. As shown
in Figure 9 (b), we observe that beyond 256 steps, early stopping outperforms standard diffusion, with
accuracy improving from 44.26% to 46.89% at 1024 steps. Early stopping captures convergence states and
terminates upon stabilization, while preventing performance degradation from excessive diffusion.

4.2.3. Sequential Scaling Law is also Broken and Constrained by PSC

The inherent limitations of sequential scaling for DLLMs. While sequential scaling has shown promise
in enhancing reasoning capabilities, it remains constrained by the inherent characteristics of DLLMs. As

10


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

shown in Figure 9 (d), the performance improvements are at first increasing but eventually converge. This
limitation arises from the fact that sequential scaling relies on the model’s ability to maintain context over
extended reasoning chains, a challenge for current DLLMs.

Sequential Scaling also meets over-thinking challenges. Similar to diffusion scaling, as shown in Figure 9
(d), sequential scaling faces its own set of over-thinking challenges. As the model attempts to extend its
reasoning across longer contexts, it may encounter diminishing returns or even performance degradation.
This phenomenon is particularly evident in tasks that require intricate reasoning over extended text, where
the model’s ability to track and integrate information can become strained.

Diffusion Scaling can naturally yield Sequential Scaling benefits. As shown in Figure 9 (e), diffusion
scaling alleviates the limitations of sequential scaling. We identify three stages in DLLMs during diffusion: (1)
sequential scaling, (2) compression, and (3) convergence. In the first stage, increasing diffusion steps leads
to stable performance but longer solutions, indicating that DLLMs explore suitable lengths for reasoning. In
the second stage, the model compresses its reasoning by eliminating redundancy, generating more efficient
solutions. In the third stage, the model converges on an optimal strategy, achieving high performance while
reducing computational cost.

Takeaways

. DLLMs are deficient in three basic Long CoT capabilities, hindering their effectiveness.
. DLLM can be optimized via parallel, diffusion, and sequential scaling. Diffusion scaling inherently

encompasses the benefits of sequential scaling.

. The performance of both diffusion and sequential scaling is ultimately upper-bounded by a parallel-
sequential contradiction. But Parallel scaling law remains the most effective strategy, although it is
also the most computationally expensive.

5. Related work

The application of diffusion models to text generation has emerged as an alternative to autoregressive methods.
Early work by D3PM [2] proposed discrete denoising diffusion probabilistic models, and Diffusion-BERT [10]
demonstrated scalability to BERT-style architectures. SEDD [15] achieved performance comparable to
GPT-2. Recent progress has broadened the scope of Diffusion Large Language Models (DLLMs) [25, 23, 8].
LLaDA [16] and Dream [27] scaled to billion-parameter models with notable inference gains. The D2F
strategy [21] further enhanced inference by enabling block-level autoregression and parallel decoding,
maintaining a balance between speed and accuracy. This direction aligns with the growing interest in
applying DLLMs to extended reasoning [22, 28]. Diffusion-of-Thought (DoT) [26] combines diffusion
with chain-of-thought reasoning. Building on this, Zhao et al. [28] and Tang et al. [19] applied diffusion-
augmented SFT and GRPO to strengthen reasoning. Similarly, Trado [22] exploits overlooked sampling
signals, yielding further reasoning gains.

However, while DLLMs exhibit notable parallel decoding in text generation and consistently strong step-
by-step reasoning, these features appear conceptually opposed: parallelism implies simultaneous processing,
whereas sequential reasoning demands ordered progression. This apparent Parallel-Sequential Contradiction
(PSC) suggests that both the underlying mechanisms and the practical effectiveness of DLLMs’ diffusion-based
reasoning remain insufficiently understood.

11


Fo LA

o

LANGUAGE ANALYSIS
REASONING GROUP.

6. Conclusion

In this work, we formalize the Parallel-Sequential Contradiction (PSC) to explain why DLLMs, though built
for parallel decoding, revert to autoregression as reasoning difficulty rises. Empirically, DLLMs exploit
parallelism only when tokens are locally decidable; otherwise, they fall back to sequential computation,
reducing efficiency. Further, we first define three-dimensional scaling: parallel, diffusion, and sequential
scaling, and show that PSC restricts the latter two while parallel scaling holds. We mitigate PSC through
parallel-focused prompting, diffusion early stopping, and parallel scaling, improving both accuracy and
throughput. Future work should align training and architectures with PSC-aware reasoning and design
benchmarks, isolating its effects.

References

[1]

[2]

[3

L4

—

[5]

[6]

L7

—

[8

ee

[9]

[10]

Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness
of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025.

Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured
denoising diffusion models in discrete state-spaces. Advances in neural information processing systems,
34:17981-17993, 2021.

Qiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking the capabilities of
thought: A reasoning boundary framework to quantify and optimize chain-of-thought. Advances in
Neural Information Processing Systems, 37:54872-54904, 2024.

Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang
Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of-thought for
reasoning large language models. arXiv preprint arXiv:2503.09567, 2025.

Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen
Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning
language models. arXiv preprint arXiv:2505.22617, 2025.

Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for
multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.

Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,
and Asli Celikyilmaz. ROSCOE: A suite of metrics for scoring step-by-step reasoning. 2022.

Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe
Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv
preprint arXiv:2506.20639, 2025.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong
Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement
learning. arXiv preprint arXiv:2501.12948, 2025.

Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert:
Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029,
2022.

12


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

[11] Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion
chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025.

[12] Inception Labs. Mercury: A diffusion large language model. Technical report, Inception Labs, 2025.
URL https://www.inception-labs.ai/mercury. Commercial-grade diffusion LLM for code
generation. Achieves over 1000 tokens/second on NVIDIA H100.

[13] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. In Advances in Neural Information Processing Systems,
volume 35, pages 22199-22213, 2022.

[14] Tianyi Li, Mingda Chen, Bowei Guo, and Zhigiang Shen. A survey on diffusion language models. arXiv
preprint arXiv:2508.10875, 2025.

[15] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of
the data distribution. arXiv preprint arXiv:2310.16834, 2023.

[16] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong
Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025.

[17] Yatao Nie, Jie Chen, Yufan Zhang, et al. Large language diffusion with masking. arXiv preprint, 2025.
URL https://arxiv.org/abs/2502.09992.

[18] Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting:
Improving zero-shot chain-of-thought reasoning across languages. arXiv preprint arXiv:2310.14799,
2023.

[19] Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. wd1: Weighted policy optimization
for reasoning in diffusion language models. arXiv preprint arXiv:2507.08838, 2025.

[20] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-
and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv
preprint arXiv:2305.04091, 2023.

[21] Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion Ilms can do
faster-than-ar inference via discrete diffusion forcing, aug 2025. URL https: //arxiv.org/abs/
2508.09192. arXiv:2508.09192.

[22] Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement
learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025.

[23] Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han,
and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel
decoding. arXiv preprint arXiv:2505.22618, 2025.

[24] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance
general chinese embedding, 2023.

[25] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada:
Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025.

13


Fo LA

o

[26]

[27]

[28]

[29]

[30]

LANGUAGE ANALYSIS
REASONING GROUP.

Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang,
Zhenguo Li, Wei Bi, et al. Diffusion of thought: Chain-of-thought reasoning in diffusion language
models. Advances in Neural Information Processing Systems, 37:105345-105374, 2024.

Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025.

Siyan Zhao, Devaansh Gupta, Qinging Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion
large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025.

Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in
large language models. arXiv preprint arXiv:2205.10625, 2022.

Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen,
Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language
diffusion models. arXiv preprint arXiv:2505.19223, 2025.

14


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

Appendix

A. Mathematical Proof of DLLM Degrading to Autoregressive

Goal. We rigorously show, via information theory and optimization, that the intrinsic statistical property
of a generative task, namely its sensitivity to perturbations of initial conditions, fundamentally determines
its optimal (lowest-loss) generation strategy. Concretely, we axiomatize two classes of tasks: serial tasks
(step-by-step reasoning) exhibiting cascading sensitivity to initial conditions, and parallel tasks exhibiting
partial invariance, and we prove that serial tasks induce significantly higher conditional entropy for “skip-
step” parallel predictions S, | S; than parallel tasks, forcing any loss-minimizing learner to converge to an
autoregressive strategy.

A.1. Problem Setup and Notation

Let all step states S; take values in a metric space (Od). Specifically, the true data distribution p? of a
task is considered serial if, for any given s; € O, the mapping from s/, to a subsequent state s;, is highly
divergent within a sufficiently small neighborhood N(s1,€) = {s}|d(s1,s{) < e}. Conversely, a task’s true
data distribution p} is considered parallel if, for any given s; € , there exists at least one subsequent state
S, (where k>1) that is insensitive to perturbations within its neighborhood N(sj,¢€). Formally, this leads to
the following definitions for the two tasks.

Definition 1 (Serial tasks: cascading sensitivity). A data-generating distribution p? is serial if for any s1 € O
and any k > 1, that satisfies:

: S

lim Es u(N(s12)) [pa(Sk = 8% | $1 = 54)] = 0, (3)
where s; is the reference outcome drawn from the true conditional p? (Sx | $1 = s1), and U(N(s1,€)) the uniform

distribution over this neighborhood. Equivalently, arbitrarily small perturbations of S; almost surely drive future
states away from the reference trajectory at step k, capturing sensitive dependence on initial conditions.

Definition 2 (Parallel tasks: partial invariance). A data-generating distribution p}) is parallel if there exists
some k > 1 and a constant C € (0,1] such that for any s; € Q, that satisfies:
lim Ey. .u(n(s.e)) [Po (Sk = sk | $1 =81)] = C € 0,1], (4)

where s, is the reference outcome drawn from p}(S, | S1 = 1). Thus, a structurally stable downstream state
persists with significant probability despite infinitesimal perturbations of the initial condition.

A.2. Learning problem

Let po be a parametric generative model trained by minimizing cross-entropy with respect to the true data
distribution #, i.e.,

L(pe, P) = Ex~p[— log po(x)] = H(p) + Dex (lp), (5)
so minimizing cross-entropy is equivalent to minimizing Dx (f||p¢) and to maximum likelihood. For any
conditional subproblem, the optimum satisfies pj (5S; | S1) = p(S, | S1), and the minimal expected negative
log-likelihood equals the conditional entropy,

4 . _ p(xy)
L* = B,~p(s,) |H((Sx | $1 = 51) | where H(Y | X) = oe OY 108 SG) (6)
15


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

Autoregression and chain rule. A step-by-step reaosning strategy factorizes a joint distribution as a
product of conditionals via the chain rule, thereby replacing a high-entropy “skip” conditional pg(S; | $1)
by a sequence of typically lower-entropy one-step conditionals pg(S;41 | S;,...). It is the standard rationale
behind likelihood-based training of ALLMs under teacher forcing.

A.3. Discretization

To compare entropies on a general metric space, consider a finite measurable partition II, of O with mesh
size at most e, and define the discretization and quantization map ¢, : OQ — [m,]| that assigns each s € O to

(¢)

its cell index, where m,; = |ITe|. Let S (©) = ¢-(S;) and write p,’(-) for the induced discrete laws; we analyze

tm (S() | ge) = 5,), which is well-defined, and relate back to the original problem by taking e — 0. Two
standard facts underpin the analysis: (i) for a fixed finite support, entropy is maximized by the uniform
distribution; (ii) the Shannon entropy is bounded below by the min-entropy — log pmax, and admits tighter
lower bounds in terms of the binary entropy function H; and the support size.

Lemma 1 (Pointwise probability caps). Fix k > 1 and s, € O. Under Definition 1, for any 6 > 0 there exists
€9 > O such that for all € < €o,

max pose” =se| $1 = gels) < 4, (7)
$y ~ Sq

where s, ~ N'(s1) = e(s1) # €(s}) Ae(s}) € me.

Under Definition 2, there exist C > 0 and €9 > O such that for all € < €o,

vmax (5°) = sq | S{” = @-(s1)) > C. (8)
s,~N"(s1)

Proof sketch. By Definition 1, for serial tasks, the conditional probability of a reference outcome, averaged
over shrinking neighborhoods of s}, vanishes. This forces any mass that could be concentrated on a particular
cell containing s;, to diminish as the mesh refines. In contrast, for parallel tasks, Definition 2 guarantees
a persistent mass C € (0,1] associated with a stable outcome across neighborhoods, which uniformly
lower-bounds the maximum conditional atom in e.

A.4. Main proposition and quantitative bounds

Proposition 1 (Skip-step parallel predictions on serial vs. parallel tasks). For any k > 1, the optimal expected
skip-prediction loss on serial-task data strictly exceeds that on parallel-task data:

L§(po(Sk | $1 =s1)) > Lp(po(Sk | $1 =s4)), (9)

equivalently,
Es u(n(s,e)) Hs (Sk | $1 = 81)] > Ey ucv(s2))[Hp (Sk | $1 = 51) ], (10)

In the discrete case, this reduces to showing

Ey ris) Hs (S” | $4? = ge(s1))] > Benno lH (S” | $1? = ge(s1))] (11)

with a strictly positive gap that can be quantified through discretization and classical entropy bounds.

16


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

Proof. It suffices to compare the conditional entropies pointwise and then take expectations. Fix s; anda
partition II,. We first define the maximum of generation probability of serial tasks:

pS. (€38)) = max Pose” = 5, | § = g.(s!)) (12)
sy Sy

and analogously pt... (e;s,) under ps).

By the lemma, p?,,,(€; 8) — 0.as ¢ — 0, while for parallel tasks one has p/,,,(€;s,) > C for all sufficiently
small ¢. For any discrete distribution over m, points with maximal atom pmax, Fano’s inequality implies

A< Ey (pmax) + (1 _ Peecixs) log (me _ 1), (13)
where H, is the binary entropy.
Thus, for parallel tasks,
Hp(S,? | St? = ge(s1)) < Ho(Pmax(€51)) + (1 — Pmnax(€i81)) log(tme — 1), (14)

and since p!.,,(€;s{) > C > 0, the entropy is uniformly bounded away from the maximal value log(m,) by a
constant determined by C.

For serial tasks, since p?,,,(€;s,) > 0, we have error probability p. + 0. Now, we should apply the
contrapositive of Fano’s inequality. Specifically, given the Fano’s inequality:

it follows that H + 0 => pe — 0. Conversely, pe > 1 implies H — Hingy. In this sense, the condition is
satisfied:
Hp(S, | $\° = ge(st)) + log (me — 1),if PSvax (E54) + 0, (16)

which reflects the extreme dispersion dictated by sensitivity. Therefore, it satisfies:

Ey .ni(s)/Hs(S.? | ge) = f(s1))] — Ey wn'(s)[H(S,” ge) = e(s1))]| — strictly positive. (17)

QED.

A.5. Consequences for optimal strategy

Because the minimum achievable expected NLL equals the conditional entropy, the high conditional entropy
of skip-step predictions in serial tasks implies a high irreducible loss for any pg(S, | Si) objective. A
loss-minimizing learner therefore prefers factorizing the prediction into a chain of low-entropy one-step
conditionals, i.e., an autoregressive strategy, which aligns with the chain rule factorization and standard
maximum-likelihood training. By contrast, in parallel tasks, the existence of a stable high-probability outcome
for some downstream state S, produces a low-entropy, high-confidence conditional, so optimizing pg(S; | $1)
can be preferable and can support non-autoregressive or partially parallel generation plans.

Takeaway. Task-intrinsic sensitivity versus invariance dictates the shape of the optimal conditional distri-
butions; via the cross-entropy/KL equivalence, this in turn selects the generation procedure that globally
minimizes expected loss, with serial tasks forcing autoregression and parallel tasks permitting advantageous
skip-step or parallel predictions.

17


sy,

LANGUAGE ANALYSIS
REASONING GROUP.

from typing import List To determine the total distance
James runs in a week, we need to
def has_close_elements( break down the problem into
numbe [float], smaller steps.
threshole a i Calculate the  distancece
: James runs in one session.
if in given list of numbers, are any two : Calculate the distance James
er to each other than given threshold. runs in one w

To find the distance from the origin, we
to calculate the net ea.
ce and the net westward dista

from typing import List

Calculate the distance James

463 times the t had yesterday
Let's calculate the net eastward distance:
seastward 723 - westward 279 =
max_num = numbers/[0] eastward 444 kilometers
for num in numbers: eeastward 444 + westward 509 =
ifnum > max_num: eastward 953 kilometers
max_num = num seastward 953 + westward 463 * the
return max_num distance it yesterday = eastward 953 +
westward 463 * the distance it

runs in a week.

i range(len(numbers)):
forj in range(i+1, len(numbers)):
if abs(numbers/i] - numbers|j])< threshold:
return True
return False

(a) Decoding order in no-reasoning (b) Decoding order in (c) Decoding order in no- (d) Decoding order in
tasks of LLaDA. reasoning tasks of LLaDA. __ reasoning tasks of Dream. reasoning tasks of LLaDA.

Figure 10: Decoding order of Dream and LLaDA on BigGSM [3].

B. Diffusion-Step Evaluation Details

In this section, we provide additional technical details on the methodology used to evaluate the impact of
diffusion steps and sampling lengths in Diffusion-based Language Models (DLLMs). We specifically focus on
how these parameters influence the efficiency and accuracy of the models when tackling complex reasoning
tasks.

For our analysis, we utilize the BigGSM dataset [3], which includes a diverse range of complex reasoning
tasks designed to test current models’ ability to perform long-form reasoning. In particular, we assess the
performance of two representative DLLMs on these tasks and compare them against a standard ALLM. We
systematically vary both the number of diffusion steps and the sampling lengths to evaluate their combined
effects on the reasoning efficiency of DLLMs. The number of diffusion steps tested ranges from 1 to 1024,
while the maximum token lengths vary from 1 to 1024 tokens, with low-confidence remasking. In each
experiment, the number of diffusion steps is set equal to the maximum token length. This range allows us
to assess the model’s performance under different levels of token generation and diffusion refinement. For
ALLMs, we adjust the maximum token length between 1 and 1024 and the temperature between 0.2 and
0.7, aiming to achieve comparable performance to that of the DLLMs.

For each setting, we track the following metrics:

¢ Accuracy: The percentage of correct answers generated by the model.
* Model Output Length: The number of tokens generated by the model before reaching the stopping
token (calculated using the GPT-40 tokenizer).

When the maximum token length is less than or equal to 512, the model output length typically constitutes
50% to 80% of the maximum token length. Specifically, when generating a maximum token length of
512, achieving optimal performance requires 512 diffusion steps combined with low-confidence remasking
strategies. We utilize this remasking approach to ensure the best performance.

Our evaluation demonstrates that the efficiency of DLLMs in reasoning tasks is strongly influenced by the
balance between diffusion steps and sampling lengths. While a higher number of diffusion steps generally
improves reasoning accuracy, it increases computational requirements. Thus, while sufficient diffusion steps
are essential for effective reasoning, an excessive number can significantly reduce processing efficiency.

18


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

C. Early-Stop Strategy

The early stopping mechanism is based on the dynamic stability of tokens, which monitors the variation
of the newly updated tokens during the diffusion process to judge whether the generation has converged.
We calculate the overlap ratio between the current step’s selected tokens (current_tokens) and the previous
step’s tokens (prev_tokens) in each diffusion step. When the overlap ratio of the selected tokens remains
stable over three consecutive steps and exceeds a threshold 6 = 0.99, early stopping is triggered. The overlap
ratio is calculated as:

N
1
overlap ratio = N S"I(current_tokens; = prev_tokens;), (18)
j=l

where N is the number of tokens updated in the current step, and I is the indicator function. This mechanism
is controlled by the parameter early stop threshold = 0.99, which controls the sensitivity.The higher the
threshold, the more stable the token sequence needs to be before triggering early stopping.

The parameter settings use a block-based diffusion strategy: the total generation length of 512 tokens
is divided into blocks of length block_length = 32. Temperature = 0.7 helps balance exploration and ex-
ploitation. We choose low_ confidence strategy, which updates tokens with low confidence. This combination
ensures the quality of the generated text while improving efficiency by using fewer diffusion steps, which
typically converge to a value smaller than the maximum 512 steps.

D. Prompting Experiment Details

D.1. Experimental Setup

In this study, we employ the following models: Dream-7B-Instruct [27], LLaDA-8B-Instruct [17], LLaDA-
v1.5 [30], and LLaDOU-Math [11]. To optimize performance, we experiment with a temperature range of
[0, 1], choose top-p=0.95 and block-length=32, and select the maximum token length from the set {128,
256, 512}, as well as the diffusion step from {128, 256, 512}. For each model, we use the default decoding
settings. Additionally, we apply low-confidence remasking to explore the scaling behavior. All experiments
conduct on a single A100 or A800 80G GPU.

D.2. Sequential Reasoning Prompting

These methods were originally designed primarily for Autoregressive Large Language Models (ALLMs) and
have played a key role in optimizing their reasoning capabilities:

¢ Least-to-Most [29]: In autoregressive models, this method systematically breaks down complex problems
into multiple simpler sub-problems, guiding the model to reason step by step rather than attempting to
solve the entire complex problem at once. This effectively reduces the complexity of single-step generation
and enhances the model’s ability to handle complex reasoning tasks.

Zero-CoT [13]: This strategy uses a simple natural language instruction (€.g., “Let’s think step by step”)
to activate the inherent sequential reasoning ability of autoregressive models, enabling them to generate
logically coherent reasoning chains without the need for examples. This not only lowers the barrier to
prompt design but also significantly improves the zero-shot reasoning performance, enhancing both the
efficiency and generality of reasoning.

19


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

¢ Plan-and-Solve [20]: By clearly dividing the reasoning process into a planning phase and a detailed
execution phase, this strategy helps autoregressive models first construct a solution framework and then
fill in specific content. This enhances the structural integrity and global consistency of the solution,
proving particularly effective for tasks requiring multi-step logical reasoning and long-range dependency
modeling.

Together, these strategies strengthen the sequential reasoning ability of autoregressive models, guiding them
through prompt design to generate continuous and logically sound reasoning paths in a more systematic and
reliable manner. However, we found that these strategies are not suitable for DLLMs.

D.3. Constraint-guided Reasoning Prompting

Complex-CoT: The original version of Complex-CoT [6] leverages a few-shot reasoning technique to prompt
LLMs into performing more sophisticated reasoning processes. This approach enhances the model’s ability to
handle tasks that require a series of logical inferences or multi-step reasoning, thereby improving the overall
performance on complex questions. Specifically, by providing a few-shot example that demonstrates how to
perform intricate reasoning, the model learns to apply similar patterns to new, unseen problems.

In contrast, the Constrained-Guided Version of Complex-CoT introduces a crucial modification to meet
specific requirements. Rather than using few-shot examples, we reframe the prompting method as instruction-
based, zero-shot constraints created by human experts. These constraints guide the model’s reasoning process
without the need for training on a set of example problems. To implement this approach, the following
prompting structure is used to ensure that the model approaches each question with the necessary depth
and detail:

Complex-CoT (Constrained-Guided Version)

You should think about the following question as thoroughly and in as much detail as possible.

Question: {question}

MARP: The original MARP [3] employs an instruction-based, in-context-learning approach to guide LLMs in
structuring and constraining each step of the reasoning process. This method decomposes complex problems
into manageable components by promoting multi-step reasoning, while ensuring each step is focused and
achievable. By constraining reasoning at each stage, MARP prevents overgeneralization and ensures logical,
organized outputs.

To meet the requirements of the Constrained-Guided Version, we modify MARP in two ways: first, by
organizing reasoning into discrete steps, and second, by enabling parallel processing within each step. This
approach allows the model to perform multiple operations simultaneously without compromising clarity
or precision. The key concept is to balance step-by-step reasoning with parallel processing, enhancing
task efficiency. Each reasoning step involves multiple basic operations, ensuring clarity and minimizing
computational overhead.

The following prompt structure is used to guide the model’s reasoning process:

20


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

MARP (Constrained-Guided Version)

Reason step by step, but process operations in parallel.

¢ At each step, you may perform multiple simple operations (up to 5).

¢ Each operation must remain basic and not involve excessive complexity.

¢ If you choose to perform more operations in a single step, then each operation must be correspond-
ingly smaller in scope.

Question: {question}
be J

D.4. Parallel-encouraging Prompting

This parallel-encouraging strategy is essential for reducing PSC in reasoning. To adapt MARP for DLLM,
we enable the model to process multiple operations concurrently, avoiding the bottleneck of sequential
processing, where each step depends on the completion of the previous one. This parallel processing speeds
up reasoning and enhances scalability. At the same time, operational complexity constraints ensure the
reasoning process remains clear and manageable. This method balances parallel execution with simplicity,
allowing for effective multi-step reasoning without overwhelming the model with overly complex tasks.
Moreover, the model adjusts operation complexity dynamically. When tasked with more operations in a given
step, each operation must be simpler, preventing cognitive overload and helping the model stay focused on
individual tasks.

Ultimately, this approach enables the model to execute parallel reasoning efficiently while maintaining
clarity and precision. The detailed prompting for implementation is as follows:

Reasoning in parallel. In each step, do as many basic operations as you can, up to 5.
Any single operation cannot be too complex.
If you use more operations in a step, the maximum allowed size for any operation decreases.

Question: {question}
eee .NQQ(jY 7 70._e_07_\0%}

E. DLLM’s Limited Capabilities of Long CoT Reasoning

E.1. Long Chain-of-Thought Capabilities

Following Chen et al. [4], the Long Chain-of-Thought (Long CoT) reasoning capabilities comprise three
linked components: deep reasoning, exploration, and reflection.

Deep Reasoning. Given s; as the i-th reasoning step, Deep reasoning models the conditional probability
Po(So,$1,---,SxK|S0), facilitating multi-step logical inference through iterative refinement. The associated
reverse process can be characterized by a factorization:

K
Po(So,$1,---,SK|So) = II Po(si+1|Si)- (19)
i0

Exploration. Exploration stems from the probabilistic nature of the reverse process. At each exploration
step s;, multiple samples st can be drawn from the conditional distribution pg(s;|s;,i < j), enabling the

21


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

model to explore diverse plausible continuations or solutions. This is formalized as:
si ~ palsjleni<j), k= 1,.++K, (20)

where K controls the breadth of exploration. This sampling diversity enhances robustness by covering
multiple reasoning paths and mitigating premature convergence to suboptimal outputs.

Reflection. We view reflection as a self-correction mechanism arising from iterative conditioning on latent
states. At each reverse step, the model revises its belief about the target sequence using the immediately
previous state and, via the accumulated latent trajectory, all prior estimates. Formally, this corresponds to
implicit message passing:

Sj ~ Po(s;|Si,i > j), (21)
where §; denotes the corrected state at step j, enabling iterative error correction and refinement.

Together, these components yield a procedure that combines structured logics with stochastic exploration
and continual self-correction, enabling effective reasoning on complex multi-step tasks.

E.2. Stategies for Self-Reflection and Self-Exploration experiments on DLLMs

To investigate whether DLLMs truly possess the fundamental capabilities for Long CoT Reasoning, we
designed two sets of experiments: self-reflection and self-exploration,using two distinct prompting strategies
to examine the basic abilities of DLLMs.

Self-Reflection: (1)Prompting Reflection, structured reflection prompts are embedded within initial in-
structions,requiring the model to perform logical self-checking during generation. (2)Autoregressive Forcing
Reflection, correction prompts (e.g., "Wait...there might be something wrong") are replaced with the end-of-
sequence (EOS) token as a post-generation intervention strategy.

Self-Exploration: (1) Prompting Exploration, which embeds exploration prompts in initial instructions to
activate multi-path reasoning. (2) Autoregressive Forcing Exploration, which replaces EOS token to “Let’s
think in another way...” to induce exploratory reasoning.

E.3. Evaluation of Self-Reflection and Self-Exploration Capabilities

In evaluating the self-reflection capabilities of the LLaDA-8B-Instruct [16] and Dream-7B-Instruct [27]
models, the BigGSM dataset was utilized. During the generation process, we employed a temperature of
0.7 for self-reflection and 0.2 for self-exploration, coupled with top-p sampling set to 0.95. Additionally,
diffusion steps were configured to 512, and the generation length was fixed at 512. For the investigation into
the models’ self-exploration capabilities, the experimental settings were identical, with the sole distinction
being the substitution of the reflection strategy with an exploration strategy. Based on the setting of Qin
et al. [18], we utilize the following reasoning metrics for deeper analysis:

Semantic Alignment: The semantic alignment metrics [7] lies in the reasoning alignment vector, which
spans from the N-step hypothesis h to the source s of length T:

r-align(h + s) = {01,02,-+- ,an}, (22)
where each alignment value can be calculated as:

1 + max/_; cos(hj, si)|
a; = r-align(h; > s) = 5 (0, 1]. (23)

22


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

Here, such an alignment value is the normalized cosine similarity between the reference step and the most
similar sentence in a context, and explicitly measures the grounding of the step-wise reasoning with respect
to the source text. The alignment vector r-align(h — s) is estimated by matching the source text and the
reasoning chain on the embeddings of the tokens and individual reasoning steps. A similar confidence
alignment score is introduced in CTC to measure whether the information of the i-th source document token
s; is supported by the hypothesis token h;, assessing whether the reasoning step h; supports the source
context s.

Repetition-word: To identify repeated, or paraphrased steps, we look at the repetition word scores [7]
between all steps in the hypothesis chain:

1— max max_ |(1/M,;) poeny hit > hj
i=2..N j=1..i—1 Dy align ( i)

For each pair of sentences, we look at the mean token alignment and find those sentences that maximize
this alignment score. In other words, Repetition-Token will punish chains where there are at least two steps
with high overlap in token embeddings.

Informativeness: Measures how well information present in the source is used in the reasoning steps, we
calculate informativeness [7]:

(1/T) Sof Fatign (Se > A) + (1/N) S34 Tatign (hi > 8)
;

Info-step gives a higher score to reasoning steps that are well-grounded with respect to the source, and
identifies the degree of information from source that is covered by the generated hypothesis. A lower
Info-Step score corresponds to the reasoning steps that are not related to the source sentences or have missed
information provided in the context.

Reasoning-Alignment : The most straightforward way to evaluate the correctness of the hypothesis chain
is to compare the degree of the overlap between the hypothesis and the reference. One way of doing that is
to measure the reasoning alignment [7] between them:

1 N
N S- ratign (hi > r).

i=1

Token-Entropy_: To calculate the token entropy, we will utilize the pipeline as follows: First, calculate the
probability of each token p(t;), which is the frequency of token t; divided by the total number of tokens N:

n(t;) = oe

Next, calculate the information content I(t;) of each token, which reflects the uncertainty contribution of
that token to the text:
I(ti) = — log(p(ti))

Finally, token-entropy is the weighted average of the information content of all tokens, given by:

-- Yow ) log (p(ti))

23


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

where p(t;) is the probability of token ¢;, and log(p(t;)) is the corresponding logarithmic information content.
Token-entropy reflects the overall uncertainty of the text. A higher value indicates that the text is more
random and diverse, while a lower value suggests that the text is more focused and repetitive.

Cosine similarity (Sim) Cosine similarity measures the degree of similarity between two vectors encoded
by BGE [24] by calculating the cosine of the angle between them. For text embedding vectors, a value closer
to 1 indicates greater semantic similarity. Let the two generated text vectors be A and B. The formula for
calculating their cosine similarity is:

A-B "_, A;B;
cosine_similarity(A, B) = Deiat AB, (24)

~ VAT By
|All - [Bl Sha 4? / BP

where A - B is the dot product of vectors A and B. ||A|| and ||B|| are the Euclidean norms (magnitudes) of
vectors A and B. A; and B; represent the components of vectors A and B along the i-th dimension.

Perplexity (PPL) of the Model Perplexity is a concept in information theory used to measure the uncertainty
of a probabilistic model in predicting samples. In natural language processing, it is employed to evaluate
how well a language model fits a set of test data.

Given a sequence of N tokens W = wy ,w2,...,Wy, where the language model predicts the probability of
this sequence P(W), the perplexity of the sequence is defined as:

PPL(W) = P(W)~® =exp (-x log P(w)) ; (25)

Because of the sequence’s independence assumption, we can compute P(W) as:
N
P(W) = [[?@ilen .++,Wi-1)- (26)
i=1

Therefore, the commonly seen formula for perplexity is:

N
PPL(W) = exp (-% Slog wn, 1) j (27)

i=1

where log P(W) is the log probability of the entire sequence. 4 SN, log P(w;|w1,...,w;-1) is the average
log probability of the sequence under the model. exp is the exponential function, used to transform the log
probability back to its original scale.

Observation of the results for both DREAM [27] and LLaDA [16] models, under both self-reflection and
self-exploration settings, the scores across all ROSCOE-SA evaluation metrics are highly similar. This indicates
that current diffusion language models (DLLMs) have not yet genuinely acquired the deeper capabilities
of self-reflection and self-exploration, as their outputs do not exhibit significant differences under varying
strategic prompts.

E.4. Evaluation of Deep-Reasoning Capability

Following Chen et al. [3], we further investigate reasoning boundaries (RBs) in deep reasoning capabilities
in mathematical reasoning. We prompt DLLMs to generate plans and assess their accuracy through manual

24


LANGUAGE ANALYSIS
o REASONING GROUP.

70
-®- Temperature=1.0 -@- Temperature=0.7
—@- Temperature=0.5 —®- Temperature=0.3 , 65
Temperature=0. 1
60
S
x
® 55
n
a
a, 50

i
un

—@- Temperature=1.0 -@- Temperature=0.7

—®- Temperature=0.5 —®- Temperature=0.3

iN
[o)

~~ Temperature=0.1

35

1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Parallel Scaling Size Parallel Scaling Size
(a) Parallel Scaling Performance on LLaDA (b) Parallel Scaling Performance on DREAM
with different temperature. with different temperature.

Figure 11: Parallel scaling performance of DLLMs under Different Temperature Settings

evaluation. When the model meets the question with fewer than 1 reasoning steps, accuracy surpasses 80%.
Conversely, when reasoning steps exceed 3-4, accuracy falls below 10%. Moreover, we first randomly select
200 samples to generate examples and split steps from the DLLM-generated rationales based on ROSCOE [7].
Further, we also manually identify the first model’s incorrect step position.

F. Three-directional Inference-Time Scaling on DLLMs

F.1. Parallel Scaling Experiment Details

In the parallel scaling section, we utilize the dual-cache generation strategy from Fast-dLLM based on
the diffusion language model LLaDA-8B-Instruct [16], and perform batch processing on the BigGSM [3]
dataset. Key configurations include:diffusion_steps=256, gen_length=256, block_length=32, and a Dynamic
Low-Confidence Remasking mechanism.

We also employed the dual_cache generation strategy from Fast-dLLM [23] on the Dream-7B-Instruct [27]
model for testing on the BigGSM reasoning dataset. The core configuration includes: diffusion_steps=256,
gen_length=256, and block_length=32.

The results are shown in Figure 11. It can be observed that the accuracy generally increases with higher
k-values. At the initial attempts, the accuracy at Temperature 1.0 was relatively low. Although it showed
significant improvement in the early stages, its later accuracy fell behind other temperatures. At Temperature
0.1, the accuracy growth was more stable initially, but eventually plateaued at around 60%, similar to
Temperature 1.0. Overall, intermediate temperatures demonstrated better pass@k accuracy performance,
achieving higher accuracy with more consistent and stable growth.

25


Bye
LANGUAGE ANALYSIS
LJ REASONING GROUP

F.2. Diffusion Scaling Experiment Details

We set the diffusion_step to be between 1 and 4096 (with max-token-length equal to 512). The other settings
are identical to those of parallel scaling. The results are shown in Figure 9 (a).

F.3. Sequential Scaling Experiment Details

We set the Max Token Length to be between 1 and 4096 (with diffusion-step equal to max-token-length).
The other settings are identical to those of parallel scaling. The results are shown in Figure 9 (d).

26
