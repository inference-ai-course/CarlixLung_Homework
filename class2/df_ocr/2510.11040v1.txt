arX1v:2510.11040v1 [cs.CL] 13 Oct 2025

Enabling Doctor-Centric Medical AI with LLMs
through Workflow-Aligned Tasks and Benchmarks

Wenya Xie!?, Qingying Xiao®, Yu Zheng, Xidong Wang!,
Junying Chen!?, Ke Ji!, Anningzhe Gao”, Prayag Tiwari‘,
Xiang Wan?, Feng Jiang!?", Benyou Wang!?”

'School of Data Science,The Chinese University of Hong Kong,
Shenzhen,2001 Longxiang Boulevard, Longgang
District, Shenzhen,518172,Guangdong,China.

?Shenzhen Research Institute of Big Data,2001 Longxiang Boulevard,
Longgang District,Shenzhen,518172,Guangdong,China.
3National Health Data Institute, Shenzhen,2001 Longxiang Boulevard,
Longgang District,Shenzhen,518172,Guangdong,China.
4Halmstad University,PO Box 823, Kristian IV:s vag 3,Halmstad,301
18,Halland,Sweden.
°Shenzhen University of Advanced Technology,No.1 Gongchang Road,
Guangming District, ,Shenzhen,518107,Guangdong,China.

*Corresponding author(s). E-mail(s): jiangfeng@suat-sz.edu.cn;
wangbenyou@cuhk.edu.cn;

Contributing authors: xie00470@umn.edu; xiaoqingying@sribd.cn;
yuzheng@link.cuhk.edu.cn; xidongwang1 @link.cuhk.edu.cn;
junyingchen2@link.cuhk.edu.cn; keji@link.cuhk.edu.cn;
anningzhegaoQ@gmail.com; prayag.tiwari@ieee.org; wanxiang@sribd.cn;

Abstract

The rise of large language models (LLMs) has transformed healthcare by offering
clinical guidance, yet their direct deployment to patients poses safety risks due
to limited domain expertise. To mitigate this, we propose repositioning LLMs
as clinical assistants that collaborate with experienced physicians rather than
interacting with patients directly. We conduct a two-stage inspiration—feedback
survey to identify real-world needs in clinical workflows. Guided by this, we con-
struct DoctorFLAN, a large-scale Chinese medical dataset comprising 92,000
Q&A instances across 22 clinical tasks and 27 specialties. To evaluate model


performance in doctor-facing applications, we introduce DoctorFLAN-test (550
single-turn Q&A items) and DotaBench (74 multi-turn conversations). Exper-
imental results with over ten popular LLMs demonstrate that DoctorFLAN
notably improves the performance of open-source LLMs in medical contexts,
facilitating their alignment with physician workflows and complementing exist-
ing patient-oriented models. This work contributes a valuable resource and
framework for advancing doctor-centered medical LLM development.

Keywords: Large Language Models, Healthcare AI, Medical Assistants, Clinical
Workflow

1 Introduction

Large Language Models (LLMs) have demonstrated significant potential in various
applications within healthcare, such as autonomous online consultations, which can
reduce costs and improve accessibility to medical services [1-8]. However, using LLMs
as a direct consulting tool for patients can bring serious health risks because patients
lacking medical expertise are easily misled by the inaccurate medical advice generated
by the model [9-11].

In contrast, developing LLMs as medical assistants for healthcare professionals
presents a safer and more practical direction. Doctors routinely deal with complex
information processing tasks, such as summarizing patient records, providing clinical
decision support, and educating patients. Using LLMs for these tasks could signifi-
cantly alleviate the workload of doctors, allowing them to perform their duties more
efficiently [12, 13]. Furthermore, large language models have shown promising results
in multi-task settings[14, 15], suggesting that LLMs have substantial potential when
applied to a multi-functional medical assistant role. Despite these promising devel-
opments, there remains a significant gap between the current capabilities of LLMs
and the complex requirements of real-world medical practice. Most existing medical
LLMs [8, 6, 16-18] have been trained on patient-centric datasets, which focus primar-
ily on tasks like pre-diagnosis and medical consultation. These datasets are limited
in scope and do not encompass the diverse and multifaceted nature of clinical tasks
encountered in actual medical environments. Moreover, previous research on LLMs
as medical assistants has often focused on a narrow set of tasks [12, 13], and these
models frequently fail to provide comprehensive responses to complex, real-world med-
ical inquiries [19, 20]. Another critical limitation lies in the current benchmark tests,
which often do not adequately assess the performance of LLMs as medical assistants.
Most widely used benchmarks rely on multiple-choice question formats [1, 10, 19-22],
which fail to align with the real-world requirements where detailed and comprehensive
responses are needed. Alternatively, these benchmarks typically assess only a small
subset of tasks [1], failing to cover the full range of workflows that doctors encounter
in practice.

To address the above issues, we aim to develop LLMs as better doctor assistants
by building comprehensive and practical datasets and evaluations. Firstly, to gain a


a > Example trajectory a > Example trajectory Other possible
for an outpatient. for an inpatient. = > trajectories.

Inquiry e Test Results

Prompts Interpretation
« Next

: Examinations
Inquiry e Disease Grading

Disease Inquiry

Symptom e Definitive Diagnosis

Initial Diagnosis
Case Summary
Differential
Diagnosis

Diagnosis

+ Emergency e Treatment Plan

Advice ¢ Medication Inquiry
e¢ Health : e Medication Advice
Guidance
e Follow-up
Plan aS Cerise
¢ Complications Analysis ° Surgery Necessity

+ Treatment Adjustment ¢ Surgical Plan

Post ¢ Preoperative Education
-Treatment Treatment

thorough understanding of doctors’ needs for medical assistants, we collaborate with
dozens of professional doctors to explore 22 tasks across four phases in real-world sce-
narios. These tasks are finalized through a two-stage survey using a heuristic-feedback
method, as shown in Figure 1. Based on these insights, we develop DoctorFLAN, a
comprehensive Chinese medical dataset containing approximately 92K samples that
capture the full spectrum of the doctor’s daily work, including both inpatient and out-
patient scenarios. It leverages GPT-4-polishing with reference enhancement, followed
by manual verification from professional doctors, to ensure samples provide reliable
and comprehensive expert responses for training our model (DotaGPT).

To develop effective doctor assistants, we construct a novel benchmark for medical
LLMs that includes both single-turn evaluation (DoctorFLAN-test) and multi-turn
evaluation (DotaBench) by simulating the dialogue with doctors in receiving patients
scenarios. The existing popular LLMs and our model, DotaGPT, are evaluated both
automatically and manually on the benchmark. The results indicate that existing
models, while acting as virtual doctors assisting patients, struggle with the diverse


DoctorFLAN-test DotaBench
Model Size P
Ee Diagnosis Treatment Post Average Average
-Diagnosis -Treatment
Open-source General LLMs
Qwen-1.8B-Chat 1.8B 5.28 4.56 3.96 5.44 4.48 4.68
Baichuan-13B-Chat 13B 6.20 6.51 6.31 7.55 6.57 7.59
Baichuan2-7B-Chat 7B 6.32 6.36 6.34 7.70 6.59 7.A1
Baichuan2-13B-Chat 13B 6.76 6.85 6.94 7.81 7.04 TAT
Yi-6B-Chat 6B 7.00 6.83 6.83 7.66 6.98 8.25
Yi-34B-Chat 34B 7.36 7.38 7.95 8.78 7.80 8.65
Open-source Medical LLMs
BianQue-2 6B 5.56 3.27 3.65 4.78 3.72 4.12
DISC-MedLLM 13B 5.56 4.23 3.54 5.14 4.24 4.97
HuatuoGPT 7B 5.32 4.24 3.72 4.92 4.29 5.88
HuatuoGPT-IT 7B 7.60 7.02 6.69 7.42 7.03 7.90
DotaGPTyi.63 6B 8.32 7.624 11.6% 7.68y 19.4% 8.44 7.814 11.9% 8.36 4 1.3%
DotaGPTpaichuan2-7B 7B 8.48 8.01; 25.9% 8.234 29.8% 8.80 8.254 95.2% 8.36 + 12.8%
Proprietary LLMs

GPT-3.5 N/A 6.40 6.85 6.26 6.74 6.64 7.83
Claude-3 N/A 7.80 8.38 8.28 8.76 8.38 9.21
GPT-4 N/A 8.00 8.41 8.28 9.04 8.42 9.41
Table 1: Automatic Evaluation Results on DoctorFLAN-test and DotaBench. The

subscript of DotaGPT (e.g., DotaGPTy;¢6g) indicates the backbone on which the
model was initially trained. The red arrows (+) with percentages indicate the improve-
ment of DotaGPT over the corresponding chat models with the same backbone.

and complex tasks required for real-world roles that assist doctors. In contrast, our
DoatGPT serving as a doctor assistant exhibits robust performance across tasks in
both DoctorFLAN-test and DotaBench.

Our contributions are threefold:

(1) We explore the underexplored scenario of developing medical models as doctor
assistants, providing essential data, models, and benchmarks that complement existing
research in this domain.

(2) We construct a 92K-sample dataset for doctor assistants by collaborating with
dozens of medical professionals, using a heuristic feedback method to identify 22 key
tasks and employing reference-enhanced polishing and manual verification.

(3) We introduce an expert-involved benchmark to assess large language models
in doctor-oriented scenarios, covering both single-turn and multi-turn interactions,
and thoroughly analyze the consistency between manual and automatic evaluations,
comparing them with widely accepted benchmarks.

2 Result

2.1 Automatic Evaluation Results

Table 1 outlines the automatic evaluation results of the existing medical models on
DoctorFLAN- test.


Take-away 1. Existing models perform poorly in the Diagnosis and Treatment
Phases.

The results reveal a notable performance decline for all models during the diagnosis
and treatment phases compared to the pre-diagnosis and post-treatment phases. This
drop may be attributed to the high medical knowledge requirements of tasks like
Disease Grading and Surgical Plan, for which models are often undertrained due to
a lack of knowledge-intensive datasets. However, DotaGPT models show a significant
improvement in these phases. Specifically, DotaGPT Baichuanz-7p and DotaGPT yi-¢p
exhibit performance increases of 11.6% and 12.4% in the diagnosis phase, and 25.9%
and 29.8% in the treatment phase, respectively. These enhancements demonstrate the
value of our tailored dataset in improving performance on complex medical tasks.

Take-away 2. Larger Models Perform Better.

When comparing Yi-6B-Chat (average score: 6.98) with Yi-34B-Chat (average
score: 7.80), and Baichuan2-7B-Chat (average score: 6.59) with Baichuan2-13B-Chat
(average score: 7.04), we observe that larger models consistently outperform their
smaller counterparts across all four phases. The models with more parameters achieve
higher average scores, likely due to their enhanced reasoning abilities, which better
equip them to handle the considerable complexity of the tasks in our evaluation.

Take-away 3. Limitations of Virtual Doctor Models in Workflow Assistance Tasks.

Virtual doctor models originally designed to provide medical advice to patients,
such as BianQue-2 and HuatuoGPT, perform relatively poorly in tasks related to
doctor workflow assistance, with scores of 4.12 and 5.88, respectively. These models are
primarily trained on large medical dialogue datasets, where the focus is on mimicking
the question-and-answer style of doctors, with the goal of functioning as a virtual
doctor. However, medical dialogues like Huatuo26M|23] are mostly based on online
consultations, which may not capture the full range of tasks involved in a doctor’s
workflow. As a result, these models struggle with more specific, nuanced tasks that
occur in everyday medical practice.

Take-away 4. Medical Dataset Fine-Tuning Does Not Always Enhance Performance
on DoctorFLAN.

A comparison between the DISC-MedLLM (4.24) and its chat counterpart,
Baichuan-13B-Chat (6.57), reveals that the medical domain-specific fine-tuning of
DISC-MedLLM does not lead to better performance on the DoctorFLAN tasks. In
fact, the fine-tuned DISC-MedLLM underperforms compared to the general-purpose
Baichuan-13B-Chat. This outcome underscores the potential risks of excessive spe-
cialization, suggesting that a balance between domain-specific fine-tuning and general
adaptability is crucial for ensuring broader model applicability.


Models Average Score

BianQue-2 4.58
HuatuoGPT 4.97
DISC-MedLLM 5.36
Baichuan2-7B-Chat 6.69
GPT-4 8.06
DotaGPT Baichuan2-7B 7.83

Table 2: Human Evaluation Results on DoctorFLAN-test. For detailed task-by-task
results.

Take-away 5. DoctorFLAN Fine-Tuning Improves Performance on Doctor-
Assistance Tasks.

In contrast, our DotaGPT variants, fine-tuned on the DoctorFLAN dataset,
demonstrate significant performance improvements over their respective chat model
counterparts. Specifically, the variant fine-tuned on Baichuan2-7B shows a substantial
improvement of 25.2%. Similarly, the DotaGPT variant fine-tuned on Yi-6B outper-
forms the Yi-6B-Chat by 11.9%. The improvement on both backbones highlights the
effectiveness of DoctorFLAN and brings our models’ performance close to those of
leading proprietary models such as Claude-3 and GPT-4.

We further evaluate DotaGPT’s performance on DotaBench to assess its ability in
practical multi-turn settings, which reflects its real-world applicability. This out-of-
domain evaluation is detailed in Table 1. Notably, our DotaGPT variants significantly
outperform models of comparable size on DotaBench, even surpassing the larger Yi-
34B-Chat model. This strong performance underscores DotaGPT’s robust ability to
generalize from DoctorFLAN to out-of-domain contexts.

2.2 Human Evaluation Results

Model Average Score
Baichuan2-7B-Chat 8.25
DotaGPT Baichuan2-7B 8.544 3.5%

Table 3: Human Evaluation Results on the DotaBench.

Aside from the automatic evaluation, we conduct manual evaluation on a subset
of models due to resource constraints, as shown in Tables 7 and 8. The results show
that on DoctorFLAN- test, DotaGPT Baichuan2-7B (7.83) outperforms patient-assistance


Pearson r: 0.82, p-value: 0.00

Human Evaluation Scores

1 2 3 4 5 6 7 8 9
Automatic Evaluation Scores

models like BianQue-2 (4.58), HuatuoGPT (4.97), and DISC-MedLLM (5.36), as well
as the general counterpart Baichuan2-7B-Chat (6.69), consistent with the automatic
evaluation results. Further, human evaluation results on DotaBench, shown in Table
3, confirm DotaGPT gaichuan2-7B’S strong performance, with an average score of 8.54,
surpassing Baichuan2-7B-Chat (8.25) by 3.5%.

To verify the reliability of our evaluation methods, we also conduct a task-level cor-
relation analysis between human and automatic evaluations on the DoctorFLAN-test.
For each model and task, we average the results across 25 samples per task (this aver-
aging is done to ensure consistency and minimize the impact of outliers or variance in
individual responses). Our analysis, covering 132 data points, reveals a Pearson corre-
lation coefficient of 0.82, indicating strong consistency between evaluation modes [24],
as shown in Figure 2.2 .

2.3 Generalization of DotaGPT on other benchmarks

To further evaluate DotaGPT’s medical knowledge and generalization capability, we
assess its performance on several established medical benchmarks, as shown in Table
4. DotaGPT Baichuan2-7B delivers competitive results across CMMLU [25], MMLU [26],
CMExam [27], and CMB-Exam [10]. Notably, it outperforms Baichuan2-7B-Chat in 3
out of 4 categories. Although DotaGPT Baichuanz-7p falls short of HuatuoGPT-II, this
performance gap may be attributed to the significantly larger training dataset used
by HuatuoGPT-II.

2.4 Case Study

To provide a clearer demonstration of our model’s ability to generate knowledge-
intensive responses in doctor-oriented tasks, we select a case from Differential
Diagnosis for comparison. As detailed in Table 5, we present the responses from both


Model CMMLU mea. CMExam MMLU wea. CMB-

Exam
Open-source Medical LLMs
DISC-MedLLM - 36.62 - 32.47
HuataoGPT-II 59.08 65.81 51.44 59.00
Baichuan2-7B-Chat 50.74 50.48 50.29 A3.33
DotaGPT Baichuan2-7B 54.58 59.76 48.49 52.42*
Proprietary LLMs

GPT-4 - - - 59.46

Table 4: Comparative Performance of Medical LLMs on Diverse Medical Benchmarks.
CMB-Exam scores are from [10], except for DotaGPT paichuan2-7B

HuatuoGPT and DotaGPT(Baichuan2-7B). Despite HuatuoGPT’s fluent responses,
they lack substantial information, often repeating general rather than medically spe-
cific answers. Conversely, DotaGPT not only accurately diagnosed the condition but
also provided detailed, professional reasoning, demonstrating its superior ability to
deliver knowledge-intensive answers.

3 Discussion

In this paper, we focus on underexplored scenarios of developing medical models as
doctor assistants. We first collaborate with dozens of doctors and conduct a two-stage
survey to accurately identify real-world clinical tasks for efficient doctor assistance.
We then create DoctorFLAN, using reference-enhanced refinement to overcome the
training limitations of previous models. Additionally, we introduce DotaBench as a
complementary evaluation to assess the effectiveness of popular medical LLMs as
doctor assistants. Benchmark results indicate that while existing LLMs face challenges
in this role, DotaGPT’s performance shows that our dataset can significantly enhance
their capability, providing a valuable supplement to current medical LLM research.

We also acknowledge the following limitations of this stage work. The Doctor-
FLAN is currently only available in Chinese and may require supplementation in other
languages. Consequently, it cannot be guaranteed that DotaGPT trained on Doctor-
FLAN will perform well in languages other than the one on which it has been tested.
However, the methodology employed to create DoctorFLAN can be applied univer-
sally across different languages. Additionally, although DotaGPT has demonstrated
impressive performance on the benchmarks, it is important to exercise caution when
using its outputs, particularly in real-world doctor-oriented interactions.

Ensuring the privacy and security of data is paramount in the development of
medical applications. The datasets used in this study, primarily derived from Medtiku,
an open-source repository of medical examination questions, and PromptCBLUE [28],
are both freely available for use. Additionally, we incorporate data from an internet
medical encyclopedia hosted by 120 Ask [29], also open to the public.

Given the potential issues with the credibility of content generated by DotaGPT,
we are committed to strictly regulating the model’s use to prevent misuse. Our


Query:

ETA ANAS . TEAR BAR, FELBERT, FARR TT oe i mT RE SA TTF

DAB UN ALB aE — AF RRA I «TTA: IA, REV RSUEILAIR AUR A EFA,

FAREL, FOR RU PAULA, BUM, AAMT, FORA, FCP

(Translated) Based on the patient’s history, symptoms, and examination results, list several possible diagnoses first, then explain why these
diagnoses may correspond to the patient’s condition and how to confirm the final diagnosis through further examination or observation.
<Case description>:

The patient was a male, 11 months old, whose parents complained of a 2-week-old white reflection in the centre of the right eye,
with no redness of the eye and nostrabismus. There was no family history of this condition, and the patient denied any history of
trauma. He had a full-term delivery with no history of oxygen intake and no history of birth injury.

Golden:

ZEF SULA BIER, UP IL SRE OT:
1. JERTERANBE - EULER BU), FERRULE, EB COE] BE eA IE he ny

2. HAR AAI (Retinoblastoma) - RAAT PICA B], ERATION, (LCE LBS OLR ACER EE,
TEM AMAIA BI... AA wT BEE AA:
- FERPEA ARE: PREC BOER) REE HT CURT ENE AAA AE AY, TCH REET IR RL, ERR] REL EE Tl al»
~ PLR MAAR: LA RRR te CR ARCS) HP PS HS Pe

- SEPHPVSRARE: BARE MM, AHA AIR, AA AZIIR -

~ PLR A FT ES LAS DUEK, (AA EAE IE I aS S AEB EL...

(Translated) Based on the history and symptoms of the child, the following are a few possible initial diagnoses:

1. Congenital cataract - The child is young and has no other medical history such as prematurity...

2. Retinoblastoma (Retinoblastoma) - Although there is no such case in the family...

ns

Why these diagnoses may be consistent with the patient’s condition:

- Congenital cataract: fundus reflections may be produced by a light source hitting the cloudy lens..

- Retinoblastoma: white fundus reflections (also known as cat’s eye reflex) in children are typical of retinoblastoma.
- Consider developmental anomalies such as PHPV: usually occurring unilaterally...

DotaGPTpaichuan2-7B: __ ;

RRMA, US ERE Ci RE Seto:

BRE IT: 1. *SERPEAABE** (Congenital Cataract):

~ PRE: ALA BG PEEL HEL DK TA BB...

2. FLFR ABE AGI** (Retinoblastoma):

-: ZELLER i as OLE EE RA ARREARS TK, (ae te BT ER RB I...
Based on the information you have provided in your medical history, here are some possible preliminary diagnoses and their plausibility:
Possible Diagnoses:

1. Congenital Cataract/:

- Rationale: A cataract usually presents as a white reflection in the pupil area or at the location of the lens... /

- FURTHER EXAMINATION: Ultrasound may be performed to assess whether the structure and position of the lens is normal...

2. Retinoblastoma (Retinoblastoma)/:

- Rationale: In infants and children, retinoblastoma is the most common malignant tumour of the eye. Although usually associated with
symptoms such as redness of the eye and strabismus, early lesions may also present only as white reflections./

HuatuoGPT:

RCE, DER —HERT RENT:

1. SARA. AER LAS OLR, TLS rsa BG [So ART ES HH EL . SS. PEK «MR
BAAR, WA. KE, PRACT AT RENTON «
2. BABE: melted AE NENA, HRS SEAN EI, MLZ RPA EK, USK IR~ MOE, ABA AA
tae — “PS FY RET
3. FABER: FIRE — APPEAR S[k» PARA AT BESS FE ML. PRR. TASER » WRAP EEK
WAS. OKSE, IBA PARRA — TT ERIS IT
4. PEM: pe WM SBORET me, TAS, HOUT © URAC EK, SKI. MS, ABA FEHR
{Hs — FY REST...
(Translated) Based on the history you have provided, here are some possible diagnoses:

1. Conjunctivitis: Conjunctivitis is a common eye condition usually caused by a viral or bacterial infection...

2. Cataracts: Cataracts are a common eye disease in the elderly, usually resulting in clouding of the lens and affecting vision.
Cataracts are also a possible diagnosis if your child has no other symptoms such as fever or cough, such as headaches or vomiting.X

3. Keratitis: Keratitis is an inflammatory disease ... Keratitis is also a possible diagnosis if your child has no other symptoms such as
fever or cough...X

4. Glaucoma: Glaucoma is a common eye disease ... Glaucoma is also a possible diagnosis if your child has no other symptoms such as
fever or cough... X

Table 5: Illustrative case study from the Initial Diagnosis task in DoctorFLAN-test,
showing Chinese model responses along with key English highlights for clarity. The
example includes the ground truth (Golden) and outputs from DotaGPT gaichuan2-7B
and HuatuoGPT. We annotate key segments using green /for medically correct infor-
mation and red Xfor incorrect or irrelevant reasoning. The full Chinese outputs are
preserved to support fine-grained comparison across models.

datasets, DoctorFLAN and DotaBench, will be released under terms that uphold the
highest ethical standards. This commitment ensures that while advancing the capa-
bilities of large language models in healthcare, we also safeguard sensitive medical
data.


4 Methods
4.1 Necessity of LLMs for Doctors

Recent advancements in medical large language models (LLMs) such as PMC-
LLaMA [30], Med-PaLM [1], Med-PaLM2 [2], and HuatuoGPT-II [5] have significantly
contributed to enhancing the domain-specific knowledge of these models and support
the subsequent application of medical LLMs. Leveraging these advancements, several
popular medical application models [3, 6, 7, 31-33] are trained on extensive patient-
doctor dialogues with the goal of functioning as autonomous virtual doctors, providing
medical consultations directly to patients.

Despite advancements, the accuracy of these models in generating expert-level
medical advice remains insufficient [11]. Directly providing their responses to patients
without medical training poses significant risks, as these patients may not be able to
identify errors. For instance, a patient with suspected appendicitis presenting with
abdominal pain and fever may receive an incomplete recommendation from the model,
potentially delaying critical intervention.

In contrast, healthcare professionals, equipped with specialized medical knowl-
edge, are capable of identifying such errors. This highlights the potential of developing
medical large language models designed to assist doctors in addition to direct patient
consultation. While recent efforts have been made to develop medical LLMs as assis-
tants to support doctors on specific scenarios, such as MedDM [12] for differential
diagnosis and treatment recommendations and Dia-LLaMA [13] for CT report gener-
ation. However, these works typically address only isolated tasks, leaving a significant
gap in the development of LLMs capable of comprehensively supporting the full
spectrum of tasks within a doctor’s workflow.

4.2 Towards Better Doctor Assistants

Developing a medical LLM capable of assisting across the entire clinical workflow
requires a dataset that comprehensively covers all relevant tasks while providing
detailed and accurate responses. Furthermore, a practical benchmark is essential to
evaluate whether the model can generate outputs that effectively support doctors in
real-world scenarios.

Training Data Across the Entire Workflow. As shown in Table 6, exist-
ing datasets for online medical consultation dialogues, such as Huatuo-26M][23],
MedDialog/16], and others[3, 17, 18], primarily provide responses for pre-diagnosis
scenarios. However, these datasets only cover a limited portion of medical scenarios,
making them unsuitable for comprehensive, end-to-end medical workflows. Conversely,
structured resources such as knowledge graphs (e.g., CMeKG[19]) and multiple-choice
question-answer datasets (e.g., MedMCQA[20] and CMExam|27]) cover a broader
range of clinical scenarios but are limited in their ability to generate knowledge-
intensive, context-rich responses. Thus, there is an urgent need for a comprehensive
dataset that not only encompasses the entire spectrum of a doctor’s workflow but also
provides detailed and context-rich answers. Such a dataset is crucial for effectively
training and deploying LLMs in clinical settings.

10


Dataset Applied Entire Knowledge-intensive

Scenarios Workflow Responses

Huatuo-26M OMCD x v
MedDialog OMCD x v
HealthCareMagicl00k OMCD x v
ChatDoctor10k OMCD x v
webMedQA OMCD x v
KUAKE-QIC OMCD x v
CMeKG KG v x
CMExam MCQA v x
MedMCQA MCQA v x
DoctorFLAN

&DotaBench DAG ¥ v

Table 6: Comparison of existing medical training datasets. OMCD represents Online
Medical Consultant Dialogue; KG represents Knowledge Graph; MCQA represents
multiple-choice Question Answer; DAQA represents doctor-oriented Question Answer.

Doctor-Assistance Benchmark for Clinical Workflows. Furthermore, exist-
ing benchmarks are insufficient for effectively evaluating models as medical assistants
due to their lack of alignment with practical, real-world scenarios. Common bench-
marks, such as PubMedQA [21], MedQA [22], MultiMedQA [1], MedMCQA [20],
CMExam [27], and CMB [10], primarily focus on assessing knowledge accuracy
through multiple-choice questions. However, real-world medical tasks are rarely lim-
ited to answering multiple-choice questions. Instead, they often require more nuanced
decision-making accompanied by detailed analysis and explanations. Similarly, bench-
marks like PromptCBLUE [28], which evaluate isolated skills such as Named Entity
Recognition in medical NLP tasks, fail to capture the integrated and contextually
rich requirements of doctor-assistant applications. While open-ended benchmarks like
HealthSearchQA [1] offer broader evaluations, they still fall short of covering the full
spectrum of tasks encountered in a doctor’s workflow. Thus, there is a clear need for
more realistic and comprehensive benchmarks that accurately simulate diverse medi-
cal practice scenarios. These benchmarks should be designed to evaluate the ability of
LLMs to function as effective doctor assistants, providing contextually aware, detailed,
and practical responses that align with real-world requirements.

4.3 Task and Dataset Development for Clinical Workflows

Prior clinical NLP systems such as cTAKES [34] have primarily focused on retrospec-
tive information extraction, aiming to standardize clinical notes through rule-based
processing for tasks like concept normalization and coding. We shift the focus
from retrospective extraction to prospective generation, designing workflow-aligned,
open-ended tasks that reflect real-world clinical needs. To support workflow-aligned
generation, we first define a set of 22 representative tasks that span the entire clin-
ical workflow. These tasks are derived through expert interviews and validated via

11


a large-scale survey with licensed physicians to ensure their practical relevance and
generalizability. Building on this task framework, we construct two complementary
datasets: DoctorFLAN, which covers single-turn Q&A aligned with each task, and
DotaBench, which extends the task design into multi-turn dialogue settings.

To ensure that the tasks identified align closely with the practical needs of medical
professionals, we organized a symposium with 16 medical experts to discuss key tasks
in the medical workflow. To avoid omissions, the experts categorize the workflow into
four phases: Pre-diagnosis, Diagnosis, Treatment, and Post-treatment. In each
phase, the experts identify and outline the specific tasks that doctors typically perform
in daily practice.

Pre-diagnosis tasks are actions that doctors perform before the diagnostic process.
The tasks identified in this phase include Triage, as outlined in Table 7. Compared to
the diagnostic and treatment tasks, the pre-diagnosis tasks generally involves fewer
complex medical decisions. However, the introduction of LLMs has the potential to
enhance workflow efficiency by automating the generation of simple decision-making
outcomes.

Diagnosis tasks encompass all activities performed by doctors during the diagnostic
process that contribute to formulating the final diagnosis. The tasks are summarized
in Table 7. Given the complexity of medical decision-making in this phase, LLMs have
significant potential to assist doctors in improving decision quality. For example, in the
questioning prompts task, LLMs can generate questions based on the patient’s condi-
tion, encouraging doctors to conduct more comprehensive and thorough inquiries. In
clinical practice, less experienced doctors may overlook critical diagnostic considera-
tions, failing to take a complete medical history. LLMs can alleviate this by providing
additional prompts that guide thorough questioning. For instance, when evaluating
a patient with abdominal pain, some doctors may focus solely on the location and
intensity of pain, while an LLM might prompt the doctor to inquire about changes in
bowel habits, potentially revealing diagnostic clues such as irritable bowel syndrome
or inflammatory bowel disease. Additionally, some tasks such as Case Summarization,
can enable LLMs to automatically generate medical case summaries, thereby saving
time and effort.

Treatment tasks refer to all actions performed by doctors after diagnosis and before
patient discharge. These tasks include outpatient tasks such as Medication Advice
and inpatient tasks such as Surgical Plan, with a complete task definition provided in
Table 7. LLMs have the potential to assist doctors in these tasks by providing advice,
thereby improving decision accuracy and consistency.

Post-treatment tlq asks are those that occur after a patient has completed their
primary treatment and is transitioning to long-term recovery or ongoing management.
The tasks in this phase primarily involve Health Guidance and Follow-up Plan, as
detailed in Table 7. While long-term management tasks involve fewer complex deci-
sions, they still require considerable time and effort from doctors. LLMs can help by
quickly generating suggestions, improving workflow efficiency in this phase.

12


Phase Specific Tasks Detailed Description
Pre-diagnosis Triage Recommend suitable departments based on patient symptoms
Inquiry Prompts Suggest follow-up questions based on patient history
Symptom Inquiry Provide key information about specific symptoms
Disease Inquiry Provide key information about specific diseases
Initial Diagnosis Identify possible conditions based on initial assessments
Case Summary Compile key points from doctor-patient dialogue into a patient case
Diagnosis Differential Diagnosis Differentiate between conditions with similar symptoms
Next Examinations Recommend necessary tests for further clarity
Test Results Interpretation | Explain the implications of test results
Definitive Diagnosis Confirm the most likely diagnosis
Disease Grading Categorize disease severity using standard criteria
Emergency Advice Provide guidance for urgent medical situations
Treatment Plan Propose potential treatment approaches
Medication Inquiry Offer detailed information about medications
Medication Advice Provide specific medication recommendations
Treatment Complications Analysis Highlight potential risks or complications
Treatment Adjustment Recommend updates based on patient response
Surgery Necessity Assess the need for surgical intervention
Surgical Plan Outline key considerations for surgery
Preoperative Education Explain surgery and postoperative care to patients
Health Guidance Advise on recovery and recurrence prevention
Post-Treatment :
Follow-up Plan Develop a plan for regular check-ups and ongoing care

Table 7: Tasks identified in the four phases.

4.4 Validating the Task Coverage through Expert
Collaboration

To further validate the universality of the tasks defined in the focus group discussions
and gain deeper insights into doctors’ needs for medical LLM assistance, we conducted
a survey with doctors from 13 tertiary hospitals. To ensure respondent qualifications,
we distribute the survey exclusively within verified professional groups composed of
licensed, practicing physicians with relevant clinical experience. The survey does not
collect any personally identifiable information, in order to respect respondent privacy
and encourage candid feedback. We initially list all 22 predefined tasks and ask partic-
ipants to rate each task on a scale from 1 to 5, where 5 indicates that LLM assistance
is crucial for improving work efficiency, and 1 signifies no impact on task efficiency.
In addition, we invite the doctors to propose new tasks across four phases of their
workflow, beyond the predefined tasks. Following this, we inquire about the challenges
they encounter when using medical LLMs in practice, providing valuable feedback for
the development of future medical assistant models. We initially receive 82 completed
questionnaires. To ensure the validity of the responses, we apply two criteria: (1) the
completion time must be more than one-third of the average duration (191.82 seconds)
observed across all submissions, indicating potential lack of thoughtful consideration,
and (2) responses should not exhibit marked uniformity (e.g., repetitive selection of
the same answer option), suggesting insufficient engagement with the content. After
applying these criteria, we identify 71 valid responses for analysis. The results reveal
that most of the 22 predefined tasks receive high ratings, with scores exceeding 4,
indicating that LLM assistance is highly effective for these tasks. As shown in Figure

13


Task Efficiency Score (4 to 5 Scale)

4.4 , tasks such as Triage, Case Summary, Medication Inquiry, and Preoperative Edu-
cation are rated particularly highly. Doctors find medical LLM assistance in these
tasks especially valuable due to their repetitive nature (e.g., Case Summary, Preop-
erative Education), relatively low medical risk (e.g., Triage), and high information
demands (e.g., Medication inquiry). None of the tasks are proposed by more than five
respondents, reinforcing that the final set of 22 tasks is widely applicable and relevant
across the surveyed doctors. Among the participants, 46.5% report using LLMs to
assist with their clinical work. When asked about the limitations of current medical
LLM capabilities, respondents show strong consensus on several issues. Specifically,
42.2% of doctors identify problems with noncompliance to instructions, 48.5% report
instances of incorrect answers, and 39.4% express concerns about the LLM’s inabil-
ity to provide accurate references. Additionally, doctors emphasize the necessity of
continuously updating the LLM’s knowledge base and incorporating self-correction
mechanisms to improve the reliability and accuracy of the model’s outputs.

4.5 Task Comparison Between Typical Medical Datasets and
Our Defined Tasks

We further compare the tasks defined in our framework with those in typical medical
datasets, using KUAKE-QIC [18] as a representative example. While some overlap
exists between the datasets, our defined tasks introduce 17 additional tasks not covered
by KUAKE-QIC, highlighting the broader scope and versatility of our approach, as
illustrated in Figure 4.5 .

14


KUAKE-QIC Overlap Our Defined Tasks
Disease Medication Surgery Complications| | Medication
Precautions Advice Necessity Analysis Advise
Test Results
Interpretation

Medication
Inquiry

1

i

i

1

1

1

Emergency {
1

1

Health Guidance) !
i

1

1

i

1

1

1

1

i

i

Advice

Follow-up Plan
Inquiry Prompts
Symptom

Differential
Inquiry Diagnosis

Next Examinations {_ Triage}

Medical
Expenses

Preoperative
Education
Surgical Plan
Disease Grading

Etiological

Analysis reatment Plan

Case Summary

Consequences
Description

Disease
Inquiry

Initial Diagnosis

Definitive
Diagnosis

Efficacy

Task1 + Instruction1 Instruction1
Patient Description Patient Description
a SS
aa [| Original Context [i Original Context
Instruction1 _———— Question1 Doctor Written
TENez ~ 7 = Question2 > — Doctor Written
a @ Generate Instruction22 @ Manual Question2
ry ((omacnen] with Reference} Answer2 Revision
Instruction22 Z Original Context Gussions Dostor Written
Question3
@ Instruction © Otgna Answer Output Answer3
Alignment $$.
DoctorFLAN DotaBench

4.6 DoctorFLAN Construction

To create a comprehensive dataset covering the entire clinical workflow, we construct
single-turn DoctorFLAN based on the 22 predefined tasks. First, we collect raw med-
ical data from a variety of sources, then we heuristically filter and map the data to
the relevant tasks. The dataset is refined in two stages: instruction normalization and
response polishing. Following the initial construction, we conduct manual verification
of a subset of the data by medical experts to ensure its quality, as shown in Figure 4.6.

Data Source. We use three primary data sources: medical multiple-choice
questions (MCQs) (e.g., https://www.medtiku.com/), medical encyclopedia entries
(e.g., https://m.120ask.com/), and high-quality existing medical datasets such as
PromptCBLUE [28]. MCQs are chosen for their ability to simulate a broad range
of clinical scenarios, making them highly relevant to real-world practice. The med-
ical encyclopedia, which contains detailed information on topics such as drugs and
symptoms, provides a comprehensive and reliable reference, especially for tasks like
Medication Inquiry. Additionally, we include overlapping datasets from resources like
the Case Summary subset in PromptCBLUE.

Preprocessing and Task Mapping. After collecting the raw data, we perform
deduplication using Jaccard similarity (threshold = 0.8) to eliminate near-duplicate


entries and improve data quality [35]. We then categorize the data into the 22 prede-
fined task types using a carefully designed set of task-specific regular expressions. Each
task is associated with multiple regex patterns, which are iteratively refined based
on expert feedback. In each iteration, we sample 50 examples for manual annotation
by a senior physician to assess classification quality. The refinement process contin-
ues until the regex-based categorization achieves over 95% agreement with expert
labels, ensuring high precision and consistency. The description of the regex process
and an example for task classification are provided in Section A of the Supplementary
Information.

Instruction:

Facial pain: Provide a detailed overview of the symptoms, possible causes, and related
diseases. Include the definition of the symptoms, typical manifestations, common causes
that may cause the symptoms, and other health issues related to these symptoms.

Reference:

associated_ diseases: Infection in the infratemporal space, chronic maxillary sinusitis,
temporal arteritis, infection under the lower jaw, infection in the submandibular space,
osteomyelitis of the jaw, bone cyst, petrositis of the temporal bone, acute osteomyelitis
of the jaw, acute submandibular sialadenitis

overview: Facial pain refers to pain on one or both sides of the face, or at the forehead.
This pain can be dull or throbbing and can be extremely sharp.Facial pain or spasms,
generally not life-threatening, but the sufferer is very miserable; even if the pain

is bearable, the facial expression appears unnatural.

cause: Facial pain is usually caused by local tissue infection or inflammation; some are
throbbing pain, some are severe stabbing pain, and the presented symptoms vary. Muscle
inflammation is the most familiar cause of facial pain, also known as temporomandibular
joint disorder. Followed by trigeminal neuralgia, herpes zoster. It can also be seen in facial
pain caused by oral inflammation, and pain caused by local trauma.

Answer:

Facial pain is a relatively common clinical symptom, characterized by various forms and
manifestations. Patients may describe facial pain as dull pain, throbbing pain, burning
sensation, or extreme stabbing pain. When considering the causes of facial pain, it is
necessary to comprehensively consider the distribution, nature, triggering and alleviating
factors, and accompanying symptoms.

According to <textbook knowledge>, diseases associated with facial pain

include, but are not limited to, infection in the infratemporal space, chronic maxillary
sinusitis, temporal arteritis, infection under the lower jaw, infection in the submandibular
space, and osteomyelitis of the jaw.

Doctor Comment:
The answer lacks specific detail compared to the original reference answer.

Table 8: The content presented has been translated from Chinese: An example of a
response judged by doctors to be less practical compared to the original, illustrating a
potential challenge in ensuring the utility of LLM-generated content in medical data.

Reference-enhanced Refinement. Although we have gathered data for the
22 tasks, the initial dataset contains issues such as poorly worded instructions and
overly brief responses. To address these problems, we implement a two-step refinement
process: instruction alignment and response polishing. In the instruction alignment
phase, medical professionals are enlisted to manually draft task-specific instructions
for each of the 22 tasks, ensuring that the instructions accurately reflect real-world

16


clinical scenarios and align with the intended task. In the response polishing phase,
we ask GPT-4 to generate more comprehensive responses by referencing the original
data to enhance their quality. The final dataset contains 92350 samples, divided into a
training set DoctorFLAN-train and a test set DoctorFLAN-test. The test set includes
25 randomly sampled entries from each task, for a total of 550 samples.

To ensure that the responses generated by GPT-4 are factually accurate and real-
istic, we use a structured review process in which a sample of 1050 responses (50
samples per item across 22 tasks) are reviewed by three medical professionals, each
reviewing 350 items. The verification process is overseen by a senior expert with a high-
level title, who has dedicated 10 hours to ensure a thorough assessment. Each model
response is reviewed alongside its corresponding reference answer, and the review-
ers are instructed to revise or refine the outputs as needed based on that reference.
Rather than conducting blind, independent annotation, this process is designed as a
reference-grounded refinement task aimed at improving factual correctness and clini-
cal appropriateness. This approach balances thoroughness with practical limitations,
ensuring credible verification within the available resources. The verification criteria
include Correctness, where a response is considered correct if it contains no factual
errors, and Practicality, where a response is deemed practical if it is more effective
than the original answer. Our results demonstrate correctness (100%) and practicality
(99.9%), underscoring the robustness of the DoctorFLAN. In a detailed examination
of the data verification stage, we identify an instance where a doctor noted the lack of
practicality, commenting on the "lack of specific details," as shown in Table 8. Such
feedback suggests that the responses refined by GPT-4 can sometimes fall short in
complex practical medical contexts, highlighting an area for future improvement.

4.7 DotaBench Construction

Extending the single-turn dataset DoctorFLAN, we introduce multi-turn DotaBench
to evaluate multi-turn dialogues involving medical assistants. This extension is moti-
vated by the need to assess an LLM’s ability to operate in realistic clinical settings,
where conversations often span multiple turns and involve a sequence of logically con-
nected questions. While DoctorFLAN captures isolated queries, DotaBench focuses
on multi-turn interactions in which each question is designed to build upon the pre-
vious one, simulating the stepwise inquiry process commonly used by physicians in
real-world consultations.

Data Source. To ensure clinical authenticity, we select CMB-Clin [10] as the
source corpus. CMB-Clin is a multi-round question-answering dataset derived from
real medical records. However, its original format consists of 2-4 standalone Q&A pairs
that lack contextual continuity, making it unsuitable for dialogue-based evaluation in
its raw form.

Reference-enhanced Refinement. To address this limitation, we work with
licensed physicians to manually restructure the data into coherent three-turn dia-
logues. Specifically, we extract key clinical elements from each case, such as chief
complaints, physical findings, and diagnostic test results, and ask physicians to refor-
mulate them into contextually connected questions that reflect realistic consultation
workflows. The original answers from CMB-Clin are retained as reference responses,

17


DoctorFLAN DotaBench

Type Single-turn 3-turns
Split train test test
Specialist 27 27 -
Task 22 22 -
7##Q/task -* 25 -
#Q in total 91,776 550 7A

Table 9: The Statistics of DoctorFLAN and
DotaBench Dataset.

Gastroenterology, Pediatrics, Obstetrics & Gynecology, Respiratory,

Medicine Cardiology, Neurology, General Surgery Stomatology, Nephrology,
Hepatology, Orthopedics, Urology, Spine Surgery, Cardiothoracic Surgery,
OphthalmologyHematology, Endocrinology, Oncology, Emergency Medicine,
Infectious Disease, Traditional Chinese MedicineRheumatology & Immunology,
Neurosurgery, Dermatology, Otorhinolaryngology (ENT), Vascular Surgery,
Multidisciplinary

Specialist

Pre-Diagnosis: Triage

Diagnosis: Inquiry Prompts, Symptom Inquiry, Disease Inquiry, Initial Diagnosis,
Case Summary Differential Diagnosis, Next Examinations, Test Results Interpretation,
Definitive Diagnosis, Disease Grading

Treatment: Emergency Advice, Treatment Plan, Medication Inquiry,

Medication Advice, Complications Analysis Treatment Adjustment,

Surgery Necessity, Surgical Plan, Preoperative Education

Post-Treatment: Health Guidance, Follow-up Plan

Table 10: Specialists and Tasks in the DoctorFLAN Dataset.

Task

which are later used to support reference-based evaluation under the LLM-as-a-judge
framework. A representative example illustrating this transformation is included in
Supplementary Tables 1 and 2. Unlike DoctorFLAN, which directly involves LLMs
in data generation, DotaBench is crafted without LLM intervention, thereby elimi-
nating the need for subsequent data verification and ensuring controlled evaluation
conditions.

4.8 Data Statistic

The statistical analysis of the DoctorFLAN and DotaBench datasets is presented in
Table 9. The DoctorFLAN dataset comprises 92,326 instances across 22 distinct tasks,
involving 27 medical specialties in total as detailed in Table 10, demonstrating the
comprehensive coverage of DoctorFLAN in real clinical scenarios. In addition, we have
extracted a subset of 25 instances from each task, referred to as DoctorFLAN-test for
evaluation. The training and test sets are created via random split. The DotaBench
dataset includes 74 instances of 3-turn conversations.

18


4.9 Model Training

We fine-tune two open-source backbone models, Yi-6B and Baichuan2-7B-Base, using
a standard supervised fine-tuning (SFT) framework with an autoregressive, decoder-
only architecture. To ensure the model captures both domain-specific expertise and
general ability, we construct a mixed training corpus comprising 92k task-aligned
medical samples from DoctorFLAN, 101k general-purpose instruction samples from
datasets such as Evol-instruct [36], ShareGPT [37], and 51k additional medical QA
pairs from CMExam [27].

All models are trained on 4 NVIDIA A100 GPUs. We set the maximum input
sequence length to 4096 tokens and used a per-GPU batch size of 4, training for 3
epochs with a learning rate of 5 x 10~°. The optimization used the AdamW optimizer
with decoupled weight decay, and gradient checkpointing is enabled to reduce memory
consumption. Mixed precision training is performed using fp16 format to accelerate
computation.

The objective function is the negative log-likelihood (NLL) of the target response
given the prompt, encouraging the model to generate accurate and fluent outputs
aligned with medical task instructions. Specifically, the loss is defined as:

by
Lsrr = —S log P(y | &, yt) (1)
t=1
where x denotes the input prompt and y; the target token at time step t. The final
model checkpoint was selected after three training epochs based on manual review
and preliminary validation performance, without using early stopping or automated
selection heuristics.

4.10 Evaluation Models

To comprehensively evaluate the performance of medical-specific models trained on
various backbones and datasets, we assess a wide range of Chinese medical LLMs on
DoctorFLAN-test and DotaBench.

Among the domain-specific models, we include BianQue-2 [8], a medical
model fine-tuned from ChatGLM-6B [38] using patient-doctor dialogues; DISC-
MedLLM [32], a model based on the Baichuan-13B-Base architecture designed for
deep medical interactions; HuatuoGPT-7B [6], fine-tuned from Baichuan-7B for Chi-
nese medical consultation; and HuatuoGPT-II-7B [5], a state-of-the-art medical LLM
built on Baichuan2-7B with extensive medical knowledge.

We also evaluate general-purpose models to provide a performance baseline.
These include Qwen-1.8B-Chat [39], fine-tuned with supervised fine-tuning (SFT)
and reinforcement learning with human feedback (RLHF); Baichuan-13B-Chat [40],
which shares the same backbone as DISC-MedLLM and demonstrates strong general
performance; and Baichuan2 models including Baichuan2-7B-Chat and Baichuan2-
13B-Chat [41]. We further include Yi-6B-Chat and Yi-34B-Chat [42], which represent
two scales of models from the Yi series, comparable to Qwen and Baichuan.

To broaden the comparison, we additionally report results from proprietary models
such as GPT-3.5, GPT-4, and Claude-3.

19


All models are evaluated using the same decoding hyperparame-
ters: max_new_tokens = 1024, top_p = 0.7, temperature = 0.5, and
repetition_penalty = 1.1. We adopt Chain-of-Thought prompting, without using
any additional augmentation techniques.

4.11 Evaluation Method

Considering both accuracy, reliability, and cost, our evaluation methodology incorpo-
rates both automatic and human evaluations.

Automatic Evaluation. Our task involves open-ended answer generation in med-
ical contexts, where multiple correct and clinically valid responses may exist. In such
settings, traditional metrics such as BLEU and ROUGE, which rely on N-gram overlap
with reference answers, are often inadequate. These metrics fail to capture semantic
consistency when answers are phrased differently yet medically equivalent, and are
also highly sensitive to variations in response length. To address these limitations, we
employ GPT-4 (gpt-4-0125-preview) for automatic evaluation, a method shown to be
highly effective in previous research [43]. To ensure evaluation accuracy, we adopt
a reference-based model evaluation approach, where the LLM refers to the provided
reference and scores responses based on predefined criteria. These scoring standards
include: Accuracy (assessing the correctness and reliability of the information), Coher-
ence (evaluating the clarity and logical flow of the responses), Relevance (measuring
how closely each response addresses the prompt), and Thoroughness (judging the
depth and completeness of the response in covering the topic). During evaluation,
we apply Chain-of-Thought (CoT) prompting both in response generation and in the
LLM-as-a-judge scoring process. We do not use any external augmentation techniques,
such as retrieved rationales or tool-assisted reasoning. The evaluation is performed
using GPT-4, accessed via the official OpenAI API with default inference settings.
To support reproducibility, we provide the full evaluation prompt in Supplementary
Figures 1 and 2.

To balance accuracy and resource constraints, we conduct human evaluation on
a subset of models. For DoctorFLAN-test, which contains 550 questions in total,
we divide them into six roughly equal parts, with 91 or 92 questions per evaluator.
Each evaluator is assigned a set of questions and tasked with rating the responses
of all six models for each question, ensuring a fair and consistent evaluation across
all models. The evaluation team consists of six healthcare professionals with varying
levels of experience: three mid-level professionals with 5-6 years of experience, two
associate senior professionals with 12 years of experience, and one senior professional
with 26 years of experience. Evaluators are compensated based on their professional
seniority, with senior professionals receiving an hourly rate of 250 RMB, while mid-
level professionals are paid 165 RMB per hour. For DotaBench, we invite three doctors
to participate in the evaluation process, with each spending an average of 3 hours
reviewing the data.

20


Data Availability

DoctorFLAN and DotaBench datasets used in this study are available
at https: //huggingface.co/datasets/FreedomIntelligence /DoctorF LAN and
https: //huggingface.co/datasets /FreedomIntelligence/DotaBench, respectively.

Code Availability

The source code for DotaGPT training and evaluation is available at
https: //github.com/FreedomIntelligence/DotaGPT.

Acknowledgements

This work was supported by the Major Frontier Exploration Program
(Grant No. C10120250085) from the Shenzhen Medical Academy of Research
and Translation (SMART), the Shenzhen Science and ‘Technology Pro-
gram (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding
(RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural
Science Foundation of China (NSFC) (12326608), Shenzhen Science and Technol-
ogy Program (Shenzhen Key Laboratory Grant No. ZDSYS20230626091302006),
and Shenzhen Stability Science Program 2023, Shenzhen Key Lab of Multi-Modal
Cognitive Computing.

Author Contributions

W.X. led the task survey design, constructed the dataset, conducted the main exper-
iments, and drafted the initial manuscript. J.F. and B.W. proposed the original idea
and, together with A.G., P.T., and X.W. (Xiang Wan), made substantial contribu-
tions to manuscript revision. Q.X. led the expert survey and coordinated the human
evaluation. Y.Z. contributed to early-stage data construction. X.W. (Xidong Wang)
developed the DotaBench dataset. J.C. helped refine the data construction method-
ology, and K.J. supported data analysis. All authors reviewed and approved the final
manuscript.

Competing Interests

The authors declare no competing interests.

Reporting Checklist

This study does not involve clinical trials or systematic reviews. Therefore, reporting
checklists such as PRISMA or CONSORT are not applicable.

21


References

[1]

[2

[3

[4

[5

(6

(7

[8

[9

[10]

Singhal, K., Azizi, S., Tu, T., Mahdavi, $.5., Wei, J., Chung, H.W., Scales, N.,
Tanwani, A., Cole-Lewis, H., Pfohl, S., et al.: Large language models encode
clinical knowledge. Nature 620(7972), 172-180 (2023)

Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Amin, M., Hou, L.,
Clark, K., Pfohl, 5.R., Cole-Lewis, H., et al.: Toward expert-level medical question
answering with large language models. Nature Medicine, 1-8 (2025)

Li, Y., Li, Z., Zhang, K., Dan, R., Jiang, S., Zhang, Y.: Chatdoctor: A medical
chat model fine-tuned on a large language model meta-ai (llama) using medical
domain knowledge. Cureus 15(6), 40895 (2023)

Wang, G., Yang, G., Du, Z., Fan, L., Li, X.: Clinicalgpt: Large language models
finetuned with diverse medical data and comprehensive evaluation. arXiv preprint
ar Xiv:2306.09968 (2023)

Chen, J., Wang, X., Ji, K., Gao, A., Jiang, F., Chen, S., Zhang, H., Dingjie, S.,
Xie, W., Kong, C., et al.: Huatuogpt-ii, one-stage training for medical adaption
of lms. In: First Conference on Language Modeling

Zhang, H., Chen, J., Jiang, F., Yu, F., Chen, Z., Chen, G., Li, J., Wu, X., Zhiyi,
Z., Xiao, Q., et al.: Huatuogpt, towards taming language model to be a doctor.
In: Findings of the Association for Computational Linguistics: EMNLP 2023, pp.
10859-10885 (2023)

Wang, H., Liu, C., Xi, N., Qiang, Z., Zhao, S., Qin, B., Liu, T.: Huatuo: Tuning
llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975
(2023)

Chen, Y., Wang, Z., Xing, X., Xu, Z., Fang, K., Wang, J., Li, S., Wu, J., Liu,
Q., Xu, X., et al.: Bianque: Balancing the questioning and suggestion ability
of health llms with multi-turn health conversations polished by chatgpt. arXiv
preprint arXiv:2310.15896 (2023)

Pal, A., Umapathi, L.K., Sankarasubbu, M.: Med-halt: Medical domain halluci-
nation test for large language models. In: Proceedings of the 27th Conference on
Computational Natural Language Learning (CoNLL), pp. 314-334 (2023)

Wang, X., Chen, G., Dingjie, S., Zhiyi, Z., Chen, Z., Xiao, Q., Chen, J., Jiang,
F., Li, J., Wan, X., Wang, B., Li, H.: CMB: A comprehensive medical bench-
mark in Chinese. In: Duh, K., Gomez, H., Bethard, S. (eds.) Proceedings of
the 2024 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (Volume 1: Long
Papers), pp. 6184-6205. Association for Computational Linguistics, Mexico
City, Mexico (2024). https://doi.org/10.18653/v1/2024.naacl-long.343 .

22


[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

https: / /aclanthology.org/2024.naacl-long.343

Fan, Y., Jiang, F., Wang, B., Li, P., Li, H.: Quantifying self-diagnostic atomic
knowledge in chinese medical foundation model: A computational analysis. arXiv
e-prints, 2310 (2023)

Wu, C., Lin, Z., Fang, W., Huang, Y.: A medical diagnostic assistant based on
Ilm. In: China Health Information Processing Conference, pp. 135-147 (2023).
Springer

Chen, Z., Luo, L., Bie, Y., Chen, H.: Dia-llama: Towards large language model-
driven ct report generation. arXiv preprint arXiv:2403.16386 (2024)

Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du,
N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learn-
ers. In: International Conference on Learning Representations (2022).
https: / /openreview.net /forum?id=gEZrGCozdqR

Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik,
A., Ashok, A., Dhanasekaran, A.S., Arunkumar, A., Stap, D., Pathak, E.,
Karamanolakis, G., Lai, H., Purohit, I., Mondal, I., Anderson, J., Kuznia,
K., Doshi, K., Pal, K.K., Patel, M., Moradshahi, M., Parmar, M., Purohit,
M., Varshney, N., Kaza, P.R., Verma, P., Puri, R.S., Karia, R., Doshi, S.,
Sampat, $.K., Mishra, S., Reddy A, S., Patro, S., Dixit, T., Shen, X.: Super-
Naturallnstructions: Generalization via declarative instructions on 1600+ NLP
tasks. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing, pp.
5085-5109. Association for Computational Linguistics, Abu Dhabi, United Arab
Emirates (2022). https://doi.org/10.18653/v1/2022.emnlp-main.340 .
https: / /aclanthology.org/2022.emnlp-main.340/

Zeng, G., Yang, W., Ju, Z., Yang, Y., Wang, S., Zhang, R., Zhou, M., Zeng, J.,
Dong, X., Zhang, R., et al.: Meddialog: Large-scale medical dialogue datasets. In:
Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 9241-9250 (2020)

He, J., Fu, M., Tu, M.: Applying deep matching networks to chinese medical ques-
tion answering: A study and a dataset. BMC Medical Informatics and Decision
Making 19(2), 52 (2019) https://doi.org/10.1186/s12911-019-0761-8

Zhang, N., Chen, M., Bi, Z., Liang, X., Li, L., Shang, X., Yin, K., Tan, C., Xu, J.,
Huang, F., et al.: Cblue: A chinese biomedical language understanding evaluation

benchmark. In: Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 7888-7915 (2022)

Zhang, S., Zhang, X., Wang, H., Guo, L., Liu, $.: Multi-scale attentive interaction

23


[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

networks for chinese medical question answer selection. IEEE Access 6, 74061-—
74071 (2018)

Pal, A., Umapathi, L.K., Sankarasubbu, M.: Medmcqa: A large-scale multi-
subject multi-choice dataset for medical domain question answering. In: Confer-
ence on Health, Inference, and Learning, pp. 248-260 (2022). PMLR

Jin, Q., Dhingra, B., Liu, Z., Cohen, W., Lu, X.: Pubmedqa: A dataset for
biomedical research question answering. In: Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.
2567-2577 (2019)

Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., Szolovits, P.: What disease
does this patient have? a large-scale open domain question answering dataset
from medical exams. Applied Sciences 11(14), 6421 (2021)

Wang, X., Li, J., Chen, S., Zhu, Y., Wu, X., Zhang, Z., Xu, X., Chen, J., Fu,
J., Wan, X., et al.: Huatuo-26m, a large-scale chinese medical qa dataset. In:
Findings of the Association for Computational Linguistics: NAACL 2025, pp.
3828-3848 (2025)

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, $., Wu, Z., Zhuang, Y., Lin, Z.,
Li, Z., Li, D., Xing, E., et al.: Judging Ilm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing Systems 36 (2024)

Li, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y., Duan, N., Baldwin,
T.: CMMLU: Measuring massive multitask language understanding in Chinese.
In: Ku, L.-W., Martins, A., Srikumar, V. (eds.) Findings of the Association for
Computational Linguistics ACL 2024, pp. 11260-11285. Association for Compu-
tational Linguistics, Bangkok, Thailand and virtual meeting (2024). https://do
i.org/10.18653/v1/2024.findings-acl.671 . https: //aclanthology.org/2024.findings-
acl.671

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Stein-
hardt, J.: Measuring massive multitask language understanding. In: International
Conference on Learning Representations

Liu, J., Zhou, P., Hua, Y., Chong, D., Tian, Z., Liu, A., Wang, H., You, C., Guo,
Z., Zhu, L., et al.: Benchmarking large language models on cmexam-a comprehen-
sive chinese medical exam dataset. Advances in Neural Information Processing
Systems 36, 52430-52452 (2023)

Zhu, W., Wang, X., Zheng, H., Chen, M., Tang, B.: Promptcblue: A chi-
nese prompt tuning benchmark for the medical domain. arXiv preprint
arXiv:2310.14151 (2023)

24


[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

120ask: Online Health Consultation Platform. https://m.120ask.com/. Accessed:
2024-06-01

Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Pmc-llama: Further finetuning
llama on medical papers. arXiv preprint arXiv:2304.14454 (2023)

Han, T., Adams, L.C., Papaioannou, J.-M., Grundmann, P., Oberhauser,
T., Loser, A., Truhn, D., Bressem, K.K.: Medalpaca—an open-source collec-
tion of medical conversational ai models and training data. arXiv preprint
arXiv:2304.08247 (2023)

Bao, Z., Chen, W., Xiao, 8., Ren, K., Wu, J., Zhong, C., Peng, J., Huang, X., Wei,
Z.: Disc-medllm: Bridging general large language models and real-world medical
consultation. arXiv preprint arXiv:2308.14346 (2023)

Tian, Y., Gan, R., Song, Y., Zhang, J., Zhang, Y.: ChiMed-GPT: A Chinese
medical large language model with full training regime and better alignment to
human preferences. In: Ku, L.-W., Martins, A., Srikumar, V. (eds.) Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 7156-7173. Association for Computational Linguis-
tics, Bangkok, Thailand (2024). https: //doi.org/10.18653/v1/2024.acl-long.386
. https://aclanthology.org/2024.acl-long.386

Savova, G.K., Masanz, J.J., Ogren, P.V., Zheng, J., Sohn, S., Kipper-Schuler,
K.C., Chute, C.G.: Mayo clinical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and applications. Journal of the
American Medical Informatics Association 17(5), 507-513 (2010)

Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., Carlini,
N.: Deduplicating training data makes language models better. In: Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 8424-8445 (2022)

Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Jiang, D.:
Wizardlm: Empowering large language models to follow complex instructions.
arXiv preprint arXiv:2304.12244 (2023)

ShareGPT: A Community Sharing OpenAI ChatGPT Conversations. https://sh
aregpt.com/. Accessed: 2024-06-01

Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng,
W., Xia, X., et al.: Glm-130b: An open bilingual pre-trained model. In: The
Eleventh International Conference on Learning Representations

Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y..,
Huang, F., et al.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023)

25


[40] Baichuan Inc.: Baichuan-13B-Chat. https://huggingface.co/baichuan-inc/Baich
uan-13B-Chat. Accessed: 2024-06-01 (2023)

[41] Yang, A., Xiao, B., Wang, B., Zhang, B., Yin, C., Lv, C., Pan, D., Wang, D.,
Yan, D., Yang, F., et al.: Baichuan 2: Open large-scale language models. arXiv
preprint arXiv:2309.10305 (2023)

[42] Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J.,
Chen, J., Chang, J., et al.: Yi: Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652 (2024)

[43] Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., Zhu, C.: G-eval: Nlg evaluation using
gpt-4 with better human alignment. In: Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, pp. 2511-2522 (2023)

Figure Legends

Figure 1. Task Categories Finalized for LLMs in Medical Assistance, Organized by
Four Phases: Pre-Diagnosis, Diagnosis, Treatment, and Post-Treatment.

Figure 2.2. Correlations between human and automatic evaluations on DoctorFLAN-
test, illustrating task-level consistency.

Figure 4.4. Comparative assessment of task efficiency scores for each task according
to our survey.

Figure 4.5. Task overlap between our defined tasks and KUAKE-QIC, highlighting
the 17 unique tasks introduced in our framework.

Figure 4.6. Reference-Enhanced Refinement in DoctorFLAN and DotaBench.

A Regex-Based Data Categorization

Note: Since our QA corpus is in Chinese, the regex patterns are expressed using
Chinese characters where necessary. Each pattern is accompanied by an English
explanation for clarity.

To enable efficient and scalable preprocessing, we employ regular expression (regex)
rules to automatically filter and categorize medical QA samples. The overall pipeline
consists of two stages: (1) general data cleaning and (2) task-specific classification
using field-wise regex matching.

General Filtering.

Before task categorization, we applied two regex-based filters to the entire dataset:

® Case-based QA Selection: We retained only questions that involve real-world
case descriptions, identified by matching sentence openings such as “fe”, “3”,
oy, ore JD or “ ast 27,

e Exclusion of Image/Table-Based Questions: We removed samples referencing
visual content using the following regex pattern:

26


Regex Rule for Filtering Text-Based Content

(RUF | img | Aa | Al [o-9]+| 4 [0-9]+?

Example: Category Classification via Multi-Field Regex Matching.

Each QA item was decomposed into five semantic fields to support modular rule
design:

Case description

Question content

Metadata (e.g., subject area such as “f¢ 423422” / infectious diseases)
Answer text

Answer Option Set

To classify data into specific clinical categories, we applied regex rules over
multiple fields in combination, instead of relying on any single field alone.

For instance, to extract QA samples related to Differential Diagnosis task, we
used the following logic:

e Field 2 (Question Content): We matched the question text against a regex that
detects diagnostic comparison terms (e.g., “2 I|”, “iFlit”, “XFll”), while filtering
out questions focused on treatment, symptoms, or procedures. The regex used was:

Regex on Question Content

27 (71 (2 2 BA BP A Be FP A HE | FL AP a oP BT
real | OH ASTER) ) (P=. (7: SE LST | Xl) ) . #8?

e Field 5 (Answer Option Set): We required that the question includes more
than one answer option, as differential diagnosis questions often present multiple
candidate conditions for selection.

Only when both conditions were satisfied, namely that relevant keywords appeared in
the question and that multiple answer options were present, did we label the sample
as belonging to the Differential Diagnosis category.

This combination-based rule design ensures higher precision and flexibility, and
can be extended to other categories by customizing field-specific regex patterns.

B Dotabench Construction Details

Supplementary Tables 1 and 2 present an example of how we convert a raw case from
CMB-Clin into a contextually linked multi-turn consultant sample in DotaBench.

C Evaluation Prompt

Supplementary Figures 1 and 2 show the complete prompt templates used for
DoctorFLAN and DotaBench, respectively.

20


Prompt | Quest

ion

{BZ BIT

Turn 1 (Transla

ago af

Ie fL40 27 A He A E87 NS
ADRGERE, BOAITAR
ed) This 49-year-o
er a bowel movement,
past medical history.

BMRA ES HA NA, th LAT DM fh BF I — PLR
RP SIBLE SE » (KHER DORE ee, “ath MIA SITS?

d male patient developed right lower abdominal pain three hours

and he can palpate a lump in the same area. He has no notable

.Based on this information, could you provide a preliminary diagnosis?

Eran

HET TARE,

RMB AYG37.8°C, D1LOUK / oy, MFI227%K/S>, ILE 100/60mmHg -

FAM
wala

SS

fia

Aire eae Da

ra Se

Br

ERA 4emxdem A), FFARR. AOE, CERRBOAM EAT -

HAE SUT?

Torn. 2 (Translated) I just performed a physical examination. The patient has a temperature of 37.8°C,

a hear

and po

A round mass approxima‘

rate of 101 bpm, respiratory rate of 22 breaths/min, and blood pressure of 100/60 mmHg.
‘ely 4 cm x 4 cm is palpable in the right inguinal region, with tenderness

orly defined borders. It is located medial to the inguinal ligament. What additional diagnostic

tests would you recommend to confirm the diagnosis?

Dew aeEiON
4~5cm

Turn 3

RRS To MLAS ALAS AAR B05.0x109/L, ARH 78%6 ° RISWLIER © BER

AeA

R-B RAMA SER, BEE, eK, EST. Key

re ASX

RMR ACE ME, RMIT, FHERLAIT TAR -
(Translated) The test results are now available. The complete blood count shows a white blood
cell count of 5.0x10/L with
a multilayered mixed-echo region along the longitudinal section of the inguinal area, with

uneven width and distal enlargement, measuring approximately 4-5 cm with well-defined borders.
Abdominal X-ray shows a step-ladder pattern of air-fluid levels. Based on this information,

could you confirm the diagnosis and recommend a treatment plan?

78% neutrophils. Urinalysis is normal. Doppler ultrasound reveals

Supplementary Table 1: A manually constructed DotaBench example consisting
of three contextually linked turns that reflect realistic consultation workflows.

Case Description

TSE (1) aE

ine]

48

ER, BEPME -
LOLIK sh, MFIK227%K/5}, BP 100/60mmHg, EK, RA So, APR PAR,

WA, FH, 4997, 3/NRRA RS HOU SR, A PART fi
(2) SEU A PBF TR RT. PARAS AY: = T 37.8°C,

| |

Fa UE ATT BEIR, Adem dem kv), AEGAN ACE, ELAR ATF ERS

=o

9
i

onan FREER: (1) SPS EE IMAL: WBC 5.0x109/L, N 78%- RRM
EAS ) SERRE HERAT -ZSeEDAMECE RK, MERE, ihn
WX, we. KA4~5em> (3) ARABX BRE EAT LY

ERI ACE -

(Translated) Present Illness History: (1) Summary: A 49-year-old male developed right lower
abdominal pain three hours ago after defecation, with a palpable mass in the same area.

No significant medical history. (2) Chief Complaint: Right lower abdominal pain and a
self-palpated mass for 3 hours. Physical Examination: Temperature: 37.8°C, Pulse: 101 bpm,
Respiration: 22/min, Blood Pressure: 100/60 mmHg. Abdomen soft, no visible peristaltic

waves, liver and s

defined borders is pa!
Auxiliary Tests: (1) Laboratory Tests: CBC: WBC 5.0x10/L, Neutrophils 78% Urinalysis:

Normal (2) Dopp

er

section with variable

(3) Abdominal X-ray:

pleen not palpable. A round mass ( 4x4 cm) with tenderness and poorly

pable in the right inguinal region, medial to the inguinal ligament.

Itrasound: Multilayered mixed-echo area along the inguinal longitudinal
width and distal enlargement; well-defined borders; 4-5 cm in length
Step-ladder air-fluid levels observed

Question 1

fA AE

Ba

DUT ARTE °

(Translated) Summarize the diagnosis and diagnostic rationale.

Question 2

FARA AIH

Zi

bly

(Translated) Summarize the differential diagnosis.

Question 3

fA ARTA TT EIU ©

(Translated) Summarize the treatment principles.

Supplementary Table 2: The original CMB-Clin case record used as the source for
DotaBench construction. Note that the original QA pairs are isolated and lack multi-

turn context.

28


Evaluation Prompt for DoctorFLAN-test

System Prompt:

Please act as an impartial judge and evaluate the quality of the response provided by an AI
assistant to the user question displayed below.

Requirements: Your assessment should focus primarily on the consistency between the assistant’s
answer and the reference answer.

Begin your evaluation by providing a short explanation. Be as objective as possible. After provid-
ing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this
format: "[[Rating |]", for example: "Rating: [[5]]".

Prompt:

[Question ]

{question}

[The Start of Reference Answer]
{reference}

[The End of Reference Answer]
[The Start of Assistant’s Answer]
{answer}

[The End of Assistant’s Answer |]

Supplementary Figure 1: Evaluation Prompt for DoctorFLAN-test.

29


Evaluation Prompt for DotaBench

System Prompt:

Please act as an impartial judge and evaluate the quality of the response provided by an AI
assistant to the user question displayed below.

Requirements: Your assessment should focus on the overall quality of the responses based on the
following criteria:

Accuracy: Evaluate the correctness and reliability of the information provided. Coherence: Assess
the clarity and logical flow of the responses. Relevance: Determine how closely each response
addresses the question asked. Thoroughness: Judge the depth and completeness of the response
in covering the topic.

You will be given the assistant’s answer and some references. The reference consists of Q&A pairs
related to the patient, which are completely accurate and can be used as a reliable source of truth.
You evaluation should focus on the assistant’s answer to the first question. Begin your evaluation
by providing a short explanation. Be as objective as possible. After providing your explanation,

you must rate the response on a scale of 1 to 10 by strictly following this format: "[[Rating ]]",

for example: "Rating: [[5]]".
Prompt:

<|The Start of Reference|>

{reference}

The End of Reference|>

The Start of Assistant A’s Conversation with User|>

HHH User:

{question_1}

H#+ Assistant A:

{answer_ 1}

<|The End of Assistant A’s Conversation with User|>

Supplementary Figure 2: Evaluation Prompt for DotaBench

30
