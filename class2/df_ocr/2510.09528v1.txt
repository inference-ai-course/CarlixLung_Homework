arX1iv:2510.09528v1 [cs.CL] 10 Oct 2025

ACCENT-INVARIANT AUTOMATIC SPEECH RECOGNITION VIA SALIENCY-DRIVEN
SPECTROGRAM MASKING

Mohammad Hossein Sameti', Sepehr Harfi Moridani', Ali Zarean”, Hossein Sameti'

‘Department of Computer Engineering, Sharif University of Technology
2Department of Computer Engineering, University of Tehran

ABSTRACT

Pre-trained transformer-based models have significantly ad-
vanced automatic speech recognition (ASR), yet they re-
main sensitive to accent and dialectal variations, resulting
in elevated word error rates (WER) in linguistically diverse
languages such as English and Persian. To address this chal-
lenge, we propose an accent-invariant ASR framework that
integrates accent and dialect classification into the recogni-
tion pipeline. Our approach involves training a spectrogram-
based classifier to capture accent-specific cues, masking the
regions most influential to its predictions, and using the
masked spectrograms for data augmentation. This enhances
the robustness of ASR models against accent variability. We
evaluate the method using both English and Persian speech.
For Persian, we introduce a newly collected dataset spanning
multiple regional accents, establishing the first systematic
benchmark for accent variation in Persian ASR that fills a
critical gap in multilingual speech research and provides a
foundation for future studies on low-resource, linguistically
diverse languages. Experimental results with the Whisper
model demonstrate that our masking and augmentation strat-
egy yields substantial WER reductions in both English and
Persian settings, confirming the effectiveness of the approach.
This research advances the development of multilingual ASR
systems that are resilient to accent and dialect diversity. Code

and dataset are publicly available at: |https://github.com/MH-
Sameti/Accent_invariant_ASR

Index Terms— Automatic Speech Recognition, Accent
Invariant, Data Augmentation, Persian accents

1. INTRODUCTION

Automatic Speech Recognition (ASR) systems have evolved
from providing transcription services for virtual assistants
to enabling sophisticated healthcare applications [1]. This
development demonstrates the critical role of ASR systems
in enhancing accessibility and efficiency across various do-
mains. Recent advancements in transformer-based models,
such as the Whisper family, have significantly improved
ASR performance by leveraging deep learning techniques
to capture complex speech patterns (2). These models have

shown remarkable performance in transcribing spoken lan-
guage across diverse contexts, including noisy environments
and spontaneous conversations (3). However, despite their
effectiveness, they indicate notable sensitivity to accent and
dialect variations, particularly in linguistically diverse lan-
guages like English and Persian (4). This sensitivity often
results in high Word Error Rates (WER) when processing
speech from speakers with non-native or regional accents,
thereby limiting the accessibility and effectiveness of ASR
technologies in global applications (5).

Accents encapsulate unique phonetic and prosodic fea-
tures that can obscure the underlying linguistic content, pos-
ing a substantial challenge for ASR systems trained mostly on
standard or homogeneous datasets. These variations can lead
to misinterpretations of phonemes and intonations, which
are crucial for accurate speech recognition. Traditional ap-
proaches to mitigating accent-related discrepancies involve
augmenting training datasets with diverse speech samples or
fine-tuning models on accent-specific data (5\(6). While these
methods can improve performance, they often demand exten-
sive data collection and may not generalize well to unseen
accents or dialects, making them resource-intensive and less
scalable.

Our main contributions are as follows:

* We propose a Saliency-driven spectrogram masking
framework that leverages Grad-CAM to identify accent-
sensitive regions and suppress them, enabling ASR models
to focus on accent-neutral linguistic features.

We design a lightweight, model-agnostic training strat-
egy that improves robustness to both known and unseen ac-
cents without requiring architectural modifications or full
model retraining.

We introduce the Persian Dialect IDentification (PDID), a
new multi-accent corpus covering 10 regional Persian ac-
cents, providing the first systematic benchmark for Persian
accent robustness.

We conduct extensive experiments on English (LibriSpeech,
EdAcc, CommonAccent) and Persian (CommonVoice-
fa, PDID), showing that our method consistently reduces


WER/CER over SpecAugment baselines on accented speech

2. RELATED WORK

Recent advances in transformer-based ASR models such as
Whisper have significantly improved speech recognition
across noisy and spontaneous conditions. However, these
models still exhibit notable sensitivity to accent and dialectal
variations, with disproportionately high WER for non-native
and regional speakers (4).

More recently, large language model (LLM)-based ap-
proaches have been integrated into ASR pipelines to enhance
robustness under accented and conversational speech {11/12}.
While these methods leverage powerful contextual reasoning
to improve recognition, they drastically increase computa-
tional and memory costs, making them impractical for real-
time or resource-constrained deployment. Moreover, their ef-
fectiveness diminishes in low-resource languages where train-
ing data and linguistic coverage are limited, reducing their
utility for accent-heavy domains such as Persian.

A growing body of work focuses on enhancing accent
robustness. Parameter-efficient adaptation methods like Mix-
ture of Accent-Specific LoRAs (MAS-LoRA) deploy
accent-specialized LoRA experts, achieving improvements
on accented corpora without full model retraining. Com-
plementary to this, Qifusion-Net introduces a layer-
adapted fusion strategy that dynamically integrates multi-
accent acoustic features, reducing CER by over 20% on
large-scale benchmarks.

Beyond architecture, spectrogram manipulation and aug-
mentation strategies remain underexplored for accent mitiga-
tion. While supervised contrastive learning has been applied
to accented speech (i5}, direct masking of accent-related
spectrogram regions has yet to be widely investigated—a gap
our work explicitly targets to inject gradient information to
the pipeline.

3. METHODOLOGY

This section details the proposed methodology for enhanc-
ing accent invariance in ASR systems. Our approach inte-
grates accent and dialect classification into the ASR training
pipeline through a multi-step process involving spectrogram-
based classification, Grad-CAM for the localization of accent
features, spectrogram masking, and fine-tuning a pre-trained
ASR model on augmented data. The following subsections
elaborate on each component of the method.

3.1. PDID Dataset

We collected speech samples from 10 regional Persian ac-
cents (Isfahani, Yazdi, Lori, Kurdish, Balochi, Southern,
Northern, Tajiki, Mashhadi, and Shirazi) using sources

GradCAM Activation Maps
+

| 1 || | |
|
Masking Policy
ASR Model

Fig. 1. Overview of our accent suppression pipeline. First,
the spectrogram is used to classify accents and generate a
Grad-CAM saliency map highlighting accent-specific fea-
tures. Next, a masking strategy is applied to suppress these
accent-related regions while preserving essential information.
Finally, the modified spectrogram is fed into the ASR model
to improve generalization across diverse accents.

such as local TV/radio and online platforms like Aparat
and YouTube. Following a pipeline similar to the EMILIA
dataset [16], we applied preprocessing steps including voice
activity detection, speaker diarization, silence-based seg-
mentation, and speech—music separation. All samples were
standardized to 16kHz, mono-channel, 16-bit WAV format
with normalized loudness, and segmented into 3—30 second
clips. After quality filtering, about 23 hours of clean accent-
labeled data remained from 200+ hours of raw speech, with
Tajiki, Shirazi, and Balochi accents included only in the test
set for robustness evaluation. Table [I] shows the distribution
of samples and hours across the training accents.

3.2. Accent Classification on Spectrograms

To effectively identify accent-specific features within speech
data, we first train an accent classifier using spectrogram rep-
resentations of the input audio. Spectrograms provide a com-
prehensive visualization of the frequency content of speech
signals over time, capturing both phonetic and prosodic char-
acteristics essential for distinguishing accents.

We utilize a diverse dataset comprising speech samples
from various accents and dialects of English. The resulting
spectrograms are normalized to ensure consistent input scales


Table 1. Distribution of samples and hours across Persian
accents in our dataset

Accent Samples Hours
Isfahani 996 ~2.2h
Yazdi 1114 ~2.4h
Shomali 7632 ~10.9h
Jonubi 147 ~1.1h
Lori 2220 ~4,0h
Kurdish 125 ~1.0h
Mashhadi 379 ~1.5h
Train 12613) -~23.0h

Table 2. Number of Samples per Class in the Dataset

Class Number of Samples
Standard 1000
Southern British 965

Irish 704

Italian 443
Egyptian 346
Vietnamese 332

Total 3790

for the classifier. Our accent classification dataset includes
samples from the Edinburgh dataset for Southern British,
Irish, Egyptian, and Italian and the LibriSpeech dataset for
Standard English (7\(8}. Table[2|contains the exact number of
samples per accent.

For accent classification, we utilize a convolutional neu-
ral network (CNN) architecture consisting of multiple convo-
lutional layers with ReLU activations and max-pooling lay-
ers to capture hierarchical acoustic features. More specifi-
cally, the architecture inputs normalized spectrograms of size
80 x 3000, where 80 is the number of frequency bins and
3000 is the number of time frames. Furthermore, four convo-
lutional layers with 32, 64, 128, and 256 filters of size 3 x 3,
each followed by ReLU activation. Max-pooling layers with a
kernel size of 2 x 2 are applied after certain layers and dropout
layers to prevent overfitting. Finally, a flattening layer is fol-
lowed by a fully connected layer with 128 neurons and ReLU
activation, including dropout for regularization, and a fully
connected layer maps to the number of accent classes in the
dataset.

The classifier is trained using the cross-entropy loss func-
tion and optimized with the Adam optimizer. Data augmen-
tation techniques such as SpecAugment are applied during
training to enhance the classifier’s robustness to variability
in speech signals (17). The final accuracy of the classifier
model is 74.6% for English accents. When applied with the
same settings to our Persian accented dataset, the classifier
achieved accuracy of 95%,

3.3. Masking Strategy

To identify the regions in the spectrograms most indicative
of accent-specific features, Gradient-weighted Class Activa-
tion Mapping (Grad-CAM) is utilized (18), which provides a
visual explanation by highlighting the areas of the input that
significantly influence the classifier’s decision.

Specifically, for each input spectrogram, the gradients of
the predicted accent class are computed concerning the fea-
ture maps of the last convolutional layer. These gradients are
then global-average-pooled to obtain weights, combined with
the corresponding feature maps to produce a heatmap high-
lighting the salient regions associated with the accent classi-
fication.

A probabilistic masking strategy based on the normalized
Grad-CAM scores is applied to suppress accent-specific fea-
tures in the spectrograms. After normalizing the Grad-CAM
activation map to obtain scores in the range [0, 1], denoted
as C(i, 7) for pixel (i, 7), a binary threshold mask T(z, 7) is

defined as:
— 1, if C(i,j) > 0.3
T(t, 9) “| ( a
0, otherwise.

(1)

Next, random probability map R(i, 7) is generated, where
each R(i,7) is sampled from a uniform distribution over
(0, 1]. Furthermore, U(A, B) declares sampling from a ran-
dom uniform distribution over [A, B]. The final mask M(¢, 7)
is computed as:

if T(i,7) =0

if C(i,j) > 0.7 and R(i,7) > 1

if 0.5 < C(t, 7) < 0.7 and R(i, 7) > U(0.7, 0.9)
, if C(i,7) < 0.5 and R(i, 7) > U(0, 0.05)

, otherwise.

OF FP re

(2)
In this strategy:

* If a pixel belongs to the region where T(i,7) = 0, it
always remains unchanged.

If a pixel is located in a region that is considered
strongly accent-related (C(t, 7) > 0.7) all such pixels
are masked.

If a pixel belongs to the region with a moderate to high
score (0.5 < C(i,7) < 0.7), itis masked with a prob-
ability between 0.7 and 0.9 using a uniform probabil-
ity distribution (U(0.7,0.9)), This ensures that nonrel-
evant pixels have a chance to be included in accent-
related features, thereby reducing errors to some extent.

If a pixel falls within the low to moderate score
(C(i,7) < 0.5), it is masked with a probability be-
tween U(0, 0.05), to account for accent-related regions
that might have been mistakenly assigned a low score,
thus mitigating errors to some extent.


Table 3. WER/CER results for English datasets (LibriSpeech, EdAcc, Unseen accents, and CommonAccent). WsPr-_t: Whis-
per_tiny, WsPrLS_t: WhisperLS tiny, WsPrSAug_t: SpecAugment baseline, ARWsPr_t: ours

Model LS Accented Unseen CMA

WsPr-t 8.0/3.2 | 42.0/37.7 | 34.7/ 26.7 | 62.2/38.5

WsPrLS -t I7 7.0/2.7 | 26.1/16.0 | 29.3/19.4 | 36.3/18.5

WsPrSAug_t 7.3/2.9 | 27.0/17.8 | 30.1/ 20.3 | 38.3/21.6

ARWspPr.-t (ours 6.8/2.7 | 23.4/15.1 | 26.7/17.9 | 34.8/18.2

Table 4.

WER/CER results for Persian datasets (CommonVoice-fa and regional accents).

WsPr_b: Whisper_base,

WsPrCV_b/m: Whisper fine-tuned on CommonVoice (base/medium), SpcAug: SpecAugment, ARWsPr: ours , ARWsPr++:

ours with GradCam++

Model Standard Accented

WsPr_b 186.4/ 209.4 | 128.6/93.6
WsPrCV_b 62.2/25.4 | 97.2/61.7
WsPrSpcAug_b }17) 61.5 / 23.9 92.8/ 51.6
ARWsPr++.b [19 62.4/22.1 | 90.3/41.5
ARWSspPr_b (ours 61.9/ 21.1 88.8 / 40.6
WsPr_m 68.3/32.1 | 129.5/89.1
WsPrCV_m [10 30.7 / 8.9 70.4 / 42.5
ARWSsPr_m (ours) 31.1/9.9 67.5 / 36.5

The masked spectrogram is then generated by element-
wise multiplication of the original spectrogram with the mask
M(i, j):

Masked Spectrogram(i, 7) = Spectrogram(2, 7) x M(i,7). (3)

This probabilistic masking strategy ensures that accent-
related features are suppressed while retaining essential lin-
guistic information, enhancing the ASR model’s ability to
generalize across different accents. As shown in Figure
the original spectrogram, Grad-CAM activation map, and
masked spectrogram illustrate the accent feature localization
and suppression process.

The masked spectrograms are combined with the primary
dataset to form an augmented training dataset. This augmen-
tation encourages the ASR model to learn accent-neutral rep-
resentations by exposing it to accented and accent-suppressed
versions of the same speech samples. Leveraging this dataset,
we fine-tune a state-of-the-art transformer-based ASR model
to improve its robustness in accent and dialect variations.

4. EXPERIMENTS AND RESULTS

We conducted experiments on both English and Persian
datasets to evaluate the effectiveness of our proposed accent-
aware masking method. As Table [3] shows For English, we
used LibriSpeech, EdAcc, and CommonAccent, while Table
shows for Persian we used the CommonVoice (fa) sub-
set along with our newly collected accented dataset PDID.
Training was performed on NVIDIA RTX 3090 GPUs using
AdamW optimizer, with learning rates of 1 x 107° for tiny
and 3 x 10~®© for base/medium models, batch sizes of 32,

16, and 4 respectively, and 10 epochs. The evaluation met-
rics were WER and CER, complemented by ablations using
Grad-CAM and Grad-CAM++ to generate accent-masking
policies. Results show that our method significantly outper-
forms both pre-trained Whisper and LibriSpeech fine-tuned
baselines, as well as a SpecAugment baseline, particularly
in accented and unseen-accent settings (19). For Persian,
fine-tuning on Common Voice (fa) improves performance, but
our accent-masked approach yields further gains across both
base and medium sizes.

Overall, these results confirm that accent-masked train-
ing consistently reduces CER and WER across both English
and Persian. The improvements are particularly strong on un-
seen accents, highlighting the robustness and generalizability
of the proposed method.

5. CONCLUSION

We proposed a saliency-driven spectrogram masking frame-
work that uses Grad-CAM to suppress accent-specific fea-
tures and encourage ASR models to learn accent-neutral rep-
resentations. Our approach is lightweight, model-agnostic,
and improves robustness without architectural modifications
or full retraining. In addition, we introduced the PDID
dataset, the first multi-accent benchmark for Persian ASR
covering 10 regional dialects. Experiments on English and
Persian showed consistent WER/CER reductions, with rel-
ative gains up to 14% on accented speech compared to
SpecAugment baselines. These results confirm that targeted
spectrogram masking is an effective strategy for accent-robust
ASR.


6. REFERENCES

[1] Matthew Perez, Duc Le, Amrit Romana, Elise Jones,

“4

“4

ay

“4

=

a

“4

—

Keli Licata, and Emily Mower Provost, “Seq2seq for au-
tomatic paraphasia detection in aphasic speech,” arXiv
preprint arXiv:2312.10518, 2023.

Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-
man, Christine McLeavey, and Ilya Sutskever, “Robust
speech recognition via large-scale weak supervision,” in
International conference on machine learning. PMLR,

2023, pp. 28492-28518.

Yingzhi Wang, Abdelmoumene Boumadane, and Ab-
delwahab Heba, “A fine-tuned wav2vec 2.0/hubert
benchmark for speech emotion recognition, speaker ver-
ification and spoken language understanding,’ arXiv
preprint arXiv:2111,02735, 2021.

Allison Koenecke, Andrew Nam, Emily Lake, Joe
Nudell, Minnie Quartey, Zion Mengesha, Connor
Toups, John Rickford, Dan Jurafsky, and Sharad Goel,
“Racial disparities in automated speech recognition,”
Proceedings of the National Academy of Sciences, vol.
117, pp. 201915768, 03 2020.

Abhinav Jain, Minali Upreti, and Preethi Jyothi, “Im-
proved accented speech recognition using accent em-
beddings and multi-task learning.”’ in Interspeech,
2018, pp. 2454-2458.

Xian Shi, Fan Yu, Yizhou Lu, Daliang Liang, Yanmin
Qian, and Lei Xie, “The accented english speech recog-
nition challenge 2020: Open datasets, tracks, baselines,
results and methods,” in ICASSP 202] - 2021 IEEE In-
ternational Conference on Acoustics, Speech and Signal

Processing (ICASSP), 2021, pp. 6918-6922.

Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur, “Librispeech: An asr corpus based
on public domain audio books,” in 2015 IEEE Inter-
national Conference on Acoustics, Speech and Signal

Processing (ICASSP), 2015, pp. 5206-5210.

Ramon Sanabria, Nikolay Bogoychev, Nina Markl, An-
drea Carmantini, Ondrej Klejch, and Peter Bell, “The
edinburgh international accents of english corpus: To-
wards the democratization of english asr;’ in ICASSP
2023-2023 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE,
2023, pp. 1-5.

Juan Zuluaga-Gomez, Sara Ahmed, Danielius Vi-
sockas, and Cem Subakan, “Commonaccent: Explor-
ing large acoustic pretrained models for accent clas-
sification based on common voice,’ arXiv preprint
arXiv:2305.18283, 2023.

[10] R. Ardila, M. Branson, K. Davis, L. Henretty, F. M.

[11

[12

[13

[14

[17

sy

“4

“4

sy

a

“4

—“

Tyers, and G. Weber, “Common voice: A massively-
multilingual speech corpus,’ in Proceedings of the
12th Conference on Language Resources and Evalua-
tion (LREC 2020), 2020, pp. 4211-4215.

Bingshen Mu, Xucheng Wan, Naijun Zheng, Huan
Zhou, and Lei Xie, “Mmger: Multi-modal and multi-
granularity generative error correction with Ilm for joint
accent and speech recognition,” IEEE Signal Processing
Letters, 2024.

Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian
Kang, Li Jie, Zhennan Lin, Yongxiang Li, and Xie Lei,
“Leveraging Ilm and self-supervised training models for
speech recognition in chinese dialects: A comparative
analysis,” arXiv preprint arXiv:2505.21138, 2025.

Raphaél Bagat, Irina [lina, and Emmanuel Vincent,
“Mixture of lora experts for low-resourced multi-
accent automatic speech recognition,’ arXiv preprint
arXiv:2505.20006, 2025.

Jinming Chen, Jingyi Fang, Yuanzhong Zheng, Yaoxuan
Wang, and Haojun Fei, “Qifusion-net: Layer-adapted
stream/non-stream model for end-to-end multi-accent
speech recognition,” arXiv preprint arXiv:2407.03026,
2024.

Tao Han, Hantao Huang, Ziang Yang, and Wei Han,
“Supervised contrastive learning for accented speech
recognition,’ ArXiv, vol. abs/2107.00921, 2021.

Haorui He, Zenggiang Shang, Chaoren Wang, Xuyuan
Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi
Li, Peiyang Shi, et al., “Emilia: An extensive, multilin-
gual, and diverse speech dataset for large-scale speech
generation,’ in 2024 IEEE Spoken Language Technol-
ogy Workshop (SLT). IEEE, 2024, pp. 885-890.

Daniel S Park, William Chan, Yu Zhang, Ekin D Chiu,
and Quoc V Le, “Specaugment: A simple data augmen-
tation method for automatic speech recognition,” arXiv
preprint arXiv: 1904.08779, 2019.

Ramprasaath R. Selvaraju, Michael Cogswell, Devi
Das, and Dhruv Batra, “Grad-cam: Visual explanations
from deep networks via gradient-based localization,” in
2017 IEEE International Conference on Computer Vi-
sion (ICCV), 2017, pp. 618-626.

Aditya Chattopadhay, Anirban Sarkar, Prantik
Howlader, and Vineeth N Balasubramanian, “Grad-
cam++: Generalized gradient-based visual explanations
for deep convolutional networks,’ in 20/8 [EEE
Winter Conference on Applications of Computer Vision

(WACY). Mar. 2018, IEEE.
