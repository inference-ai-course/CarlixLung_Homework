The NLP Cookbook: Modern Recipes for
Transformer based Deep Learning Architectures

SUSHANT SINGH’, AND AUSIF MAHMOOD?

Department of Computer Science & Engineering, University of Bridgeport, Connecticut, CT 06604, USA

Corresponding author: Sushant Singh (sushants@my.bridgeport.edu)

ABSTRACT In recent years, Natural Language Processing (NLP) models have achieved phenomenal success in linguistic
and semantic tasks like text classification, machine translation, cognitive dialogue systems, information retrieval via Natural
Language Understanding (NLU), and Natural Language Generation (NLG). This feat is primarily attributed due to the seminal
Transformer architecture, leading to designs such as BERT, GPT (L, IL, III), etc. Although these large-size models have
achieved unprecedented performances, they come at high computational costs. Consequently, some of the recent NLP
architectures have utilized concepts of transfer learning, pruning, quantization, and knowledge distillation to achieve moderate
model sizes while keeping nearly similar performances as achieved by their predecessors. Additionally, to mitigate the data
size challenge raised by language models from a knowledge extraction perspective, Knowledge Retrievers have been built to
extricate explicit data documents from a large corpus of databases with greater efficiency and accuracy. Recent research has
also focused on superior inference by providing efficient attention to longer input sequences. In this paper, we summarize and
examine the current state-of-the-art (SOTA) NLP models that have been employed for numerous NLP tasks for optimal
performance and efficiency. We provide a detailed understanding and functioning of the different architectures, a taxonomy
of NLP designs, comparative evaluations, and future directions in NLP.

INDEX TERMS Deep Learning, Natural Language Processing (NLP), Natural Language Understanding (NLU), Natural

Language Generation (NLG), Information Retrieval (IR), Knowledge Distillation (KD), Pruning, Quantization

|. INTRODUCTION

Natural Language Processing (NLP) is a field of Machine
Learning dealing with linguistics that builds and develops
Language Models. Language Modeling (LM) determines the
likelihood of word sequences occurring in a sentence via
probabilistic and statistical techniques. Since human
languages involve sequences of words, the initial language
models were based on Recurrent Neural Networks (RNNs).
Because RNNs can lead to vanishing and exploding
gradients for long sequences, improved recurrent networks
like LSTMs and GRUs were utilized for improved
performance. Despite enhancements, LSTMs were found to
lack comprehension when relatively longer sequences were
involved. This is due to the reason that the entire history
known as a context, is being handled by a single state vector.
However, greater compute resources lead to an influx of
novel architectures causing a meteoric rise of Deep Learning
[1] based NLP models.

The breakthrough Transformer [2] architecture in 2017
overcame LSTM’s context limitation via the Attention
mechanism. Additionally, it provided greater throughput as
inputs are processed in parallel with no sequential
dependency. Subsequent launches of improved Transformer
based models like GPT-I [3] and BERT [4] in 2018 turned
out to be a climacteric year for the NLP world. These
architectures were trained on large datasets to create pre-

trained models. Thereafter transfer learning was used to fine-
tune these models for task-specific features resulting in
significant performance enhancement on several NLP tasks
[5],[6],[7],[8],[9],[10]. These tasks include but are not
limited to language modeling, sentiment analysis, question
answering, and natural language inference.

This accomplishment lacked the transfer learning’s primary
objective of achieving high model accuracy with minimal
fine-tuning samples. Also, model performance needs to be
generalized across several datasets and not be task or dataset-
specific [11],[12],[13]. However, the goal of high
generalization and transfer learning was being compromised
as an increasing amount of data was being used for both pre-
training and fine-tuning purposes. This clouded the decision
whether greater training data or an improved architecture
should be incorporated to build a better SOTA language
model. For instance, the subsequent XLNet [14] architecture
possessed novel yet intricate language modeling, that
provided a marginal improvement over a simplistic BERT
architecture that was trained on a mere ~10% of XLNet’s
data (113GB). Thereafter, with the induction of ROBERTa
[15], a large BERT-based model trained on significantly
more data than BERT (160GB), outperformed XLNet. Thus,
an architecture that is more generalizable and further is
trained on larger data, results in NLP benchmarks.


The above-mentioned architectures are primarily language
understanding models, where a natural dialect is mapped to
a formal interpretation. Here the initial goal is the translation
of an input user utterance into a conventional phrase
representation. For Natural Language Understanding (NLU)
the intermediate representation for the above models’ end
goal is dictated by the downstream tasks.

Meanwhile, fine-tuning was transpiring to be progressively
challenging for task-specific roles in NLU models as it
required a greater sample size to learn a particular task,
which bereft such models from generalization [16]. This
triggered the advent of Natural Language Generation (NLG)
models that contrary to NLU training, generated dialect
utterances learned from their corresponding masked or
corrupted input semantics. Such models operate differently
from a routine downstream approach of cursory language
comprehension and are optimal for sequence-to-sequence
generation tasks, such as language translation. Models like
T5 [17], BART [18], mBART [19], T-NLG [20] were pre-
trained on a large corpus of corrupted text and generated its
corresponding cleaned text via denoising objective [21]. This
transition was useful as the additional fine-tuning layer for
NLU tasks was not required for NLG purposes. This further
enhanced prediction ability via zero or few-shot learning
which enabled sequence generation with minimal or no fine-
tuning. For instance, if a model’s semantic embedding space
is pre-trained with animal identification of “cat”, “lion” and
“chimpanzee”, it could still correctly predict “dog” without
fine-tuning. Despite superior sequence generation
capabilities, NLG model sizes surged exponentially with the
subsequent release of GPT-III [22] which was the largest
model before the release of GShard [23].

Since NLU and NLG’s exceptionally large-sized models
required several GPUs to load, this turned out costly and
resource prohibitive in most practical situations. Further,
when trained for several days or weeks on GPU clusters,
these colossal models came at an exorbitant energy cost. To
mitigate such computational costs [24], Knowledge
Distillation (KD) [25] based models like DistiIBERT [26],
TinyBERT [27], MobileBERT [28] were introduced at
reduced inference cost and size. These smaller student
models capitalized on the inductive bias of larger teacher
models (BERT) to achieve faster training time. Similarly,
pruning and quantization [29] techniques got popular to
build economically sized models. Pruning can be classified
into 3 categories: weight pruning, layer pruning, and head
pruning where certain minimal contributing weights, layers,
and attention heads are removed from the model. Like
pruning, training-aware quantization is performed to achieve
less than 32-bit precision format thereby reducing model
size.

For higher performance, greater learning was required which
resulted in larger data storage and model size. Due to the

model’s enormity and implicit knowledge storage, its
learning ability had caveats in terms of efficient information
access. Current Knowledge Retrieval models like ORQA
[30], REALM [31], RAG [32], DPR [33] attempt to alleviate
implicit storage concerns of language models by providing
external access to interpretable modular knowledge. This
was achieved by supplementing the language model’s pre-
training with a ‘knowledge retriever’ that facilitated the
model to effectively retrieve and attend over explicit target
documents from a large corpus like Wikipedia.

Further, the Transformer model’s inability to handle input
sequences beyond a fixed token span inhibited them to
comprehend large textual bodies holistically. This was
particularly evident when related words were farther apart
than the input length. Hence, to enhance contextual
understanding, architectures like Transformer-XL [34],
Longformer [35], ETC [36], Big Bird [37], were introduced
with modified attention mechanisms to process longer
sequences.

Also, due to the surge in demand for NLP models to be
economically viable and readily available on edge devices,
innovative compressed models were launched based on
generic techniques. These are apart from the Distillation,
Pruning, and Quantization techniques described earlier. Such
models deploy a wide range of computing optimization
procedures ranging from hashing [38], sparse attention [39],
factorized embedding parameterization [40], replaced token
detection [41], inter-layer parameter sharing [42], or a
combination of the above mentioned.

ll. RELATED REVIEWS/TAXONOMY

We propose a novel NLP based taxonomy providing a

unique classification of current NLP models from six

different perspectives:

> NLU Models: NLU models excel in classification,
structured prediction, and/or query generation tasks. This
is accomplished through pre-training and fine-tuning
motivated by the downstream task.

> NLG Models: Contrary to NLU models, these stand out
in sequence-to-sequence generation tasks. They generate
clean text via few and single-shot learning from
corresponding corrupted utterances.

> Model Size Reduction: Use compression-based
techniques like KD, Pruning, and Quantization to make
large models economical and pragmatic. It's useful for the
real-time deployment of large language models to operate
on edge devices.

> Information Retrieval (IR): Contextual open domain
question answering (QA) is reliant on effective and
efficient document retrieval. Hence, IR systems via
superior lexical and semantical extraction of physical



Language Understanding Models (NLU SSS

2017

Text Generation Models (NLG)

=~ ® | Distillation [-~
1

!
I = — >] Layer Pruning
I I

Model Size Reduction r >[ Pruning ia — P{1_Weight Pruning
| I a
! “ — 2] Head Pruning
F

Information Retrieval

~ ——

2020
Megatron

DistilBERT a!
|_TinyBERT cs
|__ MobileBERT

2019

Structured Dropout rnc
Poor Man's BERT

2019
Structured Pruning
2019

Specialized Heads 2020.
16 Heads Better Than 1?
—

2018

2019

~||_ Deep Self Attention

\|_ Transformer-XL 1

A
R
Cc
H
I

T
E
C
T
U
R
E
Ss

[computationally Efficient Architectures SS |

2020

Figure 1: Taxonomy of NLP Architectures

documents from a large textual corpus create SOTA in
the QA domain on multiple benchmarks outperforming
contemporary language models.
> Long Sequence Models: Attention-based computational
complexity in Transformers scales quadratically with
input length, hence it is usually fixed to 512 tokens. This
might be acceptable for co-reference resolution tasks that
benefit from smaller input lengths [43], however, is
inadequate for Question Answering (QA) tasks where
reasoning is required across multiple lengthy documents
e.g., the HotpotQA dataset [44].
> Computationally _ Efficient _ Architectures: Memory
efficient architectures with comparable accuracies to
large language models were built to reduce the high
training time of such models.

The above mentioned is a generalized categorization and not
a hard classification, few models can be _ used
interchangeably that might serve dual purposes, however,
there is a clear demarcation despite insignificant
universality. Figure 1 depicts this taxonomy giving a visual
breakdown of the significant models belonging to different
categories along with their launch years.

Ill. PRELIMANIRES TO MODERN NLP ARCHITECTURES
A traditional RNN Encoder-Decoder model [45] comprises
of two recurrent neural networks (RNN), where one
produces the encoded version of the input sequence, and the
other generates its decoded version into a different sequence.
To maximize the target’s conditional probability for an input
sequence, the model is trained jointly with the following
language modeling,

VOLUME XxX, 2017

y* = argmax PC yp | V1, V2.3 Ve-1) (1)
PCt | Vas Vor Var Ve-1) = PCM lve *) (2)

Such a system is empirically found to give superior results
than vanilla RNNs, LSTMs [46], or GRUs [47] by
implementing conditional probabilities of phase pairs in
machine translation, sequence to sequence mapping, or text
summarization tasks.

Or-1 t O; Ores
cy A 4
Vea Ve Vets
| H'4 H', A's
W',-1 | Sigmoid (o) W'. | Sigmoid (o) W't+1_ | Sigmoid (co)
a nn nnn nn nn nnnnent ge pL). >
ry ry ry
| Ulead U'e U'eea

’ :
Sigmoid(o)

val » f Veen |

W,-1 | Sigmoid (0) W, | Sigmoid (a) Wr41 | Sigmoid (oc)
a es > ——— SS a ee >

ry
Aya | HH, Aes
Ue-1 U Urey

est Gale

FIGURE 2. Encoder-Decoder Architecture

In the above architecture (Figure 2), Encoder’s final layer
E,4, transmits information to the decoder from its final
hidden V,,, the layer which contains the entire contextual
understanding of all previous words via a probability
distribution.


This combined abstract representation of all the words is fed
to the decoder to compute the desired language-based task.
Like its preceding layers, the final layer’s corresponding
learnable parameters are U,,, and V,,, at input and output
respectively at the Encoder and U',44, V'z41 at the Decoder.
Combining the weight matrices with hidden state and bias
can be expressed mathematically as follows:

Encoder:
Aes, = OU 41: Xe41 + Wear Ay + by) (3)
Ees1 = OVe41- Ata + be) (4)
Decoder:
A’ ta. = OU a Pica + W' e44-H", + Bisa) (5)
Or41 = Softmax(H' t41-V"'t41 + Bess) (6)

Thereafter, the induction of Attention [48],[49] in 2014-15
overcame the RNN Encoder-Decoder limitation that suffered
from prior input dependencies, making it challenging to infer
longer sequences and suffered from vanishing and exploding
gradients [50]. The attention mechanism eliminated the RNN
dependency by disabling the entire input context through one
final Encoder node. It weighs all inputs individually that feed
the decoder to create the target sequence. This results in a
greater contextual understanding leading to superior
predictions in target sequence generation. First, the
alignment determines the extent of match between the j'®
input and i'* output which can be determined as

ej = tanh(h;_4, h;) (7)
More precisely, the alignment scores take as input all
encoder output states and the previous decoded hidden state
which is expressed as:

Scoreaiign = Weomp: tanh(Wgec. Haec + Wenc-Henc) (8)

The decoder's hidden state and encoder outputs are passed
via their respective linear layers along with their trainable
weights. The weight a,; for each encoded hidden
representation h; is computed as:
_ exp (ets)
Ot) ST exp (ex1d" @)
The resulting context vector in this attention mechanism is
determined by:
Tx
Ce = » ajh; where T, = input sequence length (10)
j=
The Attention mechanism is essentially the generation of the
context vector computed from the various alignment scores
at different positions as shown in figure 3.
Luong’s Attention mechanism differs from the above-
mentioned Bahdanau’s in terms of alignment score
computation. It uses both global and local attention, where
the global attention uses all encoder output states while the
local attention focuses on a small subset of words. This helps
to achieve superior translation for lengthier sequences. These
attention designs led to the development of modem
Transformer architectures which use an enhanced attention
mechanism as described in the next section.

FIGURE 3. Attention Mechanism on Encoder-Decoder Model

IV. NLU ARCHITECTURES

NLU’s approach of transferring pre-trained neural language
representations demonstrated that pre-trained embeddings
improve downstream task results when compared to
embeddings learned from scratch [51],[52]. Subsequent
research works enhanced learning to capture contextualized
word representations and transferred them to neural models
[53],[54]. Recent efforts not limited to [55],[56],[57] have
further built on these ideas by adding end-to-end fine-tuning
of language models for downstream tasks in addition to
extraction of contextual word representations. This
engineering progression, coupled with large compute
availability has evolved NLU’s state of the art methodology
from transferring word embeddings to transferring entire
multi-billion parameter language models, achieving
unprecedented results across NLP tasks. Contemporary NLU
models leverage Transformers for modeling tasks and
exclusively use an Encoder or a Decoder-based approach as
per requirements. Such models are vividly explained in the
subsequent section.

IV-A TRANSFORMERS

IV-A.1. The Architecture

The original Transformer is a 6-layered Encoder-Decoder
model, that generates a target sequence via the Decoder from
the source sequence via the Encoder. The Encoder and
Decoder at a high level consist of a self-attention and a feed-
forward layer. In the Decoder an additional attention layer in
between enables it to map its relevant tokens to the Encoder
for translation purposes. Self Attention enables the look-up
of remaining input words at various positions to determine
the relevance of the currently processed word. This is
performed for all input words that help to achieve a superior
encoding and contextual understanding of all words.
Transformer architecture was built to induct parallelism in
RNN and LSTM’s sequential data where input tokens are fed
instantaneously and corresponding embeddings are
generated simultaneously via the Encoder. This embedding
maps a word (token) to a vector that can be pre-trained on
the fly, or to conserve time a pre-trained embedding space
like GloVe is implemented. However, similar tokens in
different sequences might have different interpretations
which are resolved via a positional encoder that generates

4


context-based word information concerning its position.
Thereafter the enhanced contextual representation is fed to
the attention layer which furthers contextualization by
generating attention vectors, that determine the relevance of
the i*’ word in a sequence concerning other words. These
attention vectors are then fed to the feed-forward Neural
Network where they are transformed to a more digestible
form for the next ‘Encoder’ or Decoder’s ‘Encoder-Decoder
Attention’ block.

The latter is fed with Encoder output and Decoder input
embedding that performs attention between the two. This
determines the relevance of Transformer’s input tokens
concerning its target tokens as the decoder establishes actual
vector representation between the source and _ target
mapping. The decoder predicts the next word via softmax
which is executed over multiple time steps until the end of
the sentence token is generated. At each Transformer layer,
there are residual connections followed by a layer
normalization [58] step to speed up the training during
backpropagation. All of the transformer architectural details
are demonstrated in Figure 4.

IV-A.2. Queries, Keys, and Values

The input to the Transformer’s Attention mechanism is
target token Query vector Q, its corresponding source token
Key vector K, and Values V which are embedding matrices.
Mapping of source and destination tokens in machine
translation can be quantified as to how similar each of their
tokens is in a sequence via inner dot product. Therefore, to
achieve accurate translation the key should match its
corresponding query, via a high dot product value between
thetwo. Assume Q € {Lg,D}andK € {Lx,D} where Lo, Lx
represent target and source lengths, while D denotes the word
embedding dimensionality. Softmax is implemented to
achieve a probability distribution where all Query, Key
similarities add up to one and make attention more focused
on the best-matched keys.

Wsu = softmax(Q.K") where Wsy € {Lg, Lx} (11)

Query assigns a probability to key for matching and often
values are similar to keys, therefore
Zatt = Attention(Q,K,V) =

softmax(Q.K").V = Wey.V (12)

IV-A.3. Multi-Headed Attention (MHA) and Masking

MHA enhances the model’s capacity to emphasize a
sequence’s different token positions by implementing
attention parallelly multiple times. The resulting individual
attention outputs or heads are concatenated and transformed
via a linear layer to the expected dimensions. Each of the
multiple heads enables attending the sequence parts from a
different perspective providing similar representational
forms for each token. This is performed as each token’s self-
attention vector might weigh the word it represents higher
than others due to the high resultant dot product. This is not
productive since the goal is to achieve similarly assessed
interaction with all tokens. Therefore self-attention is

computed 8 different times resulting in 8 separate attention
vectors for each token which are used to compute the final
attention vector via a weighted sum of all 8 vectors for each
token. The resultant multi-headed attention vectors are
computed in parallel which is fed to the feed-forward layer.
Each subsequent target token T,,, 1s generated using as
many source tokens in the encoder (So,.., 5:4). However,
in an autoregressive decoder only previous time stepped
target tokens are considered (Tp,..,7;), for future target
prediction purposes known as causal masking. This is
provided to enable maximal learning of the subsequently
translated target tokens. Therefore during parallelization via
matrix operations, it is ensured that the subsequent target
words are masked to zero, so the attention network cannot
see into the future. The Transformer described above
resulted in significant improvement in the NLP domain. This
leads to a plethora of high-performance architectures that we
describe in the subsequent sections.

s TRANSFORMER
A { Softmax Layer }
wo ! l
MOY { Linear Layer }

\

wx
ees |

. A
1
1
1

4
Add & Normalize
K R

Self Attention

i
Add & Normalize
A

Feed Forward Feed Forward

Add & Normalize

+H =mo0omo

Self Attention
ry

A
Ol retinal (9) = ey
“ Encoding ww ) (

i wide 1x4 input

FIGURE 4. The Multi-headed Transformer Architecture

IV-B EMBEDDINGS FROM LANGUAGE MODELS: ELMo
The goal of ELMo [59] is to generate a deep contextualized
word representation that could model (i) intricate syntactical
and semantical characteristics of word (ii) polysemy or
lexical ambiguity, words with similar pronunciations could
have different meanings at different contexts or locations.
These enhancements gave rise to contextually rich word
embeddings which were unavailable in the previous SOTA
models like GloVe. Unlike its predecessors that used a
predetermined embedding, ELMo considers all N token
occurrences (t,,t ,..,ty) for each token t in the entire
sequence before creating embeddings. The authors
hypothesize that the model could extract abstract linguistic
attributes in its architecture’s top layers via a task-specific
bi-directional LSTM.

This is possible by combining a forward and a backward
language model. At timestep k — 1, the forward language
model predicts the next token t, given the input sequence’s
previous observed tokens via a joint probability distribution

5


shown in (13). Likewise, in (14) with its order reversed, the
backward language model forecasts the prior tokens given
the future tokens.

N
P(t, to,..,tn) = | [>< tk | ty, to,..,tk-1) (13)

P(t tore w=] Jett tsntaer nt (14)

This is further implemented through a softmax on top of the
final LSTM layer as shown in Figure 5.

jLM a
hy? hye

FIGURE 5. Bi-directional LSTM based ELMo Language Model

ELMo for each token representation x, computes its
intermediary bi-directional vector representation h;,,; at each
layer j of the LSTM model as:

By (xk, Fk ne |j=1,..,L}

= {hit | f = 0,00} (15)
Mathematically hy'§ =x; will the lowest level token
representation and it could be generalized as:
hey = [hey he] j € (1. 2} (16)
ELMo learns normalized weights via softmax spask over L
layer representations. This results in a_ task-specific
hyperparameter y‘@S* that enables the task’s scaling
optimization. Hence for a particular task, the word
representation variance in different lagers is expressed as:

ELMO = E(Ry: gtask) = = ya) spask hit (17)

j=0

IV-B GENERATIVE PRE-TRAINING MODEL: GPT-I

In the first phase through unsupervised learning, the decoder-
based GPT-I is pre-trained on a large dataset. This promotes
raw data compute that eliminates the data labeling bottleneck
of supervised learning. The second phase performs task-
specific fine-tuning on considerably smaller supervised
datasets with marginal input variations. Consequently, it led

to greater task agnosticism than then SOTA models like
ELMo, ULMFiT [60] and succeeded in more sophisticated
tasks like common-sense reasoning, semantic similarity, and
reading comprehension. The pre-training of GPT-I can be
modeled as a maximization function of unsupervised tokens

{Uj,..-,Un}.-
L,(U) = > log P(ui | Uj, +++) Uj-13 9) (18)

U
where k is the context window size and conditional
probability is parametrized via @. With multi-headed-
attention and feedforward layers, a target token-based
probability distribution via softmax is produced.

hy = transformer piock(n,_,) Vi € [1,7] (19)
hy) = UW, +W, (20)
P(u) = softmax(h,W,") (21)

where (U = u_x,..,U_z) is the set of context token vector,
n is the number of layers, W, and W, are the token and
positional embedding matrices respectively. Post-pre-
training, parameter adaptation for the supervised end task
takes place. Here input sequence (x?,..,x”) from a labeled
dataset C is fed to the previous pre-trained model to obtain
the transformer’s block final activation hj” that is fed to a
parametrized (W,) linear output layer for prediction (y).
Also, the objective Lz (C) is maximized is as follows

P(y|x1,...,x™) = softmax(hi"W,) (22)
by(C) = Dexyy log PCy | 4,0) (23)

Transformer Linear

Text Extract  ENTAILMENT

Transformer Linear

Premise Delim Hypothesis
Transformer. —— SIMILARITY

Text 1 Delim Text 2 ‘S) > Linear
Text 2 Delim Text 1 Extract Transformer

eae
Context Delim Answer 1 Transformer Linear 2
Context Delim Answer 2 ‘i Transformer Linear *

x

Context Delim Answer N Tra refornes Linear

y—#4

Decoder 12
“{

GPT-|
(Generative Pretrained Model)

Decoder 1 |

------>
------>

t

FIGURE 6. GPT-1 task-based Architecture (top) and magnified views of
Transformer based Decoder (bottom)

Incorporating a secondary language modeling objective
during fine-tuning enhances learning by a _ better
generalization of the supervised model and accelerates
convergence as:

L3(C) = L2(C) + AL, (C) (24)
6


GPT performs various tasks like classification, entailment,
similarity index, Multiple-Choice Questions (MCQ) as
shown in figure 6. The extraction phase distills features from
textual bodies before which the text is separated via the
‘Delimiter’ token during text pre-processing. This token is
not required for classification tasks since it does not need to
gauge the relationship between multiple sequences.
Moreover, Q&A or textual entailment tasks involve defined
inputs like ordered sentence pairs or triplets in a document.
For MCQ tasks, contextual alterations are required at input
to achieve the correct results. This is done via a Transformer
based Decoder training objective where input
transformations are fine-tuned for their respective answers.

IV-C BIDIRECTIONAL ENCODER REPRESENTATIONS
FROM TRANSFORMER: BERT

BERT is a stack of pre-trained Transformer Encoders that
overcomes prior models’ restrictive expressiveness i.e.,
GPT’s lack of bidirectional context and ELMo’s shallow
dual context’s concatenation. BERT’s deeper model
provides a token with several contexts with its multiple
layers and the bi-directional model provides a richer learning
environment. However, bi-directionality raises concerns that
tokens could implicitly foresee future tokens during pre-
training resulting in minimal learning and leading to trivial
predictions. To effectively train such a model, BERT
implements Masked Language Modeling (MLM) that masks
15% of all input tokens randomly in each input sequence.
This masked word prediction is the new requirement unlike
recreating the entire output sequence in a unidirectional LM.
BERT masks during pre-training, hence the [MASK] token
does not show during fine-tuning, creating a mismatch as the
“masked” tokens are not replaced. To overcome this
disparity, subtle modeling modifications are performed
during the pre-training phase. If a token T; is chosen to be
masked, then 80% of the time it is replaced with the [MASK]
token, 10% of the time a random token is chosen and for the
remaining 10%, it remains unchanged. Thereafter T; cross-
entropy loss will predict the original token, the unchanged
token step is employed to maintain a bias towards the correct
prediction. This methodology creates a state of randomness
and constant learning for the Transformer encoder which is
compelled to maintain a _ distributed contextual
representation of each token. Further, as random replacement
arises for a mere 1.5% of all tokens (10% of 15%), this does
not seem to impair the language model’s understanding
ability.

Language modeling could not explicitly comprehend the
association between multiple sequences; therefore it was
deemed sub-optimal for inference and Q&A tasks. To
overcome this, BERT was pre-trained with a monolingual
corpus for a binarized Next Sentence Prediction (NSP) task.
As shown in Figure 7, sentences Y (He came [MASK] from
home) and Z (Earth [MASK] around Sun) do not form any
continuity or relationship. Since Z is not the actual next
sentence following Y, the output classification label
[NotNext] gets activated, and [IsNext] activates when
sequences are coherent.

_ 1% | IsNext Predict Probability that
English wort 10% _| persistence [2 | NotNext sentence Z follows Y
possible cl Scent | A
0% _| players 4
[eve +sotimar |
FFNN + Softmax A Ro oat Ae Ae a?
2245 e ay 4 Pes
os f
t t t ! i = hi Encoder 24 \
4 ‘
{ | Encoder 12 | | Encoder 23
L L K LN
1 1 I I
' BERT (Large) :
| BERT (Base) | arge) |
1 1
1 1 Encoder 2
\
\ Encoder 1 4 Encoder Z
ey N )
£ ot i if 2h ah ah gh sf 7 . pnt
‘ . b> , Tokenizedicis} He came [MASK] from home [SEP]
all levels input
[CLS] He came [MASK] from home [SEP] Earth [MASK] around Sun [SEP]
€ Sentence Y » & Sentence Z

FIGURE 7. The architecture of BERT’s MLM and NSP functionality

IV-D GENERALIZED AUTOREGRESSIVE PRETRAINING
FOR LANGUAGE UNDERSTANDING: XLNeT
XLNet captures the best of both worlds where it preserves
the benefits of Auto-Regressive (AR) modeling and
bidirectional contextual capture. To better comprehend why
XLNet outperforms BERT, consider the 5-token sequence
[San, Francisco, is, a, city]. The two tokens chosen for
prediction are [San, Francisco], hence BERT and XLNet
maximize log p(San Francisco | is a city) as follows:
Leerr = logp (San| is a city) +

log p (Franciscolis a city)
Lytnet = log p (San| is a city) +

log p (Francisco|San is a city)
The above can further be generalized for the target (J) and
non-target token set (7), BERT and XLNet will maximize
log p (J |N) with the following different interpretability:

Locer = > logp(el M) (25)
xer

Lover = > loge el Nex) (26)
xeyT

XLNet considers the target as well as the remaining tokens
for prediction, whereas BERT only considers the non-target
tokens. Hence, XLNet captures the inter-pair dependency
[San, Francisco] unlike BERT where either [San] or
[Francisco] leads to correct prediction. Further, via AR
XLNet performs factorized ordering on all possible token
permutations (L! =5!) of sequence length L in the set 1.e.,
{1, 2, 3, 4, 5], [1, 2, 5, 4, 3]... ., [5, 4, 3, 2, 1]} = [is, San,
Francisco, a, city] etc.

T
max
g Fant ») Log Pe ( Xe, | =) (27)
t=1

where set Z, contains all permutational sequences of length
T [1,2,..,T] and x, ‘ is the reference token. Hence the target
learns from numerous combinations attaining a richer
contextualized learning. Further for all permutable
factorization orders, the model parameters are shared to build
knowledge and bidirectional context from all factorizations
as demonstrated via equation 27.


IV-D.1. Masking
There is a challenge to determine the word order in the
sequence as the token (x,,) determining the autoregression
is not considered. This word order is partially achieved via
positional encoding, however, for contextual understanding
XLNet employs masking. Consider a generated permutation
of [2, 1, 3] in a 3-token sequence where the first token i.e., 2
has no context hence all masking results in [0,0,0] in the 2™
row of the 3X3 masking matrix. Similarly, the 24 and 3
masks would result in [0,1,0] and [1,1,0] in the 1‘ and 3"row
of the Query Stream (QS) masking matrix where the token
cannot see itself. QS matrix with an all-one diagonal
inclusion constitutes Content Stream (CS) masking matrix
where each token can see itself. This 3-token sequence
masking is demonstrated in figure 8 below.

ll 010 110
2 000 010
110 1:71

C a >
2 2 2 a a
n?) (me. [ne] | © (a)

ee aE Oe 011 111
- ee ~
pee el on 001 011
- 1 1 1
< h, | -7| hy fe 1 , 000 001

.
mem he Im YSN
at “* . s (b) (b)

- s s

x

- - \ x

-" == SAS 000 100
mend x, x, | | x, | 101 111
— 100 101
23133 || 33291 |[19352 (c) (c)

(a) (b) (c) QS Masking CS Masking

Factorization
Order

FIGURE 8. Illustration of predicting x2 in the 3-token sequence with
different factorization orders and its corresponding masking matrices

The first reference ‘2’ has no context which is gathered from
its corresponding ‘mem block’, a Transformer-XL-based
extended cached memory access. Thereafter it receives
context from token ‘3’ and ‘1’,’3’ for subsequent orderings.

IV-D.2. Model Architecture

Figure 9 demonstrates the model’s two-stream attention
framework that consists of a content and query stream
attention process to achieve greater understanding via
contextualization. This process is initiated via target-aware
representation, where the target position is baked into the
input for subsequent token generation purposes.

(i) Target Aware Representation: A vanilla implementation
of Transformer based parametrization does not suffice for
complex permutation-based language modeling. This is
because the next token distribution pg(Xz,|Xz<r) is
independent of the target position i.e., Z,. Subsequently,
redundant distribution is generated, which is unable to
discover effective representations, hence target position-
aware re-parametrization for the next-token distribution is
proposed as follows:

exp (e(x)T ho(xze,))
Lyr exp (e(x!)Thg(xz-,))
exp(e(x)" go(xz<,2t))
|X) = Br exp (€@ go (Xz2,20)

(29)

ll

Do( Xz,

where go(X,<;,Z;) is a modified representation that
additionally considers the target position Z, as an input.

(ii) Two Stream Self Attention: The formulation of gg
remains a challenge despite the above resolution as the goal
is to rely on the target position Z, to gather contextual
information x,<; via attention, hence: (1) For gg to predict
Xzz, it should utilize the position of Z, only to incorporate
greater learning, not the content xz, (2) To predict other
tokens Xz; where j > t,gg should encode the context xz,

to provide full contextual understanding.

To further resolve the above conflict, the authors propose
two sets of hidden representation instead as follows:

“+ The hidden content representation hg (x,<,) = hz, that

encodes both context and content xz,

“+ The query representation gg(xz<t,21) = gz, which
solely accesses the contextual information x,-, and
position Z, without the content xz,

hw ) 3: Content Stream »\ (« («J [x]
s * Attention

=) te) [ny (s)
Saad. *, re

4 Masked Two-Stream Attention
. ae

{Attention [a2 |
=.
Qa} KV
eyes
) r aie! Z
Kk fieJo
* Tees S

nh: [62 } Query Stream Attention y :
i 4 Masked Two-Stream Attention
Attention ee
oe Cate) Colm) Gm Lo)

—_—

FIGURE 9. (Left): Standard Attention via Content Stream and Query
Stream Attention without access to the content. (Right): LM training

The above two attention courses are parametrically shared
and updated for every self-attention layer m as:

Attention(Q = hz," ?, KV = hz"; 0) > hz)
(Content Stream: utilize both Z, and xz,)

Attention(Q = 92", KV = hy 938) > gz)
(Query Stream: use Z, without seeing xz,)

This dual attention is pictorially expressed in figure 9. For
simplicity purposes, consider the prediction of token t; that
is not allowed to access its corresponding embedding from
the preceding layer. However, to predict t;,, the token t;
needs to access its embedding and both operations must
occur in a single pass.

Therefore, two hidden representations are implemented

where hz ae is initialized via token embeddings and gz i”
through weighted transformations. From above equations
hg ae can access the history including the current position
whereas gz Adie can access only previous hz Am positions.
The token prediction happens in the final layer via gz i .
For greater sequence length processing the memory blocks

8


are derived from Transformer-XL which can process longer
than standard Transformer input sequence lengths. The
hidden representations mentioned above are also stored in
the memory blocks.

IV-E A Robustly Optimized BERT Pretraining Approach:
RoBERTa

This paper claimed that BERT was considerably
undertrained and as a result, RoBERTa incorporated a
greater training intensive regime. This was for BERT-based
models that could match or exceed the prior methods. Their
revisions include: (i) longer training duration with greater
data and batch sizes (ii) eliminating BERT’s NSP goal (iii)
longer sequence training (iv) training data’s masking pattern
modified dynamically. The authors claim superior
performance over BERT on downstream tasks for a more
diverse and voluminous CC-News dataset.

Further, BERT implements a non-efficient static masking
implementation to avoid redundant masks. For instance,
training data that is duplicated 10 times for a sequence to be
masked in 10 different ways for 40 training epochs, where
each training sequence is seen with the same mask 4 times.
RoBERTa provides slightly enhanced results via
incorporating dynamic masking where a masking pattern is
generated each time the model is fed a sequence while
pretraining larger datasets. Recent work has questioned
BERT’s NSP [61] role which was conjectured to play a key
role in its performance in language inference and Q&A tasks.
RoBERTa amalgamates both hypotheses and provides
numerous supplementary training formats that perform like
BERT and outperform it for full sentence training excluding
the NSP loss. RoBERTa provides similar and marginally
better results than BERT on GLUE benchmark as well as on
RACE and SQUAD datasets without fine-tuning for multiple
tasks.

IV-E MEGATRON LANGUAGE MODEL (LM)

Megatron was the largest model when released with the size
of 24 X BERT and 5.6 x GPT-2 and could not fit in a single
GPU. Hence the key engineering implementation was the
induction of its 8 and 64-way model, and data parallelized
version where parameters were split across (~512) GPUs. It
sustained high performance (15.1 Petaflops) and scaling
efficiency (76%), whereas BERT resulted in performance
degradation with size growth. This feat was primarily
attributed to layer normalization and residual connection re-
ordering within the transformer layers. This led to
monotonically superior performance on downstream tasks
with increased model size.

Megatron overcomes the prior model’s memory constraint
via splitting the model across several accelerators. This not
only resolves the memory usage but enhances the model
parallelism irrespective of batch size. It incorporates
distributed tensor computations to upsurge model size or
acceleration and parallelizes the attention head computation.
This does not require a new compiler or code re-write and is
implementable with a few parameters.

First, the Multi-Layer Perceptron (MLP) block partitions the
GEMM parallelly in two columns, enabling GeLU
nonlinearity applied independently to each partitioned
GEMM. This GeLU output is fed directly to the row-wise
parallelized GEMM whose output is reduced via a single all-
reduce operator (g and f) in forward and backward pass
before passing it to the dropout layer.

FIGURE 10. Parallelized Megatron’s MLP and Self-Attention blocks

Parallelism in the self-attention block is achieved by
partitioning the GEMMs column-wise for each key, query,
and value set. Hence, the workload is split across all GPUs
as matrix multiplication for each attention head is performed
on a single GPU. The resulting GEMM output, like MLP
undergoes an all-reduce operation and is parallelized across
rows as shown above in figure 10. This technique eliminates
the need for synchronization between GEMMs for MLP and
attention blocks.

V. NLG ARCHITECTURES

In NLU models, the sheer amount of data compute required
for learning numerous post pre-trained ‘fine-tuned’ tasks is
parametrically inefficient, as an entirely new model is
required for every task. These models can be exemplified as
narrow experts rather than proficient generalists. Therefore,
NLG models provide a transition towards building generic
systems, that accomplish several tasks without the necessity
to create and label a training dataset manually for each task.
Moreover, MLM in NLU models is unable to capture a rich
relationship between multiple sequences. Further, most
effective NLU models derive their methodologies from the
MLM model variants which are denoising autoencoders
trained on text reconstruction where a random subset of
words is masked out. Consequently, NLG models in the last
few years have made tremendous progress on tasks like text
translation and summarization, Q&A, NLI, conversational
engagement, picture description, with unprecedented
accuracies.


V-A LANGUAGE MODELS ARE UNSUPERVISED MULTI-
TASK LEARNERS: GPT-II

GPT-II [62] was possibly the first model that dawned on the
rise of NLG models. It was trained in an unsupervised
manner capable of learning complex tasks including
Machine Translation, reading comprehension, and
summarization without explicit fine-tuning. Task-specific
training corresponding to its dataset was the core reason
behind the generalization deficiency witnessed in current
models. Hence robust models would likely require training
and performance gauges on a variety of task domains.
GPT-II incorporates a generic probabilistic model where
numerous tasks can be performed for the same input as
p(output|input,task). The training and test set
performance improves as model size is scaled up and as a
result, it under fits on the huge WebText dataset. The 1.5
billion parameter GPT-2 outperformed its predecessors on
most datasets in the previously mentioned tasks in a zero-
shot environment. It is an extension of the GPT-I decoder-
only architecture trained on significantly greater data.

V-B BIDIRECTIONAL AND AUTOREGRESSIVE
TRANSFORMERS: BART

A denoising autoencoder BART is a sequence-to-sequence
[63] model that incorporates two-stage pre-training: (1)
Corruption of original text via a random noising function,
and (2) Recreation of the text via training the model. Noising
flexibility is the major benefit of the model where random
transformations not limited to length alterations are applied
to the original text. Two such noising variations that stand
out are random order shuffling of the original sentence and a
filling scheme where texts of any spanned length are
randomly replaced by a single masked token. BART deploys
all possible document corruption schemes as shown below in
figure 11, wherein the severest circumstance all source
information is lost and BART behaves like a language
model.

Sentence Document
Permutation Rotation

£] Token
A Masking
|

| Bidirectional

- 7 face ) f{peasc.| { coeas |
SE te >| oe - ! ~
Encoder Decoder Ty 7” ; a Pa
_< ) O--==-~2- =~ > ~~ :
AKA KKKAK (a.c.e. +>{apepe }<- aoe. |
— coe — n Text
\ - B - Ej| <s> AB C D pall Infiling

FIGURE 11. Denoised BART Model and its Noising Schemes

This forces the model to develop greater reasoning across
overall sequence length enabling greater input
transformations which results in superior generalization than
BERT. BART is pre-trained via optimization of a
reconstruction loss performed on corrupted input documents
i.e., cross-entropy between decoder’s output and original
document. For machine translation tasks, BART’s encoder
embedding layer is replaced with an arbitrarily initialized
encoder, that is trained end-to-end with the pre-trained model
as shown in Figure 12. This encoder maps its foreign
vocabulary to BART’s input which is denoised to its target

language English. The source encoder is trained in two
stages, that share the backpropagation of cross-entropy loss
from BART’s output. Firstly, most BART parameters are
frozen, and only the arbitrarily initialized encoder, BART’s
positional embeddings, and its encoder’s self-attention input
projection matrix are updated. Secondly, all model
parameters are jointly trained for few iterations. BART
achieves state-of-the-art performance on several text
generation tasks, fueling further exploration of NLG models.
It achieves comparative results on discriminative tasks when
compared with RoBERTa.

Pre-trained
Encoder

[<s> AB C D |
FIGURE 12. Denoised BART Model for fine-tuned MT tasks

V-C MULTILINGUAL DENOISING PRE-TRAINING FOR
NEURAL MACHINE TRANSLATION: mBART

V-C.1. Supervised Machine Translation

mBART demonstrates that considerable performance gains
are achieved over prior techniques [64], [65] by
autoregressively pre-trainng BART, via sequence
reconstructed denoising objective across 25 languages from
the common crawl (CC-25) corpus [66]. mBART’s
parametric fine-tuning can be supervised or unsupervised,
for any linguistic pair without task-specific revision. For
instance, fine-tuning a language pair i.e. (German-English)
enables the model to translate from any language in the
monolingual pre-training set i.e. (French English), without
further training. Since each language contains tokens that
possess significant numerical variations, the corpus is
balanced via textual up/downsampling from each language i

with the ratio J;

1 pi~
=, 30
‘pi Levit 30)

where p; is each language’s percentage in the dataset with a
soothing parameter a = 0.7. The training data encompasses
K languages: C = {C,,-—,C,} where each C; is im”
language’s monolingual document collection. Consider a
text corrupting noising function g(X) where the model is

trained to predict original text X, hence loss Lg is maximized

as:

Ly= > > logP% 1900; 6) (31)
CiEC XEC;

where language i has an instance X and above distribution P

is defined via a sequence-to-sequence model.

V-C.2. Unsupervised Machine Translation

mBART is evaluated on tasks where target bi-text or text

pairs are not available in these 3 different formats.

“* None of any kind of bi-text is made available, here back-
translation (BT) [67],[68] is a familiar solution. mBART

10


offers a clean and effective initialization scheme for
such techniques.

“+ The bi-text for the target’s pair is made unavailable,
however, the pair is available in the target language’s bi-
text corpora for other language pairs.

“+ Bi text is not available for the target pair, however, is
available for translation from a different language to the
target language. This novel evaluation scheme
demonstrates mBART’s transfer learning capability
despite the absence of the source language’s bi-text

mBART is pre-trained for all 25 languages and fine-tuned

for the target language as shown in figure 13.

Multilingual Denoising Pretraining

i :

t

t

I | Transformer Encoder | Encoder Transformer Decoder | Decoder
t

t

i

Language 1: Noise
Injected Corrupted Input
X

Nee ee ew ew oe

Language 1:
Source Input

See eee ew =

|
t
t
i | Transformer Encoder | Encoder Transformer Decoder Decoder
|
|
{

Language 2: Source
Input

(
t
t
t
t

FIGURE 13. mBART Generative Model Pre-training & Fine-tuning

V-D EXPLORING THE LIMITS OF TRANSFER

LEARNING WITH A TEXT-TO-TEXT TRANSFORMER: T5
This model was built by surveying and applying the most
effective transfer learning practices. Here all NLP tasks are
orchestrated within the same model and hyperparameters are
reframed into a unified text-to-text setup where text strings
are inputs and outputs. A high-quality, diverse and vast
dataset is required to measure the scaled-up effect of pre-
training in the 11 billion parameter T5. Therefore, Colossal

Clean Crawled Corpus (C4) was developed, twice as large as
Wikipedia.

The authors concluded that causal masking limits the
model’s capability to attend only till the i*" input entry of a
sequence, which turns detrimental. Hence TS incorporates
fully visible masking during the sequence’s prefix section
(prefix LM) whereas causal masking is incorporated for
training the target’s prediction. The following conclusions
were made after surveying the current transfer learning
landscape.

“* Model Configuration: Normally models with Encoder-
Decoder architectures outperformed decoder-based
language models.

Pre-Training Goals: Denoising worked best for fill-in-
the-blank roles where the model is pre-trained to retrieve
input missing words at an acceptable computational cost
In-Domain Datasets: In-domain data training turns out to
be effective, however pre-training small datasets
generally leads to overfitting.

Training Approaches: A pre-train, fine-tune methodology
for multi-task learning could be effective, however, each
task’s training frequency needs to be monitored.

Scaling Economically: To efficiently access the finite
computing resources, evaluation among model size
scaling, training time, and ensembled model quantity is
performed.

<9

bs

J

%

J

%

J

%

V-E TURING NATURAL LANGUAGE GENERATION: T-
NLG

T-NLG is a 78 layered Transformer based generative
language model, that outsizes the TS with its 17 billion
trainable parameters. It possesses greater speedup than
Nvidia’s Megatron, which was based on interconnecting
multiple machines via low latency buses. T-NLG is a
progressively larger model, pre-trained with greater variety
and quantity of data. It provides superior results in
generalized downstream tasks with lesser fine-tuning
samples. Hence, its authors conceptualized training a huge
centralized multi-task model with its resources shared across
various tasks, rather than allocating each model for a task.
Consequently, the model effectively performs question
answering without prior context leading to enhanced zero-
shot learning. Zero Redundancy Optimizer (ZeRO) achieves
both model and data parallelism concurrently, which perhaps
is the primary reason to train T-NLG with high throughput.

V-F LANGUAGE MODELS ARE FEW-SHOT LEARNERS:
GPT-III

The GPT family (1, H, and ID) are autoregressive language
models, based on transformer decoder blocks, unlike
denoising autoencoder-based BERT. GPT-3 is trained on
175 billion parameters from a dataset of 300 billion tokens
of text used for generating training examples for the model.
Since GPT-3 is 10 times the size of any previous language
model and for all tasks and purposes it employs few-shot
learning via a text interface, without gradient updates or fine-
tuning it achieves task agonism. It employs unsupervised
pre-training, where the language model acquires a wide

11


range of skills and pattern recognition capabilities. These are
implemented on the fly to swiftly adapt to or identify the
desired task. GPT-3 achieves SOTA in several NLP tasks
although its few-shot learning falls short in reproducing
similar results for other tasks.

V-G SCALING GIANT MODELS WITH CONDITIONAL
COMPUTATION AND AUTOMATIC SHARDING: GShard
GShard enables scaling beyond 600 billion parameters for
multilingual machine translation via a sparsely gated mixture
of experts (MoE) by automated sharding at low computation
cost and compile time. The Transformer is sparsely scaled by
inducting a position-wise mixture of experts (MoE) layer
comprising of E feed-forward networks FFN,,...,FFNg
across its Transformer.

Gs = GATE(x;) (32)
FFN,(x,) = Wo,.ReLU (Wig. x,) (33)
Ys = En Ger .FFN, (x5) (34)

where x, and y, are the tokenized input and average
weighted output to the MoE layer, wi, and wo, are an
expert’s (feed-forward layer) input and output projection
matrices. The gating network indicates the expert’s
contribution to the final output via vector G,,. This takes in
a nonzero value for the tokens which are dispatched to a
maximum of two experts that contribute to a non-zero value
in an otherwise sparse matrix.

To achieve efficient parallelization across TPU clusters: (i)
The parallelized attention layer is split along batch
dimensions and weights are replicated across all devices. (ii)
Due to size constraints, it's unfeasible to replicate MoE layer
experts across all devices, hence experts are sharded across
several devices, as shown below.

Encoder Output > Shard E

Add & Normalize
A

Encoder Output > Shard 1

Add & Normalize
A A

=I ut
Add & Normalize Add & Normalize
A A A A

Self Attention Self Attention

All to all
Combine |

All to all
Dispatch

St =
Add & Normalize
A K

ai} =I
E Parallel Models as | Add & Normalize
K A

Mixture of Experts (MoE)

Self Attention

Self Attention

[Devices 1...E]

( Input & Positional Embeddings > Shard 1 { Input & Positional Embeddings > Shard E

FIGURE 14. Sharded MoE Layered Transformer Encoder when scaled
to multiple devices, all other layers are replicated

The two factors determining model quality are (i) High
resourced languages where a vast amount of training data is
available (11) Enhancements for low-resourced languages
with limited data. Increased tasks or language pairs in a
translation model yields positive language transfer [69] for
low-resource languages.

The three-pronged strategy for reasonable training time and
efficiency for a large number of languages are: (i) Increase
network depth by stacking more layers (ii) Increase network
width by replicating the experts (iii) Sparsely assign tokens
to experts via learned routing modules. When the number of
experts per layer was quadrupled from 128 to 512 ina 12-
layer deep model, a significant performance bump of 3.3 was
observed in the BLEU score across 100 languages.
Moreover, quadrupling the width from 512 to 2048 resulted
in a diminishing gain in BLEU by 1.3. Further tripling the
layer depth from 12 to 36 for the previously mentioned
expert widths provides significant gains for low as well as
high resources languages. However, increased model depth
is not fruitful unless the model’s capacity constraint (MoE
width) is not relaxed.

Vi. MODEL SIZE REDUCTION

VI-A DISTILLATION

The goal of Knowledge Distillation (KD) is to train a smaller
student model under the supervision of a larger, more
accurate teacher model via a revised loss function to achieve
similar accuracy across unlabeled samples. The predicted
teacher model samples are supplied to enable student
learning through softer probabilistic class distribution while
predicting through hard target classification via a separate
loss function. This hard to soft label transition enables
greater information variance for student learning, for
instance, hard target classifies dog as {cow, dog, cat ,car €
0,1,0,0} and soft target as {10~°, 0.9,0.1, 10~°}. For hard
classification computation, the last fully connected layer of
a deep neural network is a vector of logits z, for which z; is
the logit for the i” class. Therefore, probability p; that the
input fits the i‘” class can be assessed by a softmax function
in (35) and temperature element T is inducted to influence
each soft target’s significance to be transferred to the student

model learning in (36).
Z 4
exp (7)

eP@) Gg,» - P(t)
x; exp (71)

= Syexp () ae
For a softer probability distribution over classes, a higher
temperature is needed (T =t). Experimentally it was
discovered that it is fruitful to train the student model on
correct (hard/ground truth) labels apart from teacher's soft
labels. Although the student model cannot exactly match the
soft targets, hard label training further assists it to not
stumble to the incorrect prediction. The soft target
distillation loss (T = t) is computed by matching the logits
between the teacher and the student model as:

= >, —pi(2Zri,T) log(pi(zsi,T)) (37)

U
where z, and z, denote the logits of the teacher and student
models, respectively. The distillation mechanism is clearly
explained in figure 15. The cross-entropy between the
ground truth label y and the soft logits of the student model
constitutes the student loss as:

12


L(y, p(Zs,T)) = >, —y; log(p;(zsi,T)) (38)

Knowledge Distillation Model
(Ete! J>[ tegen)

Py £(01 X,0)
4

Teacher Network

FIGURE 15. Language Model’s Generalized Distilled Architecture

The standard model of vanilla knowledge distillation
integrates the distilled and the student loss as shown below,
L£(x,W) = a x Ly(pZp,T), pZs.T)) + BX

L(y, p(Zs,T)) (39)
(T =t)for Lp and(T = 1) for L,
where We __ student parameters and a,f €

regulated parameters. In the original paper weighted
average was used concerming a and f, i.e., 8 = 1— @ and
for best results, it was observed that a > B.

VI-A.1. DistiIBERT

DistiIBERT, the student version of the teacher BERT
retained 97% of BERT’s language understanding
performance and was at inference time lighter, faster, and
required lesser training cost. Through KD, DistilBERT
reduces BERT size by 40%, is 60% faster and the
compressed model is small enough to be operated on edge
devices. The layer-depth of DistiLBERT is slashed by half
when compared with BERT since both possess the same
dimensionality and possess generally an equivalent
architecture. Layer reduction was performed as_ its
normalization and linear optimization were computationally
ineffective in the final layers. To maximize the inductive bias
of large pre-trained models, DistilBERT introduced a triple
loss function which linearly combined the distillation (Lp)
with the supervised training (Lym) or the masked language
modeling loss. It was observed that supplementing the prior
loss with embedding cosine loss (£,,;) was beneficial as it
directionally aligned the teacher’s and student’s hidden state
vectors.

VI-A.2. TinyBERT
To overcome the distillation complexity of the pre-training-
then-fine-tuning paradigm, TinyBERT introduced a lucid

knowledge transfer process by inducting 3 loss functions: (1)
Embedding Layer Output (ii) Attention Matrices, the Hidden
States from Transformer (iii) Output Logits. This not only
led TinyBERT to retain over 96% of BERT’s performance
at drastically reduced size but also deployed a meager 28%
of parameters and 31% of inference time across all BERT-
based distillation models. Further, it leveraged the untapped
extractable potential from BERT’s learned attention weights
[70], for (M + 1)*” layer, knowledge acquired is enhanced
by minimizing:

M+i
Lmodet = > Am“tayer(Sm Tony) (40)

m=0
where Liayer is the loss function of a Transformer or an
Embedding layer and hyperparameter 2,, signifies the
importance of m*" layer’s distillation. BERT’s attention-
based enhancement for language understanding can be
incorporated in TinyBERT as:

h

1
Latin = -) MSE(AS,AT), where A,€ R& (41)
i=1

where h denotes the number of heads, A; is the attention
matrix corresponding to student or teacher’s i‘? head, l
denotes input text length along with mean squared error
(MSE) loss function. Further, TinyBERT distills knowledge
from the Transformer output layer and can be expressed as:

Lhian = MSE(HSW,, H™) (42)
where W, € R'%™*, HS € R&@’ HT € R™4 d’ <d

where H°, H’ are the hidden states of the student and teacher
respectively, hidden sizes of the teacher and student models
are denoted via scalar values of d’ and d, W), is a learnable
matrix that transforms the student network’s hidden states to
the teacher network’s space states. Similarly, TinyBERT
also performs distillation on embedding-layer:

Lema = MSE(ESW,, ET) (43)
where ES and H™ are embedding matrices of student and
teacher networks, respectively. Apart from mimicking the
intermediate layer behavior, TinyBERT implements KD to
fit predictions of the teacher model via cross-entropy loss
between logits of the student and the teacher.

Lyrea = —Sof tmax(z").log (softmax (©) (44)

Here z’ and z* are the respective logits predicted by the
teacher and student models.

VI-A.3. MobileBERT

Unlike previous distilled models, MobileBERT achieves
task-agnostic compression from BERT achieving training
convergence via prediction and distillation loss. To train
such a deeply thin model, a unique inverted bottleneck
teacher model is designed that incorporates BERT (IB-
BERT) from where knowledge transfer distills to
MobileBERT. It is 4.3x smaller, 5.5x faster than BERT
achieving a competitive score that is 0.6 units lower than
BERT on GLUE-based inference tasks. Further, the low
latency of 62 ms on Pixel 4 phone can be attributed to the

13


replacement of Layer Normalization and gelu activation,
with the simpler Hadamard product (°) based linear
transformation.

NoNorm(h) =Y coh+fB,where Y, B € R” (45)

For knowledge transfer, the mean squared error between
feature maps of MobileBERT’s and IB-BERT is
implemented as a transfer objective.

T ON
1
Lhur =z >) Hin — Hékn)? (46)
t=1n=1
where l is layer index, T is sequence length, N is the feature
map size. For TinyBERT to harness the attention capability
from BERT, KL-divergence is minimized between per-head
distributions of the two models, where A denotes the number
of attention heads.

T A

l 1 tr st
Lhe = a2 >) Dux (Palla (47)

t=1a=1

Alternatively, a new KD loss can be implemented during
MobileBERT’s pre-training with a linear combination of
BERT’s MLM and NSP loss, where @ is a hyperparameter
between (0,1).
Lpp = aL£Lyim + (1 — a) Len + Lysp (48)
For the above-outlined objectives, 3 training strategies are
proposed:
(i) Auxiliary Knowledge Transfer: Intermediary transfer via
a linear combination of all layer transfer loss and distilled
pre-training loss.
(ii) Joint Knowledge Transfer: For superior results, 2
separate losses are proposed where MobileBERT is trained
with all layers that jointly transfer losses and perform pre-
trained distillation.
(iii)_Progressive Knowledge Transfer: To minimize error
transfer from lower to higher layers, it is proposed to divide
knowledge transfer into L layered L stages where each layer
is trained progressively.

VI-B PRUNING

Pruning [71] is a methodology where certain weights, biases,
layers, and activations are zeroed out which are no longer a
part of the model’s backpropagation. This introduces
sparsity in such elements which are visible post ReLU layer
that converts negative values to zero
((ReLU (x): max(0,x)). Iterative pruning learns the key
weights, eliminating the least critical ones based on threshold
values, and retraining the model enabling it to recuperate
from pruning by adapting to the remaining weights. NLP
models like BERT, RoBERTa, XLNet were pruned by 40%
and retained their performance by 98%, which is comparable
to DistiLBERT.

VI-B.1 LAYER PRUNING

VI-B.1-A STRUCTURED DROPOUT

This architecture [72] randomly drops layers at training and
test time that enables sub-network selection of any desired
depth, since the network has been trained to be pruning
robust. This is an upgrade from current techniques that

require re-training a new model from scratch as opposed to
training a network from which multiple shallow models are
extracted. This sub-network sampling like Dropout [73] and
DropConnect [74] builds an efficient pruning robust network
if the smartly chosen simultaneous group of weights are
dropped. Formally, pruning robustness in regularizing
networks can be achieved by independently dropping each
weight via Bernoulli’s distribution where parameter p > 0
regulates the drop rate. This is comparable to the pointwise
product of weight matrix W with an arbitrarily sampled {0,
1) mask matrix M, Wg =MOW.

The most effective layer dropping strategy is to drop every
other layer, where pruning rate p and dropping layers at
depth d such that d = O(mod [1/p]). For N groups with a
fixed drop ratio p, the average number of groups utilized
during training the network is N(1 — p), hence pruning size
for r groups, the ideal drop rate will be p* = 1 —r/N. This
approach has been highly effective on numerous NLP tasks
and has led to models on size comparable to distilled versions
of BERT and demonstrate better performance.

VI-B.1-B POOR MAN’S BERT

Due to the over-parameterization of deep neural networks,
availability of all parameters is not required at inference
time, hence few layers are strategically dropped resulting in
competitive results for downstream tasks [75]. The odd-
alternate dropping strategy drove superior results than the
top and even alternate dropping for span K = 2 across all
tasks. For instance, in a 12-layer network, dropping: top —
{11, 12}; even-alternate — {10, 12}; odd-alternate — {9, 11},
concluded in (i) dropping the final two layers consecutively
is more detrimental than eliminating alternate layers, and (ii)
preserving the final layer has greater significance than other
top layers.

Top Layer Dropping

Even Alternate Dropping Contribution Based Dropping

on ( (ra i@ ff
—— oe = ae
|} —__ __ ee
_J63GoaG) & Ce (__)

FY
"
w
EJ
"
ua
a
"
w
FJ
"
a

[2,34] [3], [7,8,9]

Symmetric Dropping Bottom Layer Dropping Odd Alternate Dropping

il
|

_—

|!

A!

>

ae
_ =
\ —_—
: —_€
a
_
—
—
7

mM

1!
[
[

DDDDIDIDAS

|
Hil
i

ad
"
a
Pd
w
a

K=4 K=7

=
wv
a
~
"
w

FIGURE 16. Layer Pruning Strategies deployed by Language Models

At higher values of K, the alternate dropping approach
signifies a large drop in performance, hypothesized due to
the elimination of lower layers. The Symmetric approach

1

&


emphasizes the conservation of top and bottom layers while
middle layers are dropped. This leads to a minimal impact on
BERT while it substantially degrades XLNet’s performance,
resulting in the second-best strategy for BERT giving robust
results even after removal of 4 layers.

Observationally XLNet demonstrates greater pruned
robustness than BERT as its learning mellows close to its 7th
layer whereas BERT keeps learning until the 11th layer.
Consequently (1) XLNet gathers task-oriented knowledge at
lower layers in contrast to BERT, (ii) XLNet’s final layers
might get fairly redundant and are liable to get dropped
without a considerable drop in performances. The authors
furthered the dropping experimentations to DistilBERT, here
dropping 30% of its layers resulted in minimal performance
degradation.

Like previous models, top-layer dropping tured out to be
most reliable as ROBERTa proved to be more pruning robust
than BERT as a 6-layered RoBERTa demonstrated similar
performances to DistilRoBERTa. All the layer dropping
strategies can be visualized from the above figure 16.

VI-B.2 WEIGHT PRUNING

Prior work focuses primarily on unstructured individual
weight pruning [76], [77], although effective its resulting
unstructured sparse matrices are challenging to process on
conventional hardware. This makes it difficult to secure
inference speedups despite model size reduction. Contrarily
structured pruning enforces highly structured weight
matrices which when optimized via dense linear algebraic
implementation, lead to substantial speedup but lower
performance than unstructured pruning due to greater
constraints.

VI-B.2-A STRUCTURED PRUNING

To overcome the above shortcomings, a novel structured
pruning paradigm was introduced [78] with low-rank
factorization which retained the dense matrix structure and
ly norm which relaxed constraints enforced via structured
pruning. The weight matrices were factorized into a product
of two smaller matrices with a diagonal mask that was
pruned while training via ly regularizer that controlled the
end sparsity of the model. This generic method FLOP
(Factorized LO Pruning) could be employed for any matrix
multiplication. For a neural network f (; @) parameterized by
0 = {0;}j=1 where each 6; represents an individual weight
or a block of weights (e.g., column matrix) and n denotes the
number of blocks. Consider a pruning binarized variable z =
{zj}7-1 where z; € {0,1}, @={6} denotes model
parameter set, post pruning via J) normalization.

6=00z Wj 6=63; (49)
Consider a matrix W to be factorized into a product of two
smaller matrices P and Q where W = P.Q and r is the
number of P columns or Q rows. Structured Pruning for each
component is attained via a pruning variable z,

W = PGQ = Yia1 Ze X Px X Ik) (50)

where G = diag(Z, ...,Z;)

VI-B.3 HEAD PRUNING

Though certain models have a greater dependency on
multiple heads in a multi-headed attention environment,
recent work reveals that a significant portion of attention
heads can be removed resulting in a pruned model with
enhanced memory efficiency, speed, and accuracy. Prior
work [79],[80] judged head importance via averaging the
attention weights over all heads at a particular position or
based their results on maximum attention weight values.
However, both approaches did not unequivocally consider
the fluctuating significance of different heads.

VI-B.3-A Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, Rest Can Be
Pruned
This model [81] unearthed three distinct layered head roles:
(i) Positional heads: Attending to an adjacent token (ii)
Syntactic heads: Attending to those with syntactic
dependency (iii) Rare Word heads: Indicating to least
frequent tokens in a sequence. Based on the above roles [81]
the revelations are summarized as (a) Small subset of heads
were key for translation (b) Key heads possessed a single,
often more specialized and interpretable model functionality
(c) The head roles corresponded to adjacent tokens attention
in an explicit syntactic dependency relation. High head
confidence via Layer-wise Relevance Propagation (LRP)
[82] relates to the proportion of a token’s attention defined
as the median of its maximum attention weight computed
over all tokens, which is expected to be crucial for a task.
The modified Transformer architecture via product of each
head’s computed representation head; and scalar gate gj,
MultiHead(Q,K,V) = Concat;(g;.head;)W°, where g;
are input independent head specific parameters, Lo
regularization is applied to g; for less important heads that
need to be disabled, where h € (number of heads).

h

Lo(r Gn) = >= [Iai = Ol) (51)

However, Ly norm is non-differentiable; hence it cannot be
inducted as a regularization term in the objective function.
Therefore, a stochastic relaxation is applied where each gate
g; is randomly picked from a head distribution obtained via
stretching (0,1) to (—€ ,1+6€) and collapsing the probability
distribution (—€, 1] to [1,1+€) to singular points 0 and 1.
This rectified stretching results in a distribution over [0,1]
that is mixed discretized-continuous. The probability sum of
heads being non-zero can be implemented as a relaxed LO
norm.

h
£_() = ) (A - PC gi = 018) (52)

The modified training regime can be expressed as £L(@, 0) =
Lyent (8,0) + AL-(O), where @ are original Transformer’s
parameters, Lyon,(9,0) is the translation model’s cross-
entropy loss and £-(@) is the regularizer.

15


VI-B.3-B ARE 16 HEADS REALLY BETTER THAN ONE?
In multi-headed attention (MHA), consider a sequence of n
d-dimensional vectors x = Xj,..,X, €IR*, and query
vector gq €R*. The MHA layer _ parameters
Wy, WWW € IR¢R** and W," € R?*¢n, when d;, = d.
For masking attention heads the original transformer
equation is modified as:
Na
MHaten(%, 4) = ) SrAttyrywrwrweO@ (53)
h=1
where €, are masking variables with values between {0,1},
Att,(x) is the output of head h for input x. The following
experiments yielded the best results [83] on pruning the
different number of heads at test times:

(i) Pruning just one head: If the model’s performance
significantly degrades while masking head h, then h is a
key head else it is redundant given the rest of the model.
A mere 8 (out of 96) heads trigger a significant change
in performance when removed from the model, out of
which half result in a higher BLEU score.

(ii) Pruning all heads except one: A single head for most
layers was deemed sufficient at test time, even for
networks with 12 or 16 attention heads, resulting in a
drastic parametric reduction. However, multiple
attention heads are a requirement for specific layers i.e.,
the final layer of the encoder-decoder attention, where
performance degrades by a massive 13.5 BLEU points
on a single head.

The expected sensitivity of the model to the masking € is

evaluated for the proxy score for head significance.

AL(x)
n = Ey~x Oe, (54)

= Att, (x)? i 3
In = Ex~x |Atty (x) DAtt, (x) (55)

where X is the data distribution, £(x) is the loss on sample
x. If J, is high, then modifying €, will likely have a
significant effect on the model, hence low J), value heads are
iteratively pruned out.

VI-C QUANTIZATION

32-bit floating-point (FP32) has been the predominant
numerical format for deep learning, however the current
surge for reduced bandwidth and compute resources has
propelled the implementation of lower-precision formats. It
has been demonstrated that weights and activation
representations via 8-bit integers (INT8) have not led to an
evident accuracy loss. For instance, BERT’s quantization to
16/8-bit weight format resulted in 4x model compression
with minimal accuracy loss, consequently, a scaled-up
BERT serves a billion CPU requests daily.

VI-C.1 LQ-NETS

This model [84] inducts simple to train network weights and
activations mechanism via joint training of a deep neural
network. It quantizes with variable bit precision capabilities
unlike fixed or manual schemes [85],[86]. Generally, a

quantized function can represent floating-point weights w,
activations a, in a few bits as:

Here q, and (t;,t,,,] are quantization levels and intervals,
respectively. To preserve quick inference times, quantization
functions need to be compatible with bitwise operations,
which is achieved via uniform distribution that
maps floating-point numbers to their nearest fixed-point
integers with a normalization factor. The LQ learnable
quantization function can be expressed as:
Qig(x,v) = ve, ifx € (ty ty41] (57)
where v € R* is the learnable floating-point basis and e, €
{-1,1}* for l=(1,..,L) enumerating K-bit binary
encodings from [—1,..,—1] to [1,..,1]. The inner product
computation of quantized weights and activations is
computed by the following bitwise operations with weight
bit-width K,,.
Kw Ka
Qiolw.v")"Qio(av9) =) > vive(b"Ob) (58)
i=1 j=1
where w,a € R” encoded by vectors bj’, b? € {-1,1}”%
wherei = 1,...,K, andj =1,...,K,andv” ER, v%e
R*c, © denotes bitwise inner product xnor operation.

VI-C.2 QBERT

QBERT [87] deploys a two-way BERT quantization with
input x € X, its corresponding label y € Y, via cross entropy-
based loss function

L(@) =

YD. CE(oftmax(We(Wy(.. Wi We(xD))I.D (59)
(bya

where W, is the embedding table, with encoder layers
W,, W, ...W,, and classifier W.. Assigning the same bit size
representation to different encoder layers with varying
sensitivity attending to different structures [5] is sub-optimal
and it gets intricate for small target size (2/4 bits) requiring
ultra-low precision. Hence via Hessian Aware Quantization
(HAWQ) more bits are assigned to greater sensitive layers to
retain performance. Hessian matrix is computed via
computationally economical matrix-free iteration technique
where first layer encoder gradient g, for an arbitrary vector
Vv as:

dgiv gi 7 Ov Ogt
aw, ow, “aw, ow,
where H, is Hessian matrix of the first encoder and v is
independent to W,, this approach determines the top
eigenvalues for different layers and more aggressive
quantization is deployed for layers with smaller eigenvalues.
For further optimization via group-wise quantization, each
dense matrix is treated as a group with its quantization range
and is partitioned following each continuous output neuron.

v+

v= Hyv (60)

VI-C.3 Q8BBERT
To quantize weights and activations to 8-bits, symmetric
linear quantization is implemented [88], where S* is the

16


quantized scaling factor for input x and (M = 22-1 — 1) is
the highest quantized value when quantizing to b bits.
Quantize(x|S*,M) = Clamp(|x xS*], —M, M) (61)
Clamp (x, a,b) = min (max(x, a), b)

Implementing a combination of fake quantization [89] and
Straight-Through Estimator (STE) [90], inference time
quantization is achieved during training with a full-precision
backpropagation enabling FP32 weights to overcome errors.

axt > : 7
= = 1, where x7 is the result of fake quantizing x.

Here
Vil. INFORMATION RETRIEVAL

For knowledge-intensive tasks like efficient data updating,
and retrieval, huge implicit knowledge storage is required.
Standard language models are not adept at these tasks and do
not match up with task-specific architectures which can be
crucial for open-domain Q&A. For instance, BERT can
predict the missing word in the sentence, “The ___ is the
currency of the US” (answer: “dollar”). However since this
knowledge is stored implicitly in its parameters, the size
substantially increases to store further data. This constraint
raises the network latency and turns out prohibitively
expensive to store information as storage space is limited due
to the size constraints of the network.

VII-A GOLDEN RETRIEVER

A conventional multi-hop based open-domain QA involves
question q and from a large corpus containing relevant
contextual S (gold) documents d,,..,d, that form a sequence
of reasoning via textual similarities that lead to a preferred
answer a. However, GoldEn Retriever’s [91] first-hop
generates a search query q, that retrieves document d for a
given question q, thereafter for consequent reasoning steps
(k = 2,..,S) a query q, is generated from the question (q)
and available context (d,,..,d,_1). GoldEn retrieves greater
contextual documents iteratively while concatenating the
retrieved context for its QA model to answer. It is
independent of the dataset and task-specific IR models where
indexing of additional documents or question types leads to
inefficiencies. A lightweight RNN model is adapted where
text spans are extracted from contextual data to potentially
reduce the large query space. The goal is to generate a search
query q;, that helps retrieve d, for the following reasoning
step, based on a textual span from the context C,, q is
selected from a trained document reader.

A = Gx(G, Cx) (62), Cyy, = C, concat IR,(q,) (63)
where G, is the query generator and /R,,(q,) are top n
retrieved documents via qx.

VII-B ORQA

The components reader and retriever are trained jointly in an
end-to-end fashion where BERT is implemented for
parameter scoring. It can retrieve any text from an open
corpus and is not constrained by returning a fixed set of
documents like a typical IR model. The retrieval score
computation is the question’s q dense inner product with
evidence block b.

hg = W,BERT9(q)|CLS] (64)

hy = WyBERT,(b)[CLS] (65), Sretr(b,q) = hq” My (66)
where W, and W, matrices project the BERT output into
128-dimensional vectors. Similarly, the reader is BERT’s
span variant of the reading model.

Astart = BERTg(q,b)[START(s)], (67)
Rena = BERTp(q,b)[END(s)], (68)
Sreaa(b,S, Q)MLP ([Astart; Renal) (69)

The retrieval model is pre-trained with an Inverse Cloze Task

(ICT), where the sentence context is relevant semantically

and is used to extrapolate data missing from the sequence q.
exp (Sretr (b, q))

Perth) = =——__=_ a aN (70)

a Yip! eparcu EXP (Sreer (b', q))

where q is treated as pseudo-question, b is text encircling q
and BATCH is a set of evidence blocks employed for
sampling negatives. Apart from learning word matching
features, it also learns abstract representations as pseudo-
question might or might not be present in the evidence. Post
ICT, learning is defined distribution over answer derivations.

exp (S(b,s, q))
Prearn(b, SIQ) = ——— aoa m 7)
‘earn yib'ETOP(k) dis'en’ exp (S(b', 8’, q))
where TOP(k) are top retrieved blocks based on S,;,. In
this framework, evidence retrieval from complete Wikipedia
is implemented as a latent variable which is unfeasible to
train from scratch hence retriever is pre-trained with an ICT.

VII-C REALM

This framework explicitly attends to a vast corpus like
Wikipedia however, its retriever learns via backpropagation
and performs Maximum Inner Product Search (MIPS) via
cosine similarity to chose document appropriateness. The
retriever is designed to cache and asynchronously update
each document to overcome the computational challenge of
multi-million order retrieval of candidate documents.

In pre-training, the model needs to predict the randomly
masked tokens via the knowledge retrieval relevance score
f (x, z), the inner product of vector embeddings between x
and z (MIPS). To implement a knowledge-based encoder,
the combination of input x and retrieved document z from a
corpus Z is fed as a sequence to fine-tune the Transformer
p(y | z,x) as shown in figure 17. This enables complete
cross attention between x and z that enables to predict the
output y where:

f(x,z) = Embed input (x)" Embed goc (z)

_ exp f(%z)
PZ |x) = 5 expt) (72)
PYIx)= ) ply 12x )p(z 1x) (73)
ZEZ
Like ORQA, BERT is implemented for embedding:
joingeer(X) = [CLS]x[SEP] (74)
joingerr (X41, X2) = [CLS]x,[SEP]x2[SEP] (75)

In the pre-training of the BERT’s masked language modeling
task, each mask in token x needs to be predicted as:

17


Unlabeled Text

The sun rises Ase] EAST (3)

er tec ae

| —
m8) 7 Encoder () :
ry

How many degress H

ina circle? (x) 360 degress (y)

Input Query

FIGURE 17. Unsupervised pre-training (top) and Supervised fine-tuning
(bottom) in REALM’s architecture

Ix

p(y |z,x) = I] p(y; | zx) (76)
j=l

P(y; | Z,x) «

exp (wf BERT yas) (joingeer (x, Zroay))) (77)

where BERTyasx(j) tepresents the Transformer output
vector corresponding to the j“” masked token. J, is the total
number of [MASK] tokens in x, and w; is the learned word
embedding for token y,;. For an open-ended Q&A fine-
tuning task, answer y is in the form of a spanned token
sequence in the target document z. The span set S(z,y)
matching y in z can be modeled as:

poy lz.x) x )° exp(MLP([hsraerc; Rewors])) (78)

seS(z,y)
Asrarr(s) = BERT srarr(s) (oingerr (x,Zpoay)) (79)
henp(s) = BERTgnp(s) (joingerr (x, Zpody)) (80)
where BERTsrarr(s) and BERTgypcs) denote the

Transformer output vectors corresponding to the start and
end tokens of span S and MLP denotes a feed-forward neural
network.

ViI-D RETRIEVAL AUGMENTED GENERATION: RAG
RAG is a flexible combination of the ‘closed-book’ i.e.,
parametric model and the performance of ‘open-book’ i.e.,
retrieval model approaches, outperforming current language
models. A parametric memory is a sequence to sequence pre-
trained model whereas a Wikipedia representation via a
dense vector index constitutes non-parametric memory,
which is accessed via a pre-trained neural retriever. Since
RAG is built as a culmination of the two it does not require
prior training since knowledge is available via retrieved pre-
trained data unlike former non-parametric architectures [92].
To achieve greater context in output sequence (y)
generation, the general-purpose RAG incorporates retrieved
text passages z for a given input x, that involve two major
components:

(i) Retriever py (Zz | x), parameterized via I], it returns the
top matched content from text passages for query x, this
RAG Sequence architecture’s retrieved passage acts as a

latent variable marginalized to achieve maximum probability
p(y | x) across top-K approximations.

Prac—sequence |x) =

pzix) | [reoitxzrus BD

z€top—k(p(.|X ))

(ii) Generator pg(y; | X,2,¥1.4-1), parameterized via 8, it
generates the current token y; based on a contextual
representation of the prior i — 1 tokens y,.;_,, input x and
retrieved passage z. The RAG Token model predicts each
target token based on a different latent passage,
simultaneously enabling the generator to select subject
matter from various documents.

PRAG-Token (y |x) =

Py(Zi | X)PeQ% |X, Zi Vrsi-1) (82)
i zétop—k(p(.|X ))
The retrieval module py(z | x) is based on Dense Passage
Retrieval (DPR) where d(z) the document’s dense
representation generated via BERT and q(x) the query
representation generated via another BERT.

py(z 1x) x exp(d(z),q(x)) (83)

End-to-End Back Propagation via q and Pg

Query Retriever P, Document \f Generator Po

Encoder (Non Parametric) Index

Q&A: Answer Generation

The sun sets in the west. (x)

Fact Verification: Fact Query

Fact Verification:
Label Generation

Scientific Question
Generation: Answer Query

Question Generation

FIGURE 18. RAG’s Parametric and Retrieval Model architecture

To effectively compute top — k(p,(.| x )) elements z with
the highest probability py(z |x) DPR employs a MIPS
index where BART is used as the generator pg(y; |
X,Zi,¥14-1). The retriever and generator are trained in
conjunction to retrieve the target document in a semi-
unsupervised manner.

VIIl-D DENSE PASSAGE RETRIEVAL: DPR

DPR enhances open-domain QA retrieval using the dual
encoder approach, unlike the computationally intensive ICT.
Its dense encoder Ep(:) indexes all M passages in a
continuous, low-dimensional (d) space that could
effectively retrieve top relevant passages for a query at run
time. A separate encoder E9(-) is deployed for the query and
d-dimensional vector to map at run time, that retrieves k
passages which are most relevant to the question vector. The
dot product computation between the query and passage
determines their similarity. sim(q,p) = Eg(q)". Eg (q). The
goal is to learn a superior embedding function via training
encoders that involve the creation of vector space where the
relevant question, passage pairs possess smaller distances
i.e., greater similarity than irrelevant ones. Assume training
data with m instances D = {(y;,P} Pia, —— Pin )}e1

18


where each instance contains one query q;, one positive
(relevant) passage p;* with n negative (irrelevant) passages
Pi,;- The loss function can be optimized as the negative log-
likelihood of the positive passage.

Ep PP Phot) =

esim(qip;)
—log ———_____—_—_—__~ (84)
esim(aip}) ae an sim( 4 ;;)

Vill. LONG SEQUENCE MODELS

Vanilla Transformers break input sequences into chunks if
their length exceeds 512 tokens, which results in loss of
context when related words exist in different chunks. This
constraint results in a lack of contextual information leading
to inefficient prediction and compromised performance and
dawned the rise of such models.

ViIl-A DEEPER SELF-ATTENTION

This 64 layered Transformer [93] was built based on the
discovery that it possessed greater character level modeling
of longer-range sequences. The information was swiftly
transmitted over random distances as compared to RNN’s
unitary step progression. However, the three following
supporting loss parameters were added to the vanilla
Transformer which accelerated convergence and provided
the ability to train deeper networks.

Med
ed

t t t t

0 1 2 3
FIGURE 19. Accelerated convergence via multiple target token
prediction across multiple positions through intermediate layers

(i) Prediction across Multiple positions: Generally
causal prediction occurs at a single position in the final
layer, however in this case all positions are used for
prediction. These auxiliary losses compel the model to
predict on smaller contexts and accelerate training
without weight decay.

(ii) Predictions on Intermediate Layer: Apart from
the final layer, predictions from all intermediate layers
are added for a given sequence, as training progresses,
lower layers weightage is progressively reduced. For n
layers, the contribution of /“” intermediate layer ceases
to exist after completing [/2n of the training.

(iii) Multiple Target Prediction: The model is modified
to generate two or greater predictions of future
characters where a separate classifier is introduced for

every new target. The extra target losses are weighed
in half before being added to a corresponding layer
loss.
The above 3 implementations are expressed in figure 19. For
sequence length L, the language model computes joint
probability autoregressive distribution over token sequences.
L

PCton) = Plto)| | Petit toa) (85)

VIII-B TRANSFORMER-XL

To mitigate context fragmentation in vanilla Transformers,
XL incorporates lengthier dependencies where it reuses and
caches the prior hidden states from where data is propagated
via recurrence. Given a corpus of tokens x =
(X1,X2...,Xr), a language model computes the joint
probability P(x) autoregressively, where the context xe; is
encoded into a fixed size hidden state.

pax) =| [PG xe) (86)

¢ ~s a. - .
(® @ © O90 0 0.0 @
%

[@)
(2)
®
e
@
@

© 09 00.0
%

fe)
fe)
fe)
)
@
fo)

10 COD 0-00 0 KS ee

@

1 1 1
@ 0 0 M19: ,0. 0 oe y © DOO. © OF,9' 0 0.0
V V

~©
°
x @
fe}
°
x Q
Q
°
e
x ©
a)

Ix x, x; Xyy LA x, LF 2 3 aos 6 3
Wis ee ae o . BN a 4y

i 2 A,
New Segment |

(a) Attention Caching -1

10 0 07OW0 000 0
x, x,

(b) Attention Caching - II

@® @ 8 ® @® © © ® ® 9 9

@:' @

@:' @

@: @: oo vu
% Me “¢ a5 he % Me % Xo X11 X12

+ Cached (I, Il) Extended L
1 Context |

FIGURE 20. Elongated context capture combining (a) and (b)

Assume two consecutive sentences of length L, s, =
[Xo Xz] and Sp44 = [Xp444) +) Xr41z] where n“” layer
hidden state sequence produced by the t“” segment s, as
h™ € R4*4, where d is the hidden dimension. The n‘”
hidden layer state for the segment s,,, is computed as
follows:

het? = [SG(AF~*) © hese (87)
Grea roa Urai = Aps1Wg beet We het Wy (88)

hy, = Transformer — Layer (q7r41,kr41) Ura.) (89)

where SG(-) represents stop-gradient, [h, ¢ h,| is the two
hidden sequence concatenation, and W the model
parameters. The key distinction from the original
Transformer lies in modeling the key k7},, and value v7,

19


concerning the extended context h7]';* and hence preceding

h®-1 are cached. This can be demonstrated from figure 20

above where prior attention span is cached by the latter

forming an elongated caching mechanism.

Such recurrence is applied to every two consecutive

segments to create a segment level recurrence via hidden

states. In the original transformer the attention score within

the same segment between query (q;) and key (k;) vector is:

Ags = Ex,WlW,.Ex; + Ex,We WU; + (90)

UP WI W,.Ex; + UP Wa WU;

From a perspective of relative positional encoding, the above

equation is remodeled in the following manner

AS = EX WE Wig Ex; + Ex,Wo WerRi-j + (91)
u! Wy, g Ex; + v' WepRi-j

Vill-C LONGFORMER

This architecture provides sparsity to the full attention matrix

while identifying input location pairs attending one another

and implements three attention configurations:

(i) Sliding Window: For a fixed window size w, each
token attends to a sequence length (n) of w/2 on either
side. This leads to the computational complexity of
O(n Xw) that scales linearly with input sequence
length and for efficiency purposes w < n. A stacked 'l'
layered transformer enables receptivity sized 'l x w'
over the entire input ‘w’ across all layers. Different ‘w’
values can be chosen for efficiency or performance.

(ii) Dilated Sliding Window: To conserve computation
and extend the receptive field size to ‘lxdxw’,
where ‘d’ variable-sized gaps are inducted for dilations
in window size 'w’. Enhanced performance is achieved
via enabling few dilation-free heads (smaller window
size) for attention on local context (lower layers) and
remaining dilated heads (increased window size)
attending longer context (higher layers).

(iii) Global Attention: The prior two implementations do
not possess enough flexibility for task-precise learning.
Hence “global attention” is implemented on few pre-
designated input tokens (n) where a token attends to
all sequence tokens and all such tokens attend to it. This
preserves the local and global attention complexity to
O(n).

Its attention complexity is the sum of local and global

attention versus ROBERTa’s quadratic complexity which is

explained by the following mathematical expressions.

nl th

(a) Full Attention —_(b) Sliding Window (c) Dilated Sliding (d) Global Attention

FIGURE 21. Longformer’s different Sparse Attention configurations

local attention = (n X w)

where n € (input sequence size),w € (window size)
global attention = (2xn Xs)
where s € (number of tokens with full attention)
Window Attention Size = no, hence (np = w)
Total attention complexity = n(nyp + 2s) € O(n)
ifnp #n
Total Memory Requirements =
n(Ny + 2s) X Number of Transforer Layers
Global attention enables chunk-less document processing,
however, its space-time complexity will be greater than
RoBERTa, if sequence length exceeds the window size.
(0(RoBERTa) = O(m)"} < O(Long former)
° ap Sy = O(n(n + 2s))
ifn>no

ViIl-D EXTENDED TRANSFORMER CONSTRUCTION:
ETC

ETC is an adaptation of the Longformer design which
receives global (n,) and long (n;) inputs where ng « n). It
computes four global-local attention variations: global-to-
global (g2g), global-to-long (g21), long-to-global (12g), and
long-to-long (21) to achieve long sequence processing.
Global inputs and the other three variations possess limitless
attention to compensate for [2's fixed radius span to achieve
a balance between performance and computational cost.
Further, it replaces the absolute with relative position
encodings which provide information of input tokens
concerning each other.

VII-E BIG BIRD

Mathematically Big Bird proves randomly sparse attention
can be Turing complete and behaves like a Longformer aided
with random attention. It is designed such as (i) a global
token group g attending to all sequence parts (ii) there exists
a group of r random keys that each query q; attends to (iii) a
local neighbor window w block that each local node attends
to. Big Bird’s global tokens are constructed using a two-fold
approach (i) Big Bird-ITC: Implementing Internal
Transformer Construction (ITC) where few current tokens
are made global that attend over the complete sequence. (ii)
Big Bird-ETC: Implementing Extended Transformer
Construction (ETC), essential additional global tokens g are
included [CLS] that attend to all existing tokens.

Its definitive attention process consists of the following
properties: queries attend to r random keys where each query
attends to w/2 tokens to the left and right of its location and
have g global tokens which are derived from current tokens
or can be supplemented when needed.

IX. COMPUTATIONALLY EFFICIENT ARCHITECTURES
IX-A SPARSE TRANSFORMER

This model’s economical performance is due to the
alienation from the full self-attention procedure that is
modified across several attention steps. The model’s output
results are derived from a factor of the full input array i.e.,
(VN) where N € Sequence Length as expressed in Figure
22. This leads to a lower attention complexity of O(NVN) in

20


contrast to Transformer’s O(N*). Further, it deciphers
sequences thirty times longer than its predecessors. Its
factorized self-attention consists of p distinct heads where

the m*" head defines a subset of attention indices Agee Cc
{j: j <i} and to generate sparsity |” | « “/n leads to
efficient choices for set A.

—

—'——_——_,
Normalization

dropout

Embedding

(b)

Normalization

———————————

dropout

(c)

Normalization
softmax

(a) (a)

FIGURE 22. (a) Sparse Transformer Architecture (b) Decoder based Full
Attention with Causal Masking (c) Stridden Sparsity (d) Fixed Sparsity

The strided attention is implemented in two dimensions
where one head attends to previous | locations and the other
attends to each I" location, where stride | value is close to
Vn. This is expressed as ae ={t,t+1,..,i} for t=
max(0,i—l) and A‘ = {j:(i—j) mod 1 = 0}. This
linear transformation leads to this dense attention:
Attention(X) = W,.attend(X,S) (92)
where W, is the post attention matrix. Similarly, to
implement factorized attention heads, one attention type is
used alternatively per residual block or interleaved or a
hyperparameter determines the ratio.

Attention(X) = W,.attend(X, AW ™4P)) (93)

where ris the current residual block index and p is the
factorized headcount. An alternative merged head approach
incorporates one head attend to target locations where both
factorized heads would attend to. This approach is
computationally more expensive by a constant factor.

Pp
Attention(X) = W,.attend| X, U Am (94)

m=1

A third alternative uses multiheaded attention, where
attention products (n,) are parallelly computed and
concatenated along the feature dimension.

Attention(X) = W,(attend(X, A)')ie(1——np) (95)
Multiple heads gave superior results whereas, for longer
sequences where attention determines computation,
sequential attention is preferred.

IX-B REFORMER

Reformer reduces the Transformer attention complexity to
OCL log L) via local sensitive hashing (LSH). This assigns
each vector x to a hash h(x), where neighboring vectors
obtain the same hash within hash buckets of similar size with
high probability and remote ones do not. The modified LSH
attention equation:

0; = >, exp (a. ky — (i, P,)) v; (96)
JEP;

where P; = {j:i = j}

P. belongs to the set where i*” position query attends to, z is

the partition function that contains a range of nearby keys to

which a query attends to. For batching purposes, attention is

performed over PB. = {0,1 — —I} > P, where P, is a subset of
P and elements not in P; are masked.
0; = Lyep, xP (qi-k; — mG, Pi) — 2G Pi))vy (97)

. 00, ifj € P;
h »P)= '
where m(j, Pi) Lo otherwise
Decoder implements masking to prevent access to future

query positions. The set P; target items can only be attended
by a query at i‘” position, by enabling attention within a
single hash bucket. To further reduce the probability of
similar items falling in different buckets, several parallel
hashing (N;ounas) is performed with distinct hash functions
{hO, h®, .}
Nrounds
P= U P;" where P,” =

{jh qi) = h(q;)} (98)

Attention is done on chunks of sorted keys queries and keys

to batch:
(r) (r) (r)
or) _ J. |i} _ “I Sj
B = {7 [E] r<[Z = [<]} (99)

From (96) and (97) we can write,

= Drag exp (2(i, Pi —

. 1 -
2 Pi) Djeplr) 5 OP (ai kj — MU, (101)
PP) = 2G PONY;
= yrrounds exn (2(i, P/ — 2(i, P,)) of” (102)
0? =
Deni EXP (Gi kg — ME, PLO) —2GR))vj (103)

21


The following example in figure 23 comprehensively
demonstrates the various working mechanisms of the
Reformer.

ql q2 3 4

Sequence kl @ e
of Queries =
Keys k2
LSH k3 e e
a jai ka e e
— qi q3 q2_ 4
chunk to
parallelize | hnksorted sequence | chunk sorted sequence 2 [chunk sorted sequence 3 | ca
A A A A AA A A A A A ry

(a) (b)

ql q2 q3 q4 q5 q6 q7 qs qo qiO0 qil qi2
k1 @ @ e@ @ @
k2 e e e e
k3 @ e@ @ co} @
ka @ @ @
ks @ @ e@ @ @
k6 e e e e
k7 e e e e
ke le C ° ° e
ko e@ | @ @
k10 = | = °
kil @ e@ @ e@ e@
k12 e e e e
(c)
ql_q3 5 q7 ql0 q4 q9 qll q2 q6 q8 qiz gi _q3 457 qi0_q4.q9 gli _q2_q6 8 qi2
kl Je lelele ql bs ee DF i. >
wo elelelele 31 |e e@ | i . 1
i olelelele| 5 H . o i e
ke @ |e © © @ a8 q elle. 1
<a = al l
uilelelelele qu te '
ka bl !
Co '
1
1

(d)

FIGURE 23. (a) Bucket formation of similar Attention vectors (b) Simple
bucketing of a Query-Key pair (c) Query-Key sequence distribution based
on (a) before Bucketing (d), (e) Bucketing and Chunking of (c)

Reversible Residual Networks [94] is another driving force
behind Reformer’s economical memory consumption where
activation values are reconstructed on the fly during
backpropagation excluding requirements to save activations
in memory. From figure 24 below each layer’s reversible
block are recomputed from the next layer’s activations as:

Y,=X,+f (Xorayer 2) Ye =X,+f (Xs payers) (104)

=X — f (Yorayer ») Xp = 1-7 (Yanaper :)
Xizy (@ = ¥, X, « &)
wo a
f(Layer1)
f(Layer2) f(Layer2)
X,-2 (a) >% X,< a a
ww \ id

(105)

FIGURE 24. Rev-Nets skipping intermediate storage via precomputation

IX-C A LITE BERT: ALBERT
ALBERT, within a single model, integrates the following
two-parameter reduction techniques that result in a mere
12M parameters as shown in figure 25. This results in almost

90% parameter

reduction

than

BERT-base

while

maintaining competitive benchmark performances.

(i) Factorized Embedding Parameterization:

For

optimal results, NLP tasks require a large vocabulary V,
where (embedding size) E =H (hidden layer) and
embedding matrix V xX E size can scale up to billion
parameters. ALBERT factorizes the embedding space E
into two smaller matrices where embedding parameters
are reduced from O(V X H)toO(V X E+ E x H).

Cross — Layer Parameter Sharing:

ALBERT is

built to share attention parameters across layers via a
feed-forward network (FFN). Consequently, its inter-
layer transitions were considerably smoother as results
indicated weight sharing’s stabilizing effect on network

(ii)
parameters.

! embedding !
' Parameter| |
! \Reduction 1
v ‘ vi
@ . (e
e Ve

=~
»
=

©
oO

BERT Layer24_ | ALBERT Layer26 |<

4 4
'
'
I
I

'
{ peRttayer2 | { ABeRTLayert) |< >

© cm) Gaon:

(b)

FIGURE 25. (a) Smaller model via Embedding size reduction (b) Effective
learning via sharing of Attention parameters

TRAINING CORPUS —
Sentence 1, Sentence 2
Sentence 3, Sentence 4

°

a nn 2
__ INPUT DOCUMENT TO BERT _ CLASSIFIER

NSP

Sentence 2, Sentence 4 o----} > NO |
| Sentence n-3, Sentence n-2°}> YES

(a)

SENTENCE 1

Cy

s

ORDER SWAPPED
(INTER SEQUENCE

a
:
¢
a

“ee

COHERENCE) %
’

ALBERT’S

SOP LOSS.

And | got a high tech job | YES |
ap

_ And | got a high tech job NO)

(b)

FIGURE 26. (a) BERT’s NSP learning via simple non-reversed pair order
(b) ALBERT’s SOP dual sentiment learning via sentence order reversal.

Like BERT’s NSP, ALBERT’s sentence-order prediction
(SOP) loss incorporated two-pronged learning from two

22


positive successive text segments that also included its
corresponding negative samples with orders reversed as
demonstrated in figure 26. This influences the model to learn
contextually the finer-grained discrepancies in any discourse
giving superior coherent performances. Its MLM target
implements n-gram masking that comprises up to 3-
character sequences, like “World Cup Football” or “Natural
Language Processing”.

1/n
= 106
pn) = (106)
IX-D ELECTRA

The advantage lies in its contextual learning via effective
discrimination, where it learns from all. input tokens unlike
BERT’s that learn from a mere 15% masked-out subset.
ELCTRA implements “replaced token detection”, as shown
in figure 27, where contamination occurs by replacing few
random tokens with probabilistic meaningful substitutions
via Generator(G), a small ‘masked language model’.

- > original

the —» [MASK]__»

wie wae Gneratol ee

= > original

taught > [MASK] —» - > replaced

the —> the —>

P tested —>
(A smaller Masked — > original

Language Model)

the ——>
student —» student—» student —_»

NS

=> original

FIGURE 27. Replaced token detection via model’s combined training

Simultaneously, via binary classification, a larger model
Discriminator (D) is jointly pre-trained to predict if each
token was restored correctly via the generator.
Luim (X, 9g) = E(Xiem —log pg (xi|x™*""))
Loise (x, a) =

E > —1( x00" = x,) log D(x", t) — 10x40"
t=1

(107)
(108)

# x) log (1 — D(x""," »)

The two encoder-based networks (G, D) transform an input
token sequence x = [x,,..,X,] into a contextualized vector
representation h, = [h,,..,h,]. Via Softmax, G yields the
likelihood of generating a t‘” position token x,, where x; =

[MASK].

_— exp(e(xe)hg(x)t)
Pe (xt | x) _ Ey exp(e(x’)?hg(x)t)
The combined loss over a large corpus y is minimized as:
bnbp dixex Lyuim (x, 9G) + AL pisc (x, 9p) (110)

(109)

IX-E LINFORMER

It demonstrates [95] that attention weights are dominated by
a few key entries, hence sequence length is down projected
to a target output matrix via low-rank self-attention that
achieves linear time and space complexity O(1). During
computation of keys and values, two linearly projected
matrices are added E,,F; € R"**, where (n x d)—
dimensional key, value layers KW and VW,’ are projected

into (k xX d)— dimensional key, value layers, thereafter
resulting (n x k)— dimensional context mapping is
computed using scaled dot-product attention.

head; = (111)
P:nxk
W,2(E;KWX)?
softmax (ee) (FVW," 4
k

Ifk << n, then a significant reduction of memory and space
consumption is achieved. For further efficient optimization,
parameter sharing between projections is performed at three
levels: (i) Headwise Sharing: for each layer two projection
matrices E and F are shared where E; = E, F; = F through
all heads i. (ii) Key-Value Sharing: including (i) key, value
projections are shared where each layer’s single projection
matrix E = E; = F; is created for each key-value projection
matrix for all heads i (iii) Layer-wise Sharing: a single
projection matrix E implemented for all layers, heads, keys,
and values. For a 12-layer, 12-head Transformer, (i), (11), (iii)
will incorporate 24, 12, 1 distinct linear projection matrices,
respectively.

IX-E PERFORMER
The standard attention (Qzxq-Kdx,)-Vixq results in
quadratic time complexity of O(I?d), preferable

implementation of Qiyq-(Kdxr-Vixa) leads to O(d?L)
where L > d. However, attention decomposition of query-
key product into its pristine form is not possible after
implementing the softmax non-linear function. However, pre
softmax decomposition of attention is possible via
approximation of lower-ranked queries and keys enabling

T
greater efficiency, specifically Q’K'? = softmax(<— =

exp (QK"). This is achieved via kernel approximation
function K (x, y) = 0(x)"@(y), the dot product of a high-
dimensional feature map @. Contrary to the kernel trick
where the dimensionality is increased, the Performer [96]
decomposes the attention matrix A(i,j) = K(qj,k;) =

exp(qi, kj") to a lower-dimensional feature map @.

X. MODELING CLASSIFICATION OF LMs

Transformer based language models (LM) can be classified

into 3 categories [97] from a modeling perspective:

(i) Autoregressive: These are pre-trained feedforward
models that predict future tokens from token history.
Here output y; is dependent on the input at time instant
x, and previous time step inputs x-,. These are primarily
decoder-based Transformers that incorporate causal
masking where attention heads are prevented from
attending to future tokens. Such models are generally
fine-tuned for text generation purposes and deploy zero-
shot learning in the GPT series.

(ii) Auto-Encoded: These Encoder based models have full
access to the input array, devoid of any masking. To
learn they are pre-trained via incorporating input token
masking schemes and then fine-tuned to reproduce the
masked tokens as output. These models (BERT) are

23


generally appropriate for token
classification tasks.

(iii) Sequence to Sequence: These Encoder-Decoder-based
generative models create data post learning from a
massive dataset. Unlike discriminative distribution
P(Y|X), they model the joint distribution P(X,Y) of
input X and target Y where input can be corrupted on
several schemes. Decoder-based causal masking is
deployed to maximize learning for subsequent target
generation. Models like BART and TS perform best on
NMT, summarization, or QA tasks.

A comprehensive overview of the above-mentioned

modeling classification is presented in figure 29.

sequence or

Xl. LANGUAGE MODEL PERFORMANCE COMPARISON
The quantitative performance of few major NLP models is
shown in figure 28 that is based on the Glue and SuperGlue
benchmarks. These benchmarks contain a variety of datasets
that judge the model on several NLP tasks. With the highest
number of trainable parameters, GPT-3 is the largest model
in this comparison. Since GPT-3 is the newest model here, it
does not participate in the older Glue benchmark.

From a qualitative perspective, the T5 within the same model
uses the same loss function and hyperparameters spread
across a variety of tasks leading to a multi-task learning
environment. It performs the best as this scalable text to text
generative (NLG) model couples the denoising objective
during its training with massive amounts of unlabelled data.
This leads to superior learning and greater generalized
performances over NLU models like RoBERTa which are
fine-tuned for individual downstream tasks after pre-
training.

SuperGlue * Glue

Parameters (Millions)

RoBERTa GPT-3 (few- BERT
shot)

NLP BASE MODEL PERFORMANCE BENCHMARK

FIGURE 28. Graphical Representation of Language Model Performance

The primary motive of several rounds of fine-tuning in NLU
models is to achieve strong performance on multiple tasks.
The major disadvantages are the requirement for a new and
typically large dataset for each task. This amplifies the
potential for poor out-of-distribution generalization leading
to unfair comparison with human-level abilities. GPT-3 does

not operate on fine-tuning as its focus is to deliver task-
agnostic execution. However, there is the scope of minimal
fine-tuning in GPT-3 which leads to one or few-shot
learning. The idea is to perform zero or minimal gradient
updates post pre-training a huge model on a massive dataset.
Though GPT-3 does not rank highly with the SuperGlue
benchmark, the key is that this generative model is the
quickest in learning any task at inference time. It matches
performance with SOTA fine-tuned models on several NLP
tasks in the zero, one, and few-shot settings. It also generates
high-quality samples and gives a solid qualitative
performance at tasks defined on the fly.

XII. CONCLUSION AND FUTURE DIRECTIONS

We provide a comprehensive and detailed summary of the
major language models that have led to the current SOTA in
NLP performance. Since the launch of the Attention
mechanism and Transformer architecture, NLP has advanced
exponentially. We presented a high-level mind map of model
classifications via a taxonomy. These classifications are
primarily based on Transformer derivative architectures,
built for specialized tasks like Language Understanding and
Generation, Model Size Reduction via Distillation,
Quantization and Pruning, Information Retrieval, Long
Sequence Modeling, and other Generalized Model
Reduction techniques. Recent language models are primarily
driven by attaining higher NLP performance requiring huge
computing resources. Thus, model scaling has been the
natural pathway in industry. This exponential scaling
coupled with higher attention complexity makes these
models infeasible to access at a global scale. Subsequently,
significant efforts have been made to engineer reasonably
sized models and an efficient attention computation to speed
up model convergence leading to lower latency in models.
Incorporating a Mixture of Expert (MoE) [98] methodology
is an effective way for large models to achieve computational
efficiency, as only a subset of the neural network is activated
for every input. Consequently, this leads to sparsity, and
although sparsity training is an active research area, current
GPUs are better suited for dense matrix computations. While
MoE models have demonstrated promise in training sparse
matrices, their communication costs and complexity impede
wide-scale deployment. Further, larger models are prone to
memorize training data leading to overfitting and reduced
learning [99]. To overcome this, models are only trained for
a single epoch on de-duplicated instances on huge datasets,
thereby exhibiting minimal overfitting.

Thus, MoE design coupled with a robust training paradigm
in the future might lead to highly scalable and efficient
models. These models will possess superior language
understanding, as data memorization would be minimized.
The current approach in SOTA models relies on supervised
learning on huge datasets. A promising area of future
enhancements in NLP would be incorporating reinforcement
learning in Machine Translation, text summarization, and
Q&A tasks.

24


LANGUGAE
MODEL DESCRIPTION TASKS MODELING
TYPE
OR NI Beading Autoregressive
GPT-l, Il, Ill Unsupervised pre-training on large datasets Comprehension, Text jecoDER eee
Autoregressive Language Modeling and Causal Masking Summarization, Common
: Transformer
Sense Reasoning, Zero-Shot
XLNET Greater Contextual Learning via Factorized Ordering on | Reading Comprehension, Autoregressive
Input’s Sequence Length Natural Language Inference, DECODER based
Bidirectional Contextual Language Modeling Sentiment Analysis, Q&A Transformer
Attenti ia Local Sensitive Hashi duci
ate via Local Sensitive Hashing reducing memory Reduced Attention Complexity ee
enabling lengthy sequence
REFORMER Incorporates re-computation of weights and activations Feces e nee DECODER based
bypassing their respective storage via reversible residual . Transformer
networks memory requirements
Sparsity in Attention Matrices for lengthy sequence
speedup and efficient computation ; Autoregressive
LONGFORMER Localized Attention for nearby tokens and all access Co-reference Resolution, OBA, DECODER based
Globalized Attention for few preselected tokens to Document Classification. Transformer
enhance receptivity
Auto-encoded
BERT Deep Bidirectional Contextualization Sentence Classification, Q&A, ENCODER based
asked Language Modeling (MLM) for continual learning Natural Language Inference, Transformer
F : : : : Auto-encoded
Diverse learning via Dynamic Masking, where tokens are : :
RoBERTa masked differently for each epoch scrilinent Atalysis, OSA, ENCODER based
arger pre-training Batch-Size Natural Language Inference geo er
Produces similar target probability distribution as its larger ; oo,
eachermodel, BERT Semantic Textual Similarity, Auto-encoded
See Generates cosine similarity between student and teacher Setnanuc Relevance, Oe, EMER Bae
models hiddentetates Textual Entailment Transformer
Smaller and efficient model via Embedding Parameter ; ;
Reduction, i.e., Factorized Parametrization Reading dat al Auto-encoded
ALBERT Layers split into groups via Cross-Layer Parameter Sharing Seman Wexta| Simian, ENCODER based
reducing memory footprint Q&A, Language Inference Transformer
Predict if the re-generated corrupted token is original or plovide> competitive .
ee GEE Vis crema GenereiGr performances on Sentiment Auto-encoded
ELECTRA p : P 8 oo . , Analysis, Natural Language ENCODER based
Effective and low-cost discriminative learning via replaced
renderer Inference tasks at 25% Transformer
oken detection compute
Superior sequence generation quality via greater noising | Supervised and Unsupervised ;
Wea een variations multi-lingual Machine Cenerotive equence
ee Flexible denoising autoencoder acts as a language model in | Translation, Q&A, Semantic EG ton a
severest noising scheme Equivalence ransformer
Positional Encodings learned at each layer for greater | More Diverse and Challenging ene
15/mTS semantical performance coreference, entailment, Q&A 15 i; e
aa Transforms all tasks in a text-to-text format to incorporate | tasks via SuperGLUE eg raison ee)
most NLP task varieties. pencnman ransformer
FIGURE 29. Tabular Representation of Language Modeling Classification
REFERENCES . . . ae
[1] LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436- [4] Devlin, J., Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
“444 (201 5) > , , “BERT: Pre-training of Deep Bidirectional Transformers for
[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Language Understanding. aed elaieT: ee: 5 3
Jones. Aidati. N’ Gottex’ Lukas Kaiser. and Ilia Pologukhia [5] Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer
dé Auention is all youtes a”. NeurIPS (2017) , Levy, and Samuel R. Bowman. “GLUE: A Multi-Task Benchmark
[3] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, and Analysis Plationn iat Natural

“Improving language understanding with unsupervised learning”,
Technical report, OpenAI Blog, 2018

[6]

Language
Understanding.” BlackboxNLP@EMNLP (2018).

Wang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh,
Julian Michael, Felix Hill, Omer Levy and Samuel R.

25


7]

8]

9]

10

=

11]

12]

13]

15]

16]

17]

18]

[19]

20]

21]

23]

24]

25]

26]

27]

28]

Bowman.“SuperGLUE: A Stickier Benchmark for General-Purpose
Language Understanding Systems.” NeurlIPS (2019).

S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark
analysis of representative deep neural network architectures,” IEEE
Access, vol. 6, pp. 64 270-64 277, 2018.

Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
“SQuAD: 100, 000+ Questions for Machine Comprehension of
Text.” ArXiv abs/1606.05250 (2016)

Rajpurkar, P., Jia, R., & Liang, P, “Know What You Don't Know:
Unanswerable Questions for SQUAD”. ArXiv, abs/1806.03822, 2018
Warstadt, Alex, Amanpreet Singh, and Samuel R. Bowman. “Neural
Network Acceptability Judgments.” Transactions of the
Associationfor Computational Linguistics 7 (2019): 625-641.
McCoy, R. Thomas, Junghyun Min and Tal Linzen. “BERTs of a
feather do not generalize together: Large variability in generalization
across models with similar test set performance.”
ArXiv abs/1911.02969 (2020)

ElSahar, Hady and Matthias Galle. “To Annotate or Not? Predicting
Performance Drop under Domain Shift.” EMNLP/IJCNLP (2019).
Ruder, Sebastian and Barbara Plank. “Learning to select data for
transfer learning with Bayesian Optimization.” EMNLP (2017).

14] Yang, Z., Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov,

and Quoc V. Le. “XLNet: Generalized Autoregressive Pretraining for
Language Understanding.” NeurIPS (2019).

Liu, Y., Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin toyanov.
“RoBERTa: A Robustly Optimized BERT  Pretraining
Approach.” ArXiv abs/1907.11692 (2019).

McCann, B., N. Keskar, Caiming Xiong, and R. Socher. “The Natural
Language Decathlon: Multitask Learning as Question
Answering.” ArXiv abs/1806.08730 (2018).

Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
Narang, M. Matena, Yanqi Zhou, W. Li, and Peter J. Liu. “Exploring
the Limits of Transfer Learning with a Unified Text-to-Text
Transformer.” J. Mach. Learn. Res. 21 (2020): 140:1-140:67.

Lewis, M., Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, A.
Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
“BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and
Comprehension.” ArXiv abs/1910.13461 (2020)

Liu, Yinhan, Jiatao Gu, Naman Goyal, X. Li, Sergey Edunov, Marjan
Ghazvininejad, M. Lewis and Luke Zettlemoyer. “Multilingual
Denoising Pre-training for Neural Machine
Translation.” Transactions of the Association for Computational
Linguistics 8 (2020): 726-742.
C. Rosset, “Turing-nlg: A 17-billion-parameter language model by
Microsoft,” Microsoft Blog, 2019
Xie, Ziang, Guillaume Genthial, S. Xie, A. Ng and Dan Jurafsky.
“Noising and Denoising Natural Language: Diverse Backtranslation
for Grammar Correction.” NAACL-HLT (2018).

22] Brown, T., B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla

Dhariwal, Arvind Neelakantan, A. Radford, Ilya Sutskever, and Dario
Amodei. “Language Models are Few-Shot
Leamers.” ArXiv abs/2005.14165 (2020)

Lepikhin, Dmitry, H. Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,
Y. Huang, M. Krikun, Noam Shazeer and Z. Chen. “GShard: Scaling
Giant Models with Conditional Computation and Automatic
Sharding.” ArXiv abs/2006. 16668 (2020)

Strubell, Emma, Ananya Ganesh, and A. McCallum. “Energy and
Policy Considerations for Deep Learning in
NLP.” ArXiv abs/1906.02243 (2019)

Hinton, Geoffrey E., Oriol Vinyals, and J. Dean. “Distilling the
Knowledge in a Neural Network.” ArXiv abs/1503.02531 (2015)
Sanh, Victor, Lysandre Debut, Julien Chaumond and Thomas Wolf.
“DistiIBERT, a distilled version of BERT: smaller, faster, cheaper,
and lighter.” ArXiv abs/1910.01108 (2019)

Jiao, Xiaoqi, Y. Yin, L. Shang, Xin Jiang, X. Chen, Linlin Li, F. Wang,
and Qun Liu. “TinyBERT: Distilling BERT for Natural Language
Understanding.” ArXiv abs/1909.10351 (2020).

Sun, Zhiqing, H. Yu, Xiaodan Song, Renjie Liu, Yiming Yang and
Denny Zhou. “MobileBERT: a Compact Task-Agnostic BERT for
Resource-Limited Devices.” ACL (2020).

[29

[30

[31

[32

[33

[34

[36

[37

[39

[40

{41

[42

[43

[45

[49

Han, Song, Huizi Mao and W. Dally. “Deep Compression:
Compressing Deep Neural Network with Pruning, Trained
Quantization, and Huffman Coding.” arXiv: Computer Vision and
Pattern Recognition (2016)

Lee, Kenton, Ming-Wei Chang, and Kristina Toutanova. “Latent
Retrieval for Weakly Supervised Open Domain Question
Answering.” ArXiv abs/1906.00300 (2019)

Guu, Kelvin, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-
Wei Chang. “REALM: Retrieval-Augmented Language Model Pre-
Training.” ArXiv abs/2002.08909 (2020)

Lewis, Patrick, Ethan Perez, Aleksandara Piktus, F. Petroni, V.
Karpukhin, Naman Goyal, Heinrich Kuttler, M. Lewis, Wen-tau Yih,
Tim Rocktaschel, Sebastian Riedel and Douwe Kiela. “Retrieval-
Augmented Generation for | Knowledge-Intensive NLP
Tasks.” ArXiv abs/2005.11401 (2020)

Karpukhin, V., Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Yu
Wu, Sergey Edunov, Danqi Chen and Wen-tau Yih. “Dense Passage
Retrieval for Open-Domain Question
Answering.” ArXiv abs/2010.08191 (2020)

Dai, Zihang, Z. Yang, Yiming Yang, J. Carbonell, Quoc V. Le, and R.
Salakhutdinov. “Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context.” ArXiv abs/1901.02860 (2019)
Beltagy, Iz, Matthew E. Peters, and Arman Cohan. “Longformer: The
Long-Document Transformer.” ArXiv abs/2004.05150 (2020)
Ainslie, Joshua, S. Ontafion, C. Alberti, V. Cvicek, Zachary Kenneth
Fisher, Philip Pham, Anirudh Ravula, S. Sanghai, Qifan Wang and L.
Yang. “ETC: Encoding Long and _ Structured Inputs in
Transformers.” EMNLP (2020).

Zaheer, Manzil, Guru Guruganesh, Kumar Avinava Dubey, Joshua
Ainslie, C. Alberti, S. Ontafion, Philip Pham, Anirudh Ravula, Qifan
Wang, L. Yang and Amr Ahmed. “Big Bird: Transformers for
Longer Sequences.” ArXiv abs/2007.14062 (2020)

Kitaev, Nikita, L. Kaiser and Anselm Levskaya. “Reformer: The
Efficient Transformer.” ArXiv abs/2001.04451 (2020)

Child, R., Scott Gray, A. Radford, and Ilya Sutskever. “Generating
Long Sequences with Sparse
Transformers.” ArXiv abs/1904.10509 (2019)

Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
Piyush Sharma, and Radu Soricut “ALBERT: A Lite BERT for Self-
supervised Learning of Language
Representations.” ArXiv abs/1909.11942 (2020)

Clark, K., Minh-Thang Luong, Quoc V. Le, and Christopher D.
Manning. “ELECTRA: Pre-training Text Encoders as Discriminators
Rather Than Generators.” ArXiv abs/2003.10555 (2020)

Plummer, Bryan A., Nikoli Dryden, Julius Frost, Torsten Hoefler and
Kate Saenko. “Shapeshifter Networks: Cross-layer Parameter Sharing
for Scalable and Effective Deep _—_ Learning.” ArXiv abs/2006.10598
(2020)

Joshi, Mandar, Danqi Chen, Y. Liu, Daniel S. Weld, Luke Zettlemoyer,
and Omer Levy. “SpanBERT: Improving Pre-training by
Representing and Predicting Spans.” Transactions of the Association
for Computational Linguistics 8 (2019): 64-77
Yang, Z., Peng Qi, Saizheng Zhang, Yoshua Bengio, William W.
Cohen, R. Salakhutdinov and Christopher D. Manning. “HotpotQA: A
Dataset for Diverse, Explainable Multi-hop Question
Answering.” ArXiv abs/1809.09600 (2018)

Cho, Kyunghyun, B. V. Merrienboer, Caglar Giilgehre, Dzmitry
Bahdanau, Fethi Bougares, Holger Schwenk and Yoshua Bengio.
“Learning Phrase Representations using RNN Encoder-Decoder for
Statistical Machine Translation.” ArXiv abs/1406.1078 (2014)
Hochreiter, S. and J. Schmidhuber. “Long Short-Term
Memory.” Neural Computation 9 (1997): 1735-1780.

Chung, J., Caglar Giilgehre, Kyunghyun Cho and Yoshua Bengio.
“Empirical Evaluation of Gated Recurrent Neural Networks on
Sequence Modeling.” ArXiv abs/1412.3555 (2014)

Luong, Thang, Hieu Pham and Christopher D. Manning. “Effective
Approaches to Attention-based Neural Machine
Translation.” ArXiv abs/1508.04025 (2015)

Bahdanau, Dzmitry, Kyunghyun Cho and Yoshua Bengio. “Neural
Machine Translation by Jointly Learning to Align and
Translate.” CoRR abs/1409.0473 (2015)

26


50]

51]

52]

53]

54]

55]

56]

57]

58]

59

60

61

62

63
64

65

66

67

68

69

70

71

72

73

74

TD

Pascanu, Razvan, Tomas Mikolov and Yoshua Bengio. “On the
difficulty of training recurrent neural networks.” JCML (2013).
Mikolov, Tomas, Kai Chen, G. S. Corrado and J. Dean. “Efficient
Estimation of Word Representations in Vector
Space.” CoRR abs/1301.3781 (2013)

Pennington, Jeffrey, R. Socher and Christopher D. Manning. “Glove:
Global Vectors for Word Representation.” EMNLP (2014).
Melamud, Oren, J. Goldberger, and I. Dagan. “context2vec: Learning
Generic Context Embedding with Bidirectional
LSTM.” CoNLL (2016).

McCann, B., James Bradbury, Caiming Xiong, and R. Socher.
“Learned in Translation: Contextualized Word
Vectors.” NIPS (2017).

Ramachandran, Prajit, Peter J. Liu and Quoc V. Le. “Unsupervised
Pretraining for Sequence to Sequence
Learning.” ArXiv abs/1611.02683 (2017)

Howard, J. and Sebastian Ruder. “Universal Language Model Fine-
tuning for Text Classification.” ACL (2018).

Liu, Xiaodong, Pengcheng He, W. Chen and Jianfeng Gao. “Multi-
Task Deep Neural Networks for Natural Language
Understanding.” ArXiv abs/1901.11504 (2019)

Ba, Jimmy, J. Kiros, and Geoffrey E. Hinton. “Layer
Normalization.” ArXiv abs/1607.06450 (2016)

Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner,
Christopher Clark, Kenton Lee, and Luke Zettlemoyer. “Deep
contextualized word representations.” NAACL-HLT (2018).

Howard, J. and Sebastian Ruder. “Universal Language Model Fine-
tuning for Text Classification.” ACL (2018).

Wang, Yuxuan, W. Che, Jiang Guo, Yijia Liu, and Ting Liu. “Cross-
Lingual BERT Transformation for Zero-Shot Dependency
Parsing.” ArXiv abs/1909.06775 (2019)

Radford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and
Ilya Sutskever. “Language Models are Unsupervised Multitask
Learners.” (2019).
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to Sequence
Learning with Neural Networks.” NIPS (2014).

Lample, Guillaume and Alexis Conneau. “Cross-lingual Language
Model Pretraining.” NeurIPS (2019).

Song, K., X. Tan, T. Qin, Jianfeng Lu and T. Liu. “MASS: Masked
Sequence to Sequence Pre-training for Language
Generation.” ICML (2019).

Wenzek, Guillaume, Marie-Anne Lachaux, Alexis Conneau, Vishrav
Chaudhary, F. Guzman, Armand Joulin and E. Grave. “CCNet:
Extracting High Quality Monolingual Datasets from Web Crawl
Data.” ArXiv abs/1911.00359 (2020)

Artetxe, M., Gorka Labaka and Eneko Agirre. “Learning bilingual
word embeddings with (almost) no bilingual data.” ACL (2017).
Lample, Guillaume, Myle Ott, Alexis Conneau, Ludovic Denoyer and
Marc'Aurelio Ranzato. “Phrase-Based & Neural Unsupervised
Machine Translation.” ArXiv abs/1804.07755 (2018)

Baldwin, Timothy T., and J. K. Ford. “TRANSFER OF TRAINING:
A REVIEW AND DIRECTIONS FOR FUTURE
RESEARCH.” Personnel Psychology 41 (1988): 63-105.

Clark, Kevin, Urvashi Khandelwal, Omer Levy and Christopher D.
Manning. “What Does BERT Look At? An Analysis of BERT's
Attention.” ArXiv abs/1906.04341 (2019)

Liu, Zhuang, M. Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
“Rethinking the Value of Network
Pruning.” ArXiv abs/1810.05270 (2019)

Fan, Angela, E. Grave and Armand Joulin. “Reducing Transformer
Depth on Demand with Structured
Dropout.” ArXiv abs/1909.11556 (2020)

Srivastava, Nitish, Geoffrey E. Hinton, A. Krizhevsky, Ilya Sutskever,
and R. Salakhutdinov. “Dropout: a simple way to prevent neural
networks from overfitting.” J. Mach. Learn. Res. 15 (2014): 1929-
1958.

Wan, Li, Matthew D. Zeiler, Sixin Zhang, Y. LeCun, and R. Fergus.
“Regularization of Neural Networks using
DropConnect.” JCML (2013).

Sajjad, Hassan, F. Dalvi, Nadir Durrani and Preslav Nakov. “Poor
Man's BERT: Smaller and Faster Transformer
Models.” ArXiv abs/2004.03844 (2020)

[76] Narang, Sharan, Greg Diamos, S. Sengupta, and E. Elsen. “Exploring
Sparsity in Recurrent Neural Networks.” ArXiv abs/1704.05119
(2017)

[77] Zhu, M. and §. Gupta. “To prune, or not to prune: exploring the
efficacy of pruning for model compression.” ArXiv abs/1710.01878
(2018)

[78] Wang, Ziheng, Jeremy Wohlwend and Tao Lei. “Structured Pruning
of Large Language Models.” ArXiv abs/1910.04732 (2020)

[79] Voita, Elena, P. Serdyukov, Rico Sennrich and Ivan Titov. “Context-
Aware Neural Machine’ Translation Learns  Anaphora
Resolution.” ArXiv abs/1805.10163 (2018)

[80] Tang, Raphael, Yao Lu, L. Liu, Lili Mou, Olga Vechtomova and Jimmy
Lin. “Distilling Task-Specific Knowledge from BERT into Simple
Neural Networks.” ArXiv abs/1903.12136 (2019)

[81] Voita, Elena, David Talbot, F. Moiseev, Rico Sennrich, and Ivan
Titov. “Analyzing Multi-Head Self-Attention: Specialized Heads Do
the Heavy Lifting, the Rest Can Be Pruned.” ACL (2019).

[82] Ding, Yanzhuo, Yang Liu, Huanbo Luan and M. Sun. “Visualizing
and Understanding Neural Machine Translation.” ACL (2017).

[83] Michel, Paul, Omer Levy, and Graham Neubig. “Are Sixteen Heads
Really Better than One?” ArXiv abs/1905.10650 (2019)

[84] Zhang, D., J. Yang, Dongqiangzi Ye, and Gang Hua. “LQ-Nets:
Learned Quantization for Highly Accurate and Compact Deep Neural
Networks.” ArXiv abs/1807.10029 (2018)

[85] Zhou, Aojun, A. Yao, Yiwen Guo, L. Xu, and Y. Chen. “Incremental
Network Quantization: Towards Lossless CNNs with Low-Precision
Weights.” ArXiv abs/1702.03044 (2017)

[86] Lin, Xiaofan, Cong Zhao and W. Pan. “Towards Accurate Binary
Convolutional Neural Network.” ArXiv abs/1711.11294 (2017)

[87] Shen, Sheng, Zhen Dong, J. Ye, L. Ma, Zhewei Yao, A. Gholami, M.
Mahoney and K. Keutzer. “Q-BERT: Hessian Based Ultra Low
Precision Quantization of BERT.” AAAI (2020).

[88] Zafrir, Ofir, Guy Boudoukh, Peter Izsak and M. Wasserblat. “Q8BERT:
Quantized 8Bit BERT.” ArXiv abs/1910.06188 (2019)

[89] Jacob, B., Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
A. Howard, H. Adam, and D. Kalenichenko. “Quantization and
Training of Neural Networks for Efficient Integer-Arithmetic-Only
Inference.” 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (2018)

[90] Bengio, Yoshua, N. Léonard and Aaron C. Courville. “Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional
Computation.” ArXiv abs/1308.3432 (2013)

[91] Qi, Peng, Xiaowen Lin, L. Mehr, Zijian Wang and Christopher D.
Manning. “Answering Complex Open-domain Questions Through
Iterative Query Generation.” ArXiv abs/1910.07000 (2019)

[92] Kumar, A., Ozan Irsoy, Peter Ondruska, Mohit lyyer, J. Bradbury,
Ishaan Gulrajani, Victor Zhong, Romain Paulus and R. Socher. “Ask
Me Anything: Dynamic Memory Networks for Natural Language
Processing.” JCML (2016).

[93] Al-Rfou, Rami, Dokook Choe, Noah Constant, Mandy Guo, and Llion
Jones. “Character-Level Language Modeling with Deeper Self-
Attention.” AAA/ (2019).

[94] Gomez, Aidan N., Mengye Ren, R. Urtasun and Roger B. Grosse.
“The Reversible Residual Network: Backpropagation Without Storing
Activations.” ArXiv abs/1707.04585 (2017)

[95] Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang and Hao Ma.
“Linformer: Self-Attention with Linear
Complexity.” ArXiv abs/2006.04768 (2020)

[96] Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan,
Xingyou Song, A. Gane, Tamas Sarlos, P. Hawkins, J. Davis, Afroz
Mohiuddin, L. Kaiser, D. Belanger, Lucy J. Colwell, and Adrian
Weller. “Rethinking Attention with
Performers.” ArXiv abs/2009.14794(2020)

[97] Hugging Face Modeling Classification of NLP Models

[98] Fedus, W., Barret Zoph, and Noam Shazeer. “Switch Transformers:
Scaling to Trillion Parameter Models with Simple and Efficient
Sparsity.” ArXiv abs/2101.03961 (2021): n. pag.

[99] Carlini, N., Florian Tramér, Eric Wallace, M. Jagielski, Ariel Herbert-
Voss, K. Lee, A. Roberts, Tom Brown, D. Song, Ulfar Erlingsson,
Alina Oprea and Colin Raffel. “Extracting Training Data from Large
Language Models.” ArXiv abs/2012.07805 (2020): n. pag.

27
