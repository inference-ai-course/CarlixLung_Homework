arX1v:2510.10959v1 [cs.LG] 13 Oct 2025

Rediscovering Entropy Regularization: Adaptive Coefficient
Unlocks Its Potential for LLM Reinforcement Learning

Xiaoyun Zhang!

Chen Hu’

Xiaojian Yuan?!
Jingqing Ruan®

Di Huang! Wang You‘

Kejiang Chen? Xing Hu!*

State Key Lab of Processors, Institute of Computing Technology, CAS
?University of Science and Technology of China, *University of Chinese Academy of Sciences
4StepFun Inc

zhangxiaoyun24@mails.ucas.ac.cn

Abstract

Reasoning ability has become a defining ca-
pability of Large Language Models (LLMs),
with Reinforcement Learning with Verifiable
Rewards (RLVR) emerging as a key paradigm
to enhance it. However, RLVR training often
suffers from policy entropy collapse, where
the policy becomes overly deterministic, hin-
dering exploration and limiting reasoning per-
formance. While entropy regularization is a
common remedy, its effectiveness is highly sen-
sitive to the fixed coefficient, making it unsta-
ble across tasks and models. In this work, we
revisit entropy regularization in RLVR and ar-
gue that its potential has been largely underes-
timated. Our analysis shows that (i) tasks of
varying difficulty demand distinct exploration
intensities, and (ii) balanced exploration may
require the policy entropy to be maintained
within a moderate range below its initial level.
Therefore, we propose Adaptive Entropy Regu-
larization (AER) — a framework that dynami-
cally balances exploration and exploitation via
three components: difficulty-aware coefficient
allocation, initial-anchored target entropy, and
dynamic global coefficient adjustment. Ex-
periments on multiple mathematical reasoning
benchmarks show that AER consistently out-
performs baselines, improving both reasoning
accuracy and exploration capability.

1 Introduction

Reasoning ability has become a crucial capability
for Large Language Models (LLMs) to solve com-
plex tasks in mathematics and coding. Reinforce-
ment Learning with Verifiable Rewards (RLVR)
has recently emerged as an effective paradigm to en-
hance this capability, driving advances in state-of-
the-art models such as OpenAI-o1 and DeepSeek-
R1 (Jaech et al., 2024; Guo et al., 2025). However,

*Xiaoyun Zhang and Xiaojian Yuan contributed equally to
this work. Work done during an internship at StepFun.

“Corresponding author.

“Under review at ACL Rolling Review, October 2025.

xjyuan@mail.ustc.edu.cn

recent studies observe that policy entropy collapse
may pose a significant bottleneck in RLVR train-
ing (Cui et al., 2025b; He et al., 2025; Cheng et al.,
2025; Dai et al., 2025), closely tied to the long-
standing exploration—exploitation dilemma (Sutton
et al., 1998). Specifically, the model’s policy often
converges prematurely to a narrow set of exploita-
tive reasoning trajectories, thereby suppressing ex-
ploration of the broader solution space (Chen et al.,
2025b). This premature convergence typically man-
ifests as a rapid decline in policy entropy during the
early stages of training (Yu et al., 2025), trapping
the policy in local optima and leading to perfor-
mance plateaus that constrain the model’s overall
reasoning potential (Cui et al., 2025b).

A conventional approach in reinforcement learn-
ing to alleviate policy entropy collapse is to intro-
duce an entropy regularization term, which explic-
itly penalizes overly deterministic policies and en-
courages exploration (Schulman et al., 2017). De-
Spite its simplicity and conceptual appeal, this tech-
nique is often omitted in recent RLVR pipelines
for LLMs (Yu et al., 2025; Hu et al., 2025b; Liu
et al., 2025b; Cui et al., 2025a), as its effective-
ness is highly sensitive to the choice of the en-
tropy coefficient. Small coefficients cannot pre-
vent entropy collapse, whereas excessively large
coefficients may induce entropy explosion (Cui
et al., 2025b; Jiang et al., 2025b). Moreover, a
slight change in the base model or dataset may flip
the effect of a tuned coefficient from beneficial to
harmful (He et al., 2025). Intuitively, the balance
between exploration (high entropy) and exploita-
tion (low entropy) should be dynamic throughout
training. Fixed coefficients struggle to deal with
this evolving trade-off (He et al., 2025; Cui et al.,
2025b). This naturally raises the question:

Can we adaptively adjust the coefficient for
entropy regularization during RLVR training ?

In this work, we revisit entropy regularization


Fixed Entropy Regularizer Our AER

a; Replace Regularizer with AER j

Over-exploitation
/

Over-exploration

Regularizer

‘a

Exploration-
Exploitation
Balance

Cl. Difficulty-Aware Coeffici
| (a) group accuracy calculation
sq A; Az, Az Ace At

¢ © @ 9 Nea, meniom g(a)
pte

a2 O Q @ 067 + az-I{p =0, g(q) = 0}

Q; © © © 1

0 I

(b) coefficient allocation rule

C2. Initial-Anchored Target Entropy

(Tasks
initial entropy Ho reduction ratio T
wy target entropy H*

(ee)

SF
Ces)
Ht=7-Ho

C3. Dynamic Global Coefficient Adjustment
H:

O41 = ay +n-+sgn(H* — Hid,

Ole=H, > H*

Figure 1: An overview of the AER framework.

in the context of RLVR for LLMs and argue that
its potential has been largely underestimated due
to the limitations of fixed-coefficient designs. Mo-
tivated by this concern, we conduct preliminary
experiments and analysis in Section 3 and have
two observations: (i) tasks of different difficulty
levels require distinct exploration intensities, sug-
gesting the need for a difficulty-aware mechanism
that enables sample-level control of entropy reg-
ularization; and (ii) effective exploration during
training requires maintaining the policy entropy at
a specific target value below its initial entropy,

Therefore, we propose Adaptive Entropy Reg-
ularization (AER) as shown in Figure 1, which
dynamically balances exploration and exploitation
through adaptive coefficients, including three com-
ponents: (i) Difficulty-Aware Coefficient Allocation
estimates task difficulty relative to the current pol-
icy and assigns sample-level entropy coefficients
to achieve fine-grained entropy regularization; (ii)
Initial-Anchored Target Entropy adaptively deter-
mines the target entropy value based on each run’s
initial entropy, maintaining consistent relative ex-
ploration budget among different settings; and (iii)
Dynamic Global Coefficient Adjustment adaptively
adjusts a global scaling factor for entropy coeffi-
cients according to the current policy entropy to
ensure that the policy entropy is maintained near
the target entropy during training. Together, these
components form an adaptive controller that main-
tains policy entropy within a reasonable range, sta-
bilizing training while retaining balanced explo-
ration. We conducted empirical evaluations on
various complex mathematical reasoning bench-

marks, and AER showed consistent improvements
in reasoning performance and diversity.
Our contributions are summarized as follows:

¢ We conduct preliminary analysis to show that
exploration should adapt to task difficulty and
that balanced exploration may require main-
taining the policy entropy within a moderate
range below its initial level, motivating adap-
tive, difficulty-aware entropy regularization.

¢ We introduce the Adaptive Entropy Regular-
ization (AER), a framework that can dynami-
cally and adaptively adjust the coefficients of
entropy regularization to better balance explo-
ration and exploitation throughout training.

¢ Extensive experiments on various mathemat-
ical reasoning benchmarks demonstrate that
AER consistently outperforms advanced base-
lines in both reasoning performance (pass @ /)
and exploration capability (pass@k), validat-
ing the potential of adaptive entropy regular-
ization in RLVR training.

2 Related Work

Reinforcement Learning for LLMs. Reinforce-
ment learning is an important paradigm for training
LLMs (Ouyang et al., 2022; Team et al., 2023; Lee
et al., 2024). Recently, reinforcement learning with
verifiable rewards (RLVR) has shown remarkable
success for advancing the reasoning capabilities
of LLMs, demonstrating significant performance
in complex tasks such as mathematics and cod-
ing (Guo et al., 2025; Jaech et al., 2024; Yang et al.,
2025; He et al., 2025; Team et al., 2025; Liu et al.,


Dataset

a

—
| Easy Dataset |
he 4

te Accuracy
Token length |3000

Accuracy (%)
Accuracy (%)
6

a

20

2) i: oO:
5000
Hard Dataset <¢ 51,00

ge
=e Accuracy a000'6 wi
|

Temperature

Zz t=1.0 [J t=06

7000 1.50] 1.438 [= Easy 1 Hard
Yih

s
6000 21.25 Qwen2.5-7B

0.963 0.951 0.932

Token length

1000 0.25 Yii40.12% (0.103

099 (0.072
{7

O12 0.0001 O12

0. 601
Entropy coefficient (A)

(a) The impact of different coefficients(b) The impact of different coefficients

on the easy dataset. on the hard dataset.

0.0) 0.001
Entropy coefficient (A)

0.0001 2 0.00

Easy Hard Easy Hard

(c) Initial entropy under various settings.

Figure 2: Preliminary experimental results. (a-b) we show the effect of different entropy coefficients on test
accuracy and average token length on easy and difficult datasets, respectively. (c) we demonstrate that different base
models, datasets, and sampling temperatures will significantly affect the value of the initial entropy.

2025b; Zeng et al., 2025). In addition, a series of
actor-only methods further reduce the resource bur-
den and complexity of RLVR (Shao et al., 2024; Li
et al., 2024; Hu et al., 2025a; Yu et al., 2025; Zheng
et al., 2025a). However, RLVR still suffers from
exploration-exploitation dilemma, which manifests
as a rapid decrease in policy entropy, thus limiting
the performance of LLMs (Cui et al., 2025b; Cheng
et al., 2025; He et al., 2025).

Exploration in Reinforcement Learning. Ex-
ploration is a central challenge in reinforcement
learning, typically approached through theoreti-
cal analysis (Cai et al., 2020; Ishfaq et al., 2021),
curiosity-driven signals (Pathak et al., 2017; Burda
et al.; Raileanu and Rocktaschel; Henaff et al.,
2022), and entropy maximization (Ziebart et al.,
2008; Toussaint, 2009). In the context of LLMs,
several studies have employed entropy as a perfor-
mance indicator (Cui et al., 2025b) or as a heuris-
tic for advantage shaping, enhancing the rollout
phase, or loss masking (Wang et al., 2025; Cheng
et al., 2025; Zheng et al., 2025b; Li et al., 2025d).
Entropy regularization or KL penalty helps con-
trol policy distributions (He et al., 2025; Liu et al.,
2025a), while complementary techniques such as
loss reweighting (Wang et al., 2025; Cui et al.,
2025b) and clip-higher (Yu et al., 2025) further
mitigate entropy collapse. Additional strategies
for promoting exploration include adjusting sam-
pling hyperparameters (Chen et al., 2025a), per-
forming self-reflection (Jiang et al., 2025a), lever-
aging external verification (Zha et al., 2025), em-
phasizing high-entropy tokens via critical-token
training (Wang et al., 2025; Li et al., 2025c; Jiang
et al., 2025b), and designing custom intrinsic sig-
nals (Li et al., 2025a; Dai et al., 2025; Gao et al.,
2025; Song et al., 2025). However, the necessity of
entropy regularization in RLVR remains debated,

with some studies questioning its impact on ex-
ploration effectiveness (Ouyang et al., 2022; Shao
et al., 2024; Hu et al., 2025b; Cui et al., 2025b).

3 Preliminary Analysis

Although adding explicit entropy regularization
(e.g., an entropy loss term in the objective) is
a straightforward, plug-and-play remedy to pre-
vent the policy from becoming overly deterministic
and thus mitigate entropy collapse, most recent
works of RLVR for LLMs do not include this tech-
nique (Hu et al., 2025b; Liu et al., 2025b; Cui et al.,
2025a; Yu et al., 2025). Yet the entropy regulariza-
tion is highly sensitive to coefficients in practice,
small coefficients cannot effectively prevent en-
tropy collapse, while large coefficients will lead to
entropy explosion, causing training instability or
performance degradation as well (Cui et al., 2025b).
Moreover, slight changes in the experimental setup
can cause the carefully selected coefficients to have
the opposite effect (He et al., 2025).

Intuitively, the degree of exploration should cor-
relate with task difficulty: excessive exploration on
easy tasks may introduce unnecessary randomness
and hinder convergence, whereas difficult tasks
often require stronger exploration to escape lo-
cal optima and discover effective reasoning tra-
jectories (Li et al., 2025a,b). To examine this in-
tuition, we trained Qwen3-4B-Base with GRPO
on mathematical datasets of different difficulty
levels', varying the strength of entropy regular-
ization through different coefficients. As shown
in Figure 2a and Figure 2b, increasing the entropy
coefficient improves test accuracy on the harder
dataset, accompanied by a longer average token
length— indicating that moderate promotion of ex-

'The easy task uses GSMB8K, while the hard task consists
of a mixture of AIME and AMC datasets.


ploration benefits challenging reasoning tasks. In
contrast, on the easier dataset, stronger entropy reg-
ularization leads to a decline in accuracy, where
excessive exploration with longer responses may
prevent convergence toward concise and correct
reasoning trajectories. These results demonstrate
that the optimal level of exploration varies with task
difficulty, highlighting the necessity of a difficulty-
aware mechanism for entropy regularization. In
addition, when the coefficient is set to an overly
large value (i.e., 0.1), both datasets showed a sud-
den drop in accuracy accompanied by a sharp in-
crease in the average token length, indicating that
excessive exploration may trigger entropy explo-
sion, leading to instability in the training process.

A recent study (Cui et al., 2025b) establishes
an empirical relation between policy entropy H.
and downstream performance #, expressed as
R = —aexp (H)-+0, where a and bare fitting coef-
ficients. This indicates an inherent trade-off: policy
performance is “purchased” at the cost of entropy.
Furthermore, policy entropy decreases monotoni-
cally without any entropy intervention (Cui et al.,
2025b), which means that effective exploration
may require maintaining policy entropy at a "sweet
spot” below its initial level to avoid entropy col-
lapse and explosion. He et al. (2025) have similar
empirical observations that they monitor policy en-
tropy during training and preventing it from falling
below a prespecified target entropy value (e.g., 0.2)
from the initial entropy, thereby improving perfor-
mance. However, as shown in Figure 2c, the initial
entropy may vary greatly due to the differences in
the base model, training data, and sampling temper-
ature, leading to inconsistent “exploration budget".
This inspires us to design a mechanism that adap-
tively determines the target entropy value based on
the level of initial entropy.

4 Methodology

Building on these insights, we propose the Adap-
tive Entropy Regularization (AER) framework
for RLVR. AER estimates task difficulty with re-
spect to the current policy and adaptively adjusts
the entropy coefficient at the sample level. It further
sets the target entropy as a fraction of the initial
policy entropy and dynamically adjusts a global
scaling factor for coefficients to prevent the policy
entropy from falling below this target, maintaining
effective exploration throughout training.

4.1 Notations and Preliminaries

Formulation. We consider reinforcement learn-
ing with verifiable rewards (RLVR) for training
LLMs. Given a question-answer pair (q, a) from
the dataset D, the policy model 79(- | q) with pa-
rameters 0 generates a response o = (01,...,07).
A rule-based reward function r(q, 0) € {0, 1} pro-
vides binary correctness judgment.

Group Relative Policy Optimization (GRPO).
GRPO (Shao et al., 2024) extends proximal pol-
icy optimization (Schulman et al., 2017) to the
group sampling setting and eliminates the need for
a value network. For each question q, a group of
G candidate responses {0;}©_, is sampled. Then,
the advantage of the 7-th response is calculated by
normalizing the group-level rewards {Rj}, :=
{r(q, 01), tee) r(q, oa)}:

j. _ 7(q,01) ~ mean({ Ri} $21)

“ sid({Ri}Z,)

GRPO adopts a clipped objective and combines a
KL penalty term:

Jarpo(9) = E,

()

q,a)~D,{o;}2 1, ~76 old (-19)

|o;|

G
= Ss" al Ss" (min (win(O) Ais, clip
oa te
(wie(0),1—e,1 +2) Aix) — BDux (moll mer ))|

(04,41 ) o)
TO(04,t19,01,<t .

7, (0i,e14,0%.<2)" Tre¢ denotes the
reference policy and § controls the strength of the

KL penalty.

where w;4(@) =

Entropy Regularization. To alleviate the policy
entropy collapse, a common approach is to explic-
itly incorporate entropy regularization as an aux-
iliary objective. Formally, for an autoregressive
policy 7g, the token-level entropy at step ¢ is de-
fined as

Ht = S Te(a | x, Y<t) log 79(a | L, Yet),

aceV
(3)
where Y denotes the vocabulary. The sequence-
level entropy is the average over response positions:

T
H(m(-|2))=- OM, @)
t=1

with T’ being the response length. The entropy
regularization objective is given by

Tent() = E(2,y)~DI[H (79 (- | x))] ’ (5)



where y controls the regularization strength.

Finally, the overall training objective combines
this entropy term with the policy optimization ob-
jective (e.g., GRPO):

TSrota(9) = Jereo(9) + Jent(9)- (6)
4.2 Adaptive Entropy Regularization (AER)

AER dynamically balances exploration and ex-
ploitation in RLVR training by incorporating three
complementary mechanisms: (i) Difficulty-Aware
Entropy Allocation, which adjusts sample-level
entropy coefficients according to task difficulty; (ii)
Initial-Anchored Target Entropy, which adap-
tively defines the target exploration level based
on the model’s initial entropy; and (iii) Dynamic
Global Coefficient Adjustment, which continu-
ously tunes the overall regularization strength to
stabilize global policy entropy. Together, these
components form a self-regulating system that al-
locates exploration where needed, adapts to differ-
ent setups, and maintains stable entropy dynamics
throughout training. Formally, the training objec-
tive at iteration ¢ is:

Taer(9) = Torro(#)

(7)
where H.(7(- | q)) denotes the sequence-level en-
tropy and A,(q,0) represents the adaptive coeffi-
cient for each sample.

C1. Difficulty-Aware Coefficient Allocation.
As discussed in Section 3, harder questions require
stronger exploration to uncover potential reason-
ing trajectories, whereas excessive exploration on
easy questions can hinder convergence. To achieve
this balance, we introduce a mechanism to estimate
each question’s difficulty relative to the current pol-
icy during training. GRPO naturally supports this
estimation: for each question q, the policy gener-
ates a group of m candidate responses {0;}'” 1,
from which we compute the group accuracy as

1 m
= 2%):
9=1

Allocation rule. Given a pivot accuracy p €
[0, 1] and a small constant ¢ > 0 (typically 107),
we define a difficulty-aware entropy coefficient as

+ az- Hp =0, g(q) = 0},

sI

where a, is a global entropy scaling factor control-
ling the overall strength of entropy regularization,
which is adaptively updated through the dynamic
adjustment mechanism introduced in C3. The coef-
ficient \;(q, 0;) is positive only for hard questions
(g(q) < p) and zero otherwise. Additionally, it
allocates larger entropy coefficients to more diffi-
cult questions based on the difference between the
current g(q) and p to encourage more exploration.

C2. Initial-Anchored Target Entropy. As ana-
lyzed in Section 3, maintaining the policy entropy
within a certain “sweet spot” below its initial value
can appropriately promote exploration and stabilize
training. However, the initial policy entropy Ha
varies substantially across base models, datasets,
and sampling temperature, making it difficult to di-
rectly specify a specific value as the target entropy.
Therefore, we seek an adaptive mechanism that can
determine the value of the target entropy based on
the initial entropy at each run.

Definition. Let Ho denote the policy entropy
at initialization (obtained in the first training step).
We define the initial-anchored target entropy as

Ht =7-Ho, E(0,1), (10)
where 7 is a predefined reduction ratio represent-
ing the desired relative decrease from the initial en-
tropy. By anchoring the target entropy to the initial
entropy, it adaptively calibrates the effective “ex-
ploration budget” across different setups, thereby
eliminating the need for repeated hyperparameter
tuning and improving cross-run stability.

C3. Dynamic Global Coefficient Adjustment.
While C1 determines where to allocate exploration
and C2 specifies how much entropy should be re-
tained, the global policy entropy can still drift from
the target H* as training proceeds. Recall that
a in Equation 9 serves as a global scaling factor
that controls the overall strength of entropy regu-
larization. Instead of keeping a; fixed as a hyper-
parameter, AER updates it dynamically according
to the observed policy entropy (He et al., 2025).
This mechanism follows the classical principle of
closed-loop control (Astrém and Murray, 2021),
in which a system continuously measures its out-
put, compares it to a target, and adjusts its input to
reduce the deviation.

Definition. Let Hi, denote the batch-level policy
entropy at iteration t. With step size 7 > 0, the
global entropy scaling factor a; in Equation 9 is


Method AIME24 AIME25 AMC23 MATHS500 Avg.
pass@1_ pass@32_— pass@1_—s pass@32_—s pass@1_—s pass@32_— pass@1_—s pass@32_—s pass@1_— pass @32
Qwen3-4B-Base
Base 9.0 40.0 32 27.0 34.6 86.7 52.2 92.0 24.8 61.5
GRPO 19.7 42.0 15.4 32.3 59.9 87.8 80.5 93.7 43.9 64.0
w/ Clip-Higher 20.4 32.0 20.0 40.3 59.6 89.7 81.1 93.5 45.3 63.9
w/ Ent-Adv 21.0 40.9 13.3 31.6 49.2 86.7 79.2 91.3 40.7 62.6
w/ KL-Cov 24.5 54.6 20.4 39.2 S72 84.9 80.3 95.5 45.6 68.6
w/ Clip-Cov 24.0 52.5 22.5 41.4 65.9 92.4 87.8 95.9 50.1 70.6
w/ AER (Ours) 25.2 49.6 221 48.9 70.2 94.8 86.7 96.6 51.1 72.5
Qwen3-8B-Base
Base LIS 48.0 8.8 35.5 45.0 88.7 67.2 94.2 3341 66.6
GRPO 20.5 47.0 18.5 34.3 62.8 88.4 82.0 94.1 46.0 66.0
w/ Clip-Higher 271 61.5 21.1 46.9 68.5 92.7 88.3 97.0 51.3 TAS
w/ Ent-Adv 23:2, 45.0 15.8 35.2 62.5 80.6 83.5 93.4 46.3 63.6
w/ KL-Cov 26.9 58.4 20.8 42.7 65.3 92.1 80.6 96.7 48.4 72.5
w/ Clip-Cov 30.8 62.1 24.4 46.9 TAS 94.1 90.4 97.8 55.0 Td
w/ AER (Ours) 31.4 63.2 25.1 48.9 75.6 94.8 89.4 97.0 55.4 76.0

Table 1: Overall performance comparison. We compared the pass @/ and pass @32 performance between different
methods using the Qwen3-4B-Base and Qwen3-8B-Base models on four mathematical reasoning benchmarks.

updated as

or = [ar+n-sen(H*—H,)],, AD
where [z], = max(z,0). If H, < H*, a; in-
creases to encourage exploration; if Hy, > H*,
decreases to suppress excessive exploration.

This dynamic global coefficient adjustment pre-
vents both entropy collapse and entropy explosion.
By continuously monitoring and correcting devi-
ations, AER maintains policy entropy near H%,
forming a dynamic self-regulating mechanism that
sustains balanced exploration without much man-
ual tuning of hyperparameters.

Workflow of AER. Each training iteration of
AER proceeds as follows: (i) estimate the group
accuracy g(q) for each question via Equation 8;
(ii) compute difficulty-aware coefficients A; (q, 0)
via Equation 9; (iii) optimize 7apr(@) in Equa-
tion 7; and (iv) update a; according to Equation 11.
This closed-loop process adaptively allocates and
regulates exploration, maintaining stable entropy
dynamics throughout RLVR training.

5 Experiments

5.1 Experiment Setup

Configurations. We adopt the widely used Qwen
series open-source models (Yang et al., 2025) for
validating the efficacy of the proposed method, in-
cluding Qwen3-4B-Base and Qwen3-8B-Base. All
models are trained on the open-source DeepscaleR

dataset (Luo et al., 2025). We use the Hugging
Face verification tool math_verify (Kydliéek and
Face, 2025) to automatically check the correctness
of model answers. Detailed hyperparameter config-
uration can be found in Appendix A.

Evaluations. We follow standard protocols to
evaluate mathematical reasoning and select widely
adopted benchmarks: AIME24 (MAA, 2024),
AIME25 (MAA, 2025), AMC23 (AI-MO, 2024)
and MATHS500 (Hendrycks et al., 2021). We report
pass@1 to evaluate performance and pass@k; to
evaluate exploration capability (Yue et al., 2025).
Greater exploration improves the likelihood of find-
ing a correct reasoning path within / attempts.

Baseline methods. We compare AER against
several baselines, including the standard
GRPO (Shao et al., 2024), GRPO with the
Clip-Higher (Yu et al., 2025) and several advanced
exploration-oriented methods: Ent-Adv (Cheng
et al., 2025), which introduces an entropy-based
advantage term to encourage longer reasoning
trajectories, and Clip-Cov and KL-Cov (Cui et al.,
2025b), which regularize policy updates through
token-—entropy covariance to stabilize training.

5.2. Main Results

Overall Performance. Table | presents the main
results on four mathematical reasoning bench-
marks. Across both model scales, AER gener-
ally achieves higher performance (pass@1/) and


Qwen3-8B-Base on AIME 2024 0.65
—

Reward

—o— GRPO

—— w/Clip-Higher —— Base

1 4 16 64 256 5 TTT

k (log2 scale)

(a) Pass@k on AIME 2024

3500

Qwen3-8B-Base on AIME 2025

3000

N
a
f=)
6

2000

pass@k (%)

1500

Response Length

—— Base

ote w/Clip-Higher

1000

1 4

16
k (log2 scale)

(d) Pass@k on AIME 2025

64 256 0 100 200

(e) Avg. Response Length

0.07
0.06
20.05
a
2 0.04
P=]
c
ww 0.03 ———
— GRPO — GRPO
— w/Clip-Higher 0.02 — w/Clip-Higher
— wi/AER ot — w/AER
300 400 500 600 0 100 200 300 400 500 600
Step Step
(b) Training Reward (c) Policy Entropy
@ 0.250
2
8 0.225
w
% 0.200
F 0.175
wn
gs 0.150 A
— GRPO — — GRPO
—— w/Clip-Higher @ =—— w/Clip-Higher
— w/AER q 0-100 — w/AER
300 400 500 600 oO 100 200 300 400 500 600
Step Step

(f) Test Score on AIME25

Figure 3: Pass@k and training dynamics (2x3 grid). Left column: pass@k on AIME24 (a) and AIME25 (d) as k
scales. Right two columns: training dynamics—reward (b), policy entropy (c), response length (e)—and test score

on AIME25 (f) over steps.

Qwen3-8B-Base AIME24 AIME25 AMC23 MATHS00

GRPO 20.5 18.5 62.8 82.0
w/ Fixed Coeff. 23.4 21.9 63.9 86.1
w/ Adaptive Coeff. (C2&C3) 29.4 21.4 73.8 89.4
w/ AER (C1&C2&C3) 31.4 25.1 75.6 89.4

Table 2: Ablation study of different components.

stronger exploration capability (pass@32) than the
baselines. For the Qwen3-4B-Base model, AER
yields an average +7.2% improvement in pass @ 1
over vanilla GRPO and +1.0% over the best base-
line (Clip-Cov). In terms of exploration, AER im-
proves the average pass@32 by +8.5% compared
to vanilla GRPO and +1.9% compared to Clip-
Cov. A similar trend is observed for the Qwen3-
8B-Base model, where AER reaches the highest
average scores—55.4% on pass @ 1 and 76.0% on
pass@32. The gains are more noticeable on chal-
lenging benchmarks such as AIME24 and AIME25,
suggesting that difficulty-aware entropy allocation
may better promote exploration on harder reason-
ing tasks. Overall, the results indicate that AER can
enhance reasoning performance and exploration di-
versity, highlighting its potential to further leverage
entropy regularization in RLVR training for LLMs.

Pass@k; Analysis. Following prior work (Yue
et al., 2025; Cheng et al., 2025; Jiang et al., 2025b),
we further examine the exploration boundary of
different methods through pass@k performance,
which reflects the model’s upper bound of rea-
soning capability. Specifically, we evaluate the

Qwen3-8B-Base model on two challenging bench-
marks—AIME2024 and AIME2025—extending k
up to 256 to assess how performance scales with
increased rollout attempts. As shown in Figure 3,
AER consistently achieves higher pass @k; perfor-
mance across all k, indicating stronger exploration
and reasoning diversity. The advantage becomes
more pronounced as & increases on the AIME25
dataset, where AER maintains a clear margin over
baselines throughout the curve. This trend suggests
that AER more effectively broadens the search
space of reasoning trajectories, enabling the policy
to discover a larger proportion of correct solutions
under extended sampling. Detailed quantitative
results can be found in Appendix C.1.

Analysis of Training Dynamics. In Figure 3b,
the training reward of AER reaches a higher level
than the baselines, suggesting that our method fa-
cilitates more effective policy improvement during
training. As shown in Figure 3c, the policy en-
tropy of naive GRPO drops sharply within the first
100 steps, indicating early entropy collapse. Al-
though the baseline slightly mitigates this effect,
its entropy still gradually declines over steps. AER
maintains the policy entropy at a moderate target
level throughout training, which help sustain con-
sistent and balanced exploration. Despite main-
taining higher entropy, AER produces shorter av-
erage response lengths than the baseline as shown
in Figure 3e, implying that our method guides ex-
ploration more effectively without unnecessary ver-


~ 56.0} 55-8 =O=-AER (AVG: Pass@1) —55.5+ 55.4 =O= AER (AVG. Pass@1)__
< S55.

bo 55.5
Boss) 8 Kane | Basal — feet
@55.0- o——— ® 54.3, r ae
a 54-5 pe wba2 54.1
a 54.5) Es4.0/-.-9—| AN |
5 sa.0 a
2. 53.5 535 53.1
53.5} ° 53.0, %e
0 02 04 06 08 0.20.30.40.50.6 08 1 41.2
AER p AER Tt
(a) AER p ablation (b) AER 7 ablation

— GRPO
— wiClip-Higher
— w/AER

— GRPO ix
0.05. — w/Clip-Higher =
— w/AER Fy

AIME2025 Test Score

o 50 100 150 200 250 300 350 400 o 50 100 150 200 250 300 350 400
Step Step

(c) Qwen3-8B Test Score (d) Qwen2.5-7B Test Score

Figure 4: AER ablations and generalization. (a) Pivot accuracy p ablation (C1, 7=0.4). (b) Reduction ratio 7
ablation (C2, p=0.2). (c) Generalization on DAPO_MATH_17K: Qwen3-8B-Base test score. (d) Generalization on

DAPO_MATH_17K: Qwen2.5-7B-Base test score.

bosity. Finally, Figure 3f shows that AER dynami-
cally maintains a stable policy entropy throughout
training and allocates more exploration to difficult
samples, thereby facilitating the policy to escape lo-
cal optima and continuously improve performance.
Overall, these results suggest that AER promotes
stable entropy dynamics, balanced exploration, and
sustained performance gains during RLVR training.

5.3. Ablation Study

Effectiveness of different components. As
shown in Table 2, building upon GRPO, introduc-
ing entropy regularization with a tuned fixed co-
efficient improves performance, suggesting that
moderate exploration is beneficial. Then applying
C2 & C3 brings further gains, particularly on chal-
lenging benchmarks such as AIME24, highlighting
the importance of maintaining an appropriate level
of policy entropy throughout training. When the
Cl is further incorporated, the full AER frame-
work achieves the best performance, e.g., improv-
ing AIME25 performance from 21.4 to 25.1.

Ablation of the pivot accuracy (9) in C1. Fix-
ing T=0.4, we sweep p to examine how it allocates
exploration (Figure 4a). The Avg. pass@J/ curve
increases as p decreases from 1.0 to 0.0, peaking
at 55.8 when p=0, and degrading once p > 0.6
(down to 53.5 at p=1.0). The monotonic 1 — 0 rise
confirms our difficulty-aware design: lowering p
gates positives to truly hard cases, focusing entropy
where exploration helps while sparing easy ones.
Although C3 stabilizes the global entropy around
the target, an overly large p makes many questions
“hard,” diluting per-sample selectivity and weaken-
ing the signal-to-noise ratio of entropy injection.
Accordingly, we recommend a small—moderate
pivot and adopt p<=0.2 by default.

Ablation of the reduction ratio (7) in C2. Fix-
ing p=0.2, we sweep 7 to study how anchoring
the target entropy to the initial entropy affects per-

formance (Figure 4b). The Avg. pass@I curve
exhibits a clear sweet spot at T=0.4 (55.4), with
a mild plateau for 7 € [0.5, 0.6] and degradation
in both directions: under-reduction at 7 <0.3 and
over-preservation at high-entropy settings 7 > 0.8.
These results substantiate the empirical premise
that moderate entropy reduction accompanies ac-
curacy gains: compared with naive GRPO, AER
stabilizes the global entropy near the prescribed
band, retaining sufficient diversity while avoiding
premature collapse. Pushing 7 too high inflates
entropy beyond the useful range, diluting the sig-
nal for exploitation; pushing it too low curtails
beneficial exploration. In practice, we thus adopt
T € (0.4, 0.6] as default.

Generalization under different experimental set-
tings. To test whether AER’s closed-loop en-
tropy control transfers across data and architectures
without retuning, we replace DEEPSCALER with
DAPO_MATH_17K and train Qwen3-8B-Base and
Qwen2.5-7B-Base using the same default hyper-
parameters (9=0.2, 7T=0.4). The test-score tra-
jectories in Figure 4c and Figure 4d show that
AER achieves faster early gains and maintains a
clear, persistent margin over Clip-Higher through-
out training, while converging to a higher (or com-
parable) asymptote than GRPO. This behavior in-
dicates that the initial-anchored target entropy (C2)
together with dynamic global scaling (C3) adapts
the effective exploration level to the new data dis-
tribution and model backbone.

6 Conclusion

This paper revisits entropy regularization in re-
inforcement learning with verifiable rewards and
identifies fixed-coefficient schemes—often prone
to entropy collapse or explosion—as a key lim-
itation to reasoning performance. We introduce
Adaptive Entropy Regularization (AER), which
adaptively balances exploration and exploitation


throughout training. On multiple mathematical rea-
soning benchmarks, AER consistently improves
both performance and exploration capability, re-
vealing that adaptive coefficient control is crucial
to rediscovering the potential of entropy regulariza-
tion for LLM reinforcement learning.

Limitations

We discuss two practical limitations of our work.
(1) Due to limited computational resources, our
experiments are restricted to 4B, 7B, and 8B back-
bones; nevertheless, the results consistently demon-
strate the efficacy of AER across these model sizes.
(2) In line with most open-source studies, we train
primarily on mathematical datasets because they
are easy to obtain and provide accurate, verifiable
rewards. This is not an important issue, and we
believe they can achieve better results if more train-
ing datasets with verifiable rewards for general do-
mains are available.

Ethics Statement

We have carefully considered the ethical impli-
cations of our research and provide the follow-
ing statements:(1) Throughout this study, we have
strictly followed established ethical guidelines, en-
suring that our findings are reported honestly, trans-
parently, and with full accuracy. (2) No sensitive
or confidential information was used at any stage
of our research. All data and materials utilized
are suitable for public release. (3) The datasets
employed in our experiments originate from pub-
licly available and peer-reviewed scientific sources,
supporting the transparency and reproducibility of
our work. (4) We offer detailed descriptions of the
datasets and the hyper-parameter configurations
used in our experiments to ensure the reproducibil-
ity and clarity of our results. (5) In the interest of
openness and to support future research, we have
made our code available anonymously on GitHub
and will fully open source it following the accep-
tance of our paper.

References

AI-MO. 2024. Amc 2023. https: //huggingface.co
/datasets/AI-MO/aimo-validation-amc.

Karl Johan Astrém and Richard Murray. 2021. Feed-
back systems: an introduction for scientists and engi-
neers. Princeton university press.

Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg
Klimov. Exploration by random network distillation.
In International Conference on Learning Representa-
tions.

Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
2020. Provably efficient exploration in policy opti-
mization. In International Conference on Machine
Learning, pages 1283-1294. PMLR.

Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen,
Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao,
Zheng Liu, Xu Miao, Yang Lu, et al. 2025a. An
empirical study on eliciting and improving rl-like
reasoning models. arXiv preprint arXiv:2503.04548.

Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling,
Qinghao Ye, Wayne Xin Zhao, and Guang Shi. 2025b.
Pass @ k training for adaptively balancing exploration
and exploitation of large reasoning models. arXiv
preprint arXiv:2508.10751.

Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai,
Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei.
2025. Reasoning with exploration: An entropy per-
spective. arXiv preprint arXiv:2506.14758.

Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang,
Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu,
Qixin Xu, Weize Chen, et al. 2025a. Process rein-
forcement through implicit rewards. arXiv preprint
arXiv:2502.01456.

Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan,
Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan,
Huayu Chen, Weize Chen, et al. 2025b. The entropy
mechanism of reinforcement learning for reasoning
language models. arXiv preprint arXiv:2505.22617.

Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen
Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui
Liu, Tong Zheng, Hongtu Zhu, et al. 2025. Cde:
Curiosity-driven exploration for efficient reinforce-
ment learning in large language models. arXiv
preprint arXiv:2509.09675.

Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi
Lu, Qingpeng Cai, Peng Jiang, and Xiangyu Zhao.
2025. Navigate the unknown: Enhancing Ilm rea-
soning with intrinsic motivation guided exploration.
arXiv preprint arXiv:2505.17621.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,
Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,
Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-
centivizing reasoning capability in Ilms via reinforce-
ment learning. arXiv preprint arXiv:2501.12948.

Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chao-
jie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang
Zhang, Jiacheng Xu, Wei Shen, et al. 2025. Sky-
work open reasoner | technical report. arXiv preprint
arXiv:2505.22312.


Mikael Henaff, Roberta Raileanu, Mingi Jiang, and
Tim Rocktaschel. 2022. Exploration via elliptical
episodic bonuses. In Advances in Neural Information
Processing Systems.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring mathematical prob-
lem solving with the math dataset. arXiv preprint
arXiv:2103.03874.

Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen.
2025a. Reinforce++: An efficient rlhf algorithm with
robustness to both prompt and reward models. arXiv
preprint arXiv:2501.03262.

Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang,
Xiangyu Zhang, and Heung-Yeung Shum. 2025b.
Open-reasoner-zero: An open source approach to
scaling up reinforcement learning on the base model.
arXiv preprint arXiv:2503.24290.

Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub,
Zhuoran Yang, Zhaoran Wang, Doina Precup, and
Lin Yang. 2021. Randomized exploration in rein-
forcement learning with general value function ap-
proximation. In International Conference on Ma-
chine Learning, pages 4607-4616. PMLR.

Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-
son, Ahmed El-Kishky, Aiden Low, Alec Helyar,
Aleksander Madry, Alex Beutel, Alex Carney, et al.
2024. Openai ol system card. arXiv preprint
arXiv:2412.16720.

Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin,
Wenyuan Xu, Yu Yue, Qianchuan Zhao, and Lin Yan.
2025a. Pag: Multi-turn reinforced Ilm self-correction
with policy as generative verifier. arXiv preprint
arXiv:2506. 10406.

Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu,
Yu Cheng, and Jing Shao. 2025b. Rethinking en-
tropy regularization in large reasoning models. arXiv
preprint arXiv:2509.25133.

Hynek Kydliéek and Hugging Face. 2025. Math-verify.
https: //github. com/huggingface/Math-Verif
y.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas
Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop,
Ethan Hall, Victor Carbune, Abhinav Rastogi, et al.
2024. Rlaif vs. rlhf: Scaling reinforcement learning
from human feedback with ai feedback. In Inter-
national Conference on Machine Learning, pages

26874-26901. PMLR.

Ang Li, Zhihang Yuan, Yang Zhang, Shouda Liu,
and Yisen Wang. 2025a. Know when to explore:
Difficulty-aware certainty as a guide for lm reinforce-
ment learning. arXiv preprint arXiv:2509.00125.

Jiazheng Li, Hong Lu, Kaiyue Wen, Zaiwen Yang, Ji-
axuan Gao, Hongzhou Lin, Yi Wu, and Jingzhao

10

Zhang. 2025b. Questa: Expanding reasoning capac-
ity in Ilms via question augmentation. arXiv preprint
arXiv:2507.13266.

Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi
Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming
Yang, Minghui Qiu, et al. 2025c. Cure: Critical-
token-guided re-concatenation for entropy-collapse
prevention. arXiv preprint arXiv:2508.11016.

Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min,
Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaigiang
Wang, Dawei Yin, Xu Chen, et al. 2025d. Staying in
the sweet spot: Responsive reasoning evolution via
capability-adaptive hint scaffolding. arXiv preprint
arXiv:2509.06923.

Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang
Yu, Ruoyu Sun, and Zhi-Quan Luo. 2024. Remax: a
simple, effective, and efficient reinforcement learn-
ing method for aligning large language models. In
Proceedings of the 41st International Conference on
Machine Learning, pages 29128-29163.

Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin
Dong, Yejin Choi, Jan Kautz, and Yi Dong. 2025a.
Prorl: Prolonged reinforcement learning expands rea-
soning boundaries in large language models. arXiv
preprint arXiv:2505.24864.

Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi,
Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.
2025b. Understanding r1-zero-like training: A criti-
cal perspective. arXiv preprint arXiv:2503.20783.

Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi,
William Y Tang, Manan Roongta, Colin Cai, Jeffrey
Luo, Tianjun Zhang, Li Erran Li, et al. 2025. Deep-
scaler: Surpassing ol-preview with a 1.5 b model by
scaling rl. Notion Blog.

MAA. 2024. American invitational mathematics exami-
nation - aime.

MAA. 2025. American invitational mathematics exami-
nation - aime.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in neural in-
formation processing systems, 35:27730-27744.

Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and
Trevor Darrell. 2017. Curiosity-driven exploration by
self-supervised prediction. In International confer-
ence on machine learning, pages 2778-2787. PMLR.

Roberta Raileanu and Tim Rocktaschel. Ride: Re-
warding impact-driven exploration for procedurally-
generated environments. In International Conference
on Learning Representations.


John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv: 1707.06347.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, YK Li, et al. 2024. Deepseekmath: Pushing
the limits of mathematical reasoning in open lan-
guage models. arXiv preprint arXiv:2402.03300.

Yuda Song, Julia Kempe, and Remi Munos. 2025.
Outcome-based exploration for Ilm reasoning. arXiv
preprint arXiv:2509.06941.

Richard S Sutton, Andrew G Barto, et al. 1998. Rein-
forcement learning: An introduction, volume 1. MIT
press Cambridge.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, Katie
Millican, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805.

Kimi Team, Angang Du, Bofei Gao, Bowei Xing,
Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun
Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025.
Kimik1. 5: Scaling reinforcement learning with Ilms.
arXiv preprint arXiv:2501.12599.

Marc Toussaint. 2009. Robot trajectory optimization
using approximate inference. In Proceedings of the
26th annual international conference on machine
learning, pages 1049-1056.

Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shix-
uan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin
Yang, Zhenru Zhang, et al. 2025. Beyond the 80/20
rule: High-entropy minority tokens drive effective
reinforcement learning for Ilm reasoning. arXiv
preprint arXiv:2506.01939.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Ly, et al. 2025. Qwen3
technical report. arXiv preprint arXiv:2505.09388.

Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,
Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,
Gaohong Liu, Lingjun Liu, et al. 2025. Dapo: An
open-source Ilm reinforcement learning system at
scale. arXiv preprint arXiv:2503.14476.

Yang Yue, Zhigi Chen, Rui Lu, Andrew Zhao, Zhaokai
Wang, Shiji Song, and Gao Huang. 2025. Does re-
inforcement learning really incentivize reasoning ca-
pacity in Ilms beyond the base model? arXiv preprint
arXiv:2504. 13837.

Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu,
Keqing He, Zejun MA, and Junxian He. 2025.
SimpleRL-zoo: Investigating and taming zero rein-
forcement learning for open base models in the wild.
In Second Conference on Language Modeling.

11

Kaiwen Zha, Zhenggqi Gao, Maohao Shen, Zhang-
Wei Hong, Duane S Boning, and Dina Katabi.
2025. RI tango: Reinforcing generator and veri-
fier together for language reasoning. arXiv preprint
arXiv:2505.15034.

Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui
Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong
Liu, Rui Men, An Yang, et al. 2025a. Group
sequence policy optimization. arXiv preprint
arXiv:2507.18071.

Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran
Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu
Wen, Chenghua Lin, Wenhao Huang, et al. 2025b.
First return, entropy-eliciting explore. arXiv preprint
arXiv:2507.07017.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
Anind K Dey, et al. 2008. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pages
1433-1438. Chicago, IL, USA.


A Implementation Details

A.1_ Training Setup

We conduct all experiments on a single node equipped with 32x NVIDIA H100 (80 GB, SXM) GPUs
interconnected via NVLink. Unless otherwise noted, all methods are trained under the same computational
budget (wall-clock hours and effective batch size) and with identical data pre-processing and evaluation
protocols to ensure fair comparison.

RL stack and integration. For the reinforcement learning (RL) stage, we adopt VERL, a post-training
framework tailored to large language models (LLMs) with verifiable or preference-based feedback. VERL
provides modular APIs that integrate seamlessly with mainstream LLM infrastructures (e.g., PyTorch
FSDP and Megatron-LM) and inference engines (e.g., VLLM), enabling (i) memory-efficient sharding
for optimizer states and activations, (ii) flexible rollout orchestration, and (ili) pluggable algorithm
components (e.g., GRPO, DAPO, and our variants). This design reduces engineering overhead and
makes experimental factors (algorithmic choices, hyperparameters, and decoding policies) isolatable and
reproducible.

Distributed training and precision. We use mixed-precision training (bf16 where supported, with
automatic casting and safe gradient scaling) and distributed data parallelism via FSDP-style full sharding
of parameters, gradients, and optimizer states. Gradient accumulation is employed to match the effective
global batch sizes reported in Table 3. Checkpointing is performed at fixed intervals to support fault
tolerance and ablations with matched training budgets.

Inference engine for rollouts and evaluation. All online rollouts and offline evaluations are executed
with VLLM, an efficient LLM inference engine that supports asynchronous batching and distributed
serving with a paged key—value (KV) cache. Using a single, shared engine for both training-time rollouts
and test-time evaluation minimizes distribution shift induced by heterogeneous runtimes. Decoding
configurations (temperature, nucleus/top-p, maximum length, stop rules) are held constant across methods
within each experiment; pass@k metrics use a fixed k (default k=32) unless otherwise specified.

Reproducibility. We fix random seeds for data sampling, parameter initialization, and decoder sampling;
we further report the exact learning rate, context length, evaluation interval, and method-specific knobs
in Table 3. All results are averaged over the same evaluation protocol to control for stochasticity in
sampling-based metrics.

A.2. Hyperparameters

Notation. In Table 3, data_train_batch_size denotes the effective number of sequences per optimizer
step after gradient accumulation (i.e., the global batch size across 32 GPUs). ppo_mini_batch_size
specifies the per-update minibatch for PPO-style objectives. The kl column is the coefficient of the KL
regularizer (set to 0 for fully KL-free variants). length is the maximum sequence length (tokens) and is
kept identical during training and evaluation to avoid truncation bias. eval_step is the validation interval
(in optimizer steps). Method-specific switches are listed under Others.

Method-specific parameters:

Clip-Higher: Uses asymmetric clipping ratios with c; = 0.2 (lower bound) and cp, = 0.28 (upper
bound) to allow more aggressive updates for positive advantages while constraining negative ones.

Ent-Adv: Implements entropy-based advantage estimation with scaling factor a = 0.4 and tempera-
ture parameter « = 2.0 to balance exploration and exploitation.

KL-Cov: Adds KL divergence covariance regularization with KL coefficient Aj; = 1.0 and covari-
ance ratio pz; = 0.002 to stabilize training dynamics.

Clip-Cov: Combines clipping with covariance regularization using symmetric clipping ratios c; =
cp = 1.0 and covariance ratio Pep = 0.0002 for enhanced stability.

12


Method data_train_batch_size ppo_mini_batch_size kl length Ir epoch eval_step Others

GRPO 128 32 0.0 8k tle6 30 20 -
w/Clip-Higher 128 32 0.0 8k tle6 30 20 =0.2, cn=0.28
w/Ent-Adv 128 32 0.0 8k  tle6 30 20 a=0.4, K=2.0
w/KL-Cov 128 32 0.0 8k  tle6 30 20 Aw=1.0, prt=0.002
w/Clip-Cov 128 32 0.0 8k tle6 30 20 C1=1.0, cr=1.0, petip=0.0002
w/AER 128 32 0.0 8k tle6 30 20 t=0.6, 8=0.2, n=0.005

Table 3: Training configurations. We keep the compute budget and decoding policy matched across methods. The
KL coefficient controls regularization strength in PPO-style objectives, DAPO removes KL entirely. Clip-Higher
uses asymmetric clipping ratios (c;, c,). Ent-Adv introduces entropy-based advantage estimation with scaling
factor a and temperature «. KL-Cov and Clip-Cov add covariance regularization with coefficients pz; and perip
respectively. AER (Adaptive Entropy Regulation) introduces adaptive entropy control with target entropy t, difficulty
threshold /, and adaptation rate 77.

¢ AER: Implements adaptive entropy regulation with target entropy t = 0.6, difficulty threshold
2 = 0.2 for distinguishing hard vs. easy samples, and adaptation rate 7 = 0.005 for dynamic
parameter adjustment. The method automatically maintains the target entropy level through adaptive
q@ parameter updates.

Scheduling and optimization. Unless stated otherwise, we use AdamW with learning rate 1x 10~°
(see Table 3) and align the number of epochs so that the total number of optimizer updates is comparable
across baselines. Evaluation is triggered every eval_step updates to monitor both pass@ 1 (accuracy)
and pass@ k; (diversity under multi-sample decoding). For RL methods, rollout budgets (number of
samples per prompt) are matched at training and validation time to ensure apples-to-apples comparisons.

Protocol fairness. All ablations modify a single factor at a time (e.g., toggling KL, changing ¢ in
APARL) while keeping the data curriculum, tokenizer, context length, and decoding policy fixed. This
controls confounders and isolates the effect of exploration—exploitation regularization on pass@ 1 and
pass@ k.

Checkpoint selection. For each method, we perform validation on the AIME2025 set every 20 opti-
mizer steps (i.e., eval_step=20) using the same decoding configuration as at test time. Unless otherwise
stated, the checkpoint used for reporting is the one that attains the highest pass@ 1 on AIME2025; all
metrics in the main tables are computed from this selected checkpoint.

B Detailed Description of Benchmarks

To fairly evaluate mathematical reasoning ability, we need to use benchmarks that cover different types of
problems, various levels of difficulty, and a range of math topics. When choosing datasets, we focus on
the following points in Table 4:

13


Dataset Core Description Key Characteristics

AIME’24 — High school Olympiad-level as-
sessment from American Invita-
tional Mathematics Examination

15 complex competition problems
Algebra/Geometry/Number theory focus
3-hour time constraint design

Multi-step reasoning verification

AIME’25 High school Olympiad-level as-
sessment from American Invita-
tional Mathematics Examination

15 complex competition problems

Algebra/Geometry/Number theory focus

3-hour time constraint design

Multi-step reasoning verification

GSM8K Elementary school math word
problem benchmark

8,500 graded problems
Natural language scenarios

Basic arithmetic operations

Step-by-step solution validation

MATH-500 Advanced mathematics evalua-

ion:set by OpenAl ¢ 500 curated problems

¢ Formal mathematical notation
¢ Non-standard solution analysis
¢ Cross-domain evaluation

AMC 2023 American Mathematics Competi-

‘ ¢ Tiered assessment structure
tions system

¢ Hybrid question types
¢ Curriculum alignment verification
* Official difficulty metrics

Table 4: Comparison of Mathematical Competition Datasets
Links:

AIME ’24: https: //huggingface.co/datasets/HuggingFaceH4/aime_2024;
AIME ’25: https: //huggingface.co/datasets/HuggingFaceH4/aime_2025;
GSMB8K: https: //huggingface.co/datasets/openai/gsm8k;
MATH-500: https: //huggingface.co/datasets/HuggingFaceH4/MATH- 500;
AMC 2023: https: //huggingface. co/datasets/AI-MO/aimo-validation-amc

14


Method AIME2024: Pass@k (%)

k=1 k=2 k=4 k=8 k=16 k=32 k=64 k=128 k=256

Base 11.5 15.5 24.2 32.1 40.2 48.0 55:1 60.9 64.3

GRPO 20.5 26.2 31.3 36.4 41.5 47.0 53:5 59.5 63.7
w/Clip-Higher ATal 34.0 41.0 48.7 55.5 61.5 65.7 68.2 69.5
w/AER 31.4 38.2 45.4 52.0 58.3 63.2 66.4 69.4 72.4

Table 5: Qwen3-8B-Base on AIME2024. Exact Pass@ k values underlying the main-text curve.

Method AIME2025: Pass@k (%)

k=1 k=2 k=4 k=8 k=16 k=32 k=64 k=128 k=256

Base 8.8 14.6 18.9 24.5 31.0 35.5 41.5 46.7 50.3
GRPO 18.5 220 2931 28.4 31.1 34.3 38.1 42.5 47.1
w/Clip-Higher 211 24.8 28.8 35.4 42.3 46.9 48.2 51.7 54.6
w/AER 25.1 30.1 35.7 41.0 46.0 48.9 54.7 59.4 62.8

Table 6: Qwen3-8B-Base on AIME2025. Exact Pass@ & values underlying the main-text curve.

C_ Detailed Results

C.1 Pass@k as a Function of k (Qwen3 - 8B - Base)

Table 5—6 report the exact values underlying the Pass@k—vs.—k plots on AIME2024 and AIME2025.
We follow the uniform evaluation setup and the checkpoint-selection protocol in the main text (validation
every 20 steps on AIME2025 and selecting the checkpoint with the highest Pass@ 1 per method). All
numbers are percentages and are monotone non-decreasing in k as expected.

Statistical remarks. (1) Small-& accuracy. At k=32, w/AER attains 63.2% on AIME2024 and 48.9%
on AIME2025, improving over Base (48.0%, 35.5%) by +15.2/+13.4 points and over GRPO (47.0%,
34.3%) by +16.2/+14.6 points; it also surpasses w/Clip-Higher (61.5%, 46.9%) by +1.7/+2.0 points.
(2) Exploration headroom. The gain from k=32 to k=256 for w/AER is +9.2 points on AIME2024
(63.2-472.4) and +13.9 on AIME2025 (48.9-+62.8), exceeding w/Clip-Higher on both splits (+8.0,
+7.7), indicating a larger pool of viable solutions at higher k. (3) Sample efficiency via k59. Define
ks59 = min{k : Pass@k > 50%}. On AIME2024, kso is 8 for w/AER, 16 for w/Clip-Higher, and 64 for
both Base and GRPO. On AIME2025, kso is 64 for w/AER, 128 for w/Clip-Higher, 256 for Base, while
GRPO does not reach 50% by k=256. These trends are consistent with the intended effect of adaptive
entropy regularization.

C.2 Ablation on Reduction Ratio 7 (AER, p=0.2)

We ablate the target-entropy ratio r € {0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1.0, 1.2} with p fixed at 0.2 on Qwen3-
8B-Base. Results in Table 7 show that overall accuracy peaks near 70.4 (Avg. = 55.4), and degrades
as T increases into the high-entropy regime (r>1.0: 54.1/53.1). On the hard splits (AIME2024/2025),
moderate entropy reduction improves accuracy up to T=0.4 (31.4/25.1), whereas pushing 7 higher
reduces performance on AIME2025. On AMC23 and MATHS500, the trend is milder, AMC23 peaks at
T=0.8 (76.6), but the overall average remains best at 7=0.4. These observations support the existence of
a favorable entropy band centered around 7 * 0.4.

C.3 Ablation on Difficulty Threshold p (AER, 7=0.4)

We ablate the difficulty threshold p (formerly denoted (; cf. Section §4.2) while fixing the target-entropy
ratio at T=0.4. Recall that samples with group accuracy g(q) < p receive a positive entropy bonus (hard
set). Table 8 shows that narrower hard sets (smaller p) generally yield higher overall accuracy: the best
average is obtained at p=0.0 (55.8), and performance degrades when p is made overly inclusive (e.g.,

15


AER (6=0.2) AIME2024 AIME2025 AMC23 MATHS00 Avg.

T=0.2 30.8 23.8 73.7 88.9 54.3
T=0.3 29.8 23.2 75.2 88.8 54.2
T=0.4 31.4 25.1 75.6 89.4 55.4
T=0.5 30.5 24.0 75.6 89.3 54.8
T=0.6 31.4 23.2 75.9 89.2 54.9
T=0.8 30.0 24.1 76.6 88.9 54.9
T=1.0 31.1 232 72.8 89.2 54.1
T=1.2 28.7 20.7 74.2 88.8 Du,

Table 7: AER target-entropy ablation (9=0.2). Exact accuracy (%). Avg. is the mean over the four datasets. Best
values per column are in bold.

AER (7=0.4) AIME2024 AIME2025 AMC23 MATHS00 Avg.
p=0.0 33.1 24.6 75.6 89.7 55.8
p=0.2 31.4 25.1 75.6 89.4 55.4
p=0.4 30.2 26.1 76.4 89.3 55.5
p=0.6 32.1 23.7 75.1 89.5 ddel
p=0.8 29.5 26.1 75.9 89.4 D522
p=1.0 29.4 21.4 73.8 89.4 53:5

Table 8: AER ablation on the difficulty threshold p (fixed 7=0.4). Exact accuracy (%). Avg. is the mean over the
four datasets. Best values per column are in bold.

p=1.0, 53.5). This indicates that allocating the entropy budget only to the genuinely difficult prompts
preserves exploitation on easier ones and avoids diluting exploration across too many samples. At the
dataset level, AIME2025 and AMC23 achieve their peaks at moderate p (0.4/0.8 and 0.4, respectively),
suggesting a mild benefit from targeted—but not indiscriminate—expansion of the hard set.

16
