arXiv:2104.08835v2 [cs.CL] 30 Sep 2021

in Proc. of EMNLP 2021

CROSSFIT ‘ff: A Few-shot Learning Challenge for
Cross-task Generalization in NLP

Qinyuan Ye

Bill Yuchen Lin

Xiang Ren

University of Southern California
{qinyuany, yuchen.lin, xiangren}@usc.edu

Abstract

Humans can learn a new language task effi-
ciently with only few examples, by leveraging
their knowledge obtained when learning prior
tasks. In this paper, we explore whether and
how such cross-task generalization ability can
be acquired, and further applied to build bet-
ter few-shot learners across diverse NLP tasks.
We introduce CROSSFIT ¥, a problem setup
for studying cross-task generalization ability,
which standardizes seen/unseen task partitions,
data access during different learning stages,
and the evaluation protocols. To instantiate
different seen/unseen task partitions in CROSS-
FIT and facilitate in-depth analysis, we present
the NLP Few-shot Gym, a repository of 160
diverse few-shot NLP tasks created from open-
access NLP datasets and converted to a uni-
fied text-to-text format. Our analysis reveals
that the few-shot learning ability on unseen
tasks can be improved via an upstream learn-
ing stage using a set of seen tasks. We also
observe that the selection of upstream learning
tasks can significantly influence few-shot per-
formance on unseen tasks, asking further anal-
ysis on task similarity and transferability. !

1 Introduction

Pre-trained language models fine-tuned with abun-
dant task-specific data have become the predomi-
nant recipe for state-of-the-art results in NLP. How-
ever, these approaches are heavily dependent on
large-scale labeled datasets that are expensive to
create, and the resulting models still generalize
poorly to out-of-distribution inputs created with
small, harmless perturbations (Ribeiro et al., 2020).
In retrospect, researchers have advocated for build-
ing more human-like, general linguistic intelli-
gence that can “reuse previously acquired knowl-
edge about a language and adapt to a new task
quickly” (Yogatama et al., 2019; Linzen, 2020).

‘Our code is at https://github.com/INK-USC/CrossFit.

NLP Few-shot Gym
A repository of 160 diverse
few-shot tasks in NLP
ae
2 | er
|

The CrossFit Challenge 9

Stage 1. Upstream Learning
(Multitask Learning, Meta-learning, etc.)

|
|
Others |

|
|
|
| Question

pAsnwering’ iy

Initialize

== <a
Conditional ‘Classification !
Generation

Can we build a few-shot learner to
ei a tl generalize beyond task boundaires?

Figure 1: We present the CROSSFIT Challenge to study
cross-task generalization in a diverse task distribution.
To support this problem setting, we introduce the NLP
Few-shot Gym, a repository of 160 diverse few-shot,
text-to-text tasks in NLP.

Existing work has approached this problem via
better few-shot fine-tuning, by re-formulating tar-
get tasks into cloze questions that resembles the pre-
training objective (Schick and Schiitze, 2020a,b),
generating prompts and using demonstrations (Gao
et al., 2020). Such progress primarily focus on
improving instance-level generalization, i.e., how
to better generalize from few labeled instances to
make predictions about new instances, within the
scope of one individual task. From a broader per-
spective, human-like learning ability also benefits
from task-level generalization, or cross-task gener-
alization, i.e., how to learn a new task efficiently
given experiences of learning previous tasks.

Such ability has been widely studied in computer
vision and robotics community (Yu et al., 2020;
Triantafillou et al., 2020), but is relatively under-
explored in NLP. Pruksachatkun et al. (2020) and
Vu et al. (2020) study transferability between one
intermediate task and a given target task, while it’s
possible to further improve performance with multi-
ple intermediate tasks. Han et al. (2018) and Bansal
et al. (2020a) focus on cross-task generalization
within the scope of classification tasks, whereas hu-


mans can generalize across different task formats
(classification, multiple choice, generation, etc.),
goals (question answering, fact checking, etc.) and
domains (biomedical, social media, etc.).

Towards developing general linguistic intelli-
gence, we present CROSSFIT, a few-shot learning
challenge to acquire, evaluate and analyze cross-
task generalization in a realistic setting, with stan-
dardized training pipeline, data access and evalua-
tion protocol. The CROSSFIT challenge requires
a model to first learn from a set of seen tasks in
an upstream learning stage, and then perform few-
shot learning on a set of unseen tasks, as illustrated
in Fig. 1. In accompany, we introduce the NLP
Few-shot Gym, a repository of 160 few-shot NLP
tasks gathered from open-access resources, cov-
ering a wide range of capabilities and goals, and
formulated into a unified text-to-text format. To
analyze the capability and limitation of existing
approaches to the CROSSFIT challenge, we design
eight specific seen/unseen task partitions.

With the CROSSFIT Challenge and the NLP Few-
shot Gym, we aim to investigate the following re-
search questions:

¢ Q1. Can we teach cross-task generalization abil-
ity to pre-trained models with existing methods?

¢ Q2. During upstream learning, is it better to
be “well-rounded” (learning from diverse tasks)
or be “specialized and targeted” (learning from
tasks in the same category with unseen tasks)?

* Q3. Does it help if we have more labelled data
for seen tasks during upstream learning?

To address the above questions, we empirically
analyze the performance of multi-task learning
and three meta-learning algorithms (MAML (Finn
et al., 2017), first-order MAML and Reptile (Nichol
et al., 2018)). We observe that these approaches
can indeed lead to better few-shot performance
on unseen tasks. Interestingly, simple multi-task
learning outperforms existing meta-learning meth-
ods in many cases, encouraging future research on
identifying the reasons and developing improved
meta-learning methods. For Q2, we observe that
performance of individual unseen tasks varies with
different selection of seen tasks, calling for more
thorough investigation of the relationship between
task similarity and transferability. As for Q3, we
find that enlarging the size of upstream data does
not necessitate better cross-task generalization abil-
ities. We envision cross-task generalization to be
an integral component towards general linguistic

intelligence, and we hope CROSSFIT serves as a
useful testbed for driving related progress.

2 Related Work

Few-shot Fine-tuning. Few-shot learning refers
to teaching models a new task with a small num-
ber of annotated examples. Large-scale pre-trained
language models (e.g., BERT (Devlin et al., 2019))
have demonstrated great ability to learn new tasks
efficiently via fine-tuning (Zhang et al., 2021).
Schick and Schiitze (2020a,b) proposed pattern-
exploiting training (PET), which formulates text
classification and NLI tasks into cloze questions (or
“prompts”’) that resemble masked language model-
ing. PET can be further improved by generating
prompts automatically and incorporating demon-
strations into the input (Gao et al., 2020); and by
densifying the supervision signal with label con-
ditioning (Tam et al., 2021). While successful, in
these approaches the downstream tasks are learned
in isolation. Our work aims to boost few-shot learn-
ing ability on unseen tasks via acquiring cross-task
generalization ability from diverse seen tasks.

Meta-learning in NLP. Recent works have ex-
plored meta-learning methods for relation classi-
fication (Han et al., 2018; Gao et al., 2019), gen-
eral text classification (Dou et al., 2019; Bansal
et al., 2020a,b), low-resource machine transla-
tion (Gu et al., 2018), cross-lingual NLI/QA
(Nooralahzadeh et al., 2020). In general, these
works apply meta-learning algorithms to a set of
sub-tasks; however the sub-tasks are either syn-
thetic (e.g., classifying a new set of five relations
is a new sub-task) or drawn from a rather narrow
distribution (e.g., QA in one language is a sub-task).
In our work, we explore a more realistic setting —
learning from a set of NLP tasks with diverse goals:
classification, question answering, conditional gen-
eration, etc. This setting is attracting attention in
NLP community rapidly and is also explored in
very recent work (Zhong et al., 2021; Mishra et al.,
2021; Bragg et al., 2021; Wei et al., 2021).

Unifying NLP Task Formats. Researchers have
explored unifying the formats of different tasks,
in order to better enable knowledge transfer, e.g.,
DecaNLP (McCann et al., 2018), UFO-Entail (Yin
et al., 2020) and EFL (Wang et al., 2021). Fol-
lowing T5 (Raffel et al., 2020), we adopt a uni-
fied text-to-text format that subsumes all text-based
tasks of interest. Related to our work, UnifiedQA


(Khashabi et al., 2020) examines the feasibility
of training a general cross-format QA model with
multi-task learning. Our work extends from these
ideas, and we significantly enlarge the task repos-
itory to 160 to broaden the coverage, in hopes to
build a general-purpose few-shot learner.

3. The CROSSFIT Challenge

In this section, we present the CROSSFIT Chal-
lenge, a problem setting for acquiring and evalu-
ating cross-task generalization. Ideally, a strong
CROSSFIT system can capture cross-task general-
ization ability from a set of seen tasks and thus
adapts to new unseen tasks efficiently.

3.1 Preliminaries

The meaning of “task” is overloaded: “tasks” can
be categorized at different granularity (e.g., text
classification vs. QA, yes/no QA vs. machine read-
ing comprehension), and from different aspects
(e.g., domain, label space). Herein we take a gen-
eral formulation by defining a “task” with its train-
ing and testing examples. We define a task T' as
a tuple of (Dirain, Daev; Ptest). Each set D is a
set of annotated examples {(2;, y;)} in text-to-text
format. In few-shot setting, the size of Drain and
Daey are required to be small (e.g., 16 example per
class for classification tasks).

Existing work mostly focuses on improving
instance-level generalization for individual task by
using task-specific templates. Performance on in-
dividual tasks is used as the measure of success.
For the CROSSFIT Challenge, we aim to acquire
cross-task generalization and build better general-
purpose few-shot learners, which calls for a differ-
ent problem setting with distinct training procedure
and evaluation protocol.

3.2 Problem Setting

Tasks and Data. To acquire and evaluate cross-
task generalization, we first gather a large reposi-
tory of few-shot tasks 7, and partition them into
three non-overlapping sets Ttrain, Tdev, Ttest- In
hopes to examine the capability and limitation of an
approach in different settings, and to answer our re-
search questions, we design multiple task partitions
with different focuses. Details of the repository and
partitions, or as we name them, the NLP Few-shot
Gym, are deferred to §4.

Learning Stages. A CROSSFIT method may
learn from 7t;ain and perform necessary tuning

with Jaey in the upstream learning stage; it is then
evaluated with few-shot tasks in Tyrese:

¢ Upstream learning stage. Here, the algorithm
has access to the D4y-qin and Dae, for each train-
ing task in Ttrain, While Dyes¢ is unavailable. The
algorithm also has access to all data in Tey, but
for validation purpose only (i.e., it is not allowed
to use Taey to update model weights).

¢ Few-shot learning stage. In this stage, Tre¢ be-
came available. Models resulting from the up-
stream learning stage are required to learn from
Derain Via a particular few-shot learning method
(e.g., direct fine-tuning). The final few-shot learn-
ing performance is evaluated on Dyes. 2

Evaluation Metric. Evaluating the performance
of a model on a diverse collection of NLP tasks is
inherently challenging, as different tasks use dif-
ferent metrics. It is thus not reasonable to simply
aggregate performance of classification tasks (e.g.,
accuracy, F1) and generation tasks (e.g., ROUGE,
BLEU) by taking the average.

To address this problem, we first narrow down to
a collection of 7 evaluation metrics: classification
Fl, accuracy, QA F1, exact match (EM), Rogue-
L, Matthew correlation, and Pearson correlation,
which cover all tasks in our experiments. Then, we
define Average Relative Gain (ARG), a metric that
computes relative performance changes before and
after the upstream learning stage for each test task,
and finally take the average across all test tasks.

For example, suppose we have yest
{T4, Tp}. If an upstream learning algorithm helps
improve the few-shot learning performance from
50% F1 score to 70% on task T'4 (i.e., a 40%
relative improvement), and from 40% accuracy
to 30% on task Tp (i.e., —25% relative improve-

ment), the final ARG on 7;., would be computed
as LO%H(=25%) _ 7 594,

The ARG metric reflects the overall performance
gain on all tasks in J¢e,¢, no matter what specific
metrics each task uses. We use ARG for a high-
level comparison, and we still analyze the perfor-
mance for each task (e.g., absolute performance
metrics, performance growth with “more shots”,
sensitivity to different selection of Ttpqin) in our
in-depth analysis.

For clarification, the performance on the Daew of a task
in Taev OF Tiest will be used for tuning hyper-parameters
during fine-tuning. The overall performance on Jaey is used
for tuning tuning hyper-parameters during upstream learning.


4 NLP Few-shot Gym

Towards learning to generalize across tasks in
CROSSFIT challenge, we need a resource that con-
tains sufficient number of tasks, covering a wide
range of NLP applications, and presented in a uni-
fied text-to-text format. Herein, we introduce the
NLP Few-shot Gym, a repository of 160 few-shot
tasks gathered from existing open-access datasets.

4.1 Dataset Selection

We choose to use Huggingface Datasets? (Lhoest
et al., 2021) as the pool of our candidate tasks.
We filter these datasets on a case-by-case basis,
mainly using the following criteria: (1) We focus
on English monolingual datasets. (2) We exclude
datasets that require information retrieval, as they
require a separate retriever. (3) We exclude se-
quence labeling tasks (e.g., dependency parsing,
NER), which are highly dependent on tokenization,
and are hard to evaluate in text-to-text format. (4)
We exclude datasets dealing with extremely long
documents (e.g., a scientific paper) as input, as
most pre-trained models cannot process such long
input sequences. We finalize our selection with 160
datasets which are detailed in Appendix A.

4.2 A Unified Text-to-Text Format

We follow Raffel et al. (2020) and convert all of
our datasets into a unified text-to-text format. For
example, the task of natural language inference
(originally a sentence-pair classification problem)
becomes: premise: <premise> hypothesis:
<hypothesis>, and the target sequence is either the
word entailment, contradiction or neutral.
As for machine reading comprehension tasks, the
input format is question: <question> context:
<context> and the target sequence is the correct
answer span. We also reference the format for QA
tasks from UnifiedQA (Khashabi et al., 2020).

4.3 Formulating Few-shot Tasks

We mainly follow the practice in (Gao et al., 2020)
for few-shot sampling. For classification and re-
gression tasks, we include 16 training examples
per class in Dtrain. For other types of tasks, we
include 32 examples in Dt;rain. In conformity with
real-world situations where labeled data are scarce,

$https://huggingface.co/datasets. It is an extensible library
that provides access to 626 open-access NLP datasets (as of
Feb 25th, 2021) with a unified, open-source API.

Classification

Sentiment Analysis
Amazon_Polarity (McAuley et al. 2013)
IMDB (Maas et al. 2011)
Poem_Sentiment (Sheng et al. 2020) ...

Paraphrase Identification
Quora Question Paraphrases (Quora)
MRPC (Dolan et al. 2005)

PAWS (Zhang et al. 2019) .

Natural Language Inference

MNLI (Williams et al. 2018)
QNLI (Rajpurkar et al. 2016)
SciTail (Knot et al. 2018) ...

Others (topic, hate speech, ...)

Conditional Generation

Summarization

Gigaword (Napoles et al. 2012)
XSum (Narayan et al. 2018) ...

Question Answering

Reading Comprehension
SQUAD (Rajpurkar et al. 2016)
QuoRef (Dasigi et al. 2019)

TweetQA (Xiong et al. 2019) ..

Multiple-Choice QA
CommonsenseQA (Talmor et al. 2019)
OpenbookQA (Mihaylov et al. 2018)
Al2_ARC (Clark et al. 2018) ..

Closed-book QA

WebQuestions (Berant et al. 2013)
FreebaseQA (Jiang et al. 2019)
KILT-NQ (Kwiatkowski et al. 2019) ...

Others (yes/no, long-form QA)

Others

Regression

Mocha (Chen et al. 2020)
Yelp Review Full (Yelp Open Dataset) ...

Dialogue Others

Acronym Identification

Sign Language Translation
Autoregressive Entity Linking
Motion Recognition

Pronoun Resolution ...

Empathetic Dialog (Rashkin et al. 2019)
KILT-Wow (Dinan et al. 2019) ...

Others (text2SQL, table2text ...)

Figure 2: Task Ontology for the NLP Few-shot Gym.
Full information is listed in Appendix A.

we assume a development set Dg-, which shares
the same size with Dtrain.

We sample Dirgin and Daey splits from each
dataset’s original train set with 5 different random
seeds. This helps us reduce variance during few-
shot evaluation, and also enlarges the number of
few-shot tasks used for learning. Consequently,
the “effective size” of our NLP Few-shot Gym is
160 x 5 = 800, while we use the number 160
throughout the paper to avoid possible confusion.

We use the original development set for each
dataset as Diest, or withhold 20% of the dataset
when the official development split is not available.
The held-out test examples are sampled once before
sampling Dirain and Ddev.

4.4 Task Ontology and Partitions

As mentioned in §3.2, a CROSSFIT method is ex-
pected to first acquire cross-task generalization on
a set of Train and evaluate such ability on Test. To
comprehensively analyze to what extent a trained
model can generalize, and how its behavior differs
in different scenarios, we need to build different
partitions of CTarentins Tien Test)

Towards this goal, we first manually classify
the 160 tasks and form a task ontology with cate-
gories and sub-categories, as shown in Fig. 2. The
first-level categories include classification, ques-
tion answering, conditional generation, and oth-


ers. Further, we design eight different partitions
of (Ttrain, Taev; Ttest). We illustrate four partitions
in Fig. 3 and provide more details in Table 1.

Our Partition 1 randomly split all 160 few-shot
tasks into the three sets, where |Tt-ain| = 120 and
\Taev| = |Ttest| = 20. The design of Partition 1
mimics the real-world language learning environ-
ment where the goal is to build a general-purpose
few-shot learner, and a set of diverse tasks (Tty-ain)
are used to train the learner. Our Partition 2.1-2.3
withhold 10 classification tasks for development
and 10 more for testing. The 7z;-ain is controlled
to have either 100% classification tasks, 100%
non-classification tasks, or half-and-half. These
three partitions help us to understand the influence
brought by different task distribution in 7;;-ain. The
remaining four partitions still focus on crossing
task boundaries, but in a finer granularity: seen
and unseen tasks are in the same category, but not
the same sub-category. For example, Partition 3.1
has 57 non-NLI classification tasks as Tirain, and
8 NLI tasks as Ties¢. These partitions help us to un-
derstand whether cross-task generalization in this
finer granularity is easier for model to acquire.

5 Methods to CROSSFIT

We mainly use BART-Base (Lewis et al., 2020) as
the text-to-text transformer for our analysis in the
CROSSFIT setup. We leave confirmatory experi-
ments with T5-v1.1-Base and BART-Large model
in Appendix C.

Direct Fine-tuning on Test Tasks. This serves
as the basic baseline method for the CROSSFIT
challenge, which does not make use of Ttrain OF
Tdey, or go through the upstream learning stage.
For each task T’ © Ttest, we directly fine-tune the
text-to-text model with its Dz;ain, tune the hyper-
parameters with Dgey, and assess its performance
with the test set Dtest. We use the performance of
direct fine-tuning as the base for computing ARG
scores of other CROSSFIT approaches. We expect
a model trained with upstream learning would cap-
ture cross-task generalization ability and thus have
better ARG scores.

Multi-task Learning (MTL). A _ straight-
forward yet effective method is to combine the
data in the training tasks to learn a multi-task

“We later discuss the limitation of this design in §6-Q2

Both Dirain and Daey are used, as Dae» is used for gra-
dient updates in meta-learning algorithm. We do so to make
sure that the data access for the two methods is fair.

O Training Task 2: DevTask ©) Test Task Unused Task

ef @e|/|* oo e
3° coi 9°
Gusstion
Others Answreing

, @
©
Question

Answreing

Conditional
Generation

Conditional
Generation

Classification
% es

2°
of’e @, e°

(a) Random Split

(b) 45non-class

@ @@
@
OP

Question
Answreing

Question
Others Answreing Others

Conditional
Generation

Conditional
Generation

Classification Classification

@

€ e
(2) ©
Q9 ef

(c) Held-out-NLI (d) Held-out-MRC

Figure 3: Illustration for different task partitions.
We evaluate a CROSSFIT approach on different task
partitions to examine its generalization ability in dif-
ferent scenarios. Full details in Table 1. The locations
and distances in this figure are hypothetical and for il-
lustrative purposes only.

model, before fine-tuning it on each test task.
Specifically, we gather source-target examples for
all tasks in 7;;q;n and fine-tune the text-to-text
model with these examples. Then we use the
resulting checkpoint as initialization and perform
the same procedure in “direct fine-tuning” for each
test task in Ttes¢. The performance gain over the
direct fine-tuning is used for computing its overall
ARG score.

Model-Agnostic Meta-learning (MAML).
Cross-task generalization ability, closely aligns
with the concept of learning to learn. Hence, we
use MAML (Finn et al., 2017), a representative
meta-learning approach during upstream learning.
The core concept of MAML is to learn a set of
initialization weight, from which the model adapts
fast to a new task within few gradient updates.
In MAML training, we iterate through tasks in
Tirain to update the model. For each train task
(Dirain; Paev), We first sample a support batch
Bsupport from Derain and a query batch Bayery
from Dgey. We use fg to denote the text-to-text
model with parameters 0. Using Bsupport, We
first compute the updated parameters 6’ with
gradient descent (i.e., the inner loop). Due to the
large size of pre-trained text-to-text models, we


No. Shorthand Tisai Trev Tiss ARG(Multi) ARG(MAML) ARG(FoMAML) ARG(Rept.) | Details
1 Random 120 20 20 35.06% 28.50% 22.69% 25.90% Fig. 4(a)

2.1 45cls 45 cls. 10 cls. 10 cls. 11.68% 9.37% 10.28% 13.36%

2.2 | 23cls+22non-cls 23 cls. + 22non-cls. 10 cls. 10 cls. 11.82% 9.69% 13.75% 14.34% Fig. 5

2.3 45non-cls 45 non-cls. 10 cls. 10 cls. 11.91% 9.33% 11.20% 14.14%

3.1 Held-out-NLI 57 non-NLI cls. / 8 NLI 16.94% 12.30% 12.33% 14.46% Fig. 4(b)

3.2 | Held-out-Para 61 non-Paraphrase cls. / 4 Para. Iden. 18.21% 17.90% 21.57% 19.72% Fig. 4(c)

4.1 | Held-out-MRC 42 non-MRC QA / 9 MRC 32.81% 27.28% 28.85% 28.85% Fig. 4(d)

4.2 | Held-out-MCQA 29 non-MC QA 1 22MCQA | 12.20% 4.69% 6.73% 7.67% — | Fig. 4(e)

Table 1: (TtrainsTdev,Ttest) partitions used in the study (full lists in Appendix B), and their ARG scores when
upstream learning methods are applied. “cls.” stands for “classification”, “Para. Iden.” for “paraphrase identifica-
tion’, “MRC” for “machine reading comprehension” and “MCQA’” for “multiple-choice QA”.

use one gradient update in the inner loop, ie.,
0’ = 0-—aVoLl(fo, Bsupport). Then we apply the
updated text-to-text model fg to Byuery, and do
one step of meta-optimization (i.e., the outer loop),
with 6 — 6 — BVoL( for, Bauery)-

First-order MAML. First-order MAML (Finn
et al., 2017) avoids second-order optimization and
improves training stability using the first-order
approximation by differentiating with respect to
the fast weights 0’ instead of the original parame-
ters 6 for the gradient VoL( for, Bguery), ie, 9 —
6 — BV aL for, Bauery)-

Reptile. Reptile (Nichol et al., 2018) is another
memory-efficient, first-order meta-learning algo-
rithm that first makes multiple gradient updates in
the inner loop, then directly uses 6’ — 6 to approxi-
mate VoL( for, Bauery)s ie, PC O+ BO — 0).

6 Empirical Analysis

In this section we look to interpret the results and
answer our research questions. We summarize the
ARG scores in Table 1 and plot the performance of
each test task (for each partition) in Fig. 4-5.

50m Q1. Can we teach pre-trained LMs to gener-
alize across tasks with existing methods?

Overall Performance. From Table 1, we ob-
serve that, on average, the tested upstream learning
methods indeed improve cross-task generalization:
their ARG scores are positive, meaning that they
are better than direct fine-tuning (ARG=0%). Fur-
ther, by aggregating results from all upstream learn-
ing methods and task partitions, we find that the
performance on 51.47% test tasks are significantly
improved (> 5% relative improvement compared
to direct fine-tuning); 35.93% tasks are relatively
unaffected (between +5%); and 12.60% tasks suf-
fer from worse performance (< —5%).

Correlated Performance Gains. The perfor-
mance gain obtained with different upstream learn-
ing methods are correlated with each other — i.e.,
tasks that benefit from multi-task learning is likely
to also benefit from meta-learning. For the Ran-
dom partition, the Spearman Correlation between
the relative improvement brought by MTL and
MAML is 0.66, with p value equals to 0.0015. This
suggests that different upstream learning methods,
while taking different optimization objectives, cap-
ture similar inductive bias from 7Tt;ain.

MTL is a strong baseline. Surprisingly, the
most straight-forward multi-task learning method
is hard to beat. This could be counter-intuitive, as
meta-learning methods are specifically designed
for rapid generalization to unseen tasks, sharing
the same goal with our CROSSFIT challenge. We
think there are three possible reasons: (1) Due to
memory constraints, we limit the number of inner-
loop updates to be one, which may be insufficient.
Also, meta-learning methods are highly sensitive
to hyper-parameters and even random seeds (An-
toniou et al., 2019), which we do not tune exhaus-
tively for practical reasons. (2) Text-to-text trans-
formers have much more complex architectures,
while most meta-learning methods are typically
applied to small feed-forward/convolutional net-
works. (3) The CROSSFIT challenge has a highly
diverse set upstream tasks, which may introduce
under-explored difficulties. That being said, we
believe it is important to identify the true cause,
and to develop improved meta-learning methods
for the CROSSFIT challenge as future work.

Forgetting Pre-Trained Knowledge. A few test
tasks have negative performance gain after up-
stream learning, including Glue-COLA (measuring
linguistic acceptability) and Domain Crawl (sepa-
rating domain names into tokens) in the Random


100%

50%

25%

0%

-25%

Relative Performance Gain (%)

50%
40%

0%

(a) Random

— direct fine-tuning

75% 4-

—@H multi-task learning

== mami (first-order maml

| reptile

yn> ae it wy Ooh Asi me c® gor gad a ott ore Na ww oe
ow v gor os per Are i> oe mae er” es oN a’ a a or - An ae? aoe As yes wi ‘ oh e 0S poe ae
as “ye oO ee whe” 0%" = gn” wer ¥
a & °
nt

(b) Held-Out-NLI

50%

(c) Held-Out-Para (d) Held-Out-MRC

30%
20% +-
10% 4

40%
30%

0%

20% +-
10% +-~

80%
60%
40%
20%

0%

-10%

-10%

Relative Performance Gain (%)

“ae oo?

iG
an? ave"

aw an os ok et® so 9
ov” Sse V9

© 2 a oe
e° Ee can as
gre gv' ow sve

60%

-20%
GC hk xd - A S Vg KS , Ge nae
gaye co? go’ OF eye Teast Oro Toor He
ne
as

we

(e) Held-Out-Multiple-Choice

50%
40%
30%
20%
10%
0%
-10%
-20% _
-30% T T T T T T T T

Relative Performance Gain (%)

T
x “ a _ gt
ote iO

ov S

Figure 4: Experimental results for the CROSSFIT challenge with different task partitions.

Pros ann’ is — eo? os 08 3 Seo 02 eS
ove ere as ow

eee ee oo

(oe)

The details of each

partition is shown in Table 1. Relative performance gain is computed based on the results of direct fine-tuning.
Best viewed in color. Green color is used to highlight the Average Relative Gain (ARG) for each method.

Partition setting. For Glue-COLA, similar observa-
tions are reported by Pruksachatkun et al. (2020)
in an intermediate-task transfer learning setting,
where the authors conjecture catastrophic forget-
ting of the masked language modeling (MLM)
tasks may be the cause. BART uses denoising pre-
training objective, a variant of MLM. Intuitively,
Domain Crawl is also one of the most similar tasks
to denoising in all test tasks, which further sup-
ports this hypothesis. We thus conjecture that for
test tasks that resemble pre-training objectives, up-
stream learning could hurt performance due to the
catastrophic forgetting phenomena.

Understanding negative transfer (Wu et al., 2020)
and selecting source tasks to avoid negative transfer
(Vu et al., 2020) are also growing research topics.
In this work we refrain from further investigation;
however we believe combating negative transfer
and thus improving CROSSFIT performance is a
promising future direction.

som Q2. Well-rounded or specialized? Which is
a better strategy of upstream learning?

“Learning to be well-rounded vs. learning to
be specialized” is a common dilemma that human
learners struggles with. For the CROSSFIT chal-
lenge, the former refers to learning from a set of
diverse tasks in upstream learning; the latter refers
to learning from a set of tasks closer to target few-
shot tasks. To study this research question, we want
to find out which option works better in upstream
learning. Put differently, we aim to analyze the
influence of upstream task selection for a fixed
set of the downstream tasks.

Setup. We first conduct controlled experiments
with Partition 2.1-2.3, where Tres; is a fixed set
of classification tasks, and Ttpqin varies. In Par-
tition 2.1, all tasks in Ttrain are classification
tasks (i.e., “specialized and targeted”); in Partition


(a) Multi-task Learning

100%
Mmm 45 classification tasks

75% + Mmm 23 classification + 22 non-classification tasks 7
45 non-classification tasks

50% 4
sp? aI
& & oy

Relative Performance Gain (%)

25% 4 Re
an on Lb i
-25% - - + + ; ; + r
) PAY ce Rn Co Ro oo) od ay e
ew a oe om et i on ar go se ye we®
e A o C\s) en pr” ye?
g™
eS (b) Meta-Learning
S& 100%
<
i ss
2
8
5 50% 4
E eae
& 25% 4 NOP
5 97 9° 9
— rc. , = __fo th rT
g
2
3
@ 25% : 1 + + + ; ; ; + : :
ce AY © . g® 4h got ct oo) AS) e
so SF 9G ge cc god GoM OM gee?
x Kn) a 5) an pr? ye?
gn 00

Figure 5: Comparison for the controlled experiment on
Partition 2.1-2.3. Tres; is a fixed set of 10 classification
tasks, while 7i,ain Varies.

2.2, half of the tasks are classification tasks (i.e.,
“well-rounded”); in Partition 2.3, all tasks are non-
classification tasks (i.e., “specialized in an opposite
direction’, for a controlled experiment).

Analysis and Discussion. It is surprising at first
that non-classification tasks and classification tasks
are equivalently helpful in terms of ARG scores
(see Fig. 5). On a second thought, this observation
is encouraging as it demonstrates that acquiring
cross-task generalization is feasible and promising,
even when 7Ji;ain and Ties¢ are drastically differ-
ent. It also suggests that our categorization of tasks
(§4.4) may not align with how models learn trans-
ferable skills: selecting 7Tz;-ain tasks that have the
same format and goal as the test task may not lead
to optimal transfer.

In retrospect, we acknowledge that our design of
ontology and partitions based on task format and
goal is flawed. This is merely one aspect of “task
similarity”. However, understanding the complex
relationship between tasks is another challenging
and under-explored problem. We consider our on-
tology as a starting point, rather than a fixed final
one. We use the current ontology to guide our ex-
periment and analysis, and we hope future analysis
could help build a more informative ontology.

Case Studies. We further look at cases where a
test task appear in 7;,,¢ of multiple partitions. For
example, AI2_ARC and Race-High are in the Ttes¢
of both Random partition and Held-out-MCQA
partition. We present the results in Table 2. In
general, the performance of these tasks varies when

Test Task Partition Ainaitti  Ameia
Random 15.89% 11.55%
Glue-QNLT  Held-Out-NLI 10.88% 10.94%
Random 1.30% 4.22%
A1D_ARG Held-Out-MCQA 6.49%  —6.22%
eT Random 26.71% 6.59%
8" Held-Out-MCQA 7.27% + —6.28%
QuoRef Random 25.47% 3.99%
Held-Out-MRC 12.25% 4.64%

Table 2: Performance comparison of test task perfor-
mance when different 7i,-ain Sets are used in upstream
learning. See text in Q2 for in-depth analysis.

Relative Performance Gain (%)

Figure 6: Controlling upstream learning data size in
with Held-out-Para Partition. Enlarging the size of data
during upstream learning does not necessitate better
cross-task generalization ability.

different 71,-ain sets are used. However, we have
not found consistent patterns of what type of Ttrain
lead to better performance for a specific test task.

“¢ Q3. Does it help if we have more labelled
data for upstream tasks?

As described in §4.3, we limit our upstream tasks
to be also few-shot: classification tasks have 16 ex-
amples per class, and non-classification tasks have
32 examples. This decision is empirically deter-
mined following prior works (Schick and Schiitze,
2020a,b; Gao et al., 2020) and makes our exten-
sive analysis practical and efficient. It is possible
that using more data for each upstream task can
significantly improve cross-task generalization. To
investigate this, we conduct a set of controlled ex-
periments where the number of examples in up-
stream tasks are changed to [2, 4, 8] times of the
original size. We use the Held-out-Para Partition
and multi-task learning for the experiments, and
present the result in Fig. 6. Surprisingly, we find
that the effect from using more upstream data is
inconsistent on different target tasks. The overall
ARG for all sizes are close: even 8x larger up-


stream data leads to only 4% improvement in ARG.
We conclude that enlarging the size of data during
upstream learning does not necessitate better cross-
task generalization ability. This also justifies our
decision to keep upstream tasks few-shot.

"¢ Q4-Q6. Additional Analysis

Due to space limit, we summarize our other find-
ings below and defer the details to Appendix C.

Few-Shot — More-Shot (Q4). In practice, users
may continue to collect data over time. We wonder
if cross-task generalization ability is still helpful
for medium/high-resource target tasks. We find
that the performance gain from upstream learning
is still evident when 1024 shots are available. The
performance gap diminishes with millions of train-
ing examples.

Using Different Base Models (Q5). We extend
our analysis on BART-base (139M) to larger pre-
trained text-to-text Transformers: BART-Large
(406M) and T5-v1.1-Base (248M). Generally, the
performance grows with models sizes with only
few exceptions, which suggests that upstream learn-
ing methods we use are model-agnostic, and can
be applied to larger models to further improve few-
shot performance.

Integration with PET Training (Q6). Pattern-
exploiting training (PET) (Schick and Schiitze,
2020a,b) was originally proposed for classification
tasks and encoder language models. We test a few
variants of PET training with BART-Base and try
applying PET training after upstream learning. In
general we observe deteriorated performance com-
pared to direct fine-tuning. We hypothesize that
PET methods are not directly applicable to encoder-
decoder language models used in our study.

7 Conclusion and Future Work

In this paper, we study the problem of building
better few-shot learners via acquiring cross-task
generalization ability from diverse NLP tasks. To-
wards our goal, we introduce the CROSSFIT Chal-
lenge, an task setup that standardizes the training
pipeline, data access and evaluation protocol. We
also present the NLP Few-shot Gym, a reposi-
tory of 160 diverse few-shot NLP tasks, to sup-
port CROSSFIT learning in different scenarios. We
empirically demonstrated that cross-task general-
ization can be acquired via multi-task learning and

meta-learning; confirmed that the selection of seen
tasks would influence the few-shot performance on
unseen tasks.

We have highlighted several unexpected or un-
desired observations in our analysis, for which
we invite future work in understanding and com-
bating related issues. In addition, we envision
the CROSSFIT Challenge and the NLP Few-shot
Gym to serve as the testbed for many interesting
“meta-problems”, such as (1) learning to generate
prompt for diverse task formats and further improve
learning efficiency (Shin et al., 2020; Gao et al.,
2020); (2) learning to select appropriate source
tasks to learn from during upstream learning (Za-
mir et al., 2018; Standley et al., 2020), potentially
with task2vec methods (Achille et al., 2019; Vu
et al., 2020); (3) applying task augmentation strate-
gies to prevent over-fitting (Murty et al., 2021); (4)
learning to accumulate knowledge and avoid catas-
trophic forgetting in an continual learning setup
(Jin et al., 2021); (5) decomposing complex tasks
into atomic tasks and exploring cross-task general-
ization through the lens of compositionality (An-
dreas et al., 2016; Khot et al., 2021).

Acknowledgments

We thank authors and crowd-workers of all datasets
used in our study. We thank huggingface datasets
team for making datasets more accessible. We
thank anonymous reviewers and members of USC
INK Lab for their valuable feedback. This
work is supported in part by the Office of the
Director of National Intelligence (ODND, In-
telligence Advanced Research Projects Activity
(IARPA), via Contract No. 2019-19051600007;
the DARPA MCS program under Contract No.
N660011924033; the Defense Advanced Research
Projects Agency with award W911NF-19-20271;
NSF IS 2048211.

References

A. Achille, Michael Lam, Rahul Tewari, A. Ravichan-
dran, Subhransu Maji, Charless C. Fowlkes, Stefano
Soatto, and P. Perona. 2019. Task2vec: Task em-
bedding for meta-learning. 2019 IEEE/CVF Inter-
national Conference on Computer Vision (ICCV),
pages 6429-6438.

Tiago A. Almeida, José Maria G. Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of sms
spam filtering: New collection and results. In Pro-
ceedings of the 11th ACM Symposium on Document


Engineering, DocEng ’11, page 259-262, New York,
NY, USA. Association for Computing Machinery.

Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. MathQA: Towards interpretable
math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 2357-2367, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016. Learning to compose neural net-
works for question answering. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1545-
1554, San Diego, California. Association for Com-
putational Linguistics.

Antreas Antoniou, Harrison Edwards, and Amos
Storkey. 2019. How to train your MAML. In Inter-
national Conference on Learning Representations.

Trapit Bansal, Rishikesh Jha, and Andrew McCallum.
2020a. Learning to few-shot learn across diverse
natural language classification tasks. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics, pages 5108-5123, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.

Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,
and Andrew McCallum. 2020b. Self-supervised
meta-learning for few-shot natural language classifi-
cation tasks. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 522-534, Online. Association
for Computational Linguistics.

Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Proceedings of the sec-
ond PASCAL challenges workshop on recognising
textual entailment, volume 6, pages 6—4. Venice.

Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetE-
val: Unified benchmark and comparative evaluation
for tweet classification. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020,
pages 1644-1650, Online. Association for Computa-
tional Linguistics.

Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas-
tian Riedel, and Pontus Stenetorp. 2020. Beat the
AI: Investigating adversarial human annotation for
reading comprehension. Transactions of the Associ-
ation for Computational Linguistics, 8:662-678.

Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
Giampiccolo. 2009. The fifth pascal recognizing tex-
tual entailment challenge. In TAC.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533-1544, Seattle, Wash-
ington, USA. Association for Computational Lin-
guistics.

Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
International Conference on Learning Representa-
tions.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language. In
Thirty-Fourth AAAI Conference on Artificial Intelli-
gence.

Michael Boratko, Xiang Li, Tim O’Gorman, Rajarshi
Das, Dan Le, and Andrew McCallum. 2020. Pro-
toQA: A question answering dataset for prototypi-
cal common-sense reasoning. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1122-1136,
Online. Association for Computational Linguistics.

Jan A. Botha, Manaal Faruqui, John Alex, Jason
Baldridge, and Dipanjan Das. 2018. Learning to
split and rephrase from Wikipedia edit history. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
732-737, Brussels, Belgium. Association for Com-
putational Linguistics.

Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Belt-
agy. 2021. Flex: Unifying evaluation for few-shot
nip.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.

Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019. SemEval-2019
task 3: EmoContext contextual emotion detection in
text. In Proceedings of the 13th International Work-
shop on Semantic Evaluation, pages 39-48, Min-
neapolis, Minnesota, USA. Association for Compu-
tational Linguistics.

Anthony Chen, Gabriel Stanovsky, Sameer Singh, and
Matt Gardner. 2020a. MOCHA: A dataset for train-
ing and evaluating generative reading comprehen-
sion metrics. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 6521-6532, Online. Associa-
tion for Computational Linguistics.


Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-
nandez, and Doug Downey. 2019. CODAH: An
adversarially-authored question answering dataset
for common sense. In Proceedings of the 3rd Work-
shop on Evaluating Vector Space Representations
for NLP, pages 63-69, Minneapolis, USA. Associ-
ation for Computational Linguistics.

Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact: A large-scale
dataset for table-based fact verification. In Interna-
tional Conference on Learning Representations.

Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difficulty of natural yes/no questions. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 2924-2936, Min-
neapolis, Minnesota. Association for Computational
Linguistics.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv,
abs/1803.05457.

Arman Cohan, Waleed Ammar, Madeleine van Zuylen,
and Field Cady. 2019. Structural scaffolds for ci-
tation intent classification in scientific publications.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume I (Long and Short Papers), pages
3586-3596, Minneapolis, Minnesota. Association
for Computational Linguistics.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Machine Learning Challenges Work-
shop, pages 177-190. Springer.

Pradeep Dasigi, Nelson F. Liu, Ana Marasovié¢,
Noah A. Smith, and Matt Gardner. 2019. Quoref:
A reading comprehension dataset with questions re-
quiring coreferential reasoning. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5925-5932, Hong Kong,
China. Association for Computational Linguistics.

Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
Proceedings of the 11th International AAAI Confer-
ence on Web and Social Media, ICWSM ’17, pages
512-515.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of

deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leippold.
2020. Climate-fever: A dataset for verification of
real-world climate claims. ArXiv, abs/2012.00614.

Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents. In International Conference on Learning
Representations.

William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).

Zi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.
2019. Investigating meta-learning algorithms for
low-resource natural language understanding tasks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 1192-—
1197, Hong Kong, China. Association for Computa-
tional Linguistics.

Matthew Dunn, Levent Sagun, Mike Higgins, V. U.
Giiney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with con-
text from a search engine. ArXiv, abs/1704.05179.

Ondfej DuSek, David M. Howcroft, and Verena Rieser.
2019. Semantic noise matters for neural natural lan-
guage generation. In Proc. of the 12th International
Conference on Natural Language Generation, pages
421-426, Tokyo, Japan. Association for Computa-
tional Linguistics.

Ondiej DuSek, Jekaterina Novikova, and Verena Rieser.
2020. Evaluating the State-of-the-Art of End-to-End
Natural Language Generation: The E2E NLG Chal-
lenge. Computer Speech & Language, 59:123-156.

Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique
Laforest, and Elena Simperl. 2018. T-REx: A large
scale alignment of natural language with knowledge
base triples. In Proceedings of the Eleventh Inter-
national Conference on Language Resources and
Evaluation (LREC-2018), Miyazaki, Japan. Euro-
pean Languages Resources Association (ELRA).

Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstrac-
tive hierarchical model. In Proceedings of the 57th


Annual Meeting of the Association for Computa-
tional Linguistics, pages 1074-1084, Florence, Italy.
Association for Computational Linguistics.

Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELIS:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 3558-3567, Florence,
Italy. Association for Computational Linguistics.

Manaal Faruqui and Dipanjan Das. 2018. Identifying
well-formed natural language questions. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 798-803,
Brussels, Belgium. Association for Computational
Linguistics.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning, vol-
ume 70 of Proceedings of Machine Learning Re-
search, pages 1126-1135. PMLR.

Tianyu Gao, A. Fisch, and Danqi Chen. 2020. Making
pre-trained language models better few-shot learn-
ers. ArXiv, abs/2012.15723.

Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng
Li, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0:
Towards more challenging few-shot relation classifi-
cation. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
6250-6255, Hong Kong, China. Association for
Computational Linguistics.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL workshop on textual entailment and
paraphrasing, pages 1-9. Association for Computa-
tional Linguistics.

Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and
Montse Cuadros. 2018. Hate Speech Dataset from
a White Supremacy Forum. In Proceedings of the
2nd Workshop on Abusive Language Online (ALW2),
pages 11-20, Brussels, Belgium. Association for
Computational Linguistics.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and
Aleksander Wawer. 2019. SAMSum corpus: A
human-annotated dialogue dataset for abstractive
summarization. In Proceedings of the 2nd Workshop
on New Frontiers in Summarization, pages 70-79,
Hong Kong, China. Association for Computational
Linguistics.

Andrew Gordon, Zornitsa Kozareva, and Melissa
Roemmele. 2012. SemEval-2012 task 7: Choice
of plausible alternatives: An evaluation of common-
sense causal reasoning. In *SEM 2012: The First

Joint Conference on Lexical and Computational Se-
mantics — Volume I: Proceedings of the main con-
ference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 394-398,
Montréal, Canada. Association for Computational
Linguistics.

Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,
and Kyunghyun Cho. 2018. Meta-learning for low-
resource neural machine translation. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 3622-3631,
Brussels, Belgium. Association for Computational
Linguistics.

Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius,
and Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of Biomedical Informatics, 45(5):885-892.
Text Mining and Natural Language Processing in
Pharmacogenomics.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan
Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel:
A large-scale supervised few-shot relation classifica-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4803-
4809, Brussels, Belgium. Association for Computa-
tional Linguistics.

Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
643-653, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fiirstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named en-
tities in text. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 782-792, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.

Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. Toward
semantics-based answer pinpointing. In Proceed-
ings of the First International Conference on Human
Language Technology Research.

Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. 2019. Cosmos QA: Machine reading
comprehension with contextual commonsense rea-
soning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
2391-2401, Hong Kong, China. Association for
Computational Linguistics.


Chao Jiang, Mounica Maddela, Wuwei Lan, Yang
Zhong, and Wei Xu. 2020. Neural CRF model for
sentence alignment in text simplification. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 7943—
7960, Online. Association for Computational Lin-
guistics.

Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-
baseQA: A new factoid QA data set matching trivia-
style question-answer pairs with Freebase. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume I (Long and Short Papers), pages 318-323,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.

Xisen Jin, Mohammad Rostami, and Xiang Ren. 2021.
Lifelong learning of few-shot learners across nlp
tasks. ArXiv, abs/2104.08808.

Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking be-
yond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceedings
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 252-262, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-
mat boundaries with a single QA system. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2020, pages 1896-1907, Online. As-
sociation for Computational Linguistics.

Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020. Qasc: A
dataset for question answering via sentence compo-
sition. Proceedings of the AAAI Conference on Arti-
ficial Intelligence, 34(05):8082-8090.

Tushar Khot, Daniel Khashabi, Kyle Richardson, Pe-
ter Clark, and Ashish Sabharwal. 2021. Text mod-
ular networks: Learning to decompose tasks in the
language of existing models. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1264-1279, On-
line. Association for Computational Linguistics.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SciTail: A textual entailment dataset from science
question answering. In AAAI.

Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim.
2019. Abstractive summarization of Reddit posts
with multi-level memory networks. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1

(Long and Short Papers), pages 2519-2531, Min-
neapolis, Minnesota. Association for Computational
Linguistics.

Neema Kotonya and Francesca Toni. 2020. Ex-
plainable automated fact-checking for public health
claims. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7740-7754, Online. Associa-
tion for Computational Linguistics.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, [lia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. Transactions of the Association
for Computational Linguistics, 7:453-466.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. RACE: Large-scale ReAd-
ing comprehension dataset from examinations. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
785-794, Copenhagen, Denmark. Association for
Computational Linguistics.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with
application to the biography domain. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1203-1213,
Austin, Texas. Association for Computational Lin-
guistics.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, and
C. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web, 6:167-195.

Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Proceedings of the Thirteenth International Confer-
ence on Principles of Knowledge Representation
and Reasoning, KR’ 12, page 552-561. AAAI Press.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017), pages 333-342, Vancou-
ver, Canada. Association for Computational Linguis-
tics.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational


Linguistics, pages 7871-7880, Online. Association
for Computational Linguistics.

Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, A. Thakur, Patrick von Platen, Suraj Patil,
Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario vSavsko, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clement Delangue, Th’eo Ma-
tussiere, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, Franccois
Lagunas, Alexander M. Rush, and Thomas Wolf.
2021. Datasets: A community library for natural
language processing.

Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics.

Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and
Xiang Ren. 2020a. Birds have four legs?!
NumerSense: Probing Numerical Commonsense
Knowledge of Pre-Trained Language Models. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 6862-6868, Online. Association for Computa-
tional Linguistics.

Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang
Ren. 2020b. CommonGen: A constrained text gen-
eration challenge for generative commonsense rea-
soning. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 1823-1840,
Online. Association for Computational Linguistics.

Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering, pages 58—
62, Hong Kong, China. Association for Computa-
tional Linguistics.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume I: Long Papers), pages 158-167, Vancou-
ver, Canada. Association for Computational Linguis-
tics.

Tal Linzen. 2020. How can we accelerate progress to-
wards human-like linguistic generalization? In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5210-
5217, Online. Association for Computational Lin-
guistics.

Annie Louis, Dan Roth, and Filip Radlinski. 2020. “Id
rather just go to bed”: Understanding indirect an-
swers. In Proceedings of the 2020 Conference on

Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7411-7425, Online. Associa-
tion for Computational Linguistics.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142—150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. J. Assoc. Inf: Sci. Technol., 65(4):782-796.

Irene Manotas, Ngoc Phuoc An Vo, and Vadim Sheinin.
2020. LiMiT: The literal motion in text dataset. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020, pages 991-1000, Online.
Association for Computational Linguistics.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC-2014),
pages 216-223, Reykjavik, Iceland. European Lan-
guages Resources Association (ELRA).

Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Proceedings of Sinn und Bedeutung, 23(2):107-124.

Binny Mathew, Punyajoy Saha, Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Ani-
mesh Mukherjee. 2020. Hatexplain: A benchmark
dataset for explainable hate speech detection. arXiv
preprint arXiv:2012.10289.

Julian McAuley and J. Leskovec. 2013. Hidden factors
and hidden topics: understanding rating dimensions
with review text. Proceedings of the 7th ACM con-
ference on Recommender systems.

Bryan McCann, N. Keskar, Caiming Xiong, and
R. Socher. 2018. The natural language decathlon:
Multitask learning as question answering. ArXiv,
abs/1806.08730.

Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Ef-
fective transfer learning for identifying similar ques-
tions: Matching user questions to covid-19 faqs.
In Proceedings of the 26th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data
Mining, KDD ’20, page 3458-3465, New York, NY,
USA. Association for Computing Machinery.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In Proceedings of the 2018 Conference on


Empirical Methods in Natural Language Processing,
pages 2381-2391, Brussels, Belgium. Association
for Computational Linguistics.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773.

Ioannis Mollas, Zoe Chrysopoulou, Stamatis Kar-
los, and Grigorios Tsoumakas. 2020. Ethos:
an online hate speech detection dataset. ArXiv,
abs/2006.08328.

Shikhar Murty, T. Hashimoto, and Christopher D. Man-
ning. 2021. Dreca: A general task augmentation
strategy for few-shot natural language inference.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1953-1967, Online. As-
sociation for Computational Linguistics.

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX), pages 95-100, Mon-
tréal, Canada. Association for Computational Lin-
guistics.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1797-1807, Brussels, Bel-
gium. Association for Computational Linguistics.

Alex Nichol, Joshua Achiam, and John Schulman.
2018. On first-order meta-learning algorithms.
ArXiv, abs/1803.02999.

Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 4885-4901, Online. Association
for Computational Linguistics.

Farhad Nooralahzadeh, Giannis Bekoulis, Johannes
Bjerva, and Isabelle Augenstein. 2020. Zero-shot
cross-lingual transfer with meta learning. In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 4547-4562, Online. Association for Compu-
tational Linguistics.

A. Othman and M. Jemni. 2012. English-asl gloss par-
allel corpus 2012: Aslg-pce12.

Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL’05), pages 115-
124, Ann Arbor, Michigan. Association for Compu-
tational Linguistics.

Dimitris Pappas, Petros Stavropoulos, Ion Androut-
sopoulos, and Ryan McDonald. 2020. BioMRC: A
dataset for biomedical machine reading comprehen-
sion. In Proceedings of the 19th SIGBioMed Work-
shop on Biomedical Language Processing, pages
140-149, Online. Association for Computational
Linguistics.

Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktaschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2020. How context affects lan-
guage models’ factual predictions. In Automated
Knowledge Base Construction.

Fabio Petroni, Tim Rocktischel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 2463-2473, Hong Kong, China. As-
sociation for Computational Linguistics.

Mohammad Taher Pilehvar and Jose Camacho-
Collados. 2019. WiC: the word-in-context dataset
for evaluating context-sensitive meaning represen-
tations. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 1267-1273, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Amir Pouran Ben Veyseh, Franck Dernoncourt,
Quan Hung Tran, and Thien Huu Nguyen. 2020.
What does this acronym mean? introducing a new
dataset for acronym identification and disambigua-
tion. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 3285-
3301, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.

Yada Pruksachatkun, Jason Phang, Haokun Liu,
Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe
Pang, Clara Vania, Katharina Kann, and Samuel R.
Bowman. 2020. Intermediate-task transfer learning
with pretrained language models: When and why
does it work? In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 5231-5247, Online. Association for
Computational Linguistics.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring


the limits of transfer learning with a unified text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1-67.

Altaf Rahman and Vincent Ng. 2012. Resolving com-
plex cases of definite pronouns: The Winograd
schema challenge. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 777-789, Jeju Island, Korea.
Association for Computational Linguistics.

Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain yourself!
leveraging language models for commonsense rea-
soning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 4932-4942, Florence, Italy. Association
for Computational Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQUAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383-2392, Austin,
Texas. Association for Computational Linguistics.

Hannah Rashkin, Eric Michael Smith, Margaret Li, and
Y-Lan Boureau. 2019. Towards empathetic open-
domain conversation models: A new benchmark and
dataset. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 5370-5381, Florence, Italy. Association
for Computational Linguistics.

Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,
and Sameer Singh. 2020. Beyond accuracy: Be-
havioral testing of NLP models with CheckList. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4902—
4912, Online. Association for Computational Lin-
guistics.

Anna Rogers, Olga Kovaleva, Matthew Downey, and
Anna Rumshisky. 2020. Getting closer to ai com-
plete question answering: A set of prerequisite real
tasks. Proceedings of the AAAI Conference on Arti-
ficial Intelligence, 34(05):8722-8731.

Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and
Karthik Sankaranarayanan. 2018. DuoRC: Towards
complex language understanding with paraphrased
reading comprehension. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1683-1693, Melbourne, Australia. Association for
Computational Linguistics.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
vatula, and Yejin Choi. 2020. Winogrande: An ad-
versarial winograd schema challenge at scale. Pro-
ceedings of the AAAI Conference on Artificial Intel-
ligence, 34(05):8732-8740.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 4463-—
4473, Hong Kong, China. Association for Computa-
tional Linguistics.

Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 3687-3697, Brussels, Belgium. Association
for Computational Linguistics.

Timo Schick and Hinrich Schiitze. 2020a. Exploiting
cloze questions for few-shot text classification and
natural language inference. Computing Research
Repository, arXiv:2001.07676.

Timo Schick and Hinrich Schiitze. 2020b. It’s not just
size that matters: Small language models are also
few-shot learners. Computing Research Repository,
arXiv:2009.07118.

Emily Sheng and David Uthus. 2020. Investigating
societal biases in a poetry composition system. In
Proceedings of the Second Workshop on Gender
Bias in Natural Language Processing, pages 93-106,
Barcelona, Spain (Online). Association for Compu-
tational Linguistics.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,
Eric Wallace, and Sameer Singh. 2020. AutoPrompt:
Eliciting Knowledge from Language Models with
Automatically Generated Prompts. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
4222-4235, Online. Association for Computational
Linguistics.

Damien Sileo, Tim Van De Cruys, Camille Pradel,
and Philippe Muller. 2019. Mining discourse mark-
ers for unsupervised sentence representation learn-
ing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
3477-3486, Minneapolis, Minnesota. Association
for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1631-1642, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.

Trevor Scott Standley, A. Zamir, Dawn Chen,
L. Guibas, Jitendra Malik, and S. Savarese. 2020.


Which tasks should be learned together in multi-task
learning? In ICML.

Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,
and Claire Cardie. 2019. DREAM: A challenge data
set and models for dialogue-based reading compre-
hension. Transactions of the Association for Com-
putational Linguistics, 7:217-231.

Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2019a. Quarel: A
dataset and models for answering questions about
qualitative relationships. Proceedings of the AAAI
Conference on Artificial Intelligence, 33(01):7063-
7071.

Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019b. QuaRTz: An open-domain dataset of
qualitative relationship questions. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5941-5946, Hong Kong,
China. Association for Computational Linguistics.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4149-4158, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Derek Tam, R. R. Menon, M. Bansal, Shashank
Srivastava, and Colin Raffel. 2021. Improving
and simplifying pattern exploiting training. ArXiv,
abs/2103.11955.

Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural
text. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
6076-6085, Hong Kong, China. Association for
Computational Linguistics.

James Thorne, Andreas Viachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction
and VERification. In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume I (Long
Papers), pages 809-819, New Orleans, Louisiana.
Association for Computational Linguistics.

Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pas-
cal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,
Carles Gelada, Kevin Swersky, Pierre-Antoine Man-
zagol, and Hugo Larochelle. 2020. Meta-dataset: A

dataset of datasets for learning to learn from few ex-
amples. In International Conference on Learning
Representations.

Sowmya Vajjala and Ivana Lu¢i¢é. 2018. On-
eStopEnglish corpus: A new corpus for automatic
readability assessment and text simplification. In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 297-304, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-
dro Sordoni, Adam Trischler, Andrew Mattarella-
Micke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-
ploring and predicting transferability across NLP
tasks. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7882-7926, Online. Associa-
tion for Computational Linguistics.

Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma. 2021. Entailment as few-shot learner.
arXiv preprint arXiv:2104.14690.

Sinong Wang, Madian Khabsa, and Hao Ma. 2020. To
pretrain or not to pretrain: Examining the benefits
of pretrainng on resource rich tasks. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2209-2213, On-
line. Association for Computational Linguistics.

William Yang Wang. 2017. “liar, liar pants on fire”: A
new benchmark dataset for fake news detection. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 422-426, Vancouver, Canada.
Association for Computational Linguistics.

Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. Blimp: The benchmark of linguis-
tic minimal pairs for english. Transactions of the As-
sociation for Computational Linguistics, 8:377-392.

Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics, 7:625—-641.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2021. Finetuned lan-
guage models are zero-shot learners.

Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
In Proceedings of the 3rd Workshop on Noisy User-
generated Text, pages 94-106, Copenhagen, Den-
mark. Association for Computational Linguistics.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American


Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
I (Long Papers), pages 1112-1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations, pages 38-45, Online. Asso-
ciation for Computational Linguistics.

Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020. Break it down: A question under-
standing benchmark. Transactions of the Associa-
tion for Computational Linguistics, 8:183-198.

Sen Wu, Hongyang R. Zhang, and Christopher Ré.
2020. Understanding and improving information
transfer in multi-task learning. In International Con-
ference on Learning Representations.

Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
mi, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5020—
5031, Florence, Italy. Association for Computa-
tional Linguistics.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2013-2018, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2369-2380, Brussels, Belgium. Association
for Computational Linguistics.

Wenpeng Yin, Nazneen Fatema Rajani, Dragomir
Radev, Richard Socher, and Caiming Xiong. 2020.
Universal natural language processing with limited
annotations: Try few-shot textual entailment as a
start. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 8229-8239, Online. Associa-
tion for Computational Linguistics.

Dani Yogatama, Cyprien de Masson d’ Autume, Jerome
Connor, Tomas Kocisky, Mike Chrzanowski, Ling-
peng Kong, A. Lazaridou, Wang Ling, L. Yu,
Chris Dyer, and P. Blunsom. 2019. Learning and
evaluating general linguistic intelligence. ArXiv,
abs/1901.11373.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018. Spider: A _ large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911-3921, Brussels, Belgium. Association for
Computational Linguistics.

Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,
Karol Hausman, Chelsea Finn, and Sergey Levine.
2020. Meta-world: A benchmark and evaluation for
multi-task and meta reinforcement learning. In Pro-
ceedings of the Conference on Robot Learning, vol-
ume 100 of Proceedings of Machine Learning Re-
search, pages 1094-1100. PMLR.

Amir R. Zamir, Alexander Sax, William B. Shen,
Leonidas J. Guibas, Jitendra Malik, and Silvio
Savarese. 2018. Taskonomy: Disentangling task
transfer learning. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 93-
104, Brussels, Belgium. Association for Computa-
tional Linguistics.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can
a machine really finish your sentence? In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 479 1-
4800, Florence, Italy. Association for Computational
Linguistics.

Hao Zhang, Jae Ro, and Richard Sproat. 2020. Semi-
supervised URL segmentation with recurrent neu-
ral networks pre-trained on knowledge graph enti-
ties. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 4667—
4675, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.

Rui Zhang and Joel Tetreault. 2019. This email could
save your life: Introducing the task of email subject
line generation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 446-456, Florence, Italy. Association
for Computational Linguistics.

Sheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin Duh,
and Benjamin Van Durme. 2018. Record: Bridging


the gap between human and machine commonsense
reading comprehension. ArXiv, abs/1810.12885.

Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q
Weinberger, and Yoav Artzi. 2021. Revisiting few-
sample {bert} fine-tuning. In International Confer-
ence on Learning Representations.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems - Volume 1, NIPS’15, page 649-657, Cam-
bridge, MA, USA. MIT Press.

Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scram-
bling. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume I (Long and Short Papers), pages
1298-1308, Minneapolis, Minnesota. Association
for Computational Linguistics.

Ruiqi Zhong, Kristy Lee, Zheng Zhang, and D. Klein.
2021. Adapting language models for zero-shot
learning by meta-tuning on dataset and prompt col-
lections. ArXiv, abs/2104.04670.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.

Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Roth. 2019. “going on a vacation” takes longer
than “going for a walk”: A study of temporal com-
monsense understanding. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 3363-3369, Hong Kong,
China. Association for Computational Linguistics.


Selected Tasks in NLP Few-shot Gym

Table 3: Tasks in NLP Few-shot Gym.

Task Name Ontology Reference
acronym_identification other Pouran Ben Veyseh et al. 2020
ade_corpus_v2-classification cls/other Gurulingappa et al. 2012
ade_corpus_v2-dosage other/slot filling Gurulingappa et al. 2012
ade_corpus_v2-effect other/slot filling Gurulingappa et al. 2012
adversarialqa qa/machine reading comprehension _ Bartolo et al. 2020

aeslc cg/summarization Zhang and Tetreault 2019
ag_news cls/topic Gulli (link)

ai2_arce qa/multiple-choice qa Clark et al. 2018
amazon_polarity cls/sentiment analysis McAuley and Leskovec 2013
anli cls/nli Nie et al. 2020
app_reviews other/regression Missing

aqua_rat qa/multiple-choice qa Ling et al. 2017

art (abductive nli) other Bhagavatula et al. 2020
aslg_pc12 other Othman and Jemni 2012
biomre qa/machine reading comprehension __ Pappas et al. 2020
blimp-anaphor_gender_agreement other/linguistic phenomenon Warstadt et al. 2020
blimp-anaphor_number_agreement other/linguistic phenomenon Warstadt et al. 2020
blimp-determiner_noun_agreement_with_adj_irregular_1 other/linguistic phenomenon Warstadt et al. 2020
blimp-ellipsis_n_bar_1 other/linguistic phenomenon Warstadt et al. 2020
blimp-ellipsis_n_bar_2 other/linguistic phenomenon Warstadt et al. 2020
blimp-existential_there_quantifiers_1 other/linguistic phenomenon Warstadt et al. 2020
blimp-irregular_past_participle_adjectives other/linguistic phenomenon Warstadt et al. 2020
blimp-sentential_negation_npi_licensor_present other/linguistic phenomenon Warstadt et al. 2020
blimp-sentential_negation_npi_scope other/linguistic phenomenon Warstadt et al. 2020
blimp-wh_questions_object_gap other/linguistic phenomenon Warstadt et al. 2020
boolq qa/binary Clark et al. 2019
break-QDMR other Wolfson et al. 2020
break-QDMR-high-level other Wolfson et al. 2020
circa cls/other Louis et al. 2020
climate_fever cls/fact checking Diggelmann et al. 2020
codah qa/multiple-choice qa Chen et al. 2019
common_gen other Lin et al. 2020b

commonsense_qa
cos_e

cosmos_qa
crawl_domain
crows_pairs

dbpedia_14
definite_pronoun_resolution
discovery

dream

duore

e2e_nlg_cleaned
eli5-askh

eli5-asks

eliS-eliS

emo

emotion
empathetic_dialogues
ethos-directed_vs_generalized
ethos-disability
ethos-gender
ethos-national_origin
ethos-race
ethos-religion
ethos-sexual_orientation
financial_phrasebank
freebase_qa

gigaword

glue-cola

glue-mnli

glue-mrpc

glue-qnli

glue-qqp

glue-rte

glue-sst2

glue-wnli
google_wellformed_query
hate_speech18
hate_speech_offensive
hatexplain

health_fact

hellaswag

hotpot_ga

imdb

jeopardy

kilt_ay2

qa/multiple-choice ga
other/generate explanation
qa/multiple-choice ga
other

other

cls/topic

other

cls/other
qa/multiple-choice ga
qa/machine reading comprehension
other

qa/long-form qa
qa/long-form qa
qa/long-form qa
cls/emotion

cls/emotion

cg/dialogue

cls/hate speech detection
cls/hate speech detection
cls/hate speech detection
cls/hate speech detection
cls/hate speech detection
cls/hate speech detection
cls/hate speech detection
cls/sentiment analysis
qa/closed-book qa
cg/summarization
cls/other

cls/nli

cls/paraphrase

cls/nli

cls/paraphrase

cls/nli

cls/sentiment analysis
cls/nli

cls/other

cls/hate speech detection
cls/hate speech detection
cls/hate speech detection
cls/fact checking
qa/multiple-choice ga
qa/machine reading comprehension
cls/sentiment analysis
qa/closed-book qa
other/entity linking

Talmor et al. 2019
Rajani et al. 2019
Huang et al. 2019
Zhang et al. 2020
Nangia et al. 2020
Lehmann et al. 2015
Rahman and Ng 2012
Sileo et al. 2019

Sun et al. 2019

Saha et al. 2018
DuSek et al. 2020, 2019
Fan et al. 2019

Fan et al. 2019

Fan et al. 2019
Chatterjee et al. 2019
Saravia et al. 2018
Rashkin et al. 2019
Mollas et al. 2020
Mollas et al. 2020
Mollas et al. 2020
Mollas et al. 2020
Mollas et al. 2020
Mollas et al. 2020
Mollas et al. 2020
Malo et al. 2014
Jiang et al. 2019
Napoles et al. 2012
Warstadt et al. 2019
Williams et al. 2018
Dolan and Brockett 2005
Rajpurkar et al. 2016
(link)

Dagan et al. 2005; Bar-Haim et al. 2006
Giampiccolo et al. 2007; Bentivogli et al. 2009

Socher et al. 2013
Levesque et al. 2012
Faruqui and Das 2018
de Gibert et al. 2018
Davidson et al. 2017
Mathew et al. 2020
Kotonya and Toni 2020
Zellers et al. 2019
Yang et al. 2018
Maas et al. 2011
(link)

Hoffart et al. 2011

Continued on next page


Task Name Ontology Reference

kilt_fever cls/fact checking Thorne et al. 2018
kilt_hotpotqa qa/closed-book qa Yang et al. 2018

kilt_nq qa/closed-book qa Kwiatkowski et al. 2019
kilt_trex qa/closed-book qa Elsahar et al. 2018
kilt_wow cg/dialogue Dinan et al. 2019
kilt_zsre qa/closed-book qa Levy et al. 2017

lama-conceptnet
lama-google_re
lama-squad

lama-trex
liar

limit
math_qa
mc_taco

medical_questions_pairs

mocha

multi_news
numer_sense
onestop_english
openbookqa

paws
piqa

poem_sentiment

proto_gqa
qa_srl
qasc
quail
quarel

quartz-no_knowledge
quartz-with_knowledge

quoref
race-high

race-middle
reddit_tifu-title
reddit_tifu-tldr

ropes

rotten_tomatoes

samsum
scicite
sciq
scitail
search_qa
sick

sms_spam

social_i_qa

spider

squad-no_context
squad-with_context
superglue-cb
superglue-copa
superglue-multirc
superglue-record

superglue-rte

superglue-wic
superglue-wsc

swag
tab_fact
trec

trec-finegrained

tweet_eval
tweet_eval
tweet_eval
tweet_eval
tweet_eval
tweet_eval
tweet_eval
tweet_eva!
tweet_eval
tweet_eval
tweet_eval
tweet_qa

-emoji

-emotion

-hate

-irony

-offensive
-sentiment
-stance_abortion
-stance_atheism
-stance_climate
-stance_feminist
-stance_hillary

web_questions

wiki_auto
wiki_bio
wiki_ga
wiki_split
wikisql

wino_grande

wiga
xsum

yahoo_answers_topics
yelp_polarity
yelp_review_full

qa/closed-book qa
qa/closed-book qa
qa/closed-book qa
qa/closed-book qa
cls/fact checking
other
qa/multiple-choice qa
qa/binary
cls/paraphrase
other/regression
cg/summarization
qa/closed-book qa
cls/other
qa/multiple-choice ga
cls/paraphrase

other

cls/sentiment analysis
other

other
qa/multiple-choice qa
qa/multiple-choice ga
qa/multiple-choice qa
qa/multiple-choice ga
qa/multiple-choice ga
qa/machine reading comprehension
qa/multiple-choice qa
qa/multiple-choice ga
cg/summarization
cg/summarization
qa/machine reading comprehension
cls/sentiment analysis
cg/summarization
cls/other
qa/multiple-choice ga
cls/nli
qa/closed-book ga
cls/nli

cls/other
qa/multiple-choice qa
cg/other
qa/closed-book qa
qa/machine reading comprehension
cls/nli
qa/multiple-choice ga
qa/multiple-choice ga
qa/machine reading comprehension

cls/nli

cls/other

cls/other
qa/multiple-choice ga
cls/fact checking
cls/other

cls/other

cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
cls/emotion
qa/machine reading comprehension
qa/closed-book qa
cls/other

cg/other

cls/other

cg/other

cg/other
qa/multiple-choice qa
qa/multiple-choice ga
cg/summarization
cls/topic
cls/sentiment analysis
other/regression

Petroni et al. 2019, 2020
Petroni et al. 2019, 2020
Petroni et al. 2019, 2020
Petroni et al. 2019, 2020
Wang 2017

Manotas et al. 2020
Amini et al. 2019

Zhou et al. 2019
McCreery et al. 2020
Chen et al. 2020a
Fabbri et al. 2019

Lin et al. 2020a

Vajjala and Lucié 2018
Mihaylov et al. 2018
Zhang et al. 2019

Bisk et al. 2020

Sheng and Uthus 2020
Boratko et al. 2020

He et al. 2015

Khot et al. 2020

Rogers et al. 2020
Tafjord et al. 2019a
Tafjord et al. 2019b
Tafjord et al. 2019b
Dasigi et al. 2019

Lai et al. 2017

Lai et al. 2017

Kim et al. 2019

Kim et al. 2019

Lin et al. 2019

Pang and Lee 2005
Gliwa et al. 2019

Cohan et al. 2019

Welbl et al. 2017

Khot et al. 2018

Dunn et al. 2017
Marelli et al. 2014
Almeida et al. 2011

Sap et al. 2019

Yu et al. 2018

Rajpurkar et al. 2016
Rajpurkar et al. 2016

de Marneffe et al. 2019
Gordon et al. 2012
Khashabi et al. 2018
Zhang et al. 2018

Dagan et al. 2005; Bar-Haim et al. 2006

Giampiccolo et al. 2007; Bentivogli et al. 2009

Pilehvar and Camacho-Collados 2019
Levesque et al. 2012

Zellers et al. 2018

Chen et al. 2020b

Li and Roth 2002; Hovy et al. 2001
Li and Roth 2002; Hovy et al. 2001
Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Barbieri et al. 2020

Xiong et al. 2019

Berant et al. 2013

Jiang et al. 2020

Lebret et al. 2016

Yang et al. 2015

Botha et al. 2018

Zhong et al. 2017

Sakaguchi et al. 2020

Tandon et al. 2019

Narayan et al. 2018

(link)

Zhang et al. 2015; (link)

Zhang et al. 2015; (link)



Ne

Ne

Ne

B Details about Task Partition

B.1 Partition 1. Random

"train”: ['glue-mrpc', 'math_qa',
, ‘tab_fact', '‘aqua_rat',
wiki_qa', 'blimp-ellipsis_n_bar_1'
determiner_noun_agreement_with_adj
definite_pronoun_resolution',
"blimp-ellipsis_n_bar_2',

"commonsense_qa',
"paws',

religion',

‘glue-qqp',
=rte’,
', 'boolq', ‘kilt_hotpotqa',
, 'tweet_eval-sentiment',
level', ‘qasc', ‘imdb',
web_questions', '‘duorc',

stance_feminist', ‘limit',
social_i_qa', ‘anli',

‘kilt_zsre',

, ‘blimp-existential_there_quantifiers_i',

rotten_tomatoes', '‘superglue-cb',
climate_fever', '‘lama-google_re',
wh_questions_object_gap',
dbpedia_14', '‘yahoo_answers_topics
anaphor_gender_agreement ',
"dev": ['cos_e', 'kilt_fever',
tweet_qa', 'wikisql', ‘lama-trex',
‘liar', ‘wiki_bio', ‘dream',
financial_phrasebank '],
"test": ['quoref', 'wiki_split',
ethos-sexual_orientation',
race-high',

crawl_domain', 'freebase_qa',

‘quarel',
"tweet_eval-emoji',
,» '‘openbookga' ,
_irregular_1',
"hellaswag',
"kilt_ay2',
, 'tweet_eval-stance_abortion', ‘reddit_tifu-tldr',
‘jeopardy',
"ethos-directed_vs_generalized',
'blimp-anaphor_number_agreement ',
‘quartz-no_knowledge',
"crows_pairs',
"ethos-gender',
‘yelp_review_full',
"common_gen' ,

"hotpot_qa',

"hate_speechi8',
"eli5-asks',

"ade_corpus_v2-classification',

"ethos-disability',
"blimp-sentential_negation_npi_scope',
"blimp-sentential_negation_npi_licensor_present',
"glue-qnli',

"e2e_nlg_cleaned',

"tweet_eval-stance_atheism',
"codah', 'tweet_eval-offensive',
"acronym_identification', '‘blimp-
"ethos-national_origin', 'spider', '
"superglue-wsc', 'numer_sense', 'ade_corpus_v2-dosage',
"squad-no_context', '‘google_wellformed_query', 'xsum', ‘wigqa'
"ade_corpus_v2-effect', '‘qa_srl', '‘ethos-
"superglue-multirc', ‘ethos-race', ‘eli5-askh',
"glue-sst2', 'mocha', 'tweet_eval-hate', ‘glue
‘lama-conceptnet', '‘hate_speech_offensive', 'superglue-wic
"aslg_pcl2', '‘sick', 'tweet_eval-stance_climate
"glue-mnli', 'medical_questions_pairs', 'break-QDMR-high-
"trec-finegrained', ‘adversarialqa', ‘onestop_english', '

“swag', 'proto_qa', ‘scitail', 'tweet_eval-
"scicite', ‘blimp-irregular_past_participle_adjectives', '
"cosmos_qa', 'superglue-record', 'squad-with_context', '‘emotion'
‘race-middle', ‘'kilt_wow', ‘sciq', ‘'wino_grande', '
"poem_sentiment', ‘ropes', ‘reddit_tifu-title', ‘piqa', '
"search_qa', ‘wiki_auto', ‘mc_taco', '‘blimp-
"emo', ‘kilt_nq', ‘kilt_trex', '‘quartz-with_knowledge',
", 'app_reviews', '‘superglue-copa', 'blimp-

‘gigaword', 'multi_news', ‘aeslc', ‘quail'],

"trec', ‘eli5-eli5', ‘art', ‘empathetic_dialogues', '
"tweet_eval-stance_hillary', '‘discovery', 'tweet_eval-emotion',
"health_fact', 'samsum', '

"lama-squad'

"glue-wnli',
"sms_spam',

‘biomrc',

"yelp_polarity', '

"superglue-rte', ‘glue-cola',

‘ai2_arc', ‘amazon_polarity',
"tweet_eval-irony', ‘break-QDMR', '
Veincas a,

'

‘hatexplain', ‘ag_news',

B.2 Partition 2.1. 45cls

"train”: ["superglue-rte”,
"glue-mrpc”,
cola”, “sick”,
health_fact”, "glue-mnli”, “imdb”,
"yahoo_answers_topics”, "liar”,
stance_climate”, "glue-qnli”,
classification”, "wiki_auto”,
"tweet_eval-irony”,

"dev": ["tweet_eval-stance_feminist”,
amazon_polarity”, "“hate_speech18”",
tweet_eval-stance_atheism”],

"test": ["superglue-cb”, "dbpedia_14”
financial_phrasebank”, "tab_fact”,

"paws",

"tweet_eval -
"tweet_eval-stance_hillary”,
"ethos-sexual_orientation”,

"glue-sst2",
"tweet_eval-emoji”,
"hate_speech_offensive”,

"ethos-gender”, "onestop_english”,

”

sentiment”, “discovery”, "glue-rte”, "superglue-wsc”, "scicite”,
"tweet_eval-offensive”, "emotion”, "hatexplain”, "glue-
"glue-qqp”, "“tweet_eval-emotion”, "sms_spam”,
"ethos-disability”, "glue-wnli”, "scitail”, "trec-finegrained”,
"tweet_eval-stance_abortion”, "circa", "tweet_eval-
"ethos-directed_vs_generalized”, "ade_corpus_v2-
"superglue-wic”, "“google_wellformed_query”,
"trec”, "rotten_tomatoes”, "kilt_fever”],
ethos-national_origin”, "tweet_eval-hate”, "ag news”, ”
"poem_sentiment”, "climate_fever”, "“medical_questions_pairs”,

"wiki_qa”, "emo", "yelp_polarity”,
"anli”, "“ethos-race”],

"ethos-religion”,

B.3 Partition 2.2. 23cls+22non-cls

"train”: [”ade_corpus_v2-dosage”,
sentential_negation_npi_scope”,
"lama-google_re”, "“lama-squad”,
quartz-no_knowledge”, "race-high”,
emotion”, "“ethos-disability”,
glue-qqp”, "glue-rte”,
"sms_spam”, "superglue-rte”,
tweet_eval-sentiment”, "tweet_eval
"dev": ["tweet_eval-stance_feminist”,
amazon_polarity”, "hate_speechi8”,
tweet_eval-stance_atheism”],
"test": ["superglue-cb”, "dbpedia_14”
financial_phrasebank”, "tab_fact”,

"biomrc”,
"commonsense_qa”,
"math_qa”,

"ethos-sexual_orientation”,
"glue-wnli”, "hatexplain”,
"superglue-wsc”,

-stance_hillary”],

”

"blimp-ellipsis_n_bar_2”, "blimp-
"crows_pairs”, "duorc”,
"openbookqa”,

"hellaswag”,
Mee aan om n
piqa”, "proto_qa”,
"ropes", "sciq”, "“wiki_bio”, "discovery”,
” non er) ”
glue-cola”, "glue-mnli”, "glue-mrpc”,
"health_fact”, “imdb”, “paws”, "scicite”, “sick”,
"tweet_eval-emotion”, "tweet_eval-offensive”, ”

"kilt_zsre"”,

"numer_sense”,
"reddit_tifu-tldr”,

”

ethos-national_origin”, "tweet_eval-hate”,
"poem_sentiment”, "climate_fever”,

"ag news”,
"medical_questions_pairs”,

"wiki_qa”, "emo", "yelp_polarity”,
"anli”, "“ethos-race”]

"ethos-religion”,

B.4 Partition 2.3. 45non-cls



Ne

NANunkwW

Ne

Ne nw

mB WwW

w

"train": ["ade_corpus_v2-dosage”,

"art", "biomrc”, "blimp-anaphor_number_agreement”, "blimp-

ellipsis_n_bar_2”, "blimp-sentential_negation_npi_licensor_present”, "blimp-
sentential_negation_npi_scope”, "break-QDMR-high-level”, "“commonsense_qa”, "crows_pairs”, "dream",
"duorce”, "“eli5-asks”, "eli5-eli5”, "freebase_qa”, "gigaword”, "hellaswag", "hotpot_qa”, "kilt_ay2”,

"kilt_hotpotqa”, "kilt_trex”, "kilt_zsre”, "lama-conceptnet”, "lama-google_re”, "lama-squad”, ”
math_qa”, "numer_sense”, "openbookqa”, "piqa”, "proto_qa”, "qa_srl”, "quarel”, "quartz-no_knowledge
". "race-high”, "reddit_tifu-title”, "reddit_tifu-tldr”, “ropes”, "sciq”, "social_i_qa”, "spider”
"superglue-multirc”, "wiki_bio”, "wikisql”, "xsum"”, "yelp_review_full”],

"dev":
amazon_polarity”,
tweet_eval-stance_atheism”],

["tweet_eval-stance_feminist”,
"hate_speechi8”,

”

"ethos-national_origin”, "tweet_eval-hate”,
"poem_sentiment”, "climate_fever”,

"ag news”,
"medical_questions_pairs”,

"test": ["superglue-cb”, "dbpedia_14”, "wiki_qa”, “emo”, "yelp_polarity”, "ethos-religion”, ”

financial_phrasebank”, "tab_fact”, "anli”, "ethos-race” ]
}
B.5 Partition 3.1. Held-out-NLI
{

"train": ["ade_corpus_v2-classification”, "ag_news”, "amazon_polarity”, “circa”, "climate_fever”, ”
dbpedia_14"”, "discovery", "emo", "emotion", "ethos-directed_vs_generalized”, "ethos-disability”, "”
ethos-gender”, "ethos-national_origin”, "ethos-race”, "ethos-religion”, "“ethos-sexual_orientation”,

"financial_phrasebank”, "glue-cola”, "glue-mrpc”, "glue-qqp”, "glue-sst2”, ”
google_wellformed_query”, "hate_speechi8", "“hate_speech_offensive”, "hatexplain”, "health_fact”, ”
imdb”, "kilt_fever”, "liar", "medical_questions_pairs”, "onestop_english”, “paws”, "“poem_sentiment”
, "“rotten_tomatoes”, "scicite”, “sick”, "sms_spam”, "“superglue-wic”, "superglue-wsc”, "tab_fact”, ”
trec”, "trec-finegrained”, "tweet_eval-emoji”, "tweet_eval-emotion”, "tweet_eval-hate”, "“tweet_eval
-irony”, "tweet_eval-offensive”, "tweet_eval-sentiment”, "tweet_eval-stance_abortion”, "tweet_eval -
stance_atheism”, "tweet_eval-stance_climate”, "tweet_eval-stance_feminist”, "tweet_eval -
stance_hillary”, "wiki_auto”, "wiki_qa”, "yahoo_answers_topics”, "yelp_polarity”

is

"dev": Cl],

"test": ["anli”, "glue-mnli”, "glue-qnli”, "glue-rte”, "glue-wnli”, "scitail”, "sick", "superglue-cb” ]

3
B.6_ Partition 3.2. Held-out-Para
{

"train": ["ade_corpus_v2-classification”, "ag_news”, "amazon_polarity”, "anli”, “circa”, "climate_fever”
» "dbpedia_14", "discovery", "emo”, "emotion”, "ethos-directed_vs_generalized”, "ethos-disability”,

"ethos-gender”, "“ethos-national_origin”, "ethos-race”, "ethos-religion”, "ethos-sexual_orientation
". "financial_phrasebank”, "glue-cola”, "glue-mnli”, "glue-qnli”, "glue-rte”, "glue-sst2”, "glue-
wnli”, "“google_wellformed_query”, "hate_speechi8", "hate_speech_offensive”, "“hatexplain”, ”
health_fact”, "imdb", "kilt_fever”, “liar”, "“onestop_english"”, "“poem_sentiment”, "“rotten_tomatoes”,

"scicite”, "scitail”, "sick", "sms_spam”, "“superglue-cb”, "superglue-rte”, "superglue-wic”, ”
superglue-wsc”, "tab_fact”, "trec”, "trec-finegrained”, "tweet_eval-emoji”, "tweet_eval-emotion”, ”
tweet_eval-hate”, "tweet_eval-irony”, "tweet_eval-offensive”, "tweet_eval-sentiment”, "tweet_eval -
stance_abortion”, "tweet_eval-stance_atheism”, "tweet_eval-stance_climate”, "tweet_eval-
stance_feminist”, "tweet_eval-stance_hillary”, "wiki_auto”, "wiki_qa”, "yahoo_answers_topics”, ”
yelp_polarity”],

“deve = El;

"test": ["glue-mrpc", "glue-qqp”, "medical_questions_pairs", “paws” ]

p

B.7_ Partition 4.1. Held-out-MRC

ie

"train": ["ai2_arc”, "aqua_rat”, "boolq”, "codah”, "commonsense_qa”, "cosmos_qa”, "dream", "“eli5-askh”,
"eli5-asks”, "eli5-eli5”, "freebase_qa”, "hellaswag”, "jeopardy”, "kilt_hotpotqa”, "kilt_nq”, ”
kilt_trex”, "kilt_zsre”, "“lama-conceptnet”, "lama-google_re”, "lama-squad”, "lama-trex”, "math_qa”,

"mc_taco”, "numer_sense”, "openbookqa”, "“qasc”, "quail”, "quarel"”, "quartz-no_knowledge”, "quartz-
with_knowledge”, "race-high”, "race-middle”, "sciq”, "search_qa"”, "social_i_qa”, "squad-no_context”
» "“superglue-copa”, "superglue-multirc”, “swag”, "web_questions”, "wino_grande”, "wiqa”

Ie

“dev = El,

"test": ["adversarialgqa”, "“biomrc”, "duorc”, "hotpot_qa”, "quoref”, "ropes", "squad-with_context”, ”
superglue-record”, "tweet_qa”],

}
B.8 Partition 4.2. Held-out-MCQA
{

"train": ["adversarialqa”, "biomrce”, "boolq”, "duorc”, "eli5-askh”, "eli5-asks”, "eli5-eli5”, ”
freebase_qa”, "hotpot_qa”, "jeopardy", "kilt_hotpotqa”, "kilt_nq”, "kilt_trex"”, "kilt_zsre", "lama-
conceptnet”, "lama-google_re”, "lama-squad”, "lama-trex”, "mc_taco”, "numer_sense”, "quoref”, "
ropes”, "search_qa”, "squad-no_context”, "squad-with_context”, "superglue-multirc”, "superglue -
record”, "tweet_qa”, "web_questions”

1,
"dev": CI],


5 "test": ["ai2_arc”, "“aqua_rat”, "codah”, "“commonsense_qa”, "cosmos_qa”, "dream", "hellaswag”,
"openbookgqa”, "“qasc”, “quail”, "quarel”, "quartz-no_knowledge”, "quartz-with_knowledge”,
high”, "“race-middle”, "sciq”, "social_i_qa”, "superglue-copa”, "swag”, "wino_grande”, "wiqa” ]
6 }

"math_qa”,
"race-

B.9 Partition 5. Held-out-GLUE

To examine whether combining our methods with template-based training (Schick and Schiitze, 2020a,b
Gao et al., 2020) results in even better few-shot performance, we add another partition that uses all

non-GLUE classification tasks aS Ti;ain, and all GLUE tasks as Tiest.

>

1 {

2 "train": ["ade_corpus_v2-classification”, "ag_news”, "“amazon_polarity”,

"anli", "circa”,

,» "dbpedia_14", "discovery", "emo”, “emotion”, "ethos-directed_vs_generalized”,

"ethos-gender”, "ethos-national_origin”, "ethos-race”, "“ethos-religion”,
". "financial_phrasebank”, "google_wellformed_query”, "“hate_speechi8”,

,

hatexplain”, "“health_fact”, “imdb”, "kilt_fever”, “liar”, "medical_questions_pairs”,
onestop_english”, "paws", "poem_sentiment”, "rotten_tomatoes”, "scicite”,
sms_spam", "superglue-cb”, "superglue-wic”, "“superglue-wsc”, "tab_fact”,

"tweet_eval-emoji”, "“tweet_eval-emotion”, "tweet_eval-hate”, "tweet_eval-irony”,
offensive”, "tweet_eval-sentiment”, "tweet_eval-stance_abortion”, "“tweet_eval-stance_atheism”,
tweet_eval-stance_climate”, "“tweet_eval-stance_feminist”, "tweet_eval-stance_hillary”,

,

"wiki_qa”, "“yahoo_answers_topics”, "yelp_polarity”],
3 "dev": (C1,
4 "test": ["glue-cola”, "glue-mnli”, "glue-mrpc”, "glue-qnli”, "“glue-qqp”,
wnli”]

"glue-rte”,

"climate_fever”

"ethos-disability”,

"hate_speech_offensive”,

"scitail”, “sick”,
'trec’,

"ethos-sexual_orientation

”

”

"trec-finegrained”

"glue-sst2”,

"tweet_eval -

”

"wiki_auto”,

"glue-

Continued on next page.


C_ Additional Results and Analysis

on Q4. Does the improved cross-task general-
ization ability go beyond few-shot settings?

In real-world applications, annotated data usu-
ally grow for a few-shot task over time. Is up-
stream learning still helpful when a target task
has more shots? To study this question, we study
CommonsenseQA (in Held-out-Multiple-Choice Par-
tition), ROPES (in Held-out-MRC Partition), and
MNLI (in Held-out-NLI Partition) as target tasks in
medium and high-resource scenarios. We take their
corresponding checkpoints after upstream learn-
ing and conduct experiments in medium and high-
resource scenarios. That is, we randomly sam-
ple {32,64,...,4096} examples from the three
datasets, and use them as Dy;qin. Then, we sample
a Ddey With the same size as D}y-qin,, or has the size
of 1024 if |Dtrain| > 1024. We also try fine-tuning
with the full dataset.© The performance of these
settings is shown in Fig. 7.

From Fig. 7, we see that the benefits brought
by upstream learning methods extend into medium
resource cases with up to 2048 training examples.
For CommonsenseQA, checkpoints from upstream
learning outperform direct fine-tuning significantly,
even with the full dataset. This finding encourages
the use of upstream learning before task-specific
fine-tuning when the target task has limited an-
notation. On the other hand, for resource-rich
tasks (e.g., MNLD, the improvement brought by
upstream learning diminishes. This aligns with the
findings of (Wang et al., 2020) who discuss the
benefits of pre-training on resource-rich tasks.

| gy Q5. Can we further improve few-shot perfor-
mance by using different/larger pre-trained
models?

We have been mainly using BART-Base (139M
parameters) as the main network, while it is possi-
ble to further push the limits of few-shot learning
by using scaling up to larger models or using differ-
ent model architectures. Previous work has shown
that scaling up model size leads to better perfor-
mance (Raffel et al., 2020; Brown et al., 2020).
Moreover, since meta-learning algorithms are natu-
rally unstable, it is important to verify whether they

®We do five random samples of 1024 examples as Dae

and use the remaining examples in the original train set as
Dtrain. We use the original dev set for testing.

function as expected with larger models. In Q5, we
experiment with T5-v1.1-Base (248M)’ and BART-
Large (406M) model with Held-out-Para Partition
to verify these assumptions. We only consider first-
order methods, as second-order optimization with
these larger models is impossible with our available
computation.

Our results are plotted in Fig. 8. In Fig. 8(a) we
compare the few-shot performance of direct fine-
tuning on these three pre-trained models. On aver-
age, few-shot performance grows with models size,
with a few exceptions such as QQP+T5-v1.1-Base
and MRPC+Bart-Large. In Fig. 8(b-c) we plot
the effect brought by upstream learning method
for larger models. Except for FAOMAML+T5-v1.1-
Base®, upstream learning methods consistently im-
proves few-shot performance on 7;¢,:, which ver-
ifies that upstream learning methods we use are
model-agnostic, and can be applied to larger mod-
els to further improve few-shot performance.

“@- Q6. Can we use pattern-exploiting training
to replace direct fine-tuning to achieve even
better performance?

Pattern-exploiting training (PET) is a novel method
that formulate a target task into cloze-style ques-
tions (Schick and Schiitze, 2020a,b; Gao et al.,
2020). This approach narrows the gap between
the masked language modeling objective during
pre-training and downstream task fine-tuning, and
therefore leads to more efficient transfer. PET is
demonstrated to be effective with encoder mod-
els (e.g., ROBERTa), however, whether it is appli-
cable to text-to-text models with auto-regressive
decoders is underexplored to the best of our knowl-
edge. In Q6, we study whether applying PET-
style methods to text-to-text models is feasible, and
whether combining the two methods further pushes
the few-shot performance.

To align with the experiment settings in (Schick
and Schiitze, 2020a,b; Gao et al., 2020), we intro-
duce a new task partition “Held-out-GLUE”, which
uses non-GLUE classification tasks as Tipgin, and
GLUE tasks as Ties. We use the top 3 patterns in
(Gao et al., 2020) for each GLUE task, and use the

7T5-Base was trained on a mixture of downstream tasks
during its pre-training; such practice strays from the purpose
of our study. Therefore, we use T5-v1.1-Base model, which is
trained with the C4 Corpus only.

8We observe instability in training loss during FOMAML
training for TS-v1.1-Base.


Commonsense QA, Held-out-Multiple-Choice

Ropes, Held-out-MRC

MNLI, Held-out-NLI

60% 70%
BART-Base

—— Multi-Task Learning

50% + —f— Meta-Learning aa

50% +

QA-F1

40% 4

Accuracy
£
3S
s

30% +
30% +

20% 20%

60% pnp ey

Accuracy

32 64 128 256 512 1024 2048 4096 8717(all) 32
# Train Examples

64 128 256 512 1024 2048 4096 9900(all)
# Train Examples

30% “— T T T T T T T
48 96 192 384 768 1536 3072 391678(all)

# Train Examples

Figure 7: Performance comparisons in medium and high-resource scenarios. Benefits brought by upstream learning

lasts in medium-resource scenarios.

Accuracy (%)
8 8
BS s

g
s

(a) Direct Fine-tuning w. Different Base Models __ (b) T5-v1.1-Base x, (c) Bart-Large
& S 50%
80% 4 ME Bart-Base = == multi = == multi
(@mm 15-v1.1-Base & 40% + mmm first-order mam & 40% + mum first-order maml
4 @m Bart-Large ma reptile mm reptile
9 8 30% 4 PO a ecarnemearermeasn 8 30% + ;
Ss Ss
o o
1 E 20% 4 E 20%4
2 2
J @ 10% 4 il @ 10% 4
o o
2 0% 2 0% mm
40% 4 Rs} Rs}
& -10% & -10%
IS
eo

‘3 No
ow ger op

oe we
w

Ne yn ye
ga elt?
we

Pc
gv® a ac
we

Figure 8: Extending upstream learning to larger pre-trained text-to-text models. (a) Absolute performance with
direct fine-tuning with different pre-trained models. (b-c) Relative performance gain using upstream learning.

ensemble of the three models to produce the final
prediction.

Since pattern-exploiting training is originally de-
signed for encoder models (e.g., BERT/RoBERTa),
we first tried two of its variants that adapts it to
our auto-regressive transformer models. The first
variant generates complete sentence, e.g., generate
“The movie is great. A wonderful piece” from “The
movie is great. A <mask> piece” for sentiment
classification. The second variant generates only
the word “wonderful”, from “The movie is great.
A <mask> piece”. Though the first variant is more
similar to the denoising pre-training objective of
BART, we find the second variant to have better
performance.

We then launch pattern-exploiting training us-
ing variant two with the original BART-Base mod-
els. We observe negative performance on aver-
age (leftmost blue bar in Fig. 9). Performance
is improved with CoLA and MRPC, but not with
the remaining GLUE tasks. We further launch
experiments with/without pattern-exploiting train-
ing, with our upstream learning checkpoints. Still
pattern-exploiting training leads to deteriorated per-
formance on average.

We stop further investigation since this is out of
the scope of our study. Still we believe it is im-
portant to identify the reasons and develop pattern-
exploiting methods for auto-regressive models.

D_ Reproducibility

Implementation. All our experiments are imple-
mented with Huggingface Transformers’ (Wolf
et al., 2020). For higher-order optimization in
the meta-learning approach optimization, we use
higher library!®. Our code has been uploaded in
supplementary materials, and is also open-sourced
at https://github.com/INK-USC/CrossFit.

Hyper-parameters. We mainly follow the prac-
tice in (Gao et al., 2020). During few-shot fine-
tuning, we select the learning rate from {le —
5, 2e—5, 5e—5}, and the batch size from {2, 4, 8},
based on Dgey performance. We set the total num-
ber of updates to be 1000, number of warmup up-
dates to be 100. We evaluate the model on Daey
every 100 steps.

Infrastructure and Runtime. Upstream learn-
ing are done with one single Quadro RTX 8000
(48GB). Upstream learning jobs finishes within 3
hours on average. Fine-tuning experiments are all
done with one single GPU, with either NVIDIA
Quadro GP100, NVIDIA Quadro RTX 8000,
NVIDIA Quadro RTX 6000, NVIDIA GeForce
RTX 1080 Ti, or NVIDIA GeForce RTX 2080 Ti,
based on availability. Fine-tuning on one few-shot

*https://github.com/huggingface/transformers
‘Ohttps://github.com/facebookresearch/higher


60%
— direct fine-tuning lm multi-task leaming == mami mm fomaml mam reptile

direct fine-tuning + template multi-task learning + template ml. maml-+ template ft. fomaml + template reptile + template
Cn

40% 4

30% +

20% +

Relative Performance Gain (%)

10% 4
o% {tn
-10% + r 1
Ae gt ath it co? oe 49" oe
gv? gy? gv ow oy” es oe gs yes pe

Figure 9: Combining upstream learning with pattern-exploiting training.

task (with hyperparmeter tuning for all 5 random
samples) takes approximately 4 hours on average.

Number of Parameters. BART-Base model
contains 139 million parameters. T5-v1.1-Base
model contains 246 million parameters. BART-
Large model contains 406 million parameters.
