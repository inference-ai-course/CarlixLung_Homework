Noisy Text Data: Achilles’ Heel of popular transformer based NLP models

Kartikay Bagla!, Ankit Kumar’, Shivam Gupta*, Anuj Gupta”
'Delhi Technological University
?Vahan Inc.
> Ninja Salary

Abstract

In the last few years, the ML community
has created a number of new NLP models
based on transformer architecture. These mod-
els have shown great performance for vari-
ous NLP tasks on benchmark datasets, often
surpassing SOTA results. Buoyed with this
success, one often finds industry practitioners
actively experimenting with fine-tuning these
models to build NLP applications for industry
use cases. However, for most datasets that are
used by practitioners to build industrial NLP
applications, it is hard to guarantee the pres-
ence of any noise in the data. While most trans-
former based NLP models have performed ex-
ceedingly well in transferring the learnings
from one dataset to another, it remains unclear
how these models perform when fine-tuned on
noisy text.

We address the open question by Kumar et al.
(2020) to explore the sensitivity of popular
transformer based NLP models to noise in the
text data. We continue working with the noise
as defined by them - spelling mistakes & ty-
pos (which are the most commonly occurring
noise). We show (via experimental results)
that these models perform badly on most com-
mon NLP tasks namely text classification, tex-
tual similarity, NER, question answering, text
summarization on benchmark datasets. We fur-
ther show that as the noise in data increases,
the performance degrades. Our findings sug-
gest that one must be vary of the presence of
noise in their datasets while fine-tuning popu-
lar transformer based NLP models.

1 Introduction

It is a well known fact that pre-trained contex-
tualized language models such as BERT (Bidirec-
tional Encoder Representations from Transformers)
Devlin et al. (2018), BART (Bidirectional and Auto-
Regressive Transformer) Lewis et al. (2019), ROBERTa
(robustly optimized BERT pretraining approach) Liu
et al. (2019), ALBERT (A Lite BERT) Lan et al.
(2019), XLNet (Generalized autoregressive pretrain-
ing for language understanding) Yang et al. (2019),

TS (Text-to-Text Transfer Transformer) Raffel et al.
(2019) have shown remarkable gains in performance
for various Natural Language Processing (NLP) tasks.
This includes most common downstream tasks such as
Text Classification, Textual Similarity, Summarization,
Name-Entity Recognition, Question Answering, Ma-
chine Translation etc.

Given this fantastic progress, machine learning
teams in the industry are actively experimenting with
fine-tuning these models on their data to solve indus-
try use cases. These include applications such as chat-
bots, sentiment analysis systems, intelligent ticketing
systems, entity recognition systems, machine transla-
tion systems etc. To building these applications practi-
tioners often assemble the required dataset by collect-
ing text data from data sources & applications such
as chats, emails, discussions from user forums, social
media conversations, output of machine translation sys-
tems, automatically transcribing text from speech data,
automatically recognized text from printed or handwrit-
ten material, etc. The text data from such applications
is often noisy (The amount of noise may differ depend-
ing on the source). For example, the data coming from
discussions on user forums & social media conversa-
tions, the noise in the text data can be significantly high.

Kumar et al. (2020) shows that the performance
of pre-trained BERT degrades significantly when fine-
tuned on noisy text data. We extend their work to show
this holds true for most popular transformer based NLP
models, namely BERT, BART, RoBERTa, ALBERT,
XLNet and T5. To be precise Kumar et. al () bench-
marked the performance of BERT when fine tuned on
noisy text data for 2 tasks - text classification and tex-
tual similarity using 3 datasets - IMDB, SST-2 and STS-
B. The main contributions of this paper are three fold:

e We extend the work of Kumar et al. (2020) to
benchmark the performance of BERT when fine
tuned on noisy text data for other fundamental
NLP tasks - Question Answering, NER and Sum-
merization using SQUAD, CoNLL and Billsum
datasets respectively.

We benchmark the performance of other pop-
ular transformer based NLP models - BART,
RoBERTa, ALBERT, XLNet and T5 on funda-
mental NLP tasks - text classification, textual simi-


larity, Question Answering, NER and Summeriza-
tion on noisy text data!.

¢ We show that all the above mentioned models per-
form badly when fine-tuned on noisy text data.
Further, as the noise increases, performance be-
comes worse.

This work is motivated from a business use case
where we built a conversational system over WhatsApp
to screen job seekers for blue collar jobs. The candi-
dates often are not even college graduates. This paired
with the fat finger problem? over a smartphone keypad
often results in many typos and spelling mistakes in the
responses job seekers send to our conversational sys-
tem. Though this work is inspired from our business
use case, our findings are applicable to other use cases
that deal with noisy text data.

2 Previous Work

Modern communication mediums such as SMS, chats,
twitter, messaging apps encourage brevity and infor-
malism, leading to non-canonical text. This presents
significant challenges to the known NLP techniques.
Research community has done a lot of work on validat-
ing various transformer based language models in the
presence of noisy text. The NLP community has done
a lot of work on understanding the effects of noise on
the performance of NLP models. Taghva et al. (2000)
evaluate the effect of OCR errors on text categoriza-
tion. Wu et al. (2016) introduced ISSAC, a system to
clean dirty text from online sources. Belinkov and Bisk
(2017) show that character basedneural machine trans-
lation (NMT) models are also prone to synthetic and
natural noise even though these model dobetter job to
handle out-of-vocabulary issues and learn better mor-
phological representation.

Aspillaga et al. (2020) evaluated RoBERTa, XL-
Net, and BERT in Natural Language Inference (NLI)
and Question Answering (QA) tasks. They used
BiDAF (Seo et al., 2016) and Match-LSTM (Wang and
Jiang, 2016) as baselines to compare stress tests against
Transformer-based models. They did two type of tests
- distraction test and noise test(spelling errors). They
show that RoBERTa, XLNet and BERT are more ro-
bust in stress tests than recurrent neural network mod-
els. Ravichander et al. (2021) describes a real world sce-
nario where question answering system can be affected
by different types of noise such as keyboard errors and
ASR errors. They evaluate SOTA methods on natural
and synthetic noisy data and demonstrate that the per-
formance of QA systems is impacted by the real world
noise. They further analyze synthetic noise and its im-
pact on the downstream question answering system and
presented an initial exploration of mitigation strategies

‘Certain models are not applicable for certain tasks. We
skip those tasks
*https://en.wikipedia.org/wiki/Fat-finger_error

for real world noise. Alshemali and Kalita (2020) test
DNN models on different NLP tasks like Classification,
Machine Translation, Question Answering, Textual En-
tailment, Tagging etc. They illustrate the vulnerability
of DNNs to adversarial examples — inputs modified
by introducing small perturbations to deliberately fool
the target model into giving incorrect results. Agarwal
et al. (2007) study the effect of different kinds of noise
on automatic Text Classification. They present de-
tailed experimental results with simulated noise on the
Reuters21578 and 20-newsgroups benchmark datasets;
also with results on real-life noisy datasets from vari-
ous CRM domains. They use spelling errors to gen-
erate synthetic dataset and show the effect of noise
on final accuracy of text classification task. Subrama-
niam et al. (2009) present a survey on the types of text
noise and techniques to handle it. Belinkov and Bisk
(2017) show that character based neural machine trans-
lation (NMT) models are prone to synthetic and natu-
ral noise even though these model do better job to han-
dle out-of-vocabulary issues and learn better morpho-
logical representation. Ribeiro et al. (2018) developed
a technique, called semantically equivalent adversarial
rules (SEARs) to debug NLP models. SEAR generates
adversial examples to penetrate NLP models. Authors
experimented this techniques for three domains: ma-
chine comprehension, visual question answering, and
sentiment analysis.

Sun et al. (2020) explored robustness of BERT in deal-
ing with noisy data. They experimented with senti-
ment analysis and question answering tasks. They in-
ject keyboard typos in data in two ways - a) adding
typos randomly b) adding typos only in the most in-
formative words. They show that typos in informa-
tive words cause severer degradation. Pal and Tople
(2020) presented novel attack techniques that utilized
the unintended features learnt in the teacher (public)
model to generate adversarial examples for student
(downstream) models. They show that using length-
based and sentence-based misclassification attacks for
the Fake News Detection task trained using a context-
aware BERT model, one gets misclassification accu-
racy of 78% and 39% respectively for the adversarial
examples. Jin et al. (2019) introduced TEXTFOOLER,
a system which generates adversarial text and applied it
to text classification and textual entailment to success-
fully attack the pre-trained BERT among other models.

3 Experiments

We evaluate the 6 transformer based NLP models -
BERT?, BART, RoBERTa, ALBERT, XLNet and TS.
For this evaluation we use the following tasks [and cor-
responding datasets]:

1. Sentiment Analysis [IMDB movie reviews (Maas
et al., 2011) and Stanford Sentiment Treebank
(SST-2) (Socher et al., 2013)]

3BERT pase uncased model


2. Textual similarity [Semantic Textual Similarity
(STS-B) (Cer et al., 2017)]

3. Question Answering [SQuAD2.0 (Rajpurkar et al.,
2016)]

4. Named Entity Recognition [CoNLL (Sang and
De Meulder, 2003) ]

5. Text summerization [Billsum (Kornilova and E1-
delman, 2019)]

On each of these 6 datasets, we report the performance
of each of the 6 models*. both - with and without noise.

3.1 Noise

We directly borrow the notion of noise as defined by
(Kumar et al., 2020). They focus on the noise intro-
duced by spelling mistakes and typos. All the bench-
mark datasets used consist of examples X — Y where
X is the text input and Y is the corresponding label.
They call the original dataset as Do. From Do they cre-
ate new datasets Ds, Dio, Djs, Doo and Dos. Dx is a
variant of Do with k% noise in each datapoint in Do.

To create Dx, they take i data point x; € Dx, and
introduce noise in it. They represent the modified data-
point by Ry toe . Then, Dx is simply the collection (x;x
noIse, yi), Vi. To create a from x;, they randomly
choose k% characters from the text of x; and replace
them with nearby characters in a qwerty keyboard. For
example, if character d is chosen, then it is replaced by
a character randomly chosen from e, s, x, c, f, or r. This
is because in a qwerty keyboard, these keys surround
the key d. They inject noise in the complete dataset.
Once split Dj into train and test chunks.

3.2 Text Classification

For text classification we use IMDB movie reviews
(Maas et al., 2011) and Stanford Sentiment Treebank
(SST-2 ) (Socher et al., 2013) datasets in binary pre-
diction settings. IMDB datasets consist of 25000 train-
ing and 25000 test sentences. We represent the original
IMDB dataset (one with no noise) as IMDBo. Using the
process of introducing noise (as described in section
3.1), we create 5 variants of IMDBy namely IMDB;,
..., IMDB»; with varying degrees of noise.

SST-2 dataset consists of 67349 training and 872 test
sentences. Here too we we add noise as described in
Section 3.1 to create 5 variants of SST-29 - SST-2s,...,
SST-225.To measure the performance for text classifica-
tion, we use F1 score.

3.3 Textual Similarity

For the textual similarity task, we use the Semantic
Textual Similarity (STS-B) (Cer et al., 2017) dataset.
The dataset consists of 5749 training and 1500 test data
points. Each data point consists of 2 sentences and a

“Certain models are not applicable for certain tasks. We
skip those.

% error | BERT | RoBERTa | ALBERT | XLNet
“~~ ~ 10.938 | 0.947 ]~ 0.935” | 0.948" ~
5 0.914 0.929 0.908 0.922
10 0.882 0.905 0.865 0.894
15 0.843 0.872 0.782 0.859
20 0.81 0.835 0.781 0.83
25 0.758 0.805 0.77 0.775

% error | BERT | RoBERTa | ALBERT | XLNet
“~"O ~ 10918 | ~ 0.933 |” 0.9077 | 0.913 ©
5 0.88 0.893 0.856 0.885
10 0.847 0.865 0.802 0.833
15 0.804 0.827 0.763 0.807
20 0.79 0.781 0.76 0.754
25 0.746 0.733 0.689 0.721

Table 2: Fl scores on SST-2.

score between 0-5 representing the similarity between
the two sentences. We represent the original data set
by STS-Bo and create 5 noisy variants. Here, we use
Pearson-Spearman correlation to measure model’s per-
formance.

3.4 Question Answering

For the question answering task, we use the
Stanford Question Answering Dataset version 2.0
(SQUAD2.0)(Rajpurkar et al., 2016). This dataset has
129,941 training and 5915 test paragraph and question
pairs. The evaluation metric we used is Fl. F1 score
takes each gold answer as bags of words and doesn’t
require choosing the exact same span as a human’s,
which is seen as more reliable.

3.5 Named Entity Recognition

We used the CoNLL-2003(Sang and De Meulder,
2003) dataset for Named Entity Recognition. It con-
sists of 22,137 sentences totally and is split into
14,987,and 3,684 sentences for the training and test
sets, respectively. It is tagged with four linguistic en-
tity types (PER, LOC, ORG, MISC). We used average
F1 score as evaluation metric.

3.6 Text Summarization

For the text summarization task, we used the Bill-
Sum(Kornilova and Eidelman, 2019) dataset. It
contains 23,000 US Congressional bills and human-
written reference summaries from the 103rd-115th
(1993-2018) sessions of Congress. We use ROUGEI-
F1(ROUGE (2004)) metric for the summarization task.

3.7 Results

The results of various experiments are shown in Tables
1, 2, 3, 4, 5 and 6 respectively. Note that 0% er-
ror case represents a no noise scenario. Interestingly


% error | BERT | RoBERTa | ALBERT | XLNet
“~~ ~"1°0:896 | ~ 0.906 ]~ 0.89 ~ ] 0.883 ©
5 0.7794 0.848 0.766 0.746
10 0.624 0.761 0.59 0.571
15 0.49 0.668 0.388 0.449
20 0.398 0.513 0.232 0.336
25 0.355 0.497 0.252 0.288

Table 3: Pearson-Spearman correlations on STS-B.

SQUAD2
% error | BERT | RoBERTa | ALBERT
~~ 7 V 7187 T8236 ~ 7 7801 7
5 64.98 77.04 70.38
10 55.16 68.6 60.33
15 48.18 61.74 53.02
20 41.7 54.08 46.03
25 33 48.16 39.17

Table 4: Fl scores on SQUAD2.

% error | BERT | RoBERTa | ALBERT | XLNet
~~ ~0 ~ "194.49 7 ~ 96.05 |” 93.35" 7] 95.85 7

5 90 93.62 87.17 92.23

10 84.96 91.18 83.41 90.02

15 81.19 88.47 78.8 87.23

20 78.42 85.88 74.48 84.28

25 75.25 83.6 70.75 81.88

Table 5: Fl scores on CoNLL.

BillSum
% error | t5-small | bart-base | distil-bart | t5-base
~~ 71" 5701 [ 7 57:76 ~ | 5799 ~ 7 60.82 ~

5 53.23 56.01 55.84 57.66
10 49.76 54.85 54.46 55.33
15 47.49 53.07 53.41 53.39
20 45.54 52.52, 52.39 51.76
25 43.76 51.41 51.41 49.98

Table 6: ROUGEI1-F1 scores on BillSum.

for most scenarios across tasks - ROBERTa consistently
gives better performance as compared to other three
models for same amount of noise. At the same time
BERT and ALBERT show bad performance.

4 Conclusion and Future Work

In this work, we studied the effect of synthetic noise
(spelling mistakes) in text data on the performance
of popular transformer based language models. Our
experiments show that as the noise in the data in-
creases, model performance drops significantly. Our
work shows that one must be congnizant of the pres-
ence of any noise in their text data if they are fine tun-
ing NLP models on noisy text data. Further, if there is
noise is text data, then either 1) one has to preprocess
data until all noise is removed. This can become a full
fledged project in its own. 2) make changes to the ar-
chitecture of these models to make them robust to noise.
We leave this as a future work.

It will also be interesting to see how these models
perform in the presence of other types of noise. It also
remains to be seen if the results will hold when the
noise is restricted to only frequent misspellings. Also it
remains to be seen why RoBERTa shows more stability
to noise unlike BERT and ALBERT.

References

Sumeet Agarwal, Shantanu Godbole, Diwakar Pun-
jani, and Shourya Roy. 2007. How much noise is
too much: A study in automatic text classification.
In Seventh IEEE International Conference on Data
Mining (ICDM 2007), pages 3-12. IEEE.

Basemah Alshemali and Jugal Kalita. 2020. Improv-
ing the reliability of deep neural networks in nlp: A
review. Knowledge-Based Systems, 191:105210.

Carlos Aspillaga, Andrés Carvallo, and Vladimir
Araujo. 2020. Stress test evaluation of transformer-
based models in natural language understanding
tasks. arXiv preprint arXiv:2002.06261.

Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic
and natural noise both break neural machine transla-
tion. arXiv preprint arXiv:1711.02173.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv: 1708.00055.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv: 1810.04805.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Szolovits. 2019. Is bert really robust. A Strong Base-
line for Natural Language Attack on Text Classifica-
tion and Entailment.


Anastassia Kornilova and Vlad Eidelman. 2019. Bill-
sum: A corpus for automatic summarization of US
legislation. CoRR, abs/1910.00523.

Ankit Kumar, Piyush Makhija, and Anuj Gupta. 2020.
Noisy text data: Achilles’ heel of BERT. In Proceed-
ings of the Sixth Workshop on Noisy User-generated
Text (W-NUT 2020), pages 16-21, Online. Associa-
tion for Computational Linguistics.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learn-
ing of language representations. arXiv preprint
arXiv: 1909.11942.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and
comprehension. arXiv preprint arXiv: 1910.13461.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv: 1907.11692.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th annual meeting of the as-
sociation for computational linguistics: Human lan-
guage technologies-volume 1, pages 142—150. Asso-
ciation for Computational Linguistics.

Bijeeta Pal and Shruti Tople. 2020. To transfer or
not to transfer: Misclassification attacks against
transfer learned text classifiers. arXiv preprint
arXiv:2001.02438.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. arXiv preprint arXiv: 1910.10683.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQUAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383-2392, Austin,
Texas. Association for Computational Linguistics.

Abhilasha Ravichander, Siddharth Dalmia, Maria
Ryskina, Florian Metze, Eduard Hovy, and Alan W
Black. 2021. Noiseqa: Challenge set evaluation
for user-centric question answering. arXiv preprint
arXiv:2102.08345.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging nlp models. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 856-865.

Lin CY ROUGE. 2004. A package for automatic evalu-
ation of summaries. In Proceedings of Workshop on
Text Summarization of ACL, Spain.

Erik F Sang and Fien De Meulder. 2003.  Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. arXiv
preprint cs/0306050.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empiri-
cal methods in natural language processing, pages

1631-1642.

L. Venkata Subramaniam, Shourya Roy, Tanveer A.
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text.
In Proceedings of The Third Workshop on Analytics
for Noisy Unstructured Text Data, AND ’09, page
115-122, New York, NY, USA. Association for Com-
puting Machinery.

Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari
Asai, Jiugang Li, Philip S. Yu, and Caiming Xiong.
2020. Adv-bert: Bert is not robust on misspellings!
generating nature adversarial samples on bert. ArXiv,
abs/2003.04985.

Kazem Taghva, Thomas A Nartker, Julie Borsack,
Steven Lumos, Allen Condit, and Ron Young. 2000.
Evaluating text categorization in the presence of ocr
errors. In Document Recognition and Retrieval VIII,
volume 4307, pages 68-74. International Society for
Optics and Photonics.

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv: 1608.07905.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
man and machine translation. arXiv preprint
arXiv: 1609.08 144.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
XlInet: Generalized autoregressive pretraining for
language understanding. Advances in neural infor-
mation processing systems, 32.
