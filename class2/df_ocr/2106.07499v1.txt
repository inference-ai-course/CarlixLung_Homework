arX1v:2106.07499v1 [cs.CL] 14 Jun 2021

An Empirical Survey of Data Augmentation
for Limited Data Learning in NLP

Jiaao Chen®* Derek Tam‘

Colin Raffel!
°Georgia Institute of Technology

Mohit Bansal
UNC Chapel Hill

Diyi Yang®

{jchen8 96, dyang888}@gatech.edu
{dtredsox, craffel, mbansal}@cs.unc.edu

Abstract

NLP has achieved great progress in the past
decade through the use of neural models and
large labeled datasets. The dependence on
abundant data prevents NLP models from be-
ing applied to low-resource settings or novel
tasks where significant time, money, or ex-
pertise is required to label massive amounts
of textual data. Recently, data augmentation
methods have been explored as a means of im-
proving data efficiency in NLP. To date, there
has been no systematic empirical overview of
data augmentation for NLP in the limited la-
beled data setting, making it difficult to un-
derstand which methods work in which set-
tings. In this paper, we provide an empirical
survey of recent progress on data augmenta-
tion for NLP in the limited labeled data setting,
summarizing the landscape of methods (in-
cluding token-level augmentations, sentence-
level augmentations, adversarial augmenta-
tions and hidden-space augmentations) and
carrying out experiments on 11 datasets cover-
ing topics/news classification, inference tasks,
paraphrasing tasks, and single-sentence tasks.
Based on the results, we draw several conclu-
sions to help practitioners choose appropriate
augmentations in different settings and discuss
the current challenges and future directions for
limited data learning in NLP.

1 Introduction

Deep learning methods have achieved strong per-
formance on a wide range of supervised learn-
ing tasks (Sutskever et al., 2014; Deng et al.,
2013; Minaee et al., 2021). Traditionally, these re-
sults were attained through the use of large, well-
labeled datasets. This make them challenging to
apply in settings where collecting a large amount
of high-quality labeled data for training is expen-
sive. Moreover, given the fast-changing nature of
real-world applications, it is infeasible to relabel

*Equal contribution.

every example whenever new data comes in. This
highlights a need for learning algorithms that can
be trained with a limited amount of labeled data.

There has been a substantial amount of re-
search towards learning with limited labeled data
for various tasks in the NLP community. One
common approach for mitigating the need for la-
beled data is data augmentation. Data augmen-
tation (Feng et al., 2021) generates new data by
modifying existing data points through transfor-
mations that are designed based on prior knowl-
edge about the problem’s structure (Yang, 2015;
Wei and Zou, 2019). This augmented data can
be generated from labeled data, and then di-
rectly used in supervised learning (Wei and Zou,
2019), or in semi-supervised learning for unla-
beled data through consistency regularization (Xie
et al., 2020) (“consistency training”). While var-
ious approaches have been proposed to tackle
learning with limited labeled data — including un-
supervised pre-training (Peters et al., 2018; De-
vlin et al., 2019; Raffel et al., 2020), multi-task
learning (Glorot et al., 2011; Liu et al., 2017; Au-
genstein et al., 2018), semi-supervised learning
(Zhu, 2005; Chapelle et al., 2009; Miyato et al.,
2017; Xie et al., 2020), and few-shot learning
(Deng et al., 2019) — in this work, we focus on
and compare different data augmentation meth-
ods and their application to supervised and semi-
supervised learning.

In this survey, we comprehensively review and
perform experiments on recent data augmentation
techniques developed for various NLP tasks. Our
contributions are three-fold: (1) summarize and
categorize recent methods in textual data augmen-
tation; (2) compare different data augmentation
methods through experiments with limited labeled
data in supervised and semi-supervised settings
on 11 NLP tasks, and (3) discuss current chal-
lenges and future directions of data augmentation,
as well as learning with limited data in NLP more


broadly. Our experimental results allow us to con-
clude that no single augmentation works best for
every task, but (i) token-level augmentations work
well for supervised learning, (ii) sentence-level
augmentation usually works the best for semi-
supervised learning, and (iii) augmentation meth-
ods can sometimes hurt performance, even in the
semi-supervised setting.

Related Surveys. Recently, several surveys also
explore the data augmentation techniques for NLP
(Hedderich et al., 2020; Feng et al., 2021). Hed-
derich et al. (2020) provide a broad overview of
techniques for NLP in low resource scenarios and
briefly cover data augmentation as one of several
techniques. In contrast, we focus on data augmen-
tation and provide a more comprehensive review
on recent data augmentation methods in this work.
While Feng et al. (2021) also survey task-specific
data augmentation approaches for NLP, our work
summarizes recent data augmentation methods in
a more fine-grained categorization. We also fo-
cus on their application to learning from limited
data by providing an empirical study over differ-
ent augmentation methods on various benchmark
datasets in both supervised and semi-supervised
settings, so as to hint data augmentation selections
in future research.

2 Data Augmentation for NLP

Data augmentation increases both the amount (the
number of data points) and the diversity (the va-
riety of data) of a given dataset (Cubuk et al.,
2019). Limited labeled data often leads to over-
fitting on the training set and data augmentation
works to alleviate this issue by manipulating data
either automatically or manually to create addi-
tional augmented data.Such techniques have been
widely explored in the computer vision field, with
methods like geometric/color space transforma-
tions (Simard et al., 2003; Krizhevsky et al., 2012;
Taylor and Nitschke, 2018), mixup (Zhang et al.,
2018), and random erasing (Zhong et al., 2020;
DeVries and Taylor, 2017). Although the dis-
crete nature of textual data and its complex syn-
tactic and semantic structures make finding label-
preserving transformation more difficult, there
nevertheless exists a wide range of methods for
augmenting text data that in practice preserve la-
bels. In the following subsections, we describe
four broad classes of data augmentation methods:

2.1 Token-Level Augmentation

Token-level augmentations manipulate words and
phrases in a sentence to generate augmented text
while ideally retaining the semantic meaning and
labels of the original text.

Designed Replacement. Intuitively, the seman-
tic meaning of a sentence remains unchanged if
some of its tokens are replaced with other tokens
that have the same meaning. A simple approach
is to fetch synonyms as words for substitutions
(Kolomiyets et al., 2011; Yang, 2015; Zhang et al.,
2015a; Wei and Zou, 2019; Miao et al., 2020). The
synonyms are discovered based on pre-defined
dictionaries such as WordNet (Kolomiyets et al.,
2011), or similarities in word embedding space
(Yang, 2015). However, improvements from this
technique are usually minimal (Kolomiyets et al.,
2011) and in some cases, performance may even
degrade (Zhang et al., 2015a). A major draw-
back stems from the lack of contextual infor-
mation when fetching synonyms—especially for
words with multiple meanings and few synonyms.
To resolve this, language models (LMs) have been
used to replace the sampled words given their con-
text (Kolomiyets et al., 2011; Fadaee et al., 2017;
Kobayashi, 2018; Kumar et al., 2020). Other work
preserves the labels of the text by conditioning on
the label when generating the LMs’ predictions
(Kobayashi, 2018; Wu et al., 2019a). In addition,
different sampling strategies for word replacement
have been explored. For example, instead of sam-
pling one specific word from candidates by LMs,
Gao et al. (2019) propose to compute a weighted
average over embeddings of possible words pre-
dicted by LMs as the replaced input since the av-
eraged representations could augment text with
richer information.

Random Insertion, Replacement, Deletion and
Swapping. While well-designed local modifica-
tions can preserve the syntax and semantic mean-
ing of a sentence (Niu and Bansal, 2018), random
local modifications such as deleting certain tokens
(lyyer et al., 2015; Wei and Zou, 2019; Miao et al.,
2020), inserting random tokens (Wei and Zou,
2019; Miao et al., 2020), replacing non-important
tokens with random tokens (Xie et al., 2017, 2020;
Niu and Bansal, 2018) or randomly swapping to-
kens in one sentence (Artetxe et al., 2018; Lample
et al., 2018; Wei and Zou, 2019; Miao et al., 2020)
can preserve the meaning in practice. Different


Methods Level Diversity Tasks Related Work
. . Kolomiyets et al. (2011), Zhang et al. (2015a),
eh Token Low gant Classification Yang (2015), Miao et al. (2020),
P 4 & Wei and Zou (2019)
Wand senlacement Text classification Kolomiyets et al. (2011), Gao et al. (2019)
Pract LM Token Medium Sequence labeling Kobayashi (2018), Wu et al. (2019a)
Via Machine translation __Fadaee et al. (2017)
Random insertion Sequence abelin lyyeret al. @015), Mieetal. 2017)
dAaion. mvancie > Token Low Madhiae waneatin i Artetxe et al. (2018), Lample et al. (2018)
+ SWAPPUIE : : Xie et al. (2020), Wei and Zou (2019)
Dialogue generation
in m tation TOKen High ian nee to aah Nye et al. (2020), Feng et al. (2020)
& guage me 8 Furrer et al. (2020) , Guo et al. (2020)
Text generation
Text classification
Machine translation Yu et al. (2018), Xie et al. (2020)
Paraphrasing Sentence High Question answering Chen etal. (2019), He et al. (2020)
Dialogue generation Chen et al. (2020c), Cai et al. (2020)
Text summarization
Conditional Sent Hich Text classification Anaby-Tavor et al. (2020), Kumar et al. (2020)
generation enrence 8 Question answering Zhang and Bansal (2019), Yang et al. (2020)
: Text classification Miyato et al. (2017), Ebrahimi et al. (2018b)
Waite box Rene Medium Sequence labeling  Ebrahimi et al. (2018a), Cheng et al. (2019),
Machine translation Chen et al. (2020d)
Text classification
Sequence labeling Jia and Liang (2017)
Black-box Token or Medi Machine translation _— Belinkov and Bisk (2017), Zhao et al. (2017)
attack Sentence eam Textual entailment Ribeiro et al. (2018), McCoy et al. (2019)
Dialogue generation Min et al. (2020), Tan et al. (2020)
Text Summarization
iddeaeeace ‘Token ox Text classification Hsu et al. (2017), Hsu et al. (2018)
Adu son Semtanea High Sequence labeling Wu et al. (2019b), Chen et al. (2021)
P Speech recognition Malandrakis et al. (2019), Shen et al. (2020)
Text classification Miao et al. (2020), Chen et al. (2020c)
Interpolation Token High Sequence labeling Cheng et al. (2020b), Chen et al. (2020a)
Machine translation Guo et al. (2020)
Table 1: Overview of different data augmentation techniques in NLP. Diversity refers to the difference of augmented

data from existing data and the amount of different augmented data could be generated.

kinds of operations can be further combined (Wei
and Zou, 2019), where each example is randomly
augmented with one of insertion, deletion, and
swapping. These noise-injection methods can effi-
ciently be applied to training, and show improve-
ments when they augment simple models trained
on small training sets. However, the improvements
might be unstable due to the possibility that ran-
dom perturbations change the meanings of sen-
tences (Niu and Bansal, 2018). Also, finetuning
large pre-trained models on specific tasks might
attenuate improvements due to preexisting gener-
alization abilities of the model (Shleifer, 2019).

Compositional Augmentation. To increase the
compositional generalization abilities of models,

recent efforts have also focused on composi-
tional augmentations (Jia and Liang, 2016; An-
dreas, 2020) where different fragments from dif-
ferent sentences are re-combined to create aug-
mented examples. Compared to random swap-
ping, compositional augmentation often requires
more carefully-designed rules such as lexical over-
lap (Andreas, 2020), neural-symbolic stack ma-
chines (Chen et al., 2020e), and neural program
synthesis (Nye et al., 2020). With the potential
to greatly improve the generalization abilities to
out-of-distribution data, compositional augmenta-
tion has been utilized in sequence labeling (Guo
et al., 2020), semantic parsing (Andreas, 2020;
Nye et al., 2020; Furrer et al., 2020), language


modeling (Andreas, 2020; Shaw et al., 2020), and
text generation (Feng et al., 2020).

2.2 Sentence-Level Augmentation

Instead of modifying tokens, sentence-level aug-
mentation modifies the entire sentence at once.

Paraphrasing. Paraphrasing has been widely
adopted as a data augmentation technique in var-
ious NLP tasks (Yu et al., 2018; Xie et al., 2020;
Kumar et al., 2019; He et al., 2020; Chen et al.,
2020b,c; Cai et al., 2020), as it generally pro-
vides more diverse augmented text with different
word choices and sentence structures while pre-
serving the meaning of the original text. The most
popular is round-trip translation (Sennrich et al.,
2015; Edunov et al., 2018), a pipeline which first
translates sentences into certain intermediate lan-
guages and then translates them back to gener-
ate paraphrases. Translating through intermediate
languages with different vocabulary and linguis-
tic structures can generate useful paraphrases. To
ensure the diversity of augmented data, sampling
and noisy beam search can also be adopted during
the decoding stage (Edunov et al., 2018). Other
work focuses on directly training end-to-end mod-
els to generate paraphrases (Prakash et al., 2016),
and further augments the decoding phase with syn-
tactic information (Iyyer et al., 2018; Chen et al.,
2019), latent variables (Gupta et al., 2017), and
sub-modular objectives (Kumar et al., 2019).

Conditional Generation. Conditional genera-
tion methods generate additional text from a lan-
guage model, conditioned on the label. After train-
ing the model to generate the original text given
the label, the model can generate new text (Anaby-
Tavor et al., 2020; Zhang and Bansal, 2019; Ku-
mar et al., 2020; Yang et al., 2020). An extra fil-
tering process is often used to ensure high-quality
augmented data. For example, in text classifi-
cation, Anaby-Tavor et al. (2020) first fine-tune
GPT-2 (Radford et al., 2019) with the original ex-
amples prepended with their labels, and then gen-
erate augmented examples by feeding the fine-
tuned model certain labels. Only confident ex-
amples as judged by a baseline classifier trained
on the original data are kept. Similarly, new an-
swers are generated on the basis of given ques-
tions in question answering and are filtered by cus-
tomized metrics like question answering probabil-
ity (Zhang and Bansal, 2019) and n-gram diversity
(Yang et al., 2020). Generative models used in

this setting have been based on conditional VAE
(Bowman et al., 2016; Hu et al., 2017; Guu et al.,
2017; Malandrakis et al., 2019), GAN (lyyer et al.,
2018; Xu et al., 2018) or pre-trained language
models like GPT-2 (Anaby-Tavor et al., 2020; Ku-
mar et al., 2020). Overall, these conditional gen-
eration methods can create novel and diverse data
that might be unseen in the original dataset, but
require significant training effort.

2.3 Adversarial Data Augmentation

Adversarial methods create augmented examples
by adding adversarial perturbations to the original
data, which dramatically influences the model’s
predictions and confidence without changing hu-
man judgements. These adversarial examples
(Morris et al., 2020; Zeng et al., 2020) could
be leveraged in adversarial training (Goodfellow
et al., 2015) to increase neural models’ robustness,
and can also be utilized as data augmentation to in-
crease the models’ generalization ability (Miyato
et al., 2017; Cheng et al., 2019).!

White-Box methods _ rely on model architecture
and parameters being accessible and create ad-
versarial examples directly using a model’s gra-
dients. Unlike image pixel values that are contin-
uous, textual tokens are discrete and cannot be di-
rectly modified based on gradients. To this end,
adversarial perturbations are added directly to to-
ken embeddings or sentence hidden representa-
tions (Miyato et al., 2017; Zhu et al., 2020; Jiang
et al., 2019; Chen et al., 2020d) which creates “vir-
tual adversarial examples”. Other approaches vec-
torize modification operations as the difference of
one-hot vectors (Ebrahimi et al., 2018b,a), or find
real word neighbors in a model’s hidden represen-
tations via its gradients (Cheng et al., 2019).

Black-Box methods are usually model-agnostic
since they do not require information from a model
or its parameters and usually focus on task-specific
heuristics for creating adversarial examples. For
example, by enumerating feasible substitutions on
the basis of word similarity and language mod-
els, Ren et al. (2019) and Garg and Ramakr-
ishnan (2020) select adversarial word replace-
ments which severely influence the predictions
from the text classification model. To attack read-
ing comprehension systems, Jia and Liang (2017)

‘Ror more detailed discussion on textual adversarial ex-

amples, please refer to recent comprehensive surveys (Zhang
et al., 2020b; Huq and Pervin, 2020; Goel et al., 2021).


and Wang and Bansal (2018) insert distracting
but meaningless sentences at different locations
in paragraphs and Ribeiro et al. (2018) leverage
rule-based paraphrasing to produce semantically-
equivalent adversarial examples. Likewise, for
multi-hop question answering, Jiang and Bansal
(2019) insert shortcut reasoning sentences and
Trivedi et al. (2020) constructed disconnected rea-
soning example by removing certain supporting
facts. For machine translation, Belinkov and Bisk
(2017) attacks character-based models by natural
or synthesized typos and Tan et al. (2020) further
adopt subword morphology level attacks. Sim-
ilar attacks also help dialogue generation (Niu
and Bansal, 2019) and text summarization (Cheng
et al., 2020a; Fan et al., 2018). Other methods do
not rely in editing input text directly; Iyyer et al.
(2018) leverage round-trip translation to generate
paraphrases in given syntactic templates and Zhao
et al. (2017) search for adversarial examples in un-
derlying semantic space with GANs (Goodfellow
et al., 2014). Some of these heuristics could be
further refined to obtain simple adversarial data
augmentation approaches. For example, McCoy
et al. (2019) craft adversarial examples for natural
language inference using sophisticated templates
which create lexical overlap between the premise
and the hypothesis to fool the model. Min et al.
(2020) proposes two simple yet effective adver-
sarial transformations that reverse the position of
subject and object or the position of premise and
hypothesis.

2.4 Hidden-Space Augmentation

This line of work generates augmented data by
manipulating the hidden representations through
perturbations such as adding noise or perform-
ing interpolations with other data points. Hidden-
space perturbations augment existing data by
adding perturbations to the hidden representations
of tokens (Miyato et al., 2017; Zhu et al., 2020;
Jiang et al., 2019; Chen et al., 2020d; Shen et al.,
2020; Chen et al., 2021) or sentences (Hsu et al.,
2017, 2018; Wu et al., 2019b; Malandrakis et al.,
2019).

Interpolation-Based Methods. Interpolation-
based methods create new examples and labels by
linear combinations of existing data-label pairs.
Given two data-label pairs, virtual data-label pairs
are created through linear interpolations of the
pair of data points. Such interpolation-based

methods can generate infinite augmented data in
the “virtual vicinity” of the original data space,
thus improving the generalization performance
of models. Interpolation-based methods were
first explored in computer vision (Zhang et al.,
2018), and have more recently been generalized
to the text domain (Miao et al., 2020; Chen et al.,
2020c; Cheng et al., 2020b; Chen et al., 2020a)
by performing interpolation between original data
and token-level augmented data in the output
space (Miao et al., 2020), between original data
and adversarial data in embedding space (Cheng
et al., 2020b), or between different training
examples in general hidden space (Chen et al.,
2020c). Different strategies to select samples to
mix have also been explored (Chen et al., 2020a;
Guo et al., 2020; Zhang et al., 2020a) such as
k-nearest-neighbours (Chen et al., 2020a) or
sentence composition (Guo et al., 2020).

We summarize the preceding overview of recent
widely-used data augmentation methods in Table
1, characterizing them with respect to augmenta-
tion levels, the diversity of generated data, and
their applicable tasks.

3 Consistency Training with DA

While data augmentation (DA) can be applied in
the supervised setting to produce better results
when only a small labeled training dataset is avail-
able, data augmentation is also commonly used in
semi-supervised learning (SSL). SSL is an alter-
native approach for learning from limited data that
provides a framework for taking advantage of un-
labeled data. Specifically, SSL assumes that our
training set comprises labeled examples in addi-
tion to unlabeled examples drawn from the same
distribution. Currently, one of the most common
methods for performing SSL with deep neural net-
works is “consistency regularization” (Bachman
et al., 2014; Tarvainen and Valpola, 2017). Consis-
tency regularization-based SSL (or “consistency
training” for short) regularizes a model by en-
forcing that its output doesn’t change significantly
when the input is perturbed. In practice, the input
is perturbed by applying data augmentation, and
consistency is enforced through a loss term that
measures the difference between the model’s pre-
dictions on a clean input and a corresponding per-
turbed version of the same input.

Formally, let fg be a model with parameters 0,
f be a fixed copy of the model where no gradients


News Classification

Topic Classification

Methods Types
AG News 20Newsgroup Yahoo Answers PubMed
None - 78.8(8.9) 65.2(4.8) 56.6(9.4) 63.7(6.1)/49.3(3.9)
SR 79.4(5.9) 66.1(2.5) 56.0(10.1) 62.4(5.7)/48.3(3.9)
LM 76.8(5.1) 60.0(14.4) 56.2(8.4) 60.9(3.0)/47.4(2.5)
3 RI Token 79.5(4.9) 66.6(0.6) 57.3(12.0) 63.7(4.2)/49.4(2.1)
FS RD 79.6(5.0) 66.8(3.0) 58.0(8.3) 63.4(5.0)/49.3(1.5)
= RS 79.5(5.3) 64.8(10.8) 57.1(10.3) 63.8(7.4)/49.5(3.3)
a WR 79.7(2.0) 67.5(4.2) 59.3(8.9) 64.9(4.9)/49.4(2.5)
RT Sentence 80.1(4.3) 65.1(7.9) 57.1(9.6) 60.2(5.1)/46.3(6.4)
ADV 78.2 (5.3) 65.5(1.6) 53.8(4.89) 37.4(2.6)/19.9(10.6)
Cutoff Hidden  79.3(5.0) 66.6(1.4) 57.3(9.3) 60.5(8.3)/46.6(9.4)
Mixup 80.0 (6.52) 65.9(3.1) 57.8(4.19) 51.4(19.3)/39.8(3.2)
SR 69.6(29.3) 65.7(1.8) 51.4(9.4) 59.3(5.9)/43.1(11.9)
3 LM 68.5(13.7) 68.3(2.1) 53.2(6.3) 61.5(6.6)/46.4(4.4)
Ga RI Token 65.8(5.5) 66.7(1.1) 50.5(3.2) 61.4(11.3)/44.4(17.4)
Es RD 73.2(14.0) 66.1(3.3) 51.5(7.5) 59.3(7.1)/46.0(3.8)
a RS 71.6(16.6) 65.0(2.0) 51.1(7.1) 64.2(12.1)/46.7(11.5)
‘| WR 74.1(12.3) 69.3(2.5) 55.6(5.9) 60.4(7.5)/43.7(14.2)
vo
ae RT Sentence  82.1(8.2) 68.8(2.4) 59.8(3.9) 64.3(1.2)/49.8(1.9)
ADV Hidden 82.3(2.33) 66.8(5.9) 55.9(3.89) 62.2(10.8)/46.2(9.8)
Cutoff 79.9(5.5) 67.9(0.8) 60.1(1.0) 62.7(9.0)/48.1(3.2)

Table 2: Topic Classification and News Classification results with 10 examples. We report the average results across 3 different
random seeds with the 95% confidence interval and bold the best results.. For PubMed, we report the accuracy and F1 score.

are allowed to flow, x; be a labeled datapoint with
label y, x, be an unlabeled datapoint, and a(2) be
a data augmentation method. Then, a typical loss
function for consistency training is

CE(fo(x1),y) + AwCE(fg(u), fola(zu)))

where CE is the cross entropy loss and A,
is a tunable hyperparameter that determines the
weight of the consistency regularization term. In
practice, various other measures have been used
to minimize the difference between fj(7,) and
fo(a(xvu)), such as the KL divergence (Miyato
et al., 2018; Xie et al., 2020) and the mean-squared
error (Tarvainen and Valpola, 2017; Laine and
Aila, 2017; Berthelot et al., 2019). Because gra-
dients are not allowed to flow through the model
when it was fed the clean unlabeled input x,,, this
objective can be viewed as using the clean unla-
beled datapoint to generate a synthetic target dis-
tribution for the augmented unlabeled datapoint.
Xie et al. (2020) showed that consistency train-
ing can be effectively applied to semi-supervised
learning for NLP. To achieve stronger results, they

introduce several other tricks including confidence
thresholding, training signal annealing, and en-
tropy minimization. Confidence thresholding ap-
plies the unsupervised loss only when the model
assigns a class probability above a pre-defined
threshold. Training signal annealing prevents the
model from overfitting on easy examples by ap-
plying the supervised loss only when the model
is less confident about predictions. Entropy min-
imization trains the model to output low-entropy
(highly-confident) predictions when fed unlabeled
data. We refer the reader to (Xie et al., 2020) for
more details on these tricks.

4 Empirical Experiments

4.1 Datasets and Experiment Setup

To provide a quantitative comparison of the DA
methods we have surveyed, we experiment with 10
of the most commonly used and model-agnostic
augmentation techniques from different levels in
Table 1, including: (4) Token-level augmenta-
tion: Synonym Replacement (SR) (Kolomiyets


Methods Types Inference Paraphrase Single Sentence
MNLI  QNLI RTE QQP MRPC SST-2 CoLA
None 35.2(0.7) 51.8(7.0) 49.8.1) 63.9(99.1) 61.8(21.2) 60.5(13.1) 12.9(6.32)
SR 35.1(2.3) 51.4(7.2) 51.5(3.4) — 61.3(9.7)  59.7(26.3) 62.1(17.4)__—72.2(11.6)
LM 35.3(0.8) 51.0(8.0)  49.0(1.4) 62.411) 61.0(24.3) —62.8(9.8) 6.8(15.8)
ig RI Token 349(2:6) 51.5(8.4) 51.5014) 60.6(10.9) 60.6(25.0)  63.3(12.2) _7.8(7.42)
2 RD 35.5(2.1) 51.1(8.4) 50.9(2.4) 62.4(11.3) 61.2(22.0) 59.7(18.4) _—7.1(16.6)
5 RS 35.101.1)  51.5(7.0) 50.9(5.0) — 62.6(6.7) —63.2(22.5) —61.2(10.8) 5.2(17.0)
= WR 34.5(2.6) 52.0(3.8) 50.0(0.9) 60.6(10.2) 61.0(25.3) 61.8(12.5) 7.0(10.6)
a RT Sentence 35.3(0.5) 51.1(9.6) 50.8(4.4) 60.5(17.8) 61.8(23.7) —62.0(1.99) 8.37(8.35)
ADV 33.3(4.7) 49.7(1.8) 48.3012.1) 57.5(24.7) 61.5(21.5) 53.3(13.07) —_1.37(4.66)
Cutoff Hidden 35.1(2.3) 51.4(8.3) = 52.2(3.6) 62.6(8.8)  61.0(21.2)  63.5(8.45) 12.4(9.58)
Mixup 32.6(3.5) 49.9(1.4) 49.8(9.2)  63.0(0.3) 62.1(19.8) 62.3(12.3) 4.03(8.68)
SR 35.6(1.0) 52.1(4.5)  52.9(5.4) 53.5(10.7) 68.1(4.0) —61.8(37.9) —-6.65(5.69)
3 LM 35.0(3.3) 52.5(4.2) 50.2(6.5) 47.9(34.1) 68.4(3.8) — 57.3(14.2) 6.38(6.3)
2 RI Token 35.8(1.7) 52.1(4.1) 50.7(1.4) 59.6(5.1)  64.9(8.9) 58.3(14.8) 6.55(0.91)
Py RD 35.2(0.5) 52.1(5.2) 52.6(4.9)  56.1(16.0) 62.4(30.6) 55.7(16.4) 4.33(10.9)
3 RS 34.6(2.5) 52.1(6.2) 51.5(3.7) 49.8(7.9)  63.2(22.5) 55.2(15.3) _7.77(11.77)
r WR 34.8(2.5) 52.1(4.1) 50.911.8)  51.8(16.0) 63.1(23.5) 54.8(13.8) — 5.43(17.8)
2 RT Sentence 35.3(2.7) 52.7(4.8) 51.6(4.1)  63.9(7.5)  62.2(12.5)  61.9(20.8) _—-11.6(14.5)
ADV Hidden 36.2(8.9) 50.6(1.9) 50.9(6.8) 59.1(14.7) 63.9(9.1) 53.1(5.0) 7.64(25.1)
Cutoff 35.3(2.8) 52.5(4.3) 51.7(6.5) — 62.9(9.9) 68.6(4.4) 54.3(9.8) 4.11(11.8)

Table 3: GLUE results with 10 labeled examples per class. We report the average results across 3 different random seeds with

the 95% confidence interval and bold the best results.

et al., 2011; Yang, 2015), Word Replacement
based on Language Model (LM (Kumar et al.,
2020), Random Insertion (RI) (Wei and Zou,
2019; Miao et al., 2020), Random Deletion (RD)
(Wei and Zou, 2019), Random Swapping (RS)
(Wei and Zou, 2019), and Word Replacement
(WR) based on TF-IDF in Vocabulary Set (Xie
et al., 2020); (i) Sentence-level augmentation:
Roundtrip Translation (RT) (Xie et al., 2020;
Chen et al., 2020c); (ii1) Hidden-space Augmen-
tation: Adversarial training (ADV) (Goodfellow
et al., 2015), Cutoff (Shen et al., 2020), and
Mixup in the embedding space (Zhang et al.,
2018). Most aforementioned techniques are not
label-dependent (except mixup), thus can be ap-
plied directly to unlabeled data.

We test them on different types of benchmark
datasets including: (i) news classification tasks in-
cluding AG News (Zhang et al., 2015b) and 20
Newsgroup (Joachims, 1997); (11) topic classifica-
tion tasks including Yahoo Answers (Chang et al.,
2008) and PubMed news classification ((Zhang
et al., 2015b) (iii) inference tasks including MNLI,
QNLI and RTE (Wang et al., 2018); (iv) similarity
and paraphrase tasks including QQP and MRPC
(Wang et al., 2018); and (v) single-sentence tasks
including SST-2 and CoLA (Wang et al., 2018).

For all datasets, we experiment with 10 labeled

data points per class 7 in a supervised setup, and
an additional 5000 unlabeled data points per class
in the semi-supervised setup. We use BERT pase
(Devlin et al., 2019) as the base language model
and use the same hyper-parameters across all
datasets/methods. We utilize accuracy as the eval-
uation metric for all datasets except for CoLA
(which uses Matthews correlation) and PubMed
(which uses accuracy and Macro-F1 score). Be-
cause the performance can be heavily dependent
on the specific datapoints chosen (Sohn et al.,
2020), for each dataset, we sample labeled data
from the original dataset with 3 different seeds to
form different training sets, and report the average
result. For every setup, we fine-tune the model
with the same seed as the dataset seed (in contrast
to many works which report the max across dif-
ferent seeds). The detailed experimental setup is
described in the Appendix.

4.2 Results

News/Topic Classification Tasks. The results
are shown in Table 2. We observe that in su-
pervised settings, token-level augmentations work
the best. Specifically, word replacement works
well, getting the highest or second highest score

The results for 100 labeled data points per class are
shown in the Appendix.


every time; in the semi-supervised settings, sen-
tence level augmentations (round-trip translation)
works the best, getting the highest or second high-
est score every time. This makes sense since for
many classification tasks, multiple words indicate
the label, and so dropping several words will not
affect the label.

Inference Tasks. As shown in Table 3, we ob-
serve that token-level augmentations work the best
overall (e.g., random insertion, random deletion,
and word replacement) for both supervised and
semi-supervised settings. This is a bit surprising
since the inference tasks usually heavily depend
on several words, and changing these words can
easily change the label for inferene tasks.

Similarity and Paraphrase Tasks. From Ta-
ble 3, in the supervised settings, we observe
that token-level augmentations (random swap-
ping) achieve the best performances, while hid-
den space augmentations work well in semi-
supervised settings, with cutoff performing the
best on average. This makes sense since for para-
phrasing tasks, augmenting the text usually con-
sists of paraphrases, and so can easily change
whether two texts are paraphrases of each other.

Single Sentence Tasks. Based on the single-
sentence tasks results in Table 3, hidden space
augmentations (cutoff) provides the biggest boost
in performance in supervised settings, while in
semi-supervised settings, sentence level augmen-
tations (roundtrip translation) works best. We note
most augmentation methods hurt performance on
CoLA, a task for judging grammatical acceptabil-
ity. This could be caused by the fact that most
of augmentation methods try to preserve meaning
and not grammatical correctness.

Overall, no single augmentation works the
best for every task in the supervised or semi-
supervised setting. However, several overall con-
clusions can be made: first, augmentation does not
always improve performance, and can sometimes
hurt performances, even in the semi-supervised
setting. This suggests that we may need to design
different augmentations for different tasks. Sec-
ond, token-level augmentations (especially word
replacement and random swapping) work well in
general for supervised learning, especially when
there is extremely limited labeled data. Third,
round-trip translation usually works the best for
semi-supervised learning, showing the most con-

sistent gains. However, if the computation is lim-
ited, cutoff may be a better choice.

5 Other Limited Data Learning Methods

This work mainly focuses on data augmentation
and semi-supervised learning (consistency regu-
larization) in NLP; however, there are other or-
thogonal directions for tackling the problem of
learning with limited data. For completeness, we
summarize this related work below.

Low-Resourced Languages. Most languages
lack large monolingual or parallel corpora, or suf-
ficient manually-crafted linguistic resources for
building statistical NLP applications (Garrette and
Baldridge, 2013). Researchers have therefore
developed a variety of methods for improving
performance on low-resource languages, includ-
ing cross-lingual transfer learning which trans-
fers models from resource-rich to resource-poor
languages (Do and Gaspers, 2019; Lee and Lee,
2019; Schuster et al., 2019), few/zero-shot learn-
ing (Johnson et al., 2017; Blissett and Ji, 2019;
Pham et al., 2019; Abad et al., 2020) which uses
only a few examples from the low-resource do-
main to adapt models trained in another domain,
and polyglot learning (Cotterell and Heigold,
2017; Tsvetkov et al., 2016; Mulcaire et al.,
2019; Lample and Conneau, 2019) which com-
bines resource-rich and resource-poor learning us-
ing an universal language representation.

Other Methods for Semi-Supervised Learning.
Semi-supervised learning methods further reduce
the dependency on labeled data and enhance the
models when there is only limited labeled data
available. These methods use large amounts of
unlabeled data in the training process, as unla-
beled data is usually cheap and easy to obtain
compared to labeled data. In this paper, we fo-
cus on consistency regularization, while there are
also other widely-used methods for NLP including
self-training (Yarowsky, 1995; Zhang and Zong,
2016; He et al., 2020; Lin et al., 2020), genera-
tive methods (Xu et al., 2017; Yang et al., 2017;
Kingma et al., 2014; Cheng et al., 2016), and co-
training (Blum and Mitchell, 1998; Clark et al.,
2018; Cai and Lapata, 2019).

Few-shot Learning. Few-shot learning is a
broad technique for dealing with tasks with less
labeled data based on prior knowledge. Com-
pared to semi-supervised learning which utilizes


unlabeled data as additional information, few-shot
learning leverages various kinds of prior knowl-
edge such as pre-trained models or supervised
data from other domains and modalities (Wang
et al., 2020). While most work on few-shot fo-
cuses on computer vision, few-shot learning has
recently seen increasing adoption in NLP (Han
et al., 2018; Rios and Kavuluru, 2018; Hu et al.,
2018; Herbelot and Baroni, 2017). To better lever-
age pre-trained models, PET (Schick and Schiitze,
2021a,b) converts the text and label in an example
into a fluent sentence, and then uses the probabil-
ity of generating the label text as the class logit,
outperforming GPT3 for few shot learning (Brown
et al., 2020). How to better model and incorporate
prior knowledge to handle few-shot learning for
NLP remains an open challenge and has the poten-
tial to significantly improve model performance
with less labeled data.

6 Discussion and Future Directions

In this work, we empirically surveyed data aug-
mentation methods for limited-data learning in
NLP and compared them on 11 different NLP
tasks. Despite the success, there are still certain
challenges that need to be tackled for improve
their performance. This section highlights some
of these challenges and discusses future research
directions.

Theoretical Guarantees and Data Distribution
Shift. Current data augmentation methods for
text typically assume that they are label-preserving
and will not change the data distribution. How-
ever, these assumptions are often not true in prac-
tice, which can result in noisy labels or a shift
in the data distribution and consequently a de-
crease in performance or generalization (e.g., QQP
in Table 3). Thus, providing theoretical guaran-
tees that augmentations are label- and distribution-
preserving under certain conditions would ensure
the quality of augmented data and further acceler-
ate the progress of this field.

Automatic Data Augmentation. Despite being
effective, current data augmentation methods are
generally manually-designed. Methods for auto-
matically selecting the appropriate types of data
augmentation still remain under-investigated. Al-
though certain augmentation techniques have been
shown effective for a particular task or dataset,
they often do not transfer well to other datasets

or tasks (Cubuk et al., 2019), as shown in Ta-
ble 3. For example, paraphrasing works well for
general text classification tasks, but may fail for
some subtle scenarios like classifying bias because
paraphrasing might change the label in this set-
ting. Automatically learning data augmentation
strategies or searching for an optimal augmenta-
tion policy for given datasets/tasks/models could
enhance the generalizability of data augmentation
techniques (Maharana and Bansal, 2020).

Acknowledgments

We would like to thank the members of Geor-
gia Tech SALT and UNC-NLP groups for their
feedback. This work is supported by grants from
Amazon and Salesforce, ONR Grant N0O0014-18-
1-2871, DARPA YFA17-D17AP00022.

References

Alberto Abad, Peter Bell, Andrea Carmantini, and
Steve Renais. 2020. Cross lingual transfer learning
for zero-resource domain adaptation. ICASSP 2020
- 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP).

Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,
Amir Kantor, George Kour, Segev Shlomov, Naama
Tepper, and Naama Zwerdling. 2020. Do not have
enough data? deep learning to the rescue! In
The Thirty-Fourth AAAI Conference on Artificial In-
telligence, AAAI 2020, The Thirty-Second Innova-
tive Applications of Artificial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Ed-
ucational Advances in Artificial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020,
pages 7383-7390. AAAI Press.

Jacob Andreas. 2020. Good-enough compositional
data augmentation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7556-7566, Online. Association
for Computational Linguistics.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In International Conference on
Learning Representations.

Isabelle Augenstein, Sebastian Ruder, and Anders
Sgégaard. 2018. Multi-task learning of pairwise
sequence classification tasks over disparate label
spaces. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers).

Philip Bachman, Ouais Alsharif, and Doina Precup.
2014. Learning with pseudo-ensembles. In Ad-


vances in Neural Information Processing Systems,
volume 27. Curran Associates, Inc.

Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic
and natural noise both break neural machine transla-
tion.

David Berthelot, Nicholas Carlini, Ian Goodfellow,
Nicolas Papernot, Avital Oliver, and Colin A Raf-
fel. 2019. Mixmatch: A holistic approach to semi-
supervised learning. In Advances in Neural Infor-
mation Processing Systems, pages 5050-5060.

Kevin Blissett and Heng Ji. 2019. Zero-shot cross-
lingual name retrieval for low-resource languages.
In Proceedings of the 2nd Workshop on Deep Learn-
ing Approaches for Low-Resource NLP (DeepLo
2019), pages 275-280, Hong Kong, China. Associa-
tion for Computational Linguistics.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory, pages 92-100.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-
drew Dai, Rafal Jozefowicz, and Samy Bengio.
2016. Generating sentences from a continuous
space. In Proceedings of The 20th SIGNLL Con-
ference on Computational Natural Language Learn-
ing, pages 10-21, Berlin, Germany. Association for
Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey
Wu, Clemens Winter, Chris Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot
learners. In Advances in Neural Information Pro-
cessing Systems, volume 33, pages 1877-1901. Cur-
ran Associates, Inc.

Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng
Zhang, Xiaofang Zhao, and Dawei Yin. 2020. Data
manipulation: Towards effective instance learning
for neural dialogue generation via learning to aug-
ment and reweight. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 6334-6343, Online. Association
for Computational Linguistics.

Rui Cai and Mirella Lapata. 2019. Semi-supervised
semantic role labeling with cross-view training. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 1018-
1027, Hong Kong, China. Association for Computa-
tional Linguistics.

Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek
Srikumar. 2008. Importance of semantic representa-
tion: Dataless classification. In Proceedings of the
23rd National Conference on Artificial Intelligence
- Volume 2, AAAT’08, pages 830-835. AAAI Press.

O. Chapelle, B. Scholkopf, and Eds A. Zien. 2009.
Semi-supervised learning (chapelle, o. et al., eds.;
2006) [book reviews]. [EEE Transactions on Neu-
ral Networks, 20(3):542-542.

Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi
Yang. 2021. Hiddencut: Simple data augmentation
for natural language understanding with better gen-
eralization. In ACL.

Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang,
and Diyi Yang. 2020a. Local additivity based data
augmentation for semi-supervised NER. In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1241-1251, Online. Association for Compu-
tational Linguistics.

Jiaao Chen, Yuwei Wu, and Diyi Yang. 2020b. Semi-
supervised models via data augmentation for clas-
sifying interactive affective responses. In AffCon@
AAAI.

Jiaao Chen, Zichao Yang, and Diyi Yang. 2020c. Mix-
Text: Linguistically-informed interpolation of hid-
den space for semi-supervised text classification. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2147-—
2157, Online. Association for Computational Lin-
guistics.

Luoxin Chen, Weitong Ruan, Xinyue Liu, and Jianhua
Lu. 2020d. SeqVAT: Virtual adversarial training for
semi-supervised sequence labeling. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 8801-8811, On-
line. Association for Computational Linguistics.

Mingda Chen, Qingming Tang, Sam Wiseman, and
Kevin Gimpel. 2019. Controllable paraphrase gen-
eration with a syntactic exemplar. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 5972-5984.

Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn
Song, and Denny Zhou. 2020e. Compositional gen-
eralization via neural-symbolic stack machines. Ad-
vances in Neural Information Processing Systems,

33.

Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang,
and Cho-Jui Hsieh. 2020a. Seq2sick: Evaluat-
ing the robustness of sequence-to-sequence models
with adversarial examples. Proceedings of the AAAI
Conference on Artificial Intelligence, 34(04):3601-
3608.

Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.
Robust neural machine translation with doubly ad-
versarial inputs. Proceedings of the 57th Annual


Meeting of the Association for Computational Lin-
guistics.

Yong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-
cob Eisenstein. 2020b. AdvAug: Robust adversar-
ial augmentation for neural machine translation. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5961—
5970, Online. Association for Computational Lin-
guistics.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.

Kevin Clark, Minh-Thang Luong, Christopher D. Man-
ning, and Quoc Le. 2018. Semi-supervised se-
quence modeling with cross-view training. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1914—
1925, Brussels, Belgium. Association for Computa-
tional Linguistics.

Ryan Cotterell and Georg Heigold. 2017. Cross-
lingual character-level neural morphological tag-
ging. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 748-759, Copenhagen, Denmark. Asso-
ciation for Computational Linguistics.

Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay
Vasudevan, and Quoc V Le. 2019. Autoaugment:
Learning augmentation strategies from data. In Pro-
ceedings of the IEEE conference on computer vision
and pattern recognition, pages 113-123.

Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao,
Dong Yu, Frank Seide, Mike Seltzer, Geoff Zweig,
Xiaodong He, Jason Williams, Yifan Gong, and
Alex Acero. 2013. Recent advances in deep learning
for speech research at microsoft. In JEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).

Shumin Deng, Ningyu Zhang, Zhanlin Sun, Jiaoyan
Chen, and Huajun Chen. 2019. When low re-
source nlp meets unsupervised language model:
Meta-pretraining then meta-learning for few-shot
text classification. In Proceedings of the AAAI Con-
ference on Artificial Intelligence.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Terrance DeVries and Graham W. Taylor. 2017. Im-
proved regularization of convolutional neural net-
works with cutout.

Quynh Do and Judith Gaspers. 2019. Cross-lingual
transfer learning with data selection for large-scale
spoken language understanding. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 1455-1460, Hong Kong,
China. Association for Computational Linguistics.

Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018a.
On adversarial examples for character-level neural
machine translation.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018b. Hotflip: White-box adversarial exam-
ples for text classification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
31-36.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
scale. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 489-500, Brussels, Belgium. Association
for Computational Linguistics.

Marzieh Fadaee, Arianna Bisazza, and Christof Monz.
2017. Data augmentation for low-resource neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 567—
573, Vancouver, Canada. Association for Computa-
tional Linguistics.

Lisa Fan, Dong Yu, and L. Wang. 2018. Robust neu-
ral abstractive summarization systems and evalua-
tion against adversarial information. Interpretabil-
ity and Robustness for Audio, Speech and Language
Workshop at Neurips 2018.

Steven Y Feng, Varun Gangal, Dongyeop Kang,
Teruko Mitamura, and Eduard Hovy. 2020. Genaug:
Data augmentation for finetuning text generators. In
Proceedings of Deep Learning Inside Out (DeeLIO):
The First Workshop on Knowledge Extraction and
Integration for Deep Learning Architectures, pages
29-42.

Steven Y. Feng, Varun Gangal, Jason Wei, Chandar
Sarath, Soroush Vosoughi, Teruko Mitamura, and
Eduard Hovy. 2021. A survey of data augmenta-
tion approaches for nlp. In Association for Compu-
tational Linguistics Findings.

Daniel Furrer, Marc van Zee, Nathan Scales, and
Nathanael Scharli. 2020. Compositional generaliza-
tion in semantic parsing: Pre-training vs. specialized
architectures.

Fei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao Qin,
Xueqi Cheng, Wengang Zhou, and Tie-Yan Liu.
2019. Soft contextual data augmentation for neural
machine translation. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 5539-5544.


Siddhant Garg and Goutham Ramakrishnan. 2020.
BAE: BERT-based adversarial examples for text
classification. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 6174-6181, Online.
Association for Computational Linguistics.

Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 138-147, Atlanta, Georgia. Association for
Computational Linguistics.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Jnter-
national Conference of Machine Learning.

Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan,
Jason Wu, Stephan Zheng, Caiming Xiong, Mohit
Bansal, and Christopher Ré. 2021. Robustness gym:
Unifying the nlp evaluation landscape.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative
adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 27, pages 2672-2680. Curran Associates,
Inc.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adver-
sarial examples. stat, 1050:20.

Demi Guo, Yoon Kim, and Alexander Rush. 2020.
Sequence-level mixed sample data augmentation.
In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 5547-5552, Online. Association
for Computational Linguistics.

Ankush Gupta, Arvind Agarwal, Prawaan Singh, and
Piyush Rai. 2017. A deep generative framework for
paraphrase generation.

Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,
and Percy Liang. 2017. Generating sentences by
editing prototypes.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A
large-scale supervised few-shot relation classifica-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4803—
4809, Brussels, Belgium. Association for Computa-
tional Linguistics.

Junxian He, Jiatao Gu, Jiajun Shen, and Marc’ Aurelio
Ranzato. 2020. Revisiting self-training for neural
sequence generation. In International Conference
on Learning Representations.

Michael A. Hedderich, Lukas Lange, Heike Adel,
Jannik Strétgen, and Dietrich Klakow. 2020. A
survey on recent approaches for natural language
processing in low-resource scenarios. CoRR,
abs/2010.12309.

Aurélie Herbelot and Marco Baroni. 2017. High-risk
learning: acquiring new word vectors from tiny data.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
304—309, Copenhagen, Denmark. Association for
Computational Linguistics.

Wei-Ning Hsu, Hao Tang, and James Glass. 2018.
Unsupervised adaptation with interpretable disen-
tangled representations for distant conversational
speech recognition. Interspeech 2018.

Wei-Ning Hsu, Yu Zhang, and James Glass. 2017.
Unsupervised domain adaptation for robust speech
recognition via variational autoencoder-based data
augmentation. 2017 IEEE Automatic Speech Recog-
nition and Understanding Workshop (ASRU).

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Toward con-
trolled generation of text. In Proceedings of the
34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learning
Research, pages 1587-1596. PMLR.

Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and
Maosong Sun. 2018. Few-shot charge prediction
with discriminative legal attributes. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics, pages 487-498, Santa Fe, New
Mexico, USA. Association for Computational Lin-
guistics.

Aminul Huq and Mst. Tasnim Pervin. 2020. Adversar-
ial attacks and defense on texts: A survey.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé IIT. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1681-1691, Beijing, China. Association for Com-
putational Linguistics.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume I (Long Papers), pages 1875-1885.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12-22, Berlin, Germany. Association for Computa-
tional Linguistics.


Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2021-2031, Copenhagen, Denmark. Association for
Computational Linguistics.

Haoming Jiang, Pengcheng He, Weizhu Chen, Xi-
aodong Liu, Jianfeng Gao, and Tuo Zhao. 2019.
Smart: Robust and efficient fine-tuning for pre-
trained natural language models through principled
regularized optimization.

Yichen Jiang and Mohit Bansal. 2019. Avoiding rea-
soning shortcuts: Adversarial evaluation, training,
and model development for multi-hop QA. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2726-
2736, Florence, Italy. Association for Computa-
tional Linguistics.

Thorsten Joachims. 1997. A probabilistic analysis of
the rocchio algorithm with tfidf for text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ’97, page
143-151, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339-351.

Durk P Kingma, Shakir Mohamed, Danilo
Jimenez Rezende, and Max Welling. 2014. Semi-
supervised learning with deep generative models.
In Advances in Neural Information Processing
Systems, volume 27. Curran Associates, Inc.

Sosuke Kobayashi. 2018. Contextual augmentation:
Data augmentation by words with paradigmatic re-
lations. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 452-457,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2011. Model-portability experi-
ments for textual temporal analysis. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 271-276, Portland, Oregon, USA.
Association for Computational Linguistics.

Alex Krizhevsky, I. Sutskever, and G. Hinton. 2012.
Imagenet classification with deep convolutional neu-
ral networks. Advances in neural information pro-
cessing systems, 25(2).

Ashutosh Kumar, Satwik Bhattamishra, Manik Bhan-
dari, and Partha Talukdar. 2019. Submodular
optimization-based diverse paraphrasing and its ef-
fectiveness in data augmentation. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume I (Long
and Short Papers), pages 3609-3619, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.

Varun Kumar, Ashutosh Choudhary, and Eunah Cho.
2020. Data augmentation using pre-trained trans-
former models. In Proceedings of the 2nd Work-
shop on Life-long Learning for Spoken Language
Systems, pages 18-26, Suzhou, China. Association
for Computational Linguistics.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In 5th Inter-
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. In Ad-
vances in Neural Information Processing Systems 32
(NeurIPS 2019).

Guillaume Lample, Alexis Conneau, Ludovic De-
noyer, and Marc’ Aurelio Ranzato. 2018. Unsuper-
vised machine translation using monolingual cor-
pora only. In International Conference on Learning
Representations.

Chia-Hsuan Lee and Hung-Yi Lee. 2019. Cross-
lingual transfer learning for question answering. In
arXiv.

Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan
Moreno, Xiao Huang, Prashant Shiralkar, and Xiang
Ren. 2020. TriggerNER: Learning with entity trig-
gers as explanations for named entity recognition. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 8503—
8511, Online. Association for Computational Lin-
guistics.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume I: Long Papers), pages 1-10, Vancouver,
Canada. Association for Computational Linguistics.

Adyasha Maharana and Mohit Bansal. 2020. Adver-
sarial augmentation policy search for domain and
cross-lingual generalization in reading comprehen-
sion. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Online. Association for Computational
Linguistics.

Nikolaos Malandrakis, Minmin Shen, Anuj Goyal,
Shuyang Gao, Abhishek Sethi, and Angeliki Met-
allinou. 2019. Controlled text generation for data


augmentation in intelligent artificial agents. In Pro-
ceedings of the 3rd Workshop on Neural Generation
and Translation, pages 90-98, Hong Kong. Associ-
ation for Computational Linguistics.

R. Thomas McCoy, Ellie Pavlick, and Tal Linzen.
2019. Right for the wrong reasons: Diagnosing syn-
tactic heuristics in natural language inference.

Zhengjie Miao, Yuliang Li, Xiaolan Wang, and Wang-
Chiew Tan. 2020. Snippext: Semi-supervised opin-
ion mining with augmented data. In Proceedings of
The Web Conference 2020, pages 617-628.

Junghyun Min, R. Thomas McCoy, Dipanjan Das,
Emily Pitler, and Tal Linzen. 2020. Syntactic
data augmentation increases robustness to inference
heuristics.

Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-
jes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.
2021. Deep learning based text classification: A
comprehensive review.

Takeru Miyato, Andrew M. Dai, and Ian Goodfel-
low. 2017. Adversarial training methods for semi-
supervised text classification. International Confer-
ence on Learning Representations (ICLR).

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama,
and Shin Ishii. 2018. Virtual adversarial training:
a regularization method for supervised and semi-
supervised learning. JEEE transactions on pat-
tern analysis and machine intelligence, 41(8):1979-
1993.

John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,
Di Jin, and Yanjun Qi. 2020. TextAttack: A frame-
work for adversarial attacks, data augmentation, and
adversarial training in NLP. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations,
pages 119-126, Online. Association for Computa-
tional Linguistics.

Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith.
2019. Low-resource parsing with crosslingual con-
textualized representations. In Proceedings of the
23rd Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 304-315, Hong
Kong, China. Association for Computational Lin-
guistics.

Tong Niu and Mohit Bansal. 2018. Adversarial over-
sensitivity and over-stability strategies for dialogue
models. In The SIGNLL Conference on Computa-
tional Natural Language Learning (CoNLL).

Tong Niu and Mohit Bansal. 2019. Automatically
learning data augmentation policies for dialogue
tasks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),
pages 1317-1323, Hong Kong, China. Association
for Computational Linguistics.

Maxwell I. Nye, Armando Solar-Lezama, Joshua B.
Tenenbaum, and Brenden M. Lake. 2020. Learning
compositional rules via neural program synthesis.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227-2237.

Ngoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and
Alexander Waibel. 2019. Improving zero-shot trans-
lation with language-independent constraints. In
Proceedings of the Fourth Conference on Machine
Translation (Volume 1: Research Papers), pages 13-
23, Florence, Italy. Association for Computational
Linguistics.

Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek
Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.
2016. Neural paraphrase generation with stacked
residual Istm networks.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-

former. Journal of Machine Learning Research,
21(140): 1-67.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial ex-
amples through probability weighted word saliency.
In Proceedings of the 57th annual meeting of the as-
sociation for computational linguistics, pages 1085—
1097.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging NLP models. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 856-865, Melbourne, Australia. Asso-
ciation for Computational Linguistics.

Anthony Rios and Ramakanth Kavuluru. 2018. Few-
shot and zero-shot multi-label learning for structured
label spaces. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 3132-3142, Brussels, Belgium. As-
sociation for Computational Linguistics.

Timo Schick and Hinrich Schiitze. 2021a. Exploiting
cloze-questions for few-shot text classification and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 255-269, Online. Association for Com-
putational Linguistics.


Timo Schick and Hinrich Schiitze. 2021b. It’s not just
size that matters: Small language models are also
few-shot learners. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 2339-2352, Online. As-
sociation for Computational Linguistics.

Sebastian Schuster, Sonal Gupta, Rushin Shah, and
Mike Lewis. 2019. Cross-lingual transfer learning
for multilingual task oriented dialog. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 3795-3805, Min-
neapolis, Minnesota. Association for Computational
Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Improving neural machine translation models
with monolingual data. Computer Science.

Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and
Kristina Toutanova. 2020. Compositional general-
ization and natural language variation: Can a seman-
tic parsing approach handle both?

Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru
Qu, and Weizhu Chen. 2020. A simple but tough-
to-beat data augmentation approach for natural lan-
guage understanding and generation. arXiv preprint
arXiv:2009. 13818.

Sam Shleifer. 2019. Low resource text classification
with ulmfit and backtranslation.

Patrice Y. Simard, David Steinkraus, and John C. Platt.
2003. Best practices for convolutional neural net-
works applied to visual document analysis. Seventh
International Conference on Document Analysis and
Recognition, 2003. Proceedings., pages 958-963.

Kihyuk Sohn, David Berthelot, Nicholas Carlini,
Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-
gus Cubuk, Alexey Kurakin, and Chun-Liang Li.
2020.  Fixmatch: Simplifying semi-supervised
learning with consistency and confidence. Advances
in Neural Information Processing Systems, 33.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th Interna-
tional Conference on Neural Information Processing
Systems-Volume 2, pages 3104-3112.

Samson Tan, Shafiq Joty, Min-Yen Kan, and Richard
Socher. 2020. It’s morphin’ time! Combating
linguistic discrimination with inflectional perturba-
tions. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics,
pages 2920-2935, Online. Association for Compu-
tational Linguistics.

Antti Tarvainen and Harri Valpola. 2017. Mean teach-
ers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning

results. In Advances in neural information process-
ing systems, pages 1195-1204.

Luke Taylor and Geoff Nitschke. 2018. Improving
deep learning with generic data augmentation. In
2018 IEEE Symposium Series on Computational In-
telligence (SSCI), pages 1542-1547.

Harsh Trivedi, Niranjan Balasubramanian, Tushar
Khot, and Ashish Sabharwal. 2020. Is multihop QA
in DiRe condition? measuring and reducing dis-
connected reasoning. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 8846-8863, On-
line. Association for Computational Linguistics.

Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,
Guillaume Lample, Patrick Littell, David
Mortensen, Alan W Black, Lori Levin, and
Chris Dyer. 2016. Polyglot neural language
models: A case study in cross-lingual phonetic
representation learning. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1357-1366, San
Diego, California. Association for Computational
Linguistics.

Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman. 2018.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Pro-
ceedings of the 2018 EMNLP Workshop Black-
boxNLP: Analyzing and Interpreting Neural Net-
works for NLP, pages 353-355, Brussels, Belgium.
Association for Computational Linguistics.

Yaqing Wang, Quanming Yao, James T. Kwok, and Li-
onel M. Ni. 2020. Generalizing from a few exam-
ples. ACM Computing Surveys, 53(3):1-34.

Yicheng Wang and Mohit Bansal. 2018. Robust ma-
chine comprehension models via adversarial train-
ing. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 575-581,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

Jason Wei and Kai Zou. 2019. EDA: Easy data aug-
mentation techniques for boosting performance on
text classification tasks. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 6382-6388, Hong Kong,
China. Association for Computational Linguistics.

Xing Wu, Shangwen Ly, Liangjun Zang, Jizhong Han,
and Songlin Hu. 2019a. Conditional bert contextual
augmentation. In International Conference on Com-
putational Science, pages 84-95. Springer.


Zhanghao Wu, Shuai Wang, Yanmin Qian, and Kai Yu.
2019b. Data Augmentation Using Variational Au-
toencoder for Embedding Based Speaker Verifica-
tion. In Proc. Interspeech 2019, pages 1163-1167.

Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,
and Quoc Le. 2020. Unsupervised data augmenta-
tion for consistency training. Advances in Neural
Information Processing Systems, 33.

Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Lévy, Aim-
ing Nie, Dan Jurafsky, and Andrew Y. Ng. 2017.
Data noising as smoothing in neural network lan-
guage models. CoRR, abs/1703.02573.

Jingjing Xu, Xuancheng Ren, Junyang Lin, and
Xu Sun. 2018. Dp-gan: Diversity-promoting gener-
ative adversarial network for generating informative
and diversified text.

Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan.
2017. Variational autoencoder for semi-supervised
text classification. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 31.

Diyi Yang, William Yang Wang. 2015. That’s so an-
noying!!!: A lexical and frame-semantic embedding
based data augmentation approach to automatic cat-
egorization of annoying behaviors using #petpeeve
tweets. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-

ing.

Yiben Yang, Chaitanya Malaviya, Jared Fernandez,
Swabha Swayamdipta, Ronan Le Bras, Ji-Ping
Wang, Chandra Bhagavatula, Yejin Choi, and Doug
Downey. 2020. Generative data augmentation for
commonsense reasoning. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020,
pages 1008-1025, Online. Association for Compu-
tational Linguistics.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. In Proceedings of the 34th Interna-
tional Conference on Machine Learning - Volume

70, ICML’ 17, page 3881-3890. JMLR.org.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In 33rd
annual meeting of the association for computational
linguistics, pages 189-196.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V.
Le. 2018. Qanet: Combining local convolution
with global self-attention for reading comprehen-
sion. CoRR, abs/1804.09541.

Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji
Zhang, Bairu Hou, Yuan Zang, Zhiyuan Liu, and
Maosong Sun. 2020. Openattack: An open-source
textual adversarial attack toolkit.

Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin,
and David Lopez-Paz. 2018. mixup: Beyond empir-
ical risk minimization. In International Conference
on Learning Representations.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1535-1545, Austin, Texas. Associ-
ation for Computational Linguistics.

Rongzhi Zhang, Yue Yu, and Chao Zhang. 2020a. Se-
qMix: Augmenting active sequence labeling via se-
quence mixup. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 8566-8579, Online.
Association for Computational Linguistics.

Shiyue Zhang and Mohit Bansal. 2019. Address-
ing semantic drift in question generation for semi-
supervised question answering. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 2495-2509, Hong Kong,
China. Association for Computational Linguistics.

Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi,
and Chenliang Li. 2020b. Adversarial attacks on
deep-learning models in natural language process-
ing: A survey. ACM Transactions on Intelligent Sys-
tems and Technology (TIST), 11(3):1-41.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015a.
Character-level convolutional networks for text clas-
sification. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems - Volume I, NIPS’15, page 649-657, Cam-
bridge, MA, USA. MIT Press.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015b.
Character-level convolutional networks for text clas-
sification. Advances in Neural Information Process-
ing Systems, 28:649-657.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2017.
Generating natural adversarial examples.

Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li,
and Yi Yang. 2020. Random erasing data augmenta-
tion. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence, volume 34, pages 13001-13008.

Chen Zhu, Yu Cheng, Zhe Gan, Sigi Sun, Tom Gold-
stein, and Jingjing Liu. 2020. Freelb: Enhanced
adversarial training for natural language understand-
ing. In CLR.

Xiaojin Jerry Zhu. 2005. Semi-supervised learning
literature survey. Technical report, University of
Wisconsin-Madison Department of Computer Sci-
ences.


A Experimental Setup

We train our models on NVIDIA 2080ti and
NVIDIA V-100 gpus. Supervised experiments
take 20 minutes, and semi-supervised experiments
take two hours. The BERT-base model has 100M
parameters. We use the same hyperaparameter
across all datasets, and so only use the validation
set to find the best model checkpoint. We use a
learning rate of 2e—°, batch size of 16, ratio of un-
labeled to labeled data of 3, and dropout ratio of
0.1 for different augmentation methods.

B_ Results for 100 Labeled Data per Class

News/Topic Classification Tasks The results
are shown in Table 4. We observe that overall, in
both the supervised settings and semi-supervised
setting, all the methods perofrmly similarly, with
2 points of each other. This indicates that data aug-
mentation methods work well with limited labeled
data, and with more labeled data, its effectiveness
is removed.

Inference Tasks As shown in Table 5, we ob-
serve that most augmentation methods hurt the
performance in both the supervised and semi-
supervised setting, with a greater drop in perfor-
mance in the semi-supervised setting.

Similarity and Paraphrase Tasks Similar to
inference tasks, we observe in Table 5 that most
augmentation methods hurt the performance in
both the supervised and semi-supervised setting,
with a greater drop in performance in the semi-
supervised setting.

Single Sentence Tasks Unlike inference tasks
and paraphrase tasks, augmentations methods
help performance, as seen in Table 5, except for
CoLA. We hypothesize the reason is because most
augmentatiom methods seek to preserves mean-
ing, not grammatical correctness, which is what
CoLA measures. In the supervised and semi-
supervised setting, hidden level augmentations
work well, with cutoff performing the best.

C_ Case Study

We analyze several data augmentation methods
and check whether the label is preserved for these
and if this affects its performance. We look at 25
examples for the best performing data augmenta-
tion method and the worst performing data aug-
mentation method for 20 News Group and RTE.

For 20 News Group, Random Deletion was the
best performing, and Language Model was the
worst performing. In both cases, there were no
examples where the label flipped, which makes
sense since the input is usually several paragraphs
with multiple references to the topic. Several ex-
amples are shown in Appendix. For RTE, Lan-
guage Model was the worst performing and Cut-
off was the best performing augmentation. Lan-
guage Model flipped 24% of the labels with 4%
uncertain, while Cutoff flipped 4% of the labels
with 12% uncertain. We show several examples of
when the label flipped for RTE in the Table 6.


Types

News Classification

Topic Classification

Methods
- AG News 20Newsgroup Yahoo Answers PubMed

None - 87.9(1.05) 79.5(0.3) 68.6(0.71) 75.2(1.5)/59.5(2.0)

SR 88.5(0.87) 80.0(2.2) 69.7(1.62) 76.5(1.0)/60.7(0.7)

LM 88.1(1.00) 80.5(1.8) 68.8(3.2) 75.8(2.5)/59.9(1.7)

3 RI Token 88.0(2.08) 80.1(3.1) 69.1(1.68) 76.2(2.9)/60.3(1.7)
FS RD 88.1(0.84) 80.2(2.9) 68.7(2.2) 76.9(0.6)/60.9(0.6)
x RS 88.4(0.97) 79.5(2.1) 69.0(2.03) 76.6(0.2)/60.6(0.7)
Ps WR 87.9(1.19) 79.3(2.5) 69.4(5.89) 76.4(1.8)/60.4(1.6)
RT Sentence 88.3(0.17) 80.4(0.7) 68.8(1.88) 76.1(0.5)/60.3(0.5)

ADV 87.6(0.33) 78.5(1.4) 67.4(0.74) 75.6(4.0)/59.8(3.5)
Cutoff Hidden  88.3(0.38) 79.8(1.0) 68.7(0.47) 75.9(1.3)/60.1(0.7)
Mixup 88.6(1.31) 80.5(3.4) 68.27(1.76) 74.8(1.8)/59.2(0.2)

SR 88.8(0.95) 81.2(8.4) 68.8(1.3) 76.6(1.5)/60.7(1.8)

3 LM 88.4(1.87) 81.4(1.0) 68.8(1.8) 76.4(1.3)/60.4(0.7)
5 RI Token 88.4(1.45) 80.3(3.0) 68.4(2.64) 76.8(1.2)/60.7(1.1)
5 RD 88.7(0.5) 80.5(0.8) 68.8(1.66) 77.1(1.0)/61.2(1.5)
ZR RS 88.5(1.35) 80.9(2.2) 68.7(1.67) 76.9(1.7)/61.0(1.5)
‘e WR 87.7(1.35) 81.5(1.3) 68.7(1.2) 76.5(0.5)/60.6(1.0)
a RT Sentence 88.7(0.40) 81.7(1.0) 69.7(1.06) 77.0(1.2)/61.6(1.1)
ADV Hidden 88.0(1.04) 80.4(2.9) 68.9(1.74) 76.7(1.5)/60.9(1.2)
Cutoff 88.9(0.25) 81.3(4.6) 69.3(1.76) 76.7(2.1)/60.7(.1)

Table 4: Topic Classification and News Classification results with 100 examples. We report the average results across 3
different random seeds with the 95% confidence interval and bold the best results.. For PubMed, we report the accuracy and
F1 score.

Methods ‘Types Inference Paraphrase Single Sentence
MNLI QNLI RTE QQP MRPC SST-2 CoLA

None 45.0(6.9) 63.2(10.7) 59.9(3.1) —-71.0(2.6) — 68.1(7.4)_-82.7(4.0) 8.79.5)

SR 44.6(7.2)  62.999.4) 61.0(10.0) 68.9(2.2) 66.7(4.4) 84.001.9)  24.6(5.1)

LM 45.4(6.2) 60.6(7.7) 61.5(9.1) —-69.6(1.7)  67.2(2.8) __83.8(3.1) ——:18.5(9.7)

~ RI Token  45:8(7-5) 64.2(10.7) 60.0(11.3)  69.2(0.6)  69.1(4.8)  84.3(1.4) _ 27.3(19.9)
3 RD 43.7(8.4)  63.6(9.4)  59.2(9.0) 69.2(1.5) —69.2(5.5) — 82.3(2.05) = 20.2(21.5)
5 RS 42.4(6.2)  63.3(9.1) 57.8(11.9) 68.301.6)  69.0(3.4) = 82.5(5.0) = 24.3(20.8)
= WR 44.6(6.3)  61.6(8.8)  57.8(9.3)  66.7(11.8) 66.9(6.4)  83.5(1.9) —_:17.7(23.3)
* RT Sentence 44.8(7.8)  59.0(7.6) 60.4(5.7)  69.9(4.0)  69.6(1.6)  84.3(3.27) 19.2(7.63)
ADV 39.1(10.9) 50.1(3.1) = 57.3(8.7) ~—63.701.9) 68.7 (6.3) —-69.8(5.3) 16.5(9.2)
Cutoff Hidden 44.9(5.5)  63.0(10.2) 59.3(8.8) 69.9(0.7) 66.5(1.3) —84.7(0.9) —_26.0(16.3)
Mixup 35.7(7.3) 51.4(4.4) 60.5(6.52) 64.5(5.4)  67.9(7.1) 83.5(3.4) _ 20.1(18.8)

SR 42.9(7.3) 60.1(6.2) = 58.5(9.7) —65.0(6.0) = 67.6(3.1) —-85.1(3.5) 18.9(6.7)

3 LM 43.7(4.5)  60.9(10.4) 56.9(8.3) 59.3(12.0) 70.0(4.4) 83.9(4.1) —_21.7(6.8)
< RI Teken 44.7(4.6)  62.5(10.5) 56.0(6.3) 68.3(0.1) = 67.0(3.9) = 84.2(3.0) —_23.0(10.3)
Py RD 41.4.9) 59.4(6.4) 56(0.0) 69.3(2.8) 70.4(7.4) = 83.6(2.3) 13.1(6.1)
3 RS 40.3(2.0)  60.3(8.7) 56.4(11.6) 66.8(2.3)  69.0(3.4)  84.5(3.6) 19.4(2.7)
r- WR 43.9(3.1)  60.5(8.8)  56.3(7.1) —65.4(4.3) = 67.2(2.1) 3.34.5) 16.9(6.2)
BD RT Sentence 45.4(7.7)  63.8(5.0) 59.9(9.1) = 68.3(2.9) —-67.5(0.7) —- 83.9(1.7) —.20.4(3.6)
ADV Hidden 44.1(3.4) 58.1(4.0)  58.6(5.2) 63.0(10.8) 67.6(5.2) — 80.0(7.3) 13.5(7.8)
Cutoff 42.7(4.2) 60.3(7.4) 57.9(12.6)  67.2(4.4) 71.4(2.0) —82.5(5.4) —-23.9(2.7)

Table 5: GLUE results with 100 labeled examples per class. We report the average results across 3 different random seeds
with the 95% confidence interval and bold the best results.


Original Cutoff (Best) Language Model (Worst)
Sentence 1: The Walt Dis- Sentence 1: The Walt Dis- Sentence 1: The Walt Disney
ney Co. donated one of the ney Co. donated one of the Co. donated one of the world’s

world’s most significant private
collections of African artwork,
yesterday, to the Smithsonian’s
National Museum of African
Art.

Sentence 2: Disney gave the
Smithsonian a trove of sought-
after African art.

world’s most significant private
collections of African artwork,
yesterday, to the Smithsonian’s
National Museum of African
one

Sentence 2: Disney gave the
Smithsonian a trove of south
African art.

most significant private collec-
tions of African artwork [PAD]
[PAD] [PAD] to the Smith-
sonian’s National Museum of
African Att.

Sentence 2: Disney gave the
Smithsonian a trove of [PAD]
African art.

Entailment

Entailment

Not Entailment

Sentence 1: An explosion, fol-
lowed by a raging fire, demol-
ished a plastics factory, killing
at least three people and injur-
ing at least 37.

Sentence 2: A massive blast at
a plastics factory killed at least
two people.

Sentence 1: An explosion, fol-
lowed by a raging fire, demol-
ished a the factory, killing at
least three people and injuring
at least 37.

Sentence 2: A massive blast at
a plastics factory killed at shot
two people.

Sentence 1: An explosion, fol-
lowed by [PAD] [PAD] fire,
demolished a plastics factory,
killing at least three people and
injuring at least 37.

Sentence 2: A massive blast at
a plastics [PAD] killed at least
two people.

Entailment

Entailment

Not Entailment

Sentence |: The prize is named
after Alfred Nobel, a pacifist
and entrepreneur who invented
dynamite in 1866. Nobel left
much of his wealth to estab-
lish the award, which has hon-
oured achievements in physics,
chemistry, medicine, literature
and efforts to promote peace
since 1901.

Setence 1: The prize is named
after Alfred Nobel, a pacifist
and entrepreneur who invented
dynamite in 1866. Nobel left
much of his wealth to estab-
lish the nobel which has hon-
oured achievements in physics,
chemistry, medicine, literature
and efforts to promote peace
since 1901.

The prize is named after Al-
fred Nobel, a pacifist and en-
trepreneur who invented dyna-
mite in 1866 . Nobel left much
of his wealth [PAD] [PAD]
[PAD] [PAD], which has hon-
oured achievements in physics,
chemistry, medicine, literature
and efforts to promote peace
since 1901.

Sentence 2: Alfred Nobel in-
vented dynamite in 1866.

Sentence 2: Alfred Nobel in-
vented dynamite in 1866.

Sentence 2: Alfred Nobel in-
vented dynamite in 1866.

Entailment

Table 6: Examples of different data augmentation methods on RTE and whether they preserve the original label or

not

Entailment

Not Entailment
