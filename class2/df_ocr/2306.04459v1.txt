2306.04459v1 [cs.CL] 5 Jun 2023

arXiv

Uncertainty in Natural Language Processing:
Sources, Quantification, and Applications

Mengting Hu, Zhen Zhang, Shiwan Zhao, Minlie Huang and Bingzhe Wu

Abstract—As a main field of artificial intelligence, natural language processing (NLP) has achieved remarkable success via deep
neural networks. Plenty of NLP tasks have been addressed in a unified manner, with various tasks being associated with each other
through sharing the same paradigm. However, neural networks are black boxes and rely on probability computation. Making mistakes
is inevitable. Therefore, estimating the reliability and trustworthiness (in other words, uncertainty) of neural networks becomes a key
research direction, which plays a crucial role in reducing models’ risks and making better decisions. Therefore, in this survey, we
provide a comprehensive review of uncertainty-relevant works in the NLP field. Considering the data and paradigms characteristics, we
first categorize the sources of uncertainty in natural language into three types, including input, system, and output. Then, we
systemically review uncertainty quantification approaches and the main applications. Finally, we discuss the challenges of uncertainty
estimation in NLP and discuss potential future directions, taking into account recent trends in the field. Though there have been a few
surveys about uncertainty estimation, our work is the first to review uncertainty from the NLP perspective.

Index Terms—Natural Language Processing, Uncertainty Estimation, Pre-trained Language Models

1. INTRODUCTION

ATURAL Language Processing (NLP) is a multidisci-
N binasy field that encompasses computer science, arti-
ficial intelligence, and linguistics. Its aim is to develop ma-
chines that understand natural language and allow humans
to interact with it using natural language. Benefiting from
the development of deep neural networks (DNNs), NLP
technology has a wide range of applications, including sen-
timent analysis (SA), machine translation (MT), question-
answering (QA) systems, etc. With the development of pre-
trained language models (PLMs) [1], plenty of NLP tasks
can be tackled in a similar manner by sharing the same
paradigm. That is to say, various tasks can be simply formu-
lated into a classification, regression, generation, etc., prob-
lem. A unified trend is being observed in the development
of NLP field, which enables the efficient utilization of PLMs
and promotes knowledge transfer across tasks.

Though NLP field has achieved great success, it relies
heavily on neural networks. Such a technique is a black
box and depends on probability computation. There must
be probable to make mistakes, which is inevitable. Thus,
estimating the uncertainty of neural networks becomes a
crucial research direction, which measures the reliability
and trustworthiness of models. Especially in safety-critical
applications like autonomous systems or medical diagnosis,
uncertainties in predictions can have severe consequences
if not appropriately addressed. Here, we demonstrate the
importance of uncertainty through an example in the medi-

e Mengting Hu and Zhen Zhang are with the College of Software, Nankai
University. E-mail: mthu@nankai.edu.cn, zhangz@mail.nankai.edu.cn

e = =Shiwan Zhao is an independent researcher. E-mail: zhaosw@gmail.com

e Minlie Huang is with the Department of Computer Science, Tsinghua
University. E-mail: aihuang@tsinghua.edu.cn

e Bingzhe Wu is with Tencent AI Lab. E-mail: bingzhewu@tencent.com

This work has been submitted to the IEEE for possible publication. Copyright
may be transferred without notice, after which this version may no longer be
accessible.

Sa

© User Input: Decision: Can I take Xanax for anxiety?

nn

1] NLP System with uncertainty

Generation Answer: Yes, you can take Xanax for anxiety. (Uncertainty: 90%)

Information: "Domain"(Medical, Uncertainty: 3%) ; "Xanax" (Medicine,
ncertainty: 5%); "Anxiety" (Symptom, Uncertainty: 13%)

. ’

a ae

‘
‘

t

i

t

'

'

' e . aoe

' Action: abstain answer/human decision
i

'

i

t

i

'

I

@ Output Predetermined Answers Based Information: "I don't know, for |
symptoms and medication instructions it is important to consult a healthcare

' provider for appropriate recommendations."

Fig. 1. An illustration of NLP systems applied in the medical domain.

cal domain. As depicted in Fig. 1, a user inquires about the
potential use of the addictive drug “Xanax” for anxiety, the
system fails to differentiate the intensity of anxiety symp-
toms, resulting in a direct affirmation of its usability. This is
clearly deemed inappropriate for patients, considering the
grave side effects associated with the drug “Xanax”. How-
ever, incorporating a probability of uncertainty would aid in
determining whether to abstain from the response or defer
to the expertise of professionals for a final decision. Some
studies have shown that accurately quantifying uncertainty
can help identify situations where the model is uncertain,
thereby improving the reliability and interpretability of the
output [2, 3, 4].

Considering the unified trends of NLP and the essen-
tiality of uncertainty, a systematic review of uncertainty-
relevant NLP and the corresponding solutions is still lack-
ing, which we aim to fill in this survey. Initially, we arise
a research question: what are the sources of uncertainty
in the NLP field? Here, we naturally connect uncertainty
with multiple stages of an NLP system and summarize


the sources into input, system, and output. More concretely,
natural language input to NLP system is inherently ambigu-
ous and context-dependent, making it difficult to achieve
perfect performance and reliability in many NLP tasks [5,
6, 7]. Then, the system, such as the network initialization,
architecture, and interior computation, etc., could introduce
randomness, leading to uncertainty. Lastly, the uncertainties
of outputs are due to complicating factors, such as a lack
of broader knowledge, unreasonable model parameters, or
specification of task output.

In light of the in-depth analysis of uncertainty sources,
we further review the literature about the uncertainty quan-
tification. Uncertainty estimation is ubiquitous in DNNs,
and a common example is represented by confidence of the
network output. Specifically, the Softmax scores obtained
from these networks provide a direct means of estimating
confidence values that are easily converted to uncertainty
scores, e.g. subtracting the confidence from 1. However, the
predictions are typically represented as point estimates [8,
9], and mistakes are inevitable. Guo et al. [10] point out that
DNNs tend to exhibit excessive confidence, making their
confidence scores inaccurate. This is problematic because
under-/over-confident in the network can lead to wrong
decisions and actions based on overconfident predictions. To
address this challenge, researchers have developed various
uncertainty estimation methods for DNNs. This survey cat-
egorizes uncertainty estimation methods into three groups
based on different modeling approaches: (1) calibration con-
fidence-based methods, (2) sampling-based methods, and
(3) distribution-based methods. These methods go beyond
traditional confidence level measurements to quantify the
uncertainty in model predictions, identify situations where
NLP systems are uncertain about outputs, and offer insights
into model behavior. Throughout the survey, we discuss
the features and challenges associated with each of these
estimation methods.

Recently, the applications of uncertainty estimation in
NLP are diverse and increasing. In this survey, we mainly
divide the applications into three categories, including (1)
data filtering and action guidance, which is to select the re-
quired data according to the uncertainty and take the next
action. In typical active learning, filter the data through the
uncertain estimation of the teacher model, and guide the
student network to train; another example is the detection
of out-of-distribution data. (2) Improve the performance and
efficiency of the system based on uncertainty, (3) Output qual-
ity assessment, such as the quality assessment of machine
translation and understanding whether the answer of the
QA system is trustworthy.

The challenges of uncertainty estimation in applications
depend on the problem setting, including the paradigm of
the NLP model and the type of task (e.g., classification,
generation, regression, etc.). When observing the challenges
faced by the three applications mentioned above, we consid-
ered different paradigms for classification in deterministic
estimation methods in this survey. Recently, with the emer-
gence of large pre-trained models such as T5 [11] and the
GPT family [12, 13, 14], which can capture linguistic patterns
and knowledge from massive text data. The capabilities of
NLP models in various paradigms have greatly improved.
However, the complexity of large models makes it difficult

2

to understand the models themselves, which include high-
dimensional spaces, variable lengths of generated text, and expres-
sions of uncertainty for interpretation of results. These will be
described in detail in the survey [4, 15].

Overall, the main purpose of this survey paper is to
systematically review the advances and challenges of un-
certainty in the NLP field. To the best of our knowledge,
our paper is the first to summarize this topic. Concretely,
considering recent trends and paradigms in NLP, we pro-
vide a categorization of uncertainty sources according to the
stages of NLP systems. Besides, a comprehensive review of
the uncertainty estimation (a.k.a. quantification) technique
in recent years is further provided. We also summarize the
applications of uncertainty in NLP. Finally, we conclude
the challenges based on the characteristics of text data
and the NLP paradigms, highlighting future challenges and
potential trends.

1.1 Organization of The Survey

The rest of the survey is organized as follows: In Sec 2, we
introduce two kinds of uncertainty backgrounds in deep
learning and illustrate the main sources of uncertainty in
NLP systems in Fig. 2. In Sec 3, we classify uncertainty
estimation techniques based on their modeling approach.
We also abstract the insights behind each category of tech-
nology and provide a comparison of different technologies.
Following this, we present commonly used uncertainty eval-
uation metrics in NLP. Then, in Sec 4, we summarize the
application directions of uncertainty in NLP under the three
major categories and sort out the detailed literature of spe-
cific applications under each category. We then discuss the
current challenges of uncertainty estimation and potential
future research directions in Sec 5 and conclude this survey
in Sec 6.

2 UNCERTAINTY SOURCES

In this section, we aim to analyze the uncertainty sources of
NLP from the input, system, and output stages, respectively.
In advance, we review the theoretical background of uncer-
tainty definition.

2.1 Theory Background of Uncertainty

In the field of machine learning, uncertainty is commonly
divided into aleatoric uncertainty and epistemic uncertainty
([3, 6, 16, 17]). In general, these two types of uncertainty can
be summarized as follows:

e Aleatoric Uncertainty It is also known as data
uncertainty, which refers to the uncertainty inherent
in data due to its randomness or noise. This type
of uncertainty is irreducible, meaning it cannot be
eliminated through model improvements or tuning.
It can arise from a variety of sources, such as noisy
observations, overlapping classes, ground truth er-
rors, inherent randomness, or other factors that are
not entirely predictable.

e Epistemic Uncertainty It is also known as model
uncertainty, which is reducible uncertainty that arises
from a lack of knowledge or understanding about the


R «| Epistemic | Aleatoric Combined
€mark ‘ Uncertainty} ‘Uncertainty! ' Uncertainty !
Eee ees i] ra 2 1
tie wo oa Number of
| Language [PP REPOSEEORTERG Categories
ete | Classification || '-7>-7>7>-ooooo?
| Paradigm i
oe Ei Granularit '
NLP System ee fg
“A mem PE
_ Re, poeaeteccceees - Dependencies
= 2, | Sequence; 4) | recrersaesnararet
= ' Labeling 4 oe
I : eta) Boundary
nteraction “a Relationship |!
— 4
Model
[+ Structure
Uncertainty Generative
Paradigm
Sources fr | es
Language
Quality
Model
te an ae Regression Output '
Paradigm : Representation !
a ee 4
L y

Fig. 2. Illustration of sources of uncertainty. The figure includes the sources of uncertainty in the interaction of the NLP system. We start from the
three processes of Input, System, and Output to analyze the possible causes of each uncertainty. It is worth noting that these three parts are
interrelated. As the query passes through the NLP system, due to the combination of neural networks and different task specifications in complex
ways, the type of uncertainty in the output prediction becomes complicated, including both the aleatoric and epistemic uncertainty, which are hard

to decompose. Thus, we refer to it as combined uncertainty.

model itself. This can include uncertainty about the
model's structure choice, and parameters, which can
be reduced by increasing the amount and quality of
training data. Epistemic uncertainty can also result
from out-of-distribution examples, such as new lan-
guages or domains, and may be caused by model
structure errors or training procedure errors.

It is worth noting that there is no clear distinction
between epistemic uncertainty and aleatoric uncertainty,
and these types of uncertainty can even be mutual [17].
In particular, it may not always be clear which type of
uncertainty is dominant in a given NLP system, and the
two types of uncertainty can interact in complex ways, thus
making combinations complex and difficult to decouple.
Nonetheless, it is important to understand the source and
classification of uncertainty, e.g., in MT, decoupled uncer-
tainty can be used to infer whether predictions originate
from noisy and ambiguous references, or out-of-distribution
examples or noisy annotations [18].

To better understand the sources of uncertainty in NLP
systems, it is necessary to incorporate NLP usage scenarios.
To this end, we consider human-machine interactions in
NLP tasks to mine factors that may contribute to uncer-
tainty. Ultimately, we have determined three main sources
of uncertainty: (1) Input text entered by a user, (2) System
composed of models, and (3) Output returned to the user.
In the following sub-parts, we will discuss these three
sources. Meanwhile, in Fig. 2, we provide a detailed source
overview. It’s worth emphasizing that three sources tend to
be interconnected, and addressing one may help to mitigate
uncertainty in the others.

2.2 Uncertainty Source from Input

During the usage of an NLP system, a user would feed
input to it, expecting to obtain processed results or inter-
active feedback. Due to the language’s intrinsic ambiguity
and unknown query, the input itself contains uncertainty.
Formally, assuming that the model parameters W have been
trained on a dataset D = {(x1,y;)}4_,, and there exists a
correct and specific hypothesis space H, if an input text falls
within the hypothesis space H, then the input adheres to
the data distribution. However, if an input contains inherent
uncertainty or noise of the language, it may fall on the
boundary of the hypothesis space or even outsides [19].
In such cases, the system is prone to making incorrect
judgments. Consequently, it is necessary to capture and
quantify this resulting uncertainty. As depicted in the upper-
left of Fig. 2, we distinguish common sources of uncertainty
in inputs from two perspectives, including the language
intrinsic and unknown query, respectively.

2.2.1 Language Intrinsic

Language inherently contains ambiguity that leads to un-
certainty [20]. That is to say, ambiguity is a natural feature
of language that arises from inconsistencies and contextual
factors in the text [21]. For instance, the word “apple” has
various meanings when appearing in different contexts,
ie. “I like to eat apple” and “Steven Jobs built up Apple”.
Inconsistencies can take the form of incomplete sentences
or expressions [21], while contextual semantics encompass
the completeness and accuracy of information expression, as
well as social and cultural aspects and differences between
the unique context of text production (including features
of space, time, authorship, etc.) and multiple interpretation
contexts [20, 22, 23].


Natural language noise contains out-of-vocabulary
(OOV) and distraction. OOV indicates spelling errors or
emerging words that are unseen tokens for a running NLP
system. These would result in unsureness. Besides, distract-
ing refers to the presence of some words in a sentence that
are not related to the meaning of the sentence. This kind
of noise is ubiquitous in informal texts [24, 25, 26]. For
example, Liu et al. [25] in entity recognition find that a part
of the original sentence retains enough words to express the
relationship, and the rest has many irrelevant words that can
be regarded as noise that may hinder the performance of the
extractor. Kadavath et al. [26] set different prompt templates,
such as helpful, incorrect, and distracting hints, and find
that distracting hints in natural language generation (NLG)
also lead to deviations in the generated answers.

As mentioned above, natural language is inherently
complex, ambiguous, and context-dependent. It can be ex-
pressed in different ways to convey the same ideas or
concepts. Therefore, the mapping relationship in NLP is also
a source of uncertainty [23]. This complexity can result in
multiple input-output mapping scenarios, such as one-to-
one and many-to-one mapping. In such cases, the model
must select an output from several possible ones, which
can be challenging and uncertain. For instance, in MT, the
same source sentence can have multiple semantic equivalent
translations, resulting in multiple nature of machine transla-
tion learning tasks [22, 23, 27]. Similarly, Text-to-SQL needs
to deal with many-to-one cases, indicating that multiple
input texts correspond to the same SQL query [28]. This
is also caused by the uncertainty of natural language.

2.2.2 System Unknown Query

In the application of the actual world, systems usually
encounter an unknown query differently distributed from
training data [16, 29]. Systems tend to produce unreliable
or even catastrophic predictions, thereby harming the trust
of users. The uncertainty of the field is derived from the
model that the model cannot explain the out-of-distribution
(OOD) sample due to the lack of external knowledge. The
source of this uncertainty is the input data extracted with the
unknown subspace. Although DNNs can extract the know]-
edge in the domain from the domain migration sample, they
cannot extract the knowledge samples in the domain from
the external sample [16]. For example, QA systems deployed
in search engines or individual assistants require elegantly
processing OOD input, because users usually propose a
problem that is not within the scope of system training
distribution [26, 30]. Another example is to make entailment
judgments for breaking news articles in search engines [31].
If an input is not acceptable, the model needs to acknowl-
edge its uncertainty and turn to humans or better (but more
costly) models [32].

2.3 Uncertainty Source from System

The sources of uncertainty induced by NLP systems mainly
originate from model blocks, which are usually introduced
in the design and training of neural networks. The design
of DNNs involves explicit modeling of the network archi-
tecture and its stochastic training process. The assumptions
made about the problem structure, based on the network
design and training, are referred to as inductive biases [33].

2.3.1 Model Structure

Since neural networks are manually designed, their struc-
tures are also a source of uncertainty. In other words,
changing the structure of networks might alter decision-
making, bringing randomness to model training and infer-
ence stages. Here we discuss from the views of architecture
and parameter scale.

In an end-to-end model, many factors are relevant to
model architecture, such as the dimension of hidden states,
and interior neuron connections, directly affect the perfor-
mance of the network and thus introduce uncertainty [16,
33, 34] When studying complex AI systems, it needs to take
into account the predictions of several independent models
in downstream tasks [35] due to the effect of pipeline propaga-
tion. The cost and risk of any decision-making process that
produces different errors need to be balanced. This creates
uncertainty when confidence in one module’s predictions
is allowed to have an impact on the next module. For
this reason, if the uncertainty and confidence scores can be
reliably interpreted as probabilities, the rules of probabilistic
calculus can be applied, allowing one system to abort the de-
cision if its predictions are not confident enough [36, 37, 38].
This is a useful property in many situations. For example,
in the prediction of an information extraction system [6],
there are locations where the information extraction system
extracts useful information and uses confidence to check
whether the extracted information is meaningful. After this,
another model makes a final prediction of the extracted
value of the field.

The parameter scale is a critical factor that contributes
to uncertainty in a fixed model structure [39]. The size
of a model can have a significant impact on the level of
uncertainty. In particular, larger models tend to possess a
greater number of parameters and increased complexity.
This enables them to better capture the intricacies of the
training data, leading to reduced error on the training set.
However, the larger parameter count also renders these
models more susceptible to overfitting [10], potentially caus-
ing poor performance when presented with unseen data. On
the other hand, smaller models have fewer parameters and
lower complexity, making them more prone to underfitting
the training data. Consequently, these models may exhibit
higher error rates on the training set but have the potential
to generalize better on previously unseen data, thereby
potentially reducing uncertainty. It is important to recognize
that the relationship between model size and uncertainty
is multifaceted, and a comprehensive evaluation should
consider various factors such as model complexity, dataset
size and characteristics, and the regularization techniques
employed during training.

2.3.2 Model Training

During the model training, we distinguish uncertainty
sources into the dataset and hyperparameter views, respec-
tively. Initially, to learn an intelligent system, high-quality
training data is essential. Yet due to the different language
abilities of annotators, texts in the dataset pose various
challenges for them [40]. As a result, this directly affects
the correlation between samples and corresponding labels.
Uncertainty arises whenever there are multiple possible


interpretations of the data, but the knowledge or infor-
mation to definitively choose one of them is not available
[41]. Then, uncertainty may be part of the pre-processing
stage. For example, NLP tools used for pre-processing can
introduce uncertainty [42]. More concretely, Part-of-speech
taggers are often trained on data from historical periods.
Yet, recent data is often not available. Making results for
recent data is possibly unreliable, leading to uncertainty, and
this uncertainty is usually irreducible. Furthermore, Bender
et al. [43] discuss ethical issues raised by PLMs, including
issues of bias and fairness. They argue that language models
encode biases and assumptions in the training data, which
perpetuate social inequalities. This highlights sources of
uncertainty related to the ethical implications of PLMs, and
the need to consider them carefully for transparency.

The setting of hyperparameters, such as batch size, learn-
ing rate, and epoch number, is also random, resulting in
different local optimal solutions and different final models
[16]. Dodge et al. [44] study the fine-tuning process of PLMs
and find that hyperparameters and tuning such as weight
initializations, data orders, and early stopping are critical
to achieving high performance on downstream tasks. This
highlights that under the large model, fine-tuning the model
requires more strategies and hyperparameter requirements
and adjustments, which will also introduce new uncertainty
in the model framework.

2.4 Uncertainty Source from Output

Since making mistakes is inevitable for an NLP system, its
output is not fully trusted, ie. presenting uncertainty. Here
we summarize the uncertainty of the output according to the
paradigms and the nature of the task, namely classification,
sequence labeling, generation, and regression paradigms. It is
worth noting that although the task outputs are quite dif-
ferent, their sources of uncertainty are often combined and
complex. That is to say, uncertainties sourced from the input
text and model itself will affect the output. Therefore, in the
right part of Fig. 2, we do not specifically define whether the
uncertainty of output is epistemic or aleatoric uncertainty.

2.4.1 Classification Paradigm

The output of the classification paradigm is usually con-
nected with the number and granularity of categories.
Firstly, as the number of categories increases, the classi-
fication task becomes more difficult. There may be more
errors or misclassification space, which enlarges uncertainty.
This is because models may have difficulty distinguishing
between similar categories, or be more error-prone due to
the inherent ambiguity of the classification task. Secondly,
the classifying granularity (e.g., classifying at words or
sentences level, defining hierarchical or multi-label cate-
gories) may introduce uncertainty. More concretely, word-
level classification may have variability. For example, a
word like “Washington” could be classified as a person
or place depending on the context, and this classification
itself may be vague or indeterminate. Classification at a
coarser level (e.g. classifying entire sentences or documents)
depends on more factors (e.g. contribution of each token,
contextual semantics, length of sentence/document, etc.).

2.4.2 Sequence Labeling Paradigm

Sequence labeling aims to assign a tagger to each token
in the given input text. It needs to consider contexts to
determine the dependencies of label sequences. Specifically,
for sequence dependencies, the labels assigned to elements
can depend on the labels assigned to adjacent elements in
the sequence. This can lead to non-determinism because (1)
due to the inherent non-determinism of natural language
itself, the label assigned to an element may be ambiguous,
which needs to be determined according to the context and
neighboring elements in the sequence [45, 46]. (2) The labels
assigned to adjacent elements in the sequence should be
consistent and follow a certain pattern. However, this can be
challenging as the model may struggle to capture and model
complex dependencies and relationships between adjacent
elements [47].

In addition, the boundary, e.g. the start and end positions
of entities in named entity recognition (NER) tasks, is also
crucial. Correct entity boundaries are effective in mitigating
error propagation in entities that are linked to knowledge
bases [48]. Some studies consider entity boundary detection
as a subtask in NER [49, 50]. In summary, analyzing the
prediction uncertainty in sequence labeling can be roughly
divided into boundary uncertainty and boundary entity
classification uncertainty. Both of these are inseparable from
boundary detection and are dependent on the sequence.
This analysis increases the rationality and reliability of the
output results.

2.4.3. Generation Paradigm

A more challenging output is the generation paradigm,
where the uncertainty of the prediction is affected by un-
controllable factors. The output space of generating natural
language has O(|7 |) dimensions. For example, MT models
have hundreds of millions of parameters, and the search
space is exponentially large, which poses great challenges
for search algorithm. We usually observe only one reference
for a given source sentence, leading to uncertainty in the fi-
nal output. At present, tools and strategies can be borrowed
and combined from machine learning and statistics [10, 23,
51, 52]. Beam search is an effective search strategy, but some
external uncertainty (such as the quality of training data)
will lead to large beam performance degradation [23].

One of the goals of NLG is to ensure that the generated
text conveys the intended semantic content of the sentence.
However, in decision-making tasks that rely on NLG, e.g.
QA system [4], it is crucial to ensure invariance to the output
space, but this is often not explicitly specified in the model.
The meaning of the generated text is especially important
in terms of its beliefs. While a system may be reliable even
when generating output using many different expressions,
it may be less reliable when answering questions with
inconsistent meanings.

Another key purpose of NLG is to ensure the language
quality. which is typically evaluated based on several fac-
tors, including adequacy, fluency, readability, and variation
[53]. A recent example of an effective strategy is reinforce-
ment learning from human feedback (RLHF) [54]. But it has
been found to be wrongly calibrated [26]. This is not sur-
prising because RL fine-tuning tends to collapse language


Pon EG)
“Calibration! Viele y, ul wy ‘]
Megecmome

(a) Calibration-based model

( Member 1 | 1 a 1

Me bond.
[ Memier2 J>>[y"2| a int

[ Input x* ] x"

Varty",,...V"w)

( Member M M [Member m Ly"

(b) Sampling-based model

(c) Distribution-based model

Fig. 3. Rationale visualization of three types of uncertainty modeling.
For a given input sample x, each method provides a prediction y
whose uncertainty is quantified as w. (a) Calibration-based uncertainty
representation, (b) Sampling-based uncertainty estimation method, (c)
Distribution-based uncertainty estimation method. = denotes a specific
distribution. The mean EF and variance Var are only used to keep the
visualization simple, there are other quantifications in practice.

model predictions to the behavior that yields the highest
reward, which raises the inherent problem of whether the
model’s output is biased or unfair. Furthermore, the length
of the input text can vary widely [55, 56], leading to un-
certainty in the output. This particularly exists for longer
sequences, where the joint likelihood decreases due to the
conditional independence of labeling probabilities [4]. Rare
words also contribute to this problem because they have
small probabilities and therefore occur less frequently in the
sequence [23, 57].

2.4.4 Regression Paradigm

The uncertainty source in the classification output can be
easily explained. A discrete probability distribution over the
classifier’s output is naturally provided through a softmax
layer. However, in the regression paradigm, the output is a
single numerical value in continuous target spaces [58]. The
representation of the output becomes important, specifically
the range of the output. For example, FLORES [59] divides
0-100 points into ten intervals, each representing different
translation quality. In semantic text similarity assessment
[60], the objective is to predict a similarity score for a
sentence pair (.51,.52) within the range [0,5]. A score of 0
indicates complete dissimilarity, while a score of 5 signifies
the sentences are equal in meaning. Calibrated regression
[61] extends the calibration method of classification to re-
gression. It applies the obtained algorithm to the Bayesian
deep learning model and calibrates the confidence score in
the [0, 1] interval.

3 UNCERTAINTY ESTIMATION

Summarizing the sources of uncertainty in NLP systems can
help reduce uncertainty, however, accurately distinguishing
uncertainty types is challenging [16]. In this section, We
first introduce the background of uncertainty estimation
techniques, Then, As shown in Fig. 3, according to the
modeling characteristics, we propose a taxonomy of uncer-
tainty estimation in NLP, and the specificity of linguistic
uncertainty modeling.

Eentropy

Confidence

KL

cP

Gumbel

Smoothing

Fig. 4. An overview of the taxonomy of uncertainty estimation tech-
niques. The inner circle represents uncertainty modeling methods, the
outer circle represents actual uncertainty methods, and some methods
are represented by abbreviations.

3.1. Uncertainty Estimation Background

Modern neural networks are parameterized by a set of
model weights W, providing a formalization of uncer-
tainty in the BNN case, and these are sufficient statistics
w = (W;). Fora given dataset D = {x;, yi;}®_, with distri-
bution p(x, y), for a classification model P(y* = w.|x*,D)
trained on D, the distribution on prediction y* is described
as:

Ply" =welx",D) = | Ply" = ule", 9) plO1D) 48,
a

(a) Data (b) Model

where the aleatoric uncertainty is formalized as the poste-
rior probability P(y* = w,|x*,@) of over class labels for a
given parameter, while p(6|D) is used as the posterior distri-
bution of the model parameters, describing the uncertainty
of the model parameters, given a dataset D.

However, p(6|D) is intractable using Bayes’ rule pos-
terior distributions, and it is necessary to use variational
inference methods to achieve p(@|D) ~ q(@) [62, 63, 64]. In
the uncertainty estimation method, three ways are mainly
considered, which will be placed in the next subsequent
section.

3.2 Uncertainty Estimation Methods

After reviewing the uncertainty estimation methods com-
monly used in recent NLP tasks, we generally divide them
into three types, namely, calibration confidence-based meth-
ods, sampling-based methods, and distribution-based meth-
ods. We provide the usage and hierarchical classification of
these methods in Fig. 4, and below we summarize from each
of these three types.

3.2.1 Calibration Confidence-based Methods

Calibration methods aim to correct the reliability of the
uncertainty estimates provided by a model. The basic idea
is to measure the accuracy of the predicted probabilities
against the true probabilities. As shown in Fig. 5, if the
predicted probabilities are well calibrated, then they will


accurately reflect probabilities, and the model will be con-
sidered reliable. It is worth noting that the probability
distribution can be obtained by a single forward pass of the
network, also including some post-processing methods. We
introduce below two commonly used methods of expressing
uncertainty, as well as calibration methods in NLP research.

Softmax Response. Confidence method is a widely used
uncertainty estimation method. It is based on the idea that
the predicted category with the highest probability is the
most likely category, and the uncertainty can be represented
by Softmax Response (SR). There are also methods based on
the difference (SD) between the top two values of the Soft-
max output, with a slight difference indicating uncertainty
and a significant difference indicating confidence. Given an
input x, the uncertainty describe as:

usr(x) =1—- max ply = ex). (2)
USD (x) = usr (x) _ niee p(y = co/x). (3)
c2

co#c

Entropy-based. Prediction entropy, which can be under-
stood as the total uncertainty of the output distribution.
When the certainty of the output distribution is higher, the
entropy value is lower. Maximum entropy is reached when
all outcomes have equal probability. In other words, entropy
is highest value when the model is unable to distinguish
between different outcomes. The predictive entropy at point
x() is equal to the conditional entropy of the output random
variable Y:

PE(x) = — f vlyle)m plyle dy, (4)

In NLP research, the entropy value can represent a variety
of uncertain information, such as the N-grams word entropy
value as a heuristic feature to select domain adaptation data
[65], the entropy value at the sentence level can represent
semantic uncertainty [4], The composition of information
entropy in NLP is diverse and complex, and these issues
need to be considered when using entropy values.

Recently, the problem of overconfidence can arise in
DNNs [10], often requiring calibration to obtain reliable
confidence. A simple but effective calibration method is
Temperature scaling (TS), which was originally introduced
into the machine learning community as a tool for knowl-
edge distillation [66]. Specifically, TS helps introduces a
temperature parameter J’ > 0 and generates a calibrated
prediction vector by mapping:

PAG)

ah (5)

q) = osu(

where ogy4(z) = e”/ a e** refers to the logits z pass the
Softmax function. With T = 1,q = p, when T > 1,
the entropy of q“) increases, which helps reduce confidence
s® and combat overconfidence, and larger T will make
the distribution flatter. Similarly, T < 1 reduces entropy
and increases confidence, making the distribution sharper
to help with underconfidence predictions. The temperature
parameter T is trained with NLL on the validation set. TS
acts as a post-processing calibration method, treating each
prediction independently without explicitly taking contex-
tual information into account. Therefore, it’s important to

1.0 2 1.04 ZZ Gap
= output

ECE = 0.087

Empirical Accuracy

se Perfectly calibrated
e Well calibrated
004+ ~@- Under /Over-Confidence

00 02 O04 06 O08 10
Mean predicted probability
(a) Calibration curve

Confidence intervals
(b) Reliability diagram (ECE)

Fig. 5. Illustration of uncertainty representation method based on cali-
bration confidence. a) Calibration curve, the closer to the perfect curve,
the better the confidence calibration. b) Reliability diagram combined
with ECE calibration indicators.

consider whether token-level or sequence-level calibration
is more appropriate for the specific NLP task at hand.

In addition, there are some implicit calibration methods
to combat overconfidence, such as Label smoothing and
Re-weighting techniques [67, 68, 69]. Label smoothing is
a regularization technique that replaces the one-hot target
vector y with a weighted combination of targets that
have a uniform distribution. The amount of smoothing is
controlled by an a parameter. In the K — class setting, the
true class represented by the value 1 is replaced by 1 — a,
and the other classes are assigned the value of ;+,. Label
smoothing has been shown to have several benefits for the
model predictions. It implicitly calibrates model predictions
similar to TS and does not require post-operation quantifi-
cation. Similarly, reweighting techniques such as Focal loss
can also reasonably reduce the sharpness, and thus improve
the calibration effect and performance [67, 70]. By increasing
the entropy of the target distribution, label smoothing and
Focal loss prevent the model from producing extremely low-
entropy predictions.

Conformal Prediction (CP) [71] is a framework for
constructing calibrated predictive models that provide a
measure of confidence for each prediction. When CP is
formulated in terms of a set of predictions C(Xp41), it pro-
vides finite-sample, distribution-free guarantees for events
in which C contains Y;,41.CP is based on hypothesis testing,
where for a given input x and possible output y, a statistical
test is performed to accept or reject the null hypothesis that
the pairing (x,y) is correct. It can be seen as a post-hoc
calibration method, which means that it does not require
access to the training data during the calibration process.
Instead, it uses the model’s predictions on a validation set
or a hold-out set to construct a set of prediction regions or
intervals.

Challenging: How to solve the trade-off between accu-
racy and calibration? As increasing the accuracy may result
in overfitting and poor calibration, while increasing the
calibration may result in underfitting and poor accuracy.
Finding the optimal balance requires careful considera-
tion of the model complexity and the application require-
ments. Furthermore, NLP tasks often involve making pre-
dictions at different levels of granularity, such as sentence-
level, document-level, or token-level predictions. Uncer-
tainty quantification of texts with different granularities is
also worth thinking about.


3.2.2 Sampling-based Methods

Sampling methods provide a representation of the target
random variable, which can be divided into parametric and
predictive sampling, from which implicit conditional distri-
butions of the posterior model parameters can be derived.
Bayesian variational inference approximates the posterior
distribution of model parameters. It optimizes a variational
objective function. This method estimates uncertainty in
model predictions by sampling from the approximate poste-
rior distribution. For instance, variational inference methods
approximate intractable posterior distributions [62, 72] by
optimizing a family of tractable distributions. A commonly
used method is MC dropout [73], which involves randomly
dropping out some neurons in the network during testing.
This technique results in a different prediction for each
dropout configuration. These methods are based on Markov
Chain Monte Carlo and further extensions [74]. Further-
more, sampling-based methods also include ensemble meth-
ods. These methods involve training multiple models with
different initializations or architectures and combining their
predictions to make a final prediction.

Monte Carlo dropout (MCD) is the typical sampling
method. Specifically, the MCD-based method performs MZ
stochastic forward passes with activated dropout, explicit
ensembling to approximate integral Eq. 1:

M
‘(a l Die) go A(i
Pye, D) = ES PU }x,8), 8 ~ (0)
i=1

(6)
Each P(y |x, 6) sampled from the distribution q is a
categorical distribution of class labels on a given input x. It
visualizes as a point on the simplex [8].

Therefore, these methods construct an implicit condi-
tional distribution simplex by sampling, and the character-
istic is that whether a certain answer can be determined
through the sharpness of the simplex, that is, the expression
containing uncertainty. Given such a collection of distribu-
tions, it is expected that the entropy of the distribution P
will indicate the uncertainty of the prediction. It should be
noted that although the uncertainty expressed by entropy
is effective, it cannot be directly distinguished as aleatoric
uncertainty or epistemic uncertainty. To address this, mea-
sures such as mutual information can be used to estimate
prediction uncertainty due to epistemic uncertainty [75, 76].

In addition, Bayesian active learning by disagreement
(BALD; [77]) can be seen as a way to quantify uncertainty
based on MCD:

K
1
UBALD = — 5 P- log Pe + Va S pr log pi, (7)
k=1 cjm

From a Bayesian perspective, MCD is a way of approxi-
mating Bayesian inference in neural networks [73]. It can be
understood as placing a prior distribution on the weights of
the neural network, and then using dropout to sample from
the posterior distribution.

Ensemble Methods. An alternative approach to
Bayesian approximation involves creating ensembles of
multiple independent deterministic neural networks. The
technique involves training M neural network classifiers,
with different models randomly initialized and optimized

8

individually, and combining their networks outputs to form
a single classification function f(x) : X — Y, For example,
this can be achieved by simply averaging the predictions of
the members:

1 M
f(x) := a > fil). (8)

The premise behind this is to leverage diverse perspec-
tives to improve the generalization of the model, on the
basis that a group of decision-makers is typically more
effective than a solitary one. Ensembles have been found to
be particularly effective in uncertainty estimation networks
in neural systems. Moreover,to measure different sources
of uncertainty, mutual information (MI; [75, 77]) between
ensemble predictions and their parameters is often used
in NLP models [78, 79, 80]. Furthermore, MCD can be
thought of as an ensemble technique that simulates different
models using dropout [34], wherein a set of independently
trained networks with their own weights are generated. By
aggregating the predictions from these networks, we can
obtain more accurate and robust predictions. In addition,
recent work has achieved “better” uncertainty by combining
the expressive power of ensemble methods with MCD [81].
It has been demonstrated that sampling-based methods
exhibit inherent calibration and robustness to unknown data
representations [82].

Challenging: How to design methods to reduce time and
computational cost? A set of models with varying initial-
izations has the potential to explore multiple modes of
the parameter space [83]. Reducing sampling time and
computational cost is a challenging task in sampling-based
uncertainty estimation models. It is important to maintain
the diversity of individual models while achieving these
goals.

3.2.3  Distribution-based Methods.

As shown in Fig. 3, a distribution-based model is also a
deterministic model, but requires a specific distribution for
uncertainty modeling. In general, distribution-based models
need to pass some prior /posterior assumptions to allow the
model to learn distribution information during training. The
following are two commonly used examples of distribution-
based uncertainty modeling.

Dirichlet-based Uncertainty Models (DBU) utilize log-
its and construct a Dirichlet distribution. Unlike standard
(Softmax) neural networks, this model predicts the parame-
ters of the Dirichlet distribution instead of point estimates.
The density of the Dirichlet distribution is defined as

C
‘ 4 4 1 sy GD
Dir(pMla®) = Fay [[ of ~”, (9)
c=.

where B(a)) is the C-dimensional multinomial beta func-
tion.

Specifically, the P(y|x,@) obtained by the neural
network is a categorical distribution Cat(p™), which is a
point on the simplex, and the Dirichlet distribution is a prior
distribution over categorical distribution, which is parame-
terized by its concentration parameters, and the epistemic
uncertainty on x can be expressed by q“ = Dir(a),


so it can be interpreted as the distribution of the categorical
distribution.
(é)
i) __ Oc i i
p=, y =argmax [p\”) .
ao cEeC

(10)

There are currently some studies using the Dirichlet
distribution to quantitative uncertainty [8, 84, 85, 86]. The
advantage is that the network can obtain uncertainty es-
timates once forward. In addition, this parameterization
allows the calculation of closed-form classical uncertainty
measures [9], such as the differential entropy of Dirichlet

distribution maine = = H(Dir(a™)) or mutual information

myn = Ty, (p)) [8]

" MOHSOuAE constructing distributions by modeling net-
works is also a deterministic approach. For example, model-
ing a Dirichlet distribution on class probabilities rather than
the point estimate of a Softmax output [8, 84, 85].

Gaussian-based Uncertainty Models (GBU). Parameter-
izing uncertainty information using the mean and variance
of a Gaussian distribution is a common approach used
in various fields. Specifically, referring to u(x) and a(x)
as functions parameterized by W, the output mean and
standard deviation are computed for input x, assuming
y ~ N (u(x), o(x)?) for the data generating process in the
regression setting, and for the logits vector z in the classi-
fication setting samples are then converted to probabilities
using a Softmax operation. This process can be described as
The process is described as:

u~ N (u(x), o(x)’),
p = Softmax(u), (11)
y ~ Categorical(p).

This method models uncertainty as a Gaussian distri-
bution, where the mean represents the most likely value
or prediction, and the variance represents the level of un-
certainty or variability. By embedding words as Gaussian
distributional potential functions in an infinite dimensional
function space, Vilnis et al. [87] not only maps word types
to vectors, but also to soft regions in space, allowing uncer-
tainty modeling of meaning and metaphor and providing a
rich geometry of the latent space.

In NLP systems, Gaussian distributions and Maha-
lanobis distance can be used together in uncertainty esti-
mation techniques [88, 89]. Gaussian distributions are used
to model variables, while Mahalanobis distance are used
to compare simulated distributions with actual observed
data and quantify the distance between them. For example,
measure the Mahalanobis distance between a test instance
and the closest class conditional Gaussian distribution to
estimate uncertainty:

P(A — pe), (12)

— min(h©
ump = min(h’ — fe)
where hh‘ denotes the hidden representation of the i — th
sample.

Furthermore, the Bayesian neural network can approx-
imate the distribution of the function by learning the pa-
rameter distribution of some models, and the Gaussian
processes (GPs) non-parametric Bayesian model directly
uses the function f to approximate the distribution of the

9

function [90], which can measure the uncertainty of the
model. Formally, a Gaussian process can be defined as
a set of random variables in which any limited number
of variables follow a joint Gaussian distribution. To fully
specify GP, two functions are needed, the mean function
m(x) and the covariance function k(x, x’). If a function f is
based on GPs distribution of these two functions, it can be
defined as:

= E[f(x)]
E[(f(x) — m(x))(f(x’) — m(x’))|
f(x _ GP(m(x), k(x, x’)),

where m(x) is the mean function, which is usually the 0
constant, and k(x,x’) is the kernel or covariance function,
which describes the covariance between values of f at the
different locations of k(x and x’).

GPs are an alternative kernel-based framework that
provides competitive results for point estimation [91, 92,
93], and they explicitly epistemic uncertainty in data and
predictions. This makes GPs ideal when well-calibrated
uncertainty estimates are required. A commonly used ap-
proach to uncertainty estimation using GPs is the posterior
predictive distribution. The posterior predictive distribution
is the distribution of the predicted value of the output
variable at any given input point, given the observed data.
The mean of the posterior predictive distribution gives the
predicted value, while the variance provides a measure of
uncertainty. The larger the variance, the more uncertain the
forecast. However, the time complexity of GPs increases
with the size of the data, which makes it intractable in many
practical applications.

Challenging: How to make reasonable assumptions about dif-
ferent distributions in distribution-based uncertainty estimation
modeling? Distribution-based uncertainty estimation model-
ing often requires making assumptions about the under-
lying distribution of the data, which can be challenging
when dealing with complex or multi-modal data. Develop-
ing techniques for selecting appropriate distributions and
assessing their validity is necessary to ensure the accuracy
of the model and the reliability of the uncertainty estimates.

(13)

3.3. Uncertainty Estimation Metrics
3.3.1 Calibration Metrics

The calibration metric assesses the consistency of the clas-
sifier in producing reliable predictions. Fig. 5 can be intu-
itively visualized by (a) Calibration curves and (b) Reliabil-
ity diagram. This part focuses on the following metrics for
evaluating the calibration of predictions produced by deep
learning classifiers using uncertainty estimation techniques.

Expected Calibration Error (ECE) [10]. It denotes the ex-
pected calibration error, which aims to evaluate the expected
difference between model prediction confidence and accu-
racy. The concrete formulation is as follows:

|Bl N;
ECE= S> a lace(e i) — conf (b;)],

w=1

(14)

where b; represents the i-th bin and |B] represents the total
number of bins. N denotes the number of total samples. N;


represents the number of samples in the i-th bin. acc(b;)
denotes the accuracy and conf(b;) denotes the average of
confidences in the i-th bin.

Maximal Calibration Error (MCE) [10]. MCE repre-
sents the maximum deviation between model accuracy and
prediction confidence. In programs that require a reliable
measure of confidence, it is desirable to minimize the worst-
case deviation between confidence and accuracy. MCE is
similar to ECE in that this approximation involves bins:

MCE = : |acc(bm) — conf(bm)|.

max

jai

(15)

Brier Score (BS). The BS is a metric that evaluates the
proximity of a model’s predicted probability to the actual
class probability, which is always 1. A desirable BS is near
zero, as this indicates minimal deviation from the true
class probabilities due to the squared differences [94]. BS
is often written as the mean square prediction error (MSE)
in regression tasks, defined as:

{ <2 ;
BS=— dW — 9. (16)

Both BS and mean squared error (MSE) involve calcu-
lating the squared difference between predicted and actual
values. BS is often used to evaluate probabilistic predictions,
while MSE is more commonly used to evaluate regression
models.

Negative Log-likelihood (NLL). The NLL is a widely
accepted metric for evaluating the effectiveness of a prob-
abilistic model [95], and in the realm of deep learning it is
commonly referred to as cross entropy loss. When presented
with a probabilistic model and a set of n samples, this
measure can be used to gauge the model’s calibration:

NLL =—)Y_ log(#(y|x)).
i=l

Both NLL and Negative Log Predictive Density (NLPD)
are calculated based on the negative logarithm of proba-
bility, and NLPD calculates the negative logarithm of the
predicted density of new data given the training data and
model.

Furthermore, some NLP research proposes to calibrate
evaluation metrics to make them more suitable for evalu-
ating specific tasks. For instance, Lin et al. [96] proposes a
new metric called Compositional Expected Calibration Error
(CECE) in his Seq2Seq graph parsing work to measure the
behavior of the model in predicting the structure of the
combined graph, which evaluates the performance of the
model on graph elements. This metric calibrates the case to
better explain the behavior of the model in the combined
graph structure and the distributional behavior of predicted
graph structures under offset.

(17)

3.3.2 Uncertainty Indication

Apart from assessing confidence calibration, there are sev-
eral other evaluation indicators for uncertainty. These met-
rics are primarily focused on constructing classification
metrics by utilizing either uncertainty or confidence to
determine the model’s capability to differentiate between
misclassified and outlier samples.

10

Active Learning § 4.1.1 Select training data

OOD detection § 4.1.2 Detect abnormal data

Selective prediction § 4.1.3 J» Abstain risk data

Model Efficiency § 4.2.1 |——~ Calculated necessity

Information embedding

Model Performance § 4.1.2

Model Analysis § 4.3.1 | Uncertain calibration

Decision-making
Output Analysis § 4.3.2 KT

Interaction
L 4 t 1

Application

Training/inference guidance

IN U! Ajurejisouy) Jo suoneorddy

Assessment

Action

Fig. 6. An overview of the application of uncertainty estimation to NLP
systems. The red box indicates how the uncertainty is applied, and the
blue box is the action taken using the uncertainty.

The Area Under the Curve (AUC). AUC is a commonly
used measure the area under the receiver-operator curve,
the formulation is as follows:

DLtoed? Uted: Lg (to) < 9(ta)]
[Do |

where D° is the set of negative examples, and D! is the set

of positive examples. 1[g(to) < g(t1)] denotes an indicator

function which returns 1 if g(to) < g(t,) otherwise return 0.

Area Under the Receiver-Operator Characteristic Curve
(AUROC), and the AUROC metric measures the probability
that a randomly selected accurate response has a greater
uncertainty score than a randomly selected inaccurate re-
sponse. Higher values indicate superior performance, with
an ideal uncertainty score of 1 and a value of 0.5 for
a random uncertainty measure. Therefore, AUROC scores
high when the model is confident about correct predictions
but uncertain about incorrect ones. Depending on the axes
and decision information used, AUC can produce varying
score indicators, such as AUCPR (Area Under the Precision-
Recall Curve) can be seen as an extension of the AUC score.
However, AUC is based on the True Positive Rate (TPR)
and False Positive Rate (FPR), while AUCPR is based on
Precision and Recall.

The Area Under the Risk Coverage Curve (AUC-RCC)
[97]. AUC-RCC evaluates the quality of uncertainty estima-
tion in terms of its ability to reject predictions with high
uncertainty and avoid misclassification. AUC-RCC is based
on the risk coverage curve, which plots the cumulative risk
(or loss) as a function of the uncertainty level used for
rejecting predictions.

The Reversed Pair Proportion (RPP) [98]. Given a
labeled dataset D of size n, RPP is used to evaluate the
distance of the uncertainty estimator & from the ideal value.
In order to minimize the AUC of RCC, the selective classifier
should be intuitive for the correct classification output u = 0
for the example of correct classification output, and u = 1
for the wrong example. Therefore, the formula is defined as
follows:

AUC =

(18)

1


Heteroscedastic

bv,

i

“ation GPs
Ensembie

System Evaluation

Efficacy

Opy
ian

Ente
Gaus.
Gumb,

Si
el

3
=

&
8
3
&

G

KL

Fig. 7. An overview of the applied classification of uncertainty. The
inner circle represents the application, the middle circle represents the
uncertainty modeling method used, and the outer circle refers to the
specific uncertainty estimation method.

2 in the denominator is used to normalize the

where n
values.
RPP measures the proportion of pairs of examples with
an inverse confidence-error relationship, and an ideal confi-
dence estimator would have an RPP value of 0. In contrast
to AUROC, which measures overall discrimination, the RPP

incorporates consistency of ranking.

4 APPLICATIONS OF UNCERTAINTY IN NLP

In this section, we review the applications of uncertainty
estimation to NLP systems. As shown in Fig. 6, the high-
level perspective is mainly divided into three dimensions:
data, system, and assessment. The proportions of each type
and its sub-types are further counted, depicted in Fig. 7.

4.1. Data Filter and Action Guidance

In the data dimension, we summarize the application of
uncertainty as filtering and guidance. It mainly involves
three primary aspects, discussed separately below.

4.1.1 Active Learning
Active learning (AL) is a machine learning technique that
reduces the amount of labeled data required for training
by selecting the most informative unlabeled data for anno-
tation. This is based on the idea that not all data points
are equally useful during learning. Uncertainty-based AL
is a popular approach that assumes that data with higher
uncertainty is more informative and likely to enhance model
performance. As shown in Table 1, uncertainty-based AL
has been applied to various NLP tasks, and performance
metrics (e.g., Accuracy, Fl) are mainly used for AL evalu-
ation. As shown in Fig. 7, AL mainly uses the MCD based
on sampling, and the confidence and entropy method based
on calibration, focusing more on the quality of data filtered
by the teacher model and the overall performance of the
student system.

Benefiting from the development of NLP’s pre-training
model, uncertainty-based AL can effectively reduce the la-
beling cost and improve model performance. For example,

11

self-supervised language modeling and uncertainty filtering
data can be used for AL in the cold-start setting, effectively
reducing labeling costs and improving model performance
[100]. Ein-Dor et al. [101] combine BERT with the uncer-
tainty estimation in the study of text classification AL to
achieve good results in data screening of minority classes. In
addition, pre-trained models allow us to improve data filter-
ing capabilities in few-shot learning. Mukherjee et al. [105]
consider fine-tuning DNNs with limited labeled data. This
work develops two strategies for AL data filtering: filtering
hard data with high uncertainty and filtering soft data with
low uncertainty improves the performance of PLMs in few-
shot learning scenarios. Combining pre-trained models with
uncertainty-based active learning improves model perfor-
mance by leveraging pre-existing knowledge and selecting
informative examples for fine-tuning.

Moreover, data can be filtered using uncertainty and ad-
ditional information. For example, ACTUNE [102] proposes
uncertainty-based data filtering and guidance, allowing
switching between data filtering and model training. This
approach chooses highly deterministic unlabeled samples
as actively labeled, while low-uncertainty samples are se-
lected for model self-training. AL usually needs to traverse
all unlabeled data to find informative unlabeled samples,
which are always close to the decision boundary with large
uncertainty. AUSDS [104] annotates unlabeled text samples
with high uncertainty through adversarial attacks, which
significantly compresses the search space and protects the
decision boundary from drastic changes.

Since AL can select samples with large uncertainty, it
is natural to imagine applying uncertainty-based AL to
multiple domains in the NLP. For example, Lyu et al. [110]
identify language as a key factor in the difference between
data inside and outside the distribution in QA. They further
propose a data selection strategy for AL based on an un-
certainty measure developed using deterministic sequence
probability and MCD sequence probability. This approach
was shown to be more effective in selecting uncertain data
for multilingual QA. Information in real-world scenarios is
often multi-domain and rich in unlabeled data. Siddhant
et al. [2] point out that classical uncertainty estimation
can only account for arbitrary uncertainty (e.g., entropy
or confidence). In contrast, using MCD+ BALD and Back-
prop+BALD as two uncertainty estimation methods can
achieve excellent results in the three NLP tasks of emotion
classification, NER, and semantic role labeling.

However, there are also some challenges and _ limita-
tions of uncertainty-based AL, such as how to measure
uncertainty effectively, how to trade off exploration and
exploitation, and how to deal with the class imbalance
and data diversity. This section provides a comprehensive
review of existing work on uncertainty-based AL in NLP,
covering data selection strategies, NLP issues involved, and
uncertainty estimation methods.

4.1.2 OOD/Outlier Detection

Out-of-distribution (OOD)/outlier detection is an impor-
tant application of uncertainty estimation in NLP. It helps
to distinguish in-distribution examples from OOD/outlier
examples. As shown in Fig. 7, detecting OOD data in NLP
covers three paradigms of uncertainty estimation. The goal


12

TABLE 1
Application of Uncertainty-Based Data Correlation in NLP, and some task/method names are represented as abbreviations.

Application Paradigm Tasks Types UE_Methods Metrics
Confidence [100, 101, 102];
TC [99, 100, 101]; Entropy [99, 104]; Accuracy [99, 100, 101];
Active Learnin Class [102, 103, 104, 105]; KL [103]; [102, 103, 105, 106];
8 . NLI [106]; MCD [101, 102, 105];
SA [103]. Ensemble [101]; F1 [100, 101, 106].
Evidence [106].
Confidence [109]; .
Active Learning SeqLab ria tay 108], Entropy [104, 108]; Aeeioey (OA
peers MCD [2, 107]. OP MF eheds
. : : QA [110]; Confidence [15]; ROUGE [15];
Active Learning Generative AS[15, 111]. MCD [110, 111]. BLEU ya, [15, 110, 111].
Confidence [88, 112, 114,115]; Accuracy [112, 114, 116];
Entropy [76, 113]; [76, 103, 113];
SA [76, 88, 112]; TS [113]; F1, [89, 115, 116];
SLU [88, 113]; KL [103]; ECE [114];
OOD detection Class NLI [114]; Mahalanobis [88, 89]; AUROC [88, 113, 117];
TC [114, 115, 116], Evidence [117]; AUPR [89, 113, 117];
[89, 103, 117]. MCD [116]; FAR95 [88, 112, 113];
Ensemble [116]; AUCRCC [89],
Gaussian [113]. FPR90 [117].
OOD detection SeqLab NER [76, 86, 89]. peice eet BORee fer ‘
LM [76]; MCD [118, 119]; Perplexity [76];
OOD detection Generative MT [118, 119]; Entropy [76, 113]; BLEU var [118, 119];
QA [78]. Ensemble [78]. Accuracy [78].
Confidence [120, 121]; AUROC [30, 121].
Selective prediction Generative QA [30, 120, 121]. Smoothing [121]; Accuracy [30];
MCD [30, 121]; Precision/Recall [120].
Duplicate Detection [120, 122]; Confidence [122, 123]; Accuracy [80, 122];
Selective prediction Class BILL (120, 122]; MOT [EDI Pl [60];
SA [123]; Ensemble [80]; AUROC [80, 122, 123];
TC [80]. Bayes by Backprob [80]. AUPR [123].

is to identify examples that are different from the training
data and the model has not been seen before. Metrics used
to detect system performance include task performance
metrics and uncertainty classification metrics. The task per-
formance metric evaluates the model’s ability to correctly
handle the corresponding task, while the uncertainty clas-
sification metric measures the uncertainty quality of the
model’s predictions. We summarize the types of detection
data into three ones: unknown domain data, misinformation
data, and misclassification data.

Unknown Domain Data Detection. Domain shift in
NLP can be of different types such as background shift
(same task, but style/domain change) and semantic shift
(unseen labels) [124]. For example, traditional data selec-
tion based on testing domain knowledge often fails in
unknown domains such as patents and tweets. Thus, un-
known domain data detection is an important field of outlier
data, including multi-domain intersection, open domain,
etc. Uncertainty-based detection techniques can help solve
these problems to improve the reliability and usability of
NLP models in real scenarios. The corresponding NLP tasks
have applications. For instance, MULTIUAT [119] addresses
the challenge of learning multilingual and multi-domain
translation models due to heterogeneous and imbalanced
data, which makes the model converge inconsistently over
different corpora. Li et al. [78] perform multi-task-intensive
retrieval in open-domain QA, exploiting epistemic uncer-
tainty to deal with corpus inconsistencies. Yu et al. [76] show
that maximizing the uncertainty of training data using en-

tropy can enhance prediction accuracy on unseen domains
and outperform CNN, BERT, and Transformer baselines,
even without knowledge of unknown domains.

Different domains have various distributions. Therefore,
some previous works apply distribution-based uncertainty
estimation for OOD detection. Zhou et al. [88] propose an
unsupervised unknown domain detection method using
a contrastive learning framework. They fine-tune Trans-
formers with a contrastive loss to improve representation
compactness. They further use Mahalanobis distance and
Gaussian distribution in the penultimate layer to accurately
detect OOD instances. Hu et al. [117] exploit uncertainty
in OOD detection for text classification tasks using eviden-
tial neural networks based on Dirichlet distribution. They
propose a framework that adopts auxiliary outliers and
pseudo off-manifold samples to train the model with prior
knowledge of a certain class, which has high vacuity for
OOD samples. In addition, PLMs may affect OOD detection
performance. Hendrycks et al. [112] systematically study
the OOD robustness of pre-trained Transformers on various
models. To measure OOD detection performance, they use
the negative prediction confidence as the outlier supervision
score and show that the pre-trained Transformer performs
better than previous models both in generalizing to OOD
examples and in detecting OOD examples.

Misinformation Data Detection. The proliferation of
misinformation on social media platforms is a major con-
cern for society. One approach to developing uncertainty-
based methods and preventing misinformation is to use


uncertainty-based methods to detect and prevent misinfor-
mation and rumor spreading. To name a few, Zhang et al.
[125] conduct research on uncertainty estimation methods,
specifically Bayesian deep learning and Evidence Lower
Bound (ELBO) objective function for rumor detection.
Kochkina et al. [126] research error/outlier detection for au-
tomatic rumor verification. They propose two uncertainty-
based instance rejection methods using aleatoric and epis-
temic uncertainty estimates obtained through confidence-
based and MCD-based methods. This research aims to
solve the problem of resolving rumors circulating online by
prioritizing difficult instances for human fact-checkers and
interpreting model performance during rumor unfolding.
Furthermore, Feng et al. [127] study “none of the above”
(NOTA) detection tasks in dialogue systems, i.e. the case
where no correct response (i.e. ground truth) exists in the
candidate set. To this end, the paper utilizes MCD as an
uncertainty estimation method to measure the ability to cap-
ture uncertainty for end-to-end retrieval models. Overall,
the speed at which misinformation spreads on social media
makes manual verification difficult, and these tools can help
detect and mitigate the impact of misinformation, thereby
reducing its potential negative impact on the public.

Misclassification Data Detection. To detect OOD, epis-
temic uncertainty is caused by limited training data or
model structure. However, misclassification detection also
requires modeling aleatoric uncertainty caused by noise and
ambiguity in data [128]. Hendrycks et al. [123] study un-
certainty estimation such as confidence-based and Softmax
prediction probability for sentiment classification. The goal
is to indicate when classifiers are likely to make mistakes to
increase their adoption and prevent accidents. The authors
provide a misclassification detection baseline for neural net-
works that outperforms MaxProb for selective prediction.
Vazhentsev et al. [89] investigate the usage of uncertainty
estimation for Transformer-based NER and text misclassi-
fication, and add an experimental setup given OOD data.
The research proposes two computationally efficient mod-
ifications, including Mahalanobis Distance with a spectral-
normalized network, which approach or outperform com-
putationally intensive approaches.

However, how to determine the OOD boundary? One of
the main reasons for OOD boundary is the limited input
information available to NLP models, as they typically rely
on a limited set of features extracted from the input text.
This can lead to many models that are well-calibrated or
achieve excellent uncertainty estimation in IDs, but poor
results in OOD samples [26]. One option for finding the
boundary between ID and OOD data is to train a supervised
classifier on both ID and OOD data [8, 124]. Nevertheless,
collecting a representative set of OOD data may not be
practical due to the infinite compositionality of languages,
and selecting an arbitrary subset may introduce selection
bias and limit the generalizability of the model to unseen
OOD data. Alternatively, Ryu et al. [129, 130] propose us-
ing generative models, such as autoencoders and GANs
[131], to capture the ID data distribution and differentiate
between ID and OOD data based on reconstruction error
or likelihood. Other approaches, such as meta-learning for
OOD detection and generating pseudo-OOD data [132], also
provide decision boundary between ID and OOD [133], but

13

all require additional data or training procedures beyond
the task, which may result in significant data collection work
or inference overhead.

4.1.3 Selective Prediction

Selective prediction refers to the ability of AI systems to
abstain from making predictions when faced with novel
inputs that differ from their training data distribution. This
is essential for improving the reliability of NLP systems
in real-world safety-critical domains like biomedical and
autonomous robots, where incorrect predictions can have
serious consequences. Typically, c contains a prediction con-
fidence estimator ¢ and a threshold 7 that controls the level
of abstention:

c(x) = 1[é(x) > 7] (20)

For dataset D, the coverage of the threshold t corresponds
to the fraction of answered instances (where C(x) > 7), and
the risk is the error of these answered instances.

Selective predictive systems make a trade-off between
coverage and risk. As shown in Table 1, QA systems for med-
ical domains require high precision and discard questions
that will not be answered by the QA system. This presents a
cost-saving opportunity [120]. In addition, the example of a
medical diagnosis model usually provides high-confidence
classifications even when requires human intervention. The
failure of classifiers indicates when they are likely mistaken,
limiting their adoption or causing serious accidents. The
application of selective prediction in NLP is crucial for
enabling the reliable deployment of NLP systems in real-
world applications.

Recently, selective prediction in NLP has received atten-
tion. Andersen et al. [80] study MCD for selectivity predic-
tion in text classification. A semi-automatic text classifica-
tion framework that minimizes unreliable and error-prone
classifications by explicitly modeling uncertainty. Varshney
et al. [122] handle NLI tasks based on ID and OOD datasets,
allowing systems to avoid making predictions when they
might go wrong, improving their reliability in safety-critical
domains. Furthermore, Varshney et al. [121] investigate
selective prediction in NLP using 17 datasets covering NLI,
duplicate detection, and QA tasks. In BERT-based experi-
ments, the results show that these methods outperform the
MaxProb method when comparing whether the MCD and
label smoothing can improve the performance of selectivity
prediction. Garg et al. [120] conduct research on filtering out
unanswered questions in medical QA systems. They focus
on Transformer-based question models to improve answer
models and estimate uncertainty. This research shows that
confidence scores for answers can be approximated from
question text alone without requiring answers. By filter-
ing out error-prone problems, the improved QA model
improves the reliability of real-world NLP applications.
Kamath et al. [30] explore selectivity prediction in QA tasks
under domain shift. Here TS is used to calibrate confidence
scores, which enables the model to discard answers when
necessary to account for distributions different from the
training data.

Selective prediction in NLP involves using uncertainty
to determine which data to process, resulting in higher
accuracy on the answered subset. However, determining


14

TABLE 2
Uncertainty-based applications related to efficiency and performance improvement in NLP systems.

Application Paradigm Tasks Types UE_Methods Metrics
GLUE [134, 135]; : :
SA [136, 137, 138]; Contiennce [Ssh Accuaracy [36, 134];
NLI [137]; . [136, 137, 138];
Effi Cla Fact verification [138]; Entropy [134]; Consistency [138];
teacy SS Topic classification [137, 138]; TS [137, 138]; Layers [138];
Sentence classification [36]; ’ ’ Time% [134];
Shopping review [36]; CP [138] Speedup [36, 136, 138].
Sentences-matching [36]. ;
Confidence [135, 138];
‘ ; GLUE [136]; . Speedup [136];
Efficacy Regression cinilarity [135, 138]. Oo ast Relative scores [135].
BLEU [38];
Efficacy Generative Text generation [38]. Confidence [38]. RO a [38];
Layers [38].
; Gumbel [7];
Performance Class rel MED Zh Fl DSe
Fake News Detection [139]. a ae Accuaracy [7, 139].
BS [140, 141, 142];
Gumbel [141]; MAE [140, 141];
Performance Structure KGs [140, 141, 142]. Confidence [140, 142]. Accuaracy [140];
FI [140].
MCD [37]; :
Performance SeqLab NER [37, 68, 143]. Entropy [143]; roe teal 143];
Smoothing [68]. :
MT (27, 144, 145]; Le Pie Mal BLEU [27, 144, 145];
Performance Generative ’ Accuaracy [146];

QA [146].

how to rank examples according to confidence metrics and
balancing the trade-offs between risk and coverage are still
considerations in selective prediction.

4.2 NLP System Efficiency and Performance

The framework of an NLP system is usually multi-layer or
multi-module. Uncertainty estimation can be exploited for
each component to improve the computational efficiency or
performance of the network. Although performances could
be also improved by better dealing with data in Sec 4.1, here
in this section, we review from the perspective to leverage
uncertainty in the interior computation of NLP systems and
models. As presented in Fig. 6, uncertainty is introduced
for deciding the necessity of calculation. And it promotes
performance by providing more information or guidance.

4.2.1 Uncertainty and Efficiency

PLMs have impressive performance but require substantial
computational resources, which limits their use in real-
world applications [135, 137, 138]. In particular, autoregres-
sive decoding using a full stack of Transformer layers for
each output token is computationally expensive, and this
approach is commonly used in NLP [38]. This limitation
makes it difficult where fast inference and low computa-
tional cost are essential. Additionally, over-parameterization
of PLMs can lead to overthinking in decision-making, where
the model may focus on complex or irrelevant features
in later layers instead of relying on simpler features from
earlier layers that generalize better [136]. As demonstrated
in Fig. 7, the calibration-based uncertainty estimation ap-
proach is more commonly leveraged to improve efficiency.

Norma_Confidence [145];

MCD [144, 145]. AUROC [146].

-. ___ 1 Yes

Layer/Module 3 —>/C; | Is certain? |0.94

_—.

Layer/Module 2 Is certain? [0.62

_—

Layer/Module | Is certain? [0.35

p=E > Prediction

Input

Fig. 8. Illustration of the early exit method based on uncertainty. Com-
mon layers/modules such as Transformer, C;, are usually a classifier,
determining whether the early exit is determined by its confidence.

Its calculation is convenient and sufficient. Accurate confi-
dence information allows the model to improve efficiency
in a more optimal way.

An uncertainty-based early exit in NLP is a technique to
improve the efficiency of neural models. As shown in Fig. 8,
early exit allows the model to exit the sequence being pro-
cessed early based on uncertainty about the predicted out-
put. This strategy works by introducing a threshold value
that determines when the model should stop computation
in advance via the uncertainty of the predicted output. The
threshold value can be set through the desired trade-off
between accuracy and efficiency. If the uncertainty score
of the prediction is below the threshold, the model con-


tinues processing the sequence. Yet if the uncertainty score
exceeds the threshold, the model exits early and returns
the prediction. This approach can significantly reduce the
computation required to process the sequence and improve
the overall efficiency of the model.

To name a few, Schuster et al. [38] propose confidence
adaptive language modeling to improve the efficiency of
autoregressive decoding in PLMs. Confidence-based uncer-
tainty estimation allows PLMs to generate new tokens with
intermediate network layers rather than the full model.
Thereby it reduces the amount of computation. The ap-
proach builds on distribution-free uncertainty quantification
and provides a principled approach to improving model
efficiency while maintaining predictive quality. Liu et al.
[36] design a pre-trained model called FastBERT that applies
self-distillation and knowledge distillation to achieve faster
and more accurate results in Chinese and English sentence
classification tasks. To further improve efficiency, FastBERT
utilizes normalized entropy for uncertainty estimation, re-
moving cases with low uncertainty from the batch and send-
ing cases with high uncertainty to the next layer for further
reasoning. Xin et al. [135] propose BERxiT, which combines
BERT and exit for regression tasks in NLP. Furthermore,
Table 2 shows the application is mainly general performance
metrics and efficiency improvement comparison metrics.
By using uncertainty-based early exit, NLP models can
achieve faster processing times and reduce computational
costs without sacrificing accuracy.

Overall, uncertainty estimation has the potential to en-
hance the performance and efficiency of NLP systems.
However, one challenge is how to incorporate uncertainty
estimates into the decision-making process of NLP systems
without introducing bias or overconfidence. Additionally,
it is essential to trade off the computational cost of un-
certainty estimation with its potential benefits in terms of
performance and efficiency.

4.2.2 Uncertainty and Performance

Uncertainty information can also be used to guide the
model and improve its performance. For instance, Xiao et
al. [3] show that applying uncertainty estimation to NLP
tasks such as sentiment analysis, NER, and language mod-
eling models can learn accurate mappings. Additionally,
uncertainty can provide richer embedding information. By
incorporating uncertainty into the model, knowledge graph
embeddings can improve the accuracy and robustness of
models. Uncertain-aware embeddings can incorporate prob-
abilistic uncertainty into the embedding process, which
allows for more precise representations [140, 141, 142, 147].
It can be seen from Table 2 that, in addition to performance
indicators, uncertainty estimation is mainly exploited for
uncertainty calibration indicators.

Uncertainty can also be incorporated into the model’s
training and inference stages to improve model perfor-
mance. Gui et al. [37] propose to improve NER task per-
formers with MCD, which predicts error-prone draft la-
bels in the first stage. A two-stream self-attention model
in the second stage aims to refine these draft predictions.
This approach helps to prevent error propagation and im-
proves the accuracy of NLP models. In addition, Zhou
et al. [145] assume that language features alone cannot

15

TABLE 3
Uncertainty-based applications related to NLP system evaluation.

Tasks Types UE_Methods Metrics
Confidence [149], ee hie
[26, 148, 150]; MSE [150];
NLPD ‘i
MT [18, 93] TS [26, 149]; NEL oe das]
[148]; [18]; ’
Gis [2], MAE [93];
QA [149, 150] MCD [18, 148]; crea ua
2S; 151): AUROC [26];
Ensemble [148]; BS [26];

Sharpness [18, 148];
Pearson [18, 148].
NLPD [152];

RMSE [152];
Pearson [152].

Heteroscedastic [18].

Document Quality

Assessment [152]. GPs [152].

completely solve the difficulties encountered by the MT
model. They propose an adaptive curriculum learning strat-
egy to evaluate the uncertain information of the model.
This strategy enhances the performance of neural machine
translation, taking into account the complexity and rarity
of information in translation pairs. In MT tasks, a source-
language sentence may have multiple semantically similar
expressions. And a source-language sentence may also have
multiple valid counterparts in another language. Thus such
tasks are many-to-many problems, which inherently contain
uncertainty. Wei et al. [27] leverage confidence and KL
distributions to capture the relationship between multiple
semantically equivalent source sentences and use general
semantic information to enhance hidden representations for
better translation results. Accurate uncertainty refers to the
process of adjusting a model’s uncertainty estimate to better
reflect its true performance. This helps ensure the model
makes reliable predictions and improves its accuracy.

4.3 Reliability and Trustworthy Assessment

In this section, we review the applications of uncertainty
in the output stage of NLP system. Here the main insight
is that can we fully trust the output of a neural network? The
answer is obviously no. The main reason is that its output
relies on probability and thereby mistakes are inevitable.
The question can be converted to how much can we trust the
output? In other words, it means reliability or trustworthy
extents. Since uncertainty is reflected as a numerical value,
which naturally helps us to assess the quality of the model
or output.

By quantifying and analyzing uncertainty, we can gain
valuable insights into the strengths and weaknesses of our
NLP models, and better understand the risks and limitations
of the decisions we make based on them. For instance, if an
NLP model has high uncertainty in its predictions for certain
data points or scenarios, we expect to investigate further or
use additional information to improve the performance of
the model. Uncertainty can be used to evaluate models’ per-
formance. Confidence scores can be compared to the actual
correctness of decisions made by the model. This approach
enables us to determine how reliable and trustworthy the


model is in terms of making accurate decisions. In particular,
some metrics given in Sec 3.3 can be exploited to evalu-
ate the confidence calibration and uncertainty classification
ability of the model, and then reflect the advantages and
disadvantages of the model.

Furthermore, assessing the reliability of generated con-
tent is crucial. Especially in the exponential search space
of the generative model, the use of uncertainty for model
and output analysis is valuable. Table 3 shows that in recent
years, research on uncertainty estimation in MT and QA
has gradually increased, and various evaluation indicators
have increased to explain the model or output results.
Typically, multiple candidate translations can be generated
in MT, making it difficult to determine the best translation.
Estimating the uncertainty of each candidate translation
can help researchers choose the translation with the lowest
uncertainty, leading to better translation quality [18, 93,
148]. Recently, as PLMs have become more powerful and
widely used, it has become increasingly important to be able
to explain how they arrive at answers and provide some
degree of uncertainty or confidence in predictions. There
are various techniques for evaluating the performance and
credibility of PLMs, including self-evaluation techniques,
calibration scores, and other methods [26, 149, 150, 151].
These techniques can help to ensure that NLP systems are
trustworthy and reliable, and can be used with confidence
by users in a range of different applications.

Last but not least, an intuitive question is how to express
uncertainty to improve model evaluation and obtain valuable
information? This is a natural challenge because language
expression itself is inherently uncertain. Quantifying the
uncertainty expressed by the model, making effective eval-
uations, and reducing the interaction bias caused by expres-
sion are all important considerations. We provide a more
detailed discussion of this challenge in Sec 5.3.

5 CHALLENGES AND FUTURE DIRECTIONS
5.1 Extremely High-Dimensional Language-space

The field of NLP is challenged by the extremely high-
dimensional language space, as highlighted by the need
to estimate predictive entropy, which requires taking an
expectation in output space. This output space has a di-
mensionality of O(|7 |), which presents significant com-
putational challenges. Additionally, the lack of a normalized
probability density function over sentences necessitates ap-
proximating the expectation using Monte Carlo integration,
which involves averaging the likelihoods of a finite set of
sampled sentences. However, Monte Carlo integration be-
comes difficult for entropy as it is often dominated by low-
probability sentences that have large and negative logs. MT
models, for example, can contain hundreds of millions of
parameters, which exponentially increases the search space.
Additionally, with only a single reference for each source
sentence, it becomes difficult to measure the fitness of MT
models to data distributions. As a result, researchers face
significant scientific challenges in adapting tools from both
the machine learning and statistics fields to effectively tackle
this problem.

In addition, the complexity of uncertainty in NLP sys-
tems is also constantly increasing due to the evolving mod-

16

els and the demand for human-Al interaction. Although the
current uncertainty estimation techniques are rich, further
exploration is still needed to address issues such as time
cost and inference accuracy, and how to consider the impact of
natural language features on uncertainty estimation. Some un-
certainty estimation techniques can also be limited in their
scalability. Firstly, TS is a convenient calibration technique,
but the performance of TS is very sensitive to the choice
of the scaling factor. Choosing the optimal scaling factor
may require extensive hyperparameter tuning, which can be
time-consuming and computationally expensive. Secondly,
ensembling multiple models or MCD can be effective for
uncertainty estimation but can also be resource-intensive
and difficult to scale to large models. Finally, distribution-
based uncertainty estimation can provide valuable insights
into the uncertainty of DNNs, but it requires distributional
assumptions, such as Gaussian or a mixture of Gaussian
distributions, which may not apply in all situations and
limit its applicability. In the era of large PLMs such as
ChatGPT and GPT-4, researchers are not accessible to the
model. Even if a PLM is open-sourced, retraining is often
necessary, which can be computationally expensive and
time-consuming.

5.2 Variable Length Generation

The challenge of variable text length in NLP cannot be
ignored. One major obstacle is the significant variation in
the length of sentences [55]. As Malinin et al. [8] point out,
variable-length text generation is particularly challenging
in NLG, where longer sequences tend to have lower joint
likelihoods due to the conditional independence of token
probabilities. In other words, as the length of a sequence in-
creases, its joint likelihood decreases exponentially, leading
to a linear increase in its negative log probability. This means
that longer sentences contribute more to entropy, which
exacerbates the difficulty of dealing with variable-length
texts. Text length normalization is commonly adopted to
combat variable text length, which assumes that the uncer-
tainty in a sentence is independent of its length [55, 153,
154]. Specifically, for estimating the probability e = e1,
of generating the output sequence, length normalization
divides the score s(e) by the length / of the survival text to
get s’(e) = s(e)/I. For example, Kuhn et al. [4] apply length
normalization of the log probability when estimating the
semantic entropy of a QA system. However, this technique
may not be useful in all cases, especially when dealing with
long sentences, as it fails to capture the increased complexity
and difficulty of longer text sequences.

5.3 Expression of Uncertainty in Language Model

In real life, information is rarely black and white, which
is why expressing uncertainty is necessary to support the
decision-making process. While highly certain expressions
are commonly leveraged, uncertain expressions tell us about
the confidence, source, and limitations of the information. In
the current research, we find two main forms of uncertainty
expression: probabilistic expression and natural expression,
as demonstrated in Fig. 9. The former one generally has rel-
evant values available for analysis, but the community has
found different results on the calibration of neural models.


Different Templates for Uncertainty Representation

Answer Prefix:

1. I’m 90% sure it’s ...

2. I vaguely remember it’s ...

Answer Suffix:

1. ...But I would need to double-check.

2. ...With 100% confidence.

Answer Self-evaluation:

1. ...With what confidence could you answer this ques-
tion? and output an answer like 0%, 10%, 20%, ...,
100%.

Fig. 9. Illustration of a problem template for uncertainty estimation.

For example, Desai and Durrett [114] show that pre-trained
Transformers are relatively well calibrated, while Wang et
al. [155] find severe miscalibration in MT. Jiang et al. [149]
study calibration in generative QA and observe only a weak
correlation between the log-likelihood model assigned to
their answers and the correctness of the answers. In general,
PLMs output a possibility for a given tag sequence, but do
not output the entire meaning [4].

On the one hand, whether this uncertainty expression is
suitable for generative NLP systems remains questionable.
We may need stronger correlation indicators to focus on the
uncertainty expression of probability. On the other hand,
naturalistic representations of uncertainty cover a wide
range of discursive behaviors, such as signaling hesitancy,
attributing information, or acknowledging limitations, and
such representations are intuitive to humans. As the ex-
amples displayed in Fig. 9, QA prompts are able to ex-
press uncertainty. Through different prefixes, suffixes, or
confidence levels of self-assessment outputs, we can obtain
numerical uncertainty representations to further analyze the
calibration of the model.

Kadavath et al. [26] expect to observe large benefits of
few-shot evaluation with natural language methods, but
instead. Yet no major gains are observed in early QA ex-
periments. Zhou et al. [151] find that non-deterministic ex-
pressions can affect language production, and that changes
in these expressions can have a substantial impact on overall
accuracy, especially when using high-certainty expressions,
including in accuracy and calibration. The authors further
hypothesize that this may be due to the usage of hyperbolic
or exaggerated language in the training set, where numbers
are used non-literally. Then, the model somehow recog-
nizes idiomatic, non-literal usage of these extreme values,
resulting in lower performance of the task when hints are
introduced.

In summary, effective understanding and expression of
uncertainty are crucial for NLP applications to ensure better
decision-making. While there is a wealth of literature on
methods for estimating uncertainty, there is little under-
standing of how linguistic uncertainty interacts with natural
language. This lack of attention has resulted in a poor
understanding of how models interact with natural lan-
guage, leading to challenges in formulating and evaluating
uncertainty estimates.

17

5.4 NLP and Social Security

It is imperative to analyze uncertainty information in
NLP systems when integrating social and ethical demands.
Therefore, in the following discussion, we will examine
future directions from these three perspectives. As an im-
portant part of general Al, NLP systems need to account
for moral uncertainty, that is, try to explain what decisions
should be made, given various details of the circumstances
of their decisions, including the choices they face and their
theoretical rationale. Newberry et al. [156] discuss the idea
of using an automated “moral parliament” as a way to
mitigate the risk of value erosion in AI systems. By incor-
porating a variety of values from different stakeholders, the
AI could be directed by a simulated group representing dif-
ferent values. This approach would better reflect the moral
uncertainty of humans and avoid committing AI to one
value system. Perhaps the AI system will become a superior
version of the current chatbot, surpassing humans in mullti-
ple fields [157]. As AI systems become more powerful and
have a greater impact on society, there is an increasing need
to ensure that they are designed with appropriate ethical
and moral considerations, despite uncertainty surrounding
implementation details across multiple paradigms. This in-
cludes developing AI systems that can incorporate ethical
uncertainty and allow for multi-stage scrutiny to ensure
they are safe and beneficial for society.

Overall, in addition to the limitations and challenges
of existing uncertainty estimation methods, there is a need
for more effective methods that can provide interpretable
and reliable uncertainty estimates. This is an active area
of research, and developing better uncertainty estimation
methods can help improve the reliability, robustness, and
safety of NLP systems.

6 CONCLUSION

Considering the semantic ambiguity of natural language,
uncertainty is an important attribute of text information. For
the first time, this survey aims to provide a comprehensive
review of uncertainty estimation in the NLP field, includ-
ing its sources, quantification approaches, and applications.
We first depict the backgrounds of uncertainty types and
summarize the uncertainty sources of an NLP system by
its multiple stages. Then, uncertainty estimation methods
are systematically reviewed, with pointing out the tech-
nical advantages and application difficulties respectively.
Meanwhile, evaluation metrics for uncertainty estimation
are also discussed. We further investigate relevant literature
on uncertainty-aware applications in the NLP field, dividing
it into three directions and providing detailed analyses.
Finally, we highlight four challenges in the combination
between uncertainty estimation and the development of
PLMs. Promising research areas are also explored, including
the discussion on the scalability of uncertainty technology,
the expression of uncertainty, and AI security.

REFERENCES

[1] T.-X. Sun, X.-Y. Liu, X.-P. Qiu, and X.-J. Huang, “Paradigm shift
in natural language processing,” Machine Intelligence Research,
vol. 19, no. 3, pp. 169-183, 2022.


[2]

[3]
[4]

[5]

[6]
7]

[8]

19]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

A. Siddhant and Z. C. Lipton, “Deep Bayesian active learning for
natural language processing: Results of a large-scale empirical
study,” in EMNLP, 2018, pp. 2904-2909.

Y. Xiao and W. Y. Wang, “Quantifying uncertainties in natural
language processing tasks,” in AAAI, 2019, pp. 7322-7329.

L. Kuhn, Y. Gal, and S. Farquhar, “Semantic uncertainty: Lin-
guistic invariances for uncertainty estimation in natural language
generation,” in ICLR, 2023.

K. Ethayarajh, “Is your classifier actually biased? measuring
fairness under uncertainty with bernstein bounds,” in ACL, 2020,
pp. 2914-2919.

J. Kivimaki et al., “Uncertainty estimation with calibrated confi-
dence scores,” 2022.

J. Pei, C. Wang, and G. Szarvas, “Transformer uncertainty esti-
mation with hierarchical stochastic attention,” in AAAI, 2022, pp.
11147-11155.

A. Malinin and M. Gales, “Predictive uncertainty estimation via
prior networks,” NeurIPS, 2018.

A.-K. Kopetzki, B. Charpentier, D. Ziigner, S. Giri, and
S. Giinnemann, “Evaluating robustness of predictive uncertainty
estimation: Are dirichlet-based models reliable?” in ICML, 2021,
pp. 5707-5718.

C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration
of modern neural networks,” in ICML, 2017, pp. 1321-1330.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer
learning with a unified text-to-text transformer,” JMLR, pp. 5485-
5551, 2020.

A. Radford, K. Narasimhan, T. Salimans, I. Sutskever ef al.,
“Improving language understanding by generative pre-training,”
2018

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever
et al., “Language models are unsupervised multitask learners.”
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-
wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Lan-
guage models are few-shot learners,” NeurIPS, pp. 1877-1901,
2020.

J. Xu, S. Desai, and G. Durrett, “Understanding neural abstractive
summarization models via uncertainty,” in EMNLP, 2020, pp.
6275-6281.

J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt,
J. Feng, A. Kruspe, R. Triebel, P. Jung, R. Roscher et al., “A
survey of uncertainty in deep neural networks,” arXiv preprint
arXiv:2107.03342, 2021.

E. Hiillermeier and W. Waegeman, “Aleatoric and epistemic
uncertainty in machine learning: An introduction to concepts and
methods,” Machine Learning, pp. 457-506, 2021.

C. Zerva, T. Glushkova, R. Rei, and A. F. Martins, “Disentangling
uncertainty in machine translation evaluation,” in EMNLP, 2022,
pp- 8622-8641.

A. Delaforge, J. Azé, S. Bringay, C. Mollevi, A. Sallaberry, and
M. Servajean, “Ebbe-text: Explaining neural networks by explor-
ing text classification decision boundaries,” IEEE Transactions on
Visualization and Computer Graphics, 2022.

V. Dragos, “An ontological analysis of uncertainty in soft data,”
in FUSION, 2013, pp. 1566-1573.

S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach, “Lan-
guage (technology) is power: A critical survey of “bias” in NLP,”
in ACL, 2020, pp. 5454-5476.

L. VENUTL “The translator’s invisibility,” Criticism, pp. 179-212,
1986.

M. Ott, M. Auli, D. Grangier, and M. Ranzato, “Analyzing
uncertainty in neural machine translation,” in ICML, 2018, pp.
3956-3965.

R. Levy, “A noisy-channel model of human sentence comprehen-
sion under uncertain input,” in EMNLP, 2008, pp. 234-243.

T. Liu, X. Zhang, W. Zhou, and W. Jia, “Neural relation extraction
via inner-sentence noise reduction and transfer learning,” in
EMNLP, 2018, pp. 2195-2204.

S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain,
E. Perez, N. Schiefer, Z. H. Dodds, N. DasSarma, E. Tran-Johnson
et al., “Language models (mostly) know what they know,” arXiv
preprint arXiv:2207.05221, 2022.

X. Wei, H. Yu, Y. Hu, R. Weng, L. Xing, and W. Luo, “Uncertainty-
aware semantic augmentation for neural machine translation,” in
EMNLP, 2020, pp. 2724-2735.

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

18

B. Qin, L. Wang, B. Hui, B. Li, X. Wei, B. Li, F. Huang, L. Si,
M. Yang, and Y. Li, “SUN: Exploring intrinsic uncertainties in
text-to-SQL parsers,” in COLING, 2022, pp. 5298-5308.

U. Arora, W. Huang, and H. He, “Types of out-of-distribution
texts and how to detect them,” in EMNLP, 2021, pp. 10687-
10701.

A. Kamath, R. Jia, and P. Liang, “Selective question answering
under domain shift,” in ACL, 2020, pp. 5684-5696.

M. Carlebach, R. Cheruvu, B. Walker, C. Ilharco Magalhaes, and
S. Jaume, “News aggregation with diverse viewpoint identifi-
cation using neural embeddings and semantic understanding
models,” in Argument Mining, 2020, pp. 59-66.

J. Xin, R. Tang, Y. Yu, and J. Lin, “The art of abstention: Selective
prediction and error regularization for natural language process-
ing,” in ACL-IJCNLP, 2021, pp. 1040-1051.

P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez,
V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro,
R. Faulkner et al., “Relational inductive biases, deep learning, and
graph networks,” arXiv preprint arXiv:1806.01261, 2018.

B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and
scalable predictive uncertainty estimation using deep ensem-
bles,” NeurIPS, 2017.

A. Amini, W. Schwarting, A. Soleimany, and D. Rus, “Deep
evidential regression,” NeurIPS, pp. 14927-14937, 2020.

W. Liu, P. Zhou, Z. Wang, Z. Zhao, H. Deng, and Q. Ju, “Fast-
BERT: a self-distilling BERT with adaptive inference time,” in
ACL, 2020, pp. 6035-6044.

T. Gui, J. Ye, Q. Zhang, Z. Li, Z. Fei, Y. Gong, and X. Huang,
“Uncertainty-aware label refinement for sequence labeling,” in
EMNLP, 2020, pp. 2316-2326.

T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran,
Y. Tay, and D. Metzler, “Confident adaptive language modeling,”
NeurIPS, pp. 17 456-17 472, 2022.

W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling
to trillion parameter models with simple and efficient sparsity,”
The Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232-
5270, 2022.

C. Beck, H. Booth, M. El-Assady, and M. Butt, “Representation
problems in linguistic annotations: Ambiguity, variation, uncer-
tainty, error and bias,” in 14th Linguistic Annotation Workshop,
2020, pp. 60-73.

G.-P. Bonneau, H.-C. Hege, C. R. Johnson, M. M. Oliveira, K. Pot-
ter, P. Rheingans, and T. Schultz, “Overview and state-of-the-art
of uncertainty visualization,” Scientific Visualization: Uncertainty,
Multifield, Biomedical, and Scalable Visualization, pp. 3-27, 2014.

M. John, “Uncertainty in visual text analysis in the context of the
digital humanities,” 2017.

E. M. Bender and B. Friedman, “Data statements for natural lan-
guage processing: Toward mitigating system bias and enabling
better science,” TACL, pp. 587-604, 2018.

J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi,
and N. Smith, “Fine-tuning pretrained language models: Weight
initializations, data orders, and early stopping,” arXiv preprint
arXiv:2002.06305, 2020.

C. Liang, Y. Yu, H. Jiang, S. Er, R. Wang, T. Zhao, and C. Zhang,
“Bond: Bert-assisted open-domain named entity recognition with
distant supervision,” 2020.

A. Pandey, S. Daw, N. Unnam, and V. Pudi, “Multilinguals at
SemEval-2022 task 11: Complex NER in semantically ambiguous
settings for low resource languages,” in SemEval, 2022, pp. 1469-
1476.

R. Panchendrarajan and A. Amaresan, “Bidirectional lstm-crf for
named entity recognition,” in Proceedings of the 32nd Pacific Asia
conference on language, information and computation, 2018.

J. Li, A. Sun, J. Han, and C. Li, “A survey on deep learning for
named entity recognition,” IEEE Trans. on Knowl. and Data Eng.,
p- 50-70, 2022.

F. Zhai, S. Potdar, B. Xiang, and B. Zhou, “Neural models for
sequence chunking,” in AAAI, 2017.

I. Partalas, C. Lopez, N. Derbas, and R. Kalitvianski, “Learning
to search for recognizing named entities in Twitter,” in WNUT,
2016, pp. 171-177.

O. Levy and Y. Goldberg, “Linguistic regularities in sparse and
explicit word representations,” in CoNLL, 2014, pp. 171-180.

V. Kuleshov and P. S. Liang, “Calibrated structured prediction,”
NeurIPS, 2015.


[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

[76]

[77]

A. Stent, M. Marge, and M. Singhai, “Evaluating evaluation
methods for generation in the presence of variation.” in CICLing,
2005, pp. 341-351.

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,
D. Drain, S. Fort, D. Ganguli, T. Henighan et al., “Training a
helpful and harmless assistant with reinforcement learning from
human feedback,” arXiv preprint arXiv:2204.05862, 2022.

K. Murray and D. Chiang, “Correcting length bias in neural
machine translation,” in Proceedings of the Third Conference on
Machine Translation: Research Papers, 2018, pp. 212-223.

A. Malinin and M. Gales, “Uncertainty estimation in autoregres-
sive structured prediction,” in ICLR, 2022.

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their
compositionality,” NeurIPS, 2013.

Y. Wang, D. Beck, T. Baldwin, and K. Verspoor, “Uncertainty
estimation and reduction of pre-trained models for text
regression,” Transactions of the Association for Computational
Linguistics, vol. 10, pp. 680-696, 2022. [Online]. Available:
https: / /aclanthology.org /2022.tacl-1.39

F. Guzman, P-J. Chen, M. Ott, J. Pino, G. Lample,
P. Koehn, V. Chaudhary, and M. Ranzato, “The FLORES
evaluation datasets for low-resource machine translation:
Nepali—English and Sinhala—English,” in EMNLP-IJCNLP, Hong
Kong, China, Nov. 2019, pp. 6098-6111. [Online]. Available:
https: / /aclanthology.org /D19-1632

C. Corley and R. Mihalcea, “Measuring the semantic similarity
of texts,” in Proceedings of the ACL Workshop on Empirical Modeling
of Semantic Equivalence and Entailment. Ann Arbor, Michigan:
Association for Computational Linguistics, Jun. 2005, pp. 13-18.
[Online]. Available: https: //aclanthology.org /W05-1203

V. Kuleshov, N. Fenner, and S. Ermon, “Accurate uncertainties
for deep learning using calibrated regression,” in International
conference on machine learning. PMLR, 2018, pp. 2796-2804.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra,
“Weight uncertainty in neural network,” in ICML, 2015, pp. 1613-
1622.

D. P. Kingma, T. Salimans, and M. Welling, “Variational dropout
and the local reparameterization trick,” NeurIPS, 2015.

C. Louizos and M. Welling, “Structured and efficient variational
deep learning with matrix gaussian posteriors,” in ICML, 2016,
pp. 1708-1716.

S. Ruder and B. Plank, “Learning to select data for transfer
learning with Bayesian optimization,” in EMNLP, Sep. 2017, pp.
372-382.

G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in
a neural network,” arXiv preprint arXiv:1503.02531, 2015.

J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and
P. Dokania, “Calibrating deep neural networks using focal loss,”
NeurIPS, pp. 15 288-15 299, 2020.

E. Zhu and J. Li, “Boundary smoothing for named entity recog-
nition,” in ACL, 2022, pp. 7096-7108.

Y. Xiao, P. P. Liang, U. Bhatt, W. Neiswanger, R. Salakhutdinov,
and L.-P. Morency, “Uncertainty quantification with pre-trained
language models: A large-scale empirical analysis,” in EMNLP
Findings, Dec. 2022, pp. 7273-7284.

T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss
for dense object detection,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 2980-2988.

V. Vovk, A. Gammerman, and G. Shafer, Algorithmic learning in a
random world.

R. M. Neal, “Bayesian training of backpropagation networks by
the hybrid monte carlo method,” Citeseer, Tech. Rep., 1992.

Y. Gal and Z. Ghahramani, “Dropout as a bayesian approxima-
tion: Representing model uncertainty in deep learning,” in ICML,
2016, pp. 1050-1059.

G. E. Hinton and D. Van Camp, “Keeping the neural networks
simple by minimizing the description length of the weights,” in
6COLT93, 1993, pp. 5-13.

B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker,
“On variational bounds of mutual information,” in ICML, 2019,
pp. 5171-5180.

Y. Yu, H. Sajjad, and J. Xu, “Learning uncertainty for unknown
domains with zero-target-assumption,” in ICLR, 2023.

N. Houlsby, F. Huszdér, Z. Ghahramani, and M. Lengyel,
“Bayesian active learning for classification and preference learn-
ing,” arXiv preprint arXiv:1112.5745, 2011.

[78]

[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]
[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

[96]

[97]

[98]

[99]

[100]

[101]

[102]

[103]

19

M. Li, M. Li, K. Xiong, and J. Lin, “Multi-task dense retrieval via
model uncertainty fusion for open-domain question answering,”
in EMNLP-Findings, 2021, pp. 274-287.

V. Raina and M. Gales, “Answer uncertainty and unanswerability
in multiple-choice machine reading comprehension,” in ACL-
Findings, 2022, pp. 1020-1034.

J. S. Andersen and W. Maalej, “Efficient, uncertainty-based mod-
eration of neural networks text classifiers,” in ACL-Findings, 2022,
pp. 1536-1546.

R. Pop and P. Fulop, “Deep ensemble bayesian active learning:
Addressing the mode collapse issue in monte carlo dropout via
ensembles,” arXiv preprint arXiv:1811.03897, 2018.

Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin,
J. Dillon, B. Lakshminarayanan, and J. Snoek, “Can you trust your
model’s uncertainty? evaluating predictive uncertainty under
dataset shift,” NeurIPS, 2019.

S. Fort, H. Hu, and B. Lakshminarayanan, “Deep ensembles: A
loss landscape perspective,” arXiv preprint arXiv:1912.02757, 2019.
M. Sensoy, L. Kaplan, and M. Kandemizr, “Evidential deep learn-
ing to quantify classification uncertainty,” NeurIPS, 2018.

B. Charpentier, D. Ziigner, and S. Giinnemann, “Posterior net-
work: Uncertainty estimation without ood samples via density-
based pseudo-counts,” NeurIPS, pp. 1356-1367, 2020.

Z. Zhang, M. Hu, S. Zhao, M. Huang, H. Wang, L. Liu, Z. Zhang,
Z. Liu, and B. Wu, “E-ner: Evidential deep learning for trustwor-
thy named entity recognition,” arXiv preprint arXiv:2305.17854,
2023.

L. Vilnis and A. McCallum, “Word representations via gaussian
embedding,” arXiv preprint arXiv:1412.6623, 2014.

W. Zhou, F. Liu, and M. Chen, “Contrastive out-of-distribution
detection for pretrained transformers,” in EMNLP, 2021, pp.
1100-1111.

A. Vazhentsev, G. Kuzmin, A. Shelmanov, A. Tsvigun, E. Tsym-
balov, K. Fedyanin, M. Panov, A. Panchenko, G. Gusev, M. Burt-
sev, M. Avetisian, and L. Zhukov, “Uncertainty estimation of
transformer predictions for misclassification detection,” in ACL,
2022, pp. 8237-8252.

C. K. Williams and C. E. Rasmussen, Gaussian processes for machine
learning, 2006.

K. Shah, T. Conn, and L. Specia, “An investigation on the
effectiveness of features for translation quality estimation,” in
MTSummit, 2013.

D. Beck, T. Cohn, and L. Specia, “Joint emotion analysis via multi-
task gaussian processes,” in EMNLP, 2014, pp. 1798-1803.

D. Beck, L. Specia, and T. Cohn, “Exploring prediction uncer-
tainty in machine translation quality estimation,” in SIGNLL,
2016, pp. 208-218.

M. Assel, D. D. Sjoberg, and A. J. Vickers, “The brier score does
not evaluate the clinical utility of diagnostic tests or prediction
models,” Diagnostic and prognostic research, pp. 1-7, 2017.

J. Quinonero-Candela, C. E. Rasmussen, F. Sinz, O. Bousquet,
and B. Scholkopf, “Evaluating predictive uncertainty challenge,”
Lecture Notes in Computer Science, pp. 1-27, 2006.

Z. Lin, D. Phan, P. Pasupat, J. Z. Liu, and J. Shang, “On compo-
sitional uncertainty quantification for seq2seq graph parsing,” in
ICLR, 2023.

R. El-Yaniv and Y. Wiener, “On the foundations of noise-free
selective classification,” J. Mach. Learn. Res., p. 1605-1641, 2010.
J. Xin, R. Tang, Y. Yu, and J. Lin, “The art of abstention: Selective
prediction and error regularization for natural language process-
ing,” in ACL-IJCNLP, 2021, pp. 1040-1051.

J. Zhu, H. Wang, T. Yao, and B. K. Tsou, “Active learning with
sampling by uncertainty and density for word sense disambigua-
tion and text classification,” in COLING, 2008, pp. 1137-1144.

M. Yuan, H.-T. Lin, and J. Boyd-Graber, “Cold-start active learn-
ing through self-supervised language modeling,” in EMNLP,
2020, pp. 7935-7948.

L. Ein-Dor, A. Halfon, A. Gera, E. Shnarch, L. Dankin,
L. Choshen, M. Danilevsky, R. Aharonov, Y. Katz, and N. Slonim,
“Active Learning for BERT: An Empirical Study,” in EMNLP,
2020, pp. 7949-7962.

Y. Yu, L. Kong, J. Zhang, R. Zhang, and C. Zhang, “AcTune:
Uncertainty-based active self-training for active fine-tuning of
pretrained language models,” in NAACL, 2022, pp. 1422-1436.
K. Margatina, G. Vernikos, L. Barrault, and N. Aletras, “Active
learning by acquiring contrastive examples,” in EMNLP, 2021,
pp. 650-663.


[104]

[105]

[106]

[107]

[108]

[109]

[110]

[111]

[112]

[113]

[114]

[115]

[116]

[117]

[118]

[119]

[120]

[121]

[122]

[123]

[124]

[125]

[126]

[127]

[128]

D. Ru, J. Feng, L. Qiu, H. Zhou, M. Wang, W. Zhang, Y. Yu,
and L. Li, “Active sentence learning by adversarial uncertainty
sampling in discrete space,” in EMNLP, 2020, pp. 4908-4917.

S. Mukherjee and A. Awadallah, “Uncertainty-aware self-training
for few-shot text classification,” in NeurIPS, H. Larochelle,
M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020, pp.
21 199-21 212.

S. Lei, X. Zhang, J. He, F. Chen, and C.-T. Lu, “Uncertainty-aware
cross-lingual transfer with pseudo partial labels,” in NAACL-
Findings, 2022, pp. 1987-1997.

A. Shelmanov, D. Puzyrev, L. Kupriyanova, D. Belyakov, D. Lar-
ionov, N. Khromov, O. Kozlova, E. Artemova, D. V. Dylov,
and A. Panchenko, “Active learning for sequence tagging with
deep pre-trained models and Bayesian uncertainty estimates,” in
EACL, 2021, pp. 1698-1712.

A. Chaudhary, J. Xie, Z. Sheikh, G. Neubig, and J. Carbonell, “A
little annotation does a lot of good: A study in bootstrapping low-
resource named entity recognizers,” in EMNLP-IJCNLP, 2019, pp.
5164-5174.

M. Liu, Z. Tu, T. Zhang, T. Su, X. Xu, and Z. Wang, “Ltp: a new
active learning strategy for crf-based named entity recognition,”
Neural Processing Letters, pp. 2433-2454, 2022.

Z. Lyu, D. Duolikun, B. Dai, Y. Yao, P. Minervini, T. Z. Xiao,
and Y. Gal, “You need only uncertain answers: Data efficient
multilingual question answering,” TWorkshop on Uncertainty and
Ro-Bustness in Deep Learning, 2020.

A. Gidiotis and G. Tsoumakas, “Should we trust this summary?
Bayesian abstractive summarization to the rescue,” in ACL-
Findings, 2022, pp. 4119-4131.

D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic, R. Krishnan, and
D. Song, “Pretrained transformers improve out-of-distribution
robustness,” in ACL, 2020, pp. 2744-2751.

Y. Shen, Y.-C. Hsu, A. Ray, and H. Jin, “Enhancing the generaliza-
tion for intent classification and out-of-domain detection in slu,”
in COLING, 2021, pp. 2443-2453.

S. Desai and G. Durrett, “Calibration of pre-trained transform-
ers,” in EMNLP, 2020, pp. 295-302.

X. Zhang, F. Chen, C.-T. Lu, and N. Ramakrishnan, “Mitigating
uncertainty in document classification,” in NAACL, 2019, pp.
3126-3136.

J. He, X. Zhang, S. Lei, Z. Chen, F. Chen, A. Alhamadani, B. Xiao,
and C. Lu, “Towards more accurate uncertainty estimation in text
classification,” in EMNLP, 2020, pp. 8362-8372.

Y. Hu and L. Khan, “Uncertainty-aware reliable text classifica-
tion,” in SIGKDD, 2021, p. 628-636.

T. Z. Xiao, A. N. Gomez, and Y. Gal, “Wat zei je? detecting out-
of-distribution translations with variational transformers,” arXiv
preprint arXiv:2006.08344, 2020.

M. Wu, Y. Li, M. Zhang, L. Li, G. Haffari, and Q. Liu,
“Uncertainty-aware balancing for multilingual and multi-
domain neural machine translation training,” in EMNLP, 2021,
pp. 7291-7305.

S. Garg and A. Moschitti, “Will this question be answered? ques-
tion filtering via answer model distillation for efficient question
answering,” in EMNLP, 2021, pp. 7329-7346.

N. Varshney, S. Mishra, and C. Baral, “Investigating selective
prediction approaches across several tasks in IID, OOD, and
adversarial settings,” in ACL-Findings, 2022, pp. 1995-2002.

N. Varshney, S. Mishra, and C. Baral, “Towards improving selec-
tive prediction ability of nlp systems,” in RepL4NLP, 2022, pp.
221-226.

D. Hendrycks and K. Gimpel, “A baseline for detecting misclas-
sified and out-of-distribution examples in neural networks,” in
ICLR, 2017.

D. Hendrycks, M. Mazeika, and T. Dietterich, “Deep anomaly
detection with outlier exposure,” in ICLR, 2019.

Q. Zhang, A. Lipani, S. Liang, and E. Yilmaz, “Reply-aided de-
tection of misinformation via bayesian deep learning,” in WWW,
2019, p. 2333-2343.

E. Kochkina and M. Liakata, “Estimating predictive uncertainty
for rumour verification models,” in ACL, 2020, pp. 6964-6981.

Y. Feng, S. Mehri, M. Eskenazi, and T. Zhao, ““none of the above”:
Measure uncertainty in dialog response retrieval,” in ACL, 2020,
pp- 2013-2020.

J. Mukhoti, A. Kirsch, J. van Amersfoort, P. H. Torr, and
Y. Gal, “Deterministic neural networks with inductive biases

[129]

[130]

[131]

[132]

[133]

[134]

[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

[147]

[148]

[149]

[150]

[151]

[152]

[153]

20

capture epistemic and aleatoric uncertainty,” arXiv preprint
arXiv:2102.11582, p. 13, 2021.

S. Ryu, S. Kim, J. Choi, H. Yu, and G. G. Lee, “Neural sentence
embedding using only in-domain sentences for out-of-domain
sentence detection in dialog systems,” Pattern Recognition Letters,
pp. 26-32, 2017.

S. Ryu, S. Koo, H. Yu, and G. G. Lee, “Out-of-domain detection
based on generative adversarial network,” in EMNLP, 2018, pp.
714-718.

I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-
sarial nets,” stat, p. 10, 2014.

M. Tan, Y. Yu, H. Wang, D. Wang, S. Potdar, S. Chang, and
M. Yu, “Out-of-domain detection for low-resource text classifi-
cation tasks,” in EMNLP-IJCNLP, 2019, pp. 3566-3572.

K. Lee, H. Lee, K. Lee, and J. Shin, “Training confidence-
calibrated classifiers for detecting out-of-distribution samples,”
in ICLR, 2018.

S. Geng, P. Gao, Z. Fu, and Y. Zhang, “Romebert: Robust training
of multi-exit bert,” arXiv preprint arXiv:2101.09755, 2021.

J. Xin, R. Tang, Y. Yu, and J. Lin, “BERxiT: Early exiting for BERT
with better fine-tuning and extension to regression,” in EACL,
2021, pp. 91-104.

W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “Bert loses
patience: Fast and robust inference with early exit,” NeurIPS, pp.
18 330-18 341, 2020.

R. Schwartz, G. Stanovsky, S. Swayamdipta, J. Dodge, and N. A.
Smith, “The right tool for the job: Matching model and instance
complexities,” in ACL, 2020, pp. 6640-6651.

T. Schuster, A. Fisch, T. Jaakkola, and R. Barzilay, “Consistent
accelerated inference via confident adaptive transformers,” in
EMNLP, 2021, pp. 4962-4979.

L. Wei, D. Hu, W. Zhou, and S. Hu, “Uncertainty-aware prop-
agation structure reconstruction for fake news detection,” in
COLING, 2022, pp. 2759-2768.

X. Chen, M. Chen, W. Shi, Y. Sun, and C. Zaniolo, “Embedding
uncertain knowledge graphs,” in AAAI, 2019, pp. 3363-3370.

X. Chen, M. Boratko, M. Chen, S. S. Dasgupta, X. L. Li, and A. Mc-
Callum, “Probabilistic box embeddings for uncertain knowledge
graph reasoning,” in NAACL, 2021, pp. 882-893.

K. Boutouhami, J. Zhang, G. Qi, and H. Gao, “Uncertain
ontology-aware knowledge graph embeddings,” in JIST, 2020,
pp. 129-136.

Y. Li, L. Liu, and S. Shi, “Rethinking negative sampling for
handling missing entity annotations,” in ACL, 2022, pp. 7188-
7197.

S. Wang, Y. Liu, C. Wang, H. Luan, and M. Sun, “Improving
back-translation with uncertainty-based confidence estimation,”
in EMNLP-IJCNLP, 2019, pp. 791-802.

Y. Zhou, B. Yang, D. F. Wong, Y. Wan, and L. S. Chao,
“Uncertainty-aware curriculum learning for neural machine
translation,” in ACL, 2020, pp. 6934-6944.

S. Zhang, C. Gong, and E. Choi, “Knowing more about questions
can help: Improving calibration in question answering,” in ACL-
TJCNLP, 2021, pp. 1958-1970.

N. Kertkeidkachorn, X. Liu, and R. Ichise, “Gtranse: generalizing
translation-based model on uncertain knowledge graph embed-
ding,” in JSAI, 2020, pp. 170-178.

T. Glushkova, C. Zerva, R. Rei, and A. F. Martins, “Uncertainty-
aware machine translation evaluation,” in EMNLP-Findings, 2021,
pp. 3920-3938.

Z. Jiang, J. Araki, H. Ding, and G. Neubig, “How can we know
when language models know? on the calibration of language
models for question answering,” TACL, pp. 962-977, 2021.

S. Lin, J. Hilton, and O. Evans, “Teaching models to express their
uncertainty in words,” 2022.

K. Zhou, D. Jurafsky, and T. Hashimoto, “Navigating the grey
area: Expressions of overconfidence and uncertainty in language
models,” arXiv preprint arXiv:2302.13439, 2023.

A. Shen, D. Beck, B. Salehi, J. Qi, and T. Baldwin, “Modelling
uncertainty in collaborative document quality assessment,” in
Proceedings of the 5th Workshop on Noisy User-generated Text (W-
NUT 2019), 2019, pp. 191-201.

S. Jean, O. Firat, K. Cho, R. Memisevic, and Y. Bengio, “Montreal
neural machine translation systems for WMT’15,” in Proceedings
of the Tenth Workshop on Statistical Machine Translation, 2015, pp.
134-140.


[154] P. Koehn and R. Knowles, “Six challenges for neural machine
translation,” in Proceedings of the First Workshop on Neural Machine
Translation, 2017, pp. 28-39.

[155] S. Wang, Z. Tu, S. Shi, and Y. Liu, “On the inference calibration of
neural machine translation,” in ACL, 2020, pp. 3070-3079.

[156] T. Newberry and T. Ord, “The parliamentary approach to moral
uncertainty.”

[157] D. Hendrycks, “Natural selection favors ais over humans,” 2023.

21
