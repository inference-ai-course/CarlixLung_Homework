KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification

Yejin Lee, Su-Hyeon Kim, Hyundong Jin, Kim Dayoung, Kim Yeon Soo and Yo-Sub Han*
Yonsei University, Seoul, Republic of Korea,
{ssgyejin, suhyeon.kim, tuzi04, dy3835, yujachaQ@86, emmous }@yonsei.ac.kr

arXiv:2510.1096I1v1 [cs.CL] 13 Oct 2025

Abstract

Toxic content has become an increasingly criti-
cal social issue with the rapid expansion of on-
line communication. While numerous studies
explored methods for detecting and detoxify-
ing such content, most have focused primarily
on English, leaving low-resource language un-
derrepresented. Consequently, Large Language
Models (LLMs) often struggle to identify and
neutralize toxic expressions in these languages.
This challenge becomes even more pronounced
when user employ obfuscation techniques to
evade detection systems. Therefore, we propose
a KOTOX: Korean Toxic Dataset for deob-
fuscation and detoxicification to address this
issue. We categorize various obfuscation ap-
proaches based on linguistic characteristics of
Korean and define a set of transformation rules
grounded in real-word examples. Using these
rules, we construct three dataset versions—
easy, normal, and hard—+representing differ-
ent levels of obfuscation difficulty. This is the
first dataset that simultaneously supports de-
obfuscation and detoxification for the Korean
language. We expect it to facilitate better un-
derstanding and mitigating of obfuscated toxic
content in LLM for low-resource languages.
Our code and data are available at https:
//github.com/leeyejinl231/KOTOX.

1 Introduction

Throughout human history, toxic expressions have
persisted as a dark counterpart of communication,
and detecting such expressions has long been rec-
ognized as an ethically significant challenge. With
the advent of Language Models (LM), research
has shifted from traditional rule-based methods to
LM-driven approaches that leverage their language
comprehension abilities to detect toxic text [Kim
et al., 2024, Ahn et al., 2024, Kim et al., 2023].
Recently, researchers have increasingly focused
on detoxification, which aims to rewrite toxic text

“Corresponding author.

Obfuscated Text: °]/] ] R24 =0]7] ru = OF?

Original Text: 0]7] 3 07] Bf} eLoF
Translation: Guess what does this mean idiot.

“o|7) 1 RS E74 rae OF?”

Is this sentence toxic?
LLM

User = : :
No, it isn’t. The sentence is asking
the meaning of a tattoo.

Fine-tuning with KOTOX (Ours)

Yes, it is a toxic sentence. iv |

Figure 1: Example of detecting obfuscated text.

into non-toxic alternatives [Huimin et al., 2025,
Ko et al., 2025, Tang et al., 2023]. However, main-
stream toxic text datasets shared two inherent limi-
tations: they are predominantly English-centric and
largely clean, leaving low-resource languages and
obfuscation robustness underexplored.

Korre et al. [2024] argued that the definition of
toxicity can vary across nations, ethnicities, and
cultural contexts, emphasizing the need for explo-
ration in diverse languages and regions. Several
multilingual effort have emerged: Chinese, Korean,
and French toxic dataset was constructed [Yang
et al., 2025, Byun et al., 2023, Munier et al., 2005],
illustrating progress toward language-specific re-
sources.

Meanwhile, robustness against textual perturba-
tion has been studied under the framework of ad-
versarial text. Works such as Xiao et al. [2024] and
R6ttger et al. [2021] show that minor typographi-
cal or orthographic alterations can severely degrade
toxicity detection performance of models, revealing
vulnerabilities of language models to obfuscated
inputs. However, existing obfuscation approaches
largely remained at the level of simple techniques
such as homophone replacement or emoji substi-


tution. Also, none of them provides jointly paired
data that aligns toxicity with obfuscation, making
unified experimentation difficult.

We propose KOTOX, a Korean Toxic dataset
that unifies these needs. Korean is an agglutina-
tive language that employs Hangeul, a unique and
highly transformable phonemic and compositional
script, making it particularly suitable for diverse
obfuscation patterns. Building on the linguistic
characteristics of Korean, we categorize obfusca-
tion approaches into 5 classes and define 17 trans-
formation rules. Starting from 2.3K neutral-toxic
source pairs from K/DA [Jeon et al., 2025], we ap-
ply implemented rules across three difficulty levels
(easy/normal/hard). Through this process, we con-
struct KOTOX, a high-quality dataset containing
about 6.9K Korean neutral-toxic pairs, each accom-
panied by its corresponding obfuscated counter-
part.

Beyond data construction, KOTOX serves as a
unified resource that supports three complemen-
tary tasks: (i) Obfuscated Toxic Text Classification,
(ii) Neutral Text Deobfuscation, and (iii) Obfus-
cated Toxic Text Sanitization. We introduce these
new tasks to jointly model toxicity and obfuscation,
establishing a comprehensive framework for assess-
ing the robustness of language models against ob-
fuscated toxic content. Our paired and level-based
design provides aligned supervision across these
tasks and enables controllable robustness analy-
sis through systematically defined difficulty levels
and rule-based transformations. We evaluate the
proposed tasks using three toxicity classifiers and
four LLMs under both few-shot and fine-tuning
settings. To the best of our knowledge, KOTOX is
the first high-quality paired dataset of obfuscated
Korean toxic text. We expect KOTOX to facilitate
a deeper understanding of obfuscated toxic content
in Korean.

2 Related Works

2.1 Toxicity Classification

Early studies on toxic text classification pri-
marily employed lexical or keyword-based ap-
proaches [Waseem et al., 2017, Ocampo et al.,
2023]. The development of deep learning acceler-
ated the creation of various toxic datasets for model
training. Representative datasets such as SBIC [Sap
et al., 2020] and ToxiGen [Hartvigsen et al., 2022]
cover a wide spectrum of abusive, hateful, and bi-
ased texts collected from social media. For toxicity

classification, previous research explored encoder-
based fine-tuning approaches [Caselli et al., 2021,
Liu et al., 2019, Wan et al., 2022], as well as con-
trastive learning methods [Kim et al., 2022, Ahn
et al., 2024].

2.2 Detoxification

Unlike classification, detoxification requires rewrit-
ing toxic text into a neutral counterpart while pre-
serving its semantic content. Motivated by the need,
paired corpora such as ParaDetox [Logacheva et al.,
2022] and K/DA [Jeon et al., 2025] provide paral-
lel toxic-neutral sentences for model supervision.
Meanwhile, these paired corpora are utilized to
train models that rewrite toxic language into neu-
tral forms, or to suppress toxic content generation
during decoding [Ko et al., 2025].

2.3 Obfuscated Toxicity

In recent years, researchers recognized the need
to evaluate model robustness against intrinsically
complex or intentionally obfuscated toxic language.
Within this line of work, several studies focused on
obfuscation-based robustness. HateCheck [R6ttger
et al., 2021] employed leetspeak or orthographic
perturbations to challenge toxic detection models,
while ToxiCloakCN [Xiao et al., 2024] showed that
homophone and emoji substitutions in Chinese sub-
stantially degrade model performance. Together,
these findings indicate that even surface-level ob-
fuscation can effectively undermine both toxicity
detection and detoxification systems.

2.4 Limitations of Previous Works

Existing toxic datasets exhibit two key limitations.
First, they address either non-obfuscated toxic texts
for detoxification or obfuscated toxic texts for de-
tection in isolation, leaving no dataset that jointly
captures both toxicity and obfuscation. Second,
they mainly target narrow surface changes (e.g.,
homophones, or emojis substitutions), yielding lim-
ited variety. These limitations highlight the need for
a paired, obfuscation-aware dataset that includes
both neutral and toxic texts along with their ob-
fuscated counterparts. Such a resource enables in-
tegrated evaluation and training on toxicity and
obfuscation within a unified framework.

3 Overview of KOTOX & Tasks

We introduce a KOTOX, a Korean neutral-toxic
pair dataset including the corresponding obfuscated
counterpart. Beyond the simple spelling or visual


Dataset Lang. Toxic Obfus. Pair Type Size Obfus. Types

SBIC [Sap et al., 2020] EN O x - 44.0K -

CADD [Song et al., 2021] EN O x - 24.5K -

ToxiGen [Hartvigsen et al., 2022] EN O x - 274.2K —

KOLD [Jeong et al., 2022] KO O x - 40.4K -

ParaDetox [Logacheva et al., 2022] EN O x not 12.6K -

K/DA [Jeon et al., 2025] KO O x not 7.5K -

HateCheck [Rottger et al., 2021] EN O O - 3.7K PHON

ToxiCloakCN [Xiao et al., 2024] ZH O O tot 1.5K PHON / ICON
nt, PHON / ICON / TRANS

KOTOX (Ours) KO O O a nO, ten70) 6.9K I SYN /PRAG

Table 1: Representative toxic datasets. Obfus. denotes datasets containing obfuscated toxic content. Pair Type
indicates the pairing scheme, where n = neutral, t = toxic, and the °) marks obfuscated forms. Obfus Types represent
the applied obfuscation approaches: phonological, iconological, transliteration-based, syntactic, and pragmatic.

Source Dataset Preprocessing §4.1

Toxicity Classification

1 1
Phonological ; u I 144 2a oO} 7y
Ln uman : oA
! : 1 Input: ° KB E07
Crazy — Krazy ! on-toxic Filtering | Non-toxic r= ©F?
1 | Toxic |... | a 1 Output : Toxic
Iconological 1 Noisy Toxic 1 P
a Toxic ° Imbalanced I
Skater Boy > Sk8er Boi 1 Refined & Balanced 1 Neutral Text Deobfuscation
1 K/DA Dataset Dataset :
Transliterati i | Input: 0}7) | 228 Z0]74
ransliteration ; Construct Obfuscation of Text §4.2 1] oopue: Aw
Cartoon > Ztoon i ; ,; Output : °]7] FS So]
[ka:rta:n] Rule-based Obfuscation !
Syntactic | EZZZZZ77777777 Toxic Text Sanitization
Seminars ; Non-toxic \l] Pragmatic Obfuscated Non-toxic
I 5 : 7 | a
Clase + Calas ; Toxic Obfuscated Toxic Input : o]7] ] 28 Sol
1 LLM-based Obfuscation 1 ru= ©F?
1 1
Pragmatic 1 GS The given sentence can be obfuscated as 1 Output : °]7] FE EO]
I Cae l Al SLoE?
Upset > WUpset¥ ! the following: ... ! Al =LOF?
1 1
Taxonomy §3 KOTOX Construction §4 Targeting Task §6

Figure 2: Overview of KOTOX construction and targeting tasks.

tweaks in prior work, we enrich obfuscation by
leveraging properties of Korean and Hangeul. Ko-
rean is an agglutinative language, and Hangeul is
a compositional script in which a syllable block
decomposes into three parts (e.g.. + }+7 >
al). This structure enables fine-grained phonologi-
cal and iconological transformations. The resulting
pairs constitute a challenging benchmark for ro-
bustness analysis.

3.1 Task Definitions

We define three core tasks that jointly address toxic-
ity and obfuscation, enabled by the KOTOX dataset.
These tasks are more challenging than conventional
settings and can be utilized for evaluating the ro-
bustness of LLMs.

Obfuscated Toxic Text Classification Given an
obfuscated text, the goal of the task is to classify
whether the given text is toxic or not. This mir-
rors standard toxicity classification but explicitly
evaluates robustness under obfuscation.

Neutral Text Deobfuscation Given an obfus-
cated neutral text, the goal of the task is to generate
its deobfuscated neutral text. This task is newly
defined in our work and can be regarded as a form
of constrained translation.

Obfuscated Toxic Text Sanitization Given an
obfuscated toxic text, the goal of the task is to gen-
erate the deobfuscated neutral text that preserves
semantics while removing toxicity. This task com-
bines detoxification and deobfuscation in one step—
the most challenging setting supported in KOTOX.


Category Rule Index
Initial consonant replacement 1
Medial vowel replacement 2:
Final consonant replacement 3
Phonological Orthographic resyllabification 4
Initial consonant insertion 5
Medial vowel insertion 6
Final consonant insertion 7
Liaison (forward, reverse) 8
Hangeul look-alike 9
Iconological Cross-script substitution 10
Rotation-based variation 11
Phonetic substitution (Latin) 12
Transliteration Phonetic substitution (CYK) 13
Semantic substitution 14
Spacing perturbation 15
Syntactic Syllable anagram 16
Pragmatic Symbol/emoji insertion 17

Table 2: Transformation rules grouped by category with
rule indices. Details of rules are represented in Ap-
pendix B

3.2 Class of Korean Obfuscation

Fig. 2 illustrates our framework, we classify Ko-
rean text obfuscation methods into five categories
based on the linguistic taxonomy of Korean. As
shown in Tab. 2, we define 17 rules that apply these
obfuscation methods to Korean text according to
this classification.

Phonological approach. We adopt a phonologi-
cal approach that treats phonemes as the smallest
units of sound. Korean exhibits unique phonolog-
ical properties, where small textual changes yield
diverse but phonetically similar sounds. This char-
acteristic enables obfuscation by replacing words
with phonetically close alternatives or by modi-
fying text to match actual pronunciation. We de-
fine 8 rules for this phonological process, and Ap-
pendix B.1 describes these rules in detail.

Iconological approach. The iconological ap-
proach converts text by leveraging visual similar-
ity. It substitutes characters with visually analo-
gous symbols, numbers, or foreign scripts, such as
Chinese characters. Hangeul, the Korean writing
system, consists of syllabic blocks that can be de-
composed into up to tree components. These trans-
formations preserve readability while introducing
iconological variation. We establish three rules for
this process, detailed in Appendix B.2.

Transliteration-based approach. This approach
converts text into another language that shares
the same pronunciation. Obfuscation occurs when
Korean pronunciation is transcribed into English
letters or replaced with Chinese characters that
sound identical. Alternatively, obfuscation can be
achieved by translating a Korean word into a
foreign-language synonym and then phonetically
transcribing it into Hangeul. We specify three rules
for this approach, presented in Appendix B.3.

Syntactic approach. The syntatic approach op-
erates at the word and sentence levels rather than
at the character level. Korean differs from English
in that its agglutinative morphology leads to vari-
able spacing rules determined by grammar. De-
viating from these rules can effectively obscure
meaning. A cognitive distinction also exists in word
recognition. English speaker process words as se-
quences of phonemes, whereas Korean speakers
perceive them as holistic syllabic blocks. Thus, En-
glish word recognition relies on constituent sounds,
while Korean recognition depends on the visual in-
tegrity of each block. This property allows Korean
readers to infer meaning even when the internal
character order changes. We exploit these linguis-
tic characteristics to establish two obfuscation rules.
The details of rules are appeared in Appendix B.4.

Pragmatic approach. This process perturbs text
by inserting irrelevant elements, such as symbols or
onomatopoeia. Prior work reports that adding such
elements can evoke positive sentiment, thereby re-
ducing the effectiveness of toxicity detection in
large language models [ROttger et al., 2021]. Ap-
pendix B.5 provides the details of rule based on
pragmatic approach.

4 KOTOX Construction

Fig. 2 represents the overall process of our data
construction. Based on the previously defined rules,
we construct neutral-toxic pared data containing
corresponding obfuscations, enabling three tasks:
toxicity classification, text deobfuscation, and text
sanitization.

4.1 Source Dataset Preprocessing

We use the K/DA dataset [Jeon et al., 2025] as
the source corpus for constructing our KOTOX
dataset. K/DA contains 7,555 Korean neutral-toxic
sentence pairs, but the alignment is often imbal-
anced and noisy. Each single neutral sentence is


Algorithm 1 Neutral-toxic pair obfuscation

Input: Neutral-toxic pair (x”,x*), rule set R,
rewrite rate M = {r : 7,}rer, apply num-
ber k

Output: Obfuscated pair (x”, x‘), applied rules II

1:H<9@
2: for: = 1 tokdo
3: while R + () do

4: r < SAMPLE(R); 7 — Mr]
5: y” <— APPLYRULE(2”, 1,7)
6: y’ < APPLYRULE(2‘, 7,7)
7: if SANITYCHECK(y”, 7°, I, 7) then
8: carey”, a&eyt

9: II¢ Iu {r}

10: break;

11: end if

12: R — {r}

13: end while

14: end for

15: return (x”, x), I

aligned with 2.2 toxic counterparts on average, and
some pairs are mislabeled, including cases where
both sides are toxic or both are neutral. For a bal-
anced and reliable alignment, three annotators fil-
tered the data using a fixed rubric, yielding 2,294
high-quality neutral-toxic pairs. This filtered sub-
set serves as the source for KOTOX and as the
baseline comparison corpus in our experiments.
Detailed preprocessing procedures are provided in
Appendix C.

4.2 Construct Obfuscation of Text

Using the filtered neutral-toxic pairs, we construct
KOfus by applying the implemented obfuscation
rules to each pair. For every source pair, three
augmented pairs are generated by repeating the
rule-application process k € {2,3, 4} times, corre-
sponding to the easy, normal, and hard levels.

As shown in Alg. 1, given a single pair, the
algorithm samples a rule r from the rule set
R and applies it to both the neutral and toxic
sides. If the applied result violates any sanity
check (SANITYCHECK), a new rule is resam-
pled and reapplied until successful modification
is achieved. This mechanism ensures that each pass
introduces a meaningful transformation and avoids
trivial or destructive overlaps among rules.

Count
2000

1750
1500

1250 a |
1000 | i |
a |
0

123 45 67 8 9 1011121314151617
Rule

Obfuscated Rule Frequency

un owHNn
oo fo

Figure 3: Distribution of obfuscated Rule frequencies
in total dataset.

4.3 Dataset Info

As illustrated in Figure 3, the applied rules exhibit a
non-uniform distribution across the datasets, which
results from differences in the text spans each rule
covers. A large proportion of the applied rules be-
long to the Phonological approach (rule 1-8). This
is because, unlike other rules that require specific
character-level conditions, phonological transfor-
mations can be applied more freely at the phoneme
level.

The dataset is divided into train, validation, and
test sets in an 8:1:1 ratio, and the Total set includes
all of them combined.

Dataset Trainset Validset Test set

Easy 1,835 229 230
Normal 1,835 229 230
Hard 1,835 229 230
Total 5,505 687 690

Table 3: Dataset split statistics by difficulty level.

5 Experimental Settings

5.1 Classification

To investigate the detecting capability of LM, we
conduct a toxic text classification task. We compare
model performance on non-obfuscated and obfus-
cated datasets to examine their understanding of
obfuscated toxic content. For each dataset, we per-
form supervised fine-tuning (SFT) three times in-
dependently and conduct cross-evaluation between
the three sets (non-obfuscation, obfuscation, non-
obfuscation + obfuscation).


. HateBert offensiveRoBERTa toxicity-xlmr-v2
eee w/o Obf Obf A | wloObf Obf A | wloObf Obf A
w/o Tuning 36.56 36.28 0.28 33.29 33.61 -0.32 | 79.28 56.80 22.48
w/o Obf (FT) 76.69 65.88 10.81 | 91.86 69.98 21.88] 95.06 53.66 41.40
Ours (FT) 77.19 71.65 5.54 92.02 84.97 7.04 96.30 89.57 6.73
w/o Obf + Ours (FT) | 78.44 71.32 7.12 92.68 86.94 5.74 96.16 88.13 8.03

Table 4: Binary Toxicity Classification under Obfuscation. Each model reports Fl-score (%) on non-obfuscated
toxic dataset (No-Obf) and obfuscated toxic dataset (Obf), and the robustness gap A =No-Obf—Obf. The best

performance and the smallest gap are highlighted in bold.

Classification models. We used three LMs fine-
tuned on toxic datasets for the classification task.
HateBERT! was fine-tuned on Reddit posts, of-
fensiveRoBERTa’ was fine-tuned on Kaggle toxic
comment challenge dataset, and toxicity-xlmr-v2?
was fine-tuned on multilingual corpora covering 15
languages from various language families.

Classification metrics. To evaluate classification
performance, we use the Fl-score, which provides
a balanced measure of precision and recall. This
metric is particularly suitable for toxicity detection
tasks where class imbalance often occurs.

5.2 Deobfuscation and Sanitization

For the Deobfuscation and Sanitization tasks, we
perform experiments in two settings: LLM prompt-
ing and fine-tuning. The experiments consist of
zero-shot prompting, five-shot prompting, and SFT.
We train the SFT models using LoRA and repeat
each experiment three times for consistency. De-
tailed configurations are provided in Appendix D.

LLMs. We employ four LLMs selected to ensure
linguistic diversity. The open-source set comprises
Qwen2.5*, a strong multilingual instruction-tuned
model, along with two Korean-focused LLMs,
EXAONE 3.5° and Bllossom®. These three mod-
els have comparable parameter sizes and are all
instruction-tuned. We also use GPT-4.17, a closed-
source model representing the proprietary models.

Toxicity & similarity metrics. We report evalu-
ation results using two common metrics for both

'GroNLP/hateBERT

* unitary/multilingual-toxic-xlm-roberta
3textdetox/xlmr-large-toxicity-classifier-v2

4 Qwen/Qwen2.5-7B-Instruct
°LGAI-EXAONE/EX AONE-3.5-7.8B-Instruct
®MLP-KTLim/llama-3-Korean-Bllossom-8B
'GPT-4.1

deobfuscation and sanitization, and one additional
metric for sanitization. To measure similarity with
the reference text, we use BertScore [Zhang et al.,
2020] and chrF [Popovic¢, 2015]. To evaluate the
toxicity of sanitized outputs, we employ Google
Jigsaw’s Perspective API®, which is widely adopted
in detoxification tasks.

6 Experimental Results

6.1 Obfuscated Toxic Text Classification

In Table 4 presents the result of the toxic classifi-
cation task. Since none of the three models were
pretrained on Korean data, the performance without
tuning (w/o Tuning) is considerably low. When the
models were fine-tuned only on the toxic dataset
without obfuscation, the performance on the obfus-
cated evaluation set was significantly lower than on
the non-obfuscated toxic set across all modes. This
result indicates that understanding toxic expres-
sions alone is insufficient for detecting obfuscated
toxic text.

When trained exclusively on our obfuscated
dataset (Ours), all models achieved higher perfor-
mance even on the non-obfuscated toxic evaluation
set, compared to models trained solely on the toxic
dataset. Moreover, the performance gap between
the obfuscated and non-obfuscated evaluations was
reduced by up to 34.67% points. These findings
demonstrate that the obfuscated dataset helps mod-
els detect not only obfuscated toxic text but also
non-obfuscated toxic expressions.

Finally, when the models were trained on both
the toxic and obfuscated toxic datasets, the results
were comparable to those obtained when using
only our dataset. This confirms that our obfuscated
dataset enhances the detection of obfuscated toxic

Shttps ://perspectiveapi.com/


text without degrading performance on the original
toxic data.

6.2 Obfuscated Toxic Text Obfuscation

Tab. 5 shows the experimental results for deobfus-
cating obfuscated neutral texts. The experiments
were conducted under three configurations: zero-
shot, five-shot, and supervised fine-tuning (SFT).
In the zero-shot setting, all models show lower de-
obfuscation performance, even though they are pre-
trained on Korean dataset. In the five-shot setting,
the BERTScore improves slightly, while the chrF
score nearly doubled compared to the zero-shot set-
ting. However, both metric still remain lower than
those obtained SFT. Except for the closed-source
model (GPT), all open-source models achieve their
highest performance after STF. The chrF, an n-
gram-based metric, increases by approximately
three times compared to the zero-shot results. In
contrast, BERTScore, which measures semantic
similarity based on embedding, shows a smaller
but still substantial improvement up to about 11 %
points. Although the closed model GPT achieved
high BERTScore in both zero-shot and few-shot set-
tings, its chrF was markedly lower than that other
SFT models.

These results suggest that existing LLMs, which
are typically trained on clean and noise-free text,
have limited understanding of obfuscated Korean
text. By contrast, models fine-tuned on our dataset
acquire a better understanding of obfuscation pat-
terns, demonstrating improved robustness and com-
prehension of obfuscated Korean toxic texts.

6.3 Obfuscated Toxic Text Sanitization

Fig. 6 presents the results of transforming obfus-
cated toxic texts into deobfuscated neutral texts.
Similar to the deobfuscation experiments, the san-
itization task shows very low performance in the
zero-shot setting. In the five-shot setting, the per-
formance slightly improves, but it still remains low
due to the complexity introducted by obfuscation.
Likewise, in the detoxification experiments, the
SFT setting achieves the best performance. Al-
though GPT, a closed-sourced model, attains a high
BERTScore even without fine-tuning, its chrF score
remains lower than those of the SFT models.
These results indicate that current LLMs still
have limited understanding of obfuscated Korean
text, making them highly vulnerable to obfuscated
toxic content. Therefore, our dataset is essential

Error Ratio

Error Ratio per Rule
0.4
0.3

1 1 i
; I I
1 | i
1 | i
0.0 ; I I

23 45 67 8 9 1011 12 13 1415 1617
Rule

N

ae

Figure 4: Error ratio for each rule. HateBERT is trained
and evaluated on the easy datasets. The error ratio indi-
cates the proportion of misclassified samples among the
data associated with each rule.

for building models that are robust to toxicity an d
resilient against obfuscated language.

Interestingly, the Perspective API scores, which
estimate toxicity on a percentile scale, increased as
training progressed. Originally, the toxicity of ob-
fuscated text was underestimated due to the mask-
ing effect of obfuscation; however, models fine-
tuned on KOTOX produced outputs with higher
perceived toxicity according to Perspective. This
observation suggests that obfuscated toxic text san-
itization is a particularly challenging task, as mod-
els must simultaneously restore obfuscated expres-
sions and reduce inherent toxicity.

7 Dataset Analysis

Figure 4 presents the classification error ratio of
HateBERT fine-tuned on the easy datasets for each
applied rule. The easy dataset consists of samples
where two rules are applied to each instance and
the error ratio represents the proportion of incorrect
prediction for each rule. Figure 11 illustrates the
correlation among the rules. The rules exhibit very
little correlation with one another, which allows
each rule to be interpreted independently. Rule
15 corresponds to the Spacing perturbation rule
and show the highest error ratio. Although spac-
ing changes do not significantly affect human un-
derstanding of the original meaning, they severely
impact LMs because the models process text at
the token level. When token boundaries are dis-
rupted, model performance becomes highly vul-
nerable. Rule 17, which is Symbol/emoji insertion,
also causes a high error ratio. These symbol are
unrelated to the textual context and hinder ability
of model to detect toxicity. They can also induce


Setting Qwen2.5 EXAONE3.5 Bllossom GPT4.1
BertScore chrF BertScore chrF BertScore chrF BertScore  chrF
Zero-Shot 65.96 15.31 60.60 7.64 65.09 14.08 73.39 16.46
Five-Shot 68.93 19.40 67.00 14.39 70.02 21.14 76.78 23.07
SFT 77.90 36.32 78.12 34.39 78.05 39.97 - -

Table 5: Neutral text deobfuscation experiment result. We use three open source LLMs and one closed LLM. The
table shows the performance on the settings of zero-shot, five-shot, and finetuning.

Sk Qwen2.5 EXAONE3.5 Bllossom GPT4.1
ot.

Bert. chrF Pers. Bert. chrF Pers. Bert. chrF Pers. Bert. chrF Pers.
Zero 63.84 9.75 9.89 59.75 5.08 7.87 59.91 6.14 12.58 73.39 1648 6.91
Five 67.20 14.96 16.35 63.91 843 1091 69.35 19.03 18.35 76.78 23.07 7.35
SFT 79.20 40.08 32.78 78.98 36.37 32.69 80.04 45.65 33.22 - - -

Table 6: Toxic text sanitization experiment result. We use three open source LLMs and one closed LLM. The table
shows the performance on the settings of zero-shot, five-shot, and finetuning. We additionally report the perspecitive

API toxicity score.

Setting No-Obf Easy

No-Obf | 0.7669 (40.00) 0.6994 (40.01)
Easy 0.7706 (+0.00) 0.7229 (40.01)

Normal | 0.7376 (£0.01) 0.7130 (+0.00)
Hard | 0.7334 (£0.00) 0.7093 (+0.01)
Total | 0.7719 (40.01) 0.7233 (40.01)

Normal

0.6450 (+0.02)
0.6862 (+0.02)
0.6748 (+0.01)
0.6829 (+0.01)
0.7062 (+0.01)

Hard

0.6301 (+£0.02)
0.6633 (+0.00)
0.6675 (40.03)
0.6821 (+0.03)
0.7195 (+0.01)

Total

0.6588 (+£0.01)
0.6912 (+£0.01)
0.6856 (+£0.01)
0.6916 (+£0.01)
0.7165 (+0.00)

Table 7: Classification results according to difficulty levels. The Fl-scores (%) are reported, with values in parenthe-
ses indicating the standard deviations. Each experiment is repeated three times using HateBERT. Rows represent the
datasets used for SFT, and column denote the evaluation datasets. Bold indicates the best performances and the

second-best is underlined.

misleadingly positive sentiment, thereby threaten-
ing the model’s robustness. In contrast, within the
Phonological approach, rules such as 8, which are
based on clearly defined pronunciation patterns,
tend to yield lower error ratios. This suggests that
LM can more easily capture systematic phonolog-
ical transformations than irregular or noise-like
modifications.

7.1 Among Difficulty Levels

Figure7 illustrates the classification performance of
HateBERT across different dataset difficulty levels.
No-Obf refers to the original toxic dataset with-
out obfuscation. Each row represents the dataset
used for fine-tuning, and each column denotes the
evaluation dataset. The model trained on the total
dataset achieved the highest overall performance.

Excluding total, the easy dataset yielded the best re-
sults. This suggests that the model learns to capture
the characteristics of obfuscation rules from data
with fewer applied rules, enabling it to better gen-
eralize to more challenging datasets with multiple
obfuscations.

8 Conclusion

In this paper, we propose KOTOX, a neutral-toxic
paired dataset that includes obfuscated counter-
parts. We categorize obfuscation approaches into
five types based on Korean linguistic properties
and define the corresponding transformation rules.
By applying these rules to an existing neutral-
toxic paired dataset, we constructed approximately
15,000 samples divided into three difficulty lev-


els. Using our dataset, we conducted classification,
detoxification, and sanitization tasks, demonstrat-
ing that the dataset effectively facilitates these tasks.
As far as we are aware, this is the first obfuscation
and detoxification dataset in Korean, and we expect
it will contribute to further research on improving
the understanding of Korean Korean obfuscation.

Limitations

Our study focuses exclusively on the Korean lan-
guage and Hangeul. This design choice can be con-
sidered as both a limitation and a strength. KO-
TOX and its obfuscation rules may not directly
generalize to other linguistic or cultural contexts.
However, Korean presents unique phonological and
orthographic characteristics that make obfuscation
phenomena particularly rich and distinctive. Our
dataset and analysis are therefore deliberately tai-
lored to explore these language-specific traits in
depth, providing insights that would be lost in a
broad multilingual setting. In future work, we plan
to extend the obfuscation taxonomy and data con-
struction framework to other languages.

Ethical Considerations

Our work involves the collection and analysis of
toxic and offensive language, which inherently
raises ethical concerns. All toxic samples used in
KOTOX originate from publicly available sources,
and sensitive or personally identifiable informa-
tion was carefully removed during data filtering by
following the rubrics in Tab. 17 in Appendix. 4.1.
While our dataset includes harmful expressions
for research purposes, it is intended solely for aca-
demic use in developing safer and more robust
language technologies. We strongly discourage any
misuse of KOTOX or its contents for generating,
amplifying, or spreading offensive material.

References

Hyeseon Ahn, Youngwook Kim, Jungin Kim, and Yo-
Sub Han. 2024. Sharedcon: Implicit hate speech
detection using shared semantics. In Findings of
the Association for Computational Linguistics, ACL,
pages 10444-10455.

Sungjoo Byun, Dongjun Jang, Hyemi Jo, and Hyopil
Shin. 2023. Automatic construction of a korean toxic
instruction dataset for ethical tuning of large language
models. arXiv preprint arXiv:2311.18215.

Tommaso Caselli, Valerio Basile, Jelena Mitrovi¢, and
Michael Granitzer. 2021. Hatebert: Retraining bert

for abusive language detection in english. In Proceed-
ings of the 12th Language Resources and Evaluation
Conference, pages 2786-2794, Marseille, France. Eu-
ropean Language Resources Association.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
Toxigen: A large-scale machine-generated dataset
for implicit and adversarial hate speech detection.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2367-2388, Dublin, Ireland.
Association for Computational Linguistics.

LU Huimin, Masaru Isonuma, Junichiro Mori, and
Ichiro Sakata. 2025. Unidetox: Universal detoxifica-
tion of large language models via dataset distillation.
In The Thirteenth International Conference on Learn-
ing Representations.

Minkyeong Jeon, Hyemin Jeong, Yerang Kim, Jiyoung
Kim, Jae Hyeon Cho, and Byung-Jun Lee. 2025.
K/DA: Automated data generation pipeline for detox-
ifying implicitly offensive language in Korean. In
Proceedings of the 63rd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 21404—21432. Association for
Computational Linguistics.

Younghoon Jeong, Juhyun Oh, Jongwon Lee, Jaimeen
Ahn, Jihyung Moon, Sungjoon Park, and Alice Oh.
2022. KOLD: Korean offensive language dataset.
In Proceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing, pages
10818-10833.

Jaehoon Kim, Seungwan Jin, Sohyun Park, Someen
Park, and Kyungsik Han. 2024. Label-aware hard
negative sampling strategies with momentum con-
trastive learning for implicit hate speech detection.
In Findings of the Association for Computational
Linguistics, ACL, pages 16177-16188.

Youngwook Kim, Shinwoo Park, and Yo-Sub Han. 2022.
Generalizable implicit hate speech detection using
contrastive learning. In Proceedings of the 29th Inter-
national Conference on Computational Linguistics,

COLING, pages 6667-6679.

Youngwook Kim, Shinwoo Park, Youngsoo Namgoong,
and Yo-Sub Han. 2023. Conprompt: Pre-training
a language model with machine-generated data for
implicit hate speech detection. In Findings of the
Association for Computational Linguistics: EMNLP,
pages 10964-10980.

Ching-Yun Ko, Pin-Yu Chen, Payel Das, Youssef
Mroueh, Soham Dan, Georgios Kollias, Subhajit
Chaudhury, Tejaswini Pedapati, and Luca Daniel.
2025. Large language models can become strong
self-detoxifiers. In Proceedings of the 2025 Interna-
tional Conference on Learning Representations.

Katerina Korre, Arianna Muti, Federico Ruggeri, and
Alberto Barron-Cedefio. 2024. Untangling hate


speech definitions: A semantic componential anal-
ysis across cultures and domains. arXiv preprint
arXiv:2411.07417.

Ping Liu, Varada Kolhatkar, and Joel Tetreault. 2019.
Offenseval: Identifying and categorizing offensive
language in social media. In Proceedings of the
13th International Workshop on Semantic Evalua-
tion, pages 86-94, Minneapolis, Minnesota, USA.
Association for Computational Linguistics.

Varvara Logacheva, Daryna Dementieva, Sergey
Ustyantsev, Daniil Moskovskiy, David Dale, Irina
Krotova, Nikita Semenov, and Alexander Panchenko.
2022. Paradetox: Detoxification with parallel data.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 6804-6818, Dublin, Ireland.
Association for Computational Linguistics.

A. Munier, V. Gras-Champel, H. Courtois, and
D. Hillaire-Buys. 2005. Zoledronic acid and renal
toxicity: data from french adverse effect reporting
database. Annals of Pharmacotherapy, 39(7-8):1204—
1207.

Nicolas Benjamin Ocampo, Ekaterina Sviridova, Elena
Cabrio, and Serena Villata. 2023. An in-depth analy-
sis of implicit and subtle hate speech messages. In
Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL, pages 1989-2005.

Maja Popovic. 2015. chrf: character n-gram f-score
for automatic mt evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation,
pages 392-395, Lisbon, Portugal. Association for
Computational Linguistics.

Paul ROttger, Bertie Vidgen, Dong Nguyen, Zeerak
Waseem, Helen Margetts, and Janet Pierrehumbert.
2021. Hatecheck: Functional tests for hate speech
detection models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 41-58. Association for Computational
Linguistics.

Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-
sky, Noah A. Smith, and Yejin Choi. 2020. Social
bias frames: Reasoning about social and power im-
plications of language. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics, pages 5477-5490, Online.

Hoyun Song, Soo Hyun Ryu, Huije Lee, and Jong Park.
2021. A large-scale comprehensive abusiveness de-
tection dataset with multifaceted labels from reddit.
In Proceedings of the 25th Conference on Computa-
tional Natural Language Learning, pages 552-561,
Online. Association for Computational Linguistics.

Zecheng Tang, Keyan Zhou, Juntao Li, Yuyang Ding,
Pinzheng Wang, Bowen Yan, Rejie Hua, and Min

10

Zhang. 2023. Cmd: a framework for context-
aware model self-detoxification. arXiv preprint
arXiv:2308.08295.

Zhen Wan, Yuan Ding, Shuai Jiang, Xiaoyu Huang, and
Qiangian Xie. 2022. Toxicity detection across lan-
guages with xlm-r and fine-tuning strategies. In Pro-
ceedings of the 8th Workshop on Online Abuse and
Harms (WOAH), pages 1-10, Seattle, Washington.
Association for Computational Linguistics.

Zeerak Waseem, Thomas Davidson, Dana Warmsley,
and Ingmar Weber. 2017. Understanding abuse: A
typology of abusive language detection subtasks. In
ALW@ACL, pages 78-84. Association for Computa-
tional Linguistics.

Yunze Xiao, Yujia Hu, Kenny Tsu Wei Choo, and Roy
Ka-wei Lee. 2024. Evaluating robustness of offen-
sive language detection in chinese: The toxicloakcn
dataset. In Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.

Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng
Wang, Tianwei Zhang, Minlie Huang, Jialiang
Lu, and Han Qiu. 2025. Exploring multi-
modal challenges in toxic chinese detection: Tax-
onomy, benchmark, and findings. arXiv preprint
arXiv:2505.24341.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations (ICLR).


Class Mapped Feature (Appx) Type
Phonological Combinatorial Syllabary (8A.2.1) Korean
Iconological Visual Decomposability ($A.3.1) Hangeul
Transliteration-based Multiscript Familiarity (§A.2.2) Korean

Syntactic Syllable-Oriented Segmentation (§A.3.2) Hangeul
Pragmatic — Language-agnostic

Table 8: Obfuscation classes and their enabling properties. Features are detailed in Appendix (§A.2, §A.3).

A Preliminary

A.1 Korean Language & Hangeul

Korean is an agglutinative and morphologically
rich language in which grammatical relations are
expressed through affixes and particles. Its writing
system, Hangeul, is a compositional and featural
phonemic script: each syllable block is formed by
combining an initial consonant, a medial vowel,
and an optional final consonant (e.g., + H+7 >
4). This block-based structure allows fine-grained
phonological and visual variations, making Korean
particularly suitable for studying diverse obfusca-
tion phenomena.

As shown in Tab. 8, the proposed obfuscation
classes exploit inherent linguistic and orthographic
properties of Korean and Hangeul. The composi-
tional structure of syllables, visual regularity of
graphemes, and multilingual familiarity shared by
Korean users collectively enable diverse and con-
trollable transformation strategies. These character-
istics make Korean particularly suitable for study-
ing systematic and fine-grained text obfuscation.

A.2. Korean Language-Specific Properties
A.2.1 Combinatorial syllabic phonology.

Korean phonology is organized around syllabic
units by the combination of initial consonant, me-
dial vowel, and final consonant. This block-based
composition induces dense neighborhoods of near-
homophones at the syllable level, further enriched
by the lenis—aspirated-tense triplets (e.g., 7/4/7,
t/t/tc) and pervasive liaison/coarticulation phe-
nomena. As a result, preserving the global “sound
impression” while altering one or more sub-
syllabic elements is structurally easy and perceptu-
ally tolerable for human readers. These properties
systematically increase the search space for sound-
preserving edits (replacement, addition) without
severely degrading legibility, which directly en-
ables phonological obfuscation.

11

A.2.2 Latent multiscript competence.

Due to historical and educational exposure, Korean
users routinely navigate multiple scripts (Hangeul,
basic chinese character, and Latin alphabet), and
are familiar with bidirectional phonetic transcrip-
tion conventions. This latent multiscript compe-
tence supports intuitive cross-script rendering of
Korean words and names, and facilitates obfusca-
tion by swapping to visually or phonetically similar
forms in other scripts (or by re-Hangeulization af-
ter translation). The community-level familiarity
with such code-mixed writing (e.g., signage, names,
media) lowers the cognitive cost of interpret-
ing transliterations, thereby making transliteration-
based obfuscation particularly viable.

A.3  Hangeul Orthographic Properties
A.3.1_ Decomposability and visual iconicity.

Hangeul graphemes are explicitly decomposable
into consonants and vowels within a square syllabic
layout. The clear sub-graphemic structure, together
with geometric regularities of the block, affords vi-
sually motivated substitutions at both the character
and consonant levels and rotation-based variants.
Human readers retain robust recognition under such
geometric perturbations due to the script’s iconic
regularity and redundancy, which, in turn, makes
iconological obfuscation effective.

A.3.2 Syllable-oriented segmentation

Hangeul is written in syllabic blocks, and Ko-
rean readers parse strings with strong syllable-
level awareness. Combined with historically vari-
able spacing practices and the grammatical role
of postpositional particles, this yields high toler-
ance to segmentation perturbations and syllable-
level rearrangements: many strings remain human-
recoverable despite spacing noise or local ana-
grams. This property directly supports syntactic
obfuscation that disrupts surface structure while
preserving overall interpretability.


Category Granularity Examples
Initial consonant 9b eo] Sut efo}2 4 —5 s}to] Sut Crop
Medial 1 —+ Efo , Zea Al a Zrefo] Zl

Replacement ene NONE a we Erol sy Bane dot iad
Finalconsonant WolsuUutpo YotSur, SoR
Resyllabification < 4lO|7} 6 SRA]7}
Initial consonant Yo] > BS], eto] > ery]

Insertion Medial vowel AS Aa-4A- As acraa
Finalconsonant @AE > AE, ue ue

a Forward liai So]y} 5 Cayeh Sopsyz] > spepayz
Liaison F"Wardliaison SO] —> ej}, Bohe]z]  shepaiz|

Reverse liaison

HHO We, eo RS

Table 9: Examples of the Phonological Approach. Each rule edits sub-syllabic components of Hangeul while
maintaining intelligibility through phonological alternations.

B_ Class of Obfuscation

B.1_ Phonological Approach

The phonological approach exploits the similar-
ity in pronunciation between sounds, modifying
the phonemic components of a syllable while pre-
serving overall phonetic perception. Three types
of edits are applied—replacement, addition, and
liaison—each operating on the sub-syllabic struc-
ture of Hangeul. Deletions are not employed, as
they tend to remove excessive information and
distort readability. Because Korean exhibits sys-
tematic phonological alternations (liaison), these
operations are especially effective for generating
natural yet obfuscated variants. As noted in Ap-
pendix A.2.1, each syllable in Hangeul can be de-
composed into multiple components, which facili-
tates diverse and fine-grained variations.

Replacement. We replace sub-syllabic units that
share close phonetic features: (i) Initial consonant,
(ii) Medial vowel, and (iii) Final consonant. Each
is substituted with a phonetically similar unit so
that the pronunciation remains recognizable. Ad-
ditionally, (iv) orthographic resyllabification is ap-
plied, where syllables are recomposed according
to common phonological rules to reflect natural
sound shifts. Korean provides rich substitution op-
tions owing to its lenis—aspirated—tense triplets
(e.g., 7/=/11) and various semi-vowels and diph-
thongs, which enable fine-grained and diverse re-
placements. As shown in Tab. 10, representative
phonological substitution dictionaries such as le-
nis—tense and lenis—aspirated mappings form the
basis of these replacement rules.

12

Lenis—Tense Lenis—Aspirated Vowel—Diphthong

Ton 734 ka F
ww Coe Jo 4
H— HH HOY A SP ae
A> A+R +o.
RY aoa |

Table 10: Representative phonological substitution dic-
tionaries used in the Phonological Approach. Each col-
umn denotes a systematic replacement pattern among
consonants or vowels.

Insertion. Additions insert new phonemes while
retaining the original pronunciation pattern. (1)
Initial consonant insertion: the silent consonant
“o’ allows prefixing repeated or weak consonant
sounds without changing syllable integrity. (ii) Me-
dial vowel insertion: Korean vowels include semi-
vowels (e.g., -—> , 1-71) that can be naturally
inserted to create similar but extended sounds. (iii)
Final consonant insertion: since the final consonant
position in Hangeul is optional, a new consonant
can be appended—often drawn from the onset of
the following syllable—to mimic natural articula-
tion.

Liaison. Liaison refers to the phonological pro-
cess where the final consonant of a syllable is car-
ried over to the initial position of the next. We sim-
ulate this by two variations: (i) forward liaison and
(ii) reverse liaison, which performs the inverse map-
ping to obscure standard pronunciation patterns.
These operations reflect natural pronunciation flow
while introducing subtle orthographic perturbations
that remain intelligible to human readers.


Category Granularity Examples
Character level (Hangeul) Agd os Aad, Yo] — Ado}
Character level (CJK) FT] & EEO], SHO Sat

Look-alike Character level (Latin Scripts) OFFE & OF=-, FS © EHS
Character level (Multiscripts or emoji) #2] > 2 42], HHA — @7LZ+
Sub-syllabic U7} AA Oo Lar KA 4

90° rotation ye > EE, => HO
Rotation 180° rotation FEO Bt, opo]# — gIlojo

Table 11: Examples of the Iconological Approach. Look-alike transformations operate at both the character and
jamo levels, substituting visually similar glyphs across scripts (Hangeul, CJK, Latin, symbols, or emoji). Rotation-
based rules alter glyph orientation (90° or 180°) to generate visually perturbed yet readable text.

B.2 Iconological Approach

The iconological approach leverages the visual de-
composability of Hangeul consonants and the in-
dependence of their graphical forms. As discussed
in Sec. A.3.1, the clear sub-graphemic structure
of Hangeul, together with the geometric regular-
ity of its syllabic blocks, enables visually moti-
vated substitutions at both the character and con-
sonant levels, as well as rotation-based variants.
As illustrated in Tab. 11, Hangeul allows a variety
of iconographic transformations owing to its syl-
labic block structure and clear geometric regularity.
These transformations are designed to modify the
visual appearance of text while maintaining overall
recognizability to human readers.

Look-alike substitution. This method substi-
tutes characters with visually similar glyphs at two
different granularities: (i) at the character level,
where entire syllable blocks are replaced with visu-
ally analogous symbols drawn from Hangeul, CJK,
Latin, or other foreign scripts and even emojis; and
(ii) at the sub-syllabic level, where individual con-
sonant and vowel components are substituted with
shape-correlated symbols. These substitutions ex-
ploit the geometric resemblance across scripts and
provide a rich set of visually deceptive yet legible
variants.

Rotation. Rotation-based obfuscation manipu-
lates the glyph orientation of Hangeul characters.
By rotating syllable blocks or subcomponents by
90° or 180°, we produce text that visually resem-
bles the original while disrupting standard ortho-
graphic patterns. Such geometric perturbations pre-
serve readability to humans but often confuse auto-
matic recognition models.

13

Hangeul—>Hangeul Hangeul—CJK Sub-syllabic
=a BokR 167
youd aa Lev
4] > UY sof cot
ites FR 78 zed
y-8 4 at don
tof 434 HO
oe 4 AGK
a 4 Bok oO
a} + uf e354 ROK
238 $7 ROK
vt —> at uy] > Wl aAoOT7
qo ZOA BOL
2} —» OF 4&3 ZOW
Poe EO SOB

Table 12: Representative iconological substitution dic-
tionaries used in the Iconological Approach. Each
column shows systematic visual mappings between (i)
Hangeul—Hangeul replacements, (11) Hangeul—CJK sub-
stitutions, and (iii) sub-syllabic correspondences.


Category Granularity Examples

Phonetic Transliteration

CJK substitution  2%ta] > 7kAba], ote] S Bote]
Latin substitution "%Jechi > mangBrpa!, A)A]B- g VA)

Semantic Transliteration

English meaning 7}A] fal Zo] Hz 5 EE FI Zo] HZ}
Japanese meaning 2/2] 4 Eto > Ze] 6 cH Fo]

Table 13: Examples of the Transliteration-based Approach. Phonetic transliteration replaces parts of Hangeul
words with phonetically similar units in CJK or Latin scripts, while semantic transliteration substitutes words with
phonetic renderings of their foreign-language meanings (e.g., English or Japanese).

B.3 Transliteration-based Approach

As discussed in Sec. A.2.2, Korean users are inher-
ently familiar with multiple writing systems, includ-
ing Hangeul, basic Chinese characters (Hanja), and
the Latin alphabet, due to historical and educational
exposure. This multilingual competence enables
intuitive transliteration-based obfuscation, where
parts of text are replaced with characters or sounds
drawn from other scripts that share phonetic or
semantic associations. Broadly, two strategies are
employed: one exploits phonetic similarity (sound-
based substitution), and the other leverages seman-
tic equivalence (meaning-based substitution).

Phonetic transliteration. Phonetic translitera-
tion replaces parts of a Korean word with CJK
or Latin characters that share similar pronunciation.
For instance, the Chinese character 7K (pronounced
“su’’) can substitute the syllable 4= in =} 6], re-
sulting in 7<4J4j. Partial substitutions that target
only specific consonants or vowels are also possi-
ble (e.g., AA] — g WA]#F). Such CJK or Latin
replacements preserve phonetic resemblance while
introducing script-level variation that hinders auto-
matic recognition.

Semantic transliteration. Semantic translitera-
tion exploits the meaning of the original phrase
by translating it into a foreign language and then
re-Hangeulizing the phonetic rendering of the trans-
lated words. For example, the Korean verb +=} oj]
can be semantically translated into Japanese as ¢
72 & va, and then phoneticized back into Hangeul
as 74C}A}O]. This substitution thus conveys the
same meaning through a cross-lingual phonetic
rendering that remains easily interpretable to Ko-
rean readers. This approach leverages bilingual
familiarity—especially with English and Japanese—
to generate natural yet obfuscated variants easily
interpretable by Korean readers.

14

LLM-based obfuscation. Unlike other obfusca-
tion classes, the transliteration-based approach is
difficult to implement in a purely rule-based man-
ner, as it often requires contextual awareness and
semantic substitution rather than simple character
mapping. Among its variants, phonetic translitera-
tion with CJK characters can be handled determin-
istically through predefined rules, whereas Latin-
based and semantic transliteration demand higher-
level reasoning and cross-lingual understanding.
To address this, we employed a lightweight and
efficient language model, GPT-5 nano, to perform
LLM. assisted obfuscation for these cases.

While Hanja (CJK) characters align one-to-one
with Hangeul syllables, Latin script does not ex-
hibit such a direct correspondence, which fre-
quently led to undesirable substitutions that altered
contextually important words. In contrast, semantic
transliteration inherently involves translation into a
foreign language, making LLM utilization not only
beneficial but necessary.

As shown in Fig. 5 and Fig. 6, we designed care-
fully crafted prompts to guide the model in gen-
erating contextually appropriate obfuscations. Un-
like the few-shot or zero-shot prompts used for En-
glish tasks, these prompts were written in Korean
to better align with the linguistic characteristics
of Hangeul and to encourage the model to reflect
native Korean phonological and orthographic nu-
ances.


Phonetic Transliteration with Latin Scripts
= 3 GPTo]tH.
24s 514 248 42

yA edule! 2] oj]A]
a > Bolett:

ag ee aaa ea
SUES Bole H7) oHoretet.

2HE7|& BHA z ORC}.

a7
a

od oa) Ny FF
UL > oft Fe

HH
2
[=]
Ee
a &
=
=

{ "input":

{ "input":
{ "input":
{ "input":
{ "input":

cee "output": ban torn Ae ay
wa SAAS", “output”: "boo DIMIT. 2 YAFAyo" }
aE ac", Taman "bango] We the lovet}" }

Figure 5: The prompt used for phonetic transliteration obfuscation with Latin scripts. It provides the task descriptions
and instructions.

Semantic Transliteration

EAS S 2227] S BPA AOR.

- FFA] PO}EA]_@ -> don’t i Sia > EE DFTpF}Oo]
- Oo} ZO} Q -> OFF nice F]Q -> of LHO]A BQ

HHH A

BS

A}

bal

BZ] BA,
2oi7| +834] 2 WAS A.

rit of, 10, DY
POH ap

28,
2

fo EI mtr} 1st
IN 24 Atom

H
FESS,

Ae Au] Ae By,

HH Sa
{ "input":
{ "input": "E7}2]
{ "input": "ry WAy ag Wey.
ere Aue] Strpey.” }

{ "input": "Hro] ye Soye..", "output": "a ojZ ye

Be 7A) Por] _.2", "output": "OS EE T=LrpApo]" }
We Be be SOrR", "output": "7b2] Be HEL Ob 012 SHB" }
SE] A HA WY MAA", “output”: "]oS} Awa] SEY OIA. s]e|zyel

Zu} AAU." |

Figure 6: The prompt used for semantic transliteration obfuscation with various languages. It provides the task
descriptions and instructions.

15


Category Language Examples
Korean 3444 Far 825944508 vee
mE aati (PeEOAGR English this place is dirty — thi splace is dir ty
ay ut ae aye ae
Syllable/word anagram Rowan nie tell ats = oar Sotsrsee
English happy trip + hpapy tirp
oO] 4ERto] Szy7}A} — Ho] Sols AZ
English I wanna go home — Iwnan ago hoem

Table 14: Cross-lingual examples of Syntactic Obfuscation. Spacing and syllable-level rearrangements in Korean
correspond to word or character boundary shifts in English, but Hangeul’s block-based structure allows greater

flexibility while maintaining readability.

Category Language Examples
we . Korean ESPKL GAS ESOURE (GAA) EAS
Enioyl iiseriion English what a fool > what °O a (fo.ol) =AS

Table 15: Cross-lingual examples of Pragmatic Obfuscation. Each language employs visually or emotionally
expressive cues—emojis, symbols, or tone markers—to modulate perceived sentiment, often reducing apparent

toxicity while retaining original meaning.

B.4_ Syntactic Obfuscation

As noted in Sec. A.3.2, Hangeul is written in syl-
labic blocks and Korean readers parse text with
strong syllable-level awareness. Combined with
historically flexible spacing and the grammatical
role of postpositions, this yields high tolerance to
segmentation noise and local rearrangements. Thus,
surface perturbations that disrupt spacing or sylla-
ble order often remain human-recoverable while
confusing automatic detectors.

Spacing perturbation. We randomly insert or re-
move spaces at plausible boundaries (e.g., between
syllable blocks or morphemes), preserving word
order while altering the visual segmentation. When
composed with other rules, spacing noise increases
ambiguity without severely degrading readability.

Syllable-level anagram. We locally reorder syl-
lables within a word/phrase under constraints
that keep the syllable inventory intact and limit
edit distance. Unlike alphabetic scripts (character-
by-character decoding) or logographic scripts
(character-as-morpheme), the block-based unit in
Hangeul often allows such micro-rearrangements
to stay interpretable to human readers.

16

B.5 Pragmatic Obfuscation

Pragmatic obfuscation is language-agnostic and
alters discourse cues rather than lexical content.
We insert visually salient symbols or emojis near
sentiment-bearing tokens, which can soften per-
ceived polarity or distract pattern-based heuris-
tics, thereby reducing toxicity detection rates while
keeping the underlying proposition intact. Such
modifications exploit the tendency of large lan-
guage models and toxicity classifiers to rely on
surface-level emotional markers rather than deep
semantic understanding.

Irrelevant symbol insertion. We constrain the
symbol injection rate and avoid splitting inside
syllable blocks or linguistic morphemes. Hearts,
brackets, or emoticons are placed around target
spans to modulate tone (e.g., °O, « ), 2AS),
creating a visually disfluent but emotionally soft-
ened expression. These pragmatic cues preserve
human readability and contextual meaning while
significantly degrading the reliability of automatic
toxicity detection, highlighting a unique challenge
in modeling human-like interpretation of style and
intent.


C. Dataset Construction Details

C.1_ Details of Filtering K/DA

To construct our obfuscated Korean toxic text
dataset, we used K/DA [Jeon et al., 2025] as the
primary source. K/DA is a Korean paired dataset
originally developed for the detoxification task,
where neutral sentences were transformed into
toxic counterparts through LLM-based rewriting.
To capture rapidly evolving slang and online ex-
pressions, K/DA first collected toxic text from var-
ious online communities and built a large corpus.
For each neutral sentence, similar toxic samples
were retrieved using a semantic similarity metric
and then provided as examples to an LLM, which
generated corresponding toxic paraphrases.

Despite its scale and utility, K/DA presents sev-
eral quality limitations. A non-negligible number
of cases contain mislabeling, where already-toxic
sentences are annotated as neutral. Some sentences
are syntactically or semantically ill-formed to the
point of being uninterpretable. The dataset also in-
cludes real personal names, posing potential ethical
concerns. Furthermore, a single neutral sentence in
K/DA is often paired with multiple, near-duplicate
toxic variants, resulting in redundancy, lexical im-
balance between neutral and toxic subsets, and sub-
optimal suitability for classification tasks.

To address these issues, we conducted a manual
filtering process. Following the rubric in Table 17,
three native Korean annotators independently re-
viewed all 7,555 neutral-toxic pairs without discus-
sion. If a neutral sentence was deemed problematic,
the entire set of pairs linked to that neutral sample
was removed, whereas if the toxic side alone was
flawed, only the corresponding pair was discarded.
Inter-annotator consistency was evaluated using
Gwet’s AC1 coefficient, which yielded a score of
0.7408 (p < 0.001, z = 125.75, SE = 0.0059).
This value indicates a high level of agreement
among annotators, supporting the reliability of the
filtering decisions.

After filtering, only the 5,160 pairs marked as
valid by all annotators were retained. We further ex-
cluded extremely short sentences consisting of two
tokens or fewer, as they offered limited opportunity
for meaningful obfuscation. In cases where multi-
ple toxic variants were associated with the same
neutral sentence, a single toxic example was ran-
domly selected. The resulting corpus comprises
2,294 high-quality neutral—-toxic pairs, which
serve as the foundation for our obfuscated dataset.

17

Rule Rewrite Rate
Initial consonant replacement 0.5
Medial vowel replacement 0.3
Final consonant replacement 0.5
Orthographic resyllabification 0.5
Initial consonant insertion 0.3
Medial vowel insertion 0.5
Final consonant insertion 0.5
Liaison (Forward & Reverse) 0.3
Hangeul look-alike 0.3
Cross-script substitution 0.5
Rotation-based variation 0.3
Phonetic substitution (CYK) 0.3
Phonetic substitution (Latin) 0.5
Semantic substitution 0.5
Spacing perturbation 0.5
Syllable anagram 0.3
Symbol/emoji insertion 0.5

Table 16: Per-rule rewrite rates used in dataset construc-
tion. Rates represent the fraction of tokens targeted for
modification within each sentence.

C.2. Dataset Construction Environment

We utilized several libraries for data generation,
including hgtk 0.2.1, six 1.17.0, openai 1.109.1,
jamo 0.4.1, KoNLPy 0.6.0, and KoG2Padvaned’.

C.3. Hyperparameters for Dataset
Construction

During dataset construction, each neutral-toxic pair
from K/DA was processed through the obfuscation
procedure described in Alg. 1. For each pair, a set
of transformation rules was applied up to k times.
Since the scope of application differs across rules—
some can be applied to nearly every token, while
others only affect limited contexts—we controlled
the overall rewrite intensity using a global rewrite
rate. Specifically, the rate was set to 0.5 or 0.3 of
the total number of tokens in a sentence, depending
on rule coverage. The detailed per-rule rewrite rates
used for all 17 rules are summarized in Tab. 16.

C.4 Dataset Statistics

Table 18 summarizes the statistics of the final KO-
TOX dataset generated through the aforementioned
obfuscation process. The dataset contains a total

https ://github.com/seongmin-mun/
KoG2Padvanced. git


of 6,882 neutral-toxic pairs, evenly divided into
three difficulty levels (Easy, Normal, and Hard) ac-
cording to the number of applied rules per sentence.
Table 19, 20, 21 further provide qualitative exam-
ples illustrating how different rule combinations
are reflected across difficulty levels.

D_ Experimental Details

D.1 Details of LMs used for Classification

We use three transformer-based language models
fine-tuned on toxic or offensive text corpora for
toxicity classification.

HateBERT HateBERT [Caselli et al., 2021] is
a BERT model further pre-trained on Reddit posts
containing abusive and offensive language. It is
optimized for English toxic comment detection and
serves as a strong domain-adapted baseline.

Multilingual-Toxic-XLM-RoBERTa This
model is based on XLM-RoBERTa and fine-tuned
on multilingual toxic datasets covering 15 lan-
guages. It enables cross-lingual toxicity detection
and serves as our multilingual baseline.

Toxicity-XLMR-v2_ Toxicity-XLMR-v2 is a
large XLM-RoBERTa model fine-tuned on diverse
multilingual corpora for toxicity classification. It
provides strong generalization across languages
and complements the English-centric HateBERT.

D.2. Details of LLMs Used for Deobfuscation
and Sanitization

All models used in our experiments are instruction-
tuned large language models (LLMs).

Qwen2.5 Qwen?.5 is a multilingual causal LLM
by Alibaba with significantly improved Korean ca-
pability over its predecessors. Although version
3 is available, we use 2.5 since the newer “think-
ing” mode often produces overly verbose outputs
unsuitable for our tasks.

Exaone 3.5 Exaone 3.5, developed by LG AI
Research, is a Korean-specialized LLM. We adopt
version 3.5 instead of 4.0 to avoid verbosity issues
from the new “thinking” control while maintaining
strong linguistic quality and response stability.

LLaMA-3-Korean-Bllossom LLaMA-3-

Korean-Bllossom extends Meta’s LLaMA-3
through continued Korean pretraining and instruc-
tion tuning. It serves as an open-source alternative

18

emphasizing fluency and consistency in Korean
generation.

GPT-4.1 GPT-4.1 is OpenAI’s closed-source
frontier LLM, representing one of the most capa-
ble general-purpose models currently available. It
serves as a strong closed-source baseline for deob-
fuscation and sanitization tasks.

D.3. Details of Metrics

Accuracy Accuracy measures the proportion of
correctly predicted samples. However, in balanced
binary classification tasks, a trivial model that al-
ways predicts a single class can easily achieve 50%
accuracy. Therefore, it is often reported together
with Fl-score for a more reliable assessment.

Fl-score F1-score is the harmonic mean of Pre-
cision and Recall. In binary or imbalanced clas-
sification tasks, Fl-score is widely preferred over
accuracy since it better captures the balance be-
tween false positives and false negatives. We treat
the harmful class as the positive label when com-
puting Fl-score, which is a common convention in
hate speech detection studies.

BERTScore Since our dataset is in Korean, we
employ the multilingual BERT-based implementa-
tion of BERTScore following the default configu-
ration of the official library. This allows semantic
similarity to be computed across diverse linguistic
variations.

chrF Korean exhibits agglutinative morphology,
where particles and affixes are attached to word
stems. As a result, token-level n-gram metrics such
as BLEU or ROUGE may underestimate similarity.
We therefore report character-level matching scores
using chrF, which better captures morphological
overlap.

Perspective API We additionally use Google’s
Perspective API to estimate toxicity scores of gen-
erated sentences. This tool is widely adopted in
toxicity and hate-speech detection research for pro-
viding a standardized toxicity estimation.

D.4 Experimental Environmetns

We conducted training and inference on Ryzen
9950x and Threadripper 9960X CPUs, and
NVIDIA RTX Pro 6000 GPUs. The experiments
were performed on Rochy Linux 9.6 unsing Py-
Torch 2.8.0, Transformers 4.56.2, BitsAndBytes
0.48.0, Kernels 0.10.2, PEFT 0.17.1, Scikit-learn


Rule

Filtering Reason

Misaligned Neutrality

Slang or Informal Vulgarity

Non-standard or Unintelligible
Expression

False Neutrality or Label Ambiguity

Masked or Corrupted Text

Personally Identifiable Information

Semantic Ill-formedness

Duplication / Near-Duplication

Length Insufficiency

Label Noise (Inverse Pairing)

Neutral text already conveys toxic or sarcastic intent, compro-
mising its role as a non-harmful counterpart.

Neutral sample contains slang or mild expletives (e.g., “7H—”,
““”| St”) inappropriate for detoxified text.

Text includes invented words, broken grammar, or unintelligi-
ble noise generated by LLMs.

Toxic text lacks explicit offensiveness or appears indistinguish-
able from neutral tone, making label assignment unreliable.

Presence of masking artifacts (e.g., “** 4)”, “2***”) or pre-
processing errors that corrupt readability.

Sentences expose real names, usernames, or identifiable enti-
ties, raising privacy and ethical concerns.

Either side of the pair is semantically incoherent or ungram-
matical, hindering model training.

Multiple toxic variants are paired with the same neutral sen-
tence, leading to redundancy and imbalance.

Sentences are too short (S2 tokens) to allow meaningful trans-
formation or obfuscation.

Neutral and toxic roles are swapped or mislabeled, resulting in
reversed polarity between pairs.

Table 17: Rubrics for filtering K/DA. Each rule specifies a criterion for discarding or retaining pairs to ensure dataset

quality and label consistency.

Difficulty #Samples # Applied Rules # Rule Combinations # Total Rules Avg. # Span
Easy 2,294 2 197 17 7.94
Normal 2,294 3 1,254 17 8.14
Hard 2,294 4 2,079 17 8.20
Total 6,882 2-4 3,530 LY 8.09

Table 18: Statistics of the KOTOX dataset by difficulty level. Each level is defined by the number of applied
obfuscation rules per pair. A total of 6,882 samples were generated and evenly distributed across three difficulty

levels.

19


Type Example Label
Neutral (Original) BAS 27S Dae] HSS AA] 430 BAO]
Neutral (Obfuscated) 24S 27}2 9 oN Meo Aa UO BA} (5, 10]
Toxic (Original) Btls S7ts Aa We WAlSete] WHS GS AO SAU a4
Toxic (Obfuscated) 242 S7}S Jaf ue cE upla—ebve | | Bed 7 —S FOR SAY FF
Neutral (Original) o}LJ Ok = 3 SI-E7] As] 9/0] Ok Eb arr
Neutral (Obfuscated) 2}5Jok 3 3 A7]-S Hs] 9)0]OF EF spar [16, 6]
Toxic (Original) OW JOH 3 3 W7}2] QL SL SLE] Wale Glo] ehar hele} x x
Toxic (Obfuscated) = 9FOFL a 9 AES] BY Se AS7] BSR Qo] ea oe} xx
Neutral (Original) AFU7| SARS 2414 THE 742]
Neutral (Obfuscated) 2A4U7} SAE 214 RE AAZI [4, 13]
Toxic (Original) otte Aa 7B SA, aS eS Anse AeA SS FE 294
Toxic (Obfuscated) = “SARE AB 7 SAAT, Sa See ASE WA SS FE OH
Table 19: Easy examples of KOTOX.

Type Example Label
Neutral (Original) OS AS tal 7h ela]
Neutral (Obfuscated) © first E}9] AB Jrtda ae [11, 14, 12]
Toxic (Original) 28 AG al 7H Fela] Arp Sols Of Se4oF Str pc] gaa
Toxic (Obfuscated) =o] cheum haekgyo onneun 4°14] OTAKUS ©] 4 aO-S 4 Ff Str} cancer Fy
Neutral (Original) AF OF OL TH YF QIU?
Neutral (Obfuscated) % OStO (RY) TAO, 7 By? [5, 1, 17]
Toxic Original) 4 Sk 94 HAO] 7] Az} wea] L a] 3 3
Toxic (Obfuscated) An okn | 2h4 || FALE O [A] 8 Ae Eq] 4-- 4
Neutral (Original) AU Aa] Bay Al etol?
Neutral (Obfuscated) 1 UL] 3 4] Bap ey AZ eto]? [7, 3, 15]
Toxic (Original) AU al obseuy Ey A] ol IF?
Toxic (Obfuscated) = AU] 21 A] Ob BE EF APY ZISup?

Table 20: Normal examples of KOTOX.
Type Example Label
Neutral (Original) of eu] 2 Aol]H AEWA wo} aa
Neutral (Obfuscated) ©) 2] — Ao] H AEA of [1, 13, 9, 8]
Toxic (Original) Fol 4 AvtopA] 4 aA aeas
Toxic (Obfuscated) wt ZL A@QOA B4 AAS
Neutral (Original) A7\7| ZF] 7H EE HO] BLE ct
Neutral (Obfuscated) FJAU] +S}4) 7 EY OAs JAY [1, 5, 14, 4]
Toxic (Original) 77] HAAS BA AS] 2H SE Bo] oc et
Toxic (Obfuscated) YAS ELMS Bol] Aaa] 4] SE Avo] Sez gaa
Neutral (Original) 1 7] eel4] BE Saottt
Neutral (Obfuscated) Wg | e1%|r Jally gun set Ja [11, 6, 4, 12]
Toxic (Original) 18 7Sel2| 82 Sasa, AF°] See ISSA Ol RIE
Toxic (Obfuscated) AA g |tZ2] really $Sste], GEO tailse HPYSELS Yo mortal =

Table 21: Hard examples of KOTOX.

20


1.7.2, EasyDict 1.13, Pandas 2.3.3, Accelerate
1.10.1. For evaluation metrics, we additionally
used Evaluate 0.4.6, SacreBLEU 2.5.1, BERTScore
0.3.13, OpenAI 1.109.1.

D.5

Classification. We fine-tuned the LM using su-
pervised learning for the classification task. The
fine-tuning process employed a dropout rate of 0.1,
with hyperparameters set as follows: 15 epochs, a
batch size of 16, a learning rate of 2e-5, a max-
imum sequence length of 245, and the AdamW
optimizer. The model with the best evaluation loss
was selected as the final checkpoint. Each experi-
ment was repeated with seeds 42, 43, and 44.

Hyperparameters for Fine-tuning

Deobfuscation and Sanitization. For each task,
we fine-tuned the LLM using LoRA (a=16,
dropout=0.1, r=64) under 16-bit precision. The fine-
tuning configuration included 5 epoch, a batch size
of 16, a learning rate of 2e-5, a weight decay of
le-2, a maximum sequence length of 1024, the
AdamW optimizer, a warmup ratio of 0.03, and a
cosine learning rate scheduler. The best-performing
model based on evaluation loss was selected, and
each experiment was repeated with seeds 42, 43, 44.
For both Deobfuscation and Sanitization tasks, we
apply zero-shot and five-shot prompting schemes.
Prompt templates for the Deobfuscation task are
shown in Figures 7 and 8, and for the Sanitization
task in Figures 9 and 10.

21

E_ Additional Experimental Results

E.1 Full Results on Main Experiments

In this section, we report additional experimental
results that were not included in the main paper.

E.1.1 Classification

Tab. 22 shows the classification Fl-score and stan-
dard deviations. Similar to the Fl-scores, mod-
els fine-tuned on the combined dataset of non-
obfuscated toxic text and obfuscated text generally
achieved higher performance than those trained on
a single type of data. Furthermore, models trained
solely on the obfuscated dataset also performed
well in detecting non-obfuscated toxic texts, indi-
cating their generalization capability.

Fig. 11 shows the rule-wise correlation matrix
of HateBERT fine-tuned on the easy dataset. The
easy dataset contains samples with two applied
rules per instance. As observed, there are no strong
correlations between the rules, suggesting that each
rule operates independently.


Deobfuscation Zero-shot

You are an expert in Korean language correction.

The sentence I provide may be non-standard, use internet slang, phonetic spelling, intentional misspellings, or be obfuscated
in other ways.

Your task is to translate and normalize the sentence into natural, standard Korean.

Rules:

1. Preserve the original meaning.

2. If the tone is conversational, keep it conversational but in standard Korean.
3. Remove unnecessary emoticons or special characters.

4. Make the sentence grammatically smooth and natural.

Translate:
Input sentence: $input
Output sentence:

Figure 7: The zero-shot prompt used for deobfuscation. It provides the task descriptions and instructions.

Deobfuscation Five-shot

You are an expert in Korean language correction.

The sentence I provide may be non-standard, use internet slang, phonetic spelling, intentional misspellings, or be obfuscated
in other ways.

Your task is to translate and normalize the sentence into natural, standard Korean.

Rules:

1. Preserve the original meaning.

2. If the tone is conversational, keep it conversational but in standard Korean.
3. Remove unnecessary emoticons or special characters.

4. Make the sentence grammatically smooth and natural.

Examples:
Input sentence 1: $example1_input
Output sentence 1: $example1_output

Input sentence 2: $example2_input
Output sentence 2: $example2_output

Input sentence 3: $example3_input
Output sentence 3: $example3_output

Input sentence 4: $example4_input
Output sentence 4: $example4_output

Input sentence 5: $example5_input
Output sentence 5: $example5_output

Translate:
Input sentence: $input
Output sentence:

Figure 8: The five-shot prompt used for deobfuscation. It provides the task descriptions, instructions, and five
few-shot examples.

22


Sanitization Zero-shot

You are an expert in Korean language correction and detoxification.

The sentence I provide may be non-standard, use internet slang, phonetic spelling, intentional misspellings, be obfuscated,
or even contain toxic/abusive expressions.

Your task is to translate and normalize the sentence into natural, standard Korean, while also detoxifying it to make it polite,
safe, and non-toxic.

Rules:

1. Preserve the intended meaning as much as possible, but remove or soften toxic, offensive, or abusive language.
2. If the tone is conversational, keep it conversational but in standard Korean.

3. Remove unnecessary emoticons or special characters.

4. Make the sentence grammatically smooth and natural.

5. Ensure the output is non-toxic, respectful, and safe.

Translate:
Input sentence: $input
Output sentence:

Figure 9: The zero-shot prompt used for sanitization. It provides the task descriptions and instructions.

Sanitization Five-shot

You are an expert in Korean language correction and detoxification.

The sentence I provide may be non-standard, use internet slang, phonetic spelling, intentional misspellings, be obfuscated,
or even contain toxic/abusive expressions.

Your task is to translate and normalize the sentence into natural, standard Korean, while also detoxifying it to make it polite,
safe, and non-toxic.

Rules:

1. Preserve the intended meaning as much as possible, but remove or soften toxic, offensive, or abusive language.
2. If the tone is conversational, keep it conversational but in standard Korean.

3. Remove unnecessary emoticons or special characters.

4. Make the sentence grammatically smooth and natural.

5. Ensure the output is non-toxic, respectful, and safe.

Examples:
Input sentence 1: $example1_input
Output sentence 1: $example1_output

Input sentence 2: $example2_input
Output sentence 2: $example2_output

Input sentence 3: $example3_input
Output sentence 3: $example3_output

Input sentence 4: $example4_input
Output sentence 4: $example4_output

Input sentence 5: $example5_input
Output sentence 5: $example5_output

Translate:
Input sentence: $input
Output sentence:

Figure 10: The five-shot prompt used for sanitization. It provides the task descriptions, instructions, and five few-shot
examples.

23


Sorel hateBert offensiveRoBERTa toxicity-xlmr-v2
ne w/o Obf Obf A w/o Obf Obf A w/o Obf Obf A
. 36.56 36.28 0.28 33.29 33.61 -0.32 79.28 56.80 22.48
w/o Tuning
(£5.59) (43.06) (40.28) | (£0.08) (40.48) (40.56) | (£10.44) (£13.42) (+22.21)
76.69 65.88 10.81 91.86 69.98 21.88 95.06 53.66 41.40
w/o Obf (FT) (£0.95) (41.16) (42.27) | (42.12) (48.22) (42.89) | (40.22) (40.11) (40.16)
T71A9 71.65 5.54 92.02 84.97 7.04 96.30 89.57 6.73
Ours (FT) (£1.67) (40.78) (41.98) | (£1.08) (43.33) (42.89) | (40.22) (40.11) (40.16)
78.44 71.32 7A2 92.68 86.94 5.74 96.16 88.13 8.03
w/o Obf + Ours (FT) | (41.63) (40.99) (41.02) | (40.33) (40.96) (40.95) | (40.88) (42.48) (41.66)

Table 22: Binary Toxicity Classification under Obfuscation. Each model reports f1-score on non-obfuscated (No-
Obf) and obfuscated (Obf) sets, and the robustness gap A =No-Obf—Obf.

5 hateBert offensiveRoBERTa toxicity-xlmr-v2
etting
w/o Obf Obf A w/o Obf Obf A w/o Obf Obf A
. 50.14 50.14 0.00 49.13 49.88 -0.75 79.78 61.89 17.89
w/o Tuning
(£0.25) (£0.26) (40.00) | (£1.51) (40.21) (£1.30) | (49.91) (47.61) (£16.59)
76.74 66.23 10.51 91.88 72.03 19.85 95.07 60.65 34.42
w/o Obf (FT) (£0.87) (£1.40) (42.27) | (£2.09) (46.25) (44.16) | (40.22) (40.11) (40.11)
77.32 71.86 5.46 92.03 85.07 6.96 96.30 89.59 6.71
Ours (FT) (£1.60) (40.52) (41.08) | (£1.07) (£3.18) (42.11) | 40.22) (40.11) (40.11)
78.48 71.33 7TA5 92.68 86.96 5.72 96.16 88.14 8.02
w/o Obf + Ours (FT) | (4.1.64) (40.99) (40.65) | (40.33) (40.94) (4061) | (40.88) (42.48) (£1.60)

Table 23: Binary Toxicity Classification under Obfuscation. Each model reports accuracy on non-obfuscated (No-
Obf) and obfuscated (Obf) sets, and the robustness gap A =No-Obf—Obf.

Label Co-occurrence Correlation Matrix

“s,

SOMIDMNAWNHE

Rule

11

Rule

"a

1234567 8 91011121314151617

0.80

Co-occurrence Ratio

-0.20

-0.00

Figure 11: Correlation heatmap of label

24


Qwen2.5 EXAONE3.5 Bllosom GPT4.1

Setting
BertScore chrF  BertScore chrF  BertScore chrF  BertScore’§ chrF
65.96 15.31 60.60 7.64 65.09 14.08 00.0 00.0
Zero-Shot = 4004) = (4Oll) ~~ (LOI) ~— (40.05) (£0.06) (40.01) — (40.00) _~— (40.00)
68.93 19.40 67.00 14.39 21.14 21.14 00.0 00.0
Five-Shot (40.68) (£0.99) (40.47) (40.36) (40.32) (40.81) (40.00) — (£0.00)
77.90 36.79 78.12 34.96 78.05 40.67 - -
SFT (£0.11) (40.48) (40.02) (40.24) (40.04) (40.11)
Table 24: Toxic text deobfuscation.
Qwen2.5 EXAONE3.5 Bllosom GPT4.1
Setting
BertScore chrF  BertScore chrF _ BertScore chrF  BertScore  chrF
00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Zero-Shot — (40.00) (£0.00) (40.00) (£0.00) ~— (£0.00) (40.00) (£0.00) _— (40.00)
. 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Five-Shot (40.00) (£0.00) (40.00) (£0.00) (40.00) (40.00) (40.00) — (£0.00)
00.0 00.0 00.0 00.0 00.0 00.0 - -
SFT (0.00) (£0.00) (£0.00) (£0.00) ~— (£0.00) — (£0.00)

Table 25: Toxic text sanitization.

25
