2401.01262v2 [es.CL] 3 Jan 2024

arXiv

Fairness Certification for Natural Language
Processing and Large Language Models

Vincent Freiberger! and Erik Buchmann?

Dept. of Computer Science, Leipzig University, Germany!”
Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI)
Dresden/Leipzig, Germany!”
freiberger@cs .uni-leipzig.de’ buchmann@informatik.uni-leipzig.de”

Abstract. Natural Language Processing (NLP) plays an important role
in our daily lives, particularly due to the enormous progress of Large Lan-
guage Models (LLM). However, NLP has many fairness-critical use cases,
e.g., aS an expert system in recruitment or as an LLM-based tutor in ed-
ucation. Since NLP is based on human language, potentially harmful
biases can diffuse into NLP systems and produce unfair results, discrim-
inate against minorities or generate legal issues. Hence, it is important
to develop a fairness certification for NLP approaches.

We follow a qualitative research approach towards a fairness certifica-
tion for NLP. In particular, we have reviewed a large body of literature
on algorithmic fairness, and we have conducted semi-structured expert
interviews with a wide range of experts from that area. We have system-
atically devised six fairness criteria for NLP, which can be further refined
into 18 sub-categories. Our criteria offer a foundation for operationaliz-
ing and testing processes to certify fairness, both from the perspective
of the auditor and the audited organization.

Keywords: Fairness, Certification, NLP

1 Introduction

Fairness is important for Natural Language Processing (NLP) approaches: NLP
is used in high-stakes contexts such as healthcare fi]. It is also integrated into
daily-use technologies, e.g., voice assistants like Amazon Alexa or Al-based
chatbots like ChatGPT |3]. A lack of fairness often materializes as allocative or
representational harm ifs marginalized groups (5). An example of allocative
harm is a resume filtering system, that prefers male applicants (6). Representa-
tional harm would be a translation app that translates to gender stereotypes [7],
cultural stereotypes or demeaning language |6}|8\/9|. To avoid harm, a fair NLP
application must not only resist gender bias 13}, but also ableist {14), eth-
nical [11}/15}{16}, age-related (17], religion-related or sexuality bias 1 1}[19].
Performant NLP models tend to be opaque and complex 21). Interactions
with such sociotechnical systems are typically also complex . Hence, it is not
practically achievable for users or affected individuals to verify fairness. Fairness
certification could be embraced to reduce information asymmetries (23}.



2 Freiberger et al.

The concern of this paper is to develop a broad set of criteria, that can
be used by an auditor to assess and certify the fairness of an approach that
makes use of NLP, be it a large language model, a recruiting-tool or an AI-
chatbot for teaching. This is challenging: First, many different definitions of
fairness exist (24)(25], and some of them are contradictory [26}/27). Second, it
is still unclear yet, which fairness criteria are important for NLP approaches
and how they impact each other. For instance, residual unfairness may remain
after bias mitigation has been resolved (28). Third, efforts towards fair NLP and
certifying fairness of related AI approaches exist (29}{34]. But there is neither
an established nor a holistic framework on how fairness could be certified and
what could be audited [35}[36). To approach certification criteria that can be
applied in practice, we need to consider the challenges of professionals tasked
with auditing the fairness of a system, as well as the challenges of developers
that encounter fairness issues when creating NLP systems. Thus, our research
question is as follows:

What criteria are relevant to consider for fairness certification for NLP ap-
proaches from a practitioner’s point of view?

To approach our research question, we strive for broad, qualitative research.
We have decided to derive a basic set of auditable fairness criteria from litera-
ture on NLP, AI fairness and AI certification. Based on this set of criteria, we
have developed a concept for a semi-structured interview with stakeholders from
business and research. We have analyzed the interview transcripts in order to
find out (a) which measures for ensuring fairness need to be considered for NLP
approaches, and (b) how they influence each other. We make four contributions:

— We outline and structure an extensive, up-to-date body of research literature
on NLP fairness, AI fairness and fairness auditing from the last years.

— We describe our qualitative research approach, which is based on a series
of semi-structured interviews with 14 experts from various areas related to
NLP and algorithmic fairness.

— We devise a hierarchical coding scheme for the certification of fairness for
NLP approaches.

— We provide an overview of the six main criteria, with 18 sub-criteria on the
second hierarchy level for the auditing of NLP approaches, which we have
devised from the interview transcripts.

To the best of our knowledge, we are the first to devise a holistic, hierar-
chical coding scheme for the fairness certification of NLP approaches, which is
backed up by literature and expert interviews. Our findings allow to develop a
fairness certification for a wide range of NLP applications, including large lan-
guage models and other text-generating AI approaches. Fairness is related to
trust and reduces information asymmetries 23}. Thus, our results help to
establish a selling point for consumers, particularly marginalized groups. From
a legal perspective, a certification can be a precautionary measure [34].

Paper structure: Section |2| reviews related work and gives a theoretical
background. Section [3] outlines our research method. The Sections [4] and [5] eval-
uate and discuss our findings. Finally, Section [6] concludes.


Fairness Certification 3

2 Related Work

In this section, we review existing literature on the Natural Language Processing
(NLP) workflow and its modes of use. We provide an overview of fairness in
Artificial Intelligence (AI) approaches on which NLP builds. Finally, we describe
existing certification approaches for such AI approaches.

2.1 Natural Language Processing

NLP makes human language computable [38], and can be distinguished into Nat-
ural Language Understanding (NLU) and Natural Language Generation (NLG) (38).
NLU refers to a machine’s comprehension of human language and “extracting
valuable information for downstream tasks” 38}. Text summarization Cet
intend recognition {40}, machine translation |40], named entity recognition |41],
sentiment analysis or text classification are examples for NLU.

NLG refers to producing human-understandable text or speech in natural
languages (43). This is done by predicting the next token in a sequence of words,
based on data sources like graphics, video, text, or audio. Prominent examples
are AI-based Large Language Models (LLMs) like ChatGPT 3].

Modern NLP is based on language models that allow the creation of word
embeddings in the semantic space of a language. Building an NLP model
starts with selecting the corpus, a “collection of linguistic data, either compiled
from written texts or transcribed from recorded speech” [41]. Models are typi-
cally large-scale Deep Learning models, which are pretrained on very large sets
of text data (40). Word embeddings resulting from language models can also
be utilized as features for downstream NLP models {10}. Evaluation is typically
performed on specific benchmark data sets for the field [40].

2.2 Artificial Intelligence Fairness

Fairness can be defined as the equally performant treatment of all humans by AI,
without discriminating against any individuals or communities (45). The whole
life cycle of AI needs to be considered, as fairness problems can occur at all
stages [46]. We employ the Cross Industry Standard Process for Data Mining
(CRISP-DM) when referring to the life cycle of AI. We chose a broad and
generalist definition of fairness to encapsulate different conceptualizations of
fairness brought up by interview partners (cf. Sec.|4). Fairness and its perception
are context-dependent and communally derived . For measuring fairness,
numerous metrics have been discussed in literature [24\(27||50), some of which
even (partially) contradict each other.

Assessing fairness is pursued on different levels. This necessitates differenti-
ating between individual fairness and (sub)group fairness, which in themselves
have substantial normative differences (25]51). Bias is often brought up when
fairness is discussed, even if it is not necessarily correlated to fairness |52|. Bias
is a “dynamic and social and not [just] a statistical issue” [46]. Mitigating biases
can be handled during preprocessing, inprocessing or postprocessing in


4 Freiberger et al.

a software life cycle. Preprocessing captures the data processing before infer-
ence. Inprocessing targets the inference process itself. Postprocessing addresses
operations performed after inference.

To understand how biases impact fairness, understanding in what ways they
are harmful to which specific groups is required (4). Biases can result in discrim-
ination, i.e., in differences in the predictive power of models for an individual
based on membership in different protected groups [55|[56). It may occur directly
by utilizing protected attributes such as gender, race, disability, or sexuality.
It can also be indirectly, via correlations to excluded protected attributes (25).
These attributes may overlap for certain social groups, causing multi-dimensional
discrimination like intersectionality 57].

Different social groups use language differently (58). NLP approaches tend
to perform worse for marginalized groups because their usage of language is
underrepresented [5]. Language also transmits beliefs about social groups and
imposes labels on them, representing their societal position (4). Language use
reflects power relationships and social discrimination [1]. Biases in human lan-
guage are surfaced in word embeddings (10}/11). This is problematic, because
word embeddings are used for downstream NLP models, and biases diffuse into
these models (1). There they can cause allocative or representational harm, and

must be targeted by debiasing [10)[59).

2.3 Artificial Intelligence Certification

According to ISO/IEC 17000:2020 (60), certification is a “third party attestation
(---) related to an object of conformity assessment” (60). The object of confor-
mity assessment refers to an entity to which specific needs and expectations are
put into place (60). A certification authority performs a provider-independent
audit with comprehensive checks to assess conformity with certification criteria,
standards, or performance claims (35|[6162). Third-party auditors are “indepen-
dent organizations or individuals with no obligation or contractual relationship
to the audit target” (35). A certification communicates information, signals qual-
ity to the system user, and assurance raises trust (62\|63}. This is important in
high-stakes domains [64]. It helps providers to show their legitimacy and to
improve their services and products [34]. Certifications are defined by content,
source, and process elements (63}/64). Content captures the specific subject of
assessments in an audit. The source describes the institution providing accredi-
tation. Process describes how an assessment for certification is conducted.

For AI auditing, use-case-specific approaches have been suggested . Com-
mon auditing standards do not exist yet. This is problematic, as such
systems are increasingly deployed in high-stakes domains [29]. Current fairness
audits focus on quantitative aspects. Such audits evaluate system
outputs according to mathematical fairness definitions without procedural or
qualitative assessment, and without considering intersectionality (35). This does
not necessarily result in a fair system.


Fairness Certification 5

3 Our Research Method

Because fairness certification of NLP approaches is a novel, barely de-
fined research topic (cf. Sec. [2), we approach our research question with semi-
structured expert interviews. Such interviews bring consistency, focus, and struc-
ture to the interview while offering room for improvisation (65]/66). In particular,
we follow Myers’ qualitative research methodology and Corbin & Strauss’
approach to data analysis (66)|67].

Our research method consist of the steps literature review, interview guide,
interviewee selection, conducting and transcribing the interviews, coding the in-
terview transcripts, annotation and reflection and refinement of the interviews.
In the following, we explain these 8 steps.

Literature Review: This step provides us with all the information nec-
essary to develop an interview guide and to interpret the interview results. In
particular, we need an overview of NLP, AI fairness, and certification (cf. Sec. [2).

Interview Guide: On the basis of the literature review, we developed an
interview guide with open questions (see Appendix[3p. Our guide structures the
interview into 8 parts. In the first part, (1) we welcome our interviewees and (2)
explain the organization of the interview. Next, (3) we capture the professional
background of the interviewee, and (4) clarify the terminology for the interview.
Then (5) we ask for criteria for NLP development and (6) for sustaining NLP
fairness over time. After that, (7) we leave room for open topics to talk about.
Finally, (8) we thank our interviewee and conclude the interview.

Interviewee Selection: Our research question calls for practically appli-
cable research results drawn from the entire life cycle of NLP approaches. To
investigate relevant criteria for fairness certification, we decided to select inter-
viewees as follows: The interviewees should be working in the private sector on
NLP in a relevant role like data scientist, consultant, or manager. They should
have a minimum of two years of industry experience in NLP and, ideally, expo-
sure to fairness in AI. To broaden our findings (65||68), we wanted to gain diverse
experts in the dimensions of age, gender, and cultural background.

To identify matching candidates, we browsed LinkedIn and Xing as well as
the authors’ professional networks. Matching candidates received a flyer with
basic information on the research project and the interview guide in advance, to
let them judge whether they could provide value and reflect on the topic.

On this basis, we selected 12 interviewees. In addition, we made an exception
to include two more interviewees with a background in academia. These inter-
viewees were involved in algorithmic audits. Both had more than three years of
experience in algorithmic fairness. Table |1] gives an overview of the interview
partners in the order in which they were interviewed. The first column of the
table contains the ID of the interview partner. In the following, we will use the
ID to relate a statement to a person. The second and third columns describe the
role of the interviewee and the business area of the interviewee’s company. The
last column contains the interviewee’s professional expertise.

Conducting the Interviews: We conducted one interview in English and
13 in German, all of them via video conferences. The interviews followed the


6 Freiberger et al.

Table 1. Overview of interview partners

ID/Profession Affiliation Expertise
I, |CTO Regional, NLU, whole life cycle
AI Solutions
Iz |Data Scientist International, Wide variety in NLP, whole
IT Consulting life cycle
Iz |Data Scientist National, Wide variety in NLP, whole
IT Services life cycle
I, |Team Lead NLP National, Wide variety in NLP
AI and Research
Is |CTO National, Conversational] NLG
AI Solutions
Ig |Head of NLP National, NLG
IT Services
I; |Data Scientist International, NLU, search query handling
IT
Ig |Associate Vice International, Wide variety in NLP
President AI IT Consulting
Ig |Senior Program International, IT Conversational AI, ASR
Manager
fio |Consultant Multinational, NLU
IT Consulting
I41|Tech Lead NLP International, Wide variety in NLP
IT Consulting
li2|Research Scientist |National, Research Algorithmic fairness
1,3 |Co-Founder National, NLU, focus: preprocessing
IT Services
I,4|Senior Applied Multinational, Algorithmic fairness,
Scientist E-Commerce certification

interview guide, as explained in the second step. During the main section of the
interview, we generally encouraged further input on topics verbally and non-
verbally by active listening, open questions, and reflecting back to the intervie-
wee (65). The average duration of all 14 interviews was 51 minutes.

Transcribing the Interviews: We transcribed all interviews manually, fol-
lowing the guidelines of Dresing and Pehl (69). We adapted the guidelines by
leaving out timestamps, which were not needed. To ensure correctness, we asked
the interviewees to check our transcripts.

Coding the Transcriptions: To gain insight from the transcripts, we de-
veloped a coding scheme in an iterative approach based on open and axial cod-
ing (67). Open coding allows us to freely name concepts represented in interview
data. We use axial coding to understand concepts’ context, cause, and con-
sequence. We identify concepts for the coding scheme by interviewees naming
them explicitly, by abstracting from specific ideas to concepts, or by identifying
examples for a concept.


Fairness Certification vA

Auditing Organization

Governance
Criteria

Process Criteria

Audited Organization

Data-Related
Criteria

Governance
Criteria

Modeling &
Evaluation Criteria

Operations
Criteria

Project Planning
Criteria

Fig. 1. Top-level codes for the fairness certification of NLP approaches

In advance of our results in Sec. [4] Figure[]shows the main criteria identified
in our coding scheme. On this level, the coding scheme consists of six criteria,
that are relevant for NLP certification. The scheme is divided into the auditing
organization and the audited one. Governance Criteria are relevant for the
certification process for both of them, from opposite points of view. The general
characteristics and aims of the certification are explained by Process Criteria.
Finally, Data-Related Criteria, Project Planning Criteria, Modeling &
Evaluation Criteria and Operations Criteria specify processes and contents
of the audited organization. Note that coding is an iterative process, i.e., Figure[]]
illustrates the last version of dynamic refinement.

Annotation and Reflection: We annotated 1162 text passages, utilizing
memos to capture the central ideas and concepts represented in the passage.
After multiple iterations over the scheme, 1095 coded passages contained 587
overall codes, with 124 belonging to open coding and 473 to axial coding.

Refinement: Conducting interviews is a dynamic process. After conduct-
ing and analyzing our first four interviews, we adapted our interview guide. In
particular, we worked on our questions to simplify the interview process and to
avoid unnecessary follow-up questions. Furthermore, we learned that our broad
approach puts a lot of time pressure on the interviewer, which does not allow us
to ask in-depth questions. Thus, we decided to ask our interviewees about prior
experiences and thoughts on fairness certification first. This allows us to skip
topics in the subsequent interview, that the interviewees did not feel qualified
enough to talk about.


8 Freiberger et al.
4 Interview Findings

This section gives an overview of all criteria for NLP certification that we have
identified. Figure [2] illustrates our hierarchy of codes and sub-codes. Note that
we further subdivided the sub-codes, which we omit here for lack of space. Ap-
pendix [B] contains a more detailed description of our results.

Data Assessment

Fairness Understanding / Annotation
2 . Process Preprocessing
Design of Assessment | Criteria Data-related | ee - =
Certification Market Factors Criteria y) Data for Evaluation
i Data for Continuous
Model Reporting
& Trans; a iq Criteria "| , < Improvement
Spee) __ | Governance Criteria NLP Fairness Certification Hain Stocas
Organizational Criteria : - ala Storag)
Define & Assess the Modeling & Evaluatio. _ Modeling
Planned Application Project Planning Criteria Evaluation
Fairness Targets Criteria ; Deployment
in Requirements a Monitoring
riteria ———

“Feedback Loop

Fig. 2. Mind map of the coding scheme for the fairness certification of NLP approaches

In the following, we explain the coding of our six criteria together with their
sub-criteria captured by sub-codes. The intervieweees who addressed them are
denoted in brackets. Criteria that were mentioned by many interviewees tend to
be more important for a fairness certification of NLP approaches.

Process Criteria refer to the implementation of a fairness certification pro-
cess. Our interviewees identified three distinguished aspects of process criteria,
which we modeled as sub-codes.

The sub-code Fairness Understanding captures properties of fairness concep-
tion, that should be considered for fairness certification according to our inter-
views. Examples include cultural dependence (Ig, 13, ia), use case dependence
(h, Ts, Is, Ie, Iz, Ig, tia, Kis, Ta), dynamic (hs, Ta) and non-binary (hie) prop-
erties. It also covers the interviewees’ conceptions of what is to be considered
fair, which is nuanced, and varies between conceptions like conditional statistical
parity (4, l4, Ie, 17), counterfactual fairness (3, I4, Is, Io, 3) and notions of
inclusiveness (h, Ih, Iz, Ig, Ig, Iii).

The sub-code Certification Market Factors covers how a certification and
regulation may affect each other (%, J3, ly, Ie, fia), the market which may
build around certification (J;) and the adoption of certification dependent on
alignment with corporate goals (1, lo, fi, fis, fia).

In Design of Assessment, we capture properties for an assessment and its
scope. A requirement is finding a way to create an audit process that is as
holistic as possible (h, Ie, Is, Io, fii, lea, 3, fia). A certification shouldn’t
put small companies at a disadvantage (Js, Is, Ig, [i1). Interviewees discuss
the approach to certification. They propose, for instance, to certify individuals


Fairness Certification 9

instead of processes (Js, Ig, Ig). Building a certification on existing standards
and best practices is also agreed upon (ih, Js, Js, io, fii, Lis, [14). Another
major point of discussion was to what extent a certification process should be
risk-dependent in its scope (hh, , Io, fia) or in its obligation to be performed
(do, Tio, it).

Governance Criteria refer to all measures the certified organization should
undertake to ensure the integrity of its processes and management. Recall that
governance is important both from the perspective of the auditing and the au-
dited organization. We have identified two sub-codes for governance criteria,
which encompass several aspects.

The sub-code Model Reporting & Transparency follows the idea of disclosing
biases, basic information, data understanding, model explanations, and evalua-
tion. For basic information, our interviewees want fields of application (14, [3),
the underlying fairness definition, and the frameworks the system was built on
(14) to be disclosed. Data understanding covers all relevant information about the
data the system utilizes and how it is processed (14, 2, 43). Model explanations
cover disclosing the model with its architecture (4, Ig, 2) and explainability
of the model (h, 13, ly, Ig, Is, Ii). For evaluation, interviewees bring up that
reporting should contain what has been tested regarding the robustness of the
model against biases (J2, J4). Evaluation metrics and their resulting data from
testing should be publicly documented (1, Jg). Reporting data bias (J,), model
bias (J3), and countermeasures against biases that are implemented (Js, [i3)
should also be done. As disclosure mechanisms, our interviewees suggest utiliz-
ing Model Cards (h, I3, 4, Ig, Lie, 3, 14), open sourcing the model entirely
(Js) or giving the users an option to access relevant information to the prediction
the model made for them on demand (J).

The sub-code Organizational Criteria targets both the audited and the au-
diting organization. Interviewees highlight that the context of the auditing or-
ganization should be reflected because it can impede its ability to make neutral
and holistic assessments. This involves the region it is located in (113), the po-
litical system it is integrated into (43), its initiator (13), its integration in the
economic system (Jz, Ig), or the lack in diversity of professional backgrounds of
its auditors (Ig, Ig). The auditing organization consolidates best practices (J,
Ig, Ig, iz). For the audited organization the diversity of the development team
(oe, Ia, tio, Lui, 13), internal accountability mechanisms and employee qualifi-
cation and training should be checked. Internal accountability involves checking
for internal assessments regarding fairness that are put in place by the audited
organization (17, 4). Roles should be assigned clear responsibilities regarding
fairness (1, Lb, Is, Ig, Iz, Io, Tio, tii, fia). Interviewees mention that account-
ability also needs someone who is responsible for fairness in the organization
and supervision targeted at fairness (J3, Is, Ig, I7, Iii). Employee qualification
& training are seen by interviewees to provide a reasonable job fit for employees
(3, Ig). A holistic overview of the process of training and operating an NLP sys-
tem and what could go wrong regarding fairness (Jg, Ig) should also be provided.


10 Freiberger et al.

Moreover, the interviews reveal the importance of raising general awareness of
bias issues and ensuring general knowledge about biases (11, J3, I4, Ig, I7, 11).

Project Planning Criteria address processes in the planning stage of a
product and its underlying business problem that the certification institution
should assess. Their scope ends right before data understanding in the CRISP-
DM framework. This involves two sub-codes.

For sub-code Define & Assess the Planned Application, the interviewees agree
about checking definitions of use case (14, Ig, Ig, Lia, Li3, Lia) and stakeholders
(6, 17, Ig, 11) provided by the audited organization. This entails an assessment
for the latter regarding fairness issues or vulnerabilities to fairness risks. This
involves checking that mechanisms are in place to properly understand users and
stakeholders, as well as the fairness properties of the use case itself. Moreover,
the planned solution should fit the use case, also from a fairness perspective
Us, Ly, Is, 2). The sub-code Fairness Targets in Requirements implies checking
if a standardized set of requirements has been established to ensure fairness
regarding the project over its life cycle (Io, I4, Io).

With Data-related Criteria, we refer to all assessment processes regarding
data used for training and retraining as well as regarding procedures for handling
and transforming data. Those procedures are described by six sub-codes.

The sub-code Data Assessment comprises an assessment of data quality re-
garding fairness by checking representativeness via distribution checks (, hi,
Ig, Ig, Tig) and by assessing regarding harmful, toxic or incorrect data (Io, Li,
Tio, 3, fa). Interviewees suggest introducing processes for bias checks (1, bh,
Iy, Ts, Tz, Ig, Th) and assessments of the sourcing (h, ty, Ih, Is, Ig, Ig, Tha, Iy3)
and collection (Js, Iie, Ia) of data. Sub-code Annotation captures characteris-
tics of the annotation process to support fairness. Annotating fairness relevant
attributes (1, 1, I11) is suggested by interviewees. Moreover, comprehensive an-
notation guidelines (14, Jg, J7) and inter-annotator agreements (14, I7, Tio, Lis)
are important considerations. Sub-code Preprocessing focuses on requirements
for filtering (Js, Ty, Ti3), selecting (h, Ih, Tz, Ig, Tho, Tha, Ih3), anonymizing (Js,
Is, I7, 14) or mapping the data robustly (J) to ensure fairness. The sub-code
Data for Evaluation entails that suitable, representative data is used (Ih, Ig, [1)
and discusses where to take it from. Interviewees highlight the importance of
fairness invariance testing (Ig, I3). Sub-code Data for Continuous Improvement
comprises a monitoring process during the operation of the model and a feed-
back loop targeting improvements and counteracting fairness issues. As criteria
for monitoring, interviewees mention fairness test sets (Jo, Is, Ig, Ig, Io), drift
monitoring (Jo, I3, 4, Js, Is, I7, Tis), as well as a request assessment for under-
represented groups (Jz) as practices to be implemented by the audited company.
Feedback loop practices regarding data need to consider how in-use data like user
behavior, usage data or prediction confidence (Is, Ig, Io) and user-made correc-
tions (J,) are handled to improve or maintain fairness. Finally, the sub-code
Data Storage covers how data should be stored in development and operations
(1,0) and cached in operations (;).


Fairness Certification 11

The Modeling & Evaluation Criteria include the two sub-codes Modeling
and Evaluation, which can be explained as follows:

Modeling is about embedding fairness into the model architecture itself. In-
terviewees mention integrating hard-coded elements into the system to ensure
fairness (Iy, J5, Ig, 14), training a separate fairness classifier for the model out-
puts (Js), working with constraints (J2) or embedding fairness into the model’s
optimization itself (14, Is, I7, lio).

Evaluation investigates what tests should be conducted to evaluate a model
and how to assess the fairness of a trained model. At the center of this intervie-
wees discuss functional testing criteria which take an outside perspective on the
system’s outputs given some specific inputs. Interview partners came up with six
concepts relevant to this which are validation of predictions (hh, 1, Je), involving
affected stakeholders in evaluation (J5), adversarial testing (1, ly, I7), data sets
or benchmarks for testing (Jo, Is, Ig, Is, Io, Li1, [i3), criteria focused on human-
computer interaction (Jo, I7, [:,) and ethics criteria (1,, 4). Another important
consideration for evaluation is what metrics should be utilized to test a system’s
fairness. Interviewees name metrics focused on robustness or generalization as
an essential element to measure the model’s likelihood of generating unexpected,
unfair results (4, 1, Ig). Impact-based metrics depend on the chosen fairness
paradigm (13, I7, Ig, 2, fa). Another approach proposed by [io introduces a
metric that penalizes the model’s use of sensitive attributes or unethical content
when calculating loss.

Operations Criteria center around what should be considered regarding
fairness when deploying a model and what mechanisms must be implemented to
maintain fairness over time. That subsumes three sub-codes.

Sub-code Deployment centers around assessments suited to shipping differ-
ent model sizes and pruning models, which affect their fairness behavior (h,
13, Ig). Interviewees define the sub-code Monitoring as a continuous assessment
procedure of the audited company. This is to ensure its system’s fairness over
time, involving fairness tests with the roll-out of updates (h, bb, Js, Iz) and
intervention strategies to counteract adversarial or unintentional, but harmful,
misuse (Jo, Jip, 13). Finally, with the sub-code Feedback Loop, interviewees aim
to get the flow of information back from the operational model assessed. This
involves checking for the option of flagging system outputs that are perceived as
unfair by users and for an option for users to make corrections (4, b, Js, I7).
Interviewees consider leveraging the data which is acquired by that to under-
stand fairness issues (11, Jo, Js, I7) and, in some cases, trigger retraining (i, Js,
I,). An assessment includes checking if data utilized for retraining the system
in continuous improvement is subjected to the same assessments, filtering and
preprocessing steps as the initial data (Ji).

Table |2| provides an overview of which of our interviewees addressed which
(sub-)code. A comparison of this table with Table }1} indicates that we indeed
obtained a broad range of experts, which are likely to cover the entire range of
the fairness certification process for NLP approaches.


12 Freiberger et al.

Table 2. Second-level coding scheme for fairness certification of NLP approaches

Fi, | Be Bs | Lao To | Tz | Ts | Ao | Tio | Tis | 2 | is | ia
Process Fairness VIV IV IV IV IV IV IV VI lv
Criteria Understanding
Design of Assessment|V|V JV |v |v Jv] |W iv iv iv lv lM
Certification Market|V v
Factors
Project Define & Assess the] |W VIV IV IV IV |v iv LV I I
Planning Planned Application
Criteria
Fairness Targets in} |v] |v v
Requirements
Data-related |Data Assessment ‘ACT ALALALALALALALeLerareareans
Criteria
Annotation V |v Arata Viv|[v iv
Preprocessing Vi |v Iv Iw} lw wm VivIiv |v Iv
Data for Evaluation V\v Viv |v Vv i|Vv
Data for Continuous] |v /Vv |v |v |v |W Viv v
Improvement
Data Storage v v
Modeling & Modeling AVA ALaAtata v v
Evaluation
Criteria
Evaluation ‘ACT ALALALAL ALAA aneanareareans
Operations Deployment V\v v
Criteria
Monitoring VAPARA APARNA v
Feedback Loop V |v |v |v ,ALArarans
Governance Model Reporting &|V|Vv |v |v] |W Iv iw Viv iv
Criteria Transparency
Organizational V IV IV IY] IVI IV Vv I Vv I
Criteria

5 Discussion

In this section, we briefly discuss our results. We were interested to learn the en-
tire range of criteria that must be considered to establish fairness certification for
an NLP approach. To this end, we have conducted and coded 14 semi-structured
interviews with a wide span of diverse experts from business and research. Thus,
we think that our findings are well applicable to certifying fairness in a corpo-
rate environment. However, we did not cover other sectors, such as public or
military. Furthermore, we might not have reached theoretical saturation with
14 interviewees. Moreover, designing and testing a fairness certification process
itself was beyond our scope.

One might wonder if this work on fairness and bias was influenced by bias
itself. We explicitly tried to exclude the following biases: Selection bias (selecting
interview partners on personal preferences), bias in materials (providing docu-


Fairness Certification 13

ments before the interviews could have influenced the interviewees), verbal/non-
verbal bias (due to misunderstandings between interviewer and interviewee) and
bias in data analysis (subjective coding).

We observed, that our interviewees focused particularly on criteria for data
and functional testing of solutions. However, there is a bias in the relevance
perception of modeling between NLP fairness research and interviewees’ per-
spectives. Existing research already considers model architectures inhibiting so-
cial biases, for instance, via regularization adversarial training (6\{72| or
adapting the loss function to support fairness 73)[74]. Even though approaches
mentioned in interviews are consistent with the literature, the topic was barely
mentioned or deemed relatively unimportant in interviews (1, [5). Future re-
search may investigate this mismatch.

6 Conclusion

Fairness certification for natural language processing approaches such as large
language models, Al-based chatbots or healthcare applications is an important
issue, that is still unresolved. In this paper, we have conducted and analyzed 14
semi-structured expert interviews with mostly NLP experts in the industry and
two algorithmic fairness experts in academia. Our interviewees helped us identify
six main criteria and 18 criteria on the second hierarchy level of an open coding
scheme for certifying the fairness of an NLP approach. Those criteria are an
important building block towards operationalizing and testing NLP processes to
certify fairness, from the perspective of the auditor as well as from the perspective
of the audited organization.

Our interviewees have raised plenty of open questions for future research.
For instance: How should a certification process handle the use case dependence
of fairness or its non-binary nature and subjectiveness while in a dynamic en-
vironment? To what extent would it make sense to make such a certification
mandatory? On which best practices and standards should a certification be
built? How extensive should it be? Finally, how must a certification process be
structured to specifically address large language models?

References

1. A. Wong, J. M. Plasek, S. P. Montecalvo, and L. Zhou, “Natural language process-
ing and its implications for the future of medication safety: A narrative review of
recent advances and challenges,” Pharmacotherapy: The Journal of Human Phar-
macology and Drug Therapy, vol. 38, no. 8, pp. 822-841, 2018.

2. I. Lopatovska, K. Rink, I. Knight, K. Raines, K. Cosenza, H. Williams, P. Sorsche,
D. Hirsch, Q. Li, and A. Martinez, “Talk to me: Exploring user interactions with
the amazon alexa,” Journal of Librarianship and Information Science, vol. 51,
no. 4, pp. 984-997, 2019.

3. OpenAI, “ChatGPT [large language model],” https: //chat.openai.com| 2023.


14

10.

11.

12.

13.

14.

15.

16.

17.

18.

Freiberger et al.

S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach, “Language (technology)
is power: A critical survey of “bias” in nlp,” in Proceedings of the 58th annual
meeting of the association for computational linguistics, 2020, pp. 5454-5476.

N. Markl, “Language variation and algorithmic bias: understanding algorithmic
bias in British English automatic speech recognition,” in 2022 ACM Conference
on Fairness, Accountability, and Transparency, 2022, pp. 521-534.

. T. Sun, A. Gaut, S. Tang, Y. Huang, M. ElSherief, J. Zhao et al., “Mitigating

gender bias in natural language processing: Literature review,” in Proceedings of

the 57th Annual Meeting of the Association for Computational Linguistics, 2019,
pp. 1630-1640.

. G. Stanovsky, N. A. Smith, and L. Zettlemoyer, “Evaluating gender bias in machine

translation,” in Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 2019, pp. 1679-1684.

. A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics derived automatically

from language corpora contain human-like biases,” Science, vol. 356, no. 6334, pp.
183-186, 2017.

. L. Weidinger, J. Uesato, M. Rauh, C. Griffin, P.-S. Huang, J. Mellor et al., “Tax-

onomy of risks posed by language models,” in 2022 ACM Conference on Fairness,
Accountability, and Transparency, 2022, pp. 214-229.

T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai, “Man is to
computer programmer as woman is to homemaker? debiasing word embeddings,”
in Advances in neural information processing systems, 2016, pp. 4349-4357.

O. Papakyriakopoulos, S. Hegelich, J. C. M. Serrano, and F. Marco, “Bias in word
embeddings,” in Proceedings of the 2020 conference on fairness, accountability, and
transparency, 2020, pp. 446-457.

R. Tatman, “Gender and dialect bias in YouTube’s automatic captions,” in Pro-
ceedings of the first ACL workshop on ethics in natural language processing, 2017,
pp. 53-59.

A. Ovalle, P. Goyal, J. Dhamala, Z. Jaggers, K.-W. Chang, A. Galstyan, R. Zemel,
and R. Gupta, ““i’m fully who i am”: Towards centering transgender and non-
binary voices to measure biases in open language generation,” in Proceedings of
the 2023 ACM Conference on Fairness, Accountability, and Transparency, 2023, p.
1246-1266.

S. Hassan Awadallah, M. Huenerfauth, and C. O. Alm, “Unpacking the interde-
pendent systems of discrimination: Ableist bias in nlp systems through an intersec-
tional lens,” in Findings of the Association for Computational Linguistics: EMNLP
2021, 2021, pp. 3116-3123.

B. Bridgeman, C. Trapani, and Y. Attali, “Comparison of human and machine
scoring of essays: Differences by gender, ethnicity, and country,” Applied Measure-
ment in Education, vol. 25, no. 1, pp. 27-40, 2012.

S. L. Blodgett and B. O’Connor, “Racial disparity in natural language pro-
cessing: A case study of social media african-american english,” arXiv preprint
arXiv:1707.00061, 2017.

M. Diaz, I. Johnson, A. Lazar, A. M. Piper, and D. Gergle, “Addressing age-related
bias in sentiment analysis,” in Proceedings of the 2018 chi conference on human
factors in computing systems, 2018, pp. 1-14.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakan-
tan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,”
Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.


19

20.

21.

22.

23.

24.

25.

26.

2f.

28.

29.

30.

31.

32.

33.

34.

36.

Fairness Certification 15

S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel, “Counterfac-
tual fairness in text classification through robustness,” in Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society, 2019, pp. 219-226.

M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen, “A survey
on bias and fairness in machine learning,” arXiv preprint arXiv:2010.00711, 2020.
B. Lepri, N. Oliver, E. Letouzé, A. Pentland, and P. Vinck, “Fair, transparent,
and accountable algorithmic decision-making processes,” Philosophy & Technology,
vol. 31, no. 4, pp. 611-627, 2018.

A. Chouldechova and A. Roth, “A snapshot of the frontiers of fairness in machine
learning,” Communications of the ACM, vol. 63, no. 5, pp. 82-89, 2020.

P. Cihon, M. J. Kleinaltenkamp, J. Schuett, and S$. D. Baum, “Ai certification: Ad-
vancing ethical practice by reducing information asymmetries,” [EEE Transactions
on Technology and Society, vol. 2, no. 4, pp. 200-209, 2021.

S. Verma and J. Rubin, “Fairness definitions explained,” in 2018 ACM/IEEE In-
ternational Workshop on Software Fairness, 2018, pp. 1-7.

N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on
bias and fairness in machine learning,” ACM Computing Surveys, vol. 54, no. 6,
pp. 1-35, 2021.

A. Chouldechova, “Fair prediction with disparate impact: A study of bias in re-
cidivism prediction instruments,” Big Data, vol. 5, no. 2, pp. 153-163, 2017.

M. Defrance and T. De Bie, “Maximal fairness,” in Proceedings of the 2023 ACM
Conference on Fairness, Accountability, and Transparency, 2023, p. 851-880.

N. Kallus and A. Zhou, “Residual unfairness in fair machine learning from preju-
diced data,” in International Conference on Machine Learning. PMLR, 2018, pp.
2439-2448.

I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-
Loud, D. Theron, and P. Barnes, “Closing the ai accountability gap,” in Proceedings
of the 2020 conference on fairness, accountability, and transparency, 2020, pp. 33-
44.

P. Adler, C. Falk, S. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith,
and S. Venkatasubramanian, “Auditing black-box models for indirect influence,”
Knowledge and Information Systems, vol. 54, no. 1, pp. 95-122, 2018.

C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through
awareness,” in Proceedings of the 3rd innovations in theoretical computer science
conference, 2012, pp. 214-226.

S. Park, S. Kim, and Y. Lim, “Fairness audit of machine learning models with
confidential computing,” in Proceedings of the ACM Web Conference 2022, 2022,
pp. 3488-3499.

S. Segal, Y. Adi, B. Pinkas, C. Baum, C. Ganesh, and J. Keshet, “Fairness in the
eyes of the data: Certifying machine-learning models,” in Proceedings of the 2021
AAAI/ACM Conference on AI, Ethics, and Society, 2021, pp. 926-935.

R. N. Landers and T. S. Behrend, “Auditing the ai auditors: A framework for evalu-
ating fairness and bias in high stakes ai predictive models,” American Psychologist,
2022.

. 5S. Costanza-Chock, I. D. Raji, and J. Buolamwini, “Who audits the auditors?

recommendations from a field scan of the algorithmic auditing ecosystem,” in 2022
ACM Conference on Fairness, Accountability, and Transparency, 2022, pp. 1571—
1583.

F. Petersen, D. Mukherjee, Y. Sun, and M. Yurochkin, “Post-processing for indi-
vidual fairness,” in Advances in Neural Information Processing Systems, vol. 34,
2021, pp. 25 944-25 955.


16

37.

38.

39.

40.

Al.

42.

43.

44,

46.

A7.

48.

49.

50.

ol.

52.

53.

Freiberger et al.

C. Starke, J. Baleis, B. Keller, and F. Marcinkowski, “Fairness perceptions of
algorithmic decision-making: A systematic review of the empirical literature,” Big
Data & Society, vol. 9, no. 2, 2022.

Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language process-
ing (nlp) in management research: A literature review,” Journal of Management
Analytics, vol. 7, no. 2, pp. 139-172, 2020.

E. D. Liddy, “Natural language processing,” Encyclopedia of Library and Informa-
tion Science, vol. 2126, p. 2140, 2001.

D. D. Otter, J. Medina, and J. Kalita, “A survey of the usages of deep learn-
ing for natural language processing,” [EEE Transactions on Neural Networks and
Learning Systems, vol. 32, no. 2, pp. 604-624, 2021.

D. Khurana, A. Koli, K. Khatter, and S. Singh, “Natural language processing: State
of the art, current trends and challenges,” Multimedia Tools and Applications, pp.
1-32, 2022.

J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack, “Sentiment analyzer: Extracting
sentiments about a given topic using natural language processing techniques,” in
Third [IEEE international conference on data mining, 2003, pp. 427-434.

D. McDonald, “Natural language generation,” Handbook of Natural Language Pro-
cessing, vol. 2, pp. 121-144, 2010.

A. Matthews, I. Grasso, C. Mahoney, Y. Chen, E. Wali, T. Middleton et al., “Gen-
der bias in natural language processing across human languages,” in Proceedings of
the First Workshop on Trustworthy Natural Language Processing, 2021, pp. 45-54.
M. Ashok, R. Madan, A. Joha, and U. Sivarajah, “Ethical framework for arti-
ficial intelligence and digital technologies,” International Journal of Information
Management, vol. 62, no. 2, p. 102433, 2022.

E. Ntoutsi, P. Fafalios, U. Gadiraju, V. Iosifidis, W. Nejdl, M.-E. Vidal et al.,
“Bias in data-driven artificial intelligence systems—an introductory survey,” Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 10, no. 3,
pp. 1-14, 2020.

R. Wirth and J. Hipp, “CRISP-DM: Towards a standard process model for data
mining,” in Proceedings of the 4th international conference on the practical appli-
cations of knowledge discovery and data mining, vol. 1, 2000, pp. 29-39.

M. Skirpan and M. Gorelick, “The authority of “fair” in machine learning,” arXiv
preprint arXiv:1706.09976, 2017.

A. Schmidt and M. Wiegand, “A survey on hate speech detection using natural
language processing,” in Proceedings of the fifth international workshop on natural
language processing for social media, 2017, pp. 1-10.

A. Z. Jacobs, 8S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach, “The
meaning and measurement of bias: Lessons from natural language processing,” in
Proceedings of the 2020 conference on fairness, accountability, and transparency,
2020, p. 706.

R. Binns, “On the apparent conflict between individual and group fairness,” in
Proceedings of the 2020 conference on fairness, accountability, and transparency,
2020, pp. 514-524.

L. Cabello, A. K. Jorgensen, and A. Sggaard, “On the independence of association
bias and empirical fairness in language models,” in Proceedings of the 2023 ACM
Conference on Fairness, Accountability, and Transparency, 2023, p. 370-378.

R. Bellamy, K. Dey, M. Hind, S. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Mar-
tino, S. Mehta, A. Mojsilovic et al., “Ai fairness 360: An extensible toolkit for
detecting and mitigating algorithmic bias,” [BM Journal of Research and Devel-
opment, vol. 63, no. 4/5, pp. 4-1, 2019.


54.

55.

56.

57.

58.

59.

60.

61.

62.

63.

64.

65.

66.

67.

68.

69.

70.

71.

Fairness Certification 17

S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamil-
ton, and D. Roth, “A comparative study of fairness-enhancing interventions in ma-
chine learning,” in Proceedings of the conference on fairness, accountability, and
transparency, 2019, pp. 329-338.

I. Chen, F. D. Johansson, and D. Sontag, “Why is my classifier discriminatory?”
in Advances in neural information processing systems, 2018, pp. 3543-3554.

F. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R. Varshney, “Op-
timized pre-processing for discrimination prevention,” in Proceedings of the 31st
international conference on neural information processing systems, 2017, pp. 3995—
4004.

A. Roy, J. Horstmann, and E. Ntoutsi, “Multi-dimensional discrimination in law
and machine learning - a comparative overview,” in Proceedings of the 2023 ACM
Conference on Fairness, Accountability, and Transparency, 2023, p. 89-100.

C. Harris, M. Halevy, A. Howard, A. Bruckman, and D. Yang, “Exploring the role
of grammar and word choice in bias toward african american english (aae) in hate
speech classification,” in 2022 ACM Conference on Fairness, Accountability, and
Transparency, 2022, pp. 789-798.

J. Chen, I. Berlot-Attwell, X. Wang, S. T. Hossain, and F. Rudzicz, “Exploring text
specific and blackbox fairness algorithms in multimodal clinical nlp,” in Proceedings
of the 3rd Clinical Natural Language Processing Workshop, 2020, pp. 301-312.

I. 17000:2020, Conformity assessment-Vocabulary and general principles. ISO,
2020.

IEEE, “Ieee standard for software reviews and audits,” [EEE Std, vol. 1028, pp.
1-53, 2008.

S. Lins, T. Kromat, J. Lébbers, A. Benlian, and A. Sunyaev, “Why don’t you join
in? a typology of information system certification adopters,” Decision Sciences,
vol. 53, no. 3, pp. 452-485, 2022.

J. Lansing, A. Benlian, and A. Sunyaev, ““unblackboxing” decision makers’ inter-
pretations of is certifications in the context of cloud service certifications,” Journal
of the Association for Information Systems, vol. 19, no. 11, pp. 1064-1096, 2018.

N. Scharowski, M. Benk, S. J. Kiihne, L. Wettstein, and F. Briihlmann, “Certifica-
tion labels for trustworthy ai: Insights from an empirical mixed-method study,” in
FAccT ’28: Proceedings of the 2023 ACM Conference on Fairness, Accountability,
and Transparency, 2023, pp. 248 — 260.

M. D. Myers, Qualitative research in business & management. Sage Publications
Limited, 2020.

J. Corbin and A. Strauss, Basics of qualitative research techniques and procedures
for developing grounded theory. Sage publications, 2015.

, “Grounded theory research: Procedures, canons, and evaluative criteria,”
Qualitative sociology, vol. 13, no. 1, pp. 3-21, 1990.

M. Jakesch, Z. Bucinca, S. Amershi, and A. Olteanu, “How different groups pri-
oritize ethical values for responsible ai,” in 2022 ACM Conference on Fairness,
Accountability, and Transparency, 2022, pp. 310-323.

T. Dresing and T. Pehl, Prazisbuch interview, transkription & analyse: Anleitungen
und regelsysteme ftir qualitativ forschende. Dr. Dresing & Pehl GmbH, 2018.

T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, “Fairness-aware classifier with
prejudice remover regularizer,” in Machine Learning and Knowledge Discovery in
Databases. Springer, 2012, pp. 35-50.

M. Veale and R. Binns, “Fairer machine learning in the real world: Mitigating
discrimination without collecting sensitive data,” Big Data & Society, vol. 4, no. 2,
p. 2053951717743530, 2017.

66 66.



18

72.

73.

74.

75.

76.

77.

78.

79.

A

Freiberger et al.

M. Yurochkin and Y. Sun, “SenSel: Sensitive set invariance for enforcing individual
fairness,” arXiv preprint arXiv:2006. 14168, 2020.

C. Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson, “Decoupled classifiers for
group-fair and efficient machine learning,” in Conference on fairness, accountability
and transparency, 2018, pp. 119-133.

F. Kamiran, A. Karim, and X. Zhang, “Decision theory for discrimination-aware
classification,” in 2012 IEEE 12th International Conference on Data Mining, 2012,
pp. 924-929.

S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Algorithmic de-
cision making and the cost of fairness,” in Proceedings of the 23rd acm sigkdd
international conference on knowledge discovery and data mining, 2017, pp. 797—
806.

M. J. Kusner, J. Loftus, C. Russell, and R. Silva, “Counterfactual fairness,” in
Proceedings of the 31st international conference on neural information processing
systems, 2017, pp. 4066-4076.

M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson,
E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model reporting,” in Pro-
ceedings of the conference on fairness, accountability, and transparency, 2019, pp.
220-229.

T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi et al., “Hugging-
face’s transformers: State-of-the-art natural language processing,” arXiv preprint
arXiv:1910.08771, 2019.

M. Pushkarna, A. Zaldivar, and O. Kjartansson, “Data cards: Purposeful and trans-
parent dataset documentation for responsible AI,” in 2022 ACM Conference on
Fairness, Accountability, and Transparency, 2022, pp. 1776-1826.

Interview Guide

Table 3: Interview guide on criteria for fairness certification of NLP approaches

Min

Goal Contents and exemplary questions Expected

results

ti Buffer, wel-
come

Recent projects, travel,... relaxed start

Sort out or-
ganizational

Obtain consent for recording,
of work, outline process of

outline objective
interview, outline

GDPR, research
goal and proce-

matters

anonymization and tools used

dure clear

15

Understand
background
of intervie-
wee

In which step of the AI lifecycle are you primarily
in contact with NLP systems?

What points of contact have you had with fairness
so far?

What incidents or issues have there been with it?
What contact points might there have been about
certification?

Contact points
to topic, profes-
sional practice
of the intervie-
wee

18

Getting
started with
the topic

What does fairness of AI or NLP mean to you? How
would you define fairness of NLP?

What are your initial thoughts on the topic of fair-
ness certification of AI? And NLP specifically?

Terminology
clarified, entry
point



Fairness Certification 19

10

28

Criteria for
NLP system
develop-
ment

Which requirements should be made to processes
in development to ensure fairness? Why? (Business
understanding, data understanding, preprocessing,
annotation, modeling, explainability, evaluation)
What are the organizational requirements that
should be considered for fairness certification of|
NLP? Why? (Team, structure)

What must suppliers of software components adhere
to in order to maintain fairness of the end product?
Do you see any other issues in the NLP systems
development phase that have not yet been raised?
What are the biggest challenges for NLP providers?
Where are the opportunities?

Answers to
questions for de-
velopment crite-
ria; contextual
understanding

10

38

Criteria for
NLP system
operation

Are there any aspects regarding deployment that
should be considered?

How does an AI system need to be monitored af-
ter it goes live to detect fairness issues that arise?
Who needs to be involved and how is the monitoring
organized?

How often and how extensively do you think fairness
problems should be checked? How might such an
audit be conducted? What are the key issues that
need special attention?

What processes should be initiated in case of fair-
ness violations? How can improvements be made?
To what extent is the interaction of users with the
system to be controlled? Why?

What interactions of the user with the system have
to be considered?

What are opportunities and challenges for NLP
providers?

Understanding
how to maintain
fairness in an
operational
system and why
this is so, and
how this can
be captured in
criteria

43

Clear out
open topics

Are there any other criteria apart from the ones dis-
cussed so far that you would use for fairness certifi-
cation in NLP?

What else is important to you to say about fairness
certification in NLP?

Additional
dimensions

45

Pointing
out next
steps

Describe follow-up.
Do you know anyone else who could help me with
an interview?

Further points
of contact

B

Detailed Interview Findings

B.1 Process Criteria

Process Criteria target everything generally relevant to establishing a relevant fairness
certification process. Interviewees describe concepts that can be clustered in the cate-
gories of fairness understanding, design of assessment, and certification market factors.

Fairness Understanding Having a good understanding of the fairness properties
for the given application can be seen as an essential foundation for building an effective
certification process (12, [i4). Findings follow the structure of codes identified in open
coding, as introduced in the previous chapter. It makes sense to keep perspective on the
findings by first introducing subjective fairness understandings of interviewees. Some
tended to focus on an equal opportunity aspect, others on equal performance or other
conceptualization. The different perspectives of interviewees on fairness also may mani-
fest in their suggestions for certification measures. Interviewees one, four, six, and seven
share the fairness perspective of Fairness through Awareness (11, l4, Ie, 7) which aims
at providing similar outcomes to similar individuals [31]. This similarity establishes fair


20 Freiberger et al.

treatment on a meritocratic baseline (114). Hence, factors that are objectively relevant
to a decision, like skills, should be the baseline for a decision. Conditional statistical
parity could also be utilized as a suitable fairness conception from the literature.
A form of enablement that may account for socioeconomic backgrounds, like migration
(L14), could also be considered (i4). That would allow for the reduction of structural
inequalities like gender discrimination in our society (J6). The target could be changing
these structural inequalities by actively intervening (1, l4, I7). This concept could be
captured by statistical parity, where the demographics of groups that receive a positive
(or negative) classification meet the demographics of the overall population 31]. This
conception, however, comes with the limitation that a group’s representation in the
overall population needs to be large enough to be considered. Treatment based on a
meritocratic baseline is closely related to the notion of a non-discriminatory system.
It avoids an unjustifiable negative impact on affected individuals (J3, Js) and is how
interviewees three, four, five, ten, and 13 view fairness (J3, 4, Is, ho, 3). It closely
resembles the literature’s counterfactual fairness definition, which emphasizes decision
invariance to a protected attribute (76). It may depend on the use case in its context
when utilizing a protected attribute to justify negative impact may be acceptable (Js,
I;). As mentioned by interviewees two, four, seven, eight, nine, and eleven, inclusive-
ness ties closely into this notion (J2, L4, I7, Is, Is, 1). It means a system should not be
restricted in accessibility and usability, and there should not be a barrier to usage by a
group (J2, f1). Ideally, the system also represents a diverse spectrum and avoids echo
chambers (4, I7, Ig). There can also be trade-offs between fairness dimensions. For
instance, striving for inclusiveness can impair fairness regarding counterfactual fair-
ness in a recruitment context (J). Interviewees two, four, nine, and eleven emphasize
equal performance as their subjective fairness understanding (12, Ia, Io, 1). It focuses
on delivering the same performance to each user independent of group membership.
The system can cope, for instance, with the variance of expression without sacrific-
ing performance for certain groups (J9). It may even be interpreted as an equal user
experience (J9). Literature also employs the mathematical fairness definition of coun-
terfactual fairness to capture this conception of fairness (76). For some use cases like
voice assistants, this fairness conception can be even business-critical due to user di-
versity (2, Ig). This subjective nature makes it more difficult to clearly define fairness
in a certification process (, J3, Is, 2). Defining fairness before an assessment is an
important factor for operationalizing a fairness certification (Js, Is, Io, 2, lia). An-
other challenge in defining fairness for an assessment is rooted in the strong dependence
on context imposed by the use case (i, J3, Is, Ie, I7, Is, he, sz, ha). This use case
dependence is particularly highlighted by interviewees three, twelve, and 14. The use
case may, for instance, determine whether procedural or distributional fairness should
be the focus (Jia). Required criteria for designing content for an assessment may vastly
differ between fairness conceptions (Ig, 14). Some interviewees see the large variance in
use cases as a reason for calibrating an assessment to the specific use case by adapting
its contents to the fairness situation the application imposes (Je, I7, Iii, ha). It may
make sense to employ different fairness definitions depending on the affected stake-
holders and context for the certification process, as discussed in “Project Planning
Criteria” for the audited company, and adapt the certification process accordingly (Js,
Ig). Moreover, a dynamic environment implies changing fairness perceptions over time,
so fairness definitions for stakeholders may need to be reassessed (/i3, [i4). Ethics and
moral considerations are closely related to fairness (Ig, Io, Lo, fia). A fairness defi-
nition can be grounded in the underlying definition of ethics (Jg). Interviewee eight
quotes Joscha Bach to define ethics as a “[r]ational process negotiation with yourself


Fairness Certification 21

and others to decide how to resolve a conflict of interest under the condition of shared
value” (/Jg). Hence, grounding a fairness definition in ethics could cope with the varying
fairness conceptions (Jg). An assessment should ideally not result in a binary outcome
in the form of fair vs. not fair, but it should be more nuanced, describing to what
degree which fairness concepts are met (12). Otherwise, it is difficult to determine the
point when something starts to become unfair (Jg). It is crucial to view fairness as
a societal issue as our fairness perception is culturally rooted in the same way some
biases are (J6, fi2). Different values in another cultural context may cause a regional
dependence of fairness (Jo, 13, lia). The fairness perception in China may not align
with the typical European fairness understanding, for instance (113). This societal bias
poses a challenge for assessing and operating models on a global scale (113). In practice,
the topic of fairness in NLP systems is barely considered by most development teams
(11). That partially leads to a lack of understanding and awareness of the problems
that must be addressed in a certification process (1, ha). Besides a lack of practical
understanding of creating a good fairness certification process, some fundamental in-
sights from research to ground such a process are still missing (fia). Fairness should
not be viewed as an isolated concept as it is entangled with other variables of Respon-
sible Artificial Intelligence (RAI) (h, J7, Jo). Transparency, for instance, has already
been addressed as a substantial part of a fairness certification. Data security, privacy,
and reliability are also named as influencing factors on fairness (11, I7, Io). Improving
such should not affect fairness negatively but rather benefit it, making an integrated
assessment of fairness with other RAI variables interesting (Jo).

Design of Assessment Interviewees also give insight into what they consider a
suitable design of an assessment for certifying fairness. That includes general prop-
erties of the certification process that the certifying institution should consider and
information about the scope of an assessment. The hierarchical open coding scheme
for “Design of Assessment” is displayed in Figure]

Fig. 3. Criteria relevant to “Design of Assessment” hierarchically mapped

Assessment Properties

Design of Assessment

Assessment Properties: Assessment properties describe how an assessment should
be approached and what may be considered when creating such an assessment proce-
dure. However, potential roadblocks need to be considered for an assessment to gain
traction. First, the feasibility of such a holistic certification process that does not in-
duce unfairness among industry players is questioned to a certain extent (Js, Js, I,
I). Apart from the resource intensity of undergoing such a certification process which


22 Freiberger et al.

makes it inaccessible for small players (11, h3), there is the potential issue of only lim-
ited access which can be granted to the auditor by the company (/i2, fi3) and the issue
of complexity of the topic as well as the systems (J3, Is) which may make it unattain-
able in the near future (/j4). A certification process cannot be seen as a guarantee
that no bias issues will occur (J;). A challenge to master for the auditor is to provide
an assessment that is as objective as possible, which is difficult with humans involved
(43). The approach may define how effective a certification process turns out to be.
Currently, approaches to auditing the fairness of a system tend to be too selective and
hence do not cover the entire range of problems that need to be addressed (fa). Having
an end-to-end approach covering the system holistically may be essential (h, Je, Is,
Ig, Tia, He, Ls, ia). Also, hard-coded parts of the system should be investigated in an
assessment as they sometimes can be decisive for the system’s outputs (J¢, Jo). It may
make sense to focus on an assessment regarding the most pressing fairness issues so
that a system can operate without causing anyone considerable harm (i, J2, Js, io).
An approach that is partially practiced in the industry employs a human comparison
to define the threshold for unfairness (J,). It may be criticized as it does not reflect
the target of systems’ being as fair as possible (Jz, I4). Another approach focuses more
on certifying individuals involved in the systems development and operations than on
processes established in an organization or data and reports generated by it (Js, I, Is).
Interviewee eight describes the concept of quick checks, which is discussed further in the
chapter “Criteria for Governance”, as such an employee-focused certification process
may be more effective (Ig) and may have a longer-lasting lifespan in ensuring fairness
(Ig). Another perspective envisions a modular certification process where different en-
deavors, like anonymizing data, are all certified individually (Is). The process factors
define specific nuances a certification process should consider and what is required for
implementation. The assessment process should first differentiate between assessment
for Business-to-Business (B2B) and assessment for Business-to-Consumer (B2C) use
cases (Ig). In B2B use cases, different aspects like competitive fairness are relevant,
which are irrelevant for B2C and the other way around (Ja, /7). An assessment should
be built on standards, KPIs, and best practices (1, Js, Js, io, i, fis, fia). They
help companies align their processes and offer clarity. Existing frameworks should be
utilized to build a process on (, Js, 2, 3). Currently, only internal certification
processes are established in some companies, which generate Lessons Learned (114).
The latter eventually translates over time into the development of an independent
third-party assessment facilitated by an exchange of discovered working mechanisms
between practitioners and in exchange with policymakers (f4). Currently, the develop-
ment of processes for auditing fairness is driven by assessing the practical applicability
of research (f14). When an independent third-party certification is first introduced, it
will also manifest and improve with learnings out of experimentation and practical ap-
plication (1, fi3, 14). When standards are set and a consistent process is established,
the reasoning for reaching the result of an assessment is to be addressed, as it should
be grounded in a factual foundation (3). Transfer learning needs to be considered
separately here as the provider of the unspecified pretrained baseline model and the
provider of the custom model, which is then trained for the specific task, are involved
(2, Ig, Iii). Interviewee nine argues that the certifications of both models should be
kept distinct so that the custom model is certified independently of the ideally already
certified baseline model (J9). That may entail that only already certified baseline mod-
els are safe and viable options when pursuing certification for one’s custom model (111).
Reviewing the baseline model by oneself as a provider of a custom model is barely fea-
sible (11). Some interviewees argue that with training the custom model (12, Ig) or


Fairness Certification 23

due to the use case dependence of fairness (J), there is a responsibility shift regarding
fairness from the provider of the baseline model to the provider of the custom model.
That is because the data for training on the baseline model defines the final behavior
of the model regarding fairness (1, I2, Ig).

Scope: Interviewee nine poses the critical question of where the scope of a certifica-
tion process should end (Jg). Nearly all interviewees discuss the scope of a certification
process to a certain extent (, hb, Is, la, Is, Ie, I7, Is, Is, Iii, Iie, is, La). It may make
sense to start basic first when introducing a certification process for fairness and grow
it more nuanced and complex over time (J9, 14). The generalization of the certification
process is a major concern regarding its scope (J3, Js, I7, Is, Io, lia). A certification
should be as general as possible without sacrificing the ability to properly judge a use
case (J3, 4). There is a need to specify criteria depending on which use case is present
Us, Is, Is, ii, he, ha, ha) or the geographical location the model is used (Jo). As an
example, a hiring context and a context in the medical domain may be compared (114).
In the hiring context, it makes sense to introduce criteria assessing the similarity of
embeddings between genders in the semantic vector space. In the medical domain, the
differences between genders may be valuable information to improve treatment specif-
ically for each gender. Criteria should not favor semantic similarity there, but rather
focus on the result, which is the success of the treatment for each gender. Clustering
into similar use cases may be problematic as human perception of similarity may not
align with what models represent (/3), and there needs to be more clarity for assign-
ing clusters (1,4). Another challenge when specifying criteria is determining what is
to be considered relevant (J3). The scope that should be certified may also vary in its
thoroughness. That may depend on either the risk invoked by the system regarding
unfairness (1), 2, Ig, 19, a) or on the number of users affected by it (J3). The risk of
a system is typically rooted in its application and not just in the underlying technical
system and hence may shift with the application context changing (Jg). For a risk-
based differentiation between use cases, it may make sense to hierarchically increase
the thoroughness and frequency of an assessment with the societal impact a system has
(Is, lia). For low-risk applications, interviewee 14 suggests that no external assessment
may be required, and an internal assessment should be pursued by the company and
documented ([14). That way, internal product, and process expertise can be leveraged
in an internal audit to make it more streamlined (/i4). However, an external assessment
should be regularly required for high-risk applications (1,4). It may also make sense to
require more transparency with increasing risk (f14). To assess the risk of an applica-
tion, systemic issues like gender discrimination should be considered. This may be done
by looking at the potential severity of impact in isolation and the cumulated effect of
combined negative impacts in the context of our societal system (Ji4). Ultimately, it
is a societal consideration how much emphasis is put on certification (ha). To push
forward fairness in the industry, some interviewees suggest making a certification at
least in some cases obligatory as they fear lacking utilization of certification without an
obligation (J2, Lo, 1). Particularly, large language models are mentioned to require
a certification process (J,). Others argue that certification should much rather provide
guidelines based on which the application is certified without introducing an obligation
(Js, Is). Some interviewees suggest that it may make sense to certify a more extensive
scope than NLP fairness (Jg, J9, [i4). A certification for ML or even algorithmic fair-
ness, in general, is suggested (Jg, Jo), and an extension of the certification process to
assess other RAI principles is proposed (I).


24 Freiberger et al.

Certification Market Factors Market factors describe how a certification process
would influence its major stakeholders to act economically and how the market envi-
ronment is shaped. Currently, uncertainty is in the market as there are no established
standards or best practices and regulatory uncertainty (41, 4). There is a reciprocal
influence between a certification process and potential future regulation regarding sys-
tems’ fairness. On one side, the prospect of coming regulation in the field incentivizes
companies to prepare for future compliance by already undergoing certification (h, Is,
tha). On the other hand, certification may guide regulation by introducing standards
and giving direction to industry players (4, Ig). An important factor for adopting a
certification process lies in its alignment with the company’s business goals and viabil-
ity (3, 1). The company first needs to accept the fairness criteria, which are assessed
by the certification process (Jo). It also needs to see the potential benefits of a certifi-
cation process. A fairness assessment process may make the company less vulnerable
to potential lawsuits (h, J7), signal trustworthiness and reliability to the user (Js, Jo),
and even drive innovations (J3, Ig). Fairness issues become increasingly critical (Jo),
and certification may be an important selling point for a system (41, lio, 1, fia). The
more business-critical fairness becomes, the higher the certification adoption rate (113).
A whole market may develop around such a certification of fairness, as most companies
will require external consulting to ensure compliance with established standards and
certification requirements (1).

B.2 Governance Criteria

Governance Criteria focus on governance aspects that impact a system’s fairness and
should be assessed in a fairness certification process. A certification process may be
beneficial as it can augment the company’s internal assessment procedures and provide
feedback that is, in the long term, beneficial for the company (J7). In the context
of fairness, governance should establish a moral standard and a reciprocal fairness
awareness, as users and providers should be able to judge fairness appropriately (J3,
I4, Ig). This may be established by introducing a certification process, as it is currently
barely considered in the industry (Jo). However, there may be one constraint regarding
governance if a company is developing systems for customers who will use them for
their products. The solution provider then needs to satisfy the customer’s wishes and
hence is restricted regarding governance (Js, lio). The interviews brought to attention
two major concepts that need to be considered for governance: “Model Reporting &
Transparency” and “Organizational Criteria”.

Model Reporting & Transparency Providing model reporting and transparency
focuses on reducing information asymmetries between the system’s provider and stake-
holders by providing information about the application that may be relevant to stake-
holders, particularly for identifying biases (11, Iz, Ie, Li1, fis) or avoiding the occur-
rence of fairness issues. It can be seen as an accountability mechanism for the system’s
provider (1, I7, 13). Model reporting may also provide essential information for an au-
dit regarding fairness (J4, 3). Criteria for model reporting are hierarchically mapped
in Figure |4| Transparency may be essential in maintaining a good reputation as a
company (7). It may be limited by confidentiality requirements within the company
(3). The relevance of good model reporting and transparency may be particularly
high for API services (Iz). That is because the customer using the service or build-
ing on it faces information asymmetry and only accesses the system via an interface


Fairness Certification 25

Transparency in
Operations

Disclosures like Model
Cards

Model Reporting & ‘ ,
Data Understanding
Model Explainations

Fig. 4. Criteria relevant to “Model Reporting & Transparency” hierarchically mapped

Content made Transparent

(Iz). The form in which transparency is provided may also vary. The ideal form of
transparency in operations would be given with open-sourcing key components of the
system (Js). That enables stakeholders to improve components of the system as they
can not just understand the system but also participate (Js), which may be bene-
ficial regarding fairness as affected groups actively can contribute. That way, their
perspective might be better represented in the system’s continuous improvement. A
lower degree of stakeholder access may be chosen with only information about the sys-
tem and its inferences provided as another form of transparency in operations. Then
there need to be considerations regarding the design of the interface providing such
information as it should not negatively impact the system’s usability (J/g). Users may
not all be interested in transparency about the system (Jg). Interviewee eight suggests
making such information immediately available with the click of a button (Jg). An
often-mentioned implementation of model reporting can be found in Model Cards or
closely related conceptions (2, Is, 4, Is, 2, 3, a). The paper introducing model
cards is mentioned and referred to by interviewees (li2, 3, Ia). A model card
reports on model details, intended use, factors that may influence the model regarding
fairness, metrics evaluation data, training data, quantitative analyses, ethical consid-
erations, and caveats and recommendations [77]. The version of model cards provided
by Hugging Face is also mentioned in the context of providing transparency (Jo,
Iz). Interviewee eight suggests an extended and adapted concept (Js) which inspires
an adapted version extended with some contents mentioned in this chapter and can
be found in Table 5 in the Appendix. Introducing a similar concept to model cards
eads to better-documented models and improved transparency, which are beneficial,
as previously mentioned, not just for fairness (13, [13). Model cards can help by giv-
ing direction to model users or even providers by raising awareness on fairness issues
and potential risks of an application and starting a reflection process (Jg). The con-
tent of disclosures in operations or model reporting can be clustered in biases, basic
information, data understanding, model characteristics, and evaluation. Basic informa-
tion should include information about the intended fields of application (14, li3), the
underlying fairness definition, and the frameworks the system was built on (14). The
atter may be important information for an audit, making the process reproducible
(14). Moreover, information about the model’s generalizability in the domain should be



26 Freiberger et al.

provided, giving an indication of model quality (J4). Furthermore, it should be commu-
nicated by the company based on what fairness definition it built the system and what
the latter excludes (/i12). Data understanding covers all relevant information about the
data the system utilizes and how it is processed. This is essential as NLP systems tend
to be trained on vast corpora of sometimes even confidential data, which makes it hard
to judge for users what fairness issues may potentially arise (/13). Data understand-
ing involves providing the context of data collection, as there might be incentives for
unrepresentative data collection (Ii2). Moreover, the data source ([i2), the volume of
data (J4) as well as distributions (/12) should be made transparent. It should become
clear what has been used as training data and what as evaluation data (J,). Data split-
ting and sampling should be described, and their representativeness stated (14). All
data preprocessing steps should be described (2) and a sample of the data may be
provided (/13). Transparency may also be important regarding model explanations (i,
Ih, Is, Ia, Ie, I7, Is, ii, Liz). It should first be reported which models are utilized for
the system (14, [i2). It may also make sense to go deeper and document how the model
trains itself on all layers (/g). This ties into how a model learns certain things, which
is addressed by explainability (411). Explainability should allow the user to understand
the basis of the system’s decision and contextualize it (1, Jz, I3, I7, Is). It may be
essential to make explainability information transparent (J2, I3, la, Ie, Is, 2). The
users’ domain expertise in the specific field may help uncover particular fairness issues
(Js). Explainability enables a plausibility check by a human to see whether unwanted
decision drivers may be involved that could indicate fairness issues (J2, I3, 4). In some
cases, explainability may also be a business requirement and needed anyway (I). How-
ever, there may be a tradeoff in the quality of explainability feasible for a model and
its performance. That is because large and complex models that make operationalizing
explainability difficult tend to perform better (2, Js, 4). Smaller, interpretable mod-
els may be a solution for certain tasks (J4, Ig), as discussed in the chapter “Project
Planning Criteria”. Multiple explainability tools are utilized for black-box models for
which a minimum standard that needs to be met may be specified in a certification
(J2). But it should consider the different suitability of certain tools depending on the
use case (Jz). Mentioned tools that may be utilized include attention scores over all
layers (Jz), integrated gradient score and token influence prediction (2, I4), and visu-
alization (13, Ig). The latter may be hard to quantify (J3). Non-quantifiable measures
involve a process of human sensemaking, which induces a bias (J3). Because of that,
quantifiable explainability measures should be preferred (J3). The last consideration
for reporting explainability is the comprehensiveness of the communicated information
to non-expert users (J/g, [i2). Just reporting very technical details on the model and its
inference is probably useless for most users, and the addition of comprehensive expla-
nations is required (Jg, Ii2). The steps taken for evaluation should also be reported. It
should be reported what has been tested regarding the robustness of the model against
biases (I2, I4). There may be some form of scoring on standardized test sets targeting
various biases and allowing for a benchmarking of the model, which can be reported
(2, 4, Ig). Evaluation metrics should be publicly documented and reported (14, Js).
The reporting of biases covers data bias (14), model bias (/3), and countermeasures
against biases that are implemented (J3, f3). If biases can’t be mitigated, there should
be at least a hint to users of the system priming them for the potential occurrence of
such (J, I7). Interviewee two also suggests testing on tasks not intended for the system
but may give hints regarding bias issues that may also occur for the original task and
hence should be reported (12).


Fairness Certification 27

Organizational Criteria Organizational criteria revolve around general processes
established in an organization, accountability-related aspects, and team- and qualification-
related aspects. The findings of this work include organizational criteria for the audited
and considerations for the auditing organization. The hierarchical structure of all men-

tioned concepts is visualized in Figure [5]
Employee
Qualification & Holistic Overview
Training

Considerations for Raise Awareness &
Auditing Organization Understanding of Bias
Issues

Criteria for Audited Internal Assessments
Organization

Internal Accountability

Organizational Criteria

Supervision &
Diversity of Fairness |
Development Team Responsibility

Fig. 5. “Organizational Criteria” hierarchically mapped

Considerations for the auditing organization mainly focus on who is performing the
assessment. When first establishing a certification process, it may make sense to collab-
orate with independent auditing firms with expertise (112). It may also be important to
consider the background of the certifying third party as the region it is located in (13),
the political system it is integrated into (13), its initiator (13), or its integration in the
economic system (Jz, lio) influence it. Interviewee 13 suggests an international union
like the OECD may be a good platform to build such a certification process (113). Ex-
perts should be responsible for the certification assessment process and continuously
consolidate and update best practices (J2, Is, Io, 2). Regarding the background of
those experts, interviewees suggest involving linguists (J9), domain experts (J¢), data
scientists and data engineers (Js), and ethics or sociology experts (Je, Ig).

Criteria for the Audited Organization: Organizational criteria for the audited or-
ganization include employee qualification and training, internal accountability, and
diversity of the development team. For employee qualification and training, the job fit
of the employee is an important concept (J3, Ig). In a step like annotation, there can
be the issue that due to cost reasons, typically, employees assigned to annotate are not
on long-term contracts and hence may also lack the motivation to make a conscious
effort to follow annotation guidelines (Jg). The latter, however, is crucial for fairness
as it provides the foundation to train the model (Jg). Who is assigned what task may
indicate how successfully fairness may be implemented (Jg). Hence, it would make sense
to assess who is assigned what job and whether qualifications are sufficient to avoid


28 Freiberger et al.

imposing fairness risks (J¢). The awareness for fairness needs to be raised, and an un-
derstanding of bias issues needs to be present in the organization as there tends to be
lacking expertise regarding fairness in development teams (/1, J3, [4, Ie, I7, ii). That
needs to be done for the development team of an application by sensitizing to potential
biases and training (h, Js, Ie, I7). Again, annotators are highlighted as particularly rel-
evant for such training (J3, Ig). Moreover, customer-centric teams should be trained in
detecting and identifying biases (11). That may be achieved by establishing practices
that make customer-centric teams build empathy with the affected individuals and
change their perspective to understand the affected groups’ perceptions (411). Further-
more, such practices may be essential when first introducing a certification process as
they enhance the overall problem understanding, which in turn helps the certification
process to evolve (f11). The installment and contents of such training might be subject
to an audit. Moreover, members of the development team should all have a rough holis-
tic overview (Je, Ig). A possible measure supportive of this would be offering workshops
that foster a holistic overview (Jg). It may be checked that every development team
member is aware of their fairness impact on the product (Js). Contextualizing such
quick checks for different roles that each have a different perspective on the product
may be a valuable approach to fostering fairness (Jg). Tech-focused team members may
benefit from a quick check on data understanding and mitigation layers (Je, Jg). Anno-
tators and non-tech-focused team members can be equipped with a system and process
understanding (Je, Ig). That would shift the focus of the certification audit further onto
the employees as their workflow is certified, similar to a checklist (Jg). For internal ac-
countability, roles need to be assigned clear responsibilities, internal assessment or audit
procedures should be established, and supervision and responsibility for fairness must
be addressed. Starting with roles, establishing a fairness officer or team dedicated to
fairness makes sense (1, 2, Is, 17, Io, Lio, a). That person or team should be respon-
sible for the fairness definition in the specific use case (41, J9), conducting an internal
fairness assessment (1, ls, Ig) and providing advice on fairness to the development
team and supporting it, for instance in its decision making (hh, I7, Jo). The main issue
that arises is that such an individual or team requires financial resources (12, fii, fi).
There may also be an external assessor for fairness who reports to the company (11).
The development team functions as a problem solver for fairness issues reported by one
of the above assessors (11, J3, 1) without being the responsible instance for detecting
fairness issues (1, I¢, 1). It may also make sense for certain use cases to implement
different levels of clearance, for instance, when confronted with sensitive contents in a
feedback loop to ensure appropriate and fairness-compliant handling (J7). For the spe-
cific project, interviewees view some form of supervision or fairness responsibility as an
essential concept. Either the data curator (J3), a separately consulting organizational
instance (J7), or the project or department manager (Jé, 11) should be responsible for
fairness in the specific project. The initiative for fairness, however, needs to come from
the company’s particular commitments regarding fairness (l11). Some processes, like
annotation, also need careful supervision and verification by someone accountable (I,
Ig). An internal audit or assessment can also be a vital tool for a company to ensure
accountability regarding fairness (J7, 14). An assessment may target the impact an
application has on its stakeholders as a foundation for fairness considerations (J7). It
may be updated and tracked continuously, and with changing stakeholders, it needs to
be reiterated (7). Red teaming is suggested as another form of assessment (J7). It pro-
vides a holistic system testing for potential issues in a set time interval. The so-called
“red team” is to find fairness vulnerabilities in the system. That could be seen as an
exemplary holistic and focused company internal fairness assessment. Such assessments


Fairness Certification 29

are typically motivated by ensuring to be regulatory compliant with future legislation
besides ensuring fairness (J;4). These assessments or internal audits should take place
continuously (/7) and should not be conducted by the development team but rather by
a separate organizational instance (/j4). An aspect that may be assessed which may
vary in its importance for fairness between use cases, is the amount of human oversight
in operations (J4). One more dynamic should be considered regarding accountability
which addresses binding claims made by a supplier about his product that shift re-
sponsibility back to the supplier (J). An assessment should also check the diversity
of the development team (2, f11). Due to different perceptions and realities of life, a
diverse development team fosters problem-understanding and thus helps identify and
solve fairness issues (2). The major challenge lies in defining what a diverse team is
(Jz). It is debatable which dimensions like gender, race, or sexual orientation diversity
are relevant to consider when building a diverse team (J2). It should ideally represent
minorities (J2). Currently, development teams lack the diversity of their users in di-
mensions like age and gender (J2, fi). To be, to a certain degree, representative of
the user may be a requirement to check (J2, 1). Diversity may also be supported via
(sometimes lacking) interdisciplinary collaboration within the development team (Ie,
Iho). Interviewees particularly highlight diversity in the development team for anno-
tators as they directly impact and potentially bias data (4, ho, 3). That, however,
comes with the limitation that someone needs to be qualified to label, which may be
more or less restrictive, dependent on the use case (Ji).

B.3 Project Planning Criteria

Project planning criteria are set up to assess the considerations regarding fairness in the
project’s planning phase, where business understanding and process planning are done,
and requirements for the project are set. Interviewees are divided on whether project
planning criteria are useful to implement and what should be assessed with what kind
of purpose in mind. In such early stages of the lifecycle, one interviewee argues it
might be challenging to set criteria for certifying fairness (113). Another interviewee
argues that a consequence of criteria for business understanding may be neglecting
some products’ specialized nature, leaving little room for steering decisions (Je).

Define & Assess the Planned Application Interviewee 14 argues that in a
certification process, the purpose of use and not the technology itself should be the focus
of an assessment for certification (14). Hence, the problem definition that the system
builds on should be investigated (Js, h2, 3). That motivates an assessment that can
already occur before the actual implementation starts, even though it may be associated
with many manual processes (/;1). According to many interviewees, solid definitions of
the use case and the application planned are essential factors that should be checked
in a fairness assessment (14, Ig, I9, Lo, Lis, lia). Moreover, users or stakeholders, in
general, need to be identified (Jg). In pursuit of being able to assess the impact on
stakeholders and the planned application, three major concepts need to be considered:
First, a good user and stakeholder understanding needs to be established. Second, the
fairness properties of the use case need to be identified and assessed. Third, the planned
solution must be checked to determine whether its fit for the use case favors fairness.
The hierarchical mapping of all relevant concepts can be found in Figure [6]

User & Stakeholder Understanding: User and stakeholder understanding capture
assessing processes to gain knowledge about general attributes and characteristics of


30 Freiberger et al.

User & Stakeholder
Understanding

Define Fairness for Use Case

Model Size / Data Inputs

Use Case: Fairness Properties

Define & Assess Application

Planned Solution: Use Case ML-based Solution

Questionable

No Proxy as Objective

Fig. 6. Criteria relevant to “Define & Assess Application” hierarchically mapped

stakeholders like demographics (Je), a target group definition (Ig), the understanding of
possibly multiple levels of distinct users involved (/g), and the context users find them-
selves in (111). The assessment of the context of the users is highlighted by interviewee
eleven as some stakeholders may be dependent on the system without having alter-
natives (111), making a fairness issue more impactful. Interviewee eight highlights the
importance of an assessment for direct and indirect users as well as other stakeholders
separately, instead of a singular focus on the end user, to avoid vague results (Js). This
conception is supported by the fact that some systems operate in multi-sided markets,
and fairness impacts multiple stakeholders (J7). The composition of the target group
should be explained for an assessment to check (J7, Ig). Some use cases may natu-
rally exclude certain groups without being unfair (J7). Moreover, the audited company
should have mechanisms to assess the likelihood of marginalized groups within the in-
dividuals affected by the application (Ig). It should be assessed whether the planned
system considers the particular interests of all its user groups (J11). However, it needs
to be checked whether such processes may lead the company to develop misconceptions
of their stakeholders as there is a risk of misinterpretations about their true nature (Ig).

Use Case: Fairness Properties: Assessing the fairness properties of the use case
involves checking the definition of fairness for the use case and the implications of
the latter on the defined stakeholders (Iz, fia). A definition can be seen as essential
as fairness aspects can differ drastically between use cases and conceptualizations of
fairness (as discussed in the previous chapter). It sets the focus of what needs to be
considered for the technical system (4). As different stakeholders are involved, there
may be multiple fairness definitions to consider (/i2). What can also be assessed in an
audit is whether the fairness approach that the audited company chooses is suitable
for the use case (14). The underlying business case should also be assessed regarding
fairness (Ig, io, 4). An unfair underlying business case invalidates all attempts to
create a fair system (Ig, 19). Interviewee 10 suggests utilizing a question catalog with
yes/no questions on the use case. The business responsible for the project may fill it
out and provide it for an audit to identify the fairness of a use case (fio). Another
property of a use case that needs to be considered is its vulnerability to fairness risks
which may be introduced via sensitive criteria (I7). Name (J5) and region (J2, Is) are
just two examples of such attributes. Credit scoring is mentioned as an exemplary


Fairness Certification 31

application strongly affected by fairness risks (/i3). An assessment should not just
check whether the fairness risks of a use case are being identified but also if there are
mitigation strategies for those risks in place (J7). The last property to consider is how
much fairness is a viable business factor for the use case (113). It may indicate to the
auditor how much the company is by itself incentivized to take action.

Planned Solution: Use Case Fit: The planned solution is investigated for its fit to
the use case. That involves the ML model itself but also its application. As a first
step, it should be assessed if there is a viable alternative to the ML model in the form
of a deterministic solution or a rule-based set that should be used instead (Js, ha).
Such solutions tend to be less affected by biases (14, J5). It can also be the case that a
manual process should be preferred based, for instance, on ethical considerations (h2).
If an ML-based solution is chosen, model size and the amount of data flowing into a
model should also be assessed (J5). The bigger a model gets, the larger is the potential
and complexity regarding biases (J3). Smaller models usually also offer the advantage of
better explainability (J), which is a topic discussed with the concept “Model Reporting
and Transparency” in the chapter “Criteria for Governance”. However, there is typically
a trade-off in other KPIs like performance (1, Is). Moreover, the chosen objective
should be assessed for certification. It should not represent a loosely connected and
just correlated proxy for the business problem at hand but rather represent it most
accurately (/j2). Often, correlated information on ethnicity is used for predicting an
objectively unrelated variable leading to harmful results, for instance, for the black
community (liz). To check for that issue, the company should state its business goal so
the certifying institution can assess the chosen objective’s suitability (2). A mismatch
should be easily detectable by an auditor (12).

Fairness Targets in Requirements Another finding is that fairness should be
embedded in the requirements set for the NLP project (Jz, l4, Io). Fairness-related
criteria should be kept in an abstract form to be applicable to all projects (J2). When
defining tasks, these requirements should help to give direction in the form of a checklist
of where to focus and where problems may occur, hence introducing a reflection process
(2). A requirements catalog should direct the development and improvement of the
system towards systematically including fairness considerations by enforcing specific
steps for fairness to be considered (J9). It embodies the company’s vision regarding
fairness and its commitment to it (12, J9). Such a requirements catalog or checklist
may be assessed in an audit (Jo).

B.4 Data-related Criteria

Most interviewees agree on the relevance of data-related criteria for certification of
fairness (11, La, Is, Ic, Is, Io, Lio, Lia, ia). One general aspect that should be considered
regarding data is, that all processes leading to the data that the model eventually uses,
should be reproducible for an audit (4). Apart from the data assessment itself, it is also
discussed how data should be managed and how it should be handled and transformed
in processes over the lifecycle like annotation, preprocessing, evaluation, and continuous
improvement.

Data Assessment Data assessment focuses on judging the suitability of data for
training and retraining models that operate fairly. The data set on which it is trained


32 Freiberger et al.

Distribution Checks

_ of Data Augmentation

of Collection / External
Sourcing

Representativeness

Harmful & Incorrect Data

Data Bias Checks

Data Quality Assessment

Explorative Data
Understanding

Data Assessment

Uncover Unfairly
Annotated Data

Sourced Data Fit
Selection of Sources

Internal Processes
Assessment

Data Sourcing
Assessment

Collection Assessment

Fig. 7. Criteria relevant to “Data Assessment” hierarchically mapped

can be seen as the major fairness issue (1, hb, Is, Is, 17, lo), which gives reason for
a thorough investigation. It needs to be assessed regarding its fairness, specifically
regarding the use case it is intended to be used for (1,). The data’s specific impact
on the overall system needs to be considered (Js, J7). Subject matter experts may
be required to assess data regarding fairness in an audit (12). However, it needs to
be considered that data may be scarce for specific use cases, leaving little room for
improvements for the audited company (J6). As another limitation, interviewee five
mentions that some information may also not be available for assessment in certain
cases ([5). Data assessment entails an assessment of the data quality by the auditor
but also an assessment of company processes that aim at avoiding data affecting fairness
negatively. All relevant concepts for data assessment are portrayed in Figure [7]

Data Quality Assessment: Data quality is a concept that covers to what extent
data captures reality completely and in an undistorted way while keeping suitability
for the use case in mind [79]. Other facets of data quality are not discussed here. Rep-
resentativeness (, lb, I, Ie, I7, Is, Io, Lio, La, Le, 3) and identifying harmful and
incorrect data (2, I4, Is, Io, io, 3, fa) are mentioned for the data quality assess-
ment. Representativeness captures data, including all the variables needed, to represent
the diversity of users of the system (/i2). Interviewee twelve mentions the prominence
of lacking representativeness of certain ethnic groups, causing fairness issues for ML
models in the past (liz). To assess the representativeness of data, it makes sense to
introduce distribution checks (h, la, Is, Io, 13). Distributions can be aimed at being
representative of overall society (lio, fi2) or the system’s users (J2, Ig, I7, 1). Lacking
representation in distributions can either happen by the exclusion of certain groups (Je,
I7) or by skewed representation (1, I4, Is, Ii3) and should be explained (Jeg). Repre-
sentativeness is assessed for different attributes or target variables represented in data
distributions (4, fi3). For assessment, statistic screening (fj2) and frequency-based
screening (J4, lio) may be utilized. The latter could also be described as a NER-based
representations screening (/io). Words indicative of a class for a sensitive attribute
and related words are automatically tracked via their semantic vector representation,
which mirrors what NER does (/io). Their frequencies are compared to the repre-
sentative real-world ratio. A comparison to a reference dataset regarding frequencies


Fairness Certification 33

and semantic representation may also be utilized (lio). Checking label distributions in
the annotation process may uncover some underlying data issues (Jg). One particular
factor that may impact representativeness after data acquisition is preprocessing. Rep-
resentativeness may be negatively affected by data cleaning in the form of the removal
of outliers (io, 2) or by sampling (4). That makes assessing distributions before
and after preprocessing necessary (/12). The sourcing and collection of data need some
assessment by the certifying institution regarding representation issues. It should be
checked whether the data distribution represents the use case (J2, Ig, Ig). As data is
sometimes procured from abroad and collected there in an entirely different societal
background, there may be issues with its representativeness for the use case (J¢, Is).
It could be checked in an assessment whether the application context of data meets
its distributions. Similar issues occur when poorly fitting open datasets are used for
first model iterations (J2). Induced selection biases can be found by inspecting the
underlying data collection process (I6, liz). The context of time (iz), process char-
acteristics, education of subjects, and location of the collection could be considered
and questioned regarding possible adverse downstream effects (Je). Data augmentation
also needs to be mentioned, considering the representativeness of data (/i1, f3). It can
be assessed whether the baseline set for augmentation captures diverse enough data
to represent the system’s users (/11). Because then there likely will not be a negative
impact on representativeness after augmentation (11). A company could even utilize
data augmentation to improve the representativeness of data (13). That may be done
by augmenting larger quantities for the underrepresented group. However, this comes
with the limitation of a loss in label accuracy (113). The approach to data augmenta-
tion and its downstream impacts may be relevant to check in an audit. Online learning
utilization may enable a system to adjust automatically toward better representation
(Ig). It captures systems that automatically go through continuous improvement cycles
by automated capturing of data for retraining (Jo). When representation issues can be
identified, the system could be set up to counter-steer automatically (Ig). Harmful or
incorrect data negatively impacts data quality, and systems should be checked for it
(2, 4, ho, Lz, ha). Interviewee 13 mentions harmful contents in baseline data as an
issue that should be checked, particularly for generative models (13). Misinformation
in data is also an issue that should be investigated (Js, 13). However, in some cases,
there is a grey area where it is difficult to judge to which extent data accurately rep-
resents the true nature of facts (J/g). The last factor that should be assessed regarding
harmful data is word embeddings which become relevant in transfer learning (J2, l1,
Ig, Ia). The semantic representation of words in vector space needs to be checked for
unwanted associations like gender with occupation (lo, ly, 4). Therefore, distances
between embeddings in vector space should be checked before training a specific model
(I4, ia). A reference data set may again be established to set an expectation for vec-
tor space representation (19). Embeddings do have a downstream impact. Companies
can handle the impacts of vector space representation by utilizing readjustable models
(4) or in their decision-making by compensating for their impact (Ji4). Such coping
mechanisms may also be an aspect to assess for certification.

Internal Processes Assessment: Apart from investigating the data itself, a com-
pany’s processes for sourcing, collecting, and checking it for biases should be assessed.
Such mechanisms may be essential to consider as they can indicate how much stability
of fairness in data over time can be expected. The data sourcing assessment investigates
how the company procures its data and what considerations are made to ensure sourc-
ing quality data for building a fair system (/i3). The selection process for sources is a
central consideration here (, bb, 1, Is, he, 3). There may, for instance, be quality


34 Freiberger et al.

indicators like reviews or reputation of a data set of interest (/,), ratings for articles
or scientific journals utilized (f13), or, in the case of social media data, for instance,
an up-vote to down-vote ratio (I1). Social media, however, needs careful consideration
regarding the intended use case as it tends to be very opinionated and often biased
(14). Sources renowned for extreme perspectives, which tend to be biased, should be
avoided (J4, Ig). When sourcing externally, it should be checked for transparency about
the data collection process (1, 2). Sources used should be documented (J4), and the
sourced data should be reviewed regarding use case fit (J2, 4, Is, Is, 3). The data
collection assessment checks for suitable processes to collect data (2, fia). The col-
lection of data by the company itself, in contrast to data sourcing, gives the company
complete control over its data acquisition (12). Guidelines for the use case should be
set for the collection process to avoid bias and may be reviewed for certification (J3).
Data is sometimes synthetic and, in the case of chatbots, sometimes a result of brain-
storming different ways of expression for responses in the development team (I2). That
is why special guidelines may be required to avoid inducing bias (Jz). Assessing forms
of bias checks a company performs may help assess processes enforcing fairness (/1, Ie)
at the cost of being resource intensive (Jz). Human involvement should be checked, as
it makes sense to pursue a semi-automated approach in this case (2). That is because
a human has a more refined sense of detecting relevant biases or fairness issues than a
machine (Jz). Hence, it should be checked whether a human assesses a data sample for
potential fairness issues (J2, io). The company should establish processes for an explo-
rative data understanding of the dataset, helping to assess its quality and contents (h,
Ia, Is, lio, L11). The contents, like ways of expression (Js, Ig), contained entities (Js),
and personalized data (Is), already indicate potential biases in data towards which a
system needs to be robust. Even just the language that is used may indicate whether
a system is susceptible to gender bias (/i3). Moreover, data contents should also be
internally checked for diversity and inclusiveness (J, I7, Io, 1), potential stereotypes
represented, and harmful content (4, Jg). A company should investigate all of this,
considering metadata describing data sources to get the context that may be relevant
for identifying issues (/i1). Data bias checks should also help the company uncover un-
fairly annotated data (Je, Iz). That can be done by comparing incorrectly labeled data
between groups upon review (Ji2). If such unfairness is discovered, a company should
investigate why it happens (Je). In some cases, a company may uncover bias in data,
but there may be no suitable mitigation strategies. That should lead to documentation
of the issue for mitigation in downstream processes (Js).

Annotation The annotation process of data can be considered an important factor
for the fairness of a system based on supervised learning (14, Jg). When investigating
the annotation process of a company in an audit, it may be challenging to assess the
quality of annotations in some cases as there may not be a ground truth that can be
utilized for evaluation (J,2). Moreover, there is a high degree of subjectivity to annota-
tion (15). If there is an objective ground truth, however, one may assume that a correct
label is a fair label and evaluate fairness by the correctness of labels (/i2). To certify
annotation, one may consider processes established in a company for annotating data.
Three major concepts are identified for company processes. First, fairness-relevant as-
pects in the data should be annotated in the data (h, Js, fii). That is a crucial step
for being able to evaluate the fairness of the system for an audit or continuous im-
provement (i, 2). Another way this benefits the development of fair systems is that
it facilitates purposeful sampling allowing for fair representation of certain groups in


Fairness Certification 35

data (I2). The annotation process of fairness-relevant aspects may either be manually
labeled or automatically tagged (J2). However, one drawback to such annotations is
also mentioned with its possible negative performance impact (J1). Second, there is a
tradeoff between labor cost and time and the quality of annotations to handle (2, 11).
To a certain extent, saving cost and time will sacrifice quality and hence fairness of an-
notations. Interviewee four proposes an approach that may be requested of companies
as an option to pursue a more reflected annotation strategy regarding label quality.
This tradeoff can be managed appropriately by including filters defining what needs to
be annotated manually (14). Third, there should be a set of measures implemented by
the company that causes a minimization of fairness risks in the annotation process (Ja,
Is, Iz, Iho, iz, Iz). Like every human, human annotators have biases that motivate
annotation processes so that their biases are at least less reflected in their annotations
of data (J7, 1). While annotating data, one may require the annotator to see similar
annotated examples to the respective one that should be annotated (J7), facilitating a
more objective decision. One may also require the process to be designed so that sensi-
tive information stays hidden and cannot bias the annotators’ decisions (J7). An aspect
frequently mentioned is involving multiple annotators in labeling a data point (4, Js,
Tz, To, liz, iz). Interviewee four suggests that it should be case-dependent to either re-
quire multiple annotators or not (J4). Particularly for topics that are very opinionated
and perceived differently by every individual, like politics, multiple annotators should
be involved to reduce biases (J1). The aggregation of multiple annotators’ opinions on
the correct label can either be achieved by a majority vote between annotators (Is, li2)
or by an Inter-Annotator Agreement (J4, I7, io, 3). An Inter-Annotator Agreement
score quantifies how similarly multiple annotators annotate the same pieces of text (4,
Iho). Particularly referring to topics that tend to be opinionated, a very high score rep-
resents a high agreement which may indicate that annotators are in a bubble (Jo). It
should raise organizational questions which are discussed in “Criteria for Governance” .
However, in the case of developing generative models, finding a suitable metric as a
baseline for such an Inter-Annotator Agreement may be difficult (J5). Another integral
approach the audited company should pursue is utilizing annotation guidelines (1, Ie,
I7). These guidelines should be documented so they can be audited and as objective
as possible (I,). A beneficial consequence of introducing annotation guidelines besides
fairness is more consistency and hence higher quality of annotation (Js).

Preprocessing Preprocessing includes all steps for transforming, cleaning, selecting,
and expanding data after annotation for training and evaluation. When auditing this
step in the lifecycle, it must be considered that its scope may vary strongly between
use cases (3, [11). That makes it problematic to require certain steps for all use cases.
Instead, the steps taken should be documented and assessed regarding fairness impact
(14). One concept mentioned several times is that the steps in preprocessing should
be checked to determine whether they induce bias or counteract measures to improve
fairness from previous lifecycle steps (io, fi, 2). The same criteria established to
be met by the data before preprocessing should also be met after preprocessing (112).
Robustness-ensuring mappings are a way to transform data by mapping it to a stan-
dard form (J2). They may be helpful to pursue in some use cases as they can remove
usability hurdles (/2). They can be, for instance, utilized to enable people with migra-
tion backgrounds to achieve a similar performance compared to native speakers of a
language. For instance, articles can be mapped to a generic domain, making typical
mistakes irrelevant (J2). Anonymization may be a very related approach because it


36 Freiberger et al.

aims to drop the sensitive attribute in the data (J3, Js, I7, iia). The issue with such an
anonymization approach is that dropping the sensitive attribute makes it challenging
to assess the fairness of a model at a later stage, as the information from the sensitive
attribute is lost (J7). Because of that, anonymization counteracts the annotation of
fairness-relevant attributes as an earlier step in the lifecycle. Another problem with
anonymization is that it does not necessarily solve the problem of biased data, as there
usually are indirect encodings through correlations that can materialize as second-order
effects (Jg). In some use cases, anonymization may not be suited as it may undermine
the solution’s functionality because of a negative performance impact (Js, lio). The
same applies to differential privacy, which is an approach for comparing the effect of
including the sensitive attribute and dropping it (Js, I7). Nevertheless, in some cases,
it may be viable and should be considered by the company (Js, J7). An important
consideration for preprocessing that may be assessed is the filtering and selection of
data (13, l4, 3). Filtering and selection aim to improve the data quality for use via
filtering mechanisms and ensure appropriate data splitting. Filtering may target re-
moving harmful content like obscene language (113) or low-quality or biased data (3,
I,). However, it may be challenging for some topics to identify biased data clearly (3).
Moreover, defining fair filters objectively and calibrating how restrictive they should
be poses a challenge (J,). Particularly the use of social media data raises the question
of controllability for such a process (113). When selecting data, unbiased and balanced
data sampling is essential (, La, I7, Is, iio, 2, 13). Depending on how data is selected,
a biased selection could be made, which is to be avoided (h, I7, ho, 2). When split-
ting the data into training data and data for evaluation, it should be assessed whether
the distributions of training, evaluation, and overall data roughly match (lio, 3). As
mentioned, that helps avoid bias and ensures the integrity of evaluation results (/io). It
was also mentioned that the data splitting might consider underrepresented minorities
by representing them in the data samples equally compared to other instances that are
better represented (io).

Data for Evaluation In evaluation, some aspects and concepts regarding data
must be considered. Functional testing of a system is input-based testing assessing the
outputs and hence views the system from an external perspective suited for an audit
(io). The procedure is discussed in the chapter “Modeling & Evaluation Criteria”.
Test data is targeted at unveiling biases present in the model (J). It should represent
the diversity of actual usage of the system and the challenges that come with it (Jo,
Ii). Moreover, it should suit the assessment regarding the specific fairness definition
imposed by the audit (Ig). A relevant consideration is the sourcing of data sets utilized
for evaluation. It could be acquired by accumulating in-use data of the system over time
(2). Another way of acquiring test data is going by previous precedents of fairness
issues and utilizing relevant data regarding these (/i3). The last approach to getting
data is artificially generating it, making a targeted evaluation of fairness easier as it
can be designed to reveal biases (J2, Ig). Targeted segmentation for sampling data of
inputs can help assess specific biases (/7). However, these data sets may also be biased
and only provide partial insight into fairness issues (Jg). Another vital consideration
for functional testing is utilizing fairness invariance tests (Jz, I3). Such tests utilize
targeted replacements in inputs to assess the model’s invariance to variation in sensitive
attributes’ values (/2). These replacements, as a form of augmentation, are usually
generated by multiple language models (Jz) and involve approaches like forward and
backward translation (J2, [3). Drawbacks to this approach are mentioned with the


Fairness Certification 37

complexity of such augmentations in NLP (Js) and potential model biases present in
the models utilized for generating replacements (J3). Nonetheless, interviewee two sees
robustness invariance tests as an integral part of certifying the fairness of a system (J2).
It is suggested to propose a range of measures and a minimum standard for creating
such augmentations for fairness invariance tests (12).

Data for Continuous Improvement Data-related criteria in continuous im-
provement target data considerations for maintaining a fair system over the time of
its operation. That involves considering monitoring practices as well as feedback loop
practices. Continuous improvement is affected regarding data by two challenges: main-
taining data diversity (J9) and extending continuously relevant testing data (Jo). As
criteria for monitoring, test sets (J2, I3, Ie, Is, Ig), drift monitoring (Jo, Is, la, Is, Ie, I7,
I13), as well as a request assessment for underrepresented groups (Jz) are mentioned as
practices to be implemented by the audited company. Test sets in continuous improve-
ment are the same data-wise compared to test sets in evaluation, but they are used
continuously to assess whether fairness can be maintained or whether certain issues
have occurred (12, Is, I6, Is, Ig). Drift monitoring is described either as data-centric
(2, Ia, Is, I7, Iz) or as model-centric (2, l4, Is) in its approach. It may reveal the
necessity to retrain a system to stay fair and performant over time (Jo, Js, l4, Is, Ic,
I,3). A model-centric approach considers model behavior like changing performance or
confidence in predictions (J2, Is) and explainability (12, I4) of the latter, in the form
of explanation scores or some other form of quality assessment for the explanation,
in detecting drifts. In a data-centric approach, the variation of incoming data for the
model to handle compared to the original training data is assessed (Jo, la, Is, I7, hs).
Continuously appearing outliers may be an indication (2). It may be helpful to trace
back to the kind of requests causing the drift to understand the underlying issue (2).
The reaction strategies to an occurring drift may be interesting for an audit ([3). When
users change, assessing them for semantic overrepresentation, which may cause an un-
favorable data drift, may also be relevant (Jz). The frequency at which such drifts may
occur is dependent on the dynamics of the environment in which the application is
operational (f2, J5). That should impact the assessment of the calibration of such a
drift monitoring system. An assessment should also consider that drift detection may
lack a measurable foundation for some use cases (J3). Hence, it should only be required
for changing environments and if it is measurable. The consequences of drift monitor-
ing may materialize in improved fairness and performance, making it compatible with
business goals (2, Iz). An assessment of requests for underrepresented groups focuses
on better understanding marginalized groups’ requests (J2). That helps uncover po-
tential issues to counteract the negative impact of underrepresentation in data they
face (I2). That may be performed by checking semantic differences in the expression
of marginalized groups compared to better-represented groups (/2). Substantial devi-
ations may indicate potential issues in system performance later (Iz). Feedback loop
practices regarding data need to consider how in-use data and user-made corrections
are handled. One condition that needs to be met by both is user privacy (I7, I, lo).
That may target a confidential treatment of user feedback (J7) as well as removing
sensitive attributes from that feedback (Ji9) and ensuring data protection and data
security when saving data (Jo). In-use data is gathered by regular operation of the sys-
tem without any user interaction targeted directly at generating feedback and should
also be subject to assessment (J5, J9). One form of in-use data that can be utilized is
prediction queries dependent on confidence in the prediction, either taken for retrain-


38 Freiberger et al.

ing future models in case of low confidence or for usage as test data otherwise (Jo).
That works in favor of underrepresented groups as their queries are more likely to be
predicted with low confidence and hence taken for model improvement. Metadata may
be essential to understand fairness-relevant aspects in such a case (J¢, Ig). However, if
metadata is unavailable, the context of the query stays hidden, and it may be challeng-
ing to target model improvement toward fairness (J>). Another form of in-use data can
be gathered by indirectly measuring user discontent by analyzing behavior when the
user interacts with the application (/5). That may indicate a system’s impact on its
users, and after passing a certain threshold, it can be estimated to be unfair, requiring
intervention (J;). The indirect form of measurement offers the advantage of gather-
ing data without influencing or biasing the user in their feedback statement, which
may happen when directly asking for it (J5). A dedicated team or a quality assessor
should check usage data regularly for such aspects (Js). If users can make immediate
corrections or give feedback which may be taken to improve the system, particularly
the quality of feedback data needs attention as feedback may be biased or incorrect
(J). If the user is facilitated to correct an annotation, clear guidelines should be estab-
lished by the company and communicated to the user (14). Hence, data is consistent
with the originally annotated data, and incorrect annotations may be prevented. The
latter should also be enforced by a controlling instance that has to review and accept
user annotations (J). Moreover, there should be some form of filter rejecting harmful,
like for instance stereotyped, feedback data generated by the user from being utilized
for model retraining (Jio). There are industry examples where such data has caused a
system to become heavily biased (Jo, Lio).

Data Storage Regarding data storage which covers how data is stored in develop-
ment and operations and cached in operations, two concepts are introduced. First, the
storage of sensitive attributes should be avoided in a raw form (io). That has gained
relevance due to the trend towards the ELT (Extract Load Transform) paradigm, as
there is an interest in utilizing the raw and not the transformed data (lio). Avoid-
ing saving sensitive attributes raw can also be partially motivated by compliance with
GDPR, as personally identifiable information should not be saved (Jio). This focus on
privacy highlights again that fairness is interrelated with other RAI topics. Second,
interviewee one mentions caching of previous predictions of the system as a potential
issue (I). In case of unfair previous predictions, this necessitates clearing the cash to
restore fairness of the system (1;)).

B.5 Modeling & Evaluation Criteria

In this chapter, fairness criteria for building NLP models and evaluating them and the
system they are embedded in are explained.

Modeling Modeling, particularly model selection, is perceived as less critical for
certifying fairness compared to criteria targeting other lifecycle steps (1, Is, lio). A
potential issue mentioned regarding the utilization of pretrained models is a lacking
holistic understanding of it (J3), making it hard to assess and enforce fairness for train-
ing the custom model. That leads interviewee three to suggest only permitting the
usage of, to some degree, trustworthy and documented baseline models for certification
(J3). That could solve the existing business process and documentation conflict in case


Fairness Certification 39

a poorly documented model outperforms a well-documented one (13). This would also
incentivize providers of performant models to document them appropriately. When de-
veloping and training models, fairness can be rooted in the model architecture (12, li,
Is, Is, Iz, Iho, fa). For doing so, interviews reveal four potential strategies. The first
introduces a fairness classifier built onto the model used (J5). For implementing some-
thing like this, however, a labeling of fairness for all annotations used in the original
model would be required, which would be very resource-consuming and impractical
(Js). The second approach embeds fairness into the objectives optimized by the model
(14, Is, I7). That may be done by performing a hyper-parameter optimization of subjec-
tive fairness, even though it would be time and resource-consuming (J5). Another way
would be to calculate the loss function, which is to be optimized only on non-sensitive
attributes or to penalize using the latter (io). Either way, the question needs to be
asked whether the intended model is suitable for such approaches in practice (J7). The
third approach aims at explicitly calibrating a model to avoid certain biases or, in a
way, hard-code fairness into the system (Ja, Is, J, Ii4). That may go beyond the NLP
systems prediction and involve rules for decision-making based on the prediction (Je,
Tha). As a fourth approach, interviewee two suggests introducing constraints for model
controllability (J2). Language generation via a prompt-based model is mentioned as
an example where there should be the possibility to avoid biases by setting certain
parameters influencing constraints for the model (2). Otherwise, the model would
mirror potential biases in baseline data (Jz). All of the mentioned approaches should
be assessed regarding their impact on the performance and fairness of the system (Js).

Evaluation Evaluation is essential for assessing a model’s fairness (J3, I7, Io). It
involves testing the model or the system it is embedded in regarding various targets
or metrics. Reproducibility must be given for company internal evaluation processes
so they can be assessed in an audit (J/g). A challenging factor may be the definition of
fairness which is the foundation for evaluating fairness (Ig, 2). As the definition of
fairness differs between the auditor and all affected stakeholders (/i2), a hierarchical
approach to evaluation focused on the system’s stakeholders may make sense as a base-
line for evaluation (Jg). Another challenging factor is to be found in the context of data
as a prediction foundation. Sometimes truthfulness or accuracy may be questionable
but hard to verify, posing a major issue for evaluation (Jg). In open-domain systems like
question-answering systems or recommender systems delivering multiple suggestions,
the complexity of human language becomes a challenge for fairness evaluation (Jn, Ig).
The auditor may assess all criteria in this chapter directly as system tests. However,
they should also be embedded in the company’s internal evaluation processes, which
can be audited. Interviewees name two major concepts that drive evaluation. Func-
tional testing criteria (, lb, Is, la, Is, Ie, I7, Is, Is, Ta, Lie, iz, ia) and metrics (Js,
Ia, Iz, Is, Is, io, Lia) are defining factors identified for assessing fairness in evaluation.

Functional Testing Criteria: Functional testing criteria take an outside perspective
on the system’s outputs given some specific inputs. An outside perspective on the appli-
cation is taken, and no deep insights into the model architecture are utilized here (Ig).
The outputs for a given set of inputs indicate model behavior and resulting fairness
issues (1, Is, Ie, I7, Io, ii, Ie, fia). Functional testing of the model can be considered
an essential part of a certification process for fairness as it captures what the model
predicts and what impacts its users (J). In comparison, the already mentioned data is
the model’s baseline, but with nonlinear relationships of a model, even seemingly fair
data can result in biased models (Ig, J9). Hence, functional testing criteria capture the


40 Freiberger et al.

actual fairness of a system better. For such an approach to be meaningful, a relevant
scope of inputs is required regarding quantity (J2) and diversity of input data, repre-
senting a wide variety of user groups (Jo, 1). That may be facilitated via a mainly
automated approach to testing (J2, I3, la, 1). Adversarial testing is mentioned for
prompt-based generative language models (1, J, J7). In that case, it involves utilizing
specific queries that suggest or invoke stereotypes is a practical approach for fairness
testing (h, l4, I7). If the assumption of the stereotype is met by the most probable
words to be inferred by the generative model, a fairness issue is detected (1, [7). These
associations made by the model can be assessed in an audit (14). Interviewee seven also
points out the necessity of finding a metric for evaluating the fairness of a model going
off a defined set of prompts (J7). Another approach is utilizing specific test data sets
(2, Is, Is, Is, Is, Li, 3). The data utilized here has been discussed previously in
the chapter “Data-related Criteria”. The mentioned data is used as model input with
a specific expectation for the predictions’ quality (J2). It may make sense to analyze
the distribution of predictions to assess their quality, which may reveal systemic issues
(1, tii, fiz). Interviewee one highlights that biased distributions in data should be
accepted as a reality (J). However, a change in decision-relevant variables like qualifi-
cations in a recruitment context should also change the distribution of predictions for
a specific group (J;). Hence, relevant variables to the topic should be checked to be the
decision drivers. The latter may be achieved utilizing explainability tools discussed in
“Model reporting & Transparency” in the “Criteria for Governance” chapter. Decision-
irrelevant variables should, however, not influence model inference (J2). This can be
checked by introducing robustness invariance testing (1, l2, I4), which should be an
essential part of a fairness audit (Jz). It aims to assess how capable a system is of gen-
eralizing regarding a sensitive attribute. The latter may be, for instance, represented
in expression difficulties by non-native speakers of a language (/2). Protected groups
may be introduced, representing sensitive attributes that could become an issue in the
use case (J2). By augmenting initial test data via replacements for protected groups or
the addition of nonsense, the robustness of the model can be checked in the form of
adversarial testing (J2, J4). For most use cases, a larger invariance between a protected
group and the rest of the users results in improved fairness (J2, 1). However, there are
exceptions, like the medical domain, where separation is essential and should be the
focus (13, Ii3, fia). Assessment should be performed automatically with the additional
requirement of involving a human assessment of a subset of data as a plausibility check
for the automated assessment (J2). This invariance testing is enabled by annotating
fairness-relevant aspects like gender in data (Jz). Predictions of the model may also be
required to be validated (1, J4, Ig). Simpler models and particularly rule-based sets
have the advantage of being typically less affected by biases, and decisions based on
them tend to be based on relevant factors (J4). Previously set expectations may also
be used to validate a model’s prediction (11, Ig). A discrepancy between prediction and
what is used for validation should be investigated (Jg). Interviewee five suggests af-
fected stakeholders as a source of functional evaluation regarding the system’s fairness
(Js). As a deciding factor to overcome individual differences in opinions on the fairness
of the system, a majority vote or some other kind of threshold may be introduced (Js).
The challenge of possible discrimination of minorities which such an approach could
induce, can be addressed by sampling minorities representatively when selecting the
composition of affected individuals responsible for evaluation (J;). An audit assessment
should also consider ethics and moral considerations relevant to the system (4, 1). As
fairness and ethics are closely related, there needs to be a discussion as to what extent
fairness should represent normative values in a society (J9). A central question here is:


Fairness Certification 41

Is it fair if a system results in morally questionable actions that affect all groups in the
same way (Jj)? A good fairness understanding of the use case, which is discussed in
“Process criteria” is required to resolve ambiguity. The last criteria to consider for func-
tional testing focus on the interaction of humans with computers and the involvement
of humans to improve fairness. Humans sometimes tend to accept predictions of ma-
chines as accurate and tend to become complacent in reviewing system outputs before
utilizing them for decision-making, which is called automation bias. This automation
bias is to be avoided as it does not counteract biases in predictions from impacting
decision-making based on them (111). Hence, there should also be criteria for human
decision-making that counteract automation bias (11). Humans should generally be
involved and in the loop when assessing system fairness (Jo, I7, I). Regarding the ap-
plication of the model, there should be an assessment regarding the necessity of human
involvement in decision-making. In specific, particularly critical applications, human
involvement in decision-making may be crucial (/i2). Many fairness issues are only ap-
parent to humans and may currently not be automatically detectable (Jz). Moreover,
humans may give the automatic system a plausibility check (J2).

Metrics: Metrics aim to make the fairness of NLP models measurable. Metrics are
of essential importance for a targeted, quantifiable and reproducible assessment of the
fairness of an application (J7, Ig, Ii2). Metrics based on a generalization of the model
are named as well as metrics that follow an impact-based approach regarding fairness.
Robustness and generalization may be seen as essential elements in assuring confidence
in the model’s property of not generating unexpected, unfair results (J2, l4, Ig). For the
first, the expected calibration error is named as a metric keeping track of the robustness
of a model (Ig). Also, standard metrics commonly utilized in data science may already
indicate the generalization of a model and hence may be assessed (Ja). The impact-
based approach targets potential fairness issues for particular groups more directly (Js),
which manifests in the composition of fairness metrics. A major point of discussion in
such metrics is their level of specificity (Js, I7, ha). A specific metric for fairness will
not easily transfer between use cases as there is yet to be a dynamic fairness metric
capable of handling that variety (J7). Less specificity in the metric will lead to less
relevant results in the specific use case (J3, 4). Interviewee eight hints at a trade-off
between performance and fairness metrics that may need to be made by sacrificing some
performance for the merit of the impact-based metric (Jg). Also, some metrics should
consider and capture diversity information as a context when assessing performance
metrics (Jg). Another impact-based metric would be a performance disparity threshold
that needs to be met (12, I7, I9). The quality of service provided by the system should be
evaluated regarding differences between groups, and a threshold should set a maximum
acceptable deviation (J7, Ij). That may be expanded beyond the mere performance of
the system and might include factors like accessibility and usability of the system as
well (J2, J9). The latter aims at removing barriers to system use (J2) and may be less
quantifiable than just performance disparity. Another approach would be introducing
a metric that penalizes the model’s use of sensitive attributes or unethical content. It
would be used by the optimizer of the model when calculating loss (Ji9). By this kind
of reinforcement, some major fairness issues that could occur may be ruled out (Jo).
For instance, in question answering, responses that contain insensitive or stereotyped
content could be avoided (Jio). A wide variety of fairness metrics are also represented
in different definitions of fairness available to choose from (Js, Liz, 4). No matter what
metric is used in the end, its impact on the technical system must be assessed, and its
suitability and implications for the use case must be assessed (11, Ig).


42 Freiberger et al.

B.6 Operations Criteria

Bringing a system into operation and going through continuous improvement cycles
to maintain its quality should be investigated in a fairness certification process. These
steps in the lifecycle determine whether a system will maintain fairness over time (Jo).
Fairness can deteriorate in operations, for instance, at the cost of performance (J2).
Interviewee six mentions that the certificate must maintain integrity over time (Je),
which supports the relevance of system operations criteria for fairness certification.
Operations need to build quick reaction strategies to counteract fairness issues that
occur swiftly (h, I7, 1). Standard pipelines for retraining and publishing models are
not viable as they take too long (/7). Another consideration in operations is the resource
distribution for models, which should not put fair models at a disadvantage regarding
resource access compared to other models (Ji90). When drastic changes are made or
occur in a systems environment, there may be a requirement to renew the overall
certification assessment or at least those assessments affected by the change (Js, I7).
Such changes may come in various forms. A new version roll-out may be a reason (i,
I7, Ig). There should also be detailed documentation throughout model versions to
ensure traceability when a fairness issue occurs (f;). Adaptions in data (h, I, Is),
changing stakeholders (J7), or a change in the system’s application (Jg) may be further
reasons for renewing affected assessment steps. Deployment (J2, Jz, I¢), monitoring (h,
lh, Is, Is, I7, Is, Is, 13), and the feedback loop for continuous improvement (1, l2, Js,
Ia, Is, 17, Is, I9, Tio) are relevant to interviewees as the main criteria for operations,

Deployment Criteria for deployment are mainly brought up in the context of pro-
viding different model sizes via pruning or knowledge distillation approaches (Jz). First,
it needs to be checked whether pruned or quantized models still fulfill fairness-related
criteria established before to the same extent as their larger counterparts do (J2). It
makes little sense to perform an intense assessment for the large model that the user
does not utilize in practice and, for instance, miss out on functional assessment of the
smaller model that a user may use more frequently (Jz). Smaller models tend to be
more prone to underfitting issues and should be checked accordingly (J3). On the other
hand, larger models may need more testing as they are opaque and tend to contain
more complex and challenging to identify biases with more data being used for training
them (J3, Jg). When choosing a suitable model size, performance and fairness objectives
should play a role (Jz).

Fairness Tests

Interventions

Moderating Users

Fig. 8. Criteria relevant to “Monitoring” hierarchically mapped

Monitoring Monitoring can be seen as a set of mechanisms that ensure fairness by
only giving clearance for a new model version to be introduced or current versions to


Fairness Certification 43

stay operational if fairness criteria can be assured (h, Js, I7, Io, lz). It typically involves
fairness testing and intervention mechanisms, as shown in Figure[8| Fairness tests have
already been introduced under “Functional Testing Criteria” in the “Modeling & Eval-
uation Criteria” chapter. Such an evaluation process based on fairness test sets should
be extended to assess the model over time as it goes through continuous improvement
cycles (i, I2, Is, 17). The goal is to quantify the occurrence of fairness issues (J3) which
may be valuable information for an audit. With every new model or change in data or
parameters, the fairness dimension of the system may be affected (Jz). Interviewee one
mentions the importance of standards that can be systematically checked with every
system retraining (/,). Such testing could be run at a very granular level assessing,
for instance, confidence and explanation scores on a sentence level as a foundation for
later judging fairness (Jz). Passing scores or red flags should be established for fairness
test sets to make the fairness testing impactful (J2, I3). Interviewees are divided on
how automated such testing can be. While interviewees one and nine see potential in
automated fairness testing, interviewee seven argues the process is not automatable as
there is no dynamic fairness metric (1, I7, I9). Depending on the automation of the
testing, frequent testing is favorable (/1, [3). In large-scale organizations or in a highly
dynamic context, where retraining models takes place multiple times a day, a fairness
assessment with every retraining could cause a considerable overhead (13). The com-
pany may execute this fairness testing process right after quality assessment to assess
the fairness impact of changes in quality (Jj). In some cases, a system may be oper-
ated or operating in a potentially harmful way. To prevent the system from causing
damage, intervention mechanisms are established. Particularly for transfer learning,
there is the issue of a user or third party potentially misusing a system for unintended,
unethical purposes like spreading misinformation, which needs to be avoided if possible
(113). Interviewee 13 mentions the curse of globalization in this context, as traceability
of what is built with baseline models stays unknown to the provider (113). Moreover,
misuse has previously also caused issues with systems with automatic retraining on
usage data (J9). Targeted, hostile user behavior in the form of interactions with the
system can make it develop strikingly unfair tendencies (Jo, io). Manual controls and
automated corrective mechanisms on a quantifiable foundation may be implemented
to ensure system fairness over time (Jy). Generally, a certification process could check
whether the issue of misuse is addressed and whether potential countermeasures are
in place. Moderating users, on the other hand, can be an option to cope with user
behavior that negatively affects a system’s fairness (J3). Interviewee three mentions
four shortcomings of moderating users of a system. Its practicality is questionable as
a large volume of data needs to be handled, making it impossible to have human over-
sight (J3). That leads to utilizing models for moderation which may also be biased (J3).
Moreover, moderation may just cause a problem shift by users utilizing competitors’
services, making moderation, not incentive compatible as it counteracts business goals
(Js). In case a system develops extreme, unacceptable behavior, there also needs to be
some form of kill switch, as running the system until the next update is rolled out may
sometimes be unacceptable (J7).

Feedback Loop Typically, a system is not designed to be static but rather dynamic,
utilizing feedback data from its use for improvements (9). That offers the advantages
of outgrowing systemic biases (Jg) and raising general model quality (Jz), which aligns
with business goals. A similar approach to active learning may be beneficial for creating
an effective feedback loop for counteracting biases in the system (J1). By deducing


44 Freiberger et al.

User-made Corrections

Flagging Option
Understand Fairness Issue

Evaluation & Preprocessing

before Retraining

Fig. 9. Criteria relevant to the “Feedback Loop” hierarchically mapped

Feedback Interface

Feedback Loop

Use of Feedback Data

systematic characteristics of biased cases and giving them as input back to users, their
feedback can be more targeted (J4). To operationalize a system’s feedback loop, an
interface for the user to provide feedback or potentially even corrections for fairness
issues is required. The acquired feedback data needs to be used appropriately to improve
the system. The latter is displayed in Figure[9]

Feedback Interface: The interface should allow users to interact with the system (/,
Ih, Is, Iv). A challenge such interfaces must overcome is engaging the user to utilize
them (Jz, I4). There may be a reluctance due to convenience (J4) or disengagement
and disinterest in continuing to use the system if someone is offended by an occurring
fairness issue (J2). An audit might consider the extent to which such interaction mech-
anisms are utilized to evaluate their effectiveness in exposing fairness issues so they can
be addressed. In some cases, it may also be challenging to establish an interface in the
first place as the user only indirectly encounters the model a few stages downstream
in the process of an application (Js). So, an audit should consider how the model is
embedded in a system. An organization may also have user-centric teams that extend
feedback information acquired in a visual interface integrated into the application (J7).
Those user-centric teams must closely collaborate with the organization’s development
team as their input may be required to identify previously unknown fairness issues,
which can then be replicated and mitigated by the development team (J7). It may
make sense to check for an interface for users to correct inference of a system (1, Ig).
The number of corrections made by users may also be relevant information for a cer-
tification (14). Interviewee 4 also mentions corrections that can be made for a model
explanation as feedback on what users would base the prediction on (J). The second
form of interface targets reporting or flagging fairness issues encountered in use (i,
In, Iz, I7). The users should be able to express their discontent with the fairness issue
that they should be able to describe (1, Js, I3, I7). It should be checked whether such
an interface is immediately accessible by a user (J2).

Use of Feedback Data: The issue with such interfaces is a tendency to shift ac-
countability for fairness onto the system user, who may also be biased (J). Hence,
for certification, the use of feedback data and accountability that comes along with
it should be checked. That covers correctly understanding the fairness issue at hand,
evaluation and preprocessing of data that should flow into retraining a system, and
mechanisms for deciding when to retrain the system. Understanding the fairness issue


Fairness Certification 45

at hand involves the challenge of tracing the fairness issue back to the part of the
system where it originates (1, Ig). Mechanisms for understanding the cause of the fair-
ness issue should be implemented (h, Js, I3, I7). Assessing the expectation of the user
and his or her explanation of unfairness for the prediction may be a first step in un-
derstanding the issue (/,). Tracking who specifically reports fairness issues may hint,
for instance, at a marginalized group that may have been neglected in development
(I2). Depending on the system, such an investigation may be only done if a minimum
number of reports mention the same fairness issue (J3). After identifying the primary
cause, the issue may be tested and reproduced as a baseline for mitigation (7). The
incoming feedback data from model use that should go into retraining should not be
used without evaluating incoming data and preprocessing (/io). This step should be
automated (Ji9) as human involvement would be impractical with the amount of data.
The evaluation of incoming data and preprocessing measures may be guided by what
has been previously performed to assess data quality and to preprocess data (lio). For
instance, sensitive attributes may be removed as it may be done in the initial pre-
processing (/io). Retraining should be initiated in case a fairness issue is repeatedly
reported (J3) or regularly based on feedback data from model use (J4). It should also be
systematically checked whether the issue is removed with retraining (11). For utilizing
feedback data in the most effective way possible, an active learning approach may give
directions on what is needed for improvements regarding biases (Ia).
