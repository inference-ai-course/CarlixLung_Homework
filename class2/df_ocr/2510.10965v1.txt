2510.10965v1 [cs.CL] 13 Oct 2025

arXiv

JUDGE BEFORE ANSWER: CAN MLLM DISCERN THE FALSE PREMISE IN QUESTION?

Jidong Li', Lingyong Fang', Haodong Zhao', Sufeng Duan', Gongshen Liu‘?

‘School of Computer Science, Shanghai Jiao Tong University
Inner Mongolia Research Institute, Shanghai Jiao Tong University

ABSTRACT

Multimodal large language models (MLLMs) have witnessed
astonishing advancements in recent years. Despite these successes,
MLLMs remain vulnerable to flase premise problems. However, ex-
isting benchmarks targeting this issue are limited in scope: they often
lack fine-grained categorization, exhibit insufficient coverage, and
thus fail to provide a rigorous evaluation of the ability of models to
recognize false premises.

To bridge this gap, we introduce a fully automated pipeline for
constructing a comprehensive benchmark of false premise questions.
Our method systematically categorizes the premises into three main
types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show cur-
rent MLLMs still struggle with false premise recognition. Building
upon this benchmark, we further propose a recognition enhance-
ment framework tailored to strengthen the robustness of MLLMs
to detect false premises. Extensive experiments demonstrate that
models trained with our framework achieve significant improve-
ments in false premise recognition. Our code is publicly available at

https://github.com/JidongLi-hub/JudgeBeforeAnswer

Index Terms— False Premise, Multimodal Language Model

1. INTRODUCTION

The evolution of Multimodal Large Language Models (MLLMs)
has marked a significant milestone in artificial intelligence, with
models exhibiting remarkable capabilities in handling complex tasks
that integrate vision and language. However, a paradox has emerged:
this exponential growth in capability has not been matched by a
corresponding increase in reliability and trustworthiness. A criti-
cal manifestation of this brittleness is the “false premise problem”,
which arises when a model is presented with a question contain-
ing factually incorrect or non-sensical presuppositions. An ideal
model would identify and refute the invalid premise. Instead, cur-
rent MLLMs often accept the premise by default, proceeding with
unnecessary and flawed reasoning that culminates in a confidently
delivered, yet entirely unreliable answer [2].

Research on false premise problems initially started with single-
modal language models. For instance, benchmarks like FalseQA
revealed that while LLMs possess some capacity to recognize false
premises, they often fail to refute them explicitly. Similarly, work
in mathematical reasoning [4], such as MathClean [5], has exposed
the limitations of even powerful models in detecting defects within
problems. MathQ-Verify [6] to filter ill-posed mathematical ques-
tions, thus improving the verification performance. In addition, De-
coPrompt introduced a prompting strategy that leverages LLM
to “decode” false premises in order to mitigate hallucination-prone
prompting algorithms. As research pivots to the more complex mul-
timodal space, the problem is amplified. A work introduces the con-

cept of unresolvable detection in MLLMs[8]}. While early multi-
modal benchmarks like MAD-Bench [9] and ISEval have initi-
ated the investigation, they expose the clear need for more sophis-
ticated evaluation tools. There are also methods that introduce ad-
ditional steps, enabling the model to segment the image to verify
the existence of premises rather than making direct judgments[11].
These existing datasets are often limited in scale, with MAD-Bench
containing fewer than 1,000 samples, or they lack the fine-grained
categorization necessary for deep diagnostics.

Beyond dataset construction, another line of research has fo-
cused on enhancing models’ ability to recognize or explicitly re-
ject questions containing false premises. For instance, some re-
searchers attempted to strengthen the ability of models to respond
with “I don’t know” when facing ambiguous queries [12]. Similarly,
works such as FalseQA and ISEval highlight that while cur-
rent LLMs and MLLMs have partial capabilities in recognizing false
premises, their success is highly dependent on whether they are ex-
plicitly guided or equipped with refutation mechanisms.

This landscape reveals two fundamental gaps in current re-
search. First, there is an Evaluation Gap: a lack of a fine-grained,
comprehensive, and large-scale dataset to precisely measure the
ability of MLLMs to recognize false premises. Existing datasets
are often limited in their coverage and scope. Second, there is a
Methodological Gap: a need for a targeted training framework to
enhance a model’s ability to scrutinize and reject false premises.
While some studies have focused on prompting models to respond
with “I don’t know” to ambiguous queries, a dedicated approach to
instill this skill is missing :

This paper addresses these gaps with a synergistic, two-part
solution. First, we introduce JBA, a comprehensive dataset for
false premise questions, built using a fully automated construction
pipeline. This method categorizes premises into a hierarchical tax-
onomy of three levels and thirteen subtypes, creating a systematic
dataset while significantly reducing annotation costs. Second, we
propose JBA-GRPO, a reinforcement learning framework designed
to enhance an MLLM’s ability to recognize and explicitly address
such errors. This framework leverages the training data conve-
niently generated by the automated pipeline, creating a tailored and
effective training regimen. Extensive experiments show that rep-
resentative MLLMs still perform poorly on this task, but a model
trained with the JBA-GRPO framework achieves substantially better
performance, validating the effectiveness of this approach. The main
contributions are as follows:

e We introduce JBA, a comprehensive benchmark for evaluat-
ing false premise detection in MLLMs, created through an innova-
tive automated pipeline. It features a hierarchical taxonomy of 13
subtypes across three cognitive levels for detailed analysis.

e We propose JBA-GRPO, a reinforcement learning framework
to enhance MLLM in identifying and rejecting false premises, incor-
porating a novel “reasoning reward” to ensure logical coherence.


MLLM

vag Right Turn
Premise Sign

—y-e 7 Hh - —>

Tima sano | out a symbol in the
image or output NO.

| containing parking sign. |

Generate a problem with a false premise that
has been replaced or with the correct premise.

MLLM

Caption: The image
seems to be ina
Generate a short caption P . .
residential area, with a
parking sign standing

beside the road, two

brown buildings

behind it, and cyclists.

There is a building behind the parking sign, how many floors are there?

Negative Sample

There is a building behind the right turn sign, how many floors are there?

Fig. 1: The fully automated pipeline for constructing our false premise dataset, consisting of three stages: Visual Premise Extraction, Premise-
Aware Captioning, and Target Question Generation. First, an MLLM extracts a premise of a specific type from an input image. Next, it
generates a concise caption that must include the extracted premise. Finally, an LLM produces a false premise question or a true premise
question by embedding either a false premise obtained through replacement nor the correct premise into declarative forms.

e Experiments reveal systemic weaknesses in existing MLLM
and demonstrate the superiority of the JBA-GRPO trained model,
establishing a new standard for reliable multimodal reasoning.

2. JBA DATASET

2.1. Construction Pipeline

In this section, we propose a fully automated pipeline for generat-
ing false premise question datasets. By decomposing the task into
modular steps and leveraging the advanced text and image under-
standing capabilities of state-of-the-art LLMs and MLLMs, our ap-
proach enables the efficient construction of large-scale, diverse, and
fine-grained false premise datasets.

Fig. [i] illustrates the fully automated pipeline for constructing
our false premise dataset, which consists of three main stages. Let
I denote an input image, t a specific premise type, p the extracted
premise content, c the premise-aware caption, and q the generated
question. The pipeline is formally described as follows:

2.1.1. Visual Premise Extraction

Given an image J and a premise type t (premise types are one of the
key factors ensuring dataset comprehensiveness and are described in
detail in Section in our running example, ¢ is symbol meaning,
an MLLM is prompted to detect whether J contains content corre-
sponding to t.

p = MLLM Extract(J, t).

If the premise is present, p is output and forwarded to the next stage.
If the premise is absent, the model outputs NO, indicating a failed
match, and the next candidate image or premise type is screened.

2.1.2. Premise-Aware Captioning

Based on the extracted premise p, the MLLM is prompted to gener-
ate a concise caption c for the image J that explicitly includes p or
treats it as the central focus of the description:

c = MLLM Caption(J, p).

The goal is not to produce a detailed description of the entire image,
but to provide a few sentences that capture the premise. This com-
pression of visual information simplifies and improves the efficiency
of subsequent premise replacement in the next stage.

2.1.3. Target Question Generation

Given the premise-aware caption c and the original premise p, an-
other LLM generates both positive and negative question samples q:

q = LLM_Question(c, p).

Negative samples: Replace the original premise p with an in-
correct or contradictory (but never identical) premise p’. The LLM
then generates a question that incorporates p’ declaratively while
querying other aspects of the caption. Formally:

p' ~ IncorrectPremise(p), — qneg = LLM_Generate(c, p’).

Positive samples: No replacement is performed, and the LLM
generates a question that includes the original correct premise p
declaratively, without directly asking about p:

pos = LLM_Generate(c, p).

Carefully designed few-shot examples are incorporated in
each step above to ensure the reliability of premise extraction and
replacement as well as the quality of the generated questions.

Furthermore, we can generate specific answers for the questions,
which serve as training data as described in Sectiorf3] Fig. B]illus-
trates an example of such training data, where the answer analyzes
the false premise and explicitly points out the error.

2.2. JBA Dataset

Based on the pipeline in Section[2.1] we establish a hierarchical tax-
onomy of false premises for systematic evaluation. As shown in
Fig. |2] the taxonomy consists of three levels—Perceptual, Cogni-
tive, and Reasoning—covering thirteen distinct categories, enabling
structured analysis of model performance from basic perception to
advanced reasoning.All images we use are sourced from the Visual
Genome dataset . We use Qwen?2.5-VL-72B-Instruct as our


m= Perceptual Level
lm Cognitive Level
t= Reasoning Level

Ke
os
Visual Attributes

Fig. 2: JBA Dataset Category Distribution

MLLM and Qwen3-32B as our LLM. The following are the cat-
egory descriptions.

Perceptual-Level Premises. This level focuses on directly ob-
servable aspects of images and includes six categories: Entity Exis-
tence, Visual Attributes, Numerical Attributes, State Attributes, Tex-
tual Content, and Symbolic Meaning. These categories primarily as-
sess model ability to perceive and describe visual content accurately.

Cognitive-Level Premises. This level requires understanding
higher-order relations and contextual information. It comprises five
categories: Spatial Relations, Interaction Relations, Part-Whole
Relations, Emotional State, and Scene. Collectively, they evaluate
whether models can integrate and reason over structured relation-
ships within the image.

Reasoning-Level Premises. The highest level demands infer-
ence beyond the visible content and includes two categories: Logical
Relations and Commonsense Knowledge. This level tests a model’s
ability to combine perception with abstract reasoning to interpret
complex or implicit aspects of the scene.

3. JBA-GRPO

3.1. Preliminary

Group Relative Policy Optimization (GRPO) is a reinforcement
learning strategy that enhances the deliberative capabilities of large
language models for accuracy-focused tasks like mathematics, as
implemented in the Deepseek-R1 model [17] [18].

GRPO’s objective (Eq. [Ip is to maximize a composite function
that balances reward with training stability. It optimizes for the ex-
pected aggregate reward R(o) while a Kullback-Leibler (KL)
divergence term, scaled by (, regularizes the policy 7» by penaliz-
ing deviation from a reference policy 7+es. The expectation is taken
over outputs sampled from a previous policy, 79,,,-

mre Bamana (p) [R(o) _ BD (76||7ree)]- qd)

The aggregate reward R(o) is a weighted average over a group of
G candidate outputs (Eq. [2). For each candidate, its score from
a reward function r(-) is standardized (centered and scaled by the
group’s mean and standard deviation). This standardized score is
then weighted by an importance sampling ratio, 79(0;)/76,,4 (0:);

to correct for sampling from an older policy.

R(o) = S> mo (0%)

j=] Oola (01)

ra) — mean(trGoO}Es)
atd(r(oy}e,)

Question: What direction does the road sign point for the 202?
Type: OCR Content

Label: False

Premise: 201

Answer:

<think>The image shows a road scene with various signs. A prominent sign indicates “201” directing traffic.
The question mentions “202” which is incorrect. Analyzing the actual text on the sign confirms it reads “201”
The discrepancy lies in the road number mentioned in the question. Since the question's premise is wrong, I
should focus on correcting the text rather than answering the question as posed.</think>

<answer>The sign does not show 202. It shows 201.</answer>

Fig. 3: An example data used in the JBA-GRPO training.

3.2. Reward Design

In addition to the commonly used format reward and answer reward,
we have also designed a reasoning reward.

Although the first two rewards enforce structural correctness and
final answer accuracy, they are insufficient to ensure the logical in-
tegrity of the reasoning process itself. A model could potentially
arrive at the correct answer through a flawed or coincidental chain
of thought, a phenomenon detrimental to robust learning. For in-
stance, within the <think> block, the model might present a line
of reasoning that is logically inconsistent or even directly contradicts
its own final conclusion.

To mitigate this, we employ a large language model (LLM) as
an evaluator to assess the quality of the reasoning articulated in the
<think> tag. This is accomplished using a meticulously crafted
prompt that provides the evaluator LLM with detailed criteria. The
LLM evaluates aspects such as logical coherence, relevance to the
question, and whether the step-by-step reasoning genuinely supports
the final answer. High-quality, coherent reasoning receives a positive
reward, whereas flawed or irrelevant reasoning incurs a penalty.

3.3. Training Strategy

We trained Qwen2.5-VL-7B using a two-stage process on our JBA
dataset. First, a Supervised Fine-Tuning (SFT) phase on the SFT
subset aligned the model with our required reasoning structure and
established baseline vision-language skills. Building on this SFT-
tuned checkpoint, we then performed a Reinforcement Learning
(RL) optimization phase. This second stage used the RL subset
and three custom reward functions to refine the model’s advanced
reasoning abilities.

4. EXPERIMENT

4.1. Evaluated Models

To evaluate the capability of existing MLLMs in recognizing false
premise questions, we conduct experiments on several representative
models, including InternVL3 [20], Qwen2.5-VL [21], LLaVA-v1.5-
7B [22], and LLaVA-OneVision [23]. In addition, we further apply
the reinforcement learning method proposed in this work to fine-tune
Qwen2.5-VL-7B-Instruct , with the goal of enhancing its ability
to identify false premises more effectively.


Perceptual Level

Cognitive Level Reasoning Level

Models

FPC FPDP TPIR FPC FPDP TPIR FPC FPDP TPIR
LLaVA-OneVision 58.740.05 78.040.12 55.4+0.06 61.640.06 81.340.11 57.140.07 77.340.09 92.540.09 69.7+0.12
LLaVA-v1.5-7B 60.140.06 67.640.11 57.340.07 67.140.06 72.340.09 63.940.08  82.240.08  84.640.11 79.940.12
InternVL3-8B 74.8+0.05  86.2+0.07 69.040.06 65.740.06 77.140.09 61.040.07 81.740.08  93.0+0.09 74.740.12
Qwen2.5-VL-7B-Instruct 77.740.05 84.640.06 73.2+0.07 71.2+0.05 81.640.08  65.940.07 84.3+0.08 87.740.1 — 81.2+0.11
JBA(Ours) 81.1+0.05  86.640.06 77.240.06 75.5+0.05 84.540.07 70.240.07 86.840.07  89.2+0.09 84,540.11

Table 1: Evaluation results across different levels. The best results are highlighted in bold. The same applies to the subsequent tables.

4.2. Evaluation Metrics

To evaluate the capability of different models in recognizing false
premise questions on the JBA dataset, we introduce three metrics.

Let N denote the total number of questions in the dataset. For
each question i € {1,...,.N}, we define binary variables y$”, y}? €
{0, 1} to indicate the ground-truth labels of False Premise and True
Premise, respectively. Similarly, jf’, g!? € {0,1} denote the cor-
responding model predictions. These notations allow us to formally
specify the evaluation metrics as follows.

N
1 ~FP | TP
FPC = woo + i"), (3)

False Premise Coverage (FPC) measures the proportion of all
questions for which the model correctly identifies a false premise,
reflecting its overall detection ability.

False Premise Detection Precision (FPDP) is the proportion of
correctly identified false premise questions among those predicted as
false premise, indicating detection accuracy.

True Premise Identification Rate (TPIR) measures the pro-
portion of correctly recognized true premise questions among those
predicted as true premise, reflecting reliability in identifying non-
false premises.

FPDP = (4)

TPIR =

4.3. Main Results

Table[2|shows model performance on whole JBA dataset. Most base-
lines score low on TPIR, indicating limited ability to detect rare or
challenging false premises, while higher FPDP scores suggest they
can generally reject false premises accurately. FPC reflects overall
recognition across all samples, highlighting substantial room for im-
provement to ensure more reliable responses.

Models FPC (t) FPDP(t) TPIR(t)
LLaVA-One Vision 62.4+0.04 82.740.08 57.6+0.05
LLaVA-v1.5-7B 65.940.04 72.940.07 62.2+0.05
Intern VL3-8B 72.0+0.04 84.040.06 66.3+0.05
Qwen?2.5-VL-7B-Instruct 75.940.04  84.040.05 70.9+0.05
JBA(Ours) 79.5+0.03 86.240.05 74.9+0.05

Table 2: Evaluation results on whole JBA dataset.

Table[I] reports the model performance on the three levels of the
JBA dataset. At the perceptual level, most models achieve relatively

high scores on FPDP, indicating that they can stably exclude explicit
false premises. However, their TPIR scores are generally low, sug-
gesting that they tend to overlook complex or ambiguous perceptual
errors. At the cognitive level, InternVL3-7B and Qwen2.5-VL-7B
show a decline across all metrics, whereas LLaVA-OneVision and
LLaVA-v1.5-7B exhibit improvements in each metric. At the rea-
soning level, all models perform better than at the other two levels,
with InternVL3 achieving an FPDP of 93%, demonstrating a strong
ability to identify reasoning-related false premises. Although the
difficulty of recognizing different types of false premise questions is
inherently fixed, the results indicate that different models still exhibit
preferences for certain types of premises.

Overall, the Qwen2.5-VL-7B-Instruct model exhibits signifi-
cantly stronger capabilities in recognizing false premises compared
to other baseline models. Moreover, our proposed JBA model con-
sistently outperforms these baselines across a range of evaluation
metrics. The notable performance improvement of JBA can be
attributed to two key design principles.

First, we introduced a structured reasoning mechanism by in-
corporating a <think> tag into the model’s inference pipeline.
This tag enforces an explicit pre-analysis phase, during which the
model systematically examines both the visual content and the tex-
tual premise of the input. Through this process, the model is able
to deconstruct and compare the components of the question, thereby
identifying potential discrepancies with greater precision.

Second, we employed reinforcement learning (RL) to further
refine and generalize the model’s reasoning ability. By rewarding
the model for accurately detecting premise errors through its struc-
tured thought process, the RL framework enhances the robustness
and adaptability of the model’s inferential capabilities, particularly
when dealing with novel or unseen examples.

As a result, the superior performance of the JBA model in most
comparative evaluations validates the effectiveness of integrating ex-
plicit analytical reasoning with reinforcement learning for the task of
robust premise-error recognition.

5. CONCLUSIONS

This paper addresses the shortcomings of current multimodal false
premise datasets, which are often limited in scope and costly to
annotate. We introduce a fully automated pipeline using task de-
composition and prompt design to construct JBA, a comprehensive
dataset categorizing false premise questions into three levels and
13 subtypes. Evaluations on JBA reveal that while MLLMs show
capability in recognizing false premises, their performance is lim-
ited, indicating room for improvement. To enhance robustness, we
develop a tailored reinforcement framework for false premise recog-
nition. Our models consistently outperform baselines, validating
the efficacy and establishing JBA as a valuable dataset. These find-
ings highlight the need for specialized training to enhance MLLMs’
ability to detect and manage false premises across diverse scenarios.


(1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

6. REFERENCES

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, et al., “Learning trans-
ferable visual models from natural language supervision,” in
International conference on machine learning. PMLR, 2021,
pp. 8748-8763.

Jinzhe Li, Gengxu Li, Yi Chang, and Yuan Wu, “Don’t take
the premise for granted: Evaluating the premise critique ability
of large language models,” arXiv preprint arXiv:2505.23715,
2025.

Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng,
Zhiyuan Liu, and Maosong Sun, “Won’t get fooled again:
Answering questions with false premises,’ arXiv preprint
arXiv:2307.02394, 2023.

Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang
Zhang, and Fuli Feng, “Evaluating mathematical reasoning
of large language models: A focus on error identification and
correction,” arXiv preprint arXiv:2406.00755, 2024.

Hao Liang, Meiyi Qiang, Yuying Li, Zefeng He, Yongzhen
Guo, Zhengzhou Zhu, Wentao Zhang, and Bin Cui, “Math-
clean: A benchmark for synthetic mathematical data cleaning,”
arXiv preprint arXiv:2502.19058, 2025.

Chengyu Shen, Zhen Hao Wong, Runming He, Hao Liang,
Meiyi Qiang, Zimo Meng, Zhengyang Zhao, Bohan Zeng,
Zhengzhou Zhu, Bin Cui, et al., “Let’s verify math questions
step by step,” arXiv preprint arXiv:2505.13903, 2025.

Nan Xu and Xuezhe Ma, “Decoprompt: Decoding prompts
reduces hallucinations when large language models meet false
premises,” arXiv preprint arXiv:2411.07457, 2024.

Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming,
Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, and Kiyoharu
Aizawa, “Unsolvable problem detection for vision language
models,” in ICLR 2024 Workshop on Reliable and Responsible
Foundation Models.

Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan, “How
easy is it to fool your multimodal Ilms? an empirical analysis
on deceptive prompt,” in Neurips Safe Generative AI Workshop
2024.

Haiqi Yang, Jinzhe Li, Gengxu Li, Yi Chang, and Yuan Wu,
“Can large multimodal models actively recognize faulty in-
puts? a systematic evaluation framework of their input scrutiny
ability,” arXiv preprint arXiv:2508.04017, 2025.

Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap,
Ritwik Gupta, Xudong Wang, Joseph E Gonzalez, and Trevor
Darrell, “See say and segment: Teaching Imms to overcome
false premises,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, 2024, pp.
13459-13469.

Sungguk Cha, Jusung Lee, Younghyun Lee, and Cheoljong
Yang, “Visually dehallucinative instruction generation: Know
what you don’t know,” arXiv preprint arXiv:2402.09717, 2024.

Archit Parnami and Minwoo Lee, “Learning from few exam-
ples: A summary of approaches to few-shot learning,” arXiv
preprint arXiv:2203.04291, 2022.

[14]

(15)

[16]

(17]

[18]

[19]

[20]

[21]

[22]

[23]

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji
Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-
Jia Li, David A Shamma, et al., “Visual genome: Connecting
language and vision using crowdsourced dense image annota-
tions,” International journal of computer vision, vol. 123, no.

1, pp. 32-73, 2017.

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun
Tang, et al., “Qwen2. 5-vl technical report,’ arXiv preprint
arXiv:2502.13923, 2025.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan
Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,
Chenxu Ly, et al., “Qwen3 technical report,’ arXiv preprint
arXiv:2505.09388, 2025.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao
Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,
Yang Wu, et al., ““Deepseekmath: Pushing the limits of math-
ematical reasoning in open language models,” arXiv preprint
arXiv:2402.03300, 2024.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu
Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,
Xiao Bi, et al., “Deepseek-r1: Incentivizing reasoning ca-
pability in Ilms via reinforcement learning,’ arXiv preprint
arXiv:2501.12948, 2025.

Tim Van Erven and Peter Harremos, “Rényi divergence and
kullback-leibler divergence,” [EEE Transactions on Informa-
tion Theory, vol. 60, no. 7, pp. 3797-3820, 2014.

Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shen-
glong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie
Shao, et al., “Internvl3: Exploring advanced training and
test-time recipes for open-source multimodal models,” arXiv
preprint arXiv:2504.10479, 2025.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,
Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou,
“Qwen-vl: A frontier large vision-language model with ver-
satile abilities,” arXiv preprint arXiv:2308.12966, vol. 1, no.
2, pp. 3, 2023.

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, “Im-
proved baselines with visual instruction tuning,” in Proceed-
ings of the LEEE/CVF conference on computer vision and pat-
tern recognition, 2024, pp. 26296-26306.

Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,
Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei
Liu, et al., “Llava-onevision: Easy visual task transfer,’ arXiv
preprint arXiv:2408.03326, 2024.
