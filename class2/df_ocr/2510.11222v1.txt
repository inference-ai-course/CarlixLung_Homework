arX1iv:2510.11222v1 [cs.CL] 13 Oct 2025

Fairness Metric Design Exploration in Multi-Domain
Moral Sentiment Classification using
Transformer-Based Models

Battemuulen Naranbat?, Seyed Sahand Mohammadi Ziabari®>*, Yousuf
Nasser Al Husaini®, Ali Mohammed Mansoor Alsahag®

“Informatics Institute, University of Amsterdam, Science Park, 1098
XH, Amsterdam, The Netherlands
’Department of Computer Science and Technology, SUNY Empire State
University, Saratoga Springs, NY, USA
“Faculty of Computer Studies, Arab Open University, Oman, P.O. Boz
1569, 130, Muscat, Sultanate of Oman

Abstract

Ensuring fairness in natural language processing for moral sentiment classifica-
tion is challenging, particularly under cross-domain shifts where transformer
models are increasingly deployed. Using the Moral Foundations Twitter
Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistiIBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance masks important dispari-
ties: we observe pronounced asymmetry in transfer, with Twitter—Reddit
degrading micro-F1 by 14.9% versus only 1.5% for Reddit—+Twitter. Per-
label analysis reveals fairness violations hidden by overall scores; notably, the
authority label exhibits Demographic Parity Differences of 0.22—0.23 and
Equalized Odds Differences of 0.40—-0.41. To address this gap, we introduce
the Moral Fairness Consistency (MFC) metric, which quantifies the cross-
domain stability of moral foundation detection. MFC shows strong empirical
validity, achieving a perfect negative correlation with Demographic Parity

*Corresponding author.
Email addresses: battemuulen.naranbat@student.uva.nl (Battemuulen Naranbat),
sahand.ziabari@sunyempire.edu (Seyed Sahand Mohammadi Ziabari),
yousufnaser@aou.edu.om (Yousuf Nasser Al Husaini), a.m.m.alsahag@uva.nl (Ali

Mohammed Mansoor Alsahag)


Difference (9 = —1.000, p < 0.001) while remaining independent of stan-
dard performance metrics. Across labels, loyalty demonstrates the highest
consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These
findings establish MFC as a complementary, diagnosis-oriented metric for
fairness-aware evaluation of moral reasoning models, enabling more reliable
deployment across heterogeneous linguistic contexts.

Keywords: Moral Sentiment Classification, Moral Foundation Theory,
Fairness Metric, BERT, DistiIBERT, Reddit, Twitter

1. Introduction

In recent years, various Transformer-based Language Models (LMs) have
been seamlessly integrated into society. The rapid expansion of LM has led
to serious concerns about fairness and ethical issues [9]. For example, large
language models (LLMs), which incorporate the ‘self-attention’ mechanism
of transformer models [15], have been widely adopted in the industry due to
their ability to perform various tasks, ranging from simple text processing
to complex knowledge generation [1]. Today, the latest versions of LLMs are
being released by leading tech companies, including GPT-4 by OpenAT [11],
LLaMA-3 by Meta [12], and the newly introduced open-source DeepSeek-V3
[13]. The introduction of these models has significantly transformed the
economic, academic and geopolitical landscape, marking a new chapter in
the artificial intelligence race. The rapid advancements in such models have
introduced certain societal risks. Since language models are pre-trained on
human language artifacts, often sourced from the internet, various forms
of biases and unfair behaviors may be embedded within these models [4].
Therefore, open-source pre-trained transformer-based language models such as
BERT [24] continue to serve as important baseline models [3] for researching
ethical implications. Such open source models provide greater transparency,
making them valuable resources [10] to further investigate risk mitigation
strategies, ensure fairness, and promote ethical AI development in various
applications.

To address emerging concerns regarding the fairness of transformer based
language models, beyond traditional evaluation metrics for moral sentiment
analysis, Moral Foundation Theory (MFT) is employed. This theory, which
defines six universal foundations of moral reasoning, is structured into vice
and virtue polarities: Care/Harm, Fairness/Cheating, Loyalty/Betrayal,


Authority/Subversion, Purity/Degradation, and Liberty/Oppression
[2]. One of the primary advantages of vice and virtue polarity in moral
sentiment classification is that a broader contextual range across various
domains can be captured. For instance, in binary sentiment analysis, only
positive and negative sentiments are extracted from text. In contrast, moral
sentiment can be identified without reliance on specific linguistic structures,
languages, cultural norms, or demographic groups when MFT is used [20].
Therefore, Moral Foundation Theory offers a robust framework for moral
sentiment classification and provide a groundwork for researching ethical
implications in language models and development of novel metrics for fairness
in context of moral sentiment classification. Thus, one of the key reasons why
MFT is a suitable framework for moral sentiment classification is its ability to
provide a structured moral reasoning framework that is empirically validated
across cultures [2] which highlights that the six moral foundations operate as
universal dimensions across societies, allowing for a wider adoption in Natural
Language Processing (NLP) tasks such as in content moderation [36], policy
making and conflict resolution [26].

Using Moral Foundation Theory, scholars have generated benchmark
datasets for the classification of moral sentiment [17, 16]. Therefore, by
applying this universal theory in moral sentiment classification, an attempt
can be made to quantify the fairness of language models beyond traditional
performance metrics. To address these challenges, this paper will evaluate the
extent to which an evaluation framework incorporating novel fairness metrics
can bridge a critical research gap in the classification of moral sentiments
across various social media platforms.

Therefore, we can formulate a robust research question to address the
research gap: To what extent fairness metrics can be developed that account
for differences between social media platforms in NEP models to ensure fair
classification of moral sentiments?

Through systematic investigation of this question, this research aims to
advance the understanding of fairness in models in the field of morality and
provide context to further develop fairness metrics that ensure fair deployment
of models.

2. Related Work

Since language models and their indirect implementations become more
ubiquitous, research regarding the fairness of such implementations become


crucial for developing robust models. To grasp this gap in the research, a
deeper exploration into existing approaches and relevant theory behind is
needed.

As technology advances, the complexity of our problems increases quickly.
Especially for sophisticated and subjective tasks concerning morality of any
dimension. Yet, researchers like Jiang et al. [8], have started a challenging
research in attempting to teach morality in AI systems. This paper proposes
an experimental framework called Delphi, that is based on deep neural
networks which are capable of reasoning and predicting descriptive ethical
judgments that is often aligned with human expectations. The empirical
results of this study suggests that Deplhi has a potential to generalize to
ethical scenarios with minimal alterations in context and outperforms state
of the art neural network models [8]. However, despite the good performance
of the Delphi framework, the study highlights their key limitations in the
reliability. The limitation of the study occurs when the Delphi learns on
ageregated statistically dominant behaviors of the data, sometimes pushing
‘normative’ or ’popular’ views, which can be seen as amplification of existing
biases. Exploring further the morality in artificially intelligent systems, one
of the leading companies in LLM development - Anthropic, have introduced a
work regarding the topic of moral self-correction in large language models [7].
This research have found out that language models trained with reinforcement
learning from human feedback (RLHF) start to have the ability of morally
self-correcting itself at around 22B parameters, which can help them to avoid
harmful outputs. Therefore, the results indicate us designing and evaluating
language models in terms of morality can be done more precisely in the future.

Further works in the field of morality in AI systems are grounded by
several frameworks in morality, ethics, and values. According to a review
of the literature by Vida et al. [37], the majority of the reviewed works
employed Moral Foundation Theory [2] as a baseline for their researches,
where 49 papers out of 59 with moral frameworks mentioned employed the
MFT. However, the MFT is not the sole moral framework used in NLP that
focus on morals, ethics and values. Previous studies implement ethics and
values through Schwartz’s Values Theory and Kohlberg’s stages of moral
development theory [38, 39].

The work of Hoover et al. [17] introduces Moral Foundations Twitter Cor-
pus, which remains as a crucial resource for tasks related to moral sentiment
analysis, due to high quality hand annotated data for model training and
evaluation. Complementary to the Twitter corpus, Trager et al. [16], intro-

4


duces their own variant of corpus based on the Reddit social media platform.
This work also follows the standard based on the MFT theory, extending the
existing resource to a fundamentally distinct domain of social media. These
important resources have enabled the development of MoralBERT [26], a
fined-tuned language model that can be used for classifying moral sentiment
in different environments. Extending the BERT family language models for
moral sentiment analysis tasks could be linked to the fact that these models
show sense of social norms [24] and demonstrate ability to capture morality
by identifying ’right’ and ’wrong’ [6]. Therefore, MoralBERT utilizes aggre-
gated and domain adversarial training strategies to improve moral sentiment
predictions. The fine-tuned model shows notable improvements in single
and multi labeled tasks, beating lexicon based methods like Word2Vec in F1
score. Similarly, work of Guo et al. [5], expands on multi domain morality
learning highlighting the flaw of using heterogeneous data for morality learn-
ing by proposing a data fusion framework. This framework allows training
on multiple heterogeneous datasets, not just aggregating them, but using a
domain adversarial training to align the training data in feature space using
a weighted loss function to handle the moral label shift. Therefore, further
analysis of fairness metrics is needed for pre-trained models of the BERT
family as these models tend to retain biases from the pre-training periods [4].

Despite these advancements in performance for various strategies for
moral sentiment classification models, existing approaches still struggle with
fairness evaluations |20]. The work of Zangari et al., highlights the results
of varying researches with transformer based models, which show how these
models over represent moral sentiments from their training data [35]. One
of the few solutions that improve the explainability of such models is the
Chain-of-Though approach introduced by Jin et al., [21] to gain insights
into the underlying hidden processes of the model. Recent advancements in
sentiment analysis problems in terms of fairness focus their work on sensitive
attributes such as gender, age, sex and political stance [40]. Further research
into the fairness of machine learning models indicates that different ML
models exhibit varying degrees of fairness when the Equalised Odds metric is
applied as a fairness evaluation to the same dataset. [41]. Therefore, existing
approaches of fairness evaluation in a context of moral sentiment classification
are fundamentally different than simpler sentiment analysis tasks, because
the morality concept itself is non-binary [42]. Given this fact, work of Park
et al. [42] have proposed a framework where the morality concept is treated
pluralistically by building a pluralist moral sentence embedding space via


a contrastive learning approach. However, the results indicate that moral
pluralism is hard to deduce without supervised approach with human labels.

All these mentioned works are part of a broader research effort to under-
stand machine and algorithm behavior. One of the major works that shed
light on the importance of comprehending machine behavior was done by
Rahwan et al., where the research highlights the necessity of broad scientific
research agenda to study and understand machine behavior not just in com-
puter science discipline, but expanding the research into various scientific
fields [19]. One of the motivations for such an interdisciplinary approach is
to try to solve the ‘black-box’ nature of algorithms from multiple perspec-
tives, meaning research should combine insights from contrasting fields. For
example, computer scientists and engineers working on machine or algorithm
behavior cannot fully assess the implications of their work because they lack
specialization in evaluating behavioral impacts of their works on a broader
scale [19]. Therefore, in the context of this research, a broader understanding
is needed on how language models are evaluated for fairness in such a way
that meaningful evaluations are resulted. An extensive empirical comparison
of extrinsic fairness metrics in the domain of NLP has been conducted by
the work of Czarnowska et al, where the results of this extensive research
indicate that various existing fairness metrics are simply parametric variants
of the three main fairness metrics proposed [23], meaning that the fairness is
able to be assessed by some of the metrics depending on the setting and the
questions being asked by the researchers.

3. Methodology

In this section, the general methodology of this research is outlined, which
can be divided into three main parts that directly address the research ques-
tions: (1) exploratory data analysis and data preparation of two distinct
social media datasets to understand moral label distributions across plat-
forms, (2) in-domain and cross-domain model training and evaluation using
DistilIBERT and BERT models to quantify disparities in moral sentiment
classification between Twitter and Reddit domains, and (3) novel fairness
metric development and validation specifically designed for moral sentiment
classification to measure cross-domain consistency and enable fair evaluation
of model behavior across social media platforms. The methodology can be
seen in Figure A.2 of Appendix A.


3.1. Data

Two distinct datasets, sourced from relevant social media platforms, are
utilized in this research. The datasets, which contain posts and comments from
Reddit and Twitter (now X), have been manually annotated in accordance
with the Moral Foundation Theory [2]. It can be observed that each dataset
represents a distinct domain for moral learning, owing to their contrasting
characteristics. It is noted that Reddit is characterized by user anonymity and
unrestricted character limitations in posts and comments, whereas Twitter is
distinguished by reduced anonymity and constrained text capacities.

Moral Foundation Twitter Corpus (MFTC) '

The dataset comprises of 35,108 ’tweets’ that were hand labeled by at
least three trained annotators, in total 21 distinct annotators contributed to
label moral annotations of the MFTC. Seven distinct datasets, each varied
by topic, are included in the collection. The MFTC include ’tweets’ on the
topics from such domains as: BLM, Election, Davidson, Baltimore, MeToo,
Sandy and ALM. Originally, the data set was intended to be collected us-
ing a Python script connected to the Twitter API. However, due to recent
changes in the company’s policy, retrieving historical tweets by their ID has
become significantly more challenging and costly. As a result, permission
was granted to use a version of the dataset which was released by Morteza
Dehghani, the author of the MFTC. The dataset, as provided, includes moral
labels such as Non-Moral, Harm, Cheating, Care, Fairness, Subversion,
Loyalty, Authority, Degradation, Betrayal, and Purity. The original
MFTC dataset is made availabel in a nested JSON format, thus necessary
preprocessing steps such as flattening the structure and transforming are
applied to convert into pandas DataFrame format. Through these preprocess-
ing steps, the dataset is transformed into a format containing 33,687 unique
‘tweets’ and 128,454 total ’tweets,’ where individual ’tweets’ are annotated by
multiple annotators. After preprocessing, the structure of the MFTC dataset
appears in the following way as seen in Figure A.7a.

Moral Foundation Reddit Coprus (MFRC)
This dataset is availabel on Hugging Face”, containing 13,995 posts/comments
from various ’subreddits’ of the Reddit platform. This corpus is divided into

Thttps://osf.io/k5n7y/
"https: //huggingface.co/datasets/USC-MOLA-Lab/MFRC

7


three main topic buckets: US Politics with subreddits like ’r/conservative’,
’r/antiwork’, ’r/politics’; Everyday Morality bucket with subreddits like
’r/IamTheAsshole’, ’r/nostalgia’, ’r/relationship_ advice’ and ’r/confession’;
and the last bucket French Politics with subreddits like ’r/geopolitics’, ’r/europe’,
’r/conservative’, ’r/neoliberal’, and ’r/worldviews’. Similarly, to the Twit-
ter corpus, moral sentiments were labeled by trained annotators, in total 6
distinct annotators, with an agreement threshold of 50% for the final labels.
The dataset, as provided, includes moral labels such as Non-Moral, Thin
Morality, Care, Equality, Authority, Proportionality, Loyalty and
Purity. The structure of the MFRC looks in the following way as in Figure
A.7b.

3.2. Exploratory Data Analysis

Before going in-depth with the experimental setup, initial exploratory data
analysis has to be performed. During this process, thorough analysis of the
existing Moral Foundation Datasets was conducted. This step is considered
crucial to understanding the distributions of the moral sentiment labels that
are contained in each post or comment. First of all, the Moral Foundation
Reddit Corpus is focused on, followed by the Moral Foundation Twitter
Corpus.

Figure A.3 illustrates all different combinations of moral sentiment labels
of the MFRC, which were hand annotated by trained expert annotators. First
of all, this graph show that label Non-Moral individually occurs the most.
The label Non-Moral is assigned to Reddit posts that are neutral and do not
exhibit any moral traits. The moral label Thin Morality also shows a high
occurrence in the graph, which can be explained by the fact that in MFRC
research, before assigning to moral label Non-Moral the annotators should
have looked for signs of Thin Morality, where the given text is on the verge
of expressing some kind of morality form. To investigate the moral labels,
visualizing the unique label distribution could give a detailed overview of the
moral label class balance which can be seen in Figure A.6. Plotting the graph
for each label gives a more narrowed down moral sentiment class distribution.
The MFRC contains 8 moral foundation annotations: the most frequent is the
label Non-Moral followed by Thin Morality, Care, Equality, Authority,
Proportionality, Loyalty, and Purity.

The next step is to compare the Moral Foundation Twitter Corpus with
the Reddit corpus, so the cross-domain testing experiments can be designed
in such way that moral sentiment label shift can be captured in a controlled


10000

(a) MFTC Label Distribution (b) MFRC Label Distribution

Figure 1: Comparison of label distributions in the two corpora after label harmonization
to 5 morality labels.

way. Therefore, the same visualization approach will be applied on the
moral labels of the MFRC. The initial, distribution shows even more mixed
annotations compared to the Reddit corpus, where the label non-moral is
the most frequently occurring. Thus, to understand the label distribution,
plotting the bar chart of the unique label occurrences gives more interpretable
insights of the distribution, which can be seen in Figure A.5. Similarly,
the label non-moral shows high occurrence, followed with the labels harm,
cheating, care, fairness, subversion, loyalty, authority, degradation,
betrayal, purity and some none relevant occurrences of nm and nh, which
will be removed from the training, validation and evaluation datasets.

3.8. Data Preparation

The MFRC dataset includes Reddit comments annotated for the presence
of moral language. The annotations follow updated theoretical develop-
ments in moral sentiment [16], distinguishing Proportionality, Equality
as separate moral foundations, rather than subsuming them under a single
Fairness label as in traditional MFT schemas. This approach results in
Non-Moral, Thin Morality, Care, Equality, Authority, Proportionality,
and Loyalty. For the purposes of model consistency across datasets, we
map Equality and Proportionality labels to Fairness, aligning with the
broader MFT based taxonomy and enabling cross dataset comparison. This
label unification is necessary since the MFTC does not distinguish between
Equality and Proportionality, similar mapping was applied in the work of
MoralBERT [26] to fine-tune a BERT model with increased moral awareness.

The MFTC provides tweets annotated with a broader set of moral labels,


which includes both virtues and vices. The annotated categories include:
Non-Moral, Care, Fairness, Loyalty, Authority, Purity, Harm, Cheating,
Subversion, Betrayal, and Degradation.

To ensure interoperability between the datasets during training and evalu-
ation, we perform label alignment by focusing on the following shared cate-
gories across corpora: Care, Fairness, Loyalty, Authority, Non-Moral.
Tweets or Reddit comments with labels not covered in both corpora are ex-
cluded from in-domain and cross-domain training and evaluation to preserve
label compatibility. Additionally, regular data cleaning methods for textual
data, such as lower casing, removing whitespace and special characters were
applied to the whole corpus of the MFTC and MFRC. No lemmatization or
stemming was applied, as transformer models are designed to process subword
units and learn contextual representations inherently [30]. Excessive pre-
processing might undermine this contextual richness, which can have strong
influence on morality learning tasks. The final moral label distribution, used
for the experiments can be seen from the Figure 1.

3.4. Model

Bidirectional Encoder Representations from Transformers or in short
BERT is a pre-trained deep learning model for NLP tasks. This model is
built upon the Transformer architecture [15], which allows the model to
leverage self-attention mechanism to capture contextual word representations
bidirectionally [24]. This means that the BERT can process text from both
directions simultaneously. The ability to construct deeper context aware word
embeddings is a crucial advantage |27| of using such models for multi-label
classification problem of moral sentiment analysis.

, (] r )
Attention(Q, kK, V) = softmax V (1)
Vdk

In this research, BERT and DistilIBERT models are adopted for multi-
labeled moral sentiment classification task to assess the extent on which
we can employ new fairness metric. This choice can be rationalized due
to key advantages of using such model for multi-label classification tasks
on moral sentiment labels. Moral sentiments can be highly contextual and
unique for specific social media platforms. Therefore, BERTs ability to handle
deeper contextual understanding of the textual data can be helpful for moral
sentiment analysis tasks [6]. Using BERT family language models for moral
sentiment analysis tasks could be linked to the fact that these models show a

10


sense of social norms [24] and demonstrate the ability to capture morality by
identifying "right" and "wrong" [6].

3.5. Loss Function

Binary Cross-Entropy with Logits Loss is a deliberate choice for multi-
label classification tasks where each instance may belong to zero or more
classes. While more advanced techniques like Focal Loss [44] exist that improve
performance under imbalanced data, the main target of this paper was to study
fairness behavior rather than trying to maximize the performance. Therefore,
in moral sentiment classification, the BCE loss is appropriate because it
allows for independent probabilistic modeling of each moral dimension [32].
BCEWithLogitsLoss is defined as:

L
i
LycELogits(Y,Z) = > [yi log o(z:) + (1 — yi) log(1 — o(z:))] (2)
i=1

where o(z;) = a is the sigmoid activation applied to the raw logits 2;,
yi € {0,1} is the true label, and L is the number of labels.

3.6. Model Training

For the models, DistiIBERT [22] and full BERT architectures are fine-
tuned using the Transformers library by HuggingFace.

DistilBERT, a distilled version of BERT, serves as our lightweight baseline
due to its reduced size and faster training time, making it an efficient bench-
mark for real-world applications. It is approximately 40% smaller and 60%
faster than BERT-base while retaining 95% of its performance on standard
benchmarks [22]. Its efficiency makes it ideal for understanding baseline
behavior and assessing performance across domains. We additionally exper-
iment with the full BERT model to serve as a comparative upper baseline.
While computationally heavier, BERT offers a higher representational capac-
ity, potentially leading to better generalization in complex, multi-label tasks
such as cross-domain moral sentiment analysis. The full BERT model was
incorporated to assess the trade-off between performance and computational
complexity. Each model was fine-tuned separately on the Moral Foundations
Reddit Corpus and Moral Foundations Twitter Corpus.

The classes such as DistilBertForSequenceClassification and
BertForSequenceClassification are utilized from HuggingFace, setting

11


num_labels according to the number of labels after label harmonization,
which results in 5 moral labels. For the MFRC, we treated the task as
multi-label classification, following the dataset’s allowance of multiple moral
foundations per post. For MFTC, which uses one label per tweet, we em-
ployed multi-label classification. The problem type was set accordingly using
HuggingFace’s problem_ type flag for the multi-label case. For in-domain
experiments, a 80/10/10 split is used for training, validation, and testing.
In cross-domain scenarios, the entire source domain dataset is used for fine-
tuning, while testing is conducted on the target domain.

This paper utilizes AdamW optimizer with a learning rate of 2e-5, which is
a widely accepted setting for fine-tuning transformer models for both BERT
and DistilBERT. The models were trained for five epochs on a single NVIDIA
A-100 GPU (via Google Colab), binary cross-entropy with logits loss for
multi-label tasks, automatically handled by the Transformers framework. To
ensure reproducibility, we fixed the random seed before training and did
not apply any oversampling or balancing techniques, instead preserving the
natural label distribution to reflect real world class imbalances. This decision
was deliberate, aiming to evaluate model robustness on underrepresented
moral categories, especially relevant for cross-domain evaluation, as correcting
for class or label balance is not always necessary and often might bring hidden
risks [29]. However, it is important to note that transformer models are highly
sensitive to hyperparameter configurations where performance and fairness
can be directly influenced by the configuration of the learning rate, batch size
and number of epochs [24].

Multi-labeled Classification Task for Moral Sentiment Labels (In-domain vs.
Cross-domain)

After training the models, the performance should be assessed under two
conditions:

e In-domain: the model was fine-tuned and tested in the same social
media domain, e.g train and tested for moral sentiment classification

on MFTC.

e Cross-domain: the model fine-tuned on one social media corpus is
tested on a different social media domain full data.

These steps are critical to understand how moral sentiment classification
models behave across domains.

12


8.7. Evaluation: Performance and Fairness Metrics for Moral Sentiment
Classification

We evaluate each model both in-domain (Reddit on Reddit, Twitter on
Twitter) and cross-domain (Reddit on Twitter, Twitter on Reddit). This
allows us to test generalizability, domain robustness, and transferability of
learned moral semantics. Evaluation metrics include BCE loss value, micro-F 1
score, and Exact Match Ratio (EMR). These metrics are chosen to reflect
performance at both instance and moral label levels. For more granular
evaluation per label F1, Precision and Recall is computed for in-domain and
cross-domain scenarios. Additionally, we use bootstrapping (n = 1000) to
compute 95% confidence intervals (CI) for all performance metrics, which
provides robust estimates of variability and statistical reliability. This is
particularly important in multi-label moral classification, where label sparsity
and domain shifts can introduce high variance in model performance.

To assess model fairness, we also compute per label Demographic Parity
Difference (DP)? and per label Equalised Odds Difference (EO)* as fairness
metrics. In our setup, the sensitive attribute is the platform origin: Reddit
and Twitter, which are treated as a binary group attribute. These metrics are
computed using the output logits and prediction files from the cross-domain
experiments. This setup allows us to measure fairness violations across
domains, ensuring that a models moral label predictions do not systematically
differ based on the source domain alone. Similarly, we use bootstrapping to
compute 95% confidence interval for robust analysis, where n = 1000.

Novel Metric Exploration and Validation

To evaluate the validity of the proposed new fairness metric in the context
of the transformer models for moral sentiment classification, examining the
empirical association with existing fairness and performance metrics for
cross-domain fine-tuning scenarios. This paper will employ the Spearman
correlation coefficient [34], which is an appropriate choice given the non-linear
nature of the moral foundations relationship in the context of deep learning.

3https://fairlearn.org/main/api_reference/generated/fairlearn.metrics.
demographic_parity_difference.html

‘nttps://fairlearn.org/main/api_reference/generated/fairlearn.metrics.
equalized_odds_difference.html

13


4. Results

This section presents and highlights the results obtained from the con-
ducted experiments of fine-tuning DistilIBERT and BERT models for the
multi-label classification of moral foundation labels. The experiments can
be divided into two categories: in-domain and cross-domain. For in-domain
scenarios, a model is fine-tuned on one social media domain and tested on
the same domain, for example fine-tuned on Reddit and tested on Reddit for
multi-label classification. This results in four scenarios of experiments for
in-domain: two on DistilIBERT and two for full BERT models. Similarly,
cross-domain experiments maintain the same logic. The model is fine-tuned
on one social media domain and tested on different social media data for
multi-label classification task. Therefore, the cross-domain approach results
in four experiment scenarios. For the traditional performance metrics this
section utilizes Binary Cross Entropy with Logits as loss function, micro-F 1
score and Exact Match Ratio (EMR). Furthermore, the per label performance
results are displayed against the label sparsity and F1 score as seen in Figure
A.16 of Appendix A.

4.1. In-domain Results

The in-domain results can be observed from Table la and Figure A.15.
From the results of the experiments several key observations can be seen.
Transformer models that are trained on MFTC outperformed those models
that were trained on MFRC dataset. For instance, the lighter and faster
DistilBERT model of in-domain scenario achieved 0.772 micro-F1 score and
Exact Match Ratio of 0.742, compared to the micro-F1 score of 0.687 and
EMR of 0.645 for the DistilIBERT fine-tuned on MFRC dataset. The loss
of models that used MFTC is also noticeably lower than the models that
used MFRC for fine-tuning, where loss 0.372 for MFRC and loss of 0.286 for
the MFTC scenarios where DistilIBERT was used. Similar trend also shows
for the BERT in-domain scenarios, where the MFTC trained models have
micro-F1 of 0.768 and for MFRC trained model the micro-F 1 score is 0.685.
The MFRC trained BERT model shows higher loss of 0.379 and lower EMR
of 0.640, with comparison to the MFTC trained BERT model gaining lower
loss of 0.291 and higher EMR of 0.737, showing better classification results.

4.2. Cross-domain Results
The results of the cross-domain experiments can be seen from Table 1b and
Figure A.15, where the models were fine-tuned on one social media data set

14


Table 1: Overall Model Performance Results In-domain vs. Cross-domain

(a) In-domain experiments

Scenario Loss Micro-F1 Accuracy/EMR
DistiIBERT MFRC + MFRC 0.372 0.687 0.645
DistiBERT MFTC + MFTC 0.286 0.772 0.742
BERT MFRC —+ MFRC 0.379 0.685 0.640
BERT MFTC — MFTC 0.291 0.768 0.737

(b) Cross-domain experiments

Scenario Loss Micro-F1 Accuracy/EMR
DistiBERT MFRC > MFTC 0.421 0.672 0.650
DistiIBERT MFTC — MFRC __ 0.688 0.623 0.643
BERT MFRC > MFTC 0.416 0.673 0.649
BERT MFTC > MFRC 0.648 0.624 0.643

and tested on different social media data. As expected, the cross-domain exper-
iments revealed slightly different performance metrics. Applying the models
to a different domain decreased the overall performance. This performance
degradation can be highly visible from the increasing loss metric for all of the
scenarios and reduced micro-F 1 score. For instance, DistilBERT fine-tuned on
MFTC and tested on MFRC shows significant drop in micro-F1 score, going
from 0.772 for in-domain settings to 0.623 for cross-domain scenarios. Similar
pattern can be observed from the other experiment scenarios, for example
the BERT fine-tuned on MFTC, the performance in terms of micro-F1 have
dropped from 0.768 to 0.624. This performance of morality classification
ability drops when the model changes domain, and more interestingly, it was
not a symmetric performance drop. Models trained on the MFTC and tested
on the MFRC have shown bigger drop in performance. For instance, for
DistilBERT the micro-F1 decreased by 0.14, and the EMR dropped by 0.09
units. On contrast, models fine-tuned on MFRC and evaluated on MFTC
data did not show massive loss in performance for the micro-F1 score, only
decreasing by 0.015 units for the cross-domain experiment using DistilBERT.

Interestingly, the EMR for the MFRC — MFTC models have improved
slightly compared to the in-domain results. For DistilBERT, the EMR

15


Table 2: Per-label F; (95% CI) for In-domain vs. Cross-domain

(a) In-domain experiments

DistilBERT BERT
Label MFRC-—> WHRKO— WHRO— WERICC— MEF TC

authority 0.38 (0.34-0.45 (0.41-0.38 (0.34-0.45 (0.40—
0.42) 0.48 0.42 0.48
care 0.56 (0.53-0.58 (0.56-0.57 (0.54-0.58 (0.55—
0.59) 0.61 0.59 0.61
fairness 0.50 (0.46-0.64 (0.62-0.51 (0.48-0.64 (0.61—
0.53) 0.67 0.54 0.66
loyalty 0.41 (0.35-0.57 (0.54-0.39 (0.34-0.56 (0.53—
0.47) 0.59 0.45 0.58
non-moral 0.82 (0.81-0.88 (0.87-0.82 (0.81-0.88 (0.87—
0.82) 0.89 0.83 0.88

(b) Cross-domain experiments

DistilBERT BERT
Label MFRC-—> WHMIICO— WHRKO— WEFT > MF RC

authority 0.30 (0.28-0.03 (0.03-0.31 (0.30-0.04 (0.03—
0.31) 0.04 0.32 0.04
care 0.42 (0.41-0.08 (0.07-0.46 (0.45-0.09 (0.08-—
0.43) 0.09 0.47 0.10
fairness 0.34 (0.33-0.10 (0.09-0.36 (0.35-0.12 (0.11-—
0.35) 0.11 0.37 0.13
loyalty 0.40 (0.39-0.06 (0.05-0.38 (0.37-0.07 (0.06—
0.41) 0.08 0.39 0.08
non-moral 0.81 (0.80-0.79 (0.79-0.81 (0.80-0.79 (0.79-
0.81) 0.79 0.81 0.79

increased from 0.645 (MFRC — MFRC) to 0.650 (MFRC > MFTC). A
similar pattern was observed for BERT, with EMR increasing from 0.640 to
0.649. This suggests that while individual label predictions were slightly less
accurate on average, the models more frequently predicted the exact complete
set of labels correctly when transferring from MFRC to MFTC compared to
their performance within the MFRC domain.

Table 2, Table 3 and Table 4 presents the F;, Recall and Precision scores
for each moral label with confidence interval (CI) of 95% with bootstrap of
1000 samples, to get more granular results per label and for cross-domain and
in-domain result interpretations.

The label non-moral row is inspected first, as it is the majority label for
both of the social media domains: both models achieved F; > 0.80 when
they were evaluated in-domain and their scores fell by less than 0.02 after

16


Table 3: Per-label Recall (95% CI) for In-domain vs. Cross-domain

(a) In-domain experiments

DistilBERT BERT
Label MFRC-—> WHRKO— WHRO— WERICC— MEF TC

authority 0.34 (0.30-0.38 (0.34-0.37 (0.32-0.38 (0.34—
0.38) 0.42 0.41 0.42
care 0.53 (0.50-0.54 (0.51-0.56 (0.53-0.54 (0.51—
0.57) 0.58 0.60 0.58
fairness 0.46 (0.42-0.62 (0.59-0.48 (0.44-0.63 (0.59—
0.49) 0.65 0.52 0.66
loyalty 0.34 (0.28-0.53 (0.50-0.34 (0.28-0.54 (0.51—
0.40) 0.57 0.40 0.57
non-moral 0.80 (0.79-0.87 (0.86-0.80 (0.78-0.87 (0.86—
0.81) 0.88 0.81 0.88

(b) Cross-domain experiments

DistilBERT BERT
Label MFRC-—> WHMIICO— WHRKO— WEFT > MF RC

authority 0.21 (0.20-9.01 (0.01-0.23 (0.22-0.01 (0.01-
0.22) 0.02 0.24 0.02
care 0.38 (0.37-0.04 (0.03-0.45 (0.44-0.04 (0.04—
0.39) 0.04 0.46 0.05
fairness —_0.27 (0.26-0.05 (0.05-0.30 (0.29-0.06 (0.06—
0.28) 0.06 0.31 0.07
loyalty 0.29 (0.28-0.03 (0.02-0.28 (0.27-0.03 (0.03-
0.30) 0.04 0.29 0.04
non-moral 0.84 (0.84-0.98 (0.98-0.84 (0.83-0.98 (0.98-
0.85) 0.98 0.84 0.98

the domain switch. The result is expected because ’non-moral’ instances
constitute the majority label in both Reddit and Twitter; the models therefore
encountered thousands of neutral examples during training and distinguish
‘non-moral’ instances with good accuracy for in-domain and cross-domain
scenarios. Stable recall score for cross and in-domain and slight drop in
precision, indicate that models are still finding almost all of the true positives,
but making more mislabeling in the cross-domain cases.

For the loyalty label, scoring approximately 0.57 for in-domain Twitter
case and 0.41 for in-domain Reddit case. However, the F; score drops severely
to 0.06 for cross-domain scenarios, where the MF TC fine-tuned model gets
evaluated on MFRC dataset. This drop is similar for DistiIBERT and BERT
models. On contrast, the DistiIBERT and BERT fine-tuned on MFRC did
not experience such drastic F; score drop for the loyalty label. Reddits

17


Table 4: Per-label Precision (95% CI) for In-domain vs. Cross-domain

(a) In-domain experiments

DistilBERT BERT
Label MFRC-—> WHRKO— WHRO— WERICC— MEF TC

authority 0.42 (0.37-0.53 (0.49-0.39 (0.35-0.52 (0.47—
0.46) 0.58 0.43 0.56
care 0.57 (0.54-0.62 (0.59-0.56 (0.53-0.62 (0.59-—
0.60) 0.65 0.59 0.65
fairness 0.54 (0.50-0.65 (0.63-0.53 (0.50-0.64 (0.61—
0.57) 0.68 0.56 0.67
loyalty 0.50 (0.44-0.59 (0.56-0.45 (0.39-0.56 (0.53—
0.56) 0.62 0.52 0.59
non-moral 0.82 (0.81-0.88 (0.87-0.83 (0.82-0.87 (0.87—
0.83) 0.88 0.84 0.88

(b) Cross-domain experiments

DistilBERT BERT
Label MFRC-—> WHMIICO— WHRKO— WEFT > MF RC

authority 0.45 (0.44-0.43 (0.36-0.44 (0.42-0.46 (0.39—
0.47) 0.50 0.45 0.54
care 0.45 (0.44-0.79 (0.75-0.46 (0.45-0.77 (0.73—
0.46) 0.83 0.47 0.81
fairness 0.44 (0.43-0.55 (0.51-0.43 (0.42-0.54 (0.51—
0.45) 0.58 0.44 0.57
loyalty 0.62 (0.61-0.50 (0.42-0.60 (0.58-0.45 (0.39-—
0.63) 0.58 0.61 0.53
non-moral 0.76 (0.76-0.65 (0.65-0.77 (0.77-0.65 (0.65—
0.77) 0.65 0.77 0.66

longer and richer language might have trained the model to retain much more
clear distinction for the loyalty label. When the model is trained on Reddit
and tested on Twitter precision actually rises to 0.62, but recall goes down to
0.28 for DistilBERT and BERT approximately the same. Learned attributes
for the label loyalty from Reddit generalise only partially to Twitter, thus
the model keeps spotting many true positives but over predicts instances that
resembles false positives on Twitter.

Similar observation can be seen for fairness moral label. In-domain
DistilBERT and BERT models for MFRC show Fy score of 0.50 and 0.51,
with confidence intervals of 0.46—0.53 and 0.48-0.54, respectively. However,
for cross-domain cases the scores drop to 0.34 (0.33-0.35) for DistilBERT and
0.36 (0.35-0.37) for BERT, confirming that fairness cues do not transfer
well across domains. When the MFRC models are evaluated on Twitter the

18


precision is 0.44 while recall goes to 0.29, indicating that many Reddit fairness
cues are still detected but mislabeled in the noisier Twitter domain. The
inverse illustrates a similar pattern as previous labels, where the Twitter
fine-tuned models evaluated on Reddit retain precision around 0.55, but the
recall drops below 0.06.

The in-domain cases reveal steady performance for the care label. Both
DistilIBERT and BERT models perform similarly with F; scores steadily
showing 0.56 and 0.58, with slight overlapping in confidence intervals. In
contrast, the cross-domain cases show similar asymmetry as other labels in
transfer performance. For models fine-tuned on Reddit and evaluated on
Twitter, the F, score falls from 0.56 to 0.42 (CI 0.41-0.43) for DistiIBERT and
from 0.57 to 0.46 (0.45-0.47) for BERT. When compared to the confidence
intervals of the in-domain cases, the intervals no longer overlap which indicate
a true loss of performance. The models fine-tuned on the MFTC and evaluated
on Reddit, illustrate the same F score loss going from 0.58 to 0.08 (CI 0.07-
0.09) for DistiIBERT and to 0.09 (CI 0.08-0.10) for BERT models. For
in-domain cases, the precision and recall is consistent for both of the BERT
and DistiIBERT models. For MFRC — MFTC scenarios, the recall slightly
drops to 0.38 and 0.45 for DistilBERT and BERT models. However, when
reversed, the recall goes down dramatically to 0.04, meaning that the Twitter
fine-tuned models mislabel almost every statement that might contain cues
for care moral foundation.

Lastly, the authority label shows the same pattern as the other labels.
Where the in-domain F, scores sit at 0.38 (0.34-0.42) on Reddit fine-tuned
models, and 0.45 (0.41-0.48) on Twitter fine-tuned models. When the MFRC
models are tested on MFTC, F, score drops to 0.30 and 0.31 for DistilBERT
and BERT, and the confidence intervals (0.28—0.32) remain well below the
in-domain Twitter interval, signaling a consistent degradation. For in-domain
cases, the precision is moderate 0.42 and 0.53 for DistilIBERT and approx-
imately similar for the BERT model, although in the latter model Reddit
model is the worst performed in precision. For cross-domain cases, the Red-
dit fine-tuned DistiIBERT model maintains the precision at 0.44, but the
recall goes down to 0.21, confirming false negatives that shows degradation in
authority label classification. For the Twitter fine-tuned models precision
got slight drop, but the recall shrinks almost to 0.01-0.02, showing severe
mislabeling and no cues for detecting authority moral label outside the
training domain.

19


4.8. Fairness Analysis

From the previous results, we conclude that in multi-label moral foundation
classification, the reliance on a single aggregate metric proves insufficient,
as it inherently masks critical biases that exhibit uneven distribution across
distinct moral dimensions, therefore for the fairness metrics computing per
label metrics will reveal granular results that will be useful for developing
a novel metric that account morality foundations. The two social media
platforms serve as our sensitive group attributes for the fairness metrics, due
to our cross-domain experiments. This research utilizes Fairlearn, an open
source library, due to its well tested [28], standard ways to measure fairness
and giving overall and group specific rates or error gaps.

In our multi-label moral label classification setting, aggregate parity
metrics conceal label specific imbalances. Each moral foundation has distinct
normative weight and frequency on Twitter versus Reddit. For example,
loyalty may be rare on Twitter but central in certain subreddits. To highlight
these differences, we compute per label Demographic Parity Difference and
Equalized Odds Difference for each moral dimension m. This metric captures
the models ability to correctly detect the True Positive Rate (TRP) and
minimising False Positives Rate (FPR) for each moral foundation is equally
reliable across domains.

AVY = P(jm = 1| A= Twitter) — P(Gm =1| A = Reddit), (3)
AM) — TPR™, — TPR” FPR, — FPR” 4
EO max{ | Twitter Reddit |> | Twitter ar ( )

where for each domain d € {Twitter, Reddit}:

TPRY” = P(Gm=1|¥%m=1, A=a), (5)
FPR&”) = P(§m=1| Ym =0, A=d). (6)

Table 5 presents the fairness metrics for both DistiIBERT and BERT
models across all moral foundation labels. The results revealed significant
cross-domain disparities, with authority showing the largest demographic
parity differences for both models DistiIBERT 0.22 (0.22-0.23) and BERT
0.23 (0.22-0.23). Similarly, authority exhibited the highest equalized odds

20


Table 5: Cross-Domain Fairness Metrics by Model and Label

Model Label A Demographic Parity A Equalised Odds
authority 0.22 (0.22-0.23 0.40 (0.39-0.41
care 0.04 (0.04-0.05 0.26 (0.25-0.28

DistilBERT fairness 0.05 (0.05-0.05 0.22 (0.21-0.23
loyalty 0.03 (0.03-0.03 0.20 (0.19-0.21
non-moral 0.08 (0.08-0.08) 0.34 (0.33-0.36
authority 0.23 (0.22-0.23 0.41 (0.40-0.42
care 0.04 (0.04-0.04 0.24 (0.23-0.26

BERT fairness 0.05 (0.05-0.06 0.24 (0.23-0.25
loyalty 0.04 (0.04-0.04 0.22 (0.21-0.23
non-moral 0.09 (0.09-0.09) 0.41 (0.40-0.42

differences DistilBERT 0.40 (0.39-0.41) and BERT 0.41 (0.40-0.42), indicat-
ing substantial disparities in both prediction rates and error patterns across
Twitter and Reddit platforms. In contrast, loyalty demonstrated the small-
est demographic parity gaps for both models DistilIBERT 0.03 (0.03-0.03)
and BERT 0.04 (0.04-0.04), though its equalized odds differences remained
substantial at 0.20 and 0.22 respectively. These per label findings demon-
strated that moral themes with lower base rates, such as authority, were
disproportionately affected by cross-domain biases despite their significance.
For the label care demographic parity difference is 0.04 (0.04-0.04) for both
DistiIBERT and BERT, with equalized odds differences of 0.25 (0.24-0.26)
and 0.24 (0.23-0.25), indicating a moderate change in how care is detected
on Twitter versus Reddit. Demographic parity difference score for the label
fairness is 0.05 (0.05-0.05) for DistiIBERT and 0.06 (0.05-0.06) for BERT
with equalised odds difference of 0.23 (0.22-0.24) and 0.22 (0.21-0.23). This
suggests some domain bias but less severe than for moral label of authority.

Together, these per label results reveal a clear ordering of cross-domain ro-
bustness, where loyalty is most stable, care and fairness exhibit moderate
bias, and authority alongside non-moral suffer the greatest disparities. This
pattern highlights, that aggregate fairness measures would mask critical, label
specific weaknesses. Particularly for moral dimensions with low occurrences,
but with high moral weight. To ensure fair moral foundation tagging across
social platforms, proposing a fairness metric based on the previous results
would highlight the potentials in this research area.

21


4.4. Novel Fairness Metric Exploration

Existing group fairness metrics are important for revealing cross-domain
gaps in moral foundation classification, but they capture only one axis of
unfairness, for example TRP and FPR. Moreover, multi-label tasks with
skewed distribution for moral dimensions like authority can produce unre-
liable fairness estimates when labels are rare [31]. Therefore, the fairness
metric for the moral sentiment classification, should detect moral foundations
consistently across different social media platforms when the underlying moral
content is equivalent, regardless of platform specific linguistic conventions
or communication norms. In the scope of this research, the platform bias in
moral classification represents a form of algorithmic unfairness for several
critical reasons. But mainly it will amplify systematic discrimination or
unfairness to specific platform. Therefore, this research proposes a fairness
metric which focuses on the consistency for moral foundations. Moral Fairness
Consistency (MFC) provides a more intuitive and explainable approach that
directly measures how consistently models detect moral foundations across
platforms. We define the absolute difference in fairness for each moral label

l € {1,...,5} between cross-domain directions as:
(1 l l
Diff) = Asi ite st ttier ~ AVBiThitter-sRedalt (7)

Then, we define the Moral Fairness Consistency (MFC) score as:

Lo
MFC =1— = 5 — Diff

C ; d (8)
For instance, the label care exhibits high cross-domain consistency, with
MFC scores of 0.9556 for DistilIBERT and 0.9573 for BERT. This implies
that the absolute difference in detection rates between MFRC — MFTC and
MFTC — MERC is approximately 0.044 and 0.043, respectively. In contrast,
the authority label shows the lowest MFC scores across both models, where
0.7781 for DistiIBERT and 0.7727 for BERT indicating a considerably larger
divergence for domain transfers. These values suggest that moral foundation
labels such as loyalty and care are learned and transferred more consistently
across domains, whereas authority suffers from significant domain specific
instability. Therefore, the MFC metric captures the extent to which models
preserve fairness in moral label prediction when exposed to different social
contexts, offering a more interpretable and granular measure than traditional
fairness metrics alone.

22


Table 6: Moral Fairness Consistency (MFC) per-label

Model Label MFC (95% CI)

DistiIBERT authority 0.7781 (0.7741-0.7822)
care 0.9556 (0.9537—-0.9576)
fairness 0.9499 (0.9472-0.9524)
loyalty 0.9666 (0.9647—0.9684)
non-moral 0.9205 (0.9179-0.9234)

BERT authority 0.7727 (0.7681-0.7771)
care 0.9573 (0.9552-0.9593)
fairness 0.9451 (0.9429-0.9478)
loyalty 0.9617 (0.9599-0.9637)
non-moral 0.9076 (0.9047—0.9102)

Table 7: Spearman Correlation between MFC and Other Metrics

Model Metric p p-value
F1 Score -0.0920 0.6998
Precision 0.1226 0.6065
DistiIBERT Recall -0.0981 0.6807

DP Difference -1.0000 0.0000
EO Difference -0.9000 0.0000

F1 Score -0.1042 0.6619
Precision 0.0429 0.8574
BERT Recall -0.1410 0.5532

DP Difference -1.0000 0.0000
EO Difference -0.9000 0.0000

23


Novel Fairness Metric Validation Results

Table 6 presents the MFC scores for each moral label across both models.
The results reveal significant variation in moral fairness consistency across
different moral foundations. The authority label demonstrates the lowest
MFC scores for both DistilBERT (0.7781, CI: 0.7741—0.7822) and BERT
(0.7727, CI: 0.7681-0.7771), indicating substantial inconsistency in cross-
domain predictions for this moral dimension. This aligns with our previous
findings showing authority had the largest demographic parity and equalized
odds differences. In contrast, loyalty achieved the highest MFC scores for
both models (DistilBERT: 0.9666, CI: 0.9647—-0.9684 and BERT: 0.9617, CI:
0.9599-0.9637), suggesting more consistent moral detection across platforms.
The care and fairness labels showed moderate consistency with MFC
scores around 0.94-0.96, while non-moral exhibited intermediate consistency
(DistilBERT: 0.9205 and BERT: 0.9076).

Table 7 presents the Spearman correlation analysis between our proposed
Moral Fairness Consistency metric and established performance and fairness
measures. This analysis serves as a crucial validation step for understanding
how MFC relates to existing evaluation frameworks and whether it captures
distinct dimensions of model behavior in moral classification tasks. The novel
MFC metric exhibits weak associations towards the traditional performance
metrics. For DistiIBERT, MFC correlations with performance metrics were:
F1 Score (p = -0.092, p = 0.700), Precision (p = 0.123, p = 0.607), and
Recall (p = -0.098, p = 0.681). For BERT: F1 Score (p = -0.104, p = 0.662),
Precision (p = 0.043, p = 0.857), and Recall (p = -0.141, p = 0.553). All
performance metric correlations were non-significant (p > 0.05). For the
fairness metrics, both models exhibited identical correlation patterns with
fairness metrics. MFC showed perfect negative correlation with Demographic
Parity Difference (p = -1.000, p < 0.001) and strong negative correlation with
Equalized Odds Difference (p = -0.900, p < 0.001) for both DistilBERT and
BERT.

5. Discussion

In this section, the findings are discussed about the existing literature, the
results are interpreted, and the limitations of the study are examined. Finally,
the contribution of this research to the fields of moral sentiment classification
and fairness evaluation is outlined.

24


5.1. Comparison with the Existing Studies

The findings of this paper align well and extend the previous work in some
key aspects. Performance degradation for the cross-domain experimentation
scenarios have been consistent with the work of Guo et al. [5], where it
highlighted the challenges of using heterogeneous data for learning morality
paradigms. The results for the in-domain MFTC fine-tuned models outperform
the MFRC models in performance metrics such as micro-F1, which aligns with
the work of Hoover et al. [17], where it was demonstrated that Twitter’s text
structure and character limitations can lead to much more focused and denser
morality expressions, thus models can learn better for in-domain scenarios. On
the other hand, asymmetric patterns have been observed for the cross-domain
scenarios, where the Twitter — Reddit models show sharp drop in recall and
F1 score, while the precision remained more consistent. This pattern suggests
that Twitter fine-tuned models learn highly specific moral cues and expressions
which perform worse when put in cross-domain environment, thus failing to
generalize in Reddits diverse and longer context patterns, showing similarity
with the work of Trager et al. [16], proving contextual richness of the Reddit
platform where discussion can be much more dependent on the context of
itself. As shown in Figure A.11 and Figure A.12 (A) the Reddit posts are
significantly longer and more variable in form, while Reddit’s moral labels span
three distinct thematic clusters (US Politics, Everyday Morality, and French
Politics) suggesting Reddit’s more diverse moral discourse. This diversity
may explain the asymmetric transfer patterns observed in MFRC — MFTC
transfers showed minimal performance degradation (0.687 to 0.672 micro-F 1
for DistiIBERT), while MFTC — MFRC transfers exhibited substantial drops
(0.772 to 0.623). The broader contextual training in Reddit discussions may
have provided more generalizable moral representations. This finding indicates
that moral sentiment classification should require significant architectural
and training modifications to be able to handle cross-domain integration.
The per-label fairness analysis had revealed biases that previous studies did
not cover. While MoralBERT [26] reports improved performance through
domain adversarial training, focus was mainly on the overall accuracy metrics.
Therefore, the findings of this paper highlights that for moral foundations
such as authority, the Demographic Parity Differences of 0.22 (0.22-0.23)
and Equalised Odds Difference of 0.40 (0.39-0.41) indicate fairness violations
that standard accuracy metrics fail to highlight. The computed 95% CI
per-label confirms the statistical significance by proving it is not a random
variation. This per-label fairness analysis in the context of the moral sentiment

25


classification has added information where previous researches did not focus.

DistlBERT vs. BERT

An important finding that deserves a special attention is the similar
performance between the DistilIBERT and the full BERT model across the
experiments. The results from Table 1 demonstrate that DistilBERT achieves
performance which is identical to BERT across both in-domain and cross-
domain scenarios. For in-domain settings, DistilIBERT achieved micro-F1
scores of 0.772 (MFTC) and 0.687 (MFRC), compared to BERT’s 0.768 and
0.685 respectively. This minimal difference (0.004 and 0.002) falls well within
statistical noise, confirming that the distillation process preserved the moral
reasoning capabilities of the original model. DistilBERT in our experiments
retained 100.41% of BERT’s micro-F1 performance in-domain and 99.85% in
cross-domain settings, thus aligning with [22] and even surpassing the original
claim of retaining 95% of BERTs performance. The MFC scores further
confirm the pattern. For the most problematic authority label, DistiIBERT
achieved an MFC score of 0.7781 compared to BERT’s 0.7727, which is a
negligible difference. Similarly, for the most consistent loyalty label, the
difference was only 0.0049 (DistilBERT: 0.9666 and BERT: 0.9617). This
consistency across fairness metrics indicates that model compression through
distillation does not introduce additional bias or fairness violations.

The current results extend this validation to the specific domain of moral
sentiment classification and fairness evaluation. For researchers concerned
with both fairness and computational efficiency, DistilBERT represents an
optimal choice that maintains moral reasoning capabilities while reducing
computational costs. Although the DistiIBERT performs comparably to the
full BERT model, both in terms of performance and fairness, the distillation
process may introduce subtle violations in fairness that the novel MFC may
not capture. For example, the smaller architecture with fewer attention heads
might make it harder to capture subtle morality expressions like authority.
While the MFC metric highlights if the model is consistent across domain, but
it does not explain why this shift in fairness happens. Thus, future research
should explore interpretable attention visualization techniques like BERT Viz
[43], to understand how the model assigns weights to different morality inputs.

5.2. Novel Fairness Metric of Moral Fairness Consistency (MFC)

One of the key novelties of this paper is the exploration of the Moral Fair-
ness Consistency (MFC) metric as a novel metric for assessing the transformer-

26


based model fairness in the moral sentiment classification task in cross-domain
environment. We specifically evaluated the proposed metric in cross-domain
scenarios, where models trained on one dataset were tested on a distinct
target domain with differing moral label data distributions, to ensure that
moral foundations are detected consistently across different linguistic and
cultural contexts. The construction of the Moral Fairness Consistency metric
is based on the the Moral Foundation Theory [2], where the it is universal,
yet the moral reasoning is based on the linguistic and cultural differences of
different social media platforms.

First of all, MFC shows weak and statistically non-significant correlations
with traditional performance metrics such as F1 score, Precision and Recall
(Figure A.13). This suggests that MFC score captures dimensions of model
behavior not directly reflected in traditional performance metrics, confirming
prior observations that model accuracy alone is insufficient to characterize
fairness properties under distributional shifts [33].

Secondly, the MFC metric has a perfect negative correlation with Demo-
graphic Parity Difference and a strong negative correlation with the Equalised
Odds Difference. The perfect correlation with the DP Difference indicates
that, within this specific experimental set-up, both metrics react similarly to
domains distributional changes. However, this does not imply redundancy
between the two metrics. Demographic Parity Difference measures outcome
disparities between subgroups within the same domain. In contrast, MFC was
specifically designed to assess how consistent subgroup predictions remain
when the model is exposed to new domains with different linguistic, cultural,
or contextual norms but equivalent underlying moral content.

The per-label MFC scores showcase an order of robustness for the moral
foundations in cross-domain context, which highlight the importance of the
insights that we receive from the MFC metric to achieve fair transformer
models. For instance, the loyalty label achieved the highest consistency for
the both models, where in DistilBERT 0.9666 and BERT 0.9617. In contrast,
the label authority showcase the lowest MFC score where DistilBERT 0.7781
and BERT 0.7727 (Table 6). These results indicate significant inconsistency in
cross-domain moral label classification and indicate that ?authority’ related
morality expressions are more platform specific indicating strong difference in
linguistic and contextual patterns of each social media platform.

27


5.3. Limitations and Future Work

While this research highlights some potential novel ideas several limitations
should be addressed properly. First of all, this research is constrained to the
use of two social media platform datasets (Reddit and Twitter) which are
specifically labeled in accordance to the Moral Foundation Theory. These
two platforms are only a subset of the social media platforms that are being
used today, thus limiting the morality expressions. Additionally, the various
textual preprocessing steps can be evaluated further to handle domain specific
character use, such as emojis and and slangs. This research intentionally kept
the preprocessing of textual input to minimal, trying to keep the natural
contextual richness of the inputs.

Further research in novel fairness metric design or improvements in moral
sentiment analysis should focus on variety of social media platforms. Although,
this is still an ongoing work because creating such MFT datasets are laborious
job which require annotator training with risks of bringing more bias due
to differences in annotators morality distribution. Furthermore, this paper
focuses only on five moral foundation labels to make the distributions of the
moral labels similar to each other for both social media domains (as seen in
Figure A.16). Adding more labels to the fine-tunning would require significant
computing power to preserve the natural morality expression distribution.
Therefore, future work should focus more on preserving the natural variance
in morality expressions without trading performance with computational cost.
Hence, exploring options with multi-task learning frameworks by adopting
a shared encoder and separate output heads for different platforms [46]. In
terms of the the proposed novel metric, the MFC shows strong empirical
validation with perfect negative correlation with the existing fairness metrics
and significant difference with performance measures should be additionally
re-evaluated in different contexts, for example if it could be used as bias
mitigation technique in terms of moral sentiment adjustments and as well as
compared to additional performance and fairness metrics. Beyond the moral
sentiment classification, the MFC metric could be adapted to other fairness
related NLP tasks like hate speech detection and stance detection across
various platforms. Additionally, the morality expressions can be studied
even further with multi-modal settings by utilizing Vision-and-Language
Transformer (ViLT) [45] architecture where textual and image inputs can be
leveraged to assessed morality generalizability. Therefore, previously discussed
results and limitations indicate that successful moral sentiment classification
related tasks may require architectural changes and adaptions strategies that

28


would take into account the domain variance of the inputs. At this moment
scaling such approaches to newer platforms like TikTok or YouTube would
significantly raise the computational demands. The future work might explore
such approaches as adversarial training strategies proposed in [26] and using
the Moral Fairness Consistency metric as post-hoc fairness evaluation tool
that provides interpretable signals of fairness.

6. Conclusion

This research aimed to highlight a critical gap in fairness evaluation for
moral sentiment classification across social media platforms. The experiments
conducted in the scope of this research have shown significant asymmetric
performance drops in cross-domain evaluations. These results highlight the
the existence of the platform specific biases and unfairness transferred to cross-
domain scenarios. This means that traditional aggregate metrics fail to reveal
hidden biases and unfairness of the models in terms of moral label classification
where the model is fine-tuned for multi-label classification of moral sentiments.
Therefore, after computing the aggregate model performance metrics, it
was concluded that using per-label performance metrics will result in a
better understanding of the model behaviors. Thus, as anticipated the per-
label metrics highlight insights that aggregate metrics fail to capture. For
example, the authority label demonstrates the most severe disparities, with
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41, indicating systematic bias in how this moral foundation is detected
across platforms. The majority label of non-moral shows relatively stable
performance, which is understandable given its the most frequent label in
both of the datasets. Therefore, the more granular approach of showcasing
per-label metrics reveal that lower frequency moral foundations are affected
disproportionately for cross-domain applications, even though the initial label
distribution is more or less the same for the two social media platforms.

Furthermore, building on the experiments conducted and their respec-
tive results, this paper has introduced a novel fairness metric called Moral
Foundation Consistency (MFC), which provides more interpretable measure
of fairness in the setting of cross-domain application for moral foundation
classification. The MFC metric have shown strong negative correlation with
the existing fairness measures such as Demographic Parity Difference (p =
-1.000, p < 0.001) while remaining independent from the performance metrics.
This indicates that the new metric is capturing a unique aspect of the model

29


behavior and focusing on the explainability aspects of it. This novel metric
shows the order of the most consistent moral labels, for example loyalty
achieving the highest consistency of 0.96, while authority shows the lowest
consistency of 0.78.

An important finding worth mentioning is that the in-domain and cross-
domain experiments revealed an interesting consistency between the BERT
and DistilBERT performances across all the experimentation scenarios. The
distilled version of the BERT model achieved almost identical performance
as the full BERT model, showing a micro-F1 score of 0.772 versus 0.768
for MFTC and 0.687 versus 0.685 for MFRC. The minimal difference in
performance indicate that the distilled version retained the ability to identify
morality cues, while reducing the training time and computational resources
significantly.

In conclusion, this paper tries to improve the theoretical foundations that
are needed to ensure fair transformer model developments using the Moral
Foundation Theory framework. The research has highlighted specific areas
for future works to build on to advance the knowledge in creating morally
robust and fair models.

Declarations

Data Availability: The datasets analysed in this study are publicly available.
The Moral Foundations Twitter Corpus (MFTC) can be obtained from OSF
at https: //osf.io/k5n7y/, and the Moral Foundations Reddit Corpus (MFRC)
is available on Hugging Face at https: //huggingface.co/datasets /USC-MOLA-
Lab/MFRC.

Consent to Publish declaration: not applicable.

Funding: no external funding was received for this study.

Ethics and Consent to Participate declarations: not applicable.

References

[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N.
Akhtar, N. Barnes, and A. Mian, “A Comprehensive Overview of Large
Language Models,” arXiv:2307.06435, 2024.

[2| J. Graham, J. Haidt, S. Koleva, M. Motyl, R. Iyer, S. P. Wojcik, and P.
H. Ditto, “Moral foundations theory: The pragmatic validity of moral

30


[3]

[4

a

[5]

(6

—

[7

—

[8

—

[9]

[10]

[11]

[12]

[13]

pluralism,” in Advances in Experimental Social Psychology, vol. 47, pp.
55-130, 2013.

A. Rogers, O. Kovaleva, and A. Rumshisky, “A Primer in BERTology:
What we know about how BERT works,” arXiv:2002.12327, 2020.

M. Ali, S. Panda, Q. Shen, M. Wick, and A. Kobren, “Understanding the
Interplay of Scale, Data, and Bias in Language Models: A Case Study
with BERT,” arXiv:2407.21058, 2024.

S. Guo, N. Mokhberian, and K. Lerman, “A Data Fusion Framework for
Multi-Domain Morality Learning,” arXiv:2304.02144, 2023.

P. Schramowski, C. Turan, N. Andersen, C. A. Rothkopf, and K. Kersting,
“Large Pre-trained Language Models Contain Human-like Biases of What
is Right and Wrong to Do,” arXiv:2103.11790, 2022.

D. Ganguli et al., “The Capacity for Moral Self-Correction in Large
Language Models,” arXiv:2302.07459, 2023.

L. Jiang et al., “Can Machines Learn Morality? The Delphi Experiment,”
arXiv:2110.07574, 2022.

R. Bansal, “A Survey on Bias and Fairness in Natural Language Process-
ing,” arXiv:2204.09591, 2022.

S. Chen, Y. Li, S. Lu, H. Van, H. J. W. L. Aerts, G. K. Savova,
and D. S. Bitterman, “Evaluating the ChatGPT family of models for
biomedical reasoning and classification,” Journal of the American Med-
ical Informatics Association, vol. 31, no. 4, pp. 940-948, 2024, doi:
10.1093 /jamia/ocad256.

OpenAI et al., “GPT-4 Technical Report,” arXiv:2303.08774, 2024.

A. Grattafiori et al., “The Llama 3 Herd of Models,” arXiv:2407.21783,
2024.

DeepSeek-AlI et al., “DeepSeek-V3 Technical Report,” arXiv:2412.19437,
2024.

dl


[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22|

[23]

[24]

Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A Robustly Optimized
BERT Pretraining Approach,” arXiv:1907.11692, 2019.

A. Vaswani, “Attention is all you need,” Advances in Neural Information
Processing Systems, 2017.

J. Trager et al., “The moral foundations reddit corpus,” arXiv:2208.05545,
2022.

J. Hoover et al., “Moral Foundations Twitter Corpus: A collection of 35k
tweets annotated for moral sentiment,” 2019, doi: 10.31234/osf.io/w4f72.

M. Sultana, M. Naseer, M. H. Khan, S. Khan, and F. S. Khan, “Self-
distilled vision transformer for domain generalization,” in Proceedings
of the Asian Conference on Computer Vision (ACCV), pp. 3068-3085,
2022.

I. Rahwan et al., “Machine behaviour,” Nature, vol. 568, pp. 477-486,
2019, doi: 10.1038 /s41586-019-1138-y.

L. Zangari, C. M. Greco, D. Picca, and A. Tagarelli, “A Survey on
Moral Foundation Theory and Pre-Trained Language Models: Current
Advances and Challenges,” arXiv:2409.13521, 2024.

Z. Jin, S. Levine, F. Gonzalez, O. Kamal, M. Sap, M. Sachan, R. Mi-
halcea, J. Tenenbaum, and B. Schélkopf, “When to Make Exceptions:
Exploring Language Models as Accounts of Human Moral Judgment,”
arXiv:2210.01478, 2022.

V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled
version of BERT: smaller, faster, cheaper and lighter,” arXiv:1910.01108,
2020.

P. Czarnowska, Y. Vyas, and K. Shah, “Quantifying Social Biases in
NLP: A Generalization and Empirical Comparison of Extrinsic Fairness
Metrics,” Transactions of the Association for Computational Linguistics,
vol. 9, pp. 1249-1267, 2021, doi: 10.1162/tacl_a_ 00425.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of Deep Bidirectional Transformers for Language Understanding,”
arXiv:1810.04805, 2019.

32


[25]

[26]

[27

[28]

[29]

[30]

[31]

[32

—

[33]

Q. Tan, R. He, L. Bing, and H. T. Ng, “Domain Generalization for Text
Classification with Memory-Based Supervised Contrastive Learning,”
in Proceedings of the 29th International Conference on Computational
Linguistics (COLING), Gyeongju, Republic of Korea, pp. 6916-6926,
Oct. 2022.

V. Preniqi, I. Ghinassi, J. Ive, C. Saitis, and K. Kalimeri, “MoralBERT:
A Fine-Tuned Language Model for Capturing Moral Values in Social
Discussions,” in Proceedings of the 2024 International Conference on
Information Technology for Social Good (GoodIT ’24), pp. 433-442, 2024,
doi: 10.1145 /3677525.3678694.

D. Endalie, “Fine-Tuning BERT Models for Multiclass Amharic News
Document Categorization,” Complexity, vol. 2025, no. 1, p. 1884264,
2025.

S. Bird, M. Dudik, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H.
Wallach, and K. Walker, “Fairlearn: A toolkit for assessing and improving
fairness in AI,” Microsoft, Tech. Rep. MSR-TR-2020-32, 2020.

A. Carriero, K. Luijken, A. de Hond, K. G. M. Moons, B. van Calster, and
M. van Smeden, “The harms of class imbalance corrections for machine
learning based prediction models: a simulation study,” arXiv:2404.19494,
2024.

L. Wang, W. Zhao, Z. Wei, and J. Liu, “SimKGC: Simple Contrastive
Knowledge Graph Completion with Pre-trained Language Models,” in
Proceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Long Papers), Dublin, Ireland, pp. 4281-4294, May
2022, doi: 10.18653/v1/2022.acl-long.295.

Y. Huang, B. Giledereli, A. Kéksal, A. Ozgiir, and E. Ozkirimli, “Balanc-
ing Methods for Multi-label Text Classification with Long-Tailed Class
Distribution,” arXiv:2109.04712, 2021.

H. Tiwari, “Advancing Vulnerability Classification with BERT: A Multi-
Objective Learning Model,” arXiv:2503.20831, 2025.

S. Barocas, M. Hardt, and A. Narayanan, “Fairness and machine learning:
Limitations and opportunities,’ MIT Press, 2023.

33


[34] D. J. Sheskin, “Handbook of parametric and nonparametric statistical

[35]

[36]

[37]

[38]

[39]

[40]

[41

[42|

[43]

procedures,” Chapman and Hall/CRC, 2003.

L. Zangari, C. M. Greco, D. Picca, and A. Tagarelli, “ME?-BERT: Are
Events and Emotions what you need for Moral Foundation Prediction?,”
in Proceedings of the 31st International Conference on Computational

Linguistics (COLING), Abu Dhabi, UAE, pp. 9516-9532, Jan. 2025.

Z. Khan et al., “Natural Language Processing Techniques for Automated
Content Moderation,” International Journal of Web of Multidisciplinary
Studies, vol. 2, no. 2, pp. 21-27, 2025.

Kk. Vida, J. Simon, and A. Lauscher, “Values, Ethics, Morals? On the
Use of Moral Concepts in NLP Research,” arXiv:2310.13915, 2023.

J. Kiesel, M. Alshomary, N. Handke, X. Cai, H. Wachsmuth, and B.
Stein, “Identifying the Human Values behind Arguments,” in Proceed-
ings of the 60th Annual Meeting of the Association for Computational
Linguistics (Long Papers), Dublin, Ireland, pp. 4459-4471, May 2022,
doi: 10.18653/v1/2022.acl-long.306.

R. Rzepka and K. Araki, “Polarization of consequence expressions for an
automatic ethical judgment based on moral stages theory,” in Proceedings,
2012.

M. I. Radaideh, O. H. Kwon, and M. I. Radaideh, “Fairness and social
bias quantification in Large Language Models for sentiment analysis,”
Knowledge-Based Systems, p. 113569, 2025.

S. Uddin, H. Lu, A. Rahman, and J. Gao, “A novel approach for assessing
fairness in deployed machine learning algorithms,” Scientific Reports, vol.
14, no. 1, p. 17753, 2024.

J. Park, E. Liscio, and P. K. Murukannaiah, “Morality is Non-Binary:
Building a Pluralist Moral Sentence Embedding Space using Contrastive
Learning,” arXiv:2401.17228, 2024.

J. Vig, “A Multiscale Visualization of Attention in the Transformer
Model,” arXiv:1906.05714, 2019.

34


[44] S. Ramakrishnan and L. D. Dhinesh Babu, “Improving Multi-Label
Emotion Classification on Imbalanced Social Media Data With BERT
and Clipped Asymmetric Loss,” IEEE Access, vol. 13, pp. 60589-60601,
2025, doi: 10.1109/ACCESS.2025.3557091.

[45] W. Kim, B. Son, and I. Kim, “ViLT: Vision-and-Language Transformer
Without Convolution or Region Supervision,” arXiv:2102.03334, 2021.

[46] S. Chen, Y. Zhang, and Q. Yang, “Multi-Task Learning in Natural
Language Processing: An Overview,’ ACM Computing Surveys, vol. 56,
no. 12, art. 295, 2024, doi: 10.1145/3663363.

35


A. Additional Visualisations

Novel Metric Design, Analysis and Experiments

canara Fine-tunning 5
Use transtormiers (biaty, DistilBERT and BERT | Taling and Testing Model Behavior Analysis:

forautomaticembedding | Generalization and Moral

after minimal text domain experiments Shifcon itr veakinoes
leaning and report performance

Preprocessing Data

Novel Fairness Metric
Design

Comparison of novel and
benchmark fairness
metrics

‘MERC and MFTC
with vice/virtue
labels

Harmonizing Moral
Labels for Classification

Result interpretation
and Analysis,

Spearman Correlation
Test with Other Metrics

Figure A.2: Methodology Pipeline

MEDEA E
Pree
Py SP

Figure A.4: Original Moral Label Distribution of MFTC

36


Distribution of indivi Moral Foundations

Em

| fT PTtirtt tLe
4% f£ 4 4° 4 f eg * #

Figure A.5: Individual Moral Label Distribution of MFTC

- 2 # s

Figure A.6: Individual Moral Label Distribution of MFRC

text corpus tweet_id — annotator annotation
0 @fergusonoctober @FOX2now #AllLivesMatter Peac.... ALM  §21033092132503552 annotator00 care
1 @fergusonoctober @FOX2now #AllLivesMatter Peac... ALM  §21033092132503552 annotator01 care,purity
2 @fergusonoctober @FOX2now #AliLivesMatter Peac... ALM  521033092192503552 _annotator02 care,purity
3. @fergusonoctober @FOX2now #AllLivesMatter Peac.... ALM  §21033092132503552_annotator03 care
4 Wholeheartedly support these protests & acts 0... ALM  37681598980475841 _annotator00 subversion
128449 AT_USER Price gouging looting and rage Sandy c..._ Sandy 265598221462687744 annotator09._subversion,cheating
128450 _AT_USER Price gouging looting and rage Sandy c.... Sandy 265598221462687744 annotator11 cheating
128451 Might devastated Sandy victims lose the oppurt... Sandy 265600333068238849 annotator10 cheating
128452 Might devastated Sandy victims lose the oppurt... Sandy 265600333068238649 annotator09 cheating.harm
128453 Might devastated Sandy victims lose the oppurt.. Sandy 265600333068238849 _annotator11 cheating,harm

(a) MFTC Dataset Overview

text —_subreddit, bucket annotator annotation confidence
0 That particular part of the debate is especial... ‘europe French politics annotator03__Non-Moral Confident
1 That particular part of the debate is especial europe French poitics annotator01 Purity Confident
2 That particular part of the debate is especial europe French poitics annotator02_ Thin Morality Confident
3 /rffrance is pretty lively, with it’s own tng. europe French poitics annotator03__Non-Moral Confident
4 Irffrance \s pretty vely, with i's own ting, europe French politics annotator00__Non-Moral_ Somewhat Confident
161221 Well can discern from your vehemence toward... AmitheAsshole Everyday Morality annotatos Equality Confident
61222 Kick! Punchi t's allin the mind. if you wann... nostalgia Everyday Morality annotator05 Thin Morality Somewhat Confident
61223 Reddit can't help you this is some seriously t... confession Everyday Morality annotator05 Thin Morality Confident
61226 Yes. Disordered eating is insidious. And Rita... AmitheAsshole Everyday Morality ennotaior05 Non-Moral Somewhat Confident
61225 What parent would let a kid bring a Gameboy to... nostalgia Everyday Morality annctator05 Authority Somewhat Confident

(b) MFRC Dataset Overview

Figure A.7: Dataset overviews: (a) MFTC and (b) MFRC.

10

Per-label F1 with 95% Cis

Gm DistiIBERT MFRC-+MFRC ED DistilBERT MFRC->MFTC
Ge DistilBERT MFTC->MMFTC Ei DistilBERT MFTC->MFRC
mm BERT MFRC->MFRC (BERT MFRC-METC
im BERT MFTC-3METC (ams BERT MFTC-+MFRC

care fairness

Figure A.

loyalty

authority

8: Per-label F1 Score

37

non-moral



Lo Per-label Recall with 95% Cls

im DistiIBERT MFRC-+MFRC EE DistilBERT MFRC->MFTC
Gy DIstIIBERT MFTC>MFTC EE DistilBERT MFTC->MFRC
Wim _BERT MFRC->MFRC (a BERT MFRC->MFTC
im BERT MFTC-3METC (amy BERT MFTC-+MFRC

Recall

fairness loyalty authority non-moral

Figure A.9: Per-label Recall

6 Per-label Precision with 95% Cls

Wim DistiIBERT MFRC-+MFRC GH DistilBERT MFRC->MFTC
Ty OISCIIBERT MFTCOMFTC RE DIstIIBERT MFTC->MFRC
lm BERT MFRC->MFRC [ey BERT MFRC->MFTC
Wm BERT MFTC->MFTC iam BERT MFTC-+MFRC

fairness loyalty authority non-moral

Figure A.10: Per-label Precision

Distribution of Word Count

Mean: 34.47
==> Median: 27.00

4000

3000

Count

2000

1000

Word Count

Figure A.11: Overall Word Count Distribution of MFRC

Distribution of Word Count

14000 === Mean: 15.18

=== Median: 15.00

12000

10000

‘Count

Word Count

Figure A.12: Overall Word Count Distribution of MFTC

38



Spearman Correlation: MFC vs Metrics

a- 0.1 -0.092 ~0.0
©
S
a4 0.043 0.12 p 0.2
o
=
ve --0.4
6 S- 0.14 0.098
22
-0.6
&
-0.8
g
-1.0

DistiIBERT

Model

Figure A.13: Heatmap of Spearman correlations between MFC and selected metrics

ta0'$ Per-Label Moral Fairness Consistency (MFC) with 95% Confidence Intervals

Model
jm DistilBERT
m= BERT

0.954
0.904

MFC Score
s
&
&
L

0.80

0.754

0.70 +
non-moral faimess care authority
Moral Foundation Label

Figure A.14: Computed Moral Foundation Consistency with 95% CI

In-domain vs. Cross-domain Micro-F1 by Model

DistiIBERT BERT

HE In-domain mE in-domain
_Cross-domain EE Gross domain

14.9%

Micro-F1 (96)
Ey

MFRC Fine-tuned MFTC Fine-tuned MERC Fine-tuned MFTC Fine-tuned

Figure A.15: Performance Degradation in micro-F 1

39


Fl-Score
os es 98 ©
 v RB & ©@

zd
°

0.8

F1-Score

0.2

0.0

DistiIBERT | MFRC+MFRC

ae

BERT | MFRC>MFRC

“we

40 60 80
Label Sparsity (%)

Impact of Label Sparsity on F1-Score Across Models and Domains

DistiIBERT | MFTC-+MFTC DistiIBERT | MFRC>MFTC DistiIBERT | MFTC*>MFRC
* *
2
®
* *
et
mee
BERT | MFTC>MFTC BERT | MFRC+MFTC BERT | MFTC+MFRC
* *
a
%
4 °
®
a
"e .¢
40 60 80 40 60 80 40 60 80
Label Sparsity (%) Label Sparsity (%) Label Sparsity (%)

Figure A.16: Analysis of Label Sparsity and F1-Score

40

erone

Label
care
fairness
loyalty
authority
non-moral
