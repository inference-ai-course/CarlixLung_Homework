arXiv:2109.02941v1 [cs.CL] 7 Sep 2021

Countering Online Hate Speech: An NLP Perspective

Mudit Chaudhary, Chandni Saxena, Helen Meng
The Chinese University of Hong Kong
muditchaudhary@cuhk.edu.hk, csaxena@cse.cuhk.edu.hk
hmmeng@se.cuhk.edu.hk

Abstract

Online hate speech has caught everyone’s at-
tention from the news related to the COVID-
19 pandemic, US elections, and worldwide
protests. Online toxicity — an umbrella term
for online hateful behavior, manifests itself in
forms such as online hate speech. Hate speech
is a deliberate attack directed towards an in-
dividual or a group motivated by the targeted
entity’s identity or opinions. The rising mass
communication through social media further
exacerbates the harmful consequences of on-
line hate speech. While there has been signifi-
cant research on hate-speech identification us-
ing Natural Language Processing (NLP), the
work on utilizing NLP for prevention and in-
tervention of online hate speech lacks rela-
tively. This paper presents a holistic concep-
tual framework on hate-speech NLP counter-
ing methods along with a thorough survey on
the current progress of NLP for countering
online hate speech. It classifies the counter-
ing techniques based on their time of action,
and identifies potential future research areas
on this topic.

1 Introduction

Internet is a double-edged sword. While it of-
fers many benefits, it has also become a breed-
ing ground for radicalism, violent ideas and hate
speech. Various studies have been conducted to
analyze extent of online hate speech. A cross-
sectional study by (Keipi et al., 2016) showed that
42% of 15 to 30 year-olds from United Kingdom,
United States, Finland, and Germany experienced
hateful materials on social media.

Online hate speech took center stage in 2020
with the COVID-19 pandemic, worldwide protests,
anti-Asian rhetoric, race-related violence, and the
US presidential elections. Hate speech has many
negative effects for citizens and creates societal ten-
sions (Ziems et al., 2020; Anti-Defamation League,

2016). These negative effects were exacerbated by
social isolation (Silva et al., 2020). A rise in hate-
crimes and online hate speech was also reported
during the 2020 US elections!?. In the first quarter
of 2020, a popular social media platform removed

10 million posts for violating its hate speech pol-
3

icy”.

(Salminen et al., 2020b) define online toxicity as
‘hateful communication that is likely to cause an
individual user leave a discussion.’ Online toxicity
is a broader phenomenon that can manifest itself
in various forms such as hate speech, cyberbully-
ing, trolling, abuse, etc. (Salminen et al., 2020a).
In this paper, we focus on hate speech which has
significant similarities with other forms of online
toxicity. The definition of ‘hate speech’ is very sub-
jective and its interpretation is contingent upon the
cultural, economical and racial contexts. The many
definitions of hate speech shows a general lack of
consensus, which makes the task of identifying and
countering it challenging because it invites differ-
ing legal, ethical and socio-technical perspectives
(MacAvaney et al., 2019). (Fortuna and Nunes,
2018) projected the definitions of hate speech from
different sources into four dimensions — (i) hate
speech is to incite violence or hate, (ii) hate speech
is to attack or diminish, (iii) hate speech has spe-
cific targets and (iv) humor has a specific status.
In addition, they observe that the dimension ‘hate
speech has specific target’ and ‘hate speech is to
attack or diminish’ is common among many defini-
tions. Referencing previous work, this paper adopts
a working definition for hate speech as follows — A

'www.sandiegouniontribune.com/news/public-
safety/story/2020- 10-3 1/hate-crimes-surge-presidential-
elections

> www.fortune.com/2020/12/23/linkedin-content-
moderation-hate-speech-misinformation-2020-election-
coronavirus/

3 www.fortune.com/2020/05/12/facebook-removes-10-
million-hate-speech-posts/


negative form of communication that deliberately
attacks an individual or a group motivated by the
targeted entity’s identity, characteristics, or opin-
ions; and aims to humiliate, demotivate, harass, or
incite violence.

Moderating online hate speech, referred to as
‘online content moderation,’ has resulted in wors-
ening mental health of the content moderators
due to graphic content, descriptions and hateful
posts. It has been reported that moderators suffer
from Post-Traumatic Stress Disorder (PTSD) as
a consequence of their work*. Countering online
hate speech through Natural Language Process-
ing (NLP) has emerged as an important field of
research due to its potential of automating the pro-
cess at scale and also reducing the workload and
mental stress on the moderators. NLP has shown
great potential in tasks such as automated text clas-
sification, natural language generation and content
style-transfer (Devlin et al., 2019; Radford et al.,
2019; Shen et al., 2017). Consequently, it is being
used to tackle online hate speech. There has been
development of various benchmarking datasets for
NLP-based online hate speech detection and inter-
vention models (Fortuna et al., 2020; Mathew et al.,
2020; Qian et al., 2019).

Countering hate speech includes detection, pre-
vention and intervention methods to control online
hate speech. A significant amount of research is
being done on using NLP for hate speech detec-
tion (Schmidt and Wiegand, 2017), but there is
a paucity of work on NLP-based prevention and
intervention methods. Hence, in this paper we fo-
cus more on prevention and intervention methods
instead of detection methods. This theme paper
aims to provide comprehensive overview of the
latest techniques inside a holistic conceptual frame-
work to counter online hate speech. We classify the
methods into the proactive and reactive categories
— each has different pros and cons as well as ethical
concerns. We intend for this extensive survey and
the framework to help identify promising directions
for future research.

2 Definitions

This section provides the definitions used in the
proposed conceptual framework.

Roles. We identify three roles that are relevant for
the study:

“https://www.bbc.com/news/technology-52642633

¢ Author (First-person): The author of the on-
line content or interaction.

¢ Moderator (Second-person): The modera-
tor refers to the internet platform’s internal
systems. It can either be a human moderator,
automated moderator or both.

¢ Consumer (Third-person): The consumer
is someone who can see the released content
and is not a moderator. The consumer can
come from the general public or have a con-
nection with the author. The connection with
the author is platform-specific, e.g., Friends
on Facebook, Follower on Instagram.

Countering: Countering broadly refers to ways
that restrict online hate speech. Countering meth-
ods can include:

¢ Detection of released hate speech followed by
intervention methods; and/or

¢ Prevention of unreleased hate speech by poten-
tial hate speech prediction methods, followed
by some intervention methods

Reactive Countering Methods. Reactive meth-
ods work retrospectively by using detection meth-
ods to detect hate speech in previously posted con-
tent followed by some intervention strategy.

¢ Act on

— Content visible to the author(s), modera-
tor(s), and consumer(s); and/or

— Author(s)

¢ Act when

— Content released to consumer(s)

¢ Act to

— Curb hate speech content that has already
been released from causing further harm

Proactive Countering Methods. Proactive
methods are preventative and try to intervene
before the hate speech content reaches the
consumers.

¢ Acton

— Content visible to the author(s) and mod-
erator(s), or no one (content not created);
and/or

— Author(s)


¢ Act when

— Content is not yet released to the con-
sumer(s); and/or

— Hate speech is not yet created, e.g., when
the method predicts that a certain conver-
sation or user will lead to creation of hate
speech in the future

¢ Act to

— prevent potential hate speech content
from reaching the consumers

3 Framework

We propose a conceptual framework for countering
online hate speech as shown in Figure 1. The frame-
work includes the major components pertinent to
online hate speech countering research: causes,
consequences, countering methods, and concerns.

We design and present this framework with three
goals in mind: (1) provide an overview of latest and
major research in all components, (2) encourage the
user to consider the impact of different components
on the overall system, and (3) help identify future
directions of research.

We will later discuss how different components
relate to each other and how it can help identify
future research directions.

3.1 Causes of Hate speech

Online hate speech is not just a technological issue
but also a social issue. Searching for the roots of
online hate speech leads to an non-exhaustive list.
In this section, we provide a brief overview of the
major factors that lead to hate speech. We take
into consideration the online disinhibition effect
to understand the innate causes (Suler, 2004), the
work of (Cheng et al., 2017) for situational causes,
and some other studies.

Online disinhibition effect. This effect emerges
when people are online and feel less restraint
regarding their actions, and may act more intensely
or frequently than they would offline, especially
anonymously (Suler, 2004). Two types of
disinhibitions are — benign and toxic; where toxic
disinhibition leads to hateful, deviant or extreme
behaviour online (Suler, 2004). (Lai and Tsai,
2016) also found a positive correlation between the
(Udris, 2014)’s Online Disinhibition Scale (ODS)
and cyberbullying.

Misinformation. (Del Vicario et al., 2016a)
performed a large-scale quantitative analysis of
the diffusion of fake news and conspiracy theories
on Facebook. In their study, they observe that the
users tend to select the content that goes along
with their narrative and ignore other news, leading
to the formation of ‘echo-chambers.’ When the
primary drivers of these echo-chambers is fake
news, it can cause polarization, mistrust, rumors,
and paranoia (Del Vicario et al., 2016a). This
polarization and paranoia often manifests itself
as hate speech when it is targeted towards an
individual or a group. An example of this is the
recent Anti-asian rhetoric and online hate speech
due to COVID-19 (Ziems et al., 2020).

Situational factors. (Cheng et al., 2017)
observe certain situational factors that cause online
hate speech: personal mood, discussion context,
and contagiousness of hate speech. Personal
dissatisfaction, bad mood and anger increases
aggression towards others which can lead to
malicious behaviour online (Lee and Kim, 2015;
Cheng et al., 2017). According to their study, the
immediate context of the discussion can mold the
direction of conversation. Moreover, the authors
observe that a single malicious user or post can
lead to multiple users engage in hate speech
proliferation.

3.2 Consequences of Hate Speech

Various studies support the dire consequences
emerging from the prevalence of hate speech. It
not only gives rise to crimes but also far-fetching
psychological problems. In this section, we refine
the harms loosely categorised in (Salminen et al.,
2020a)’s framework. The harms are divided into
three levels: Individual, Collective, and Societal.

3.2.1 Individual Harms

Individual harms refer to the consequences acting
at an individual level. Hate speech leads to several
negative psychological effects on individuals such
as depression, anxiety, drug abuse, insomnia, and
in some extreme cases incite suicidal thoughts (Lai
and Tsai, 2016; Mitchell et al., 2007; Hinduja and
Patchin, 2010). It can also lead to performance
degradation at work or school, promote self-harm
and retaliatory behaviour (Lai and Tsai, 2016).


Causes

{ Online Disinhibition Effect }

Misinformation }

Situational Factors }

mated

Collective

Societal
+ Hate crime
violence

+ Security,
healthcare and

Individual

put)

+ Radicalisation and
polarisation

+ Lower quality
online community

* Psychological
Issues

+ Performance

degradation + Desensitisation to
+ Retaliatory hate

behaviour + Organized hate

legal concerns
> Division of
society

~~ oS oes ees

tot I
Legal !
__ I
qranval Hate Speech Detectio,, {I > Lack of consensus on the I
t I definition and severity of hate i
tot speech I
—_ toc Borderless nature of online hate \
s = { I speech 1
s i" Who to penalize

SS 7 | I + Protection by the freedom of I
& 3 \ speech I
o tot i
2 Tot \
e en | Ethical \
Oe = tot | i
Ry 4 toe Promotion of human values \

2 \ fie Fair and non-discriminative
a i fl + Transparency and explainability l
= + Privacy I
l W. Safety and security i}
| . Accountability i}
—— t '] - Human control of technology i}
Reactive Methods { 1) - Professional responsibility i
tt I

4

Hate Speech Countering Concerns

mat °
Riess and Improve

Figure 1: Countering online hate speech conceptual framework. Ethical concerns from (Kiritchenko et al., 2020).

3.2.2 Collective Harms

Collective harms are caused in a group of indi-
viduals. On a collective level it can lead to radi-
calization and polarization (Lee and Leets, 2002;
Del Vicario et al., 2016b), lower quality of the
online community (Kumar et al., 2018), and in-
security (Chetty and Alathur, 2018). It can also
cause desensitisation to hateful statements in on-
line users (Soral et al., 2018) and lead to organized
hate-speech against marginalized groups (Peters,
2020).

3.2.3 Societal Harms

Societal harms deal with the harms caused to a col-
lection of societal groups i.e. society. The creation
of increasingly polarized online groups divides the
society due to the collective trauma experienced
by the targeted group during online browsing. In
extreme cases, it can also result in hate crime vio-
lence (The European Commission against Racism
and Intolerance, 2016; Bakalis, 2015) which adds
to security, healthcare and legal concerns.

3.3. Hate speech countering methods

Taking inspiration from the work of (Jurgens et al.,
2019b), we classify the countering methods into
two categories: proactive and reactive. In Sec-
tion 2, we develop definitions of reactive and proac-
tive to develop further from previous works. We
acknowledge that some methods can belong to both
categories and their classification depends on the
manner in which they are applied.

3.3.1 Reactive

As per the definition, reactive methods deal with
hate speech events after they have occured. We

broadly identify three types of reactive strategies in
which Natural Language Processing can help curb
online hate speech: counter-speech generation,
style transfer, and automated and semi-automated
moderation. It is to be noted that reactive methods
work in tandem with hate speech detection
methods. Hence, we will also briefly discuss
selected works of hate speech detection because
they facilitate human intervention.

Counter-speech generation. | Counter-speech
generation is a relatively new strategy to curb
online hate speech. (Garland et al., 2020) define
counter-speech as “a generated response to online
hate in order to stop and prevent the spread of
hate speech, and if possible change perpetrators’
attitudes about their victims.” Counter-speech is
gaining interest of online moderators as it does
not infringe upon people’s freedom of expression,
making it a good alternative to deletion and block-
ing (Gagliardone, 2015; Benesch, 2014; Silverman
et al., 2016). (Wright et al., 2017) explore different
relationship dynamics of counter-speeches on
Twitter and identify the potential of deep learning
in automated classification and generation.

One of the challenges for performing NLP based
counter-speech generation or classification is the
lack of annotated datasets. To resolve this, var-
ious datasets have been recently curated. CO-
NAN is a multilingual dataset of counter-speech
(Chung et al., 2019). It consists of 4,078 hate
speech/counter-speech pairs with the following
annotations — presentation of facts, pointing out
hypocrisy or contradiction, warning of conse-
quences, affiliation, positive tone, negative tone, hu-


mor, counter-questions, and other. It also adds sub-
topic annotations for the pairs, e.g., culture, crimes,
terrorism, etc. (Qian et al., 2019) released two
benchmark datasets. They collected hate speech
conversations from Gab and Reddit. The counter-
speech to the hate speech content was then created
by Amazon Mechanical Turk workers. (Tekiroglu
et al., 2020) focused on providing a framework
for generating quality data for counter-speech gen-
eration. Moreover, they use the GPT-2 model to
generate counter-speech silver data followed by
filtering and expert validation/editing.

If counter-speech can be detected as a response
to hate speech, the platforms can increase their
visibility to other consumers so as to change the on-
going hateful narrative. (Mathew et al., 2019) pro-
posed an ensemble method (SVM with XGBoost
+ TF-IDF + Bag-of-Words) achieving 0.715 F1-
Score on their own curated counter-speech dataset
from YouTube comments. Another ensemble based
approach by (Garland et al., 2020) achieved 0.76-
0.97 accuracy on their dataset consisting of tweets.

Due to the availability of new counter-speech
datasets and crowdsourcing resources, work on
automated counter-speech generation has shown
a significant uptick. (Pranesh et al., 2020) used
the dataset from (Qian et al., 2019) to train
BART(Lewis et al., 2020), BERT (Devlin et al.,
2019) and DialoGPT (Zhang et al., 2020) models
for an automated counter-speech generation. Pari-
tyBOT takes a slightly different approach by using
curated counter-speeches on Twitter (Cuthbertson
et al., 2019). ParityBOT automatically counters
abusive tweets targeted towards women in politics
by sending curated counter-speeches that support
female leaders.

Neural Style Transfer. Neural style trans-
fer is another alternative to traditional moderation
strategies. Using neural style transfer, we can
automatically neutralize parts of hate-speech
content to make it more polite without impinging
upon one’s freedom of speech. (Carton et al.,
2018) use extractive adversarial networks to
show parts of speech that are hateful. Using the
extracted suspicious parts from the extractive
adversarial network and other methods to extract
suspicious parts (Pavlopoulos et al., 2017a; Svec
et al., 2018; Noever, 2018), we can use style
transfer (Prabhumoye et al., 2018; Yang et al.,
2019b; Nogueira dos Santos et al., 2018; Sennrich

et al., 2016) to either modify them to their more
acceptable counterparts or redact them.

Automated and Semi-automated modera-
tion. Online hate speech moderation by human
moderators is one of the traditional methods of
content moderation. The human moderators assess
the flagged content and decide the action — Delete,
Suspend or Ignore. Given the scale of social media,
it becomes impossible for a limited number of
human moderators to moderate all the flagged
comments. According to a study conducted on
63M Wikipedia talk page comments, only an
estimated 17.9% of the attacking comments were
moderated within seven days (Wulczyn et al.,
2017). Smaller platforms or non-profit platforms
do not have the resources to employ human
moderators. Furthermore, the mental stress caused
by the exposure of hateful behavior (Iyer and
Barve, 2020) on the moderators calls for automated
or semi-automated moderation.

(Pavlopoulos et al., 2017a) proposed a Recurrent
Neural Network (RNN) operation on word embed-
dings to assist online moderation. They develop
a Classification specific attention mechanism that
improves hate speech detection and also highlights
suspicious words. The highlighted words help the
moderators make their decision more efficiently. In
their semi-automated approach, the comments with
high confidence are automatically regulated while
the ones with low confidence are sent to the mod-
erators to decide. Following the same rationale,
(Svec et al., 2018) developed a two-step method
which consists of a hate speech classifier and sus-
picious word detector. They use RCNN recurrent
cells (Lei et al., 2016b) in their classifier. The suspi-
cious word detector uses a reinforcement learning
based approach (Lei et al., 2016a).

To make the hate speech classification decision
more interpretable for the moderators, (Risch and
Krestel, 2018) created a non-deep learning based
logistic regression model which predicts the proba-
bility of appropriateness for a comment. The model
inputs 3 types of features: (1) Comment features,
(2) User features, and (3) Article features. Accord-
ing to the authors, their implementation can give
the moderators insight on the influence of different
features on the classification decision. Another ap-
proach tries to improve the detection of hate speech
by adding user embeddings or user type embed-
dings to an RNN model (Pavlopoulos et al., 2017b).


The user embeddings are based on the amount of
rejections in the past. The user embeddings add an
additional context to aid the detection model, which
can potentially assist the moderators in making the
decision for low confidence detections.

For automated moderation, in addition to dele-
tion or suspension of the content, the system can
use the alternative strategies mentioned before such
as neural style transfer and counter-speech gener-
ation. The final action of automated moderation
system is a system design concern that can be de-
cided by the platform based on their context.

3.3.2 Proactive

As stated in the definition, proactive methods deal
with hate speech before a hate event occurs so
that the hate speech content is not released to the
consumers. These methods provide a timely note
to the moderator to intervene before it evolves
into severe hate speech. Proactive methods are
prone to ethical concerns as discussed in the later
section of this paper. Based on NLP interventions,
we identify ‘preemptive moderation’ as a broad
class of proactive strategies. It is worth noting that
potential hate speech prediction methods (not to be
confused with “hate speech detection”) act as tools
for assisting proactive methods. In this section, we
also come up with suggestions on using other NLP
interventions that could work with predictive detec-
tion of hate in proactive settings for future research.

Preemptive Moderation. This is a proac-
tive step to respond to hate speech by preventing
individuals’ hate content from expanding in
severity. Recent works have shown that it is
possible to predict if a conversation will turn into
a hate speech in the future (Brassard-Gourdeau
and Khoury, 2020; Karan and Snajder, 2019;
Chang and Danescu-Niculescu-Mizil, 2019; Zhang
et al., 2018). These predictive models can prevent
harm by employing intervention steps early-on.
(Karan and Snajder, 2019) explored potential
hate speech detection and worked on preemptive
moderation. The authors employed a linear SVM
model using TFIDF-weighted unigrams and
bigrams as well as a BiLSTM model using the
word embeddings of Wikipedia conversations
data. (Zhang et al., 2018) introduced a model to
detect early warning signs of toxicity and used
linguistic features for intervention strategies. The
authors investigated a moderation framework
for capturing the relation between politeness

strategies and the conversation’s future trajectory.
Working on the presence and intensity prediction
on Instagram comment threads, (Liu et al., 2018)
predicted hostility presence and hostility intensity
in Instagram comments. They used multiple
features, ranging from n-grams and word vectors
to user activity, trained a logistic regression
model with L2-regularization, and examined the
impact of sentiment on forecasting tasks. The
authors suggested to use the model for preemptive
prediction on prior hostility features and employ
intervention techniques such as comment filtering
and comment controls. (Brassard-Gourdeau and
Khoury, 2020) encoded conversations features
(text features and sentiment features) of the
messages for preemptive prediction of hate speech.
The authors validated the generality of results and
used a model for predicting hate speech on gaming
chat data.

In their positional paper, (Jurgens et al., 2019a)
argue for a need to boost research for developing
proactive strategies for hate speech intervention
that provides assistance to authors, moderators, and
social platforms to curb the hate phenomenon be-
fore it occurs. Based on the same rationale, we sug-
gest some future directions: (1) One can employ
technology to quarantine hate speech in a proac-
tive course of action and further use appropriate
intervention strategy to curb the effect of hate be-
fore it is released to the consumers. Quarantining
in this context means temporarily holding off con-
tent from consumers for review. This framework
also provides a balance between freedom of ex-
pression and relevant censorship. (2) Existing re-
searches show progress in the direction of potential
hate detection in proactive state and use of more
comprehensive features including history data (past
comments) (Chelmis and Yao, 2019), text and tree
structure of past communication (Hessel and Lee,
2019), and behavioral sequence with n-gram fea-
tures (Tshimula et al., 2020). These models can
be used to support proactive approaches and offer
help for countering hate speech.

3.4 Concerns

In this section, we briefly discuss ethical and le-
gal concerns surrounding online hate speech coun-
tering. The ethical concerns can be addressed
by the implementations at system design-level
through socio-technical contemplation and inter-
vention. However, addressing legal concerns often


need a policy-level change making it more chal-
lenging as it precedes a lengthy process requiring
multilateral support.

3.4.1 Ethical

Countering online hate speech brings various ethi-
cal concerns that need to be addressed to develop
fair and reliable countering systems. One of the
most prominent open-ended ethical dilemma is how
to counter hate speech without infringing one’s
freedom of speech. We observe counter-arguments
from both sides. A more authoritarian approach
may argue that blocking hate speech is not con-
cerned with freedom of speech as it is a threat to
public safety, while a more liberal approach may
argue that hate speech should be countered with
more speech instead of censorship, which makes
counter-speech and neural style transfer become
attractive fields of research.

(Kiritchenko et al., 2020) present a detailed and
comprehensive analysis of countering online hate
speech from an ethical perspective. They identify
eight ethics theme that we need to consider while
designing the countering methods. We briefly dis-
cuss these eight themes in form system design ques-
tions:

¢ Does the system promote human values?
E.g., infringing freedom of speech, equality
and dignity of the citizens.

Is the system fair and non-discriminative?
E.g., fixing a system that unfairly treats con-
tent based on author’s demographics or iden-
tity.

Is the system transparent and explainable?
E.g., explaining why a certain post was consid-
ered hateful and why the corresponding action
was taken to curb its influence.

Does the system respect user’s privacy?
E.g., avoiding model training on user’s per-
sonal data without consent or using privacy-
respecting learning methods.

Does the system ensure safety and security
of the users? E.g., considering the conse-
quences of wrong predictions such as ignoring
content that incites offline violence.

Does the system ensure accountability of
its decisions? E.g., auditing system design
and creating appeal procedures.

* Does the system allow human to take con-
trol? E.g., manually overriding automated
wrong decisions.

¢ Do the system operators ensure profes-
sional responsibility? E.g., not using the sys-
tems for deliberate unfair practices such as
target group surveillance.

We can find systems that try to address some of
the concerns, such as mitigating biases (Sap et al.,
2019; Xia et al., 2020; Zueva et al., 2020), develop-
ing interpretable and semi-automated moderation
approaches (Pavlopoulos et al., 2017a; Svec et al.,
2018; Risch and Krestel, 2018).

3.4.2 Legal

Legal concerns deal with policy-level interventions
which are not in control of the system designers and
require legislative actions to resolve. We identify
the following legal concerns pertaining to counter-
ing online hate speech:

¢ Lack of consensus on definition and sever-
ity of hate speech: As mentioned earlier,
there is no clear definition of hate speech,
which makes it harder to legislate laws for
it. Another issue that can be raised is when
an instance of hate speech may become liable
to legal action. This point may be elusive
because the severity of hate speech is highly
subjective to the contexts in which it happens
and often the consequences of the hate speech
are intangible in nature such as psychological
effects (McGonagle et al., 2013).

Borderless nature of the Internet and mul-
tiple justice systems: The Internet is border-
less and international in nature and so is the
reach of online hate speech. This makes the
legislation of online hate speech challenging
(Banks, 2010). This issue calls for more multi-
lateral support between different governments
and justice systems to curb severe online hate
speech. Towards the direction of multilateral
support, the Council of Europe introduced a
protocol to address online hate speech and in-
vited other countries to adopt it (Council of
Europe, 2003; European Commission, 2016).

Who to penalize: Another topic of debate
is who to penalize for the online hate speech.
Often the hate speech is perpetuated using
anonymous accounts, which makes it hard


for the enforcement agencies to track down
the perpetrators (Gagliardone, 2015). There
have also been calls to penalize the platforms
instead of the perpetrators because they failed
to moderate hate speeches?.

¢ Protected by the Freedom of Speech: An-
other issue that was previously mentioned is
the protection of hate speech by laws (Citron
and Norton, 2011). Speech, hateful or not, is
nevertheless a form of expression and silenc-
ing any speech can infringe upon one’s free-
dom of speech. It presents a serious challenge
to legislators on how to balance the freedom of
speech with security and equality. Recently,
some countries have introduced rules to re-
strict hate speech based on their content and
likelihood to cause harm (Gagliardone, 2015).

4 Discussion

In this section, we discuss some important parts
of the framework and how we can use the frame-
work to assist in identification of future research
directions.

The rationale behind differentiating between the
countering methods between proactive and reactive
is because they entail different concerns and harms.
Reactive methods are less likely to invoke the argu-
ment of infringing the freedom of speech, as there
is clear evidence of the hate speech event. In con-
trast, proactive methods act before the hate speech
event happens, putting them in an ethical gray area
as the evidence of future hate speech can be un-
clear and circumstantial. Some proactive methods
require close surveillance of the conversations as
they develop, which increases the computational
overhead and makes them prone to surveillance and
privacy infringement issues. Moreover, as proac-
tive methods try to forecast the future, the effect
of biases can be much more profound based on
the user’s demographics, identity, and network con-
nections. Proactive methods can better limit harm
as hate speech is not yet visible to the consumers,
whereas reactive methods need to be concerned
about the amount and severity of harm, which de-
pend on the time delay between posting and inter-
vention. Both types of methods come with their
pros and cons, which provides a potential future
direction for balancing these methods for more ben-
eficial outcomes.

Shttps://www.reuters.com/article/us-germany-fakenews-
idUSKBN16L14G

The framework states various causes, ethical and
legal concerns. We encourage the readers to mix
and match different methodologies with the con-
cerns to improve the countering systems. For exam-
ple, developing a pre-emptive moderation system
that respects user’s privacy using privacy respect-
ing methods such as federated learning (Yang et al.,
2019a). Moreover, the causes of hate speech can
inform development of proactive methods of coun-
tering online hate speech. The framework also
provides a checklist of concerns to be addressed in
future research and development.

5 Conclusion

This paper provides a comprehensive conceptual
framework for research on countering online hate
speech, including an extensive survey of the latest
methods and pertinent studies. It identifies the
ethical concerns faced by proactive and reactive
methods. It also discusses the ethical concerns
checklist inspired by (Kiritchenko et al., 2020) to
help the readers develop more reliable and ethically
robust countering systems.

We recognize that NLP is just one of the many
tools that can help curb online hate speech. A
complete solution will require multilateral support
among researchers, policymakers, and the average
citizen, with a focus on ethical considerations.

References

Anti-Defamation League. 2016. Anti-semitic targeting
of journalists during the 2016 presidential campaign.
ADL Report.

Chara Bakalis. 2015. Cyberhate: An issue of contin-
ued concern for the council of europe’s anti-racism
commission. Council of Europe, 8:459-472.

James Banks. 2010. Regulating hate speech online. In-
ternational Review of Law, Computers & Technol-
ogy, 24(3):233-239.,

Susan Benesch. 2014. Countering dangerous speech:
New ideas for genocide prevention. Washington,
DC: United States Holocaust Memorial Museum.

Eloi Brassard-Gourdeau and Richard Khoury. 2020.
Using sentiment information for preemptive detec-
tion of toxic comments in online conversations.
arXiv preprint arXiv:2006.10145.

Samuel Carton, Qiaozhu Mei, and Paul Resnick. 2018.
Extractive adversarial networks: High-recall expla-
nations for identifying personal attacks in social me-
dia posts. In Proceedings of the 2018 Conference


on Empirical Methods in Natural Language Process-
ing, pages 3497-3507, Brussels, Belgium. Associa-
tion for Computational Linguistics.

Jonathan P Chang and Cristian Danescu-Niculescu-
Mizil. 2019. Trouble on the horizon: Forecasting
the derailment of online conversations as they de-
velop. arXiv preprint arXiv: 1909.01362.

Charalampos Chelmis and Mengfan Yao. 2019. Minor-
ity report: Cyberbullying prediction on instagram.
In Proceedings of the 10th ACM Conference on Web
Science, pages 37-45.

Justin Cheng, Michael Bernstein, Cristian Danescu-
Niculescu-Mizil, and Jure Leskovec. 2017. Any-
one can become a troll: Causes of trolling behavior
in online discussions. In Proceedings of the 2017
ACM Conference on Computer Supported Coopera-
tive Work and Social Computing, CSCW °17, page
1217-1230, New York, NY, USA. Association for
Computing Machinery.

Naganna Chetty and Sreejith Alathur. 2018. Hate
speech review in the context of online social net-
works. Aggression and Violent Behavior, 40:108 —
118.

Yi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem
Tekiroglu, and Marco Guerini. 2019. CONAN -
COunter NArratives through nichesourcing: a mul-
tilingual dataset of responses to fight online hate
speech. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 2819-2829, Florence, Italy. Association
for Computational Linguistics.

Danielle Keats Citron and Helen Norton. 2011. Inter-
mediaries and hate speech: Fostering digital citizen-
ship for our information age. BUL Rev., 91:1435.

Council of Europe. 2003. Additional protocol to the
convention on cybercrime, concerning the criminali-
sation of acts of a racist and xenophobic nature com-
mitted through computer systems: Strasbourg, 28. 1.
2003. Council of Europe Publ.

Lana Cuthbertson, Alex Kearney, Riley Dawson, Ashia
Zawaduk, Eve Cuthbertson, Ann Gordon-Tighe, and
Kory W Mathewson. 2019. Women, politics and
twitter: Using machine learning to change the dis-
course.

Michela Del Vicario, Alessandro Bessi, Fabiana Zollo,
Fabio Petroni, Antonio Scala, Guido Caldarelli,
H. Eugene Stanley, and Walter Quattrociocchi.
2016a. The spreading of misinformation online.
Proceedings of the National Academy of Sciences,
113(3):554—559.

Michela Del Vicario, Gianna Vivaldo, Alessandro
Bessi, Fabiana Zollo, Antonio Scala, Guido Cal-
darelli, and Walter Quattrociocchi. 2016b. Echo
chambers: Emotional contagion and group polariza-
tion on facebook. Scientific Reports, 6(1):37825.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.

European Commission. 2016. The EU code of conduct
on countering illegal hate speech online.

Paula Fortuna and Sérgio Nunes. 2018. A survey on
automatic detection of hate speech in text. 51(4).

Paula Fortuna, Juan Soler, and Leo Wanner. 2020.
Toxic, hateful, offensive or abusive? what are we
really classifying? an empirical analysis of hate
speech datasets. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference, pages
6786-6794, Marseille, France. European Language
Resources Association.

Iginio Gagliardone. 2015. Countering online hate
speech. United Nations Educational, Scientific and
Cultural Organization, Paris.

Joshua Garland, Keyan Ghazi-Zahedi, Jean-Gabriel
Young, Laurent Hébert-Dufresne, and Mirta Galesic.
2020. Countering hate on social media: Large scale
classification of hate and counter speech. arXiv
preprint arXiv:2006.01974.

Jack Hessel and Lillian Lee. 2019. Something’s
brewing! early prediction of controversy-causing
posts from discussion features. arXiv preprint
arXiv: 1904.07372.

S. Hinduja and J. W. Patchin. 2010. Bullying, cyber-
bullying, and suicide. Arch Suicide Res, 14(3):206-
221.

Prithvi Iyer and Suyash Barve. 2020. Humanising digi-
tal labour: The toll of content moderation on mental
health.

David Jurgens, Eshwar Chandrasekharan, and Libby
Hemphill. 2019a. A just and comprehensive strategy
for using nlp to address online abuse. arXiv preprint
arXiv:1906.01738.

David Jurgens, Libby Hemphill, and Eshwar Chan-
drasekharan. 2019b. A just and comprehensive strat-
egy for using NLP to address online abuse. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 3658—
3666, Florence, Italy. Association for Computa-
tional Linguistics.

Mladen Karan and Jan Snajder. 2019. Preemptive
toxic language detection in wikipedia comments us-
ing thread-level context. In Proceedings of the Third
Workshop on Abusive Language Online, pages 129-
134.

Teo Keipi, Matti Nasi, Atte Oksanen, and Pekka
Rasanen. 2016. Online Hate and Harmful Content:
Cross-National Perspectives.


Svetlana Kiritchenko, Isar Nejadgholi, and Kathleen C
Fraser. 2020. Confronting abusive language online:
A survey from the ethical and human rights perspec-
tive. arXiv preprint arXiv:2012. 12305.

Srijan Kumar, William L. Hamilton, Jure Leskovec,
and Dan Jurafsky. 2018. Community interaction and
conflict on the web. Proceedings of the 2018 World
Wide Web Conference on World Wide Web - WWW
"18.

Cheng-Yu Lai and Chia-Hua Tsai. 2016. Cyberbully-
ing in the social networking sites: An online disin-
hibition effect perspective. In Proceedings of the
The 3rd Multidisciplinary International Social Net-
works Conference on SocialInformatics 2016, Data
Science 2016, pages 1-6.

Elissa Lee and Laura Leets. 2002. Persuasive sto-
rytelling by hate groups online: Examining its ef-
fects on adolescents. American Behavioral Scientist,
45(6):927-957.

So-Hyun Lee and Hee-Woong Kim. 2015. Why peo-
ple post benevolent and malicious comments online.
Commun. ACM, 58(11):74-79.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016a.
Rationalizing neural predictions. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 107-117, Austin,
Texas. Association for Computational Linguistics.

Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi
Jaakkola, Kateryna Tymoshenko, Alessandro Mos-
chitti, and Lluis Marquez. 2016b. Semi-supervised
question retrieval with gated convolutions. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1279-1289, San Diego, California. Association for
Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871-7880, Online. Association
for Computational Linguistics.

Ping Liu, Joshua Guberman, Libby Hemphill, and
Aron Culotta. 2018. Forecasting the presence and
intensity of hostility on instagram using linguistic
and social features. In Proceedings of the Interna-
tional AAAI Conference on Web and Social Media,
volume 12.

Sean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina
Russell, Nazli Goharian, and Ophir Frieder. 2019.
Hate speech detection: Challenges and solutions.
PloS one, 14(8):e0221152.

Binny Mathew, Punyajoy Saha, Hardik Tharad, Sub-
ham Rajgaria, Prajywal Singhania, Suman Kalyan
Maity, Pawan Goyal, and Animesh Mukherjee. 2019.
Thou shalt not hate: Countering online hate speech.
In Proceedings of the International AAAI Confer-
ence on Web and Social Media, volume 13, pages
369-380.

Binny Mathew, Punyajoy Saha, Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Animesh
Mukherjee. 2020. Hatexplain: A benchmark dataset
for explainable hate speech detection.

Tarlach McGonagle et al. 2013. The council of europe
against online hate speech: Conundrums and chal-
lenges. In Expert paper. Belgrade: Council of Eu-
rope Conference of Ministers responsible for Media
and Information Society.

K. J. Mitchell, M. Ybarra, and D. Finkelhor. 2007. The
relative importance of online victimization in under-
standing depression, delinquency, and substance use.
Child Maltreat, 12(4):314—324.

David Noever. 2018. Machine learning suites for on-
line toxicity detection.

John Pavlopoulos, Prodromos Malakasiotis, and Ion
Androutsopoulos. 2017a. Deeper attention to abu-
sive user content moderation. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1125-1135, Copen-
hagen, Denmark. Association for Computational
Linguistics.

John Pavlopoulos, Prodromos Malakasiotis, Juli Bak-
agianni, and Ion Androutsopoulos. 2017b. Im-
proved abusive comment moderation with user em-
beddings. In Proceedings of the 2017 EMNLP Work-
shop: Natural Language Processing meets Journal-
ism, pages 51-55, Copenhagen, Denmark. Associa-
tion for Computational Linguistics.

Michael A. Peters. 2020. Limiting the capacity for
hate: Hate speech, hate groups and the philosophy of
hate. Educational Philosophy and Theory, 0(0):1-6.

Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-
dinov, and Alan W Black. 2018. Style trans-
fer through back-translation. arXiv preprint
arXiv: 1804.09000.

Raj Ratn Pranesh, Ambesh Shekhar, and Anish Kumar.
2020. Towards automatic online hate speech inter-
vention generation using pretrained language model.

Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth M.
Belding, and William Yang Wang. 2019. A bench-
mark dataset for learning to intervene in online hate
speech. CoRR, abs/1909.04251.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.


Julian Risch and Ralf Krestel. 2018. Delete or not
delete? semi-automatic comment moderation for the
newsroom. In Proceedings of the first workshop
on trolling, aggression and cyberbullying (TRAC-
2018), pages 166-176.

Joni Salminen, Maximilian Hopf, Shammur A Chowd-
hury, Soon-gyo Jung, Hind Almerekhi, and
Bernard J Jansen. 2020a. | Developing an on-
line hate classifier for multiple social media plat-
forms. Human-centric Computing and Information

Sciences, 10(1):1—34.

Joni Salminen, Sercan Sengiin, Juan Corporan, Soon-
Gyo Jung, and Bernard J. Jansen. 2020b. Topic-
driven toxicity: Exploring the relationship be-
tween online toxicity and news topics. PloS one,
15(2):e0228723-e0228723. 32084164[pmid].

Cicero Nogueira dos Santos, Igor Melnyk, and Inkit
Padhi. 2018. Fighting offensive language on social
media with unsupervised text style transfer. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 189-194, Melbourne, Australia. As-
sociation for Computational Linguistics.

Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,
and Noah A. Smith. 2019. The risk of racial bias
in hate speech detection. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 1668-1678, Florence,
Italy. Association for Computational Linguistics.

Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In Proceedings of the fifth international
workshop on natural language processing for social
media, pages 1-10.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Controlling politeness in neural machine
translation via side constraints. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 35-40, San
Diego, California. Association for Computational
Linguistics.

Tianxiao Shen, Tao Lei, Regina Barzilay, and
Tommi Jaakkola. 2017. Style transfer from non-
parallel text by cross-alignment. arXiv preprint
arXiv:1705.09655.

Dandara Almeida Reis da Silva, Rodrigo Fernan-
des Weyll Pimentel, and Magno Concei¢ao das Mer-
ces. 2020. Covid-19 and the pandemic of fear: re-
flections on mental health. Revista de saude publica,
54:46-46. 32491094[pmid].

Tanya Silverman, Christopher J Stewart, Jonathan Bird-
well, and Zahed Amanullah. 2016. The impact of
counter-narratives. Institute for Strategic Dialogue,
pages 1-54.

Wiktor Soral, Michal Bilewicz, and Mikotaj Winiewski.
2018. Exposure to hate speech increases preju-
dice through desensitization. Aggressive Behavior,
44(2):136-146.

John Suler. 2004. The online disinhibition effect. Cy-
berPsychology & Behavior, 7(3):321-326. PMID:
15257832.

Andrej Svec, Matt Pikuliak, Marian Simko, and
Maria Bielikova. 2018. Improving moderation of
online discussions via interpretable neural models.
In Proceedings of the 2nd Workshop on Abusive Lan-
guage Online (ALW2), pages 60-65, Brussels, Bel-
gium. Association for Computational Linguistics.

Serra Sinem Tekiroglu, Yi-Ling Chung, and Marco
Guerini. 2020. Generating counter narratives
against online hate speech: Data and strategies.

The European Commission against Racism and Intoler-
ance. 2016. General policy recommendation no. 15
on combating hate speech.

Jean Marie Tshimula, Belkacem Chikhaoui, and Shen-
grui Wang. 2020. On predicting behavioral deterio-
ration in online discussion forums. In Proceedings
of the 2020 IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining.

Reinis Udris. 2014. Cyberbullying among high school
students in japan: Development and validation of the
online disinhibition scale. Computers in Human Be-
havior, 41:253-261.

Lucas Wright, Derek Ruths, Kelly P Dillon, Haji Mo-
hammad Saleem, and Susan Benesch. 2017. Vec-
tors for counterspeech on Twitter. In Proceedings
of the First Workshop on Abusive Language Online,
pages 57-62, Vancouver, BC, Canada. Association
for Computational Linguistics.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.
Ex machina: Personal attacks seen at scale.

Mengzhou Xia, Anjalie Field, and Yulia Tsvetkov.
2020. Demoting racial bias in hate speech detection.
In Proceedings of the Eighth International Work-
shop on Natural Language Processing for Social Me-
dia, pages 7-14, Online. Association for Computa-
tional Linguistics.

Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin
Tong. 2019a. Federated machine learning: Concept
and applications. ACM Trans. Intell. Syst. Technol.,
10(2).

Zichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, and
Taylor Berg-Kirkpatrick. 2019b. Unsupervised text
style transfer using language models as discrimina-
tors.

Justine Zhang, Jonathan P Chang, Cristian Danescu-
Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Nithum
Thain, and Dario Taraborelli. 2018. Conversations
gone awry: Detecting early signs of conversational
failure. arXiv preprint arXiv: 1805.05345.


Yizhe Zhang, Sigi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2020. Dialogpt: Large-scale

generative pre-training for conversational response
generation.

Caleb Ziems, Bing He, Sandeep Soni, and Srijan Ku-
mar. 2020. Racism is a virus: Anti-asian hate and

counterhate in social media during the covid-19 cri-
SiS.

Nadezhda Zueva, Madina Kabirova, and Pavel
Kalaidin. 2020. Reducing unintended identity bias
in russian hate speech detection.
