arX1v:2005.14299v1 [cs.CL] 28 May 2020

What is SemEval evaluating?
A Systematic Analysis of Evaluation Campaigns in NLP

Oskar Wysocki, Malina Florea and Andre Freitas
Department of Computer Science
The University of Manchester, United Kingdom
oskar.wysocki@manchester.ac.uk
malina.florea@student.manchester.ac.uk
andre.freitas@manchester.ac.uk

Abstract

SemEval is the primary venue in the NLP community for the proposal of new challenges and for
the systematic empirical evaluation of NLP systems. This paper provides a systematic quantita-
tive analysis of SemEval aiming to evidence the patterns of the contributions behind SemEval.
By understanding the distribution of task types, metrics, architectures, participation and citations
over time we aim to answer the question on what is being evaluated by SemEval.

1 Introduction

A large portion of the empirical methods in Natural Language Processing (NLP) are defined over canon-
ical text interpretation tasks such as Named Entity Recognition (NER), Semantic Role Labeling (SRL),
Sentiment Analysis (SA), among others. The systematic creation of benchmarks and the comparative
performance analysis of resources, representations and algorithms is instrumental for moving the bound-

aries of natural language interpretation. SemEval (May et al., 2019} |Apidianaki et al., 2018} /Bethard et|
al., 2017}|Bethard et al., 2016}|Nakov et al., 2015}|Nakov and Zesch, 2014}|Manandhar and Yuret, 2013;
Agirre et al., 2012) is the primary venue in the NLP community for the organisation of shared NLP tasks

and challenges. SemEval is organised as an annual workshop co-located with the main NLP conferences
and has attracted a large and growing audience of task organisers and participants.

Despite its recognition as a major driver in the creation of gold-standards and evaluation campaigns,
there is no existing meta-analysis which interprets the overall contribution of SemEval as a collective
effort. This paper aims to address this gap by performing a systematic descriptive quantitative analysis
of 96 tasks encompassing the SemEval campaigns between 2012-2019. This study targets understanding
the evolution of SemEval over this period, describing the core patterns with regard to task popularity,
impact, task format (inputs, outputs), techniques, target languages and evaluation metrics.

This paper is organised as follows: section 2 describes related work; 3 describes the methodology;
4 defines the underlying task macro-categories; 5 and 6 presents the number of tasks and popularity in
2012-2019; 7 discusses SemEval impact in terms of citations; 8 shows targeted languages; then, sections
9, 10, 11 analyse input, output and evaluation metrics; 11 focuses on sentiment analysis architectures
and representations; this is followed by a Discussion section; we close the paper with Recommendations
and Conclusions.

2 Related work

Each SemEval task is described by an anthology, which contains: a summary of previous editions or
similar tasks, references to previous works, detailed task description, evaluation methods, available re-
sources, overview of submitted systems and final results of the competition. It is worth noting, there is
a variation, or even inconsistency, in the structure and the level of detail in the description. Participants
are also encouraged to submit papers with systems architecture explanations. However, there is a lack of
overall analysis across different tasks and years in SemEval. There are existing studies on the analysis of

specific SemEval tasks. (Nakov et al., 2016) focuses on developing Sentiment Analysis tasks in 2013-
2015. (Sygkounas et al., 2016) is an example of a replication study of the top performing systems, in this

case systems used in SemEval Twitter Sentiment Analysis (2013-2015), and focuses on architectures


and performance. Evolution and challenges in semantics similarity were described in
2015). This is an example of a study on the performance of a given type of architecture across tasks of
the same type. There also exist studies on shared tasks in given domain, specially in clinical application
of NLP (Filannino and Uzuner, 2018), (Chapman et al., 2011). However, they refer to tasks outside the
SemEval and are more result oriented rather than task organization. Some studies discuss ethical issues
in the organisation and participation of shared tasks. An overview focusing on task competitive nature
and fairness can be found in (Parra Escartin et al., 2017). In authors also relate to
these issues, yet giving the priority to advancing the field over fair competition.

Comparatively, this paper covers a wider range of NLP topics, and compares sentiment analysis and
semantic similarity as well as other task types/groups in a systematic manner. To the best to our knowl-
edge this is the first systematic analysis on SemEval.

3 Analysis methodology

We build a corpus based on the ACL anthology archive from the SemEval workshops between the years
2012-2019. Reference material included ACL anthology papers covering the task description, tasks’
websites and papers describing the participating systems. All the reference papers included in this anal-
ysis are reported at the Appendix B. The pre-processing analysis consisted in manually extracting the
target categories for the analysis which includes: task types, input and output types, as well as evalu-
ation metrics, number of teams, languages and system architectures. Tasks were grouped based on the
similarity between task types similarity. If the same team took part in several tasks the same year, we
considered each participation as distinct. There are four missing tasks in the plotted indexes, due to
cancellation (2015-16, 2019-11), task-sharing (2013-6) or lack of supporting task description (2013-14).
Numbers of citations are the numbers returned by Google Scholar, using Publish and Perish supporting
API (Harzing, 2007). The list of citations were manually validated and noisy entries were filtered out. A
final table with all the values extracted from the corpus is included in the Appendix B.

4 Task types and groups

Based on task description we group each task within a macro-category. Then, due to a large number of
task types, tasks were clustered within 6 groups: Sentiment Analysis (SA); Semantic Analysis (SEM):
semantic analysis, semantic difference, semantic inference, semantic role labeling, semantic parsing,
semantic similarity, relational similarity; Information Extraction (TE): information extraction, temporal
information extraction, argument mining, fact checking; Machine Translation (MT); Question Answer-
ing (QA); Other (OT): hypernym discovery, entity linking, lexical simplification, word sense disam-
biguation, taxonomy extraction, taxonomy enrichment. There are also macro-categories defined by the
SemEval organizers, starting from 2015, but we found them not consistent enough for the purpose of this
analysis.

5 SemEval tasks in years

Within 8 editions of SemEval, a total of 96 tasks were successfully announced. The number of tasks
within one group is roughly similar every year (except for MT), as well as distribution of tasks in each
edition. According to Fig{Ia| we observe decreasing number of SEM tasks: 5 on average in 2012-2017,
and only 2 in 2018-2019. Moreover, there were no machine translation tasks in the last 2 years, and a
low number of MT tasks in general (only 4 tasks in 8 years).

Although SA has a relatively limited task complexity when compared to SEM or IE, which reflects a
higher variety of task types and an abundance of more specific interpretation challenges, the number of
SA tasks each year is high (4, 3, 3 and 4 in years 2016-2019). It is worth mentioning, that there are other
6 SA tasks in the forthcoming SemEval 2020. The absence of some task types may be caused by the
emergence of specialized workshops or conferences, e.g. low number of MT tasks in SemEval is caused

by the presence a separate venue for MT: the Conference On Machine Translation (Barrault et al., 2019),
which attracts more participants than SemEval in this field.


20

18 Z
16 4~- ©
g
14 D
£124- 5a
cj Q
, 10 3 ——
+ 8+ & i H
6+ & pk
5
ay a _
6 A R
0 0
S 2 & 8 $ & & BQ as ©8 & 2 8&8 & & &
fo} oO Oo fo} fo} [o} fo} oO fo} Oo fo} fo} Oo fo} oO oO
N N N N N N N N N N N N N N N N
(a) Number of tasks in SemEval 2012-2019 (b) Number of teams participating in SemEval
800
n 2
f= 600 +
£ s
oO ce)
5 ‘5 400 +
* +
200 4
12 ia
ol o Lae
N ™m t+ wy oOo ~ foe} Oo N ~m + wn Oo ~ foo} oa
a ea Coal ca a a a Coal cd ci ci ci om ca ca ci
[o} oO fo} fo} fo} fo} oO lo} fo} oO Oo Oo [o) fo} oO Oo
N N N N N N N N N N N N N N N N

(c) Cumulative number of task citations, except for citations | (d) Cumulative number of task citations in SemEval pro-
in SemEval proceedings ceedings

Figure 1: Overall plots for SemEval editions 2012-2019

6 Task popularity

As ameasure of task popularity, we analysed how many teams participated in a given task. As the number
of teams signed up to the task is usually much higher than the number submitting a system result, we
consider only the latter.

The number of teams increased significantly from 62 in 2012 to 646 in 2019, which shows not only
a popularity boost for SemEval, but an increase in the general interest for NLP. So far, a total of 1883
teams participated in this period.

In Fig{Ib} we observe a gradual increase in SemEval popularity, 30% on average each year to 2018,
with a +129% jump in 2019. This is associated mainly with a dramatic increase of interest for SA: 542
teams (84% of total) in 2019. However, at the same time, number of teams in non-SA tasks decreased
from 132 in 2017, to 115 in 2018 and 104 in 2019.

The most popular tasks groups along the years are SA and SEM, which gather more than 75% of teams
on average each year. The third most popular is IE, in which total of 235 teams participated in SemEval
from 2012 (12% of total). As a contrast, we observe a relatively low interest in QA and OT tasks. Only
41 teams participated in the last 3 years (3% of a total of 1148 in 2017-2019). Especially in OT tasks,
which concentrates novel tasks, in many cases including novel formats.

In the last 2 years, SA shows a major increase in popularity (76% of all teams, compared to 40% in
2013-2017). At the same time, in tasks such as 2019-10, 2018-4 and 2018-6, which are mathematical
question answering, entity linking on multiparty dialogues and parsing time normalization, respectively,
only 3, 4 and | teams submitted results. This divergence may be associated with an emergence of easily
applicable ML systems and libraries, which better fit to standard classification tasks more prevalent in
SA (in contrast to OT, QA nor IE).


7 The impact of SemEval papers

As a measure of the impact of SemEval outcomes in the NLP community, we analysed the numbers
of citations per task description in Google Scholar. The task description paper was used as a proxy to
analyse the task impact within the NLP community. Papers submitted by participating teams describing
systems and methods were not included on this analysis.

We considered the cumulative citations from 2012 to 2019 (Fig{Ich, with additional distinction on
citations of task description papers published in a given year (Fig 2b). Citations within SemEval pro-
ceedings were treated separately, as we focused on the impact both outside (Fig[Ich and inside (Fig{Id)
the SemEval community. In other words, citations found in Google Scholar are split into numbers of
papers out and in the SemEval proceedings.

SA and SEM have the highest impact, being the most cited tasks along the years both inside and
outside SemEval community, what can be attributed to their high popularity.

Considering the external impact, in 2019 SA and SEM anthologies contributed with 2847 (41%) and
2426 (35%) citations respectively. IE has 985 citations (14%) and QA contributed with 148 citations
(2%). The OT group, which consists of less canonical tasks, accumulated 468 citations (7%). The
impact of MT papers is noticeably lower - 84 (1%).

In terms of citations within the SemEval community (in all SemEval 2012-2019 proceedings), we
observe a similar pattern: 41% and 37% citations in 2019 come from SA and SEM (357 and 322), and
for remaining task groups proportions are almost identical as in citations outside community (Chi.sq.
p-value=0.06).

The number of citations outside is 8 times higher than inside the community. This proves the scientific
impact and coverage, which leads to beneficial effect of SemEval on the overall NLP community.

A total of 6958 citations from 2019 are depicted in Fig[2b] with distinction on the year in which the
task was published (e.g. tasks from 2016 are cited 1682 times (23%)). Similarly, a total of 876 citations
in the SemEval proceedings are presented in Fig 2c] (e.g. anthologies published in 2015 are cited 163
times in all SemEval proceedings so far). SA tasks from 2016, SEM from 2014 and IE from 2013 have
the highest impact compared within groups (40%, 28% and 42% respectively). One could expect higher
numbers of citations for older papers, however, we do not observe this pattern.

8 Languages in tasks

We analysed SemEval it terms of languages used in the tasks (Fig[2ap. We can distinguish 3 clusters:
English-only (except for 3 tasks entirely in Chinese); multi-lingual, which define identical subtasks for
several languages; cross-lingual (targeting the use of semantic relation across languages).

In total of 96 tasks, 30 investigated more than one language (multi-lingual and cross-lingual tasks) and
63 tasks were using only English.

The five most popular languages, excluding English are: Spanish (16), French (10), Italian (10), Arabic
(8), German (8). Although Chinese is the 1st language in number of speakers, only 4 tasks were organised
for Chinese.

Most of multi-lingual or cross-lingual tasks are related to SA (5 in 2016-2018) or SEM (15 in 2012-
2019), and obviously on MT tasks (3 in 2012-2014). There were 3 OT tasks, only one QA task, and no
IE tasks. Task 11 in 2017 concerning program synthesis, aiming to translate commands in English into
(program) code, attracted only one team.

In 2018 and 2019 the interest of other languages is lower compared to previous years. Languages other
than English were proposed only 5 and 3 times, respectively, whereas in 2016 and 2017 we observed the
occurrence of respectively 10 and 14 times.

9 Input and Output Analysis

In order to better understand the evolution of the semantic complexity of the tasks, we analysed them in
terms of the types used to represent input and output data in all subtasks. Based on their descriptions, we
devised a list of 25 different abstract types used, then assigning each subtask the most appropriate Input
and Output Types.


9.1 Types and Clusters

Taking into consideration both their complexity and purpose, we split the type list into 5 clusters: cluster
J: document, text, paragraph, sentence, phrase, word, number; cluster 2: score, score real value, score
whole value, class label, probability distribution; cluster 3: entity, attribute, topic, tree, Directed Acyclic
Graph (DAG); cluster 4: question, answer, query; cluster 5: Knowledge Base (KB), program, time
interval, timeline, semantic graph, syntactic labeled sentence.

9.2 Input Types

As expected, types from cluster / (sequential tokens) make up for 76% of overall input types used in all
tasks (depicted in the Appendix A, Fig[A.2p. Most popular input type is paragraph, for which about 60%
of cases represents a tweet. The remaining 24% is split across clusters 2, 3, 4 and 5. A subtle divergence
towards the right-hand side can be noticed, starting with 2015, driven mostly by tasks from SA and IE
task groups. The most dominant Input Types from each cluster are paragraph, class label, entity, question
and KB.

9.3. Output Types

As shown in Fig3al data types from clusters 2 and 3 are the majority in this case, accounting for 68%
of used representations. Class labels are repeatedly employed, especially by SA tasks. Cluster J types
are constantly used across the years, fully dependent on the task types given in a certain year, 78%
of them coming from SEM, IE and OT. Rarely used are typed from clusters 4 and 5, accounting for
just 10% of the total, half of which occur in SEM tasks during 2016 and 2017, complex tasks such as
Community Question Answering and Chinese Semantic Dependency Parsing. We also found a possible
relation between output type and popularity. In 2012-2017 tasks where outputs were in cluster 4 or 5,
attracted 8.3 teams per task on average, while in clusters 1-3 13.9 teams/task. However, despite major
increase in SemEval popularity, in 2018-2019 the former attracted only 7 teams/task, and the latter 43.5
teams/task. The group with most type variety is SEM, covering types across all clusters. On the other
side of the spectrum, SA has the least variety, despite it being the most popular task group. The most
dominant Output Types from each cluster are paragraph, class label, entity, answer and semantic graph.

10 Evaluation Metrics

We counted a total of 29 different evaluation metrics used in SemEval (Fig[3b).

At a subtask level, the most frequent metric is Fl-score, with 105 usages, followed by recall and
precision, with 51 and 49 usages respectively, and accuracy, with 26 usages. F1, recall and precision are
frequently jointly used, the last two playing the role of supporting break-down metrics for F1 in 95% of
cases. This combination is very popular, especially for IE tasks, almost half of the use coming from this
task group.

The top 5 evaluation metrics make up 84% of the total number of metrics used in all years, last 12
(almost half) being only used once. In 89% of cases when rare evaluation metrics (from Kendall’s T to the
right) are used, they occur in SA and SEM tasks e.g. Jaccard index in Affect in Tweets (2018) or Smatch
in Meaning Representation Parsing (2016). Furthermore, 67% of the least used evaluation metrics (only
used 3 times or less) appear in 2015-2017, the same period when we could see tasks experimenting the
most with input and output types.

10.1 Evaluation Metrics against Output Types

F1, recall and precision (depicted in Appendix A, FiglA.1) are mostly used for output types such as
class label, paragraph and entity (each of which is the top output type from their clusters). Meanwhile,
for output types represented by score, most used evaluation metrics are Pearson Correlation, Kendall’s
T, cosine similarity and Spearman Correlation. MAP, the 6th most used evaluation metric, is mostly
used for ranked questions/answers either in recurring tasks such as Community Question Answering.
Human judgment was only used twice, in Taxonomy Extraction Evaluation (2016) and Abstract Meaning
Representation Parsing and Generation (2017). For further reference, see Appendix A.


Task group _

2019 12
2019-10
2019 09
2019 08
2019.07
2019_06
2019_05
2019 04
2019_03
2019.02
2019 01
2018-12
2018-11
2018-10
2018 09
2018 08
2018 07
201806
2018_05
2018 04
201803
2018 02
2018 01
2017-12
2017-11
2017-10
2017-09
2017 08
2017.07
2017 06
2017.05
2017.04
2017.03
2017-02
201701
201614
2016 13
2016 12
2016 11
2016 10
2016 09
2016 08
201607
2016_06
2016_05
2016_04
2016_03
201602
2016 01
2015.18
2015_17
2015.15
2015.14
2015.13
2015 12
2015_11
2015_10
2015_09
2015_08
2015_07
2015_06
2015 _05
2015 04
201503
201502
2015 01
2014-10
2014.09
2014_08
2014_07
2014_06
2014_05
2014.04
2014_03
201402
2014 01
2013-13
2013-12
2013 11
2013 10
2013.09
2013 08
2013.07
2013.05
2013.04
201303
201302
2013 01
2012 08
2012.07
2012-06
2012.05
2012 04
201203
2012.02
2012_01

# of citations

2012
2013
2014
2015
2016
2017
2018
2019

(b) Number of task citations published in given year, except
for citations in SemEval proceeding

SS) SA QA
Bes seM OO MT

53 fam IE ZZ OT

200

BR
u
fo}

100

# of citations

50

2012
2013
2014
2015
2016
2017
2018
2019

(c) Number of task citations from given year in SemEval
proceedings

2019.06 §) Popular — I ° ©

2019.05 4 a

2019041 ® Best

2019.03 4 =
g |

2018_02 4-
201801 4
2017_06 4-
2017-05 4
2017_04 4-
2016_07 4-
2016_06 4
2016_05 +
2016_04 4-

2015 12
2015 11
2015_10
2015_09
201409
201404
2013_02

=
2018.03 4} | ie 4
/ _——e*

Other
MaxEnt
AdaBoost
XGBoost
CRF

CNN

RNN

Russian
Greek
Turkish
Czech
Chinese
Dutch
German
Arabic
French
Italian
Spanish
English
Naive Bayes
LightGBM Tree

Programming
language

Persian (Farsi)

Regression based

(a) Languages used in 96 SemEval tasks from 2012 to 2019 (d) Models used in SA tasks from 2012 to 2019 at SemEval

Figure 2:


Subtask Counts

2019.12
2019_10
2019_09
2019_08
2019_07
2019_06
2019_05
2019_04
2019_03
2019_02
2019 01
2018_12
201811
2018_10
2018_09
2018_08
2018_07
2018_06
2018_05
2018_04
2018_03
2018 02
2018_01
2017_12
2017_11
2017_10
2017_09
2017_08
2017_07
2017_06
2017_05
2017_04
2017_03
2017_02
201701
2016_14
2016_13
2016_12
2016_11
2016_10
2016_09
2016_08
2016_07
2016_06
2016_05
2016_04
2016_03
2016_02
2016_01
2015_18
2015_17
2015_15
2015.14
2015_13
2015 12
2015_11
2015_10
2015_09
2015_08
2015_07
2015_06
2015_05
2015_04
2015_03
201502
2015_01
201410
2014_09
2014_08
2014_07
2014_06
2014_05
2014_04
2014_03
2014 02
2014 01
2013_13
201312
2013_11
2013.10
2013_09
2013_08
2013_07
2013_05
201304
2013_03
201302
2013_01
2012_08
2012_07
2012_06
2012_05
2012_04
2012_03
2012_02
2012_01

document

<

text

paragraph

sentence

phrase

<A I A @@@@AAA

Ad

<
<
Vv
e

<q

a <
<<

A
A
A

word

number

score

score real value

score whole value
topic
tree
DAG

class label
attribute

distribution

(a) Output Types

VV 2017_05

vv 2016_05

question

Subtask Counts

2019 12
2019.10 >
2019.09
2019.08 <4
2019_07
2019.06
2019_05
2019.04
2019_03
> 2019 02
2019 01

201812

2018_11

2018_10

2018.09

2018_08

2018_07

2018_06

2018_05

2018_04

2018_03

2018_02

a 201801
2017_12

Vv 201711

2017_10
2017_09
2017_08
2017_07
2017_06

A

@e@A

2017_04
2017_03
2017_02
2017_01
2016 14
2016.13
2016_12
2016_11
2016_10
2016_09
2016_08
2016 07
201606

<<

201604
2016_03
2016 02
2016_01 Vv
2015.18 W-
2015_17 I
2015.15 VW
2015_14 < <q
2015.13 O00
201512 @
2015.11
2015.10 @
2015.09 @@@
<4 2015.08 <q
2015_07 <q
2015.06 <<
>
Vv

2015_05
2015_04
2015_03
2015_02
201501
2014.10
2014.09
201408
Vv 201407
201406
201405
2014 04
201403
201402
201401
2013.13
2013.12 |-W-
2013_11 >
2013_10 A
2013.09 <q
2013_08 W.
2013.07 |-W-
2013.05 Yo VW
2013_04
2013.03 <<<
@
<

<<<

201302
201301
2012_08 A
2012_07 ¥
2012_06
¥- 2012.05 y.
2012_04 Vv
2012.03 <<
2012_02 Vv
2012_01

KB

program
time interval

LAS

MAP
custom evaluation

Kendall's t

Spearman Correlation
MAE

human judgement

Earth Mover’s Distance

Kullback-Leibler Divergence
FWA

query
recall
precision
Smatch
strict F

Cumulative F&M

answer
Exact Match Ratio

timeline

semantic graph

syntactic labeled sentence
accuracy

Pearson Correlation
cosine similarity

standard coverage
BCUBED F-SCORE

(b) Evaluation Metrics

Figure 3: 96 SemEval tasks from 2012-2019

kappa index

AvgDiff
Jaccard index

VP Ade

a

J

UAS
RMSE

MaxDiff
edit distance

SA
SEM

QA
oT


11 Zooming in into Sentiment Analysis

11.1 System architectures

The systematic analysis of the prevalent methods and architectures imposed particular challenges with
regard to the data extraction process due to the intrinsic complexity of tasks (many systems include the
composition of pre-processing techniques, rules, hand-crafted features and combinations of algorithms).
Additionally, for the majority of task description papers, there is no systematic comparison between
systems within a task, and consequently within group or years.

Due to the consistent presence of SA along all years, we present an overview of the evolution of system
architectures used in SA from 2013 to 2019 (Fig[2d). In this analysis we focus on the best performing
architectures. More than one best model in a task signifies best models in subtasks or that the final
system was an ensemble of several algorithms. Regression based model encompasses linear, logistic, or
Gaussian regression, and Other includes all rule-based or heavily hand-crafted models.

We observe a drift in popularity of architectures from ML algorithms (2013-2016) to deep learning
(DL) models (2017-2019).

Despite the major adoption of DL models, traditional ML algorithms are consistently in use, both
as separate models and as ensembles with DL. This is also true for other types of tasks. In many task
description papers from 2018-2019, one can find ML-based systems as top performing participants.
SVM-based models are still popular and in some tasks outperforms DL (2018-2, 2019-5).

In the analysis of system architectures one needs to take into account that best system depends not only
on the core algorithm but also on the team expertise and supporting feature sets and language resources.

11.2 Representations

The output of the SA related tasks provide an account of the evolution of sentiment and emotion repre-
sentation in this community from 2013 until 2019 (Fig 4p.

At a discrete level, the number of maximum class labels representing sentiment intensity grew from
3 in 2013 to 7 in 2019. At a continuous score level, real-valued scores associated with sentiment was
first used in 2015; in 2016 it switched to sentiment intensity; in 2017 it was being used as a way to
determine the intensity of an emotion component out of 11 emotion types (rather than a single one,
or the generic emotional intensity of a sentence). In terms of targeted subject, the tasks grew more
granular over time: paragraph/word (2013), aspect terms (2014), sentence topic (2015), person (2016).

2013.02 2014 04 2014 09 2015 08 2015 10 201511 2015 12 2016 04 2016 05 2016 06 2016 07 2017.04 2017.05 2017.06 2018 01 2018 02 2018 03 2019 03 2019 04 2019 05 2019 06

eaccecgses: PH peegeccese

[wo | @» {PH =

[PH |

TEXT
PARAGRAPH PROBABILITY DISTRIBUTION

( SE ] SENTENCE SCORE REAL VALUE ENTITY

| PHD ) PHRASE SCORE WHOLE VALUE ATTRIBUTE

—€®—>» Sentiment Analysis in Twitter

(wo | WORD CL) CLASS LABEL TOPIC —®—>» Aspect Based Sentiment Analysis

Figure 4: Timeline of Input Types (upper row) and Output Types (lower row) in Sentiment Analysis tasks
at SemEval 2013-2019.


Additionally, discourse evolved from simpler opinionated text in the direction of figurative language, for
example: handling irony and metaphor in SA (2015), phrases comparison/ranking in terms of sense of
humor (2017), irony detection (2018) and contextual emphasis (2019).

12 Discussion: What is SemEval evaluating?

The results of the analysis substantiate the following core claims, which summarises some of the trends
identified in this paper:

e There is evidence of significant impact of SemEval in the overall NLP community.

e SemEval contributed to the construction of a large and diverse set of challenges with regard to
semantic representation, supporting resources and evaluation methodologies and metrics.

e SemEval is becoming heavily biased towards solving classification/regression problems. We ob-
serve a major interest in tasks where the expected output is a binary or multi-class label or within a
continuous real valued score.

Sentiment Analysis tasks accounts for a disproportional attention from the community.

e There are two parallel narratives running on SemEval: low entry barrier and state-of-the-art defin-
ing. SemEval contains a rich corpus of unaddressed and complex NLP tasks, which are eclipsed by
the easier low entry barrier tasks. This points to the double function of SemEval which performs
a pedagogical task, serving as an entry point for early career researchers to engage within the NLP
community and a state-of-the-art forum for pushing the boundaries of natural language interpreta-
tion. With the popularity of NLP applications and Deep Learning, the former function is eclipsing
the latter.

There is a significant trend to decrease the variety in the output and evaluation metrics in the recent
years. While in previous years, tasks focused more on novel and exploratory tasks, recent tasks
have explored, probably due to emergence of out-of-the-box DL models, this variety significantly
decreased. Consequently, participants focus on easier tasks, which in part dissipates the community
potential to address long-term challenges.

Despite the recent interest in neural-based architectures, there is clear evidence of the longevity and
lasting impact of older NLP methods.

13. Recommendations

We believe that this paper can serve as a guideline for the selection and organisation of future SemEval
tasks. Based on the analyses performed on this paper, these are the main recommendations:

e Prioritise tasks which have a clear argument on semantic and methodological challenges and nov-
elty.

e Differentiate challenges which have a competition/pedagogical purpose from research tasks.

e Support the systematic capture of task metadata and submission data in a structured manner. This
will allow for an efficient comparison between SemEval tasks and deriving insights for future Se-
mEval editions.

14 Conclusions

This paper reported a systematic quantitative analysis of SemEval, one of the primary venues for the
empirical evaluation of NLP systems. The analysis, which provides a detailed breakdown of 96 tasks in
the period between 2012-2019, provided quantitative evidence that: (i) SemEval has a significant impact
in the overall NLP community, (ii) there is a recent drift towards a bias in the direction of Deep Learning
classification methods which is eclipsing the research function of SemEval and (iii) that there is longevity
and impact of older NLP methods in comparison to Deep Learning methods.


References

Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret, editors. 2012. *SEM
2012: The First Joint Conference on Lexical and Computational Semantics — Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic
Evaluation (SemEval 2012), Montréal, Canada, 7-8 June. Association for Computational Linguistics.

Marianna Apidianaki, Saif M. Mohammad, Jonathan May, Ekaterina Shutova, Steven Bethard, and Marine
Carpuat, editors. 2018. Proceedings of The 12th International Workshop on Semantic Evaluation, New Or-
leans, Louisiana, June. Association for Computational Linguistics.

Loc Barrault, Ondej Bojar, Marta R. Costa-juss, Christian Federmann, Mark Fishel, Yvette Graham, Barry Had-
dow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Mller, Santanu Pal, Matt Post,
and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (wmt19). In Proceedings
of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1-61, Florence,
Italy, August. Association for Computational Linguistics.

Steven Bethard, Marine Carpuat, Daniel Cer, David Jurgens, Preslav Nakov, and Torsten Zesch, editors. 2016.
Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, Califor-
nia, June. Association for Computational Linguistics.

Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens,
editors. 2017. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Van-
couver, Canada, August. Association for Computational Linguistics.

Wendy W Chapman, Prakash M Nadkarni, Lynette Hirschman, Leonard W D’ Avolio, Guergana K Savova, and
Ozlem Uzuner. 2011. Overcoming barriers to NLP for clinical text: the role of shared tasks and the need for
additional creative solutions. Journal of the American Medical Informatics Association, 18(5):540-543, 09.

Michele Filannino and zlem Uzuner. 2018. Advancing the state of the art in clinical natural language processing
through shared tasks. Yearbook of medical informatics, 27(1):184192, August.

A.W. Harzing. 2007. Publish or perish. available from https://harzing.com/resources/publish-or-perish.

Sergio Jimenez, Fabio A. Gonzalez, and Alexander Gelbukh. 2015. Soft Cardinality in Semantic Text Processing:
Experience of the SemEval International Competitions. Polibits, pages 63 — 72, 06.

Suresh Manandhar and Deniz Yuret, editors. 2013. Second Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
(SemEval 2013), Atlanta, Georgia, USA, June. Association for Computational Linguistics.

Jonathan May, Ekaterina Shutova, Aurelie Herbelot, Xiaodan Zhu, Marianna Apidianaki, and Saif M. Moham-
mad, editors. 2019. Proceedings of the 13th International Workshop on Semantic Evaluation, Minneapolis,
Minnesota, USA, June. Association for Computational Linguistics.

Preslav Nakov and Torsten Zesch, editors. 2014. Proceedings of the 8th International Workshop on Semantic
Evaluation (SemEval 2014), Dublin, Ireland, August. Association for Computational Linguistics.

Preslav Nakov, Torsten Zesch, Daniel Cer, and David Jurgens, editors. 2015. Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado, June. Association for Computational
Linguistics.

Preslav Nakov, Sara Rosenthal, Svetlana Kiritchenko, Saif M. Mohammad, Zornitsa Kozareva, Alan Ritter, Veselin
Stoyanov, and Xiaodan Zhu. 2016. Developing a successful semeval task in sentiment analysis of twitter and
other social media texts. Language Resources and Evaluation, 50(1):35—65, Mar.

Malvina Nissim, Lasha Abzianidze, Kilian Evang, Rob van der Goot, Hessel Haagsma, Barbara Plank, and Mar-
tijn Wieling. 2017. Last words: Sharing is caring: The future of shared tasks. Computational Linguistics,
43(4):897-904, December.

Carla Parra Escartin, Wessel Reijers, Teresa Lynn, Joss Moorkens, Andy Way, and Chao-Hong Liu. 2017. Ethical
considerations in NLP shared tasks. In Proceedings of the First ACL Workshop on Ethics in Natural Language
Processing, pages 66-73, Valencia, Spain, April. Association for Computational Linguistics.

Efstratios Sygkounas, Giuseppe Rizzo, and Raphaél Troncy. 2016. A replication study of the top performing
systems in semeval twitter sentiment analysis. In Paul Groth, Elena Simperl, Alasdair Gray, Marta Sabou,
Markus Krotzsch, Freddy Lecue, Fabian Flock, and Yolanda Gil, editors, The Semantic Web — ISWC 2016,
pages 204-219, Cham. Springer International Publishing.


Appendix

- Fl
- Pearson Correlation

t- MAP

BCUBED F-SCORE
Kullback-Leibler Divergence
Earth Mover’s Distance

+ FWA

t- human judgement

t Cumulative F&M

- Smatch
- LAS
+ UAS

t MAE

Exact Match Ratio
Jaccard index

- AvgDiff

- standard coverage

strict F

MaxDiff
RMSE

t kappa index

f custom evaluation

t edit distance

- Kendall's Tt

- cosine similarity
t Spearman Correlation

accuracy

recall
precision

class label
paragraph
entity
score

Figure A.1:

o o ¥$ DBD YU f€¢& & OO £& £ # co vo YW
585 FSS Rs aBERELE
= = = Dp « cS =
gay ses S$ HHESB FED GD GO
— 5 56562 9 5 <¢ £ E
a vo 35 8 = vo = o¢s
o o oUt “4 2. © o uo
ies < OY ec 3S
o = oD E
fe} o) Fa
Vv ra = &
” S ra
a ©

2

°

2

[om

Heatmap for the Evaluation Metrics and Output Types


Subtask Counts

SA
SEM
IE
mT
QA
oT

2019.12
2019_10 >
2019_09 <q

2019_08 <d<
2019_07
2019 _06
2019_05
2019 04
2019 03
2019 02 y.
2019_01 Vv
2018.12 <

2018.11 > >

2018_10 Wy.

2018_09 o Oo

2018_08 <4

2018.07 Vv V

2018_06 <4

2018.05 > >

2018_04
2018_03
2018_02
2018_01
2017_12
2017-11
2017_10
2017_09
2017_08
2017_07
2017_06
2017_05
2017_04
2017_03 VV
2017_02 Vv

2017_01 W.

201614
201613

2016_12 <4
2016 11
2016_10
2016_09
2016_08
2016_07
2016 _06
2016_05
2016_04
2016_03
2016_02
2016_01
2015_18
2015_17
2015.15 ¥.
201514 <4
2015.13
201512
2015_11
2015_10
2015_09
2015_08
2015_07
2015_06
2015_05
2015_04
2015_03 >P
2015_02
2015_01
2014_10
2014_09 e @
2014 08

2014.07 <@

2014_06 y.
2014_05 A
2014_04 @
2014_03 VVVV
201402 V
2014.01 ¥-
2013.13
2013.12 V

2013.11 > >
2013_10

2013_09 <

2013_08 Vv

2013_07

Vv
2013_05 VVV
Vv
e

VP Ade

VV

AAA @@
A

<<<

<0

2013_04
2013_03 <
2013_02 e
2013_01 <
2012_08 A
2012_07 >
2012_06 YW.
2012_05 Vv.
2012_04
2012_03
2012_02
2012_01

A
<I <4
<q

A

KB
program

text
time interval

paragraph
word

topic

tree

DAG

number
score

score real value

score whole value
entity

phrase
attribute
answer
query
timeline

semantic graph

sentence
syntactic labeled sentence

question

document
class label

probability distribution

Figure A.2: Input Types used in SemEval tasks from 2012 to 2019
