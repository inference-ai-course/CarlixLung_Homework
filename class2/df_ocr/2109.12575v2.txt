arXiv:2109.12575v2 [cs.CL] 29 May 2022

Paradigm Shift in Natural Language Processing

Tianxiang Sun, Xiangyang Liu, Xipeng Qiu, Xuanjing Huang
School of Computer Science, Fudan University
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
{txsunl9,xiangyangliu20, xpqiu, xjhuang}@fudan.edu.cn

Abstract

In the era of deep learning, modeling for
most NLP tasks have converged to several
mainstream paradigms. For example, we usu-
ally adopt the sequence labeling paradigm to
solve a bundle of tasks such as POS-tagging,
NER, Chunking, and adopt the classification
paradigm to solve tasks like sentiment analy-
sis. With the rapid progress of pre-trained lan-
guage models, recent years have observed a ris-
ing trend of Paradigm Shift, which is solving
one NLP task by reformulating it as another
one. Paradigm shift has achieved great success
on many tasks, becoming a promising way to
improve model performance. Moreover, some
of these paradigms have shown great potential
to unify a large number of NLP tasks, making
it possible to build a single model to handle di-
verse tasks. In this paper, we review such phe-
nomenon of paradigm shifts in recent years,
highlighting several paradigms that have the
potential to solve different NLP tasks.!

1 Introduction

Paradigm is the general framework to model a
class of tasks. For instance, sequence labeling is
a mainstream paradigm for named entity recogni-
tion (NER). Different paradigms usually require
different input and output, therefore highly depend
on the annotation of the tasks. In the past years,
modeling for most NLP tasks have converged to
several mainstream paradigms, as summarized in
this paper, Class, Matching, SeqLab, MRC,
Seq2Seq, Seq2ASeq, and (M) LM.

Though the paradigm for many tasks has con-
verged and dominated for a long time, recent work
has shown that models under some paradigms also
generalize well on tasks with other paradigms. For
example, the MRC paradigm and the Seq2Seq

'A constantly updated website is publicly available at
https://txsun1997.github.io/nlp-paradigm-shift

paradigm can also achieve state-of-the-art perfor-
mance on NER tasks (Li et al., 2020; Yan et al.,
2021b), which are previously formalized in the se-
quence labeling (SeqLab) paradigm. Such meth-
ods typically first convert the form of the dataset to
the form required by the new paradigm, and then
use the model under the new paradigm to solve the
task. In recent years, similar methods that refor-
mulate a NLP task as another one have achieved
great success and gained increasing attention in the
community. After the emergence of the pre-trained
language models (PTMs) (Devlin et al., 2019; Raf-
fel et al., 2020; Brown et al., 2020; Qiu et al., 2020),
paradigm shift has been observed in an increasing
number of tasks. Combined with the power of these
PTMs, some paradigms have shown great potential
to unify diverse NLP tasks. One of these poten-
tial unified paradigms, (M) LM (also referred to as
prompt-based tuning), has made rapid progress re-
cently, making it possible to employ a single PTM
as the universal solver for various understanding
and generation tasks (Schick and Schiitze, 2021a,b;
Gao et al., 2021; Shin et al., 2020; Li and Liang,
2021; Liu et al., 2021b; Lester et al., 2021).

Despite their success, these paradigm shifts scat-
tering in various NLP tasks have not been system-
atically reviewed and analyzed. In this paper, we
attempt to summarize recent advances and trends
on this line of research, namely paradigm shift or
paradigm transfer.

This paper is organized as follows. In sec-
tion 2, we give formal definitions of the seven
paradigms, and introduce their representative tasks
and instance models. In section 3, we show re-
cent paradigm shifts happened in different NLP
tasks. In section 4, we discuss designs and chal-
lenges of several highlighted paradigms that have
great potential to unify most existing NLP tasks. In
section 5, we conclude with a brief discussion of
recent trends and future directions.


Label

Label

Encoder

Start End

Lx Lg Le

*

Encoder

Text TextA Text B A BC Context Question
(a) Class (b) Matching (c) SeqLab (d) MRC
B
DE F
hh X Decoder
Encoder Decoder Encoder sees
Encoder
, * f t * ft y FF
A BC <s> D E Text A _ Cc

(e) Seq2Seq

(f) Seq2ASeq

(g) (M)LM

Figure 1: Illustration of the seven mainstream paradigms in NLP.

2 Paradigms in NLP

2.1 Paradigms, Tasks, and Models

Typically, a task corresponds to a dataset D =
{X;, Vi}*_,. Paradigm is the general modeling
framework to fit some datasets (or tasks) with a
specific format (i.e., the data structure of VY and
y). Therefore, a task can be solved by multiple
paradigms by transforming it into different for-
mats, and a paradigm can be used to solve multiple
tasks that can be formulated as the same format. A
paradigm can be instantiated by a class of models
with similar architectures.

2.2 The Seven Paradigms in NLP

In this paper, we mainly consider the following
seven paradigms that are widely used in NLP
tasks, ie. Class, Matching, Seqhab, MRC,
Seq2ASeq, and (M) LM. These paradigms have
demonstrated strong dominance in many main-
stream NLP tasks. In the following sections, we
briefly introduce the seven paradigms and their cor-
responding tasks and models.

2.2.1 Classification (Class)

Text classification, which is designating predefined
labels for text, is an essential and fundamental task
in various NLP applications such as sentiment anal-
ysis, topic classification, spam detection, etc. In the

era of deep learning, text classification is usually
done by feeding the input text into a deep neural-
based encoder to extract the task-specific feature,
which is then fed into a shallow classifier to predict
the label, i.e.

Y = CLS(ENC(1)). (1)

Note that VY can be one-hot or multi-hot (in which
case we call multi-label classification). ENC(-) can
be instantiated as convolutional networks (Kim,
2014), recurrent networks (Liu et al., 2016), or
Transformers (Vaswani et al., 2017). CLS(-) is
usually implemented as a simple multi-layer per-
ceptron following a pooling layer. Note that the
pooling layer can be performed on the whole input
text or a span of tokens.

2.2.2 Matching

Text matching is a paradigm to predict the seman-
tic relevance of two texts. It is widely adopted in
many fields such as information retrieval, natural
language inference, question answering and dia-
logue systems. A matching model should not only
extract the features of the two texts, but also capture
their fine-grained interactions. The Matching
paradigm can be simply formulated as

yY = CLS(ENC(Xy, %)), (2)


where %, and % are two texts to be predicted,
can be discrete (e.g. whether one text entails or con-
tradicts the other text) or continuous (e.g. semantic
similarity between the two texts). The two texts
can be separately encoded and then interact with
each other (Chen et al., 2017b), or be concatenated
to be fed into a single deep encoder (Devlin et al.,
2019).

2.2.3 Sequence Labeling (SeqLab)

The Sequence Labeling (SeqLhab) paradigm (also
referred to as Sequence Tagging) is a fundamental
paradigm modeling a variety of tasks such as part-
of-speech (POS) tagging, named entity recognition
(NER), and text chunking. Conventional neural-
based sequence labeling models are comprised of
an encoder to capture the contextualized feature for
each token in the sequence, and a decoder to take
in the features and predict the labels, i.e.

where ¥1,--* , Yn are the corresponding labels of
X1,°** ,%n. ENC(-) can be instantiated as a re-
current network (Ma and Hovy, 2016) or a Trans-
former encoder (Vaswani et al., 2017). DEC(-) is

usually implemented as conditional random fields
(CRF) (Lafferty et al., 2001).

2.2.4 MRC

Machine Reading Comprehension (MRC) paradigm
extracts contiguous token sequences (spans) from
the input sequence conditioned on a given question.
It is initially adopted to solve MRC task, then is
generalized to other NLP tasks by reformulating
them into the MRC format. Though, to keep con-
sistent with prior work and avoid confusion, we
name this paradigm MRC, and distinguish it from
the task MRC. The MRC paradigm can be formally
described as follows,

where 4; and X, denote passage (also referred to
context) and query, and yz -- + yg4) iS a span from
Xp or Xy. Typically, DEC is implemented as two
classifiers, one for predicting the starting position
and one for predicting the ending position (Xiong
et al., 2017; Seo et al., 2017; Chen et al., 2017a).

2.2.5 Sequence-to-Sequence (Seq2Seq)

Sequence-to-Sequence (Seq2Seq) paradigm is a
general and powerful paradigm that can handle

a variety of NLP tasks. Typical applications of
Seq2Seq include machine translation and dia-
logue, where the system is supposed to output
a sequence (target language or response) con-
ditioned on a input sequence (source language
or user query). Seq2Seq paradigm is typi-
cally implemented by an encoder-decoder frame-
work (Sutskever et al., 2014; Bahdanau et al., 2015;
Luong et al., 2016; Gehring et al., 2017):

Different from Seqhab, the lengths of the input
and output are not necessarily the same. Moreover,
the decoder in Seq2Seq is usually more compli-
cated and takes as input at each step the previous
output (when testing) or the ground truth (with
teacher forcing when training).

2.2.6 Sequence-to-Action-Sequence
(Seq2ASeq)

Sequence-to-Action-Sequence (Seq2ASeq) is a
widely used paradigm for structured prediction.
The aim of Seq2ASeq is to predict an action se-
quence (also called transition sequence) from some
initial configuration co to a terminal configuration.
The predicted action sequence should encode some
legal structure such as dependency tree. The in-
stances of the Seq2ASeq paradigm are usually
called transition-based models, which can be for-
mulated as

A = CLS(ENC(1),C), (6)

where A = ay,--- ,@m is a Sequence of actions,
C = co,°*+ ,Cm—1 iS a sequence of configurations.
At each time step, the model predicts an action
az based on the input text and current configura-
tion c;_-1, which can be comprised of top elements
in stack, buffer, and previous actions (Chen and
Manning, 2014; Dyer et al., 2015).

2.2.7 (M) LM

Language Modeling (LM) is a long-standing task
in NLP, which is to estimate the probability of a
given sequence of words occurring in a sentence.
Due to its self-supervised fashion, language mod-
eling and its variants, e.g. masked language mod-
eling (MLM), are adopted as training objectives
to pre-train models on large-scale unlabeled cor-
pus. Typically, a language model can be simply
formulated as


where DEC can be any auto-regressive model such
as recurrent networks (Bengio et al., 2000; Grave
et al., 2017) and Transformer decoder (Dai et al.,
2019). As a famous variant of LM, MLM can be
formulated as

& = DEC(ENC(#)), (8)

where & is a corrupted version of x by replacing
a portion of tokens with a special token [MASK],
and Z denotes the masked tokens to be predicted.
DEC can be implemented as a simple classifier as
in BERT (Devlin et al., 2019) or an auto-regressive
Transformer decoder as in BART (Lewis et al.,
2020) and T5 (Raffel et al., 2020).

Though LM and MLM can be somehow different
(LM is based on auto-regressive while MLM is
based on auto-encoding), we categorize them into
one paradigm, (M) LM, due to their same inherent
nature, which is estimating the probability of some
words given the context.

2.3 Compound Paradigm

In this paper, we mainly focus on fundamental
paradigms (as described above) and tasks. Nev-
ertheless, it is worth noting that more compli-
cated NLP tasks can be solved by combining mul-
tiple fundamental paradigms. For instance, Hot-
potQA (Yang et al., 2018b), a multi-hop ques-
tion answering task, can be solved by combin-
ing Matching and MRC, where Mat ching is re-
sponsible for finding relevant documents and MRC
is responsible for selecting the answer span (Wu
et al., 2021).

3 Paradigm Shift in NLP Tasks

In this section, we review the paradigm shifts that
occur in different NLP tasks: Text Classification,
Natural Language Inference, Named Entity Recog-
nition, Aspect-Based Sentiment Analysis, Relation
Exaction, Text Summarization, and Parsing.

3.1 Text Classification

Text classification is an essential task in various
NLP applications. Conventional text classification
tasks can be well solved by the Class paradigm.
Nevertheless, its variants such as multi-label classi-
fication can be challenging, in which case Class
may be sub-optimal. To that end, Yang et al.
(2018a) propose to adopt the Seq2Seq paradigm
to better capture interactions between the labels for
multi-label classification tasks.

In addition, the semantics hidden in the labels
can not be fully exploited in the Class paradigm.
Chai et al. (2020) and Wang et al. (2021) adopt
the Matching paradigm to predict whether the
pair-wise input (4, £,) is matched, where 1 is
the original text and L, is the label description for
class y. Though the semantic meaning of a label
can be exactly defined by the samples belonging
to it, incorporating prior knowledge of the label is
also helpful when training data is limited.

As the rise of pre-trained language models
(LMs), text classification tasks can also be solved
in the (M) LM paradigm (Brown et al., 2020; Schick
and Schiitze, 2021a,b; Gao et al., 2021). By refor-
mulating a text classification task into a (masked)
language modeling task, the gap between LM pre-
training and fine-tuning is narrowed, resulting in
improved performance when training data is lim-
ited.

3.2 Natural Language Inference

Natural Language Inference (NLI) is typically mod-
eled in the Mat ching paradigm, where the two
input texts (Yq, 4) are encoded and interact with
each other, followed by a classifier to predict the
relationship between them (Chen et al., 2017b).
With the emergence of powerful encoder such as
BERT (Devlin et al., 2019), NLI tasks can be
simply solved in the Class paradigm by con-
catenating the two texts as one. In the case of
few-shot learning, NLI tasks can also be formu-
lated in the (M) LM paradigm by modifying the
input, e.g. “VY, ? [MASK] , %”. The unfilled
token [MASK] can be predicted by the MLM
head as Yes/No/Maybe, corresponding to Entail-
ment/Contradiction/Neutral (Schick and Schiitze,
2021a,b; Gao et al., 2021).

3.3 Named Entity Recognition

Named Entity Recognition (NER) is also a funda-
mental task in NLP. NER can be categorized into
three subtasks: flat NER, nested NER, and discon-
tinuous NER. Traditional methods usually solve the
three NER tasks based on three paradigms respec-
tively, i.e. SeqLab (Ma and Hovy, 2016; Lample
et al., 2016), Class (Xia et al., 2019; Fisher and
Vlachos, 2019), and Seq2ASeq (Lample et al.,
2016; Dai et al., 2020).

Yu et al. (2020) and Fu et al. (2021) solve flat
NER and nested NER with the Class paradigm.
The main idea is to predict the label for each span
in the input text. This paradigm shift introduces


*suorjoe Jo souanbes v SI jy pue 7 dojs ow) 3v UONVINSYUOS oy) st +) “ATeUTUINS aJeprpuLd suvoUT PY”? “ATIATJOAdsaI ‘AyIJUA 19afgo pu
Aynua yoalqns Joy purys fy ‘9S -gouaquas Arerixne suvsut *"o -<paanoodsei uouwtjues pue ‘uotutdo 4Yoadse uvour 1¥29/C ‘UMidoy ‘dsPy, -yoTeUaTROUOD suvaU @ ‘uoTdLIOsap
[eget suvow 7 ‘AjoANdodsor ‘Sutssao01d-jsod pue Sutssosoid-o1d oyeorpul 6 pue f “Sursred onueuiss/ooRUAS :SuIsABg “UOTeZIeWIUUNS 1X9} “MUU ‘UOT}ORINXS UONRII :AY
‘sisX[eue JUOWITJUOS poseg-joodse :WSY ‘UONTUsodII AUS PowWeU YAN ‘“USIOJUI oseNsur] [eINIeU “PIN “VONVOYISsv[d 1X0} :QL, “SYSe} qIN Ul YLYS swsipeseg :] IqQvy

(SIOZ “Te 39 speAurA ) (IZOZ “Te 39 UeD) (6107 “Te 19 ZAzNS) | (QLOT “YeruseYyD pur soy) (rLOc “Suruueyy puke usyD) | o[dwexgq
ws... ‘TRB quawnd. uf\...‘TR)B Wy)... Tp = yndin
pyrya x Up & ) T=taps é <— 0=1/74 6 v Go SUISIvg
4 MPR ‘xe wo... Te yCC x) wl?0'X) induy
beszbes UN qeTbes WT (1) besyzbes | w3rpeieg
(STOZ “Te 30 UURDOP)
(107 “Te 19 uecueleysy) (OZ0Z “Te 19 SuoYZ) (9107 ‘eiedey pue SuoyD) | ojdwexg
Ke puns C/utt ‘OF 3 "C' °C indnQ = wrung
ydwmo1g/spromsoy ‘7 Pa Purg ‘v) = cieciad’ 6 a lt ie? aa induy
WT (W) butyo en baszbes/sqeibes | wsipeieg
(1707 “Te 30 UeH) (8107 “Te 19 3uaZ) (LIOZ “Te 38 AAT) (p107 “Te 19 SuazZ) | o[duexg
()6 wal ! fqoy eqns, 1) quay “ inding
(x) pduiosd xv Ae iy = xX ynduy aa
WT (WW) beszbes OU sseTo | wSrpeseg
(1707 “Te 39 FT) (RT ZO “Te 19 UX) (1707 “Te 19 OB) (6107 “Te 39 uns) (9107 “Te 10 Sue) | o[duexg
()6 La Chee Hindoy, Paty) ques /C "UGG y "GSD ne ne indino
(xv) paulo f m xX ques Pig Ads0 es ‘vy TRS ‘y <— dss qnduy VSdv
WT (WW) beszbes OUN hutyo len SSeTO | W3Ipeieg
(1707 “Te 39 IND) (QIZOZ “Te 30 UX) (OZOZ “Te 39 TT) (1707 “Te 3 NA) (9107 ‘AAO pur vy) | o[dwexg
(6 TCM my) wade « wie | anding
Ba Ba uve) ‘y undsy, << ug? ...4Te induy WON
WT (WW) beszbes OU SSeTO qeTbes | wi3rpeseg
(81707 ‘ezINYOS pue yoryos) (STO7 “Te 39 WURDDIA1) (6107 “Te 39 Ul[Aoqd) (L107 “Te J UaYyD) | o[dwexg
(a)6 « ne Ke nding
(wy ‘Dy) qduioud ¢ (% ‘Dy) qduioud f Ww OY . dy ‘y ynduy TIN
WT (W) baszbes SSeTO butyo en | wsipereg
(RT ZOT “OZINYOS pue Yoryos) (e810? “Te 10 Suez) (OZOZ “Te 39 TeyD) (6107 “Te 19 UTA) | efdurexg
()6 ape Th {To} 3 é «| nding
(v) quo. f xX ws ‘Vv xX ynduy IL
WT (W) beszbas butyo len SSeToO | WsIpeieg
uIsipered peyjtys us Ipereg [BUISIIQ 4SBL



the span overlapping problem: The predicted en-
tities may be overlapped, which is not allowed in
flat NER. To handle this, Fu et al. (2021) adopt a
heuristic decoding method: For these overlapped
spans, only keep the span with the highest predic-
tion probability.

Li et al. (2020) propose to formulate flat NER
and nested NER as a MRC task. They reconstruct
each sample into a triplet (1, Q,, Vspan), where
& is the original text, Q,, is the question for entity
y, span is the answer. Given context, question,
and answer, the MRC paradigm can be adopted to
solve this. Since there can be multiple answers
(entities) in a sentence, an index matching module
is developed to align the start and end indexes.

Yan et al. (2021b) use a unified model based
on the Seq2Seq paradigm to solve all the three
kinds of NER subtasks. The input of the Seq2Seq
paradigm is the original text, while the output is a
sequence of span-entity pairs, for instance, Barack
Obama <Person> US <Location>”. Due to
the versatility of the Seq2Seq paradigm and the
great power of BART (Lewis et al., 2020), this uni-
fied model achieved state-of-the-art performance
on various datasets spanning all the three NER sub-
tasks.

3.4 Aspect-Based Sentiment Analysis

Aspect-Based Sentiment Analysis (ABSA) is a fine-
grained sentiment analysis task with seven subtasks,
i.e., Aspect Term Extraction (AE), Opinion Term
Extraction (OE), Aspect-Level Sentiment Classi-
fication (ALSC), Aspect-oriented Opinion Extrac-
tion (AOE), Aspect Term Extraction and Sentiment
Classification (AESC), Pair Extraction (Pair), and
Triplet Extraction (Triplet). These subtasks can be
solved by different paradigms. For example, ALSC
can be solved by the Class paradigm, and AESC
can be solved by the Seqhab paradigm.

ALSC is to predict the sentiment polarity for
each target-aspect pair, e.g. (LOCI, price), given a
context, e.g. °LOC1 is often considered the coolest
area of London’. Sun et al. (2019) formulate such
a classification task into a sentence-pair matching
task, and adopt the Mat ching paradigm to solve
it. In particular, they generate auxiliary sentences
(denoted as Sou) for each target-aspect pair. For
example, Saux for (LOC1, price) can be ” What do
you think of the price of LOC1?”. The auxiliary
sentence is then concatenated with the context as
(Saux, ), which is then fed into BERT (Devlin

et al., 2019) to predict the sentiment.

Mao et al. (2021) adopt the MRC paradigm to
handle all of the ABSA subtasks. In particular,
they construct two queries to sequentially extract
the aspect terms and their corresponding polarities
and opinion terms. The first query is ”Find the
aspect terms in the text.” Assume the answer (as-
pect term) predicted by the MRC model is AT, then
the second query can be constructed as ’Find the
sentiment polarity and opinion terms for AT in the
text.” Through such dataset conversion, all ABSA
subtasks can be solved in the MRC paradigm.

Yan et al. (2021a) solve all the ABSA subtasks
with the Seq2Seq paradigm by converting the
original label of a subtask into a sequence of to-
kens, which is used as the target to train a seq2seq
model. Take the Triplet Extraction subtask as an
example, for a input sentence, ’The drinks are
always well made and wine selection is fairly
priced’, the output target is constructed as ’drinks
well made Positive wine selection fairly priced
Positive”. Equipped with BART (Lewis et al.,
2020) as the backbone, they achieved competitive
performance on most ABSA subtasks.

Very recently, Li et al. (2021) propose to formu-
late the ABSA subtasks in the (M) LM paradigm.
In particular, for the input text V, and the aspect
A and opinion O of interest, they construct a con-
sistency prompt and a polarity prompt as: The A
is O? [MASK]. This is [MASK], where the first
[MASK] can be filled with yes or no for consistent
or inconsistent A and O, and the second [MASK]
can be filled with sentiment polarity words.

3.5 Relation Exaction

Relation Extraction (RE) has two main subtasks:
Relation Prediction (predicting the relationship r
of two given entities s and o conditioned on their
context) and Triplet Extraction (extracting triplet
(s,r, 0) from the input text). The former subtask
is mainly solved with the Class paradigm (Zeng
et al., 2014; Sun et al., 2020), while the latter sub-
task is often solved in the pipeline style that first
uses the SeqLab paradigm to extract the entities
and then uses the Class paradigm to predict the re-
lationship between the entities. Recent years have
seen paradigm shift in relation extraction, espe-
cially in triplet extraction.

Zeng et al. (2018) solve the triplet extraction task
with the Seq2Seq paradigm. In their framework,
the input of the Seq2Seq paradigm is the origi-


HG Class M8 Matching lll SeqLab lg) MRC

Class (2012) Clase (2013) Class (2014) lass (2015) lass (2016)

Matching (2012) GE} Metching (2014) Matching (2015) Matching (2076)

SeqLab (2012)

f Re (2012) [ Me (2013)

Seqlab (2014) Fl] seqta» 2015) SeqLab (2016)

MRC (2014)

Sec2Seq (2012) Sea2Seq (2014) 4

Seqaseq 2015)

Seq2ASeq (2019) £2) Seq2Asea (2014) Sec2ASeq (2015)

(Num (2074)

gum ors)

Class (2017)
Matching (2017)

Seqlab (2017)

Seq2Seq (2016) Seq2Seq (2017)

f Seqlab (2018) E

ee *|

MRC (2015) f MRC (2016) ata

(LM (2016) (WLM (2017) ;

GG Seq2Seq MB Seq2ASeq Hg (M)LM
a re | = |
o Class (2021) BEM
Class (2020)
Marching 2018) Matching 2019) ee) =|

SeqLab (2019)

‘MRC (2020)
AY
ql A re coats RS

SeqaSeq (2021)
‘SeqZSeq (2020) \

\
(MLM (2020) | on |

Sti tie | Seq2ASeq (2021) IE

Jc (2018) re (2019)
¢
i!

Seqzseq 2018) ie Saq2S0q 2019)

(wom 2019) JS

(LM are)

secaseq cn

Figure 2: Sankey diagram to depict the trend of paradigm shifting and unifying in natural language processing

tasks. In Section 3.8 we show how this diagram is drawn.

nal text, while the output is a sequence of triplets
{(71, $1, 01),°*+ (Tn; Sn; On)}. The copy mecha-
nism (Gu et al., 2016) is adopted to extract entities
in the text.

Levy et al. (2017) address the RE task via the
MRC paradigm by generating relation-specific ques-
tions. For instance, for relation educated_at(s, 0),
a question such as Where did s graduate from?”
can be crafted to query a MRC model. Moreover,
they demonstrate that formulating the RE task with
MRC has a potential of zero-shot generalization to
unseen relation types. Further, Li et al. (2019) and
Zhao et al. (2020) formulate the triplet extraction
task as multi-turn question answering and solve it
with the MRC paradigm. They extract entities and
relations from the text by progressively asking the
MRC model with different questions.

Very recently, Han et al. (2021) formulate the
RE task as a MLM task by using logic rules to
construct prompts with multiple sub-prompts. By
encoding prior knowledge of entities and rela-
tions into prompts, their proposed model, PTR,
achieved state-of-the-art performance on multiple
RE datasets.

3.6 Text Summarization

Text Summarization aims to generate a concise and
informative summary of large texts. There are two
different approaches to solve the text summariza-
tion task: Extractive Summarization and Abstrac-
tive Summarization. Extractive summarization ap-
proaches extract the clauses of the original text to

form the final summary, which usually lies in the
SeqLab paradigm. In contrast, abstractive sum-
marization approaches usually adopt the Seq2Seq
paradigm to directly generate a summary condi-
tioned on the original text.

McCann et al. (2018) reformulate the summa-
rization task as a question answering task, where
the question is ’What is the summary?”. Since
the answer (i.e. the summary) is not necessarily
comprised of the tokens in the original text, tradi-
tional MRC model cannot handle this. Therefore,
the authors developed a seq2seq model to solve the
summarization task in such format.

Zhong et al. (2020) propose to solve the ex-
tractive summarization task in the Matching
paradigm instead of the SeqLab paradigm. The
main idea is to match the semantics of the original
text and each candidate summary, finding the sum-
mary with the highest matching score. Compared
with traditional methods of extracting sentences
individually, the matching framework enables the
summary extractor to work at summary level rather
than sentence level.

Aghajanyan et al. (2021) formulate the text sum-
marization task in the (M) LM paradigm. They
pre-train a BART-style model directly on large-
scale structured HTML web pages. Due to the
rich semantics encoded in the HTML keywords,
their pre-trained model is able to perform zero-shot
text summarization by predicting the <title>
element given the <body> of the document.


Year Task Original Paradigm | Shifted Paradigm Paper

2015 Parsing Seq2ASeq Seq2Seq (Vinyals et al., 2015)
2016 Parsing Seq2ASeq (M) LM (Choe and Charniak, 2016)
2017 Relation Extraction Class MRC (Levy et al., 2017)
2018 Text Summarization SeqLab Seq2Seq (McCann et al., 2018)
2018 Parsing Seq2ASeq SeqLab (Gémez-Rodriguez and Vilares, 2018)
2018 | Natural Language Inference Matching Seq2Seq (McCann et al., 2018)
2018 Text Classification Class Seq2Seq (McCann et al., 2018)
2018 Relation Extraction Class Seq2Seq (Zeng et al., 2018)
2019 Sentiment Analysis Class Matching (Sun et al., 2019)
2019 | Natural Language Inference Matching Class (Devlin et al., 2019)
2020 | Named Entity Recognition SeqLab Class (Yu et al., 2020)

2020 | Named Entity Recognition SeqLab RC (Li et al., 2020)
2020 Text Summarization SeqLab Matching (Zhong et al., 2020)
2020 Event Extraction Class RC (Liu et al., 2020)
2020 Event Extraction Class SeqLab (Ramponi et al., 2020)
2020 Text Classification Class Matching (Yin et al., 2020)
2020 Text Classification Class (M) L (Brown et al., 2020)
2020 Question Answering MRC (M) L (Brown et al., 2020)
2020 Machine Translation Seq2Seq (M) L (Brown et al., 2020)
2020 | Natural Language Inference Matching (M) L (Brown et al., 2020)
2021 | Named Entity Recognition SeqLab Seq2Seq (Yan et al., 2021b)
2021 | Named Entity Recognition SeqLab (M) L (Cui et al., 2021)

2021 Sentiment Analysis Class RC (Mao et al., 2021)
2021 Sentiment Analysis Class Seq2Seq (Yan et al., 2021a)
2021 Sentiment Analysis Class (M) L (Schick and Schiitze, 2021a)
2021 Parsing Seq2ASeq MRC (Gan et al., 2021)

Table 2: Source data of Figure 2. We only list the first work for each paradigm shift.

3.7 Parsing

Parsing (constituency parsing, dependency pars-
ing, semantic parsing, efc.) plays a crucial role
in many NLP applications such as machine trans-
lation and question answering. This family of
tasks is to derive a structured syntactic or semantic
representation from a natural language utterance.
Two commonly used approaches for parsing are
transition-based methods and graph-based meth-
ods. Typically, transition-based methods lie in the
Seq2ASeq paradigm, and graph-based methods
lie in the Class paradigm.

By linearizing the target tree-structure to a se-
quence, parsing can be solved in the Seq2Seq
paradigm (Andreas et al., 2013; Vinyals et al., 2015;
Li et al., 2018; Rongali et al., 2020), the SeqLab
paradigm (Gémez-Rodriguez and Vilares, 2018;
Strzyz et al., 2019; Vilares and Gémez-Rodriguez,
2020; Vacareanu et al., 2020), and the (M) LM
paradigm (Choe and Charniak, 2016). In addition,
Gan et al. (2021) employ the MRC paradigm to ex-
tract the parent span given the original sentence
as the context and the child span as the question,
achieving state-of-the-art performance on depen-
dency parsing tasks across various languages.

3.8 Trends of Paradigm Shift

To intuitively depict the trend of paradigm shifts,
we draw a Sankey diagram? in Figure 2. We track
the development of the NLP tasks considered in
this section, along with several additional common
tasks such as event extraction. When a task is
solved using a paradigm that is different from its
original paradigm, some of the values of the origi-
nal paradigm are transferred to the new paradigm.
In particular, for each NLP task of interest, we col-
lect published papers that solve this task from 2012
to 2021 and denote the paradigm used in 2012 as
the original paradigm of this task. Then we track
the paradigm shifts in all the tasks with the same
original paradigm and count the number of tasks
that observed paradigm shifts until 2021. For each
paradigm, we denote NV as the total number of tasks
that branched out to new paradigms. Assume that
the initial value of each paradigm is 100, and the
transferred value for each out-branch is defined as
100/(N +1). Therefore, each branch in Figure 2
indicates a task that shifted its paradigm. Table 2

Sankey diagram is a visualization used to depict data
flows. Our sankey diagram is generated by http://sankey-
diagram-generator.acquireprocure.com.


lists the source data of Figure 2.

As shown in Figure 2, we find that: (1) The
frequency of paradigm shift is increasing in re-
cent years, especially after the emergence of pre-
trained language models (PTMs). To fully utilize
the power of these PTMs, a better way is to re-
formulate various NLP tasks into the paradigms
that PTMs are good at. (2) More and more NLP
tasks have shifted from traditional paradigms such
as Class, Seqhab, Seq2ASeq, to paradigms
that are more general and flexible, i.e, (M) LM,
Matching, MRC, and Seq2Seq, which will be
discussed in the following section.

4 Potential Unified Paradigms in NLP

Some of the paradigms have demonstrated potential
ability to formulate various NLP tasks into a unified
framework. Instead of solving each task separately,
such paradigms provide the possibility that a single
deployed model can serve as a unified solver for
diverse NLP tasks. The advantages of a single
unified model over multiple task-specific models
can be summarized as follows:

¢ Data efficiency. Training task-specific mod-
els usually requires large-scale task-specific
labeled data. In contrast, unified model has
shown its ability to achieve considerable per-
formance with much less labeled data.

Generalization. Task-specific models are
hard to transfer to new tasks while unified
model can generalize to unseen tasks by for-
mulating them into proper formats.

Convenience. The unified models are easier
and cheaper to deploy and serve, making them
favorable as commercial black-box APIs.

In this section, we discuss the following gen-
eral paradigms that have the potential to unify di-
verse NLP tasks: (M) LM, Matching, MRC, and
Seq2Seq.

4.1 (M)LM

Reformulating downstream tasks into a (M)LM
task is a natural way to utilizing the pre-trained
LMs. The original input is modified with a pre-
defined or learned prompt with some unfilled slots,
which can be filled by the pre-trained LMs. Then
the task labels can be derived from the filled tokens.
For instance, a movie review J love this movie”
can be modified by appending a prompt as ”/ love

this movie. It was [MASK]”, in which [MASK]
may be predicted as fantastic” by the LM. Then
the word ”fantastic’” can be mapped to the label
*positive’ by a verbalizer. Solving downstream
tasks in the (M) LM paradigm is also referred to
prompt-based learning. By fully utilizing the pre-
trained parameters of the MLM head instead of
training a classification head from scratch, prompt-
based learning has demonstrated great power in
few-shot and even zero-shot settings (Scao and
Rush, 2021).

Prompt. The choice of prompt is critical to
the performance of a particular task. A good
prompt can be (1) Manually designed. Brown
et al. (2020); Schick and Schiitze (2021a,b) manu-
ally craft task-specific prompts for different tasks.
Though it is heuristic and sometimes non-intuitive,
hand-crafted prompts already achieved competi-
tive performance on various few-shot tasks. (2)
Mined from corpora. Jiang et al. (2020) con-
struct prompts for relation extraction by mining
sentences with the same subject and object in the
corpus. (3) Generated by paraphrasing. Jiang
et al. (2020) use back translation to paraphrase
the original prompt into multiple new prompts.
(4) Generated by another pre-trained language
model. Gao et al. (2021) generate prompts using
TS (Raffel et al., 2020) since it is pre-trained to
fill in missing spans in the input. (5) Learned by
gradient descent. Shin et al. (2020) automatically
construct prompts based on gradient-guided search.
If prompt is not necessarily discrete, it can be op-
timized efficiently in continuous space. Recent
works (Li and Liang, 2021; Qin and Eisner, 2021;
Hambardzumyan et al., 2021; Liu et al., 2021b;
Zhong et al., 2021) have shown that continuous
prompts can also achieve competitive or even bet-
ter performance.

Verbalizer. The design of verbalizer also has a
strong influence on the performance of prompt-
based learning (Gao et al., 2021). A verbalizer can
be (1) Manually designed. Schick and Schiitze
(2021a) heuristically designed verbalizers for dif-
ferent tasks and achieved competitive results. How-
ever, it is not always intuitive for many tasks (e.g.,
when class labels not directly correspond to words
in the vocabulary) to manually design proper ver-
balizers. (2) Automatically searched on a set of
labelled data (Schick et al., 2020; Gao et al., 2021;
Shin et al., 2020; Liu et al., 2021b). (3) Con-
structed and refined with knowledge base (Hu


et al., 2021).

Parameter-Efficient Tuning Compared with
fine-tuning where all model parameters need to
be tuned for each task, prompt-based tuning is
also favorable in its parameter efficiency. Re-
cent study (Lester et al., 2021) has demonstrated
that tuning only prompt parameters while keeping
the backbone model parameters fixed can achieve
comparable performance with standard fine-tuning
when models exceed billions of parameters. Due
to the parameter efficiency, prompt-based tuning
is a promising technique for the deployment of
large-scale pre-trained LMs. In traditional fine-
tuning, the server has to maintain a task-specific
copy of the entire pre-trained LM for each down-
stream task, and inference has to be performed in
separate batches. In prompt-based tuning, only
a single pre-trained LM is required, and different
tasks can be performed by modifying the inputs
with task-specific prompts. Besides, inputs of dif-
ferent tasks can be mixed in the same batch, which
makes the service highly efficient.

4.2 Matching

Another potential unified paradigm is Mat ching,
or more specifically textual entailment (a.k.a. natu-
ral language inference). Textual entailment is the
task of predicting two given sentences, premise and
hypothesis: whether the premise entails the hypoth-
esis, contradicts the hypothesis, or neither. Almost
all text classification tasks can be reformulated as a
textual entailment one (Dagan et al., 2005; Poliak
et al., 2018; Yin et al., 2020; Wang et al., 2021).
For example, a labeled movie review {x: I love
this movie, y: positive} can be modified as {x: I
love this movie [SEP] This is a great movie, y: en-
tailment}. Similar to pre-trained LMs, entailment
models are also widely accessible. Such univer-
sal entailment models can be pre-trained LMs that
are fine-tuned on some large-scale annotated entail-
ment datasets such as MNLI (Williams et al., 2018).
In addition to obtaining the entailment model in
a supervised fashion, Sun et al. (2021) show that
the next sentence prediction head of BERT, with-
out training on any supervised entailment data, can
also achieve competitive performance on various
zero-shot tasks.

Domain Adaptation The entailment model may
be biased to the source domain, resulting in poor

>The reader is referred to Liu et al. (2021a) for a more
comprehensive survey of prompt-based learning.

generalization to target domains. To mitigate the
domain difference between the source task and the
target task, Yin et al. (2020) propose the cross-task
nearest neighbor module that matches instance rep-
resentations and class representations in the source
domain and the target domain, such that the entail-
ment model can generalize well to new NLP tasks
with limited annotations.

Label Descriptions For single sentence classifi-
cation tasks, label descriptions for each class are
required to be concatenated with the input text to
be predicted by the entailment model. Label de-
scriptions can be regarded as a kind of prompt to
trigger the entailment model. Wang et al. (2021)
show that hand-crafted label descriptions with min-
imum domain knowledge can achieve state-of-the-
art performance on various few-shot tasks. Nev-
ertheless, human-written label descriptions can be
sub-optimal, Chai et al. (2020) utilize reinforce-
ment learning to generate label descriptions.

Comparison with Prompt-Based Learning In
both paradigms ( (M) LMand Mat ching), the goal
is to reformulate the downstream tasks into the
pre-training task (language modeling or entail-
ment). To achieve this, both of them need to mod-
ify the input text with some templates to prompt
the pre-trained language or entailment model. In
prompt-based learning, the prediction is conducted
by the pre-trained MLM head on the [MASK]

token, while in matching-based learning the pre-
diction is conducted by the pre-trained classifier
on the [CLS] token. In prompt-based learning,
the output prediction is over the vocabulary, such
that a verbalizer is required to map the predicted
word in vocabulary into a task label. In contrast,
matching-based learning can simply reuse the out-
put (Entailment/Contradiction/Neutral, or Entail-
ment/NotEntailment). Another benefit of matching-
based learning is that one can construct pairwise
augmented data to perform contrastive learning,
achieving further improvement of few-shot per-
formance. However, matching-based learning re-
quires large-scale human annotated entailment data
to pre-train an entailment model, and domain dif-
ference between the source domain and target do-
main needs to be handled. Besides, matching-based
learning can only be used in understanding tasks
while prompt-based learning can also be used for
generation (Li and Liang, 2021; Liu et al., 2021b).


4.3 MRC

MRC is also an alternative paradigm to unify var-
ious NLP tasks by generating task-specific ques-
tions and training a MRC model to select the cor-
rect span from the input text conditioned on the
questions. Take NER as an example, one can rec-
ognize the organization entity in the input ’Google
was founded in 1998” by querying a MRC model
with ’Google was founded in 1998. Find organi-
zations in the text, including companies, agencies
and institutions” as in Li et al. (2020). In addition
to NER, MRC framework has also demonstrated
competitive performance in entity-relation extrac-
tion (Li et al., 2019), coreference resolution (Wu
et al., 2020), entity linking (Gu et al., 2021), de-
pendency parsing (Gan et al., 2021), dialog state
tracking (Gao et al., 2019), event extraction (Du
and Cardie, 2020; Liu et al., 2020), aspect-based
sentiment analysis (Mao et al., 2021), etc.

MRC paradigm can be applied as long as the
task input can be reformulated as context, ques-
tion, and answer. Due to its universality, McCann
et al. (2018) proposed decaNLP to unify ten NLP
tasks including question answering, machine trans-
lation, summarization, natural language inference,
sentiment analysis, semantic role labeling, rela-
tion extraction, goal-oriented dialogue, semantic
parsing, and commonsense pronoun resolution in a
unified QA format. Different from previously men-
tioned works, the answer may not appear in the
context and question for some tasks of decaNLP
such as semantic parsing, therefore the framework
is strictly not a MRC paradigm.

Comparison with Prompt-Based Learning _ It
is worth noticing that the designed question can be
analogous to the prompt in (M) LM. The verbalizer
is not necessary in MRC since the answer is a span in
the context or question. The predictor, MLM head
in the prompt-based learning, can be replaced by a
start/end classifier as in traditional MRC model or
a pointer network as in McCann et al. (2018).

4.4 Seq2Seq

Seq2Seq is a general and flexible paradigm
that can handle any task whose input and out-
put can be recast as a sequence of tokens. Early
work (McCann et al., 2018) has explored using the
Seq2Seq paradigm to simultaneously solve dif-
ferent classes of tasks. Powered by recent advances
of seq2seq pre-training such as MASS (Song et al.,
2019), TS (Raffel et al., 2020), and BART (Lewis

et al., 2020), Seq2Seq paradigm has shown its
great potential in unifying diverse NLP tasks.
Paolini et al. (2021) use T5 (Raffel et al., 2020)
to solve many structured prediction tasks including
joint entity and relation extraction, nested NER,
relation classification, semantic role labeling, event
extraction, coreference resolution, and dialogue
state tracking. Yan et al. (2021a) and Yan et al.
(2021b) use BART (Lewis et al., 2020), equipped
with the copy network (Gu et al., 2016), to unify all
NER tasks (flat NER, nested NER, discontinuous
NER) and all ABSA tasks (AE, OE, ALSC, AOE,
AESC, Pair, Triplet), respectively.

Comparison with Other Paradigms Compared
with other unified paradgms, Seq2Seq is partic-
ularly suited for complicated tasks such as struc-
tured prediction. Another benefit is that Seq2Seq
is also compatible with other paradigms such as
(M) LM (Raffel et al., 2020; Lewis et al., 2020),
MRC (McCann et al., 2018), etc. Nevertheless,
what comes with its versatility is the high latency.
Currently, most successful seq2seq models are in
auto-regressive fashion where each generation step
depends on the previously generated tokens. Such
sequential nature results in inherent latency at in-
ference time. Therefore, more work is needed
to develop efficient seq2seq models through non-
autoregressive methods (Gu et al., 2018; Qi et al.,
2021), early exiting (Elbayad et al., 2020), or other
alternative techniques.

5 Conclusion

Recently, prompt-based tuning, which is to for-
mulate some NLP task into a (M)LM task, has
exploded in popularity. They can achieve con-
siderable performance with much less training
data. In contrast, other potential unified paradigms,
ie. Matching, MRC, and Seq2Seq, are under-
explored in the context of pre-training. One of
the main reasons is that these paradigms require
large-scale annotated data to conduct pre-training,
especially Seq2Seq is notorious for data hungry.

Nevertheless, these paradigms have their advan-
tages over (M) LM: Mat ching requires less engi-
neering, MRC is more interpretable, Seq2Seq is
more flexible to handle complicated tasks. Besides,
by combining with self-supervised pre-training
(e.g. BART (Lewis et al., 2020) and T5 (Raffel
et al., 2020)), or further pre-training on annotated
data with existing language model as initializa-
tion (e.g. Wang et al. (2021)), these paradigms


can achieve competitive performance or even bet-
ter performance than (M) LM. Therefore, we argue
that more attention is needed for the exploration
of more powerful entailment, MRC, or seq2seq
models through pre-training or other alternative
techniques.

Acknowledgments

We would like to thank the members of the Fu-
danNLP group for helpful discussion and valuable
feedback. We would also like to thank Wenx-
uan Zhang from CUHK, and Zengzhi Wang from
NUST, for their help in adding important papers.
This work was supported by the National Natural
Science Foundation of China (No. 62022027).

References

Armen Aghajanyan, Dmytro Okhonko, Mike Lewis,
Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke
Zettlemoyer. 2021. HTLM: hyper-text pre-training
and prompting of language models. CoRR,
abs/2107.06955.

Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2013,
4-9 August 2013, Sofia, Bulgaria, Volume 2: Short
Papers, pages 47-52. The Association for Computer
Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Advances in Neural Information Processing Systems
13, Papers from Neural Information Processing Sys-
tems (NIPS) 2000, Denver, CO, USA, pages 932-
938. MIT Press.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual.

Duo Chai, Wei Wu, Qinghong Han, Fei Wu, and Ji-
wei Li. 2020. Description based text classification
with reinforcement learning. In Proceedings of the
37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research,
pages 1371-1382. PMLR.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017a. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers, pages 1870-1879.
Association for Computational Linguistics.

Danqi Chen and Christopher D. Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2014, October 25-29, 2014, Doha, Qatar, A
meeting of SIGDAT, a Special Interest Group of the
ACL, pages 740-750. ACL.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017b. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume I: Long
Papers, pages 1657-1668. Association for Compu-
tational Linguistics.

Jianpeng Cheng and Mirella Lapata. 2016. Neural sum-
marization by extracting sentences and words. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers. The Association for Computer Lin-
guistics.

Do Kook Choe and Eugene Charniak. 2016.  Pars-
ing as language modeling. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016, pages 2331-2336. The
Association for Computational Linguistics.

Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue
Zhang. 2021. Template-based named entity recogni-
tion using BART. In Findings of the Association for
Computational Linguistics: ACL/IJCNLP 2021, On-
line Event, August 1-6, 2021, volume ACL/IJCNLP
2021 of Findings of ACL, pages 1835-1845. Associ-
ation for Computational Linguistics.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entail-
ment challenge. In Machine Learning Challenges,
Evaluating Predictive Uncertainty, Visual Object
Classification and Recognizing Textual Entailment,
First PASCAL Machine Learning Challenges Work-
shop, MLCW 2005, Southampton, UK, April 11-
13, 2005, Revised Selected Papers, volume 3944 of
Lecture Notes in Computer Science, pages 177-190.
Springer.


Xiang Dai, Sarvnaz Karimi, Ben Hachey, and Cécile

Paris. 2020. An effective transition-based model for
discontinuous NER. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020,
pages 5860-5870. Association for Computational
Linguistics.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-

bonell, Quoc Viet Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
beyond a fixed-length context. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume I: Long Papers, pages
2978-2988. Association for Computational Linguis-
tics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Xinya Du and Claire Cardie. 2020. Event extraction by

answering (almost) natural questions. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2020, On-
line, November 16-20, 2020, pages 671-683. Asso-
ciation for Computational Linguistics.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin

Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing, ACL 2015,
July 26-31, 2015, Beijing, China, Volume 1: Long
Papers, pages 334-343. The Association for Com-
puter Linguistics.

Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael
Auli. 2020. Depth-adaptive transformer. In Sth
International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-
30, 2020. OpenReview.net.

Joseph Fisher and Andreas Vlachos. 2019. Merge and

label: A novel neural network architecture for nested
NER. In Proceedings of the 57th Conference of
the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Vol-
ume 1: Long Papers, pages 5840-5850. Association
for Computational Linguistics.

Jinlan Fu, Xuanjing Huang, and Pengfei Liu. 2021.

Spanner: Named entity re-/recognition as span pre-
diction. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguis-
tics and the IIth International Joint Conference on

Natural Language Processing, ACL/IJCNLP 2021,
(Volume 1: Long Papers), Virtual Event, August 1-6,
2021, pages 7183-7195. Association for Computa-
tional Linguistics.

Leilei Gan, Yuxian Meng, Kun Kuang, Xiaofei Sun,
Chun Fan, Fei Wu, and Jiwei Li. 2021. Dependency
parsing as mrc-based span-span prediction. CoRR,
abs/2105.07654.

Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagy-
oung Chung, and Dilek Hakkani-Tiir. 2019. Dia-
log state tracking: A neural reading comprehension
approach. In Proceedings of the 20th Annual SIG-
dial Meeting on Discourse and Dialogue, SIGdial
2019, Stockholm, Sweden, September 11-13, 2019,
pages 264-273. Association for Computational Lin-
guistics.

Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguis-
tics and the 11th International Joint Conference on
Natural Language Processing, ACL/IJCNLP 2021,
(Volume 1: Long Papers), Virtual Event, August 1-6,
2021, pages 3816-3830. Association for Computa-
tional Linguistics.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convolu-
tional sequence to sequence learning. In Proceed-
ings of the 34th International Conference on Ma-
chine Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017, volume 70 of Proceedings of Ma-
chine Learning Research, pages 1243-1252. PMLR.

Carlos Gdémez-Rodriguez and David Vilares. 2018.
Constituent parsing as sequence labeling. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 1314—
1324. Association for Computational Linguistics.

Edouard Grave, Armand Joulin, and Nicolas Usunier.
2017. Improving neural language models with a
continuous cache. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings. OpenReview.net.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O. K. Li, and Richard Socher. 2018. Non-
autoregressive neural machine translation. In 6th
International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. Open-
Review.net.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers. The
Association for Computer Linguistics.


Yingjie Gu, Xiaoye Qu, Zhefeng Wang, Baoxing Huai,
Nicholas Jing Yuan, and Xiaolin Gui. 2021. Read,
retrospect, select: An MRC framework to short text
entity linking. In Thirty-Fifth AAAI Conference on
Artificial Intelligence, AAAI 2021, Thirty-Third Con-
ference on Innovative Applications of Artificial Intel-
ligence, IAAI 2021, The Eleventh Symposium on Ed-
ucational Advances in Artificial Intelligence, EAAI
2021, Virtual Event, February 2-9, 2021, pages
12920-12928. AAAT Press.

Karen Hambardzumyan, Hrant Khachatrian, and
Jonathan May. 2021. WARP: word-level adversarial
reprogramming. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, Au-
gust 1-6, 2021, pages 4921-4933. Association for
Computational Linguistics.

Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and
Maosong Sun. 2021. PTR: prompt tuning with rules
for text classification. CoRR, abs/2105.11259.

Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan
Liu, Juanzi Li, and Maosong Sun. 2021. Knowl-
edgeable prompt-tuning: Incorporating knowledge
into prompt verbalizer for text classification. CoRR,
abs/2108.02035.

Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham

Neubig. 2020. How can we know what language
models know. Trans. Assoc. Comput. Linguistics,
8:423-438.

Yoon Kim. 2014. Convolutional neural networks for

sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746-1751. ACL.

John D. Lafferty, Andrew McCallum, and Fernando

C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), Williams College, Williamstown, MA, USA,
June 28 - July 1, 2001, pages 282-289. Morgan
Kaufmann.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-

ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL HLT 2016, The 2016 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, San Diego California, USA, June 12-17, 2016,
pages 260-270. The Association for Computational
Linguistics.

Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

The power of scale for parameter-efficient prompt
tuning. CoRR, abs/2104.08691.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction
via reading comprehension. In Proceedings of the
21st Conference on Computational Natural Lan-
guage Learning (CONLL 2017), Vancouver, Canada,
August 3-4, 2017, pages 333-342. Association for
Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871-7880, Online. Association
for Computational Linguistics.

Chengxi Li, Feiyu Gao, Jiajun Bu, Lu Xu, Xiang Chen,
Yu Gu, Zirui Shao, Qi Zheng, Ningyu Zhang, Yong-
pan Wang, and Zhi Yu. 2021. Sentiprompt: Senti-
ment knowledge enhanced prompt-tuning for aspect-
based sentiment analysis. CoRR, abs/2109.08306.

Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing, ACL/IJCNLP 2021, (Volume 1:
Long Papers), Virtual Event, August 1-6, 2021,
pages 4582-4597. Association for Computational
Linguistics.

Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong
Han, Fei Wu, and Jiwei Li. 2020. A unified MRC
framework for named entity recognition. In Pro-
ceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2020, On-
line, July 5-10, 2020, pages 5849-5859. Association
for Computational Linguistics.

Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna
Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019.
Entity-relation extraction as multi-turn question an-
swering. In Proceedings of the 57th Conference of
the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Vol-
ume 1: Long Papers, pages 1340-1350. Association
for Computational Linguistics.

Zuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao. 2018.
Seq2seq dependency parsing. In Proceedings of
the 27th International Conference on Computational
Linguistics, COLING 2018, Santa Fe, New Mexico,
USA, August 20-26, 2018, pages 3203-3214. Asso-
ciation for Computational Linguistics.

Jian Liu, Yubo Chen, Kang Liu, Wei Bi, and Xiao-
jiang Liu. 2020. Event extraction as machine read-
ing comprehension. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20,
2020, pages 1641-1651. Association for Computa-
tional Linguistics.


Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Recurrent neural network for text classification with
multi-task learning. In Proceedings of the Twenty-
Fifth International Joint Conference on Artificial In-
telligence, IJCAI 2016, New York, NY, USA, 9-15
July 2016, pages 2873-2879. IJCAI/AAAI Press.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
CoRR, abs/2107.13586.

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT
understands, too. CoRR, abs/2103.10385.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever,
Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-
task sequence to sequence learning. In 4th Inter-
national Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end se-
quence labeling via bi-directional Istm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers. The Association for Computer Lin-
guistics.

Yue Mao, Yi Shen, Chao Yu, and Longjun Cai. 2021. A
joint training dual-mrc framework for aspect based
sentiment analysis. In Thirty-Fifth AAAI Confer-
ence on Artificial Intelligence, AAAI 2021, Thirty-
Third Conference on Innovative Applications of Ar-
tificial Intelligence, IAAI 2021, The Eleventh Sym-
posium on Educational Advances in Artificial Intel-
ligence, EAAI 2021, Virtual Event, February 2-9,
2021, pages 13543-13551. AAAI Press.

Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
CoRR, abs/1806.08730.

Giovanni Paolini, Ben Athiwaratkun, Jason Krone,
Jie Ma, Alessandro Achille, Rishita Anubhai,
Cicero Nogueira dos Santos, Bing Xiang, and Ste-
fano Soatto. 2021. Structured prediction as transla-
tion between augmented natural languages. In 9th
International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7,
2021. OpenReview.net.

Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Ed-
ward Hu, Ellie Pavlick, Aaron Steven White, and
Benjamin Van Durme. 2018. Collecting diverse nat-
ural language inference problems for sentence rep-
resentation evaluation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31
- November 4, 2018, pages 67-81. Association for
Computational Linguistics.

Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu
Chen, Dayiheng Liu, Kewen Tang, Hougiang Li,
Jiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan
Duan. 2021. BANG: bridging autoregressive and
non-autoregressive generation with large scale pre-
training. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event, volume 139 of Proceed-
ings of Machine Learning Research, pages 8630-
8639. PMLR.

Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying Ims with mixtures of soft prompts.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2021, Online, June 6-11, 2021, pages
5203-5212. Association for Computational Linguis-
tics.

Xipeng Qiu, TianXiang Sun, Yige Xu, Yunfan Shao,
Ning Dai, and Xuanjing Huang. 2020. Pre-trained
models for natural language processing: A survey.
SCIENCE CHINA Technological Sciences.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21:140:1-140:67.

Alan Ramponi, Rob van der Goot, Rosario Lombardo,
and Barbara Plank. 2020. Biomedical event ex-
traction as sequence labeling. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 5357-5367. Associ-
ation for Computational Linguistics.

Subendhu Rongali, Luca Soldaini, Emilio Monti, and
Wael Hamza. 2020. Don’t parse, generate! A se-
quence to sequence architecture for task-oriented se-
mantic parsing. In WWW ’20: The Web Confer-
ence 2020, Taipei, Taiwan, April 20-24, 2020, pages
2962-2968. ACM / IW3C2.

Teven Le Scao and Alexander M. Rush. 2021. How
many data points is a prompt worth? In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2021,
Online, June 6-11, 2021, pages 2627-2636. Associ-
ation for Computational Linguistics.

Timo Schick, Helmut Schmid, and Hinrich Schiitze.
2020. Automatically identifying words that can
serve as labels for few-shot text classification.
In Proceedings of the 28th International Confer-
ence on Computational Linguistics, COLING 2020,
Barcelona, Spain (Online), December 8-13, 2020,
pages 5569-5578. International Committee on Com-
putational Linguistics.

Timo Schick and Hinrich Schiitze. 2021la. Exploit-
ing cloze-questions for few-shot text classification


and natural language inference. In Proceedings of
the 16th Conference of the European Chapter of the
Association for Computational Linguistics: Main
Volume, EACL 2021, Online, April 19 - 23, 2021,
pages 255-269. Association for Computational Lin-
guistics.

Timo Schick and Hinrich Schiitze. 2021b. It’s not
just size that matters: Small language models are
also few-shot learners. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online,
June 6-11, 2021, pages 2339-2352. Association for
Computational Linguistics.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In 5th Inter-
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,
Eric Wallace, and Sameer Singh. 2020. Autoprompt:
Eliciting knowledge from language models with au-
tomatically generated prompts. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 4222-4235. Associ-
ation for Computational Linguistics.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. MASS: masked sequence to se-
quence pre-training for language generation. In Pro-
ceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Pro-
ceedings of Machine Learning Research, pages
5926-5936. PMLR.

Michalina Strzyz, David Vilares, and Carlos Gémez-
Rodriguez. 2019. Viable dependency parsing as se-
quence labeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2019, Minneapo-
lis, MN, USA, June 2-7, 2019, Volume I (Long and
Short Papers), pages 717-723. Association for Com-
putational Linguistics.

Chi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-
lizing BERT for aspect-based sentiment analysis via
constructing auxiliary sentence. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
and Short Papers), pages 380-385, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.

Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng
Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang.

2020. Colake: Contextualized language and knowl-
edge embedding. In Proceedings of the 28th In-
ternational Conference on Computational Linguis-
tics, COLING 2020, Barcelona, Spain (Online), De-
cember 8-13, 2020, pages 3660-3670. International
Committee on Computational Linguistics.

Yi Sun, Yu Zheng, Chao Hao, and Hangping Qiu.
2021. NSP-BERT: A prompt-based zero-shot
learner through an original pre-training task—next
sentence predictio. CoRR, abs/2109.03564.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 3104-3112.

Robert Vacareanu, George Caique Gouveia Barbosa,
Marco Antonio Valenzuela-Escarcega, and Mihai
Surdeanu. 2020. Parsing as tagging. In Proceed-
ings of The 12th Language Resources and Evalu-
ation Conference, LREC 2020, Marseille, France,
May 11-16, 2020, pages 5225-5231. European Lan-
guage Resources Association.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998-6008.

David Vilares and Carlos Gdémez-Rodriguez. 2020.
Discontinuous constituent parsing as sequence la-
beling. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2020, Online, November 16-20, 2020,
pages 2771-2785. Association for Computational
Linguistics.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey E. Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neu-
ral Information Processing Systems 28: Annual
Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, pages 2773-2781.

Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma. 2021. Entailment as few-shot learner.
CoRR, abs/2104.14690.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016. Attention-based LSTM for aspect-
level sentiment classification. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016, pages 606-615. The As-
sociation for Computational Linguistics.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2018. A broad-coverage challenge corpus


for sentence understanding through inference. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2018, New Orleans, Louisiana, USA,
June 1-6, 2018, Volume I (Long Papers), pages
1112-1122. Association for Computational Linguis-
tics.

Bohong Wu, Zhuosheng Zhang, and Hai Zhao. 2021.
Graph-free multi-hop reading comprehension: A
select-to-guide strategy. CoRR, abs/2107.11823.

Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Ji-
wei Li. 2020. Corefga: Coreference resolution as
query-based span prediction. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2020, Online, July 5-10,
2020, pages 6953-6963. Association for Computa-
tional Linguistics.

Congying Xia, Chenwei Zhang, Tao Yang, Yaliang Li,
Nan Du, Xian Wu, Wei Fan, Fenglong Ma, and
Philip S. Yu. 2019. Multi-grained named entity
recognition. In Proceedings of the 57th Confer-
ence of the Association for Computational Linguis-
tics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers, pages 1430-1440. As-
sociation for Computational Linguistics.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for ques-
tion answering. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings. OpenReview.net.

Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng
Zhang. 2021a. A unified generative framework for
aspect-based sentiment analysis. In Proceedings of
the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACL/TJCNLP 2021, (Volume I: Long Papers), Vir-
tual Event, August 1-6, 2021, pages 2416-2429. As-
sociation for Computational Linguistics.

Hang Yan, Tao Gui, Jungi Dai, Qipeng Guo, Zheng
Zhang, and Xipeng Qiu. 2021b. A unified genera-
tive framework for various NER subtasks. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing, ACL/IJCNLP 2021, (Volume 1: Long
Papers), Virtual Event, August 1-6, 2021, pages
5808-5822. Association for Computational Linguis-
tics.

Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei
Wu, and Houfeng Wang. 2018a. SGM: sequence
generation model for multi-label classification. In
Proceedings of the 27th International Conference
on Computational Linguistics, COLING 2018, Santa
Fe, New Mexico, USA, August 20-26, 2018, pages
3915-3926. Association for Computational Linguis-
tics.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018b. Hotpotqa: A
dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018, pages 2369-2380. Association for Computa-
tional Linguistics.

Wenpeng Yin, Nazneen Fatema Rajani, Dragomir R.
Radev, Richard Socher, and Caiming Xiong. 2020.
Universal natural language processing with limited
annotations: Try few-shot textual entailment as a
start. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2020, Online, November 16-20, 2020,
pages 8229-8239. Association for Computational
Linguistics.

Juntao Yu, Bernd Bohnet, and Massimo Poesio. 2020.
Named entity recognition as dependency parsing. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 6470-—
6476, Online. Association for Computational Lin-
guistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via
convolutional deep neural network. In COLING
2014, 25th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, August 23-29, 2014, Dublin, Ire-
land, pages 2335-2344. ACL.

Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,
and Jun Zhao. 2018. Extracting relational facts by
an end-to-end neural model with copy mechanism.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2018,
Melbourne, Australia, July 15-20, 2018, Volume 1:
Long Papers, pages 506-514. Association for Com-
putational Linguistics.

Tianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun
Li. 2020. Asking effective and diverse questions:
A machine reading comprehension based framework
for joint entity-relation extraction. In Proceedings
of the Twenty-Ninth International Joint Conference
on Artificial Intelligence, IJCAI 2020, pages 3948—
3954. ijcai.org.

Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,
Xipeng Qiu, and Xuanjing Huang. 2020. Extrac-
tive summarization as text matching. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 6197-6208, On-
line. Association for Computational Linguistics.

Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [MASK]: learning vs. learning
to recall. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11,


2021, pages 5017-5033. Association for Computa-
tional Linguistics.
