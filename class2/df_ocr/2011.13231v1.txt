arXiv:2011.13231v1 [cs.CL] 26 Nov 2020

NLPStatTest: A Toolkit for Comparing NLP System Performance

Haotian Zhu Denise Mak

Jesse Gioannini Fei Xia

University of Washington, Seattle, USA
{haz060, dpm3, jessegio, fxia}@uw.edu

Abstract

Statistical significance testing centered on p-
values is commonly used to compare NLP sys-
tem performance, but p-values alone are insuf-
ficient because statistical significance differs
from practical significance. The latter can be
measured by estimating effect size. In this pa-
per, we propose a three-stage procedure for
comparing NLP system performance and pro-
vide a toolkit, NLPStatTest, that automates
the process. Users can upload NLP system
evaluation scores and the toolkit will analyze
these scores, run appropriate significance tests,
estimate effect size, and conduct power analy-
sis to estimate Type II error. The toolkit pro-
vides a convenient and systematic way to com-
pare NLP system performance that goes be-
yond statistical significance testing.

1 Introduction

In the field of natural language processing (NLP),
the common practice is to use statistical signifi-
cance testing ' to demonstrate that the improvement
exhibited by a proposed system over the baseline
reflects meaningful differences, not happenstance
(Dror et al., 2018, 2020). The American Statistical
Association emphasizes that ‘‘a p-value, or statis-
tical significance, does not measure the size of an
effect or the importance of a result” (Wasserstein
and Lazar, 2016). In other words, statistical signifi-
cance is different from practical significance. The
latter is rarely discussed in the NLP field.

To address this issue, we propose a three-stage
procedure for comparing NLP system performance,
shown in Figure 1. The first stage is building an
NLP system and using prospective power analysis
to compute an appropriate sample size for test cor-
pus. The second stage is hypothesis testing. We

'Here we adopt the frequentist approach to hypothesis
testing. The debate over frequentist and Bayesian is beyond
the scope of this paper.

stress the need for data analysis to verify assump-
tions made by significance tests and the importance
of estimating the effect size and conducting power
analysis. The last stage is to report various results
produced by the second stage.

To automate the process, we provide a toolkit,
NLPStatTest. We introduce the three-stage
comparison procedure (§2), and then describe the
the main components (§3) and implementation de-
tails ($4) of NLPStatTest. We also present ex-
perimental results for running the system on both
real-world and simulated data (§5). Lastly, we
compare NLPStatTest with existing statistical
testing toolkits (§6).

2 Comparing NLP System Performance

In this section we briefly describe the three-stage
comparison procedure and define terms that are rel-
evant to NLPStatTest. More detail about Stage
2 can be found in §3-§4.

2.1 Building an NLP System

The first stage is to build an NLP system, run it
on test data, and compare the system output with a
gold standard. The output of this stage is a list of
numerical values such as accuracy or F-scores.
Definition 1 (Evaluation unit). Let (x;,y;) be
a test instance. An evaluation unit (EU) e =
{(xj,¥j),J = 1,--- ,m} is a set of test instances
on which an evaluation metric can be meaningfully
defined. A test set is a set of EUs.

Definition 2 (Evaluation metric). Given an NLP
system A, the evaluation metric M is a function
that maps an EU e to a numerical value:

Male) = M({(Gisus)s 9 =1,-++ sm} ) 4)

where y; = A(x;) is the system output of A given
xj, and m is the size of e (i.e., the number of test
instances in e).


NLP System
Building

Identify the NLP task
choose evaluation metric

1
Hi 1
‘ 1
' 1
1

1
: 1

Prospective power
analysis

—> skewness

i : H check

i Process the corpus ; i

: produce system output : Hl [romans |

: : } normality test

i Evaluate output w.r.t. d i

: gold standard and metrics]; i

list of recommended tests

p-value or Cl

Reporting

effect size estimate

(4) Retrospective

power Analysis p-value or Cl

 stetsz ne)
effect size estimate

eu

Figure 1: The three-stage procedure for comparing NLP system performance. The pink flag boxes are the parame-
ters that users can either set or use the default values provided by NLPStatTest. The blue hexagons are system
output of NLPStatTest. a, and az are the significance levels for normality test and statistical significance test

respectively. EU stands for evaluation unit.

An EU may contain one or more test instances.
For example, a BLEU score can be computed on
one or more sentences. The EU size affects sample
size, p-value, sample standard deviation, effect size
and so on. It is therefore one of the parameters that
users can set when using NLPStatTest.

2.2 The Comparison Stage

The second stage is the comparison stage which
has four steps (see the largest box in Figure 1).

2.2.1 Data Analysis

When we compare two NLP systems A and
B, the output of Stage 1 is a set of pairs,
{(Ma(e;), Mp(e;))}, where e; is the i*” EU, and
Ma(e) (similarly Mp(e)) is defined in Equation 1.

Many statistical tests make certain assumptions
about the sample (e.g., normality for ¢ test), so it is
important to conduct data analysis to verify those
assumptions in order to choose significance tests
that are appropriate for a particular sample. If the
sample does not follow any known distribution,
non-parametric tests should be used.

NLPStatTestwill estimate sample skewness
and test for normality. Then NLPStatTest will
choose a test statistic (mean or median) for users
and recommend a list of significance tests.

2.2.2 Statistical Significance Testing

The second step in Stage 2 is statistical significance
testing, using two mutually exclusive hypotheses:
the null hypothesis Ho and the alternative H). To
compare two NLP systems, a (paired) two-sample
test is usually used, though one-sample testing of

pairwise difference is equivalent. NLPStatTest
currently only considers paired two-sample testing
for numerical data. Observations within a sam-
ple are assumed to be independent and identically
distributed (7.2.d.).

To run a significance test, users first choose
the direction of the test: left-sided, right-sided or
two-sided. Then, users specify the hypothesized
value of test statistic difference 6 and the signifi-
cance level a, which is often set to 0.05 or 0.01
in the NLP field, and choose a test from the list.
NLPStatTest will calculate the p-value and re-
ject Ho if and only ifp <a.

2.2.3 Effect Size Estimation

In most experimental NLP papers employing sig-
nificance testing, the p-value is the only quantity
reported. However, the p-value is often misused
and misinterpreted. For instance, statistical signifi-
cance is easily conflated with practical significance;
as a result, NLP researchers often run significance
tests to show that the performances of two NLP
systems are different (i.e., statistical significance),
without measuring the degree or the importance of
such a difference (i.e., practical significance).
Cohen (1990) noted “the null hypothesis, if taken
literally, is always false in the real world.” For in-
stance, because evaluation metric values of two
NLP systems on a test set are almost never exactly
the same, Hp that two systems perform equally
is (almost) always false. When Hp is false, the
p-value will eventually approach zero in large sam-
ples (Lin et al., 2013). In other words, no mat-


ter how tiny the system performance difference is,
there is always a large enough dataset on which
the difference is statistically significant. Therefore,
Statistical significance is markedly different from
practical significance.

One way to measure practical significance is
by estimating effect size, which is defined as the
degree to which the ‘phenomenon’ is present in
the population: the degree to which the null hy-
pothesis is false (Cohen, 1994). While the need
to estimate and report effect size has long been
recognized in other fields (Tomezak and Tomczak,
2014), the same is not true in the NLP field. We
include several methods for estimating effect size
in NLPStatTest (see §3.3).

2.2.4 Power Analysis

There are two types of errors in hypothesis testing:
Type I errors (false positives) and Type II errors
(false negatives). The Type I error of a signifi-
cance test, often denoted by a, is the probability
that, when Ho is true, Ho is rejected by the test.
The Type II error of a significance test, usually de-
noted by £, is the probability that under Hy, Hy
is rejected by the test. While Type I error can be
controlled by predetermining the significance level,
Type II error can be controlled or estimated by
power analysis.

Definition 3 (Statistical power). The power of a
statistical significance test is the probability that
under H;, Ho is correctly rejected by the test. The
power of a testis 1 — f.

Higher power means that statistical inferences
are more correct and accurate (Perugini et al.,
2018). While power analysis is rarely used in the
NLP field, it is considered good or standard prac-
tice in some other scientific fields such as psychol-
ogy and clinical trials in medicine (Perugini et al.,
2018). We implement two methods of conducting
power analysis in NLPStatTest(see §3.4).

2.3 Reporting Test Results

Beyond the p-value, it is important to report other
quantities to make the studies reproducible and
available for meta-analysis, including the name of
significance test used, the predetermined signifi-
cance level a, effect size estimate/estimator, the
sample size, and statistical power.

3 System Design

NLPStatTest is a toolkit that automates the
comparison procedure. It has four main steps,
shown in the large box in Figure 1. To use
NLPStatTest, users provide a data file with the
NLP system performance scores produced in Stage
1. NLPStatTest will prompt users to either mod-
ify or use the default values for the parameters in
the pink flags and then produce the output in the
blue hexagons. The users can then report (some of)
the output in Stage 3 of the comparison procedure.

3.1 Data Analysis

The first step of the comparison stage is data analy-
sis, and a screenshot of this step is shown in Figure
2. The top part (above the Run button in the purple
box) shows the input and parameters that the user
needs to provide, and the bottom part (below the
Run button in the green box) shows the output of
the data analysis step.

3.1.1 The Input Data File

To compare two NLP systems, A and B, users need
to provide a data file where each line is a pair of nu-
merical values. There are two scenarios. In the first
scenario, the pair is (u;, vi), where wu; = Ma(e;)
is the evaluation metric value (e.g., accuracy or F-
score) of an EU e; given System A (see Equation
1), and v; = Mp(e;).

In the second scenario, if wu; and v; can be calcu-
lated as the mean or the median of the evaluation
metric values of test instances in e;, users can up-
load a data file where each line is a pair of (ap, bz),
where a, and 6; are the evaluation metric values
of a test instance t, given System A and B, re-
spectively. Users then chooses the EU size m and
specifies whether the EU metric value should be
calculated as the mean or the median of the metric
values of the instances in the EU. NLPStatTest
will use ™m adjacent lines in the file to calculate
u,; and v;. If users prefers to randomly shuffle the
lines before calculating u,; and v;, they can provide
a seed for random shuffling.

3.1.2 Histograms and Summary Statistics

From the (u;, v;) pairs, NLPStatTest generates
descriptive summary statistics (e.g., mean, me-
dian, standard deviation) and histograms of three
datasets, {u;}, {v;}, and {u; — v;}, as shown in the
first table and the three histograms in Figure 2.


Data Analysis

Many statistics tests make certain assumptions about the sample.
For example, the t test assumes normality. In order to choose

significance tests that are appropriate for this particular sample,
the system will estimate sample skewness and test normality.

Evaluation unit size:
Gi

Choose a metric to represent each evaluation unit:
@® Mean
) Median

Random Seed:
[Do not shuffle |

Significance level threshold (for calculating normality):
a = 0.05

© List less-preferred significance tests.
©) List inappropriate significance tests.

| Run

Summary of Statistics

Score Mean Median i Minimum Maximum
Column 1 0.28464 0.25552 0.15964 0.02206 1.00000
Column 2 0.28051 0.24905 0.15694 0.01216 1.00000
Difference 0.00413 0.00070 0.11228 -0.83215 0.51108

[ View Histograms

Test Statistic Recommendation

Property Conclusion

The data distribution does not pass the normality

Normality rect

The skewness measure Y is -0.2637. This means the

Skewness Bg inches f ‘
data distribution is roughly symmetric.

Test Based on the skewness, the recommended test
Statistic statistic to use is: mean.

Recommended Significance Tests

The following tests are appropriate and preferred:

Test Reason

The Wilcoxon signed rank test is appropriate
since it does not assume any specific
distribution but only requires symmetry.

Wilcoxon
Signed Rank

Figure 2: Screenshot of the data analysis step. The part
above the Run button are parameters that users can set,
and the part below is NLPStatTest output.

3.1.3 Central Tendency Measure

Many statistical tests (¢ test, bootstrap test based
on t ratios, etc) are based on the mean as the test
Statistic, drawing inferences on average system per-
formance. However, when the data distribution is
not symmetric, the mean does not properly mea-
sure the central tendency. In that case, the median
is amore robust measure. Another issue associated
with mean is that if the distribution is heavy-tailed
(e.g., the ¢ and Cauchy distributions), the sample
mean oscillates dramatically.

In order to examine the symmetry of the un-
derlying distribution, NLPStatTest checks the
skewness of {u; — v;} by estimating the sample
skewness (7). Based on the y value, we use the
following rule of thumb (Bulmer, 1979) to deter-
mine whether NLPStatTest would recommend
the use of mean or median as the test statistic for
statistical significance testing:

° |y| € [0,0.5): roughly symmetric (use mean)
* |y| © [0.5, 1): slightly skewed (use median)
* |y| € [1, 00): highly skewed (use median)

3.1.4 Normality Test

To choose a good significance test for {u;—vu;}, we
need to determine if the data is normally distributed.
If it is, ¢ test is the most appropriate (and powerful)
test; if not, then non-parametric tests which do not
assume normality might be more appropriate.

If a distribution is skewed according to y, there
is no need to run normality test as the data is not
normally distributed. For a non-skewed distribu-
tion, NLPStatTest will run the Shapiro-Wilk
normality test (Shapiro and Wilk, 1965), which is
itself a test of statistical significance. The user can
choose the significance level (a1 in Figure 1).

3.1.5 Recommended Significance Tests

Based on the skewness check and normality test
result, NLPStatTest will automatically choose
a test statistic (mean or median) and recommend a
list of appropriate significance tests (e.g., ¢ test if
{u; — v;} is normally distributed).

3.2 Testing

In this step, the user sets the significance level (a2
in Figure 1) and chooses a significance test from
the ones recommended in the previous step. If the
test has any parameter (e.g., the number of trials for
bootstrap testing B), NLPStatTest will suggest


a default value which can be changed by users.
NLPStatTest will then run the test, calculate a
p-value (and/or provide a confidence interval), and
reject Ho if p < ag.

3.3 Effect Size

Effect size can be estimated by different effect size
indices, depending on the data types (numerical
or categorical) and significance tests. Dror et al.
(2020) defined effect size as the unstandardized dif-
ference between system performance, while Hauch
et al. (2012) and Pimentel et al. (2019) used the
standardized difference.

NLPStatTest implements the following four
indices. Once users select one or more,
NLPStatTest will calculate effect size accord-
ingly and display the results.

Cohen’s d estimates the standardized mean differ-
ence by

) - o (2)
where @ and % are the sample means and c denote
standard deviation of u — v. Cohen’s d assumes
normality and is one of the most frequently used
effect size indices. If Cohen’s d, or any other effect
size indices depending on G, is used to estimate
effect size, the EU size will affect the standard
deviation and thus effect size estimate.

d=

Hedges’ g adjusts the bias brought by Cohen’s d
in small samples by the following:

3
4n —9

where 7 is the size of {u; — v;}.

g=d-(1-

) (3)

Wilcoxon r is an effect size index for the Wilcoxon
signed rank test, calculated as r = wat where
W —n(n+1)/4

Z=
J n(nti)(2n+1) _ Der tt
24 48

(4)

Here, W is the test statistic for Wilcoxon signed
rank test and T'is the set of tied ranks.

Hodges-Lehmann Estimator (Hodges and
Lehmann, 1963) is an estimator for the median.
Let wu; = u; — v;. The HL estimator for
one-sample testing is given by

AL median {(u +w;)/2,iF i) (5)

3.4 Power Analysis

Power (Definition 3) covaries with sample size,
effect size and the significance level a. In par-
ticular, power increases with larger sample size,
effect size, and a. There are two common types
of power analysis, namely prospective and retro-
spective power analysis, and NLPStatTest im-
plements both types.

3.4.1 Prospective Power Analysis

Prospective power analysis is used when planning
a study (usually in clinical trials) in order to decide
how many subjects are needed. In the NLP field,
when one constructs or chooses a test corpus for
evaluation, it will be beneficial to conduct this type
of power analysis to determine how big a corpus
needs to be in order to ensure that the significance
test reaches the desired power level.

In NLPStatTest, prospective power analysis
is a preliminary and optional step. The user needs
to provide the expected mean and standard devia-
tion of the differences between samples, the desired
power level, and the required significance level.
NLPStatTest will calculate the minimally re-
quired sample size for ¢ test via a closed form,
assuming the normal distribution of the data.

3.4.2 Retrospective Power Analysis

Retrospective or post-hoc power analysis is usually
done after a significance test to determine the rela-
tion between sample size and power. There are two
scenarios associated with retrospective power anal-
ysis: When the values in {u;—v;} are from a known
distribution, one can use Monte Carlo simulation
to directly simulate from this known distribution.
To do this, one has to have an informed guess of
the desired effect size (i.e., mean difference) via
meta-analysis of previous studies.

When the distribution of the sample is unknown
a priori, one can resample with replacement from
the empirical distribution of the sample (a.k.a. the
bootstrap method (Efron and Tibshirani, 1993)) to
estimate the power.

NLPStatTest implements both methods.
Users can employ one or both; NLPStatTest
will produce a figure that shows the relation be-
tween sample size and power, as in Figure 3.

4 Implementation Details

The NLPStatTest graphical user interface can
be run locally or on the Web. There is also a com-


Retrospective Power Analysis

A power curve shows how the statistical power increases as sample size
increases. For example, using 100 evaluation units with 5 power
measurements, this test calculates the power for partitions of the data into
20, 40, 60, 80 and 100 evaluation units. At each of these sample sizes, the
power analysis is done using either a bootstrap simulation or a Monte Carlo
simulation. The Monte Carlo simulation is only available for normal data.

Number of power measurements:
20

Number of iterations at each sample size:
200

Bootstrap Parameters

Significance test:
® Wilcoxon signed rank test

Significance level:
Q = 0.05

Iterations for bootstrap significance test:
200

[ Run

Results

Power Against Different Sample Sizes for 'Wilcoxon Signed Rank Test’

0.64

0.54

0.44

Power

0.34

0.24

0.14

T T T r ‘ r T T
250 500 750 1000 1250 1500 1750 2000
Sample Size

Figure 3: Screenshot for retrospective power analysis.

Sogou - 9.05

uedin -
xmunmt -
online-B -
online-A -
NRC -

jhu -
afri-mitll -
CASICT -
ROCMT -
OSU -
PROMT -
NMT -

UU -
online-F -
online-G -

- 0.04

- 0.03

0.02

0.01

0.00

Sogou -
uedin -
xmunmt -
online-A -
NRC -

jhu -
afrl-mitll -
CASICT -
ROCMT -
OSU -
online-F -
online-G -

'
a
ov
£
3

Figure 4: Heatmap of pairwise comparison for the 16
WMT-2017 Chinese-to-English MT systems. BLEU
scores and Wilcoxon signed-rank test are used. p-
values are adjusted via Bonferroni correction. Dark
green cells indicate statistical significance (p < 0.05);
light green cells indicate non-significance (p > 0.05).

mand line version. The graphical tool, the com-
mand line tool, the source code, a user manual,
a tutorial video are available at nlpstats.ling.
washington.edu. We recommend using an up-
dated Chromium-based browser.

The client-side web interface is written in
HTML, CSS, and JavaScript (with JQuery). The
server-side code is written in Python, using the
Flask web framework. YAML is used for configu-
ration files. KaTeX is used to render mathematical
symbols. The Python code uses the SciPy and
NumPy libraries to implement statistical tests and
Matplotlib to generate the histograms and graphs.

5 Experiment

To test the output validity and speed of
NLPStatTest, we run experiments using both
real and simulated data.

5.1 Real Data from WMT-2017

The WMT-2017 shared task (Bojar et al., 2017)
reported system performance results based on hu-
man evaluation scores; unpaired testing (Wilcoxon
rank-sum) was used because not many sentences
had human evaluation scores for both MT systems
that were being compared.

Because NLPStatTest currently implements
paired testing only, we use the Wilcoxon signed-
rank test (instead of Wilcoxon rank-sum test) and
the BLEU scores (instead of human evaluation
scores) when comparing MT systems. According
to Bojar et al. (2017), a set of 15 or more sentence-
level evaluation scores constitutes a reliable mea-


(a) N(O,1) vs. N(0,1) (b) N(0.05,1) vs. N(0,1)

SSS
(d) Beta(1,4) vs. Beta(1,5)

10000 20000
Sample Size

20000 0

0 10000
Sample Size

Figure 5: Plots of p-value against sample size. Figure
(a) and (b) use two samples with normal distribution,
while (c) and (d) use Beta distribution. Ho should be
true for (a) and (c) and false for (b) and (d). We run ¢
test for (a) and (b), and Wilcoxon signed-rank test for
(c) and (d). The red dotted line stands for the threshold
a = 0.05. The light purple shade depicts the range
of p-values. The solid blue line denotes the mean of
p-values for each sample size.

sure of translation quality; thus, we set the EU
size to be 15. We also reshuffled the scores before
grouping test instances into evaluation units.
Figure 4 shows the results of pairwise compar-
isons among all 16 Chinese-to-English MT systems
(120 system pairs in total). The heatmap is similar
to the comparison results in Bojar et al. (2017) (see
Figure 5 in that paper). The minor differences of
the two heatmaps are due to different evaluation
metrics (BLEU vs. human scores), the significant
tests (Wilcoxon signed-rank vs. Wilcoxon rank-
sum), and the numbers of EUs (more test sentences
have BLEU scores than human evaluation scores).

5.2 Simulated Data

We also run simulation experiments on
NLPStatTest to validate the testing re-
sults. Here, we conduct two-sided, paired testing,
varying sample size from 30 to 25,000, each
with 20 iterations of tests to obtain a range of
p-values. As shown in Figure 5, when Ho is true
(see Fig 5(a) and 5(c)), p-values range freely
in (0,1). When Hp is false (see 5(b) and 5(d)),
p-values approach zero as sample size increases,
as expected. The fast convergence to zero in 5(d)
may be due to the small variance of the differences
between the two Beta samples (~ 0.046), even
though the difference between sample medians is
small (~ 0.02). In contrast, 5(b) converges to zero
much more slowly due to the large variance.

6 Related Work

Dror et al. (2018) made an accompanying pack-
age available * for hypothesis testing. This pack-
age includes functionalities such as testing for
normality, ¢ testing, permutation/bootstrap test-
ing, and using McNemar’s test for categorical
data. NLPStatTest implements all the afore-
mentioned tests except McNemar’s test. In addi-
tion, NLPStatTest offers data analysis, effect
size estimation, power analysis and graphical inter-
face.

NLPStatTest is based on the frequentist ap-
proach to hypothesis testing. Sadeqi Azer et al.
(2020) developed a Bayesian system? which uses
the Bayes factor to determine the posterior proba-
bility of Ho being true or false.

7 Conclusion

While statistical significance testing has been com-
monly used to compare NLP system performance,
a small p-value alone is not sufficient because sta-
tistical significance is different from practical sig-
nificance. To measure practical significance, we
recommend estimating and reporting of effect size.
It is also necessary to conduct power analysis to en-
sure that the test corpus is large enough to achieve
a desirable power level. We propose a three-stage
procedure for comparing NLP system performance,
and provide a toolkit, NLPStat Test, to automate
the testing stage of the procedure. For future work,
we will extend this work to hypothesis testing with
multiple datasets or multiple metrics.

References

O. Bojar, R. Chatterjee, C. Federmann, Y. Graham,
B. Haddow, S. Huang, M. Huck, P. Koehn, Q. Liu,
V. Logacheva, C. Monz, M. Negri, M. Post, R. Ru-
bino, L. Specia, and M. Turchi. 2017. — Find-
ings of the 2017 conference on machine translation
(WMT17). In Proceedings of the Second Confer-
ence on Machine Translation, Volume 2: Shared
Task Papers, pages 169-214, Copenhagen, Denmark.
Association for Computational Linguistics.

M. G. Bulmer. 1979. Principles of Statistics, page 57.
Dover, New York.

J. Cohen. 1990. Things I have learned (so far). Ameri-
can Psychologist, 45(12):1304 — 1312.

*https://github.com/rtmdrr/
testSignificanceNLP
Shttps://github.com/allenai/HyBayes


J. Cohen. 1994. The earth is round (p < .05). American
Psychologist, pages 997-1003.

R. Dror, G. Baumer, S. Shlomov, and R. Reichart. 2018.
The hitchhiker’s guide to testing statistical signifi-
cance in natural language processing. In Proceed-
ings of ACL-2018 (Volume 1: Long Papers), pages
1383-1392, Melbourne, Australia.

R. Dror, L. Peled-Cohen, S. Shlomov, and R. Reichart.
2020. Statistical Significance Testing for Natural
Language Processing. Morgan & Claypool.

B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York, NY.

V. Hauch, I. Bland6n-Gitlin, J. Masip, and S. L. Sporer.
2012. Linguistic cues to deception assessed by com-
puter programs: A meta-analysis. In Proceedings of
the Workshop on Computational Approaches to De-
ception Detection, pages 1-4, Avignon, France.

J. L. Hodges and E. L. Lehmann. 1963. Estimates of lo-
cation based on rank tests. Annals of Mathematical
Statistics, 34(2):598-611.

M. Lin, H. Lucas, and G. Shmueli. 2013. Too big to
fail: Large samples and the p-value problem. Jnfor-
mation Systems Research, 24:906-917.

M. Perugini, M. Gallucci, and G. Costantini. 2018. A
practical primer to power analysis for simple experi-
mental designs. International Review of Social Psy-
chology, 31.

T. Pimentel, A. D. McCarthy, D. Blasi, B. Roark, and
R. Cotterell. 2019. Meaning to form: Measuring sys-
tematicity as information. In Proceedings of ACL-
2019, pages 1751-1764, Florence, Italy.

E. Sadeqi Azer, D. Khashabi, A. Sabharwal, and
D. Roth. 2020. Not all claims are created equal:
Choosing the right statistical approach to assess hy-
potheses. In Annual Meeting of the Association for
Computational Linguistics (ACL).

S. S. Shapiro and M. B. Wilk. 1965. An analysis
of variance test for normality (complete samples)‘.
Biometrika, 52(3-4):591-611.

M. Tomezak and E. Tomczak. 2014. The need to re-
port effect size estimates revisited. An overview of
some recommended measures of effect size. Trends
in Sport Sciences, 21:19-25.

R. L. Wasserstein and N. A. Lazar. 2016. The ASA
statement on p-values: Context, process, and pur-
pose. The American Statistician, 70(2):129-133.
