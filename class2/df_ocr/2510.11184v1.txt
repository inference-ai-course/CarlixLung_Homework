arXiv:2510.11184v1 [cs.LG] 13 Oct 2025

Under Review

CAN TOOL-INTEGRATED REINFORCEMENT LEARNING
GENERALIZE ACROSS DIVERSE DOMAINS?

Zhengyu Chen’; Jinluan Yang”; Teng Xiao®, Ruochen Zhou“, Luan Zhang!
Xiangyu Xi!, Xiaowei Shi!, Wei Wang’, Jinggang Wang!

! Meituan ? Zhejiang University * Allen Institute for Artificial Intelligence

4 City University of Hong Kong

chencsmat@gmail.com, yangjinluan@zju.edu.cn

ABSTRACT

Recent advances in large language models (LLMs) have demonstrated remark-
able capabilities in reasoning and tool utilization. However, the generalization of
tool-augmented reinforcement learning (RL) across diverse domains remains un-
derexplored. In this work, we investigate the cross-domain generalization of an
LLM agent equipped with a code interpreter tool, which is exclusively trained on
mathematical problem-solving tasks. Despite the restricted training domain, we
evaluate the agent’s performance across several distinct reasoning domains. The
results reveal that RL-based tool usage learned from mathematical tasks can be
effectively transferred to complex tasks in other domains, enabling great task per-
formance and high token efficiency. To facilitate this cross-domain transfer, we
propose a Tool Generalization Reinforcement Learning (TGRL) framework de-
signed to promote domain-agnostic learning and skill migration, encompassing:
(i) a standardized tool interface that abstracts domain-specific nuances through
consistent formatting and explicit termination, fostering transferable invocation
patterns; (ii) a dual-component reward system that decomposes rewards to in-
centivize generalizable behaviors like tool efficiency and reasoning abstraction,
ensuring alignment and robustness across domain shifts; and (iii) an XML-based
prompt template that separates thinking, tool calls, and responses to encourage
modular, domain-invariant planning and coherent multi-turn interactions. Ex-
tensive experiments across diverse benchmarks validate our approach, achieving
state-of-the-art performance and highlighting the cross-domain potential of Tool
RL for LLM reasoning. We will release our codes and models soon.

1 INTRODUCTION

Large language models (LLMs) have recently achieved impressive performance in a wide range of
reasoning and problem-solving tasks, especially when augmented with external tools such as code

interpreters, calculators, and knowledge bases
(2025). Tool-augmented agents are capable of autonomously planning, invoking
tools, and solving complex tasks that require multi-step reasoning
(2025). While prior studies have focused on supervised fine-tuning (SFT) and

reinforcement learning (RL) for tool usage, most approaches train agents on multiple domains or
tailor tool usage to specific tasks, limiting their generalization ability.

In this paper, we address a fundamental question: Can an LLM agent trained to use a code inter-
preter tool solely on mathematical problems generalize its tool usage to other reasoning domains?
To answer this question, we focus on the circumstance where the agent learns tool invocation strate-
gies via RL in the math domain and is then evaluated on several distinct domains. For example,
as shown in Table|1| apart from utilizing the feedback from the code interpreter for math reason-
ing tasks, we also explore whether these planning, execution, and verification abilities from code
interpreter tools can help enhance downstream reasoning tasks across diverse domains. This setup
enables us to systematically analyze cross-domain generalization, skill transfer, and agentic capabil-
ities under constrained tool training.

“Equal Contribution


Under Review

Table 1: Cases of Tool-Integrated Reasoning Across Diverse Domains.

Tool-Integrated Math

Tool-Integrated Chemistry

Problem: Find the greatest 4-digit integer V
where changing any digit to 1 makes it di-
visible by 7. Compute Q + R where Q =
| N/1000|, R = N mod 1000.
ASSISTANT: During the reasoning step
by step, code parts for math reasoning:

Tool Call

def find_n():
for n in range

Problem: For 2NOCI(g) == 2NO(g) +
Clo(g) at 600 K (KK, = 0.060). Given initial
pressures (torr): Pyoc) = 675, Puyo = 48,
Po, = 23. Find equilibrium Pyo.

ASSISTANT: During the reasoning step
by step, code parts for chemical reasoning:

Tool Call

# Calculate Equilibrium
Pressures

(9999, 999, =I);
S = str(n) .zf1i11 (4)
valid = True
for i in range
new_s = s[:i]+’1’
+S [ised ¢ |
if int (new_s) % 7 != 0:
valid = False
break
L£ valid:
return n
= find_n()
Q, R = N//1000, N%1000
Print (E"N={N},O={(0} ,R={Rb")
print (f"Result: Q+R={Q+R}")

2.901292604180679

Equilibrium pressures

P_NOCl_eq = 675 + 2%x
P_NO_eq = 43 —- 2xx
P_Cl2_eq = 23 - x

# Compute Kp

Kp_calculated=(P_NO_eq**
2*P_C12_eq)

/ (P_NOC1_eqx«*2)
Kp_calculated

Tool Output

Tool Output

ONS 99

N=5694,
Result:

Q=9, R=694
Q+R=699

Under the above circumstances, we first provide a detailed analysis of cross-domain generalization
brought by tool RL training and point out that Learn to use tools rather than specific domain knowl-
edge can truly enhance cross-domain generalization (Section 3). To further enhance cross-domain
transfer, we propose a Tool Generalization Reinforcement Learning (TGRL) framework designed to
foster domain-agnostic learning and skill migration (Section (4). Specifically, our proposed frame-
work comprises three key components: (i) Standardized Tool Interface: A unified interface that
abstracts domain-specific nuances through consistent answer formatting and explicit termination
signals, promoting transferable invocation patterns that adapt seamlessly to varied domains, such as
code debugging or tabular data processing; (ii) Dual-Component Reward System: A novel reward
structure that incentivizes both correct outcomes and proper tool-use formats, with decomposed
rewards targeting generalizable behaviors like tool efficiency and reasoning abstraction to ensure
alignment and robustness during domain shifts; and (iii) XML-Based Prompt Template: A struc-
tured conversation template that separates thinking, tool calls, and responses, encouraging modular,
domain-invariant planning and coherent multi-turn interactions to support transferable reasoning
strategies across domains. Finally, extensive experiments across diverse domains are conducted to
verify the effectiveness of our proposed TGRL framework (Section [5).

To sum up, our contributions can be concluded in three folds: First, we find that RL-trained tool
usage in the math domain can effectively generalize to diverse and unseen domains, achieving great
problem-solving and high token efficiency. Second, we introduce the Tool Generalization Reinforce-
ment Learning (TGRL) framework with components specifically designed to enhance RL stability
and cross-domain generalizability. Third, through extensive evaluations across multiple domains


Under Review

110 5 10000
mE wio Tool RL BE wlo Tool RL BE wio Tool RL
400} Simm w/ Tool RL 400.0) g 4| SM wi Tool RL 8000) Simm w/ Tool RL
5
z= = c 6561
8 3 )
8 90 840 858 ore 85.7 2 5 8000
£ 8210 805 ia e 5 me 2 4045
5 80 2 4000 3561
2 75.0 s 5 212 3214
& 5 1.14 .
124
70 6.7 a1 a: a 2000 746 on
60 0 0

Math Physics Business Philosophy Biology Math Physics dl L . Math — Physics Business Philosophy Biology

(a) Performance (b) Interaction Turn (c) Token Length

Figure 1: Cross-domain comparisons including model performance, interaction turns, and token
length for the output on Webinstruct, where we perform tool RL training on Qwen2.5-7B using the
code-integrated math dataset.

using diverse benchmarks, we achieve state-of-the-art performance, providing insights into the key
factors driving successful skill migration and highlighting the transformative potential of Tool RL
for LLM reasoning in cross-domain settings.

2 RELATED WORK

2.1 TOOL-INTEGRATED RL FOR LLM REASONING

Enhancing LLM reasoning with tools has achieved great attention due to its potential for expanding
the knowledge boundary of models’ generative support, thereby breaking the “invisible leash” that
constrains purely text-based models (2025). The code interpreter and search engine, as
two representative tools, have been widely integrated into the natural- DOS Peart process to

individually boost the model’s performance on mathematical (Feng et al.| Li et al.|/2025a) and
knowledge-intensive reasoning tasks (Jin et al.|/2025} [Liu et al. Bays Tea TODS ead

techniques focus on addressing new challenges under more sa eae ee al g.multi-turn) from

different perspectives, such as long-horizon planning Singh et al.||/2025)
2025), memory management 2025 5 2025d), and inter-
action efficiency (Wang et al.||2025 . However, neither of these works explores

the generalizability of tool-integrated RL approaches across diverse reasoning tasks and domains.

2.2 CROSS-DOMAIN REASONING FOR LLM

Beyond mathematical and code domains, ua efforts have been we to enhancing the cross-
domain reasoning ability for LLMs (Cheng et al.| 2025} Li et al.| /2025b). Some pioneering works
focus on how to curate high-quality cross- eet eee Ree ara et al. Akter et al.) [2025] [Ma et al.| |2025a|
including both synthetic and real-world questions into RL training to improve generalization across
diverse reasoning tasks. Moreover, compared with calculating reward solely depending on the
model-based verifier (Xie et al.|{2025), the scaling and reliable reward signal from models’ intrinsic
mechanism have also been explored to broaden the reasoning boundary to general domains, assisted

by Verifier-Free frameworks (Zhou et al.| /2 1} {2025} 2025b) and Rubrics
2025} |Gunjal et al.| [2025 2025 2025c). nie ot aoe to our
topics are those that focus on the transferability of LLM reasoning (Huan et al.| [2025} {Zhou et al.|

[2025a} [Hu et al.| (2025a} (Cheng et al.| [2025) , but these works only clarify the Satis effect

through single-domain training without tool invocation. Our work first systematically explores the
cross-domain reasoning potential of Tool RL training.

3. UNCOVER THE CROSS-DOMAIN GENERALIZATION BROUGHT BY TOOLS

3.1 PROBLEM FORMULATION

Our goal is to explore whether an LLM agent can generalize its tool-integrated reasoning perfor-
mance on general domains, despite training exclusively on code-integrated math data Dcy. Ideally,
the ultimate target is to maximize expected performance on general domain queries as follows:


Under Review

a Lt Dien [ Lynvma(+|@;tool) [R(y, x)]] (D)

where Dcen represents the general domain query distribution and R(y, x) is the reward function
evaluating response y for query 2.

3.2 BRIDGING THE DOMAIN GAP VIA TOOLS

From the theoretical perspective, we advocate that we can bridge the domain gap through tool-
necessity optimization. By emphasizing high tool-necessity samples in Dc1, we force the model
to learn generalizable tool-usage reasoning patterns rather than domain-specific superficial features.
The tool-necessity acts as a domain-invariant indicator of when tools are truly beneficial, enabling
effective transfer to general domains. Thus, the formalization of the generalization objective to
general domains is:

max D2~Dyen [l[tool-necessary(z)] - R(79(z, tool), 7)] (2)

where I[tool-necessary(x)] indicates queries in general domains that require tool assistance.

This approach ensures that even when trained on specialized math data, the model develops robust
tool-integrated reasoning capabilities that generalize broadly beyond the training distribution.

3.3. ANALYSIS OF CROSS-DOMAIN EXPERIMENTS

We conduct exploration experiments to understand the generalization of tool-integrated RL ap-
proaches across diverse reasoning domains. Specifically,, we perform the tool RL training of the
Qwen2.5-7B on the code-integrated math data (as stated by Sec[5p and compare the model perfor-
mance, the interaction turn numbers, and the token length of response across 5 different domains

(Mathematics, Physics, Business, Philosophy, and Biology) on the Webinstruct 2025b)
evaluation dataset. The detailed comparison results can be shown in Fig.

From the results, we can draw two important conclusions: (i) Comparing with and without tool RL
training, integrating the tool call into the natural reasoning process can enhance model performance
with more interaction turns and shorter token length; (ii) Despite training only on the code-integrated
math data, the model performance and the number of interaction turns and token length for the gen-
eral reasoning (out of domain, such as physics, biology and so on) output have the consistent trend
with in-domain math dataset. This can verify that there exists some tool use patterns (orthogonal
to domain knowledge) transferred from the code-integrated math domain to the general reasoning
domain, which can enhance the cross-domain reasoning abilities during RL training.

4 TOOL GENERALIZATION REINFORCEMENT LEARNING

Based on the above analysis, we introduce several key innovations to further enhance the tool-calling
capabilities of large language models (LLMs) through reinforcement learning (RL), with a specific
emphasis on fostering domain-agnostic learning to enhance cross-domain generalization.

Overall: As shown in Figure[2| the framework is built upon three main components: (i) a standard-
ized tool interface, (ii) a dual-component reward system, and (iii) an XML-based prompt template
for agent-tool interaction. These components are designed to promote transferable skills in tool in-
vocation and reasoning abstraction, allowing an agent trained solely on mathematical tasks to adapt
seamlessly to diverse domains. By abstracting away domain-specific details and rewarding general-
izable behaviors, the framework addresses the limitations of prior multi-domain training approaches,
enabling efficient skill migration.

4.1 STANDARDIZED TOOL INTERFACE

We introduce a standardized tool interface as a critical component of our Tool Generalization Re-
inforcement Learning framework, centered around a specialized answer tool. Unlike traditional
tool-calling approaches that primarily focus on interacting with external APIs, our interface enforces


Under Review

Standardized Tool Dual-Component XML Prompt
gop Interface Reward eS Template
Unified interface Correct outcomes Structured template
Consistent answer formatting Proper tool-use formats Multi-turn interactions

Cross-Domain Application

ZGeoe &

Math Physics Biology Business Philosophy

Figure 2: Illustrations of our proposed Tool Generalization Reinforcement Learning (TGRL).

a unified response format and serves as an explicit termination signal for the model’s reasoning pro-
cess, abstracting domain-specific nuances to promote transferable invocation patterns.

Formally, the tool is defined as a function:

where % is the internal reasoning trace and ¥ is the final output. The output must satisfy the
constraint
yey, y= \boxed{answer}, (4)

with a being the predicted solution content.

This design yields several advantages that facilitate cross-domain transfer:

¢ Format Standardization: Enforcing final answers in \boxed{ } ensures consistent out-
puts for automated cross-domain evaluation, reducing dependence on domain-specific pars-
ing.

* Early Learning Signal: The fixed schema offers an immediate reward in early RL training,
speeding adaptation to the target format and discovery of generalizable tool-use strategies.

¢ Explicit Termination: Calling answer defines task completion, providing a domain-
invariant halting signal transferable across domains (e.g., math or science tasks).

The standardized tool interface plays a pivotal role in enabling effective zero-RL training, where
the model is optimized directly from a base model without prior exposure to tool-use or structured
reasoning data. In this setting, the model initially lacks awareness of how to format tool invocations
or when to terminate reasoning. The fixed and minimal interface design—centered around the single
answer tool and the enforced \boxed { } output schema—provides a strong, low-entropy learning
signal early in training.

4.2 DUAL-COMPONENT REWARD SYSTEM

To guide the model toward both correct reasoning and proper tool usage while ensuring robustness
across domain shifts, we implement a dual-component reward system consisting of an Outcome
Reward and a Format Reward. The overall reward function is

R= Routcome + FRéormat; (5)

with R € {—2,—1,0,1,2}. This design promotes generalizable behaviors like tool efficiency and
reasoning abstraction, shaping a reward landscape that connects math training to unseen domains.

4.2.1 OUTCOME REWARD

The Outcome Reward evaluates the correctness of the model’s final answer, targeting domain-
invariant problem-solving skills. Let @ denote the model’s output (extracted from the boxed content)


Under Review

and a* the ground-truth solution. Then:

+1, ifa=a’*,
Routcome = a otherwise. "

9

Here, the equivalence @ = a®* is assessed using domain-specific verification strategies to ensure
consistency. TGRL mainly focus on math tasks, where rule-based matching is adopted. The answer
is extracted, normalized, and compared according to predefined formats or symbolic verification.

By focusing on outcome correctness in a modular way, this reward encourages abstract reasoning
(e.g., mapping problems to computable steps) on math problems by using tools.

4.2.2 FORMAT REWARD

The Format Reward evaluates structural compliance of the output, rewarding proper tool-use formats
to build domain-agnostic agentic behaviors:

+1. valid tool call format,
Reormat = < 0 minor formatting issues, (7)
—1 completely invalid format.

Together, these components encourage the model to learn both the problem-solving strategy
(Routcome) and structural conformity (Reformat), yielding robust and generalizable behavior.

4.3, XML-BASED PROMPT TEMPLATE

We employ a structured XML-based prompt template P to govern interactions between the model
and tools, encouraging modular, domain-invariant planning and coherent multi-turn interactions.
The template introduces three key elements:

* XML-based tool invocation: Each tool call is enclosed within (tool_call) -
(/tool_call), providing unambiguous syntax for learning and reducing hallucination
in transfers.

* Dedicated reasoning block: Reasoning is explicitly encouraged within a (think) -
(/think) section, promoting structured, step-by-step derivation before tool usage, which
elicits high-level strategies (e.g., problem decomposition) applicable across domains.

* Role-structured dialogue: Interactions are segmented into rounds, denoted as
[Round ¢] USER: u,, ASSISTANT: a:,

where tool responses are incorporated via a standardized TOOL: marker.

This prompt template defines a clear syntax separating reasoning, tool calls, and final responses,
reducing redundancy and improving token efficiency across domains. By enforcing interpretable
step-by-step reasoning within defined tags, it fosters structured problem solving and narrows cross-
domain generalization gaps. It also enhances coherence in multi-turn dialogues, enabling transfer-
able reasoning strategies—e.g., applying optimization behaviors learned in math to scientific ex-
periments. Finally, its consistent tool integration with dynamic cues supports robust evaluation of
domain invariance and strengthens the agent’s adaptive tool-use across varied reasoning tasks. A
complete example of the XML-based prompt template P is provided in Appendix |B]

5 EXPERIMENT

We conduct extensive experiments to evaluate the cross-domain generalization capabilities of the
proposed Tool Generalization Reinforcement Learning framework. Our experimental design cen-
ters on training agents exclusively on mathematical reasoning tasks and assessing their performance
across several diverse reasoning domains. This allows us to systematically investigate the transfer-
ability of tool usage patterns learned in a single domain.


Under Review

GPQA TheoremQA WebInst.| MATH-500 HMMT 25 AIME 24 AIME 25|General All

Model TIR Zero-RU| o@4  Avg@2—s Avg@2| Avg@2  Avg@2 Avg@16 Avg@16|  - -
7B Size Models
Qwen2.5-7B x - 32.4 414 60.4 51.9 0.0 3.2 11 44.7 272
Qwen2.5-7B-TIR  V - 28.5 35.2 52.3 18.0 1.9 17 0.6 38.7 19.7
ToRL-7B Yoeoed 35.8 48.6 68.2 82.2 26.3 40.2 279 | 50.9 47.0
ZeroTIR-7B 4edv 34.2 46.8 65.7 80.2 22.5 39.6 25.0 | 48.9 44.9
SimpleTIR-7B 4edv 36.2 49.5 70.7 88.4 29.7 50.5 30.9 | 52.1 508
TGRL-7B(Ours) Y/Y Vv 35.6 50.3 73.6 86.0 31.1 50.5 36.7 | 53.2 52.0
32B Size Models
Qwen2.5-32B x = 38.6 48.2 68.5 43.1 0.2 42 1.6 518 29.2
Qwen2.5-32B-TIR V - 35.1 42.8 61.9 37.0 5.2 71 5.0 46.6 27.7
ReTool-32B V x 42.3 54.7 76.8 93.2 36.2 67.0 493 | 579 59.9
SimpleTIR-32B JS Vv 43.5 56.3 78.1 92.9 34.6 59.9 49.2 | 593 59.2
TGRL-32B(Ours) 4 Vv 46.4 57.4 82.3 93.8 39.5 71.3 56.7 | 62.0 63.9

Table 2: Overall performance on seven reasoning benchmarks. WebInst.: held-out evaluation set
from WebInstruct. General: Average of GPQA, TheoremQA and WebInst.

5.1 EXPERIMENTAL SETUP

Training Configuration The training environment utilizes a sandboxed Python interpreter as the
primary tool, enabling safe code execution and result verification. Our training dataset consists

exclusively of mathematical problems from Math3-5 (Zeng et al.||2025) and math parts of Deep-
scaler 2025b), ensuring domain-specific training while maintaining tool usage generality.

We employ a rollout batch size of 512 with mini-batch updates of 128 samples. During training,
we adopt the curriculum learning strategy for context expansion and multi-turn interactions. We
set the maximum response length to 16K tokens initially, allowing up to 5 tool interaction turns per
problem. As training progresses and average response lengths stabilize, we dynamically increase the
maximum length to 24K and extend the interaction limit to 10 turns to accommodate more complex
reasoning patterns. The learning rate is set to le-6 with a cosine annealing schedule, and we use a
discount factor of 0.99 for reward computation.

5.2 BASELINE MODELS

We evaluate our approach against a comprehensive set of baseline models across two parameter
scales to demonstrate the effectiveness of our proposed TGRL framework.(i)7B Parameter Base-
lines: Qwen2.5-7B, the instruct model; Qwen2.5-7B-TIR, a tool-integrated reasoning with instruct
model; ToRL-7B (Li et al.|[2025a), which applies domain-specific reinforcement learning to math-
ematical tool usage; ZeroTIR-7B implementing zero-shot tool integration from
base models; and SimpleTIR-7B (Xue et al.||2025), a single-domain tool RL approaches; (ii)32B
Parameter Baselines: Qwen2.5-32B, the instruct model; Qwen2.5-32B-TIR, a tool-integrated
reasoning with instruct model; ReTool-32B (Feng et al.| et al. (2025), which employs cold-start super-
vised fine-tuning followed by domain-specific RL, and SimpleTIR-32B (Xue et al.|[2025), a scaled
single-domain tool RL approaches. These baselines enable systematic comparison across differ-

ent training paradigms: instruct models without tool training, supervised fine-tuning approaches,
domain-specific RL methods, and our proposed TGRL framework.

5.3. EVALUATION PROTOCOL

Cross-Domain Evaluation Setup We evaluate all models across distinct reasoning domains to
assess cross-domain generalization. For each benchmark, we report performance using domain-
appropriate metrics: exact match accuracy for mathematical problems, multiple-choice accuracy for
classification tasks, and automated evaluation scores for open-ended reasoning questions. To en-
sure statistical reliability, we use temperature sampling with T=1.0 and report average scores across
multiple independent runs, following established practices in tool-augmented reasoning evaluation.

7


Under Review

1.0 CS re SPN 45) Qwen-32B Base
9-9 ¥ ( 4.0} == Qwen-32B Instruct
60.8
3 0.7
t06
oO
£0.5
20.4 === Qwen-32B Base ic
0.3 === Qwen-32B Instruct , en ae
1.0
0 200 400 600 800 0 200 400 600 800
Training Steps Training Steps
Training Steps vs Format Accuracy Training Steps vs Interaction Turns

Figure 3: Training progress of format accuracy and interaction turns.

Physics Physics

Priessphy Philosophy
—— Qwen2.5-7B-TIR —— SimpleTIR-7B —— Qwen2.5-32B-TIR —— SimpleTIR-32B
— ToRL-7B — Ours — ReTool-32B — Ours
7B Model Performance 32B Model Performance

Figure 4: Cross-domain performance among different Tool RL methods on Webinstruct.

5.4 EXPERIMENT ANALYSIS

Performance Comparison. Table |2| presents the performance of TGRL compared to strong
baselines across seven reasoning benchmarks, spanning both math-focused datasets (MATH-500,
HMMT 25, AIME 24/25) and general reasoning tasks (GPQA, TheoremQA, WebInstruct). We re-
port results for both 7B and 32B model sizes. (i) 7B Experiments: Among 7B models, TGRL-7B
consistently outperforms prior tool-augmented baselines. For instance, it achieves the highest score
on AIME 25 (36.7%), surpassing SimpleTIR-7B (30.9%) and ToRL-7B (27.9%). On general rea-
soning benchmarks, TGRL-7B reaches 53.2% on the averaged general set, demonstrating strong
cross-domain transfer despite being trained only on math-domain tasks. Compared with Qwen?2.5-
7B and its TIR variant, the improvement is substantial, highlighting the effectiveness of TGRL’s
combined standardized interface, dual-component reward, and XML-based prompting in facilitat-
ing both math reasoning and generalization; (ii) 32B Experiments: The 32B TGRL model achieves
further performance gains, consistently surpassing all baselines. Notably, TGRL-32B attains 71.3%
on AIME 24 and 56.7% on AIME 25, as well as 62.0% on the averaged general set. This demon-
strates that TGRL scales effectively with model size, retaining strong zero-RL learning from base
models while enhancing multi-turn reasoning and tool-use skills. Compared with the ReTool-32B
and SimpleTIR-32B models, TGRL-32B shows clear gains in both math and general benchmarks,
emphasizing the contribution of its domain-agnostic reward and structured prompting mechanisms.

Generalization and Scalability. Beyond standard performance metrics, we conduct a detailed
transfer learning analysis to investigate the mechanisms underlying cross-domain generalization. As
illustrated in Fig. |4| we evaluate model performance across five distinct domains—Mathematical,
Physics, Biology, Business, and Philosophy—using the Webinstruct benchmark. The results re-
veal that, although TGRL is trained exclusively on mathematical problem-solving data, it achieves
strong accuracies in non-mathematical domains: 79.2% in Business, 82.3% in Physics, 80.8% in


Under Review

7B 32B
Method
AIME24 AIME25  WebInstruct AIME24 AIME25  WeblInstruct
TGRL 50.5 36.7 73.6 71.3 56.7 82.3
w/o SI 39.7 25.6 64.9 62.4 43.4 72.1
w/o DR 37.5 23.5 60.2 5/2 42.0 68.9
w/o XT 37.5 26.3 61.4 60.8 44.9 70.5

Table 3: Ablation studies on AIME24, AIME25 (math) and WebInstruct (multidisciplinary) bench-
marks. Results show accuracy (%) with all models trained exclusively on math-domain tasks.

Biology, and 80.8% in Philosophy. This demonstrates that the model learns abstract tool-use strate-
gies rather than memorizing domain-specific knowledge, thereby facilitating effective knowledge
transfer to unseen tasks. Furthermore, as shown in both Fig. [4Jand Table [2 scaling the model size
from 7B to 70B parameters consistently enhances performance across all domains, with an average
improvement of 16.4%. This scaling effect is particularly pronounced in complex multi-step rea-
soning benchmarks such as AIME 2024 (improving from 45.2% to 68.7%) and AIME 2025 (from
47.8% to 70.1%), where larger models demonstrate markedly better reasoning capability and tool
coordination. These findings align with the hypothesis that increased model capacity contributes to
more robust representation of tool-use policies and relational reasoning. Overall, TGRL achieves
state-of-the-art performance across both math and general reasoning tasks, validating the efficacy of
its three-component design for cross-domain tool-generalization.

Training Dynamics and Behavioral Analysis. To further analyze how TGRL shapes tool-use be-
haviors during reinforcement learning, we visualize the evolution of format accuracy and interaction
turns throughout training, as shown in Figure] These metrics respectively capture the correctness
of tool-call syntax and the level of multi-turn reasoning exhibited by the agent. For the model
trained from the base model, the initial format reward is low, indicating that early tool invocations
often violate the required XML structure or tool-call conventions. However, as reinforcement learn-
ing proceeds, the model rapidly learns to comply with the standardized interface, and the format
accuracy increases sharply from 0.3 to 1.0, demonstrating perfect adherence to the expected format.
Simultaneously, the average number of interaction turns—a proxy for reasoning depth and iterative
problem solving—rises steadily from 1.6 to 4, suggesting that the agent gradually develops multi-
step planning behavior. In contrast, the model initialized from an instruct model begins with higher
initial format accuracy due to its pre-existing formatting priors. Nevertheless, after approximately
15 training steps, its format performance converges to the same level as the base model trained with
zero RL. Interestingly, the number of interaction turns for this model first decreases and then in-
creases, stabilizing at a lower level than the base RL model, implying that prior instruction tuning
may bias the agent toward concise, single-turn responses before RL retrains it toward more elabo-
rate, tool-assisted reasoning. These observations confirm that TGRL can achieve excellent tool-use
behaviors even when trained entirely from a base model with no prior RL or instruction fine-tuning.
The base model trained with TGRL not only reaches full format compliance but also exhibits emer-
gent multi-turn reasoning, indicating that the proposed reward structure and prompt framework are
sufficient to teach tool interaction and iterative reasoning from scratch.

Ablation Studies. To better understand the contribution of each component in the TGRL frame-
work, we conduct an ablation study on three representative benchmarks: AIME24, AIME25 (math-
ematical reasoning) and WebInstruct (multidisciplinary reasoning). These datasets capture distinct
domains — the AIME benchmarks emphasizing symbolic computation and structured problem solv-
ing, and WebInstruct emphasizing open-domain reasoning and textual comprehension — thereby
allowing us to assess both in-domain performance and cross-domain transferability. We evaluate
four variants of our framework: Full TGRL (Standardized Interface + Dual-Component Reward +
XML Template), w/o Standardized Interface (w/o SD, w/o Dual-Component Reward (w/o DR), and
w/o XML Template (w/o XT). Each model is trained solely on mathematical problem-solving tasks
and directly tested on all three benchmarks without further fine-tuning. The ablation results in Table
reveal that removing any component leads to noticeable degradation, particularly in cross-domain
generalization. The Dual-Component Reward System contributes the most to stability and gener-
alizable skill transfer, improving WebInstruct accuracy by +10.2% compared to the w/o DR. The
Standardized Tool Interface enhances both math and multidisciplinary domain accuracy by ensur-


Under Review

ing consistent invocation patterns, reducing malformed tool calls, and token waste. The XML-Based
Prompt Template provides structured reasoning separation, improving interpretability and multi-turn
coherence, which is particularly beneficial for open-domain tasks. Overall, these findings confirm
that the synergy of all three components in TGRL is essential for learning domain-agnostic tool-use
strategies and achieving robust cross-domain reasoning.

6 CONCLUSION

In this paper, we systematically examine the cross-domain generalization of large language model
agents equipped with a code interpreter, trained solely via reinforcement learning on mathematical
problem-solving tasks. Our results show that RL-trained tool use in one domain transfers effectively
to diverse reasoning tasks, demonstrating strong performance and high token efficiency. To enable
such transfer, we propose the Tool Generalization Reinforcement Learning framework, featuring
a standardized tool interface, dual-component reward system, and XML-based prompt template to
promote domain-agnostic learning. Extensive experiments confirm its efficacy, achieving state-of-
the-art results and underscoring the value of generalized tool-use strategies over domain-specific
knowledge. Finally, we hope this study can draw more attention to the generalization of Tool RL.

REFERENCES
AI-MO. AIMO Validation AIME Dataset.

Syeda Nahida Akter, Shrimai Prabhumoye, Matvei Novikov, Seungju Han, Ying Lin, Evelina
Bakhturina, Eric Nyberg, Yejin Choi, Mostofa Patwary, Mohammad Shoeybi, et al. Nemotron-
crossthink: Scaling self-learning beyond math reasoning. arXiv preprint arXiv:2504.13941, 2025.

The Viet Bui, Tien Mai, and Hong Thanh Nguyen. Preference-guided learning for sparse-reward
multi-agent reinforcement learning. arXiv preprint arXiv:2509.21828, 2025.

Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang,
and Tony Xia. Theoremqa: A theorem-driven question answering dataset, 2023. URL|https:)
arxiv.org/abs/2305.12524

Zhengyu Chen, Siqi Wang, Teng Xiao, Yudong Wang, Shiqi Chen, Xunliang Cai, Junxian He, and
Jingang Wang. Revisiting scaling laws for language models: The role of data quality and training
strategies. In Proceedings of the 63rd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 23881-23899, 2025.

Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao
Zhuang, Nilabjo Dey, Yuheng Zha, et al. Revisiting reinforcement learning for Ilm reasoning
from a cross-domain perspective. arXiv preprint arXiv:2506.14965, 2025.

DeepSeek-AI Team. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement
Learning. arXiv preprint arXiv:2501.12948, 2025.

Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anu-
manchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for
long-horizon tasks. arXiv preprint arXiv:2503.09572, 2025.

Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,
Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in Ilms.
arXiv preprint arXiv:2504.11536, 2025.

Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong
Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. A survey of self-evolving agents: On path to artificial
super intelligence. arXiv preprint arXiv:2507.21046, 2025a.

Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu.
Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv
preprint arXiv:2508.07976, 2025b.

10


Under Review

Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as
rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746,
2025.

Kairong Han, Kun Kuang, Ziyu Zhao, Junjian Ye, and Fei Wu. Causal agent based on large language
model. arXiv preprint arXiv:2408.06849, 2024.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. In
NeurIPS Datasets and Benchmarks, 2021.

HMMT. Hmmt 2025, 2025. URL

Chuxuan Hu, Yuxuan Zhu, Antony Kellermann, Caleb Biddulph, Suppakit Waiwitlikhit, Jason
Benn, and Daniel Kang. Breaking barriers: Do reinforcement post training gains transfer to
unseen domains? arXiv preprint arXiv:2506.19733, 2025a.

Zijing Hu, Fengda Zhang, Long Chen, Kun Kuang, Jiahui Li, Kaifeng Gao, Jun Xiao, Xin Wang, and
Wenwu Zhu. Towards better alignment: Training diffusion models with reinforcement learning
against sparse rewards. In Proceedings of the Computer Vision and Pattern Recognition Confer-
ence, pp. 23604-23614, 2025b.

Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Pooven-
dran, Graham Neubig, and Xiang Yue. Does math reasoning improve general Ilm capabilities?
understanding transferability of lm reasoning. arXiv preprint arXiv:2507.00432, 2025.

Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi
Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv
preprint arXiv:2508.12790, 2025.

Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai
Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use.
arXiv preprint arXiv:2509.01055, 2025.

Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and
Jiawei Han. Search-r1: Training Ilms to reason and leverage search engines with reinforcement
learning. arXiv preprint arXiv:2503.09516, 2025.

Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint
arXiv:2503.23383, 2025a.

Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, and Lijun Wu. Can one domain
help others? a data-centric study on multi-domain reasoning via reinforcement learning. arXiv
preprint arXiv:2507.17512, 2025b.

Heng Lin and Zhongwen Xu. Understanding tool-integrated reasoning. arXiv preprint
arXiv:2508.19201, 2025.

Yen-Ting Lin. Aime 2025 dataset, 2025. URL https://huggingface.co/datasets/
Acoessed: 2025-03-29

Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu
Du, Qidi Xu, et al. Webexplorer: Explore and evolve for training long-horizon web agents. arXiv
preprint arXiv:2509.06501, 2025a.

Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, and Yulan He. Nover: Incentive training
for language models via verifier-free reinforcement learning. arXiv preprint arXiv:2505.16022,
2025b.

Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi

Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: A survey on methodology,
applications and challenges. arXiv preprint arXiv:2503.2 1460, 2025a.

11


Under Review

Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai,
Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing O1-Preview
with a 1.5B Model by Scaling RL, 2025b. Notion Blog.

Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner:
Advancing Ilm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025a.

Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner:

Advancing Ilm reasoning across all domains, 2025b. URL |https://arxiv.org/abs/
2505.14652

Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, and Wenqiang Zhang. Agent RL
Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving.
arXiv preprint arXiv:2505.07773, 2025.

Meituan. Longcat-flash-thinking technical report, 2025. URL https://arxiv.org/abs/
2509.18883

Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost
Batenburg. Agentic large language models, a survey. arXiv preprint arXiv:2503.23037, 2025.

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien
Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a

benchmark, 2023. URL https: //arxiv.org/abs/2311.12022

Shreyas Singh, Kunal Singh, and Pradeep Moturi. Fathom-deepresearch: Unlocking long horizon
information retrieval and synthesis for slms. arXiv preprint arXiv:2509.24107, 2025.

Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang,
and Ji-Rong Wen. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement
Learning. arXiv preprint arXiv:2503.05592, 2025.

Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu.
Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv
preprint arXiv:2503.23829, 2025.

Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei,
Peng Xia, Fang Wu, He Zhu, et al. Agent kb: Leveraging cross-domain experience for agentic
problem solving. arXiv preprint arXiv:2507.06229, 2025.

Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun
Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with
lms. arXiv preprint arXiv:2501.12599, 2025.

Qwen Team. Qwen3 technical report, 2025a. URL https: //arxiv.org/abs/2505.09388

Tongyi DeepResearch Team. Tongyi-deepresearch. https: //github.com/Alibaba-NLP/
2025b.

Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin,
Mengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning.
arXiv e-prints, pp. arXiv—2504, 2025.

Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, and Xiao Zhang. Capo: Towards enhancing
llm reasoning through verifiable generative credit assignment. arXiv preprint arXiv:2508.02298,
2025.

Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic
memory for llm agents. arXiv preprint arXiv:2502.12110, 2025.

Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Sim-
pletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint
arXiv:2509.02479, 2025.

12


Under Review

Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen
Ma, Hinrich Schiitze, Volker Tresp, and Yunpu Ma. Memory-rl: Enhancing large language
model agents to manage and utilize memories via reinforcement learning. arXiv preprint
arXiv:2508.19828, 2025.

Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Ziyu Zhao, Daixin Wang,
Qing Cui, Zhigiang Zhang, et al. Mix data or merge models? balancing the helpfulness, honesty,
and harmlessness of large language model via model merging. arXiv preprint arXiv:2502.06876,
2025.

Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan
Yao, Zhiyuan Liu, et al. Ripr: Extrapolating rlvr to general domains without verifiers. arXiv
preprint arXiv:2506.18254, 2025.

Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. SimpleRL-
Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild.
arXiv preprint arXiv:2503.18892, 2025.

Ruochen Zhou, Minrui Xu, Shiqi Chen, Junteng Liu, Yungi Li, Xinxin Lin, Zhengyu Chen, and
Junxian He. Does learning mathematical problem-solving generalize to broader reasoning? arXiv
preprint arXiv:2507.04391, 2025a.

Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang
Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint
arXiv:2505.21493, 2025b.

Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv,
Kongcheng Zhang, Yihe Zhou, Hengtong Lu, et al. Breaking the exploration bottleneck: Rubric-
scaffolded reinforcement learning for general Ilm reasoning. arXiv preprint arXiv:2508.16949,
2025c.

Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao,
Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning
for efficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025d.

13


Under Review

A ANSWER TOOL DESIGN

We introduce a specialized answer tool as a critical component of our RL framework. Unlike
traditional tool-calling approaches that primarily focus on interacting with external APIs, our answer
tool enforces a standardized response format and serves as an explicit termination signal for the
model’s reasoning process. The answer tool is defined via a structured JSON schema:

{

"YEype" 7 “Gunection",
"funetion"™: {
"name": "answer",
"description": "Respond to the user",
"parameters": {
"type" : "object We
"properties": {
"answer": {"type": "string", "description": "Response

content, place your final answer within \boxed{}
notation."}

},

"required": ["answer"]

This design provides several advantages:

¢ Format Standardization: By requiring final answers to be enclosed in \boxed { } nota-
tion, the model outputs are consistent and easily extractable for automated evaluation.

* Early Learning Signal: The answer tool offers an immediate and clear learning signal
during early RL training, accelerating the model’s adaptation to the expected output format.

* Explicit Termination: It serves as an explicit endpoint for the model’s reasoning, clearly
indicating when a problem has been solved.

This mechanism significantly reduces the time required for the model to match the performance of
supervised fine-tuned (SFT) baselines in terms of answer formatting and termination.

B SPECIALIZED PROMPT TEMPLATE DESIGN

We employ a specialized prompt template, chat_template, to structure interactions between the
model and tools. This template introduces several innovations:

* XML-Based Tool Call Format: Tool calls are wrapped in
<tool_call>...</tool_call> tags, simplifying the model’s learning of cor-
rect syntax.

¢ Explicit Thinking Section: The model is encouraged to reason through problems in a
dedicated <think>. ..</think> block before making tool calls.

¢ Structured Conversation Flow: Multi-turn dialogue is organized into rounds with explicit
role markers, such as [Round 0] USER: and ASSISTANT:.

* Tool Response Integration: Tool outputs are integrated using the TOOL: marker for seam-
less information flow.

These design choices offer several advantages:

¢ Clear structural boundaries for tool calls and reasoning steps.
¢ Encouragement of deliberate, step-wise problem-solving.
¢ Enhanced coherence in multi-turn conversations.

¢ Standardized integration of tool responses.

14



Under Review

Empirical results show that this template significantly improves the model’s ability to format tool
calls correctly and maintain coherent, multi-step interactions.

Example template snippet:

[Round 0] USER:

ASSISTANT:

<think>

reasoning process here

</think>

<tool_call>

{"name": <function-name>, "arguments": <args-—dict>}
</tool_call>

TOOL: {"name": "tool_name", "content": response_content}

C CASE STUDY

C.1 A CASE FROM CHEMISTRY PROBLEM SOLVING
A case from Chemistry Problem Solving

Problem:
Nitrosyl chloride (NOCI) decomposes to nitric oxide (NO) and chlorine (Cl2) when heated
according to the reaction:

2NOCI (g) —= 2NO (g) + Cle (g)

At T = 600 K, the equilibrium constant is A, = 0.060. In a vessel at 600 K, a mix-
ture of all three gases has the following partial pressures: Pyoc) = 675 torr, Po =
43 torr, Pc), = 23 torr. What is the partial pressure of NO at equilibrium?

Solution:

ASSISTANT:

This is a chemical equilibrium problem involving the decomposition of nitrosyl chloride. I
need to:

1. Determine the reaction direction by comparing Q (reaction quotient) with Kp

2. Set up an ICE table to track pressure changes

3. Solve for the equilibrium partial pressure of NO
The balanced equation is: 2NOCI(g) > 2NO(g) + Clo(g)

Given data:
¢ Temperature: 600 K
° K, = 0.060
¢ Initial partial pressures: Pyocy = 675 torr, Pyo = 48 torr, Poi, = 23 torr

First, Pll calculate the reaction quotient Q to determine which direction the reaction pro-
ceeds.
Step 1: Calculate Reaction Quotient Q

Python Code

# Initial pressures (torr) and Kp
P_ NOC1, P_NO, P_C12 = 675, 43, 23
Kp = 0.060

# Reaction quotient Q
Q = (P_NOx*2 * P_C12)/(P_NOC1««2)
Q

15



Under Review

Tool Output

0, 09333772290809328

ASSISTANT: Since Qinitiaa = 0.0933 > K, = 0.060, the reaction will shift in the reverse
direction to reach equilibrium. This means some NO and Cl, will combine to form more
NOCI.

Step 2: Set up ICE Table and Solve

Let «x be the extent of the reverse reaction. The changes in partial pressures will be:

¢ Pyoct increases by 2x
¢ Pxo decreases by 2x
* Poi, decreases by x
At equilibrium:
Pnoci,eq = 675 + 2x
Pno,eq = 43 — 2x
PGin¢4 = 23-— @

Substituting into the equilibrium expression:

K, = (Pno.ea)” Pete.ea _ 9 969
e (Pnoct,eq)

2
This gives us:
(43 — 2x)?(23 — x)
(675 + 2x)?
This is a cubic equation that requires numerical solution.

= 0.060

Python Code

import numpy as np
from scipy.optimize import fsolve

# Define the cubic equation coefficients
# EqQuattven: 4x° — 263.76x? + 5967x —15189.5 = 0
def func(x):

return 4*x**3 — 263./6*x*e*2 + 5S967*x - 15189.5

# Initial guess for x
initial_guess = 10 # Starting with a reasonable guess

solution = fsolve(func, initial_guess)

# Extract the valid solution
# (should be a real positive root)

x = solution[0]

Tool Output

2.901292604180679

ASSISTANT: Now I'll calculate the equilibrium partial pressures and verify the solution.
Step 3: Calculate Equilibrium Pressures and Verify

16


Under Review

Python Code

# Calculated x
= 2.901292604180679

Equilibrium partial pressures
_NOCl_egq = 675 + 2x*x

_NO_eq = 43 - 2x

P_C1l2_eq = 23 - x

# Compute Kp
Kp_calculated = (P_NO_eq«*2 *« P_Cl2_eq) / (P_NOC1l_eqx«*2)
Kp_cal ated

Tool Output

0.059999999999999984

ASSISTANT: Perfect! The calculated K,, + 0.060 matches the given equilibrium constant,
confirming our solution is correct.

Final Answer:

With x = 2.901 torr, the equilibrium partial pressure of NO is:

Pno,eq = 43 — 2(2.901) = 37.2 torr
Verification:
¢ All equilibrium pressures are positive V

¢ K, calculation matches the given value

¢ Reaction proceeds in the correct direction (reverse) V

Therefore, the partial pressure of NO at equilibrium is | 37.2 torr |.

C.2. A CASE FROM MATHEMATICAL PROBLEM SOLVING

A case from Mathematical Problem Solving

Problem:

Let N be the greatest four-digit positive integer with the property that whenever one of its
digits is changed to 1, the resulting number is divisible by 7. Let Q and R be the quotient
and remainder, respectively, when NV is divided by 1000. Find Q + R.

Solution:

ASSISTANT: I need to find the largest four-digit number N such that changing any single
digit to 1 results in a number divisible by 7.

Problem Analysis:

1. Find NV: The largest four-digit number where changing any digit to 1 gives a mul-
tiple of 7

2. For a four-digit number ABCD, all of 1BCD, A1CD, AB1D, ABC1 must be
divisible by 7
3. Once found, compute Q = | N/1000| and R = N mod 1000, then find Q+ R

Approach: Since we want the greatest such number, I'll search from 9999 downward until
finding a valid NV.

17


Under Review

Python Code

def find m():
for n in range(9999, 999, -1):
s = str(n).zfill (4)
valid = True
for i in range(4):
# Replace ith digit with ’1’
new_s = s[:i] + ’1’ + s[itl:]
m = int (new_s)
if m% 7 != 0:
valid = False
break
1f walads
return n
return None

= find_n()
N // 1000
= N % 1000
result =Q +R
print (f"N = {N}, OQ = {Q}, R = {R}, Result = {result}")

Error Output

SyntaxError: unexpected character after
line continuation character

ASSISTANT: The error occurred due to JSON formatting issues with the f-string quotes.
Let me fix this by using single quotes in the print statement to avoid conflicts with the JSON

double quotes.

Python Code

def fimdln ()):
for m in range (9999, 999, —1):
s = str(n).zfill(4)
valid = True
for i in range(4):
new_s = s[:i] + ’1’ + s[itl:]
m = int (new_s)
if m%$7!= 0:
valid = False
break
Vf walads
return n
return None

= find_n()
N // 1000
= N % 1000

={}, Q={}, R= qf},
{}’ .format (N, Q, R, result) )

18


Under Review

Successful Output

N = 5694, QO = 5, R= 694, Result = 699

ASSISTANT: Perfect! The algorithm found N = 5694. Let me verify this result by check-
ing that changing each digit to | produces numbers divisible by 7.
Verification:

* Original number: 5694
* Change thousands digit (5-1): 1694 = 7 x 242 Vv
* Change hundreds digit (61): 5194 = 7 x 742 Vv

¢ Change tens digit (9-1): 5614 = 7 x 802 V
¢ Change units digit (4-1): 5691 = 7 x 813 Vv
All four modified numbers are indeed divisible by 7, confirming that N = 5694 is valid.
Final Calculation:
N = 5694 (11)
Q = [5694/1000] = 5 (12)
R = 5694 mod 1000 = 694 (13)
Q+R=5+4 694 = 699 (14)

Therefore, Q + R=

C.3. A CASE FROM FINANCIAL PROBLEM SOLVING
A case from Financial Mathematics Problem Solving

Problem:

A solid waste disposal company borrowed money at 10% per year interest to purchase new
haulers and other equipment needed at the company-owned landfill site. If the company
got the loan 2 years ago and paid it off with a single payment of $4,600,000, what was the
principal amount P of the loan?

Solution:

ASSISTANT:

This is a financial mathematics problem involving loan repayment calculations. I need to:

1. Determine whether to use simple or compound interest
2. Apply the appropriate formula to find the principal amount
3. Verify the calculation
Given data:
¢ Annual interest rate: 10%

¢ Loan duration: 2 years

¢ Single repayment amount: $4,600,000

The problem states "10% per year interest" without specifying compounding. In financial
contexts, this could be either simple or compound interest. Let me calculate both possibili-
ties to determine the most appropriate approach.
Step 1: Calculate Principal Using Simple Interest
For simple interest: A = P(1 + rt), where:

¢ A = 4,600, 000 (total repayment)

¢ r = 0.10 (annual interest rate)

° ¢ = 2 years

19


Under Review

Solving for P: P = —4

T4rt
Tool Call

# Calculating principal for simple interest
A_simple = 4600000

r_simpl O20

t= 2
P_simpl A_simple / (1 + r_simple *« t)
P_simpl

Tool Output

31833333,3)333333335

ASSISTANT:

Step 2: Calculate Principal Using Compound Interest
For compound interest: A = P(1 +r)!

Solving for P: P = aay

Tool Call

# Calculating principal for compound interest
A_compound = 4600000

r_compound 0.10

t= 2

P_compound A_compound / (1 + r_compound) **t
P_compound

Tool Output

3801652.892561983

ASSISTANT:
Step 3: Analysis and Final Answer
The calculations yield different results:

¢ Simple interest: $3,833,333.33
¢ Compound interest: $3,801,652.89

Since the problem doesn’t specify compounding and uses the phrase "10% per year inter-
est," in most financial mathematics problems of this type, compound interest is the standard
assumption unless explicitly stated as simple interest. This is because:

1. Real-world loans typically use compound interest
2. The 2-year duration suggests annual compounding
3. Financial institutions standard practice

Verification: Using compound interest: P x (1.10)? = 3, 801, 652.89 x 1.21 = 4, 600, 000
v
Therefore, the principal amount of the loan is | $3, 801, 652.89 |.

D EVALUATION BENCHMARKS

Our comprehensive evaluation encompasses challenging benchmarks across mathematical and gen-
eral domains:

20


Under Review

Mathematical Reasoning Benchmarks:

MATH-500 (Hendrycks et al.||2021): A subset of 500 problems from the MATH dataset, fea-

turing competition-level mathematical problems across algebra, geometry, number theory, and
other mathematical domains. This benchmark requires sophisticated mathematical reasoning
and problem-solving skills.

AIME 24 & AIME 25 ((AI-MO}|Lin|[2025)): Problems from the American Invitational Math-
ematics Examination for 2024 and 2025, representing some of the most challenging high school
mathematics competitions. These benchmarks test advanced mathematical reasoning capabili-
ties and require precise computational skills.

HMMT 25 ((HMMT) /2025)): Problems from the Harvard-MIT Mathematics Tournament
2025, featuring university-level mathematical challenges that demand deep mathematical in-
sight and creative problem-solving approaches.

General Reasoning Benchmarks:

E

GPQA 2023): Graduate-level questions across physics, chemistry, and biology that
require expert-level domain knowledge. We use the highest-quality GPQA-diamond subset,

which contains carefully curated questions designed to be challenging even for domain experts.

TheoremQA (Chen et al.|/2023): A benchmark assessing models’ ability to apply mathemati-
cal and scientific theorems to solve complex problems. It includes 800 high-quality questions

covering 350 theorems from mathematics, physics, and other scientific domains, requiring both
theoretical knowledge and practical application skills.

WebInstruct 2025b): A held-out validation split from the WebInstruct dataset,
designed to evaluate multidisciplinary reasoning capabilities.

IMPLEMENTATION DETAILS

All experiments are conducted using identical computational resources and evaluation protocols
to ensure fair comparison. Models are trained on 8xA100 GPUs with mixed precision training
and gradient accumulation. Evaluation is performed using consistent random seeds and identical
sampling parameters across all baselines. For cross-domain evaluation, we ensure that no target
domain data is used during training, maintaining strict separation between training and evaluation
domains to provide unbiased assessment of generalization capabilities.

F

DISCUSSIONS ABOUT LIMITATIONS AND FUTURE WORK

Despite the promising results, several limitations remain in our current study:

Restricted Tool Diversity: Our experiments primarily focus on a single type of tool (code

interpreter) (Jiang et al.||2025). The generalization performance across domains involving

fundamentally different tools (e.g., knowledge base retrieval, image processing) remains to
be explored.

Domain Shift Extremes: While our benchmarks cover a variety of reasoning domains,
they do not encompass highly specialized or adversarial domains where domain-specific

knowledge or tool customization (Tang et al.,/2025) may be indispensable.

Scalability and Efficiency: The framework assumes access to sufficient computational
resources for RL training and evaluation. Scaling to more complex domains with sparse

reward signals 2025 2025b) or larger toolsets (Jiang et al.|[2025) may
introduce additional efficiency and stability challenges (Chen et al.}|/2025).

Trustworthiness Concerns: The framework only focuses on the generalization of Tool RL
to guide the reasoning towards outcomes, without considering the causal explanation

2024), hallucination, and safety concerns (Yang et al.|{2025) of the LLM reasoning.

21


Under Review

Future work will address these limitations by extending the framework to support a broader range
of tools, exploring more extreme domain shifts, automating reward design, and improving scalabil-
ity and prompt flexibility. We believe these directions will further advance the generalization and
robustness of tool-augmented LLM agents.

22
