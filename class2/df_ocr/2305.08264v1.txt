arX1v:2305.08264v1 [cs.CL] 14 May 2023

MatSci-NLP: Evaluating Scientific Language Models on Materials Science
Language Tasks Using Text-to-Schema Modeling

Yu Song!* Santiago Miret?*

Bang Liu!'

‘University of Montreal / Mila - Quebec AI, "Intel Labs
{yu.song, bang.liu}@umontreal.ca
{santiago.miret}@intel.com

Abstract

We present MatSci-NLP, a natural language
benchmark for evaluating the performance of
natural language processing (NLP) models on
materials science text. We construct the bench-
mark from publicly available materials science
text data to encompass seven different NLP
tasks, including conventional NLP tasks like
named entity recognition and relation classifi-
cation, as well as NLP tasks specific to ma-
terials science, such as synthesis action re-
trieval which relates to creating synthesis pro-
cedures for materials. We study various BERT-
based models pretrained on different scien-
tific text corpora on MatSci-NLP to under-
stand the impact of pretraining strategies on
understanding materials science text. Given
the scarcity of high-quality annotated data
in the materials science domain, we perform
our fine-tuning experiments with limited train-
ing data to encourage the generalize across
MatSci-NLP tasks. Our experiments in this
low-resource training setting show that lan-
guage models pretrained on scientific text out-
perform BERT trained on general text. Mat-
BERT, a model pretrained specifically on ma-
terials science journals, generally performs
best for most tasks. Moreover, we propose a
unified text-to-schema for multitask learning
on MatSci-NLP and compare its performance
with traditional fine-tuning methods. In our
analysis of different training methods, we find
that our proposed text-to-schema methods in-
spired by question-answering consistently out-
perform single and multitask NLP fine-tuning
methods. The code and datasets are publicly
available’.

1 Introduction

Materials science comprises an interdisciplinary
scientific field that studies the behavior, properties

* Equal contribution.
t Corresponding author. Canada CIFAR AI Chair.
‘https: //github.com/BangLab-UdeM-Mila/
NLP 4MatSci-ACL23

and applications of matter that make up materials
systems. As such, materials science often requires
deep understanding of a diverse set of scientific dis-
ciplines to meaningfully further the state of the art.
This interdisciplinary nature, along with the great
technological impact of materials advances and
growing research work at the intersection of ma-
chine learning and materials science (Miret et al.;
Pilania, 2021; Choudhary et al., 2022), makes the
challenge of developing and evaluating natural lan-
guage processing (NLP) models on materials sci-
ence text both interesting and exacting.

The vast amount of materials science knowledge
stored in textual format, such as journal articles,
patents and technical reports, creates a tremendous
opportunity to develop and build NLP tools to cre-
ate and understand advanced materials. These tools
could in turn enable faster discovery, synthesis and
deployment of new materials into a wide variety
of application, including clean energy, sustainable
manufacturing and devices.

Understanding, processing, and training lan-
guage models for scientific text presents distinctive
challenges that have given rise to the creation of
specialized models and techniques that we review
in Section 2. Additionally, evaluating models on
scientific language understanding tasks, especially
in materials science, often remains a laborious task
given the shortness of high-quality annotated data
and the lack of broad model benchmarks. As such,
NLP research applied to materials science remains
in the early stages with a plethora of ongoing re-
search efforts focused on dataset creation, model
training and domain specific applications.

The broader goal of this work is to enable the
development of pertinent language models that can
be applied to further the discovery of new material
systems, and thereby get a better sense of how well
language models understand the properties and be-
havior of existing and new materials. As such, we
propose MatSci-NLP, a benchmark of various NLP


tasks spanning many applications in the materials
science domain described in Section 3. We utilize
this benchmark to analyze the performance of var-
ious BERT-based models for MatSci-NLP tasks
under distinct textual input schemas described in
Section 4. Concretely, through this work we make
the following research contributions:

¢ MatSci-NLP Benchmark: We construct the
first broad benchmark for NLP in the materi-
als science domain, spanning several different
NLP tasks and materials applications. The
benchmark contents are described in Section 3
with a general summary and data sources pro-
vided in Table 1. The processed datasets and
code will be released after acceptance of the
paper for reproducibility.

Text-to-Schema Multitasking: We develop
a set of textual input schemas inspired by
question-answering settings for fine-tuning
language models. We analyze the models’
performance on MatSci-NLP across those set-
tings and conventional single and multitask
fine-tuning methods. In conjunction with this
analysis, we propose a new Task-Schema in-
put format for joint multitask training that in-
creases task performance for all fine-tuned
language models.

MatSci-NLP Analysis: We analyze the per-
formance of various BERT-based models
pretrained on different scientific and non-
scientific text corpora on the MatSci-NLP
benchmark. This analysis help us better un-
derstand how different pretraining strategies
affect downstream tasks and find that Mat-
BERT (Walker et al., 2021), a BERT model
trained on materials science journals, gener-
ally performs best reinforcing the importance
of curating high-quality pretraining corpora.

We centered our MatSci-MLP analysis on ex-
ploring the following questions:

Q1 How does in-domain pretraining of language
models affect the downstream performance on
MatSci-NLP tasks? We investigate the per-
formance of various models pretrained on dif-
ferent kinds of domain-specific text including
materials science, general science and gen-
eral language (BERT (Devlin et al., 2018)).
We find that MatBERT generally performs
best and that language models pretrained on

diverse scientific texts outperform a general
language BERT. Interestingly, SciBERT (Belt-
agy et al., 2019) often outperforms materials
science language models, such as MatSciB-
ERT (Gupta et al., 2022) and BatteryBERT
(Huang and Cole, 2022).

Q2 How do in-context data schema and multi-
tasking affect the learning efficiency in low-
resource training settings? We investigate
how several input schemas shown in Figure 1
that contain different kinds of information af-
fect various domain-specific language mod-
els and propose a new Task-Schema method.
Our experiments show that our proposed Task-
Schema method mostly performs best across
all models and that question-answering in-
spired schema outperform single task and mul-
titask fine-tuning settings.

2 Background

The advent of powerful NLP models has enabled
the analysis and generation of text-based data
across a variety of domains. BERT (Devlin et al.,
2018) was one of the first large-scale transformer-
based models to substantially advance the state-of-
the-art by training on large amounts of unlabeled
text data in a self-supervised way. The pretrain-
ing procedure was followed by task-specific fine-
tuning, leading to impressive results on a variety of
NLP task, such as named entity recognition (NER),
question and answering (QA), and relation classifi-
cation (Hakala and Pyysalo, 2019; Qu et al., 2019;
Wu and He, 2019). A significant collection of large
language models spanning millions to billions of
parameters followed the success of BERT adopting
a similar approach of pretraining on vast corpora
of text with task-specific fine-tuning to push the
state-of-the-art for in natural language processing
and understanding (Raffel et al., 2020; Brown et al.,
2020; Scao et al., 2022).

2.1 Scientific Language Models

The success of large language models on gen-
eral text motivated the development of domain-
specific language models pretrained on custom text
data, including text in the scientific domain: SciB-
ERT (Beltagy et al., 2019), ScholarBERT (Hong
et al., 2022) and Galactica (Taylor et al., 2022)
are pretrained on general corpus of scientific arti-
cles; BioBERT (Lee et al., 2020), PubMedBERT
(Gu et al., 2021), BioMegatron (Shin et al., 2020)


and Sci-Five (Phan et al., 2021) are pretrained on
various kinds of biomedical corpora; MatBERT
(Walker et al., 2021), MatSciBERT (Gupta et al.,
2022) are pretrained on materials science specific
corpora; and BatteryBERT (Huang and Cole, 2022)
is pretrained on a corpus focused on batteries.
Concurrently, several domain-specific NLP bench-
marks were established to assess language model
performance on domain-specific tasks, such as
QASPER (Dasigi et al., 2021) and BLURB (Gu
et al., 2021) in the scientific domain, as well as
PubMedQA (Jin et al., 2019), BioASQ (Balikas
et al., 2015), and Biomedical Language Under-
standing Evaluation (BLUE) (Peng et al., 2019)
in the biomedical domain.

2.2 NLP in Materials Science

The availability of openly accessible, high-quality
corpora of materials science text data remains
highly restricted in large part because data from
peer-reviewed journals and scientific documents
is usually subject to copyright restrictions, while
open-domain data is often only available in
difficult-to-process PDF formats (Olivetti et al.,
2020; Kononova et al., 2021). Moreover, special-
ized scientific text, such as materials synthesis pro-
cedures containing chemical formulas and reaction
notation, require advanced data mining techniques
for effective processing (Kuniyoshi et al., 2020;
Wang et al., 2022b). Given the specificity, com-
plexity, and diversity of specialized language in
scientific text, effective extraction and processing
remain an active area of research with the goal
of building relevant and sizeable text corpora for
pretraining scientific language models (Kononova
et al., 2021).

Nonetheless, materials science-specific language
models, including MatBERT (Walker et al., 2021),
MatSciBERT (Gupta et al., 2022), and Battery-
BERT (Huang and Cole, 2022), have been trained
on custom-built pretraining dataset curated by dif-
ferent academic research groups. The pretrained
models and some of the associated fine-tuning data
have been released to the public and have enabled
further research, including this work.

The nature of NLP research in materials science
to date has also been highly fragmented with many
research works focusing on distinct tasks motivated
by a given application or methodology. Common
ideas among many works include the prediction
and construction of synthesis routes for a variety

of materials (Mahbub et al., 2020; Karpovich et al.,
2021; Kim et al., 2020), as well as the creation of
novel materials for a given application (Huang and
Cole, 2022; Georgescu et al., 2021; Jensen et al.,
2021), both of which relate broader challenges in
the field of materials science.

3 MatSci-NLP Benchmark

Through the creation of MatSci-NLP, we aim to
bring together some of the fragmented data across
multiple research works for a wide-ranging ma-
terials science NLP benchmark. As described in
Section 2, the availability of sizeable, high-quality
and diverse datasets remain a major obstacle in ap-
plying modern NLP to advance materials science
in meaningful ways. This is primarily driven by a
high cost of data labeling and the heterogeneous
nature of materials science. Given those challenges,
we created MatSci-NLP by unifying various pub-
licly available, high-quality, smaller-scale datasets
to form a benchmark for fine-tuning and evaluating
modern NLP models for materials science appli-
cations. MatSci-NLP consists of seven NLP tasks
shown in Table 1, spanning a wide range of materi-
als categories including fuel cells (Friedrich et al.,
2020), glasses (Venugopal et al., 2021), inorganic
materials (Weston et al., 2019; MatSciRE, 2022),
superconductors (Yamaguchi et al., 2020), and syn-
thesis procedures pertaining to various kinds of
materials (Mysore et al., 2019; Wang et al., 2022a).
Some tasks in MatSci-NLP had multiple source
components, meaning that the data was curated
from multiple datasets (e.g. NER), while many
were obtained from a single source dataset.

The data in MatSci-NLP adheres to a standard
JSON-based data format with each of the samples
containing relevant text, task definitions, and an-
notations. These can in turn be refactored into
different input schemas, such as the ones shown
in Figure 1 consisting of 1) Input: primary text
jointly with task descriptions and instructions, and
2) Output: query and label, which we perform
in our text-to-schema modeling described in Sec-
tion 4. Next, we describe the tasks in MatSci-NLP
in greater detail:

¢ Named Entity Recognition (NER): The
NER task requires models to extract summary-
level information from materials science text
and recognize entities including materials, de-
scriptors, material properties, and applications
amongst others. The NER task predicts the


OT rrrerrerrenrecnrnnrcnrcnrcnernnceneenrenenenneneeennennennennenceeeee eet -, Shared Text
Text: [CLS] Hydrothermal synthesis of pure BaFe12019 hexaferrite nanoplatelets NER Details
; under high alkaline system BaCl2 2H20 and FeCl3 6H20 are used as initial materials.) | EF Derails
At last, we got the pure-phase BaFe12019.[SEP]

‘Description: Named Entity Recognition \t BaCl2 2H20

‘Instruction Options:

:1. Schema: <entity name, entity type>

2. Choices: candidate entity type: number, operation, ..., material, brand

:3. Example: Text: Extremely low thermal conductivity ... pulsed currents going through
ithe asinine Description: named entity recognition \t melting. Answer: operation

Answer: BaCl2 2H20,
i Material

NER Output

‘Description: Event Argument Extraction \t used
‘Instruction Options:

i. Schema: <Trigger, Argument |: Role Type, Entity Name 2, Argument 2: Role Type> 2H20: Recipe

12. Choices: candidate_role type: recipe_precursor, atmospheric_material, ....dopant ‘Precursor, FeC13 6H20:
3. Example: Text: Extremely low thermal conductivity ... pulsed through the powders. | ' Recipe Precursor
‘Description: event extraction \t synthesized. Answer: synthesized, cu2sel- i es a
ixsx:recipe_target

i Answer: used, BaCl2 j
i i EE Output

Figure 1: Example of different question-answering inspired textual input schemas (Task-Schema , Potential
Choices, Example) applied on MatSci-NLP. The input of the language model includes the shared text (green)
along with relevant task details (blue for NER and orange for event extraction). The shared text can contain
relevant information for multiple tasks and be part of the language model input multiple times.

Task Size Meta-Dataset 1 lati fe ‘ ‘
(# Samples) | Components relevant re ation type for a given span pair
(s;, 8;). MatSci-NLP contains relation classi-
Named Entity 112.191 4 ficati k f, 1
Recopnidion i cation task data adapted from Mysore et al.
; (2019); Yamaguchi et al. (2020); MatSciRE
Relation 25.674 3
Classification , (2022).
nea 6,566 2 ¢ Event Argument Extraction: The event
argument extraction task involves extracting
Paragraph
es 1,500 1 event arguments and relevant argument roles.
Classification :
As there may be more than a single event for
Synthesis 5,547 1 a given text cify event triggers and
Action Retrieval , & ie emt, WE Specny eve seers
, require the language model to extract corre-
Checifention 9,466 1 sponding arguments and their roles. MatSci-
Sia Hil rc ; NLP contains event argument extraction task
a data adapted from Mysore et al. (2019); Yam-
Table 1: Collection of NLP tasks in the meta-dataset of aguchi et al. (2020).
the MatSci-NLP Benchmark drawn from Weston et al.
(2019); Friedrich et al. (2020); Mysore et al. (2019); ¢ Paragraph Classification: In the paragraph
Yamaguchi et al. (2020); Venugopal et al. (2021); Wang classification task adapted from Venugopal
et al. (2022a); MatSciRE (2022). et al. (2021), the model determines whether a

given paragraph pertains to glass science.

best entity type label for a given text span

s; with a non-entity span containing a “null” ¢ Synthesis Action Retrieval (SAR): SAR is

label. MatSci-NLP contains NER task data a materials science domain-specific task that
adapted from Weston et al. (2019); Friedrich defines eight action terms that unambiguously
et al. (2020); Mysore et al. (2019); Yamaguchi identify a type of synthesis action to describe
et al. (2020). a synthesis procedure. MatSci-NLP adapts
SAR data from Wang et al. (2022a) to ask

¢ Relation Classification: In the relation clas- language models to classify word tokens into

sification task, the model predicts the most pre-defined action categories.


¢ Sentence Classification: In the sentence
classification task, models identify sentences
that describe relevant experimental facts based
on data adapted from Friedrich et al. (2020).

Slot Filling: In the slot-filling task, models
extract slot fillers from particular sentences
based on a predefined set of semantically
meaningful entities. In the task data adapted
from Friedrich et al. (2020), each sentence de-
scribes a single experiment frame for which
the model predicts the slots in that frame.

The tasks contained in MatSci-NLP were se-
lected based on publicly available, high-quality
annotated materials science textual data, as well
as their relevance to applying NLP tools to mate-
rials science. Conventional NLP tasks (NER, Re-
lation Classification, Event Argument Extraction,
Paragraph Classification, Sentence Classification)
enable materials science researchers to better pro-
cess and understand relevant textual data. Domain
specific tasks (SAR, Slot Filling) enable materials
science research to solve concrete challenges, such
as finding materials synthesis procedures and real-
world experimental planning. In the future, we aim
to augment to current set of tasks with additional
data and introduce novel tasks that address materi-
als science specific challenges with NLP tools.

4 Unified Text-to-Schema Language
Modeling

As shown in Figure 1, a given piece of text can in-
clude multiple labels across different tasks. Given
this multitask nature of the MatSci-NLP bench-
mark, we propose a new and unified Task-Schema
multitask modeling method illustrated in Figure 2
that covers all the tasks in the MatSci-NLP dataset.
Our approach centers on a unified text-to-schema
modeling approach that can predict multiple tasks
simultaneously through a unified format. The
underlying language model architecture is made
up of modular components, including a domain-
specific encoder model (e.g. MatBERT, MatSciB-
ERT, SciBERT), and a generic transformer-based
decoder, each of which can be easily exchanged
with different pretrained domain-specific NLP mod-
els. We fine-tune these pretrained language models
and the decoder with collected tasks in MatSci-
NLP using the procedure described in Section 4.3.

The unified text-to-schema provides a more
structured format to training and evaluating lan-

guage model outputs compared to seq2seq and text-
to-text approaches (Raffel et al., 2020; Luong et al.,
2015). This is particularly helpful for the tasks in
MatSci-NLP given that many tasks can be refor-
mulated as classification problems. NER and Slot
Filling, for example, are classifications at the token-
level, while event arguments extraction entails the
classification of roles of certain arguments. With-
out a predefined schema, the model relies entirely
on unstructured natural language to provide the
answer in a seq2seq manner, which significantly
increases the complexity of the task and also makes
it harder to evaluate performance. The structure
imposed by text-to-schema method also simplifies
complex tasks, such as event extraction, by en-
abling the language model to leverage the structure
of the schema to predict the correct answer. We
utilize the structure of the schema in decoding and
evaluating the output of the language models, as
described further in Section 4.3 in greater detail.

Moreover, our unified text-to-schema approach
alleviates error propagation commonly found in
multitask scenarios (Van Nguyen et al., 2022; Lu
et al., 2021), enables knowledge sharing across
multiple tasks and encourages the fine-tuned lan-
guage model to generalize across a broader set of
text-based instruction scenarios. This is supported
by our results shown in Section 5.2 showing text-
to-schema outperforming conventional methods.

4.1 Language Model Formulation

The general purpose of our model is to achieve
multitask learning by a mapping function (f) be-
tween input (x), output (y), and schema (s), i.e.,
f(x,s) = y. Due to the multitasking nature of
our setting, both inputs and outputs can originate
from different tasks n, ie. x = [x1], x2, ...24n]
and y = [yl,y2,..-yn], all of which fit un-
der a common schema (s). Given the presence
of domain-specific materials science language,
our model architecture includes a domain-specific
BERT encoder and a transformer decoder. All
BERT encoders and transformer decoders share
the same general architecture, which relies on a
self-attention mechanism: Given an input sequence
of length N, we compute a set of attention scores,
A = softmax(QT™ /(\/d,)). Next, we compute
the weighted sum of the value vectors, O = AV,
where (, i, and V are the query, key, and value
matrices, and d; is the dimensionality of the key
vectors.


MatBERT, SciBERT

Transformer

Entity: <Entity Name, Entity Type>

Relation: <Relation Type, Entity Name 1,
Entity Name 2>
Event: <Trigger,Argument 1: Role Type,
Argument 2: Role Type, ... >
Paragraph: <Yes> or <No>
Synthesis Action: <Action>

Sentence: <Yes> or <No>
Slot Filling: <Slot Text>
Unified Text Output

Figure 2: Unified text-to-schema method for MatSci-
NLP text understanding applied across the seven tasks.
The language model includes a domain specific en-
coder, which can be exchanged in a modular manner,
as well as a general language pretrained transformer de-
coder.

Additionally, the transformer based decoder dif-
fer from the domain specific encoder by: 1) Ap-
plying masking based on the schema applied to
ensure that it does not attend to future positions
in the output sequence. 2) Applying both self-
attention and encoder-decoder attention to com-
pute attention scores that weigh the importance
of different parts of the output sequence and in-
put sequence. The output of the self-attention
mechanism (O,) and the output of the encoder-
decoder attention mechanism (O2) are concate-
nated and linearly transformed to obtain a new
hidden state, H = tanh(W,[O1; O2] + b.) with
W, and 6b, being the weight and biases respec-
tively. The model then applies a softmax to H
to generate the next element in the output sequence
P = softmax(W,H + bp) , where P is a proba-
bility distribution over the output vocabulary.

4.2 Text-To-Schema Modeling

As shown in Figure 1, our schema structures the
text data based on four general components: text,
description, instruction options, and the predefined
answer schema.

¢ Text specifies raw text from the literature that
is given as input to the language model.

¢ Description describes the task for a given text
according to a predefined schema containing
the task name and the task arguments.

Instruction Options contains the core expla-
nation related to the task with emphasis on
three different types: 1) Potential choices of
answers; 2) Example of an input/output pair
corresponding to the task; 3) Task-Schema :
our predefined answer schema illustrated in
Figure 2.

Answer describes the correct label of each
task formatted as a predefined answer schema
that can be automatically generated based on
the data structure of the task.

4.3 Language Decoding & Evaluation

Evaluating the performance of the language model
on MatSci-NLP requires determining if the text
generated by the decoder is valid and meaningful
in the context of a given task. To ensure consis-
tency in evaluation, we apply a constrained decod-
ing procedure consisting of two steps: 1) Filtering
out invalid answers through the predefined answer
schema shown in Figure 2 based on the structure
of the model’s output; 2) Match the model’s predic-
tion with the most similar valid class given by the
annotation for the particular task. For example, if
for the NER task shown in Figure 1 the model’s pre-
dicted token is “BaCl2 2H2O materials’, it will be
matched with the NER label of “material”, which
is then used as the final prediction for computing
losses and evaluating performance. This approach
essentially reformulates each task as a classification
problem where the classes are provided based on
the labels from the tasks in MatSci-NLP. We then
apply a cross-entropy loss for model fine-tuning
based on the matched label from the model output.
The matching procedure simplifies the language
modeling challenge by not requiring an exact match
of the predicted tokens with the task labels. This in
turns leads to a more comprehensible signal in the
fine-tuning loss function.

5 Evaluation and Results

Our analysis focuses on the questions outlined in
Section 1: 1) Studying the effectiveness of domain-
specific language models as encoders, and 2) An-
alyzing the effect of different input schemas in
resolving MatSci-NLP tasks. Concretely, we study
the performance of the language models and lan-
guage schema in a low resource setting where we


NLP Model Named Entity Relation Event Argument Paragraph Synthesis Sentence Slot Overall
Recognition Classification Extraction Classification Action Retrieval Classification Filling (All Tasks)
MatSciBERT 0.707 +0.076 0.791+0.046 0.436+0.066 0.719+0.116 0.6924.0.179 0.914+0.008 0.43640.142 0.671+0.060
(Gupta et al., 2022) 0.470+0.092 0.507 +0.073 0.251+0.075 0.623+0.183 0.484+0.254 0.660+0.079 0.1 940.062 0.456+0.042

MatBERT 5 0.804.0.071 0.451+0.091 5 7 0.9090.009 548.

(Walker et al., 2021) ( 1 0.5130.138 0.288.0.066 i. 0.614..0.134 ).05 1
BatteryBERT 0.786+0.113 0.801 ..0.081 0.457+.0.024 0.633-40.075 0.61440.128 0.9120.015 0.5200.057 0.663-.0.038
(Huang and Cole, 2022) 0.47240.150 0.46640.111 0.277 40.034 0.610+0.046 0.41949 149 0.684+.0.095 0.22440.073 0.456+0.048
SciBERT 0.734..0.079 0.451.0.077 0.696+0.094 0.701+0.138 0.911+0.017 0.48140.144  0.685+0.056
(Beltagy et al., 2019) 0.497 0.091 45 1 0.276+0.080 0.546..0.243 0.51640.217 0.617 40.143 0.224.409.0190  0.460+0.044
ScholarBERT 0. 168+0.067 0.428-40.148 0.663-.0.032 0.32240.260 0.906+.0.007 0.296+.0.085 0.468.0.028
(Hong et al., 2022) 0.101+40.034 0.27440.110 0.43340.122 0.17840.051 0.478+0.008 0.109.9.044 0.27640.024
BioBERT 0.715.031 0.797 +0.092 0.4884.0.036 0.67540.144 0.647 40.140 0.45240.114  0.670+0.061
(Wada et al., 2020) 0.459..0.055 0.465+0.134 0.274..0.049 0.578.0.102 0.446+40.231 0.19140.045  0.442+0.057
BERT 0.657.0.077 0.782..0.056 0.418..0.053 0.665..0.057 0.656.0.099 0.910+0.017 — 0.5200.019 + 0.658L0.030
(Devlin et al., 2018) 0.4610.058 0.49446 .061 0.22540.091 0.53240.194 0.515+0.067 0.63340.133 0.257+40.022 0.439..0.021

Table 2: Low-resource fine-tuning results applying unified Task-Schema setting for various BERT-based encoder
models pretrained on different domain specific text data. For each model, the top line represents the micro-F1 score
and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence
interval of two standard deviations. We denote the best performing encoder model and those that outperform the
general language BERT according to the micro-f1 with orange shading with MatBERT and SciBERT performing
best on most tasks and ScholarBERT and general language BERT generally performing worst.

perform fine-tuning on different pretrained BERT
models with limited data from the MatSci-NLP
benchmark. This low-resource setting makes the
learning problem harder given that the model has
to generalize on little amount of data. Moreover,
this setting approximates model training with very
limited annotated data, which is commonly found
in materials science as discussed in Section 2. In
our experiments, we split the data in MatSci-NLP
into 1% training subset and a 99% testing subset
for evaluation. None of the evaluated encoder mod-
els were exposed to the fine-tuning data in advance
of our experiments and therefore have to rely on
the knowledge acquired during their respective pre-
training processes. We evaluate the results of our
experiments using micro-F1 and macro-F1 scores
of the language model predictions on the test split
of the MatSci-NLP that were not exposed during
fine-tuning.

5.1 How does in-domain pretraining of
language models affect the downstream
performance on MatSci-NLP tasks? (Q1)

Based on the results shown in Table 2, we can
gather the following insights:

First, domain-specific pretraining affects model
performance. We perform fine-tuning on various
models pretrained on domain-specific corpora in a
low-resource setting and observe that: i) MatBert,
which was pretrained on textual data from materials
science journals, generally performs best for most
tasks in the MatSci-NLP benchmark with SciBERT

generally performing second best. The high perfor-
mance of MatBERT suggests that materials science
specific pretraining does help the language models
acquire relevant materials science knowledge. Yet,
the underperformance of MatSciBERT compared
to MatBERT and SciBERT indicates that the cu-
ration of pretraining data does significantly affect
performance. ii) The importance of the pretraining
corpus is further reinforced by the difference in
performance between SciBERT and ScholarBERT,
both of which were trained on corpora of general
scientific text, but show vastly different results. In
fact, ScholarBERT underperforms all other models,
including the general language BERT, for all tasks
except event argument extraction where Scholar-
BERT performs best compared to all other mod-
els. iii) The fact that most scientific BERT models
outperform BERT pretrained on general language
suggests that pretraining on high-quality scientific
text is beneficial for resolving tasks involving ma-
terials science text and potentially scientific texts
from other domains. This notion of enhanced per-
formance on MatSci-NLP when pretraining on sci-
entific text is further reinforced by the performance
of BioBERT by Wada et al. (2020). BioBERT
outperforms BERT on most tasks even though it
was trained on text from the biomedical domain
that has minor overlap with the materials science
domain. This strongly indicates that scientific lan-
guage, regardless of the domain, has a significant
distribution shift from general language that is used
to pretrain common language models.


NLP Model Single Task Single Task Prompt MMOE No Explanations Potential Choices Examples Task-Schema
MatSciBERT 0.501+0.057 0.485+0.043 0.457+.0.021 0.651 40.045 0.670+0.036 0.671+0.060
(Gupta et al., 2022) 0.320+0.078 0.238+0.017 0.228+0.038 0.438+0.052 0.435+0.061 0.456+0.042
MatBERT 0.537+0.036 0.52340.021 0.557+0.010 0.721+0.033 0.699.0.020 0.705+0.025
(Walker et al., 2021) 0.330+0.063 0.267+0.014 0.301+0.006 0.514-40.045 0.478 0.032 0.470+0.029
BatteryBERT 0.469+0.050 0.488+0.011 0.431+0.044 0.660+0.013 0.622+40.069 0.660+0.033
(Huang and Cole, 2022) 0.2880.055 0.241+0.009 0.200+0.022 0.450-.0,031 0.423 +0.039 0.416+0.054
SciBERT 0.500.0.055 0.502+0.030 0.504+.0.052 0.680+0.066 0.660+0.042 0.68540.056
(Beltagy et al., 2019) 0.300-40.080 0.24840.015 0.2750.031 0.458+0.060 0.435-40.061 0.460+0.044
ScholarBERT 0.472+0.137 0.429.0.258 0.367+0.075 0.461+0.016 0.467+0.019  0.468.-0.028
(Hong et al., 2022) 0.234+0.094 0.250+0.142 0.165+0.044 0.271+0.022 0.260+0.018  0.276+0.024
BioBERT 0.487 40.059 0.488+0.032 0.360+0.007 0.663+0.044 0.587+0.022 0.632.0.040
(Wada et al., 2020) 0.281+0.026 0.2380.017 0.151+0.002 0.4424.0.079 0.365+0.018 0.4040 046
BERT 0.4984.0.051 0.488+0.043 0.39440.009 0.601 +0.046 0.63640.052  0.65840.030
(Devlin et al., 2018) 0.266-40.044 0.23940.011 0.166+0.008 0.382+0.039 0.394.40.051 0.439+0.021
Overall 0.493+0.064 0.486+0.062 0.439+0.003 0.644.0.034 0.622+0.035 0.639.044
(All Models) 0.288.-0.063 0.246+0.032 0.21240.022 0.430.40.049 0.402.0.049 0.410+0.043

Table 3: Consolidated results among all MatSci-NLP tasks on different training settings for various BERT-based
encoder models pretrained on different domain specific text data. For each model, the top line represents the
micro-F1 score and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with
a confidence interval of two standard deviations. We highlight the performance of different schema according to

heatmap ranging from

and worst. The concentration of red hues on right side indicates that the question-

answering inspiring schema generally outperform conventional fine-tuning method. Our proposed Task-Schema
generally outperforms all other schemas across most enconder models.

Second, imbalanced datasets in MatSci-NLP
skew performance metrics: We can see from Ta-
ble 2 that the micro-F1 scores are significantly
higher than the macro-f1 across all tasks. This
indicates that the datasets used in the MatSci-NLP
are consistently imbalanced, including in the binary
classification tasks, and thereby push the micro-F1l
higher compared to the macro-F1 score. In the case
of paragraph classification, for example, the num-
ber of positive examples is 492 compared with the
total number of 1500 samples. As such, only mod-
els with a micro-F1 score above 0.66 and macro-Fl
above 0.5 can be considered to have semantically
meaningful understanding of the task. This is even
more pronounced for sentence classification where
only 876/9466 ~ 10% corresponds to one label.
All models except ScholarBERT outperform a de-
fault guess of the dominant class for cases. While
imbalanced datasets may approximate some real-
world use cases of materials science text analysis,
such as extracting specialized materials informa-
tion, a highly imbalanced can be misguiding in
evaluating model performance.

To alleviate the potentially negative effects of
imbalanced data, we suggest three simple yet ef-
fective methods: 1) Weighted loss functions: This
involves weighting the loss function to give higher
weights to minority classes. Focal loss (Lin et al.,
2017), for example, is a loss function that dy-

namically modulates the loss based on the predic-
tion confidence, with greater emphasis on more
difficult examples. As such, Focal loss handles
class imbalance well due to the additional atten-
tion given to hard examples of the minority classes.
2) Class-balanced samplers: Deep learning frame-
works, such as Pytorch, have class-balanced batch
samplers that can be used to oversample minority
classes within each batch during training, which
can help indirectly address class imbalance. 3)
Model architecture tweaks: The model architec-
ture and its hyper-parameters can be adjusted to
place greater emphasis on minority classes. For
example, one can apply separate prediction heads
for minority classes or tweak L2 regularization and
dropout to behave differently for minority and ma-
jority classes.

5.2 How do in-context data schema and
multitasking affect the learning efficiency
in low-resource training settings? (Q2)

To assess the efficacy of the proposed textual
schemas shown in Figure 1, we evaluate four dif-
ferent QA-inspired schemas: 1) No Explanations -
here the model receives only the task description;
2) Potential Choices - here the model receives the
class labels given by the task; 3) Examples - here
the model receives an example of a correct answer,
4) Task-Schema - here the model receives our pro-


posed textual schema. We compare the schemas to
three conventional fine-tuning methods: 1) Single
Task - the traditional method to solve each task sep-
arately using the language model and a classifica-
tion head; 2) Single Task Prompt - here we change
the format of the task to the same QA-format as
“No Explanations’, but train each task separately;
3) MMOE by Ma et al. (2018) uses multiple en-
coders to learn multiple hidden embeddings, which
are then weighed by a task-specific gate unit and
aggregated to the final hidden embedding using a
weighted sum for each task. Next, a task-specific
classification head outputs the label probability dis-
tribution for each task.

Based on the results shown in Table 3, we gather
the following insights:

First, Text-to-Schema methods perform better
for all language models. Overall, the Task-Schema
method we proposed performs best across all tasks
in the MatSci-NLP benchmark. The question-
answering inspired schema (“No Explanations”,
“Potential Choices”, “Examples”, ““Task-Schema
*) perform better than fine-tuning in a traditional
single task setting, single task prompting, as well
as fine-tuning using the MMOE multitask method.
This holds across all models for all the tasks in
MatSci-NLP showing the efficacy of structured lan-
guage modeling inspired by question-answering.

Second, schema design affects model perfor-
mance. The results show that both the pretrained
model and the input format affect performance.
This can be seen by the fact that while all scientific
models outperform general language BERT using
the Task-Schema method, BERT outperforms some
models, mainly ScholarBERT and BioBERT, in the
other text-to-schema settings and the conventional
training settings. Nevertheless, BERT underper-
forms the stronger models (MatBERT, SciBERT,
MatSciBERT) across all schema settings for all
tasks in MatSci-NLP, further emphasizing the im-
portance of domain-specific model pretraining for
materials science language understanding.

6 Conclusion and Future Works

We proposed MatSci-NLP, the first broad bench-
mark on materials science language understand-
ing tasks constructed from publicly available data.
We further proposed text-to-schema multitask mod-
eling to improve the model performance in low-
resource settings. Leveraging MatSci-NLP and
text-to-schema modeling, we performed an in-

depth analysis of the performance of various
scientific language models and compare text-to-
schema language modeling methods with other
input schemas, guided by (Q1) addressing the
pretrained models and (Q2) addressing the tex-
tual schema. Overall, we found that the choice of
pretrained models matters significantly for down-
stream performance on MatSci-NLP tasks and that
pretrained language models on scientific text of
any kind often perform better than pretrained lan-
guage models on general text. MatBERT gener-
ally performed best, highlighting the benefits of
pretraining with high-quality domain-specific lan-
guage data. With regards to the textual schema
outlined in (Q2), we found that significant improve-
ments can be made by improving textual schema
showcasing the potential of fine-tuning using struc-
tured language modeling.

The proposed encoder-decoder architecture, as
well as the proposed multitask schema, could also
be useful for additional domains in NLP, includ-
ing both scientific and non-scientific domains. The
potential for open-domain transferability of our
method is due to: 1) Our multitask training method
and associated schemas do not depend on any
domain-specific knowledge, allowing them to be
easily transferred to other domains. 2) The en-
coder of our proposed model architecture can be
exchanged in a modular manner, which enables
our model structure to be applied across multiple
domains. 3) If the fine-tuning data is diverse across
a wide range of domains, our method is likely to
learn general language representations for open-
domain multitask problems. Future work could
build upon this paper by applying the model and
proposed schema to different scientific domains
where fine-tuning data might be sparse, such as
biology, physics and chemistry. Moreover, future
work can build upon the proposed schema by sug-
gesting novel ways of modeling domain-specific
or general language that lead to improvements in
unified multi-task learning.

Limitations

One of the primary limitations of NLP modeling
in materials science, including this work, is the
low quantity of available data as discussed in Sec-
tion 2. This analysis is affected by this limitation
as well given that our evaluations were performed
in a low-data setting within a dataset that was al-
ready limited in size. We believe that future work


can improve upon this study by applying larger
datasets, both in the number of samples and in the
scope of tasks, to similar problem settings. The
small nature of the datasets applied in this study
also presents the danger that some of the models
may have memorized certain answers instead of
achieving a broader understanding, which could be
mitigated by enlarging the datasets and making the
tasks more complex.

Moreover, we did not study the generalization of
NLP models beyond the materials science domain,
including adjacent domains such as chemistry and
physics. This targeted focus was intentional but
imposes limitations on whether the proposed tech-
niques and insights we gained from our analysis are
transferable to other domains, including applying
NLP models for scientific tasks outside of materials
science.

Another limitation of our study is the fact that we
focused on BERT-based models exclusively and did
not study autoregressive models, including large
language models with billions of parameters high-
lighted in the introduction. The primary reason
for focusing on BERT-based models was the di-
versity of available models trained on different sci-
entific text corpora. Large autoregressive models,
on the other hand, are mostly trained on general
text corpora with some notable exceptions, such as
Galactica (Taylor et al., 2022). We believe that fu-
ture work analyzing a greater diversity of language
models, including large autoregressive models pre-
trained on different kinds of text, would signifi-
cantly strengthen the understanding surrounding
the ability of NLP models to perform text-based
tasks in materials science.

While the results presented in this study indicate
that domain-specific pretraining can lead to notice-
able advantages in downstream performance on
text-based materials science tasks, we would like
to highlight the associated risks and costs of pre-
training a larger set of customized language mod-
els for different domains. The heavy financial and
environmental costs associated with these pretrain-
ing procedures merit careful consideration of what
conditions may warrant expensive pretraining and
which ones may not. When possible, we encour-
age future researchers to build upon existing large
models to mitigate the pretraining costs.

Broader Impacts and Ethics Statement

Our MatSci-NLP benchmark can help promote the
research on NLP for material science, an impor-
tant and growing research field. We expect that
the experience we gained from the material sci-
ence domain can be transferred to other domains,
such as biology, health, and chemistry. Our Text-
to-Schema also helps with improving NLP tasks’
performance in low-resource situations, which is a
common challenge in many fields.

Our research does not raise major ethical con-
cerns.

Acknowlegments

This work is supported by the Mila internal funding
- Program P2-V1: Industry Sponsored Academic
Labs (project number: 10379), the Canada CIFAR
AI Chair Program, and the Canada NSERC Discov-
ery Grant (RGPIN-2021-03115).

References

Georgios Balikas, Anastasia Krithara, Ioannis Partalas,
and George Paliouras. 2015. Bioasq: A challenge on
large-scale biomedical semantic indexing and ques-
tion answering. In International Workshop on Mul-
timodal Retrieval in the Medical Domain, pages 26-
39. Springer.

Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-
ert: A pretrained language model for scientific text.
arXiv preprint arXiv: 1903.10676.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing

systems, 33:1877-1901.

Kamal Choudhary, Brian DeCost, Chi Chen, Anubhav
Jain, Francesca Tavazza, Ryan Cohn, Cheol Woo
Park, Alok Choudhary, Ankit Agrawal, Simon JL
Billinge, et al. 2022. Recent advances and applica-
tions of deep learning methods in materials science.
npj Computational Materials, 8(1):1—26.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Co-
han, Noah A Smith, and Matt Gardner. 2021. A
dataset of information-seeking questions and an-
swers anchored in research papers. arXiv preprint
arXiv:2105.03011.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv: 1810.04805.


Annemarie Friedrich, Heike Adel, Federico Tomazic,
Johannes Hingerl, Renou Benteau, Anika Marus-
cyk, and Lukas Lange. 2020. The sofc-exp cor-
pus and neural approaches to information extrac-
tion in the materials science domain. arXiv preprint
arXiv:2006.03039.

Alexandru B Georgescu, Peiwen Ren, Aubrey R
Toland, Shengtong Zhang, Kyle D Miller, Daniel W
Apley, Elsa A Olivetti, Nicholas Wagner, and
James M Rondinelli. 2021. Database, features, and
machine learning model to identify thermally driven
metal—insulator transition compounds. Chemistry of
Materials, 33(14):5591—5605.

Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas,
Naoto Usuyama, Xiaodong Liu, Tristan Naumann,
Jianfeng Gao, and Hoifung Poon. 2021. Domain-
specific language model pretraining for biomedical
natural language processing. ACM Transactions on
Computing for Healthcare (HEALTH), 3(1):1—23.

Tanishq Gupta, Mohd Zaki, NM Krishnan, et al. 2022.
Matscibert: A materials domain language model for
text mining and information extraction. npj Compu-
tational Materials, 8(1):1-11.

Kai Hakala and Sampo Pyysalo. 2019. Biomedical
named entity recognition with multilingual bert. In
Proceedings of the 5th workshop on BioNLP open
shared tasks, pages 56-61.

Zhi Hong, Aswathy Ajith, Gregory Pauloski, Ea-
mon Duede, Carl Malamud, Roger Magoulas, Kyle
Chard, and Ian Foster. 2022. Scholarbert: Bigger is
not always better. arXiv preprint arXiv:2205.11342.

Shu Huang and Jacqueline M Cole. 2022. Batterybert:
A pretrained language model for battery database en-
hancement. Journal of Chemical Information and
Modeling.

Zach Jensen, Soonhyoung Kwon, Daniel Schwalbe-
Koda, Cecilia Paris, Rafael G6mez-Bombarelli,
Yuriy Roman-Leshkov, Avelino Corma, Manuel Mo-
liner, and Elsa A Olivetti. 2021. Discovering rela-
tionships between osdas and zeolites through data
mining and generative neural networks. ACS central
science, 7(5):858—-867.

Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W
Cohen, and Xinghua Lu. 2019. Pubmedqa: A
dataset for biomedical research question answering.
arXiv preprint arXiv: 1909.06146.

Christopher Karpovich, Zach Jensen, Vineeth Venu-
gopal, and Elsa Olivetti. 2021. Inorganic synthe-
sis reaction condition prediction with generative ma-
chine learning. arXiv preprint arXiv:2112.09612.

Edward Kim, Zach Jensen, Alexander van Grootel,
Kevin Huang, Matthew Staib, Sheshera Mysore,
Haw-Shiuan Chang, Emma Strubell, Andrew Mc-
Callum, Stefanie Jegelka, et al. 2020. Inorganic ma-
terials synthesis planning with literature-trained neu-
ral networks. Journal of chemical information and

modeling, 60(3):1194—1201.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv: 1412.6980.

Olga Kononova, Tanjin He, Haoyan Huo, Amalie Tre-
wartha, Elsa A Olivetti, and Gerbrand Ceder. 2021.
Opportunities and challenges of text mining in mate-
rials research. Iscience, 24(3):102155.

Fusataka Kuniyoshi, Kohei Makino, Jun Ozawa, and
Makoto Miwa. 2020. Annotating and extracting syn-
thesis process of all-solid-state batteries from scien-
tific literature. arXiv preprint arXiv:2002.07339.

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
Jaewoo Kang. 2020. Biobert: a pre-trained biomed-
ical language representation model for biomedical
text mining. Bioinformatics, 36(4):1234-1240.

Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming
He, and Piotr Dollar. 2017. Focal loss for dense ob-
ject detection. In Proceedings of the IEEE interna-
tional conference on computer vision, pages 2980-
2988.

Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong
Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi
Chen. 2021. Text2event: Controllable sequence-to-
structure generation for end-to-end event extraction.
arXiv preprint arXiv:2106.09232.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015. Multi-task
sequence to sequence learning. arXiv preprint
arXiv:1511.06114.

Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan
Hong, and Ed H Chi. 2018. Modeling task re-
lationships in multi-task learning with multi-gate
mixture-of-experts. In Proceedings of the 24th ACM
SIGKDD international conference on knowledge dis-
covery & data mining, pages 1930-1939.

Rubayyat Mahbub, Kevin Huang, Zach Jensen,
Zachary D Hood, Jennifer LM Rupp, and Elsa A
Olivetti. 2020. Text mining for processing con-
ditions of solid-state battery electrolytes. Electro-
chemistry Communications, 121:106860.

MatSciRE. 2022. Material science relation extraction
(matscire).

Santiago Miret, Marta Skreta, Benjamin Sanchez-
Lengelin, Shyue Ping Ong, Zamyla Morgan-Chan,
and Alan Aspuru-Guzik. Ai4mat - neurips 2022.

Sheshera Mysore, Zach Jensen, Edward Kim, Kevin
Huang, Haw-Shiuan Chang, Emma Strubell, Jef-
frey Flanigan, Andrew McCallum, and Elsa Olivetti.
2019. The materials science procedural text cor-
pus: Annotating materials synthesis procedures
with shallow semantic structures. arXiv preprint
arXiv: 1905.06939.


Elsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga
Kononova, Gerbrand Ceder, Thomas Yong-Jin Han,
and Anna M Hiszpanski. 2020. Data-driven mate-
rials research enabled by natural language process-
ing and information extraction. Applied Physics Re-
views, 7(4):041317.

Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019.
Transfer learning in biomedical natural language
processing: An evaluation of bert and elmo on ten
benchmarking datasets. In Proceedings of the 2019
Workshop on Biomedical Natural Language Process-
ing (BioNLP 2019).

Long N Phan, James T Anibal, Hieu Tran, Shaurya
Chanana, Erol Bahadroglu, Alec Peltekian, and Gré-
goire Altan-Bonnet. 2021. Scifive: a text-to-text
transformer model for biomedical literature. arXiv
preprint arXiv:2106.03598.

Ghanshyam Pilania. 2021. Machine learning in mate-
rials science: From explainable predictions to au-
tonomous design. Computational Materials Sci-
ence, 193:110360.

Chen Qu, Liu Yang, Minghui Qiu, W Bruce Croft,
Yongfeng Zhang, and Mohit Iyyer. 2019. Bert with
history answer embedding for conversational ques-
tion answering. In Proceedings of the 42nd interna-
tional ACM SIGIR conference on research and devel-
opment in information retrieval, pages 1133-1136.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21(140):1-67.

Teven Le Scao, Angela Fan, Christopher Akiki, EI-
lie Pavlick, Suzana I[li¢, Daniel Hesslow, Ro-
man Castagné, Alexandra Sasha Luccioni, Francois
Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100.

Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina,
Raul Puri, Mostofa Patwary, Mohammad Shoeybi,
and Raghav Mani. 2020. Biomegatron: Larger
biomedical domain language model. arXiv preprint
arXiv:2010.06060.

Ross Taylor, Marcin Kardas, Guillem Cucurull,
Thomas Scialom, Anthony Hartshorn, Elvis Saravia,
Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
2022. Galactica: A large language model for sci-
ence. arXiv preprint arXiv:2211.09085.

Minh Van Nguyen, Bonan Min, Franck Dernoncourt,
and Thien Nguyen. 2022. Joint extraction of entities,
relations, and events via modeling inter-instance and
inter-label dependencies. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 4363-4374.

Vineeth Venugopal, Sourav Sahoo, Mohd Zaki, Man-
ish Agarwal, Nitya Nand Gosvami, and NM Anoop
Krishnan. 2021. Looking through glass: Knowledge
discovery from materials science literature using nat-
ural language processing. Patterns, 2(7):100290.

Shoya Wada, Toshihiro Takeda, Shiro Manabe, Shozo
Konishi, Jun Kamohara, and Yasushi Matsumura.
2020. A pre-training technique to localize medical
bert and enhance biobert.

Nicholas Walker, Amalie Trewartha, Haoyan Huo,
Sanghoon Lee, Kevin Cruse, John Dagdelen,
Alexander Dunn, Kristin Persson, Gerbrand Ceder,
and Anubhav Jain. 2021. The impact of domain-
specific pre-training on named entity recognition
tasks in materials science. Available at SSRN
3950755.

Zheren Wang, Kevin Cruse, Yuxing Fei, Ann Chia, Yan
Zeng, Haoyan Huo, Tanjin He, Bowen Deng, Olga
Kononova, and Gerbrand Ceder. 2022a. Ulsa: Uni-
fied language of synthesis actions for the represen-
tation of inorganic synthesis protocols. Digital Dis-
covery.

Zheren Wang, Olga Kononova, Kevin Cruse, Tanjin He,
Haoyan Huo, Yuxing Fei, Yan Zeng, Yingzhi Sun,
Zijian Cai, Wenhao Sun, et al. 2022b. Dataset of
solution-based inorganic materials synthesis proce-
dures extracted from the scientific literature. Scien-
tific Data, 9(1):1-11.

Leigh Weston, Vahe Tshitoyan, John Dagdelen, Olga
Kononova, Amalie Trewartha, Kristin A Persson,
Gerbrand Ceder, and Anubhav Jain. 2019. Named
entity recognition and normalization applied to
large-scale information extraction from the materials
science literature. Journal of chemical information

and modeling, 59(9):3692-3702.

Shanchan Wu and Yifan He. 2019. Enriching pre-
trained language model with entity information for
relation classification. In Proceedings of the 28th
ACM international conference on information and
knowledge management, pages 2361-2364.

Kyosuke Yamaguchi, Ryoji Asahi, and Yutaka Sasaki.
2020. Sc-comics: a superconductivity corpus for
materials informatics. In Proceedings of The 12th
Language Resources and Evaluation Conference,
pages 6753-6760.


Appendix
A Experimental Details

We performed fine-tuning experiments using a sin-
gle GPU with a learning rate was 2e-5, the hidden
size of the encoders being 768, except Scholar-
BERT which is 1024, using the Adam (Kingma
and Ba, 2014) optimizer for a max number of 20
training epochs with early stopping. All models
are implemented with Python and PyTorch, and re-
peated five times to report the average performance.
The full set of hyperparameters will be provided in
our code release upon publication.

B_ Additional Text-to-Schema
Experiments

To arrive at our data presented in Table 3, we con-
ducted experiments for all the language models
across all tasks in MatSci-NLP. The results for
seven tasks in MatSci-NLP are shown in subse-
quent tables:

¢ Named Entity Recognition in Table 4.

¢ Relation Classification in Table 5.

e Event Argument Extraction in Table 6.
¢ Paragraph Classification in Table 7.

¢ Synthesis Action Retrieval in Table 8.

¢ Sentence Classification in Table 9.

¢ Slot Filling in Table 10.

The experimental results summarized in the afore-
mentioned tables reinforce the conclusions in our
analysis of (Q2) in Section 5.2 with the text-to-
schema based fine-tuning method generally out-
performing the conventional single and multitask
methods across all tasks and all language models.


Potential
NLP Model Single Task Single Task Prompt MOE No Explanations Examples Text2Schema

Choices
MatSciBERT 0.690+0.018 0.707+0.089 0.45 140.114 0.655+0.066 0.732+0.048 0.707+0.076
(Gupta et al., 2022) 0.40340.029 0.445.0.071 0.188+40.065 0.410+40.051 0.480-40.087 0.470+.0.092
MatBERT 0.705+0.011 0.796+0.029 0.691.060 0.75640.071 0.77840.015 —-0.798.40.031
(Walker et al., 2021) 0.469+0.037 0.558+40.044 0.400.070 0.524+0.088 0.547-+40.039 0.569-40.055
BatteryBERT 0.690+0.014 0.673+.0.029 0.439+40.185 0.733+0.026 0.607+0.169 0.722.40.045
(Huang and Cole, 2022) | 0.464+0.018 0.407-.0.045 0.168+0.110 0.48340.049 0.369+0.140 0.470.0.043
SciBERT 0.686+0.015 0.598.0.027 0.708+0.115 0.724+0.045 0.73440.079
(Beltagy et al., 2019) | 0.4640.035 0.29840,048 0.465+0.115 0.471+0.069 0.497+.0.091
ScholarBERT 0.20640.350 0.179.0.088 0.109+0.142 0.13440.036 0.168+.0.044 0.1680.067
(Hong et al., 2022) 0.069+0.131 0.108+.0.057 0.018.0.033 0.071+0.023 0.098+0.045 0.101+0.034
BioBERT 0.665+0.018 0.708.0.119 0.204.40.114 0.723+0.075 0.455+0.114 0.71540.031
(Wada et al., 2020) 0.40340.030 0.431+0.115 0.019-0.000 0.47440.071 0.188-40.065 0.459.0.055
BERT 0.606+0.009 0.636.0.034 0.23540.069 0.670+0.056 0.455+0.138 0.657+4.0.079
(Devlin et al., 2018) 0.304+0.024 0.382+40.041 0.055-40.040 0.441+£0.060 0.267 +0.089 0.416+0.058

Table 4: Results of named entity recognition task among seven tasks on different schema settings for various
BERT models pre-trained on different domain specific text data. For each model, the top line represents the micro-
FI score and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a
confidence interval of two standard deviations. We highlight the best performing method.

Potential
NLP Model Single Task Single Task Prompt MMOE No Explanations Examples Text2Schema
Choices
MatSciBERT 0.671.083 0.545+0,102 0.490+0.139 0.74740.128 0.800.058 0.791+40.046
(Gupta et al., 2022) 0.439.0.137 0.219.0.035 0.218+0.073 0.461.190 0.482+0.064 0.507+0.073
MatBERT 0.714+0.023 0.644+40.050 0.591 +0.267 0.871+0.020 0.80440.071 0.848+0.045
(Walker et al., 2021) 0.487+0.075 0.310+0.078 0.297+0.143 0.62340.035 0.51340.133 0.5690.019
BatteryBERT 0.594+0.085 0.592.0.084 0.423 +0.097 0.823 0.073 0.801 +0.081 0.786+0.113
(Huang and Cole, 2022) 0.359.0.075 0.297+.0.025 0.167+0.074 0.553+40.074 0.466+40.111 0.472+0.150
SciBERT 0.699.105 0.585+0,125 0.643.088 0.79940.139 0.78340.085  0.814+40.125
(Beltagy et al., 2019) 0.495+40.099 0.267+0.042 0.311+0.098 0.527+0.204 0.47440.999 0.5280.180
ScholarBERT 0.603.0.179 0.243.0.351 0.416+0.013 0.543+0.060 0.367+0.080 0.42840.148
(Hong et al., 2022) 0.178+0.186 0.0780.139 0.33440.006 0.25240.062 0.23640.119 —0.27440.110
BioBERT 0.692+0.105 0.538+0,108 0.306+.0.032 0.74340.199 0.674+0.093  0.666+0.220
(Wada et al., 2020) 0.458+0.087 0.243..9.029 0.079 0.017 0.442.40.915 0.32340.9092 0.324+0.118
BERT 0.564+0.130 0.626+0,103 0.3680,112 0.696+0.016 0.63640.091 —-0.78240.056
(Devlin et al., 2018) 0.357+0.076 0.306-.0.075 0.100+0.018 0.382+0.939 0.3824.0.043 0.494..9.061

Table 5: Results of relation classification task among seven tasks on different schema settings for various BERT
models pre-trained on different domain specific text data. For each model, the top line represents the micro-
FI score and the bottom line represents the macro-Fl score. We report the mean across 5 experiments with a
confidence interval of two standard deviations. We highlight the best performing method.


Potential

NLP Model Single Task Single Task Prompt MOE No Explanations Examples Text2Schema
Choices
MatSciBERT 0.10840.062 0.148.0.189 0.280.0.127 0.44840.091 0.484.0.015 0.436.0.066
(Gupta et al., 2022) 0.041+0.020 0.050+0.071 0.12240.063 0.251+0.075 0.29240.052 0.251-+0.075
MatBERT 0.152+0.093 0.160+0.169 0.341+0.006 0.453+0.108 0.483+.0.063 0.451.091
(Walker et al., 2021) 0.029+0.021 0.033.9.033 0.174£0.027 0.274-40.087 0.298-.9.037 0.288-0.066

BatteryBERT 0.149+0.072 0.1620.166 0.2320.196 0.397 0.105 0.438+0.063 0.443-0.023
(Huang and Cole, 2022) | 0.030+0.039 0.036.0.029 0.104+0.088 0.23340.086 0.29840.037 0.250.068

SciBERT 0.15240.123 0.1600.189 0.312+0.015 0.449.+0.079 0.442.40.135 0.451+0.07
(Beltagy et al., 2019) 0.041 -+40.068 0.033+40.032 0.159+0.024 0.259+0.072 0.2640.103 0.276-40.080
ScholarBERT 0.349+40.102 0.44440.091 0.26240.062 0.454+40.094 0.45440.095 0.431+40.081
(Hong et al., 2022) 0.250+0.101 0.253.0.103 0.102.108 0.31240.131 0.26440.102 0.296.0.144
BioBERT 0.119-40.080 0.1600.170 0.054+0.000 0.489.40.058 0.47340.034 — 0.488.40.036
(Wada et al., 2020) 0.030+0.011 0.034..0.032 0.0130.000 0.305-40.090 0.268+.0.061 0.274.0.049
BERT 0.198.0.041 0.1600.170 0.232+0.002 0.400..0.017 0.4140.064 0.418+0.053
(Devlin et al., 2018) 0.042+40.055 0.033 +0.033 0.049.0.008 0.194+0.025 0.214+0.092 0.22540.091

Table 6: Results of event argument extraction task among seven tasks on different schema settings for various
BERT models pre-trained on different domain specific text data. For each model, the top line represents the micro-
Fl score and the bottom line represents the macro-Fl score. We report the mean across 5 experiments with a
confidence interval of two standard deviations. We highlight the best performing method.

Potential
NLP Model Single Task Single Task Prompt MMOE No Explanations Choi Examples Text2Schema
oices
MatSciBERT 0.68540.074 0.673+£0.003 0.607 +0.277 0.706+£0.013 0.69440.041  0.686+40.158
(Gupta et al., 2022) 0.58840.152 0.402+0.001 0.386+0.150 0.63340.115 0.52440.175 0.5830.226
MatBERT 0.7530.031 0.671+0.002 0.673+0.001 0.727+0.089 0.649+0.039 —-0.756+0.073
(Walker et al., 2021) 0.730-40.016 0.40240.001 0.40440,004 0.601+0.212 0.509-40.155 0.69 140.188
BatteryBERT 0.663-40.088 0.621+0.160 0.626+0.113 0.63340.075
(Huang and Cole, 2022) | 0.585+0.156 0.564+0.180 0.574+0.092 0.610+0.046
SciBERT 0.690+0.074 0.673.0.002 0.568..0.289 0.70340.041 0.6620.169 9.696 0.094
(Beltagy et al., 2019) | 0.60510.150 0.402+0.001 0.370+0.089 0.598+0.204 0.562+0.202 0.546+0.243

ScholarBERT 0.620-40.161 0.60340.271 0.658+0.029 0.66240.144 0.668+0.016 0.66340.032
(Hong et al., 2022) 0.3860.150 0.37 140.122 0.407+0.010 0.53440.260 0.405+0.007 0.433 40.122
BioBERT 0.629+0.041 0.672+0.002 0.671+0.001 0.658+0.211 0.680+0.193  0.67540.144
(Wada et al., 2020) 0.507+0.033 0.402+0.001 0.401+0.001 0.58840.258 0.62240.226 0.578 0.102
BERT 0.709+.0.090 0.672+0.001 0.672+0.003 0.685+0.050 0.629+0.291 — 0.665.0.057
(Devlin et al., 2018) 0.58540.093 0.4680.283 0.40240.001 0.562+9.291 0.468+40.283 0.53240.194

Table 7: Results of paragraph classification task among seven tasks on different schema settings for various
BERT models pre-trained on different domain specific text data. For each model, the top line represents the micro-
FI score and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a
confidence interval of two standard deviations. We highlight the best performing method.


Potential

NLP Model Single Task Single Task Prompt MMOE _ No Explanations Examples Text2Schema
Choices

MatSciBERT 0.383+0.024 0.334-40.004 0.4244.0,249 0.676+0.071 0.63140.081 0.69240.179
(Gupta et al., 2022) 0.082+0.009 0.063 .0.001 0.169+.0.096 0.505+40.094 0.445 40.153 0.48440.254
MatBERT 0.346.0.006 0.334.0.001 0.54940.087 0.669-0.061 0.744+0.010 0.717+0.040
(Walker et al., 2021) 0.06740.004 0.063-.0.000 0.300-+0.045 0.497+0.086  0.557+0.082 0.549+0.091
BatteryBERT 0.280+0.004 0.33440.001 0.311+0.062 0.558+0.179 0.492+0.181 0.614+0.128
(Huang and Cole, 2022) | 0.118+0.041 0.063-.0.000 0.073.0.028 0.35840.149 © 0.28240.184 0.4190.149
SciBERT 0.281+0.009 0.334+0.001 0.45540.081 0.727 40.114 0.623+40.069 0.701 40.138
(Beltagy et al., 2019) 0.05240 .027 0.063-.0.001 0.207.095 0.564-40.137 0.45640.135 0.516+0.217
ScholarBERT 0.437+40.104 0.489.0.105 0.330-£0.007 0.389..0.001 0.389£0.001 0.322.0.260
(Hong et al., 2022) 0.19340.076 0.266-.0.105 0.0700.015 0.19040.000 0.191+0.001 0.178+0.051
BioBERT 0.300+0.015 0.324+0,001 0.33440,062 0.561+0.128 9.54540.157  -0.64740.140
(Wada et al., 2020) 0.07340.002 0.062.0.000 0.073.0.027 0.34640.133 0.347.128 0.446..0.231
BERT 0.348+40.047 0.334+0,001 0.313.0.083 0.593+0.059 0.59440.081 —- 0.656..0.099
(Devlin et al., 2018) 0.091+0.020 0.063+.0.000 0.073.0.037 0.424+0.086 9.37140.103 — 9.51540.067

Table 8: Results of synthesis action retrieval task among seven tasks on different schema settings for various
BERT models pre-trained on different domain specific text data. For each model, the top line represents the micro-
F1 score and the bottom line represents the macro-Fl score. We report the mean across 5 experiments with a
confidence interval of two standard deviations. We highlight the best performing method.

Potential
NLP Model Single Task Single Task Prompt MMOE No Explanations Examples Text2Schema
Choices
MatSciBERT 0.8880.093 0.908+0.001 0.907 +0.001 0.908.0.010 0.903+0.019  9.905.0.020
(Gupta et al., 2022) 0.60240.151 0.476.001 0.493.0.069 0.601 --0.159 0.57340.135 0.616+40.150
MatBERT 0.908.0.011 0.908-0.001 0.907+0.000 0.906.0.016 0.903+0.018 0.909+0.009
(Walker et al., 2021) | 0.441+0.038 0.476+0.001 0.476+0.000 0.645+0.025 0.600+0.089 ©—-0.614-40.134
BatteryBERT 0.908+0.012 0.907-0.000 0.908--0.000 0.895-.0.050 0.8900.036  0.907-40.002
(Huang and Cole, 2022) | 0.452+0.045 0.475 +0.001 0.476+0.000 0.67940.080 0.685+40.074 0.519+0.144
SciBERT 0.896+0.080 0.907+0.000 0.825+0,218 0.908.0.009 0.902+0.017  9.9020.020
(Beltagy et al., 2019) | 0.42140.159 0.469+0.004 0.535+0.079 0.586+0.166 0.596+0.161 9-6230.130
ScholarBERT 0.805+0.020 0.839+40.268 0.900+0.019 0.907+0.001 0.906-+0.007
(Hong et al., 2022) 0.45840.099 0.477+0.004 0.509+0,093 9.47640.001 —-0.478+0.008
BioBERT 0.908+0.001 0.907 40.001 0.907+0.001 0.910+0.012 0.899+0,.047 0.908.015
(Wada et al., 2020) 0.476.0.001 0.478.0.001 0.50340.005 0.61440.175 0.610+0.078 0.638.40.089
BERT 0.907 +0.000 0.907 +0.001 0.906+0.007 0.905+0.010 0.892+40.035 9.910 +0.016
(Devlin et al., 2018) 0.476.000 0.476.000 0.549..0.086 0.581+0.153  0.563+0.136 0.633+0.133

Table 9: Results of sentence classification task among seven tasks on different schema settings for various BERT
models pre-trained on different domain specific text data. For each model, the top line represents the micro-
FI score and the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a
confidence interval of two standard deviations. We highlight the best performing method.


Potential

NLP Model Single Task Single Task Prompt No Explanations Examples
Choices
MatSciBERT 0.083-40.047 0.419+0.074 0.43340.121 0.428+0,187
(Gupta et al., 2022) 0.087 40.045 0.182+0.043 0.169+0.069  0.169+0.075
MatBERT 0.17940.074 0.547 +40.050 0.493+0.078 0.502+0.034
(Walker et al., 2021) | 0.087+0.030 0.2760.047 0.230+40.067  0.221+0.011
BatteryBERT 0.093..0.074 0.43340.155 0.506+0.065
(Huang and Cole, 2022) | 0.009+0.012 0.211+0.056 0.236+0.072
SciBERT 0.098+40.054 0.469+0.112 0.43240.106 0.446+0.167
(Beltagy et al., 2019) 0.020-+0.021 0.207 -+40.066 0.183+0.061 0.179-+0.071
ScholarBERT 0.286+0.042 0.32340.058 0.2760.080
(Hong et al., 2022) 0.110+0.009 0.111+40.027 0.076-40.024
BioBERT 0.096.0.171 0.319+0.059 0.42440.145
(Wada et al., 2020) 0.023+0.020 0.110+0.048 © 0.177-+0.119
BERT 0.086.0.032 0.42140.137  0.476+0.079
(Devlin et al., 2018) 0.011-+0.005 0.204.0.078 0.225-40.066

Table 10: Results of slot filling task among seven tasks on different schema settings for various BERT models
pre-trained on different domain specific text data. For each model, the top line represents the micro-F1 score and
the bottom line represents the macro-F1 score. We report the mean across 5 experiments with a confidence interval

of two standard deviations. We highlight the best performing method.

