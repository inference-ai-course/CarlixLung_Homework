arX1v:2210.09389v1 [cs.CL] 17 Oct 2022

Potrika: Raw and Balanced Newspaper Datasets in the Bangla
Language with Eight Topics and Five Attributes

Istiak Ahmad®!, Fahad AlQurashi®!, and Rashid Mehmood®*"

‘Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University,
Jeddah 21589, Saudi Arabia
*High Performance Computing Center, King Abdulaziz University, Jeddah 21589, Saudi Arabia
“Corresponding author: RMehmood @kau.edu.sa

ABSTRACT

Knowledge is central to human and scientific developments. Natural Language Processing (NLP)
allows automated analysis and creation of knowledge. Data is a crucial NLP and machine learning
ingredient. The scarcity of open datasets is a well-known problem in machine and deep learning
research. This is very much the case for textual NLP datasets in English and other major world
languages. For the Bangla language, the situation is even more challenging and the number of
large datasets for NLP research is practically nil. We hereby present Potrika, a large single-label
Bangla news article textual dataset curated for NLP research from six popular online news por-
tals in Bangladesh (Jugantor, Jaijaidin, Ittefaq, Kaler Kontho, Inqgilab, and Somoyer Alo) for the
period 2014-2020. The articles are classified into eight distinct categories (National, Sports, Inter-
national, Entertainment, Economy, Education, Politics, and Science & Technology) providing five
attributes (News Article, Category, Headline, Publication Date, and Newspaper Source). The raw
dataset contains 185.51 million words and 12.57 million sentences contained in 664,880 news arti-
cles. Moreover, using NLP augmentation techniques, we create from the raw (unbalanced) dataset
another (balanced) dataset comprising 320,000 news articles with 40,000 articles in each of the eight
news categories. Potrika contains both the datasets (raw and balanced) to suit a wide range of NLP
research. By far, to the best of our knowledge, Potrika is the largest and the most extensive dataset
for news classification.

Keywords Natural Language Processing - Single-Label Text Dataset - Bangla Text Analysis - News Article Analysis -
Machine Learning - Deep Learning

1 Introduction

Knowledge is central to human and scientific developments. Natural Language Processing (NLP) allows automated
analysis and creation of knowledge. Data is a crucial NLP and machine learning ingredient. Textual datasets are
critically needed for NLP and machine learning research and development, in all human languages including English,
more so in languages such as Bangla where NLP research is limited. The existing datasets for newspaper classification
in the Bangla language are limited in their size (the number of news article, sentences, and words) and attributes (see
Section 2 where we provide a summary of the existing datasets and establish the research gap).

We present here Potrika, the largest and the most extensive dataset to date for news classification containing 185.51
million words and 12.57 million sentences in 664,880 news articles, providing five attributes, classified into eight
distinct categories. The data contains news articles for the period 2014-2020. It is a single-label dataset that can be
used for text classification, text summarization, text generation, and other NLP-related research. It is made available
publicly and freely.

Potrika comprises two datasets, a raw dataset, and a balanced dataset created using NLP augmentation techniques.
This balanced dataset contains 320,000 news articles with 40,000 articles in each of the eight news categories. Potrika
can be used as a benchmark dataset. It may, for example, be used to create a diversity of word embedding models.
Aside from that, Potrika comprises eight distinct news categories that have been carefully picked to reduce ambiguity
and, as a consequence, render it robust for text classification. It may also be used to summarize content by referring
to the news article and its associated headline. Furthermore, it can be used to investigate NLP techniques focussed on


Istiak et al.

temporal aspects, such as identifying which category has the most influence at a particular time or which keywords
are utilized more frequently over time.

This extensive dataset, Potrika, provides an opportunity to investigate and improve the efficacy of applying machine
learning and deep learning algorithms for natural language processing on textual data in the Bangla language. The
Potrika datasets (the raw dataset and the balanced dataset) are described in detail along with various dataset statistics,
examples of news article with associated categories, and data collection method, architecture, and algorithms.

The rest of the paper is organised as follows. Section 2 provides a description of the Potrika dataset and its comparison
with the existing datasets. Section 3 provides the design and methodology of data collection and Section 4 concludes
the paper.

Table 1: Potrika: Comparison with Existing Bangla Newspaper Datasets

Articles Sentences Words Categories Catecories Balanced
(K) (M) (M) (Number) 8
Economy, State
Entertainment,
International,
Sports,
Economy,
Opinion,
Entertainment,
International,
Sports,
Hossan [2] Bangladesh,
2018 40 - ~ iB Technology,
Education,
Durporobash
Art-and-literature,
North America,
Lifestyle
Economy,
International,
Entertainment,
Sports,
Science &
Technology,
Opinion, Politics
Country

Dataset

BARD [1]

018 376 - 88 5

Shahin [3]

2020 10 - - 8

Economy,
Entertainment,
International,
Sports,

National, Politics
Science &
Technology,
Education

Potrika [4]

021 665 125 185 8

2 Data Description

Data is a crucial NLP and machine learning ingredient. NLP allows automated analysis and creation of knowledge —
knowledge that is central to human and scientific developments. This paper proposes the Potrika dataset that is a vast
collection of news articles in the Bangla language collected from six prominent news portals in Bangladesh, including
Jugantor [5], Jaijaidin [6], Ittefag [7], Kaler Kontho [8], Ingilab [9], and Somoyer Alo [10].

The scarcity of open datasets is a well-known problem in machine and deep learning research. This is very much the
case for textual NLP datasets in English and other major world languages. For the Bangla language, the situation is
even worse and the number of large datasets for NLP research is practically nil. There is little research undertaken on
the Bangla text classification challenge applied to small datasets and limited article categories. The situation is evident


Istiak et al.

from Table | that compares Potrika with the existing related datasets and shows that (apart from Potrika) the existing
datasets for newspaper classification in the Bangla language are limited in their size and other attributes.

The largest dataset after the Potrika dataset (see Table 1) is the BARD [1] dataset that provides news articles alone
without giving the headings, publication dates, and the newspaper sources of the articles. Also, the BARD [1] dataset
is unbalanced. It contains around 240K articles related to the State or National category, whereas the categories
Economy, International, Entertainment, and Sports have 19K, 32K, 31K, and 50K news articles, respectively. An
unbalanced dataset is not suitable for news article classification. The other datasets mentioned in the table, Hossan [2]
and Shahin [3], have a much smaller number of news articles and these are not suitable for deep learning algorithms.
Moreover, the BARD and Hossan datasets were provided in 2018 and therefore these are relatively old. The com-
parison given in the table makes it evident that there is a scarcity of balanced and comprehensive datasets and NLP
research for Bangla news articles.

Potrika comprises two datasets, a raw dataset, which is described in Section 2.1 and a balanced dataset described in
Section 2.2.

News Article Category Headline Date Source

SHOP wrt St! Creme we tsar ener! FF Entertainment WATA GT — 2020/01/07
ates GyStr fot TATA CT FIST OBI FAA! Gar wrt oySt

SAT CCT SAPS AOA AGA WAT WHIARAT HVT
Fe ey Fen are ene sar AEA Tee oe aor AT TE

Frets 20S Tet — Educati 2016/01/01
St Be tenet ane ane aN ae ca wee een SST ATOR
atanrsrss SsIGs Fo FAT

frases nia sre fara at fetta an
Rbana fre ee neh srr Teac Shere are aN aoa ft) teijelcn
BE AGT BIS

TT GS HOST SSAA Ate $
Pa sas sews att serfea Fetes ws Politics 2018/01/11 Inqilab
TURSTOMA HST CTT HAST FI AEM FAT SUATETT °
SIA ATA TANAT THIF 8 CTSAat STAT FAA!

ae OT 9 aS spusise Fa cer ab GRE
operat . eee

asad Te TANT AAA Sa-Siaer saa aso

fee CR Te TTT HANA TAT TT TTR AR! ; @fsaread Pat

foe TRIE Fas, TS THT 3 fT crema sea National = ate ayy at 2014/07/01
TATVAMGS AHSAIS GFA FAT HST FT!

IIPS SSIS TH 8 OREACT 80 TA weit ——
WR! TWswes ACMCA CORSA sraryferert snfTs GA International 80 WAT MAI 2020/01/03
CMR, PAF WHAT WT IA VST AW ASHI °

SAHA SPAM, UA WT TGS WI SG GAS a 3H
we PTA IF FRA UPR
ial alt welt pita Siete + a ee Economy, * — 2019/06/13) Ittefaq
IAEA A!

fA SIT SSS TT Fa Fa TT BRET
z : TRO
fF elence & Bae 2020/01/19

fA FAS AT AT BRGHNT aAAMAGT TBAT HTS face ay Technology spas fet sort

Bq Faw WAS AAAI

Figure 1: Potrika: News Article Examples for Each News Category

2.1 Potrika: The Raw Dataset

The dataset contains 665K articles divided into eight distinct categories (National, Sports, International, Entertainment,
Economy, Education, Politics, and Science & Technology). We have collected five attributes for each document that
are News Article, Category, Headline, Publication Date, and Newspaper Source. The dataset can be used in a variety of
natural language processing tasks such as single-label text classification, text summarization, named entity recognition,


Istiak et al.

word embedding model generation, news analysis task, machine translation, and so on. Figure 1 shows one example
of news article for each of the eight categories with the respective four other attributes Category, Headline, Publication
Date, and Newspaper Source.

In the raw dataset folder, the news articles are organised in eight different directories respective to each of the eight
news categories. Each directory contains eight CSV files for the six newspapers (one CSV file for each newspaper
except Jugantor and Ittefaq, which have two files each due to their website structure). Each CSV file has five columns,
one for each of the five attributes that we have collected for each news article. These five attributes have already
been mentioned. After web scraping, the data is maintained in its raw form with no cleaning, stemming, or other
pre-processing. Duplicate articles were removed from the dataset. Some English symbols, punctuations, arithmetic,
and special characters can be found throughout the articles.

Figure 2 depicts the number and percentage of the news articles for each newspaper. Most of the news articles (around
85%) belong to the top three newspapers: Inqilab, Jugantor, and Ittefaq. The newspapers Kaler Kontho, Somoyer
Alo, and Jaijaidin have contributed to 11%, 3% and 1% of the news articles, respectively. Additional information
about the distribution of news articles across the six newspapers and eight news categories is shown in Table 2. The
table shows that most of the news articles came from the Inqilab newspaper. Note that the National category has the
highest number of articles among all the eight categories. The Kaler Kontho newspaper archive has zero articles in the
Entertainment category.

NEWS ARTICLE DISTRIBUTION BY NEWSPAPER

ae
Jaijaidin, 6373, 1%
d

~ Kaler Kontho, 76742, 11%

_~
Jugantor, 192036, 29%

Ingilab, 204554, 31%

Somoyer Alo, 20732, 3% Ittefaq, 164474, 25%

Figure 2: Potrika: News Article Distribution by Newspaper

Table 2: Distribution of News Articles across Six Newspapers and Eight News Categories

Ingilab Jugantor Ittefaq ae sear yer Jaijaidin Total

National 103602 82322 80388 18003 6365 4424 295104
International 47026 30352 32618 7 4026 545 114574
Sports 26197 38550 18683 17618 3492 499 =105039
Entertainment 14898 8134 11414 0 2906 333 37685
Economy 11156 6084 3718 15306 2475 47 38786
Education 663 4838 3225 12704 426 171 22027
ScienceTech 544 5993 5179 11476 396 53 23641
Politics 468 15733 9248 1628 646 301 28024
Total 204,554 192,006 164,473 76,742 20,732 6,373 664,880

Table 3 lists the number of news articles, sentences, and words for each of the eights categories in the Potrika raw
dataset. There are a total of about 665K news articles that contain 12.57 million sentences and 185.51 million words.
The data is presented in the table the descending order of the number of news article in each category. The Na-
tional category has the most articles (295K) in the dataset, followed by the Sports and International categories, 105K
and 114K articles, respectively. Education and Science & Technology categories have the lowest number of articles
compared to the other categories.


Istiak et al.

Table 3: Potrika Raw Dataset: Number of News Articles, Sentence, and Words per Category

Category Articles (K) Sentence (M) Words (M)
National 295 5.46 87.34
International 114 1.80 26.34
Sports 105 2.21 29.81
Economy 39 0.71 11.65
Entertainment 38 0.62 8.01
Politics 28 0.53 8.4
Science & Technology 24 0.32 4.8
Education 22 0.92 9.16
Total 665 12.57 185.51

Figure 3 plots the number of news articles (the blue bars) for an increasing number of words per document (news
article). The majority of news articles contain between 100 to 250 words (see the graph peaks). The number of news
articles that contain over 800 words is relatively small. The maximum number of articles in any document in the data
is approximately 8000 (7939 to be precise). The maximum number of articles or documents for any given number of
words is around 3000. The zoomed plot inside the figure is provided for the reader’s convenience. The figure also
plots the density against an increasing number of words per document. See the y-axis on the right of the figure and the
black plot around the blue bars. The maximum density is around 0.004.

3000 —J

0.0040
1000

2500 | 0.0035

0.0030

N
3
f=)
3

0.0025

of News Articles
Py
3
6

Density

1500 —

0.0020

Numbe:

0.0015
1000 —

0.0010

0.0005
600 800. 1000 1200 1400

0.0000

1600 2400 320 4800 5600 6400 7200 8000

4000
Word Count Per News Article

Figure 3: Number of News Articles against Word Count Per News Article

Figure 4 plots density and the number of news articles against the word count per document for each category. The
zoomed plots inside the figures are provided for the reader’s convenience. Note that the National news category
holds the highest number (around 1200) of news articles comprising around 300 words per article (for the peak). The
International category has just over 600 news articles with this peak (the maximum number of articles) of around 250
words per article. The majority of news articles on entertainment, sports, politics, and the economy are around 500
words in length (look at the peaks in the zoomed figures). Note also that the Entertainment news category has the
highest density of about 0.007 whereas Education has the least density of just over 0.002. The reason for the low
density for the Education category is the fact that the maximum number of news articles is relatively low (just over 50)
and are spread across the wider spectrum of the number of words per document (up to 1500 or more).

2.2 Potrika: The Balanced Dataset

An unbalanced dataset where the number of instances in the training dataset per class label is not balanced can produce
low accuracy for a predictive classification model. In the Potrika dataset, we added a folder called “BalancedDataset”
that contains a balanced dataset where each news category has 40,000 articles and the total number of articles for all
the categories is 320,000. The balanced dataset is created using NLP augmentation techniques that will be described
in Section 3.3.


Istiak et al.

Education Entertainment Sports Politics
0.007
0.0035 0.0035
aby 0.0020
0.006
0.0030
300 300 U.0050,
1000
2507 | 0,005 250 0.0025
0.0025
8 0.0015 gy 8 8 150
g 100 g 200 El] 200 Fy 125
= 800 & & FE
2 a p> 2 0004» g 0.00202 = 400 0.0020 2
z £ 0020 =
2 50 22 150 22 150 22 75 2
. o = SS a= 5
a 25 a o 5 a6 50 a
2 600 3 3 g
2 of O00 100 0.003 a 100 = 25
3 0 500 1000 1500 5 =| 0.0015 3 0.0015
2 2 2 2 0
0 500 1000 1500
50 50
400
002 0.0010 0.0010
ot oF
0.0005 0 500 .1000 0 500 1000..1500
200 0,001 0.0005, 0.0005
MV
) dh 0.0000 , 0.000 I I 0.0000 + 0.0000
Word Count Per News Article Word Count Per News Article Word Count Per News Article Word Count Per News Article
National International Economy Science&Technology
0.005
0.0030
0.0040
1200 0.005
0.0035 0.0025
0.004
300 300
1000 0.0030
250 : 250 0.004 200
0.0020
8 150 4 175
200 0.0025 =|) 200 2 125 150 0.003
: 3
Py ” 100 = 125 ss
150 $3 150 9.0035 15 a 100 a
= = = 0.0015 § ra
5 & rae Fi 5
0.0020 & 5 a % 50 4 754) is)
100 e 100 7||}) E 25 50 |) ne
2s ! = .
z | Z i) 25
0.0015 ) 0.002
sol 6b 0 500 1000 0.0010 Wh
amp | , 0 500 1000
I
or 7 0.0010 0 Mh
0 500 1000.1500 0 500 1000,1500 0.001
- gor 0.0005
0.0005 q
0 + 0.0000 - 0.000 0.0000 f +0.
0 1600 3200 4800 6400 8000 0 1600 3200 4800 6400 8000 0 1600 3200 4800 6400 8000 0 1600 3200 4800 6400 8000
Word Count Per News Article Word Count Per News Article Word Count Per News Article Word Count Per News Article

Figure 4: Density and Number of News Articles against Word Count for Each Category

3 Design and Methodology

Figure 5 depicts a high-level architecture of our data collection process. The web scraping and Python tools including
BeautifulSoup, Requests, Pandas, and others were used to collect the articles. Our data gathering procedure was
separated into two parts. First, in algorithm 1, article links (URL) are gathered from each newspaper archive, and then
algorithm 2 retrieves the news articles, Category, Heading, Publish Date, and Source from the link using the HTML
and JavaScript tags. Finally, we export the data into a CSV file.

3.1 Link Collection for News Articles

Initially, we assembled the links to the news from six Bangla newspaper archives using web scraping and python
libraries including BeautifulSoup, Requests, Pandas, etc. There are two different versions of online archives for the
newspapers Jugantor and Ittefaq; for other newspapers, there are single online archives. There are many additional
links in the archive, but we deleted all the additional links and only saved the targeted links.


Istiak et al.

Python3, Requests,
BeautifulSoup, Pandas,
Web Scraping

Fetch News Articles,

Fetch the web page
contents from front end

Fetch the targeted
news article links

Category, Heading,
Date, Source,

Figure 5: Dataset Collection Architecture

Algorithm 1 News Articles Link Collection

procedure ARTICLELINK(archive)
for <startYear to endYear> do
for <startMonth to endMonth> do
for <startDay to endDay>do
url © archive/year/month/day
content < fetch the content from the URL
links < save the link from the content
end for
end for
end for
end procedure

> Newspaper archive link

> News articles link

3.2 Data Collection

Retrieving the data from the website automatically is most challenging because an enormous amount of additional data
is frequently available on the website link. Initially, we analyzed the pattern of the URL (Uniform Resource Locator)
and found that all the newspapers have almost the same pattern in their online archive URLs. For example, the pattern
is like “newspaperlink/archive/year/month/day” or “newspaperlink/category/articleID”’, etc. Based on the pattern, we
separately fetch the content of the URL from different newspapers and retrieved the targeted data.

Algorithm 2 Data Collection

procedure DATACOLLECTION(links)
for <i = 0 to total number of links>do
content < fetch the content from the link
article < fetch article from content
headline < fetch headline from content
category < fetch category from content
time + fetch publication date from content
end for
save the dataset as CSV file
end procedure

3.3 Balancing the Dataset using NLP Data Augmentation

NLP data augmentation techniques are most commonly used on tiny and unbalanced datasets. There are three well-
known augmentation strategies. %1) Back Translation: this approach involves translating the text material into another


Istiak et al.

language and then back into the original language. This technique preserves the context of the textual data. 2) Easy
Data Augmentation (EDA) consists of four methods: synonym replacement, random insertion, deletion, and text
swapping. 3) NLP Albumentation: For duplicate value, sentences are shuffled or excluded.

1. Back Translation: this approach involves translating the text material into another language and then back
into the original language. This technique preserves the context of the textual data.

2. Easy Data Augmentation (EDA) consists of four methods: synonym replacement, random insertion, deletion,
and text swapping.

3. NLP Albumentation: for duplicate value, sentences are shuffled or excluded.

The Potrika dataset comprises a relatively small number of documents for five categories (Education, Economy, Sci-
ence & Technology, Politics, and Entertainment) compared to the other three categories, National, International, and
Sports, which have more than 100k articles for each category. We used back translation methods on the datasets from
these five categories to create a balanced dataset. We use the Google Translator API to convert Bangla text to English
and then back to Bangla. The Google Translate API only accepts legitimate text with a limit of 5000 characters; oth-
erwise, it cannot be translated. Each of the news category in the balanced dataset has 40,000 articles after performing
the data augmentation approach. The balanced dataset has 320,000 news articles in total (each of the eight categories
have 40,000 articles).

4 Conclusion, Utilization, and Outlook

The Potrika dataset introduced in this paper is the largest and the most extensive dataset for news classification. Textual
datasets are in critical demand for NLP and machine learning research and development, in all human languages
including English, more so in languages such as Bangla where NLP research is limited. Apart from our dataset
Potrika, the existing datasets for newspaper classification in the Bangla language are limited in their size, attributes,
and other properties.

We introduced the Potrika dataset in this paper, a large single-label Bangla news article textual dataset curated for
NLP research from six popular online news portals in Bangladesh. The articles were classified into eight distinct
categories providing five attributes for each news article. The raw dataset contains over 185 million words and 12
million sentences contained in nearly 665 thousand news articles. Moreover, we created and included in Potrika an
additional dataset that is balanced to facilitate those who intend to research in NLP involving a balanced dataset. The
Potrika raw and balanced datasets can be used for text classification, text summarization, text generation, and other
NLP-related research. Potrika is made available publicly and freely.

We are working on providing applications of NLP research over the Potrika datasets, particularly on investigating
performance of machine and deep learning algorithms for news classification and on the use of clustering for automatic
labelling. The research is completed, is in the write-up stage and will soon be submitted for publication. Our earlier
research on NLP has focussed in multiple languages on a range of problems including transportation [11, 12, 13],
logistics [14, 15], healthcare [16, 17, 18], and urban development and governance [19, 20, 21]. These works fall under
the broader umbrella of smart cities and societies [22, 23] using big data and artificial intelligence technologies [24, 25]
where we have developed significant research outputs. We plan to extend our research on the Bangla language for these
problems in the future. This will involve curating additional open datasets in the Bangla language and developing NLP
research over Potrika and additional datasets.

Ethics Statement

The reuse of article text from Bangladesh’s online newspaper complies with the terms of use. All the articles were
acquired with the consent of the people, groups or organizations.

Acknowledgments

The work reported in this paper is supported by the High Performance Computing Centre (HPC Center) at King Abdu-
laziz University, Saudi Arabia. The computing tasks reported in this paper were performed on the Aziz supercomputer
at the HPC Center, King Abdulaziz University. We would also like to express our gratitude to the authors of Jugantor,
Inqilab, Jaijaidin, Kaler-kontho, Ittefaq, and Somoyer Alo, for making their news article archives publicly available.


Istiak et al.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have
appeared to influence the work reported in this paper.

References

[1] Md Tanvir Alam and Md Mofijul Islam. Bard: Bangla article classification using a new comprehensive dataset. In
2018 International Conference on Bangla Speech and Language Processing (ICBSLP), pages 1-5. IEEE, 2018.

[2] Md. Zakir Hossan. 40k bangla newspaper article. september 2018. https://www.kaggle.com/zshujon/
40k-bangla-newspaper-article, 2018.

[3] Md Mahmudul Hasan Shahin, Tanvir Anmmed, Shahriar Hasan Piyal, and Md Shopon. Classification of bangla
news articles using bidirectional long short term memory. In 2020 IEEE Region 10 Symposium (TENSYMP),
pages 1547-1551. IEEE, 2020.

[4] Istiak Ahmad, Ehab Abozinadah, Fahad Al Qurashi, and Rashid Mehmood. Potrika: Raw and balanced news-
paper datasets in the bangla language with eight topics and five attributes. https://doi.org/10.17632/
v362rp78dc. 2, 2021.

[5] Jugantor, Accessed on May 27, 2020.

[6] Jaijaidin, Accessed on May 27, 2020.

[7] Daily Ittefaq, Accessed on May 27, 2020.

[8] KalerKontho, Accessed on May 27, 2020.

[9] Daily Ingilab, Accessed on May 27, 2020.

[10] SomoyerAlo, Accessed on May 27, 2020.
]

[11] Istiak Ahmad, Fahad Alqurashi, Ehab Abozinadah, and Rashid Mehmood. Deep journalism and deepjournal v1.
0: A data-driven deep learning approach to discover parameters for transportation. Sustainability, 14(9):5711,

2022.

Ebtesam Alomari, Rashid Mehmood, and Iyad Katib. Road Traffic Event Detection Using Twitter Data, Machine
Learning, and Apache Spark. In 2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced &
Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People
and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), pages 1888-1895, Leicester,
UK, aug 2019. IEEE.

Ebtesam Alomari, Iyad Katib, Aiiad Albeshri, Tan Yigitcanlar, Rashid Mehmood, and A A Sa. Iktishaf+: A
Big Data Tool with Automatic Labeling for Road Traffic Social Sensing and Event Detection Using Distributed
Machine Learning. Sensors, 21(9):2993, apr 2021.

[14] Sugimiyanto Suma, Rashid Mehmood, and Aiiad Albeshri. Automatic Detection and Validation of Smart City
Events Using HPC and Apache Spark Platforms BT - Smart Infrastructure and Applications: Foundations for
Smarter Cities and Societies. pages 55—78. Springer International Publishing, Cham, 2020.

[15] Sugimiyanto Suma, Rashid Mehmood, Nasser Albugami, Iyad Katib, and Aiiad Albeshri. Enabling Next Genera-
tion Logistics and Planning for Smarter Societies. In Procedia Computer Science, volume 109, pages 1122-1127.
Elsevier B.V., 2017.

[16] Shoayee Alotaibi, Rashid Mehmood, Iyad Katib, Omer Rana, and Aiiad Albeshri. Sehaa: A Big Data Analytics
Tool for Healthcare Symptoms and Diseases Detection Using Twitter, Apache Spark, and Machine Learning.
Applied Sciences, 10(4):1398, feb 2020.

[17] Ebtesam Alomari, Iyad Katib, Aiiad Albeshri, and Rashid Mehmood. COVID-19: Detecting Government Pan-
demic Measures and Public Concerns from Twitter Arabic Data Using Distributed Machine Learning. Interna-
tional Journal of Environmental Research and Public Health, 18(1):282, jan 2021.

[18] Sarah Alswedani, Rashid Mehmood, and Iyad Katib. Sustainable participatory governance: Data-driven dis-
covery of parameters for planning online and in-class education in saudi arabia during covid-19. Frontiers in
Sustainable Cities, 4, 2022.

Mashael; Alsulami and Rashid Mehmood. Sentiment Analysis Model for Arabic Tweets to Detect Users’ Opin-
ions about Government Services in Saudi Arabia: Ministry of Education as a case study. In Al Yamamah Infor-
mation and Communication Technology Forum, pages 1 — 8, Riyadh, 2018.

[12

faa

[13

“4

[19

—


Istiak et al.

[20] Shoayee Alotaibi, Rashid Mehmood, and Iyad Katib. Sentiment Analysis of Arabic Tweets in Smart Cities: A

[21

[24

[25

]

“4

rome

Review of Saudi Dialect. In 2019 Fourth International Conference on Fog and Mobile Edge Computing (FMEC),
pages 330-335. IEEE, 2019.

Tan Yigitcanlar, Nayomi Kankanamge, Massimo Regona, Andres Maldonado, Bridget Rowan, Alex Ryu,
Kevin C. Desouza, Juan M. Corchado, Rashid Mehmood, and Rita Yi Man Li. Artificial Intelligence Tech-
nologies and Related Urban Planning and Development Concepts: How Are They Perceived and Utilized in
Australia? Journal of Open Innovation: Technology, Market, and Complexity, 6(4):187, dec 2020.

Rashid Mehmood, S. See, I. Katib, and I. Chlamtac. Smart Infrastructure and Applications: foundations for
smarter cities and societies. Springer International Publishing, Springer Nature Switzerland AG, 2020.

Tan Yigitcanlar, Luke Butler, Emily Windle, Kevin C. Desouza, Rashid Mehmood, and Juan M. Corchado.
Can Building “Artificially Intelligent Cities” Safeguard Humanity from Natural Disasters, Pandemics, and Other
Catastrophes? An Urban Scholar’s Perspective. Sensors, 20(10):2988, may 2020.

Tan Yigitcanlar, Rashid Mehmood, and Juan M. Corchado. Green Artificial Intelligence: Towards an Efficient,

Sustainable and Equitable Technology for Smart Cities and Futures. Sustainability 2021, Vol. 13, Page 8952,
13(16):8952, aug 2021.

Tan Yigitcanlar, Juan M. Corchado, Rashid Mehmood, Rita Yi Man Li, Karen Mossberger, and Kevin Desouza.
Responsible Urban Innovation with Local Government Artificial Intelligence (AI): A Conceptual Framework
and Research Agenda. Journal of Open Innovation: Technology, Market, and Complexity, 7(1):71, feb 2021.

10
