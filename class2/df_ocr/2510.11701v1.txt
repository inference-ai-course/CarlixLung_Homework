arXiv:2510.11701v1 [cs.CL] 13 Oct 2025

Demystifying Reinforcement Learning in Agentic Reasoning

DEMYSTIFYING REINFORCEMENT LEARNING IN
AGENTIC REASONING

Zhaochen Yu'* Ling Yang®*' = Jiaru Zou? ~—- Shuicheng Yan'' Mengdi Wang?!
National University of Singapore ?University of Illinois at Urbana-Champaign
3Princeton University

©) Code: Open-AgentRL, ® Model: DemyAgent-4B

ABSTRACT

Recently, the emergence of agentic RL has showcased that RL could also effec-
tively improve the agentic reasoning ability of LLMs, yet the key design prin-
ciples and optimal practices remain unclear. In this work, we conduct a com-
prehensive and systematic investigation to demystify reinforcement learning in
agentic reasoning from three key perspectives: data, algorithm, and reasoning
mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories
with real end-to-end tool-use trajectories yields a far stronger SFT initialization;
high-diversity, model-aware datasets sustain exploration and markedly improve
RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL,
such as clip higher, overlong reward shaping, and maintaining adequate policy
entropy could improve the training efficiency. (iii) A deliberative strategy with
fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improv-
ing tool efficiency and final accuracy. Together, these simple practices consis-
tently enhance agentic reasoning and training efficiency, achieving strong results
on challenging benchmarks with smaller models, and establishing a practical base-
line for future agentic RL research. Beyond these empirical insights, we further
contribute a high-quality, real end-to-end agentic SFT dataset along with a high-
quality RL dataset, and demonstrate the effectiveness of our insights in boosting
the agentic reasoning ability of LLMs across four challenging benchmarks, in-
cluding AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With
our recipes, 4B-sized models could also achieve superior agentic reasoning per-
formance compared to 32B-sized models.

1 INTRODUCTION

Beyond pre-training and supervised fine-tuning (SFT) stages, recent advancements in reinforce-
ment learning (RL) (Schulman et al., 2017; Rafailov et al., 2023; Yang et al., 2025c;b; Shao et al.,
2024; Wang et al., 2025d;c) have introduced a new scaling axis that aligns large language mod-
els’ (LLMs) behavior to incentivize reasoning fidelity by encouraging the generation of effective
chain-of-thought (CoT) trajectories. Building on this, the paradigm of agentic reasoning (Li et al.,
2025a; Wu et al., 2025; Li et al., 2025c; Sun et al., 2025; Jin et al., 2025; Feng et al., 2025; Dong
et al., 2025b) further empowers LLMs to move beyond self-contained generation, equipping them
with the ability to integrate external tools throughout the reasoning process. This shift has unlocked
remarkable progress across domains such as mathematics, scientific discovery, and code generation.

Despite the rapid growth of these advances, scaling RL for agentic reasoning remains challenging.
Directly applying policy optimization methods such as GRPO (Shao et al., 2024; Guo et al., 2025)
often leads LLM agents to suffer from suboptimal training and inference behaviors, including ineffi-
cient on-policy rollout sampling, reward & entropy collapse (Cui et al., 2025), and unstable training
dynamics. This highlights the unsolved limitations from three perceptiveness:

“Equal Contribution. Contact: ly1988 @princeton.edu,
t Corresponding authors.


Demystifying Reinforcement Learning in Agentic Reasoning

Perspective 2: Algoithm Design

Perspective 1: Data Curation & Training Dynamics in Agentic

and Impact on Agentic RL

Perspective 3: Reasoning
Mode for Agentic Reasoning

( a
Techniques and Training Dynamics in De casoning Mode for Agentic Reasoning

AW: How to leverage entropy |

@: What's the relationship? |
for more effective}

¢Q) Which mode is Which reasoning

" better for

better? How
much better?

\ ., What are simple
impact and the ' & yet effective paradigm is more
@ = recson behind it. \ techniques ? suitable for scaling?

ee oo pettt nnn ree esess: 6 Agentic RL pornn enn en enn es, SSS %
‘scm SFT Dataset RL Dataset Qe iy oP Gc || ‘(Effective Scaling
B 4 Techniques % @ rainin namics i 1
Synthetic/Pitch- ||| | Impact of the i " Vi Eieed V yi Reactive Mode {i Long-CoT models for |!
Style Trajectories | | | Diversity | | Gliping Strategy | | 1 Exploration(pass@k)- |! | | (short-think + frequent | |) scaling of Agentic |!
‘| | ia : 1 | | Exploiatation(average@k) |! | tool calls) i Reasoning 1
ras i oss Aggregation 17 i i
Eopea End-to-End | | | | Mode-aware Data ' | |&2Ptmpact of Entropy on |! || Deliberative Mode | |!|  Instruction-based 1
Tarajectories i! Selection i | Reward Shaping | | ! | training effectiveness || @eliberate-think + |!| models for scaling |!
ae | (es i i! rit fewer tool calls) 11] Agentic Reasoning |i
H i i! ! H
t 1 1 ' ' !
4 i | i
rh 2% ‘ : % /

3 S__ training? ef NS agentic reasoning? / \

Figure 1: An overview of our research on agentic RL.

@ Data wise. Current data curation pipelines often rely on stitch-style data synthesis (Feng et al.,
2025; Schick et al., 2023), where segments of internal reasoning are manually replaced with tool
outputs. Such patchwork overlooks the natural connectivity of reasoning and tool use, preventing
the data from faithfully mimicking real multi-turn trajectories that indicate when and why tools
should be invoked.

@ Algorithm wise. Despite rapid progress in GRPO-based variants, the optimal RL recipe for
agentic reasoning remains unclear. Existing methods differ in their optimization granularity (token-,
sequence-, or trajectory-level) and impose distinct inductive biases: some encourage exploration by
relaxing clipping (Yu et al., 2025) or managing entropy (Wang et al., 2025b), while others suppress
it through strong KL regularization (Cheng et al., 2025b) or conservative clipping. A principled
understanding of when and how to deploy these algorithms is still missing.

@Reasoning Mode wise. Open puzzles are unsolved regarding the allocation of turn budgets, the
trade-off between response length and tool-call efficiency, and the impact of long-CoT predisposi-
tions on multi-turn reasoning. These uncertainties obscure the principles of agent reasoning modes,
often leading to either overthinking (long, inefficient loops) or underthinking (premature tool re-
liance) during the agents’ reasoning process.

The above challenges motivate us to perform a systematic investigation of recent studies along these
three perspectives, aiming to identify key factors that either hinder or enhance agentic reasoning in
LLM agents as shown in fig. |. Specifically, we organize our paper as follows:

* In Section 3 (to address @), we analyze how data curation design and diversity affect SFT and
RL training stages, respectively. We observe that training directly on synthetic trajectories fails to
provide reliable signals for when and how to invoke tools, preventing agents from learning optimal
integration points, and also lacks sufficient data diversity to encourage effective exploration. To
address this, we curate real end-to-end SFT dataset and high-diversity, model-aware RL dataset
that improve the training efficiency and agentic reasoning performance.

* In Section 4 (to address @), we compare GRPO-based RL algorithms and find that conservative
clipping and KL divergence penalty overly constrain exploration during training. In addition, we
analyze the roles of pass@k and average@k as guiding metrics, revealing how they capture the
exploration—exploitation trade-off and highlight performance bottlenecks. We further show that
sustaining higher entropy, especially for weaker models, is the key to improving RL efficiency.

* In Section 5 (to address @), we investigate reasoning-mode components such as the number of
tool calls and overall response length, and their relationship to performance. We find that fewer,
more deliberate tool interactions often yield better results, showing that over-reliance on external
calls does not necessarily improve performance and that the key lies in effective and accurate tool
invocations integrated into the model’s agentic reasoning process.

Beyond the insights gained from our comprehensive study, we also provide a strong baseline model
called DemyAgent-4B, which could achieve SOTA-level performance in challenging benchmarks
and outperform larger-sized models in agentic reasoning as shown in table 2. We present our main
contributions and the detailed baseline recipes in section 6.


Demystifying Reinforcement Learning in Agentic Reasoning

2 PROBLEM FORMULATION

In this section, we first formalize the notation used throughout the paper, followed by an overview of
the reinforcement learning (RL) algorithms we adopt. Finally, we outline our training and evaluation
setup.

Research Purpose

Our goal is to demystify reinforcement learning in agentic reasoning. By systematically

analyzing three dimensions: data, algorithm, and reasoning mode. We extract insightful
and practically applicable strategies that improve the stability, efficiency, and overall perfor-
mance of agentic reasoning.

2.1 AGENTIC REINFORCEMENT LEARNING

In this section, we formulate the agentic RL training objective as:

max Epp, ywno(-la;T)(Te(t,¥) | — BDxi(mo(y | 2; T) || mree(y | 2; T)) - (1)

6

where 7 denotes the set of available tools, 7 represents the policy LLM, 7reg is the reference LLM,
rg and Diz, denote the reward function and KL divergence respectively. The input x is sampled
from dataset D, and y is the corresponding output, possibly interleaved with tool-call feedback.

Unlike conventional RL that relies purely on LLM rollouts, agentic RL (Dong et al., 2025b; Feng
et al., 2025; Li et al., 2025d; Dong et al., 2025a; Singh et al., 2025) integrates tool-call feedback
during reasoning. The rollout distribution factorizes as

tr ty
t=1 t=1
a
Agentic Reasoning Answer Generation

where 72 is the reasoning trajectory of length tr, interleaved with tool-call feedback, and y is the
final answer with length ¢,,. In this paper, we mainly focus on rule-based RL algorithms like GRPO
(Shao et al., 2024), which is widely adopted to optimize LLM-based Agents.

2.2 GRPO-BASED ALGORITHM AND TECHNIQUES

Here we utilize GRPO (Shao et al., 2024) as our baseline algorithm. To better compare the difference
between RL techniques that improve GRPO algorithm, we formulate the following objective in a
more general format:

Tonvo(0) = | WD, (RIS, res

Aex(@.){ min |ri2(8) - Ait, elip(ri,e(), 1 — eow, 1 + enigh) * Ae] — 8 Dex (re | ww}
Q)

Here Agg(G, R) is the loss aggregation granularity, r; ,(0) is the importance ratio related to the
type of loss aggregation granularity, and € represents the clip ratio. A; ; is the normalized advantage

across all tokens:
A, = ro(x, ys) — mean({rg(R1),...,7¢(Re)})
. std({rg(Ri),.--,7T¢(Rea)})

In our study, we focus on three key improvement techniques (Yu et al., 2025; Zheng et al., 2025) for
GRPO: 1) Loss Aggregation Granularity, 2) Reward Shaping, 3) Clipping Strategy. For loss

(3)


Demystifying Reinforcement Learning in Agentic Reasoning

aggregation granularity, we compare two kinds of loss, which can be formulated as:

G |Ri|
Aggro (GR) = = 7 ) See (4)
al i=1
1 G
Aggs.q(G) = a>» (5)
i=,

where Aggy .(G, 7) is the token-level loss, and Agg,.,(G) is the sequence-level loss, and the cor-
responding importance ratio is formulated as:

To(Ri,(t)|Ri,<tT)

p lok 6) = :
it (9) Trey Ral aicis?) °
r$ea(g) = (Aa(Rilt) yr, a

here ri (8) is the importance ratio of token-level loss and ros) is the importance ratio of

sequence-level loss. For the reward function. We optimize a composite reward that sums an outcome
term (solution accuracy) and a tool-use term (number of invocations), with the tool bonus clipped to
avoid degenerate “tool abuse” reward hacking. It could be formulated as:

1+0.1n if match(y;,
Toutstool(L, Y, 2) = { (yt, ¥)

min(—1+0.1n) otherwise (8)
Here, n is the number of tool invocations. For another reward function, which is known as overlong
reward shaping. Specifically, overlong reward shaping gives zero reward when the output length is
within a safe budget, then applies a linear penalty as length approaches the maximum (from Lyax —
ccache to Lmax), and assigns -1 if it exceeds Lax. This preserves a smooth learning signal near the
boundary while strongly discouraging overlong completions. It can be formulated as follows:

0, ly| < res —_ Li extties
L — Leg _
Tiength(Y) = ( = L et) I ’ Imax —_ cicache ti ly| < Lmax; (9)
cache
—1, Linax < |yl-

2.3. RECIPE DESIGN

In our study, we investigate three key techniques for agentic RL: Loss Aggregation Granularity,
Reward Shaping, and Clipping Strategy. We construct three different recipes:

GRPO-TCR: We incorporate Token-level loss, Clip higher and overlong Reward shaping
techniques with GRPO.

GRPO-SCR: We incorporate Sequence-level loss, Clip higher, overlong Reward shaping
techniques with GRPO.

GRPO-T: we adhere to the implementation in (Shao et al., 2024), and change the sample-
level loss to token-level, and we take this recipe as our baseline.

3. DATA IN AGENTIC REASONING

This section empirically examines how data affects agent training, comparing real end-to-end agentic
vs synthetic stitch-style trajectories for cold-start SFT and evaluating high-diversity and model-
aware datasets designed to maintain exploration to achieve effective RL.

3.1 REAL END-TO-END TRAJECTORIES VERSUS SYNTHETIC STITCH-STYLE TRAJECTORIES

Motivation Current agentic training pipelines often rely on LLM-edited or template-based synthetic
trajectories, which replace selected reasoning steps with tool invocations like ReTool (Feng et al.,


Demystifying Reinforcement Learning in Agentic Reasoning

Table 1: Comparison between the impact of our curated real end-to-end SFT dataset and the syn-
thetic SFT dataset on AIME 2024 and AIME 2025.

Dataset & Metric Qwen2.5-7B-Instruct Qwen3-4B-Instruct-2507

w/ synthetic trajectory w/real trajectory __w/ synthetic trajectory _w/ real trajectory

AIME 2024

average @32 6.77% 17.91% (411.14 %) 4.38% 33.23% (+28.85 %)
pass @32 42.11% 57.57% (415.46 %) 35.15% 75.66% (+40.51 %)
maj@32 10.50% 27.13% (+16.63 %) 0.01% 51.64% (451.63 %)
AIME 2025

average @32 5.21% 18.24% (+13.03 %) 3.65% 29.79 % (+26.14 %)
pass @32 25.56% 48.42% (+22.86 %) 22.22% 72.88 % (+50.66 %)
maj@32 12.08% 29.18% (+17.10 %) 0.10% 45.82% (445.72 %)

2025). While scalable, such stitch-style data inevitably misses critical decision cues: not only how
to call a tool, but also when, why, and what to do next. This raises the question: do real end-to-end
trajectories provide qualitatively richer learning signals and stronger initialization for RL?

Setup. For the synthetic baseline, we directly adopt the multi-turn SFT dataset from ReTool (Feng
et al., 2025), where challenging long-CoT steps are substituted with tool invocations and responses.
For real end-to-end trajectories, we use our curated dataset mentioned in appendix A.3. We fine-
tune Qwen2.5-7B-Instruct and Qwen3-4B-Instruct-2507 on both datasets (real vs. synthetic) under
identical settings, and evaluate the agentic reasoning performance on AIME2024/AIME2025.

Result. We evaluate using average @32 (overall agent performance), pass@32 (ability boundary
(Deng et al., 2025)), and maj@32 (performance stability). As shown in table |, real trajectories
deliver a clear improvement: Qwen3-4B-Instruct-2507 trained on real data achieves 29.97% on
average @32, 72.88% on pass @32, and 45.22% on maj @32 on AIME2025. In contrast, the synthetic
baseline yields below 10% on average @32 with unstable performance and a significantly lower
ability upper bound. Thus, real trajectories establish a much stronger and more stable starting point
for RL. Unless otherwise stated, we use SFT checkpoints trained on our curated real agentic dataset,
denoted as Qwen3-4B-RA-SFT and Qwen2.5-7B-RA-SFT.

Analysis. The superiority of real trajectories lies in their ability to capture complete agentic reason-
ing behaviors through real end-to-end reasoning processes that synthetic stitching cannot replicate.
Specifically, our dataset preserves: (i) pre-call analysis, localizing which subproblems are effi-
ciently solved via tools; (ii) guarded execution, with intermediate checks; (iii) error recovery and
strategy revision, after failed attempts; (iv) self-reflection and calibration, before invoking tools.

Takeaway 3.1: Curating Agentic SFT Data

Real agentic trajectories with coherent and end-to-end tool-use behaviors can not only teach
the agent to use tools but it also scale the ability boundary and produce more stable reason-
ing, while synthetic trajectories fail.

3.2. DIVERSE DATA MAINTAINS HIGH ENTROPY IN TRAINING

Motivation. Most existing works (Feng et al., 2025; Shang et al., 2025; Li et al., 2025d; Dong
et al., 2025a) focus on purely mathematical datasets for RL training, aiming to enhance problem-
solving ability on reasoning-heavy benchmarks. While intuitive, this narrow scope overlooks a
critical factor: dataset diversity. Prior discussions of diversity in multi-task RL (Havrilla et al.,
2024; Shen et al., 2025) only emphasize outcome-level benefits. However, how diversity influences
the training dynamics, especially policy entropy and exploration efficiency, remains underexplored.

Setup. We construct our RL dataset with higher diversity in appendix A.4, for comparison, we
choose DAPO-Math-17k as the baseline. We utilize GRPO-TCR (mentioned in section 2.3) to fine-
tune Qwen3-4B-RA-SFT and Qwen2.5-7B-RA-SFT under identical hyperparameters and training
budgets.

Result. As shown on the right in fig. 2, training with the diverse dataset leads to significantly
higher entropy gain during the early stage and sustains this entropy at a higher level throughout


AIME2025 Accuracy Average@32

Demystifying Reinforcement Learning in Agentic Reasoning

O30 AIME2025 Accuracy Average@32 Policy Entropy
—— GRPO-TCR-Qwenz2.5-7B + ReTool —— GRPO-TCR-Qwen2.5-7B + ReTool
0.28 —s— GRPO-TCR-Qwen2.5-7B + Ours —e— GRPO-TCR-Qwen2.5-7B + Ours
0.26
>
Qa
2
0.24 <
Ww
>
4
0.22 =
a
8:20 0.14
0.18
0.12
0 50 100 150 200 250 300 0 50 100 150 200 250
Step Step

Figure 2: Comparison between our dataset with higher diversity and the ReTool dataset, which only
contains math problems. Left is the average @32 accuracy on AIME2025 during training based on
two different dataset. Right is the policy entropy during the training process.

convergence. This indicates that diverse data directly drives richer exploration behaviors. Moreover,
in the left figures of fig. 2, we observe faster and more efficient learning: with our diverse dataset,
the agent achieves over 50% average @32 accuracy on AIME2025 within only 150 steps, while the
DAPO-Math baseline requires 220 steps to reach the same level.

Analysis. We interpret entropy as a proxy for exploration breadth: higher entropy means the policy
continues to consider diverse reasoning paths rather than prematurely collapsing to a narrow deter-
ministic strategy (We will discuss the entropy mechanism in detail in section 4.2 and section 4.3).
Thus, dataset diversity not only improves outcome metrics but also reshapes the training dynamics
by maintaining exploration capacity, making RL both faster and more stable.

Takeaway 3.2: Dataset Construction for RL training

Diverse RL datasets sustain higher policy entropy, directly incentivizing broader exploration
and yielding faster, more stable agentic RL training.

3.3. MODEL-AWARE DATASETS FOR MORE EFFECTIVE RL

Motivation. During training, we observed a clear divergence between two models of different
capacity: Qwen3-4B-Instruct-2507 exhibited consistent and sustained policy improvement, while
Qwen2.5-7B-Instruct failed to improve despite identical algorithms and datasets, rapidly encounter-
ing a bottleneck. Specifically, the average reward of Qwen2.5 stagnated around zero, while Qwen3
consistently achieved positive rewards. This illustrates a competence—difficulty mismatch: when
the base policy is too weak relative to the dataset, it cannot extract meaningful gradients for pol-
icy updating. To address this issue, we construct a model-aware RL dataset that adapts the task
distribution to the capacity of the model.

Setup. We use our SFT model to perform 8 rollouts per problem on the 30k RL dataset, taking the
proportion of correct solutions as a proxy for problem difficulty with respect to the given model.
After trajectory verification, we discard the problems with 0% or 100% accuracy (which provide
no learning signal) and label the remainder with three difficulty levels: easy (accuracy > 0.75),
medium (0.75 > accuracy > 0.25), and hard (accuracy < 0.25). Since the Qwen3 model already
shows effective training on the full dataset, we use its empirical difficulty histogram as the target
distribution. Based on this distribution, we curate a model-aware dataset tailored for Qwen2.5 and
retrain it using the same GRPO-TCR algorithm and hyperparameters as in the original setting.

Result. As shown in fig. 3, training with the curated model-aware dataset yields significantly more
effective improvement than with the unfiltered dataset. Moreover, fig. 3 shows that the average
reward rises substantially, producing stronger and more consistent gradient signals. Consequently, it
provides more valid rewards for the computation of advantage, amplifying the gradient signals and
leading to more effective and stable RL training. After breaking the performance bottleneck, we can
collect the model-aware dataset based on its current ability for more effective training.

300


Demystifying Reinforcement Learning in Agentic Reasoning

Pen AIME2025 Accuracy Average@32 Average Reward
—— GRPO-TCR-Qwen2.5-7B + Full-30K 0.14 —*— GRPO-TCR-Qwen2.5-7B + Full-30K aa calc
—=— GRPO-TCR-Qwen2.5-7B + Model-Aware —=— GRPO-TCR-Qwen2.5-7B + Model-Aware

eS S' 2 2
N N Nu NR
N & a [o7)

AIME2025 Accuracy Average@32
S
oOo

2
b
©
I
°
u

0 25 50 75 100.125.2150. 175.—200 0 25 50 75 100 «125. «1150S 175.—S200
Step Step

Figure 3: The comparison and analysis between the impact of the 30k full dataset and our tailored
dataset for Qwen2.5-RA-SFT on subsequent RL training. Left is the average @32 performance on
AIME2025. Right is the analysis for the average reward during training.

Takeaway 3.3: Data Selection for RL training

Model-aware data provides stronger gradient signals, amplifying learning feedback to over-
come weak-model performance bottlenecks and improve RL training efficiency.

4 ALGORITHMIC DESIGN AND TRAINING DYNAMICS IN AGENTIC RL

Recent advancements in RLVR for reasoning LLMs, especially GRPO-based methods (Yu et al.,
2025; Dong et al., 2025b; Zheng et al., 2025; Zhao et al., 2025; Liu et al., 2025a;b) have demon-
strated that there are many possible techniques that could further enhance policy optimization. Other
works (Cui et al., 2025; Deng et al., 2025; Agarwal et al., 2025; Chen et al., 2025) focus on exploring
the training dynamics (e.g., entropy and pass @k) to explain why and how these improvements could
emerge. For agentic RL, however, it remains unclear (i) what techniques work best for policy op-
timization, (ii) what is the relationship between the exploration(pass @k)-exploitation(average@k),
and (iii) how does entropy affect training effectiveness, stability, and final performance.

4.1 THE IMPACT OF THE RLVR TECHNIQUES ON AGENTIC RL

Setup. In this section, we conduct our experiments based on three different recipes: GRPO-TCR,
GRPO-SCR and GRPO-T as mentioned in section 2.3. Based on the experiment results, we aim to
pinpoint simple yet effective techniques that deliver consistent improvements in final performance
and training efficiency. Specifically, we utilize the hyperparameters and reward functions mentioned
in section 2.2 to train both Qwen3-4B-RA-SFT and Qwen2.5-7B-RA-SFT based on our complete
RL dataset to compare the training dynamics of different recipes. For GRPO-TCR, the epjgn is set to
0.28, and €jow is set to 0.20, for GRPO-SCR, the epigh is set to 0.0004 and ejoy is set to 0.0003. For
GRPO-T, the € is set to 0.20 and the reward function is Toutstoo1. For both recipes that incorporated
with overlong reward shaping, the reward is denoted as rg = Youtstool + Tlength. Specifically, we
also use three metrics to comprehensively evaluate the impact of the applied RLVR techniques on
AIME2024 and AIME2025 benchmarks.

Result. First, we compare GRPO-TCR and GRPO-T to investigate the impact of clip higher and
overlong reward shaping on Agentic RL. Specifically, for Qwen3-4B-RA-SFT, GRPO-TCR has
achieved remarkable improvements compared to GRPO-T on AIME2024/AIME2025. It achieves
70.93 %/68.13% with an initial accuracy of 29.79% and 33.23% on average @32 metric within only
450 steps. In contrast, GRPO-T only achieves the best average @ 32 performance of 54.7%/ 40.93%
on AIME2024/AIME2025, which GRPO-TCR could achieve within only 100 training steps, utiliz-
ing only 25% of the training computation of GRPO-T. The results indicate that simply applying
clip higher and overlong reward shaping techniques could effectively improve the agentic rea-
soning performance and the efficiency of agentic RL.

Then, we compare GRPO-TCR and GRPO-SCR to investigate the impact of loss aggregation
granularity on agentic RL. We observe that for Qwen2.5-7B-RA-SFT, which has weak initial per-
formance and exploration capabilities, token-level loss and sequence-level loss achieve compara-


Demystifying Reinforcement Learning in Agentic Reasoning

Avg@32 Pass@32 Maj@32
70 4
mms 754
BS
— 604
at > 70 4
T38
mo © 554
Su 5 654
Sz 0 504
= 76
odc
45 4 74-4 60 4
724
40 T + T T T T T T T T T T
0 100 200 300 400 tr) 100 200 300 400 0 100 200 300 400
Avg@32 Pass@32 Maj@32
65 4
S 604
x&
min >>>]
ta 0
me © 504
eas
Ood<t a4
354
0 100 200 300 400
Avg@32 Pass@32 Maj@32
45 4
= 404
OS
a
354
RAO
age
Sw 5 34
o=VU
Sov
6* <a ein oe
20
0 100 200 300 400 0 100 200 300 400 0 100 200 300 400
Avg@32 Pass@32 Maj@32
34
= 32
>
Din 30
rN oO
No oO 2
aN& 8
cw 5
ge So 26 4
etc
244
22 4

100 200 300 400 0 100 200 300 400

Sit

0 100 200 300 400
Step Step Step
—— GRPO-T  —*— GRPO-TCR —— GRPO-SCR

Figure 4: The overall performance of our constructed three recipes: GRPO-T, GRPO-TCR, and
GRPO-SCR on AIME2024/AIME2025 benchmark.

ble average @32 performance on AIME2024/2025. However, for Qwen3-4B-RA-SFT, which has
stronger initial performance and exploration capabilities, token-level loss consistently outperforms
sequence-level loss in terms of convergence speed and peak accuracy. Specifically, token-level loss
exceeds sequence-level loss by 3.95% on AIME24 and 3.86% on AIME25 under the same training
budget. It is because the token-level loss ensures each token contributes equally to the optimiza-
tion signal, thereby leveraging the model’s exploratory capacity more effectively. This suggests that
token-level loss could improve training efficiency and agentic reasoning ability compared to
sequence-level loss for models with better initial performance and exploration ability.

Takeaway 4.1: The Effective Techniques for Agentic RL

1. Clip higher and overlong reward shaping are simple yet effective techniques to improve

the performance of Agentic RL.
2. Token-level loss outperforms sequence-level loss when the models have better exploration
ability in convergence speed, peak accuracy, and training robustness.



Demystifying Reinforcement Learning in Agentic Reasoning

4.2 EXPLORATION—EXPLOITATION DYNAMICS IN AGENTIC RL

Motivation. Prior studies (Chen et al., 2025; Deng et al., 2025) show that in conventional RL, most
gains in the pass@k metric come from the SFT stage, where diverse external solutions expand the
model’s ability bound. Subsequent RL training primarily strengthens existing internal solutions,
yielding a more deterministic policy that improves exploitation (Pass@ 1) but often suppresses fur-
ther exploration (Pass@k). However, this characterization is largely based on a self-contained gen-
eration process, where the model relies solely on its internal capacity. In contrast, agentic RL fun-
damentally changes this dynamic: the model actively interacts with external tools during reasoning,
learning not just to refine internal solutions but to optimize its ability to explore, select, and exploit
external resources. This opens the question of whether the classical exploration—exploitation trade-
off still holds, or whether agentic RL enables a qualitatively different trajectory where exploration
is maintained or even amplified through tool use.

Observation. Our experiments in fig. 4 show that in agentic RL, both GRPO-TCR and GRPO-
SCR achieve substantial and simultaneous improvements in pass@k and average@k (over 10%
gains on AIME2024/AIME2025). However, this improvement does not hold unconditionally: with
the baseline GRPO-T, we still observe the conventional trade-off where exploration is suppressed
during training. We attribute this to the overly conservative design of GRPO-T: the combination of
a restrictive clip upper bound and strong KL-regularization creates severe constraints on distribution
shift, forcing the model to maintain self-contained generation patterns and preventing it from fully
leveraging tool interactions.

Analysis. As we mentioned above, the multi-turn interactions with tools in agentic reasoning also
introduce external information during training. The information from the external tools enables
models to “think smarter” than purely ’think longer” by developing more advanced cognitive abil-
ities that autonomously utilize the tools to reason more efficiently, and learn from the feedback
signals. Consequently, incentivizing these abilities through agentic reinforcement learning recipe
like GRPO-TCR and GRPO-SCR helps to further improve the pass@k performance and lead to
higher ability bound for more effective training and better average @k performance.

What’s more, we also observe that the gap between average @k and pass@k emerges as a critical
bottleneck for training efficiency. RL training can be interpreted as a process of progressively
converting the model’s pass@k performance into actual average @k gains, with the achievable
improvement bounded by an intrinsic ceiling determined by this gap. This perspective highlights
that not only the absolute level of pass@k but also the magnitude of the average—pass discrepancy
governs how much exploration can be effectively transformed into exploitation during training.

Takeaway 4.2: Pass@k and Average @k in Agentic RL training.

1. With external tool interactions, agentic RL can jointly improve pass@k and average @k
while conventional RL failed.
2. The critical bottleneck for training efficiency is the gap between pass @k and average @k

4.3. WHEN HIGH ENTROPY DRIVES BETTER EFFICIENCY

Motivation. Recently, entropy has become a central signal in RL research, yet prescriptions di-
verge: some advocate minimizing it for more deterministic policies (Agarwal et al., 2025; Cheng
et al., 2025b), while others exploit high-entropy tokens to foster exploration and avoid early col-
lapse (Cui et al., 2025; Wang et al., 2025b;a). These views largely arise from conventional RL. In
agentic RL, ARPO (Dong et al., 2025b) observes entropy spikes after tool calls and leverages them
via adaptive rollouts, implying that tool-call steps induce useful uncertainty. This raises a central
question: is higher/lower entropy generally beneficial, or is there an optimal range beyond which
training destabilizes?

Observation. We investigate by visualizing entropy trajectories and relating them to training effi-
ciency and reasoning performance. As shown in fig. 4 and fig. 5, GRPO-T exhibits an early entropy
collapse, whereas the entropy for models (trained with GRPO-TCR and GRPO-SCR) with better
performance rises faster and stabilizes at a higher level. This suggests that greater policy entropy
is associated with more effective agentic RL training and stronger agentic reasoning, which is


Demystifying Reinforcement Learning in Agentic Reasoning

GRPO-TCR-Qwen3-4B
GRPO-SCR-Qwen3-4B
GRPO-T-Qwen3-4B
GRPO-TCR-Qwen2.5-7B
GRPO-SCR-Qwen2.5-7B
GRPO-T-Qwen2.5-7B

0.5 4

rtanyt

0.45

Policy Entropy

0.0 +

t) 50 100 150 200 250 300 350 400 450
Step

Figure 5: The analysis for the policy entropy in agentic RL training.

not aligned with the entropy minimization theory in conventional RL. Motivated by our obser-
vation, and noting that epjgn controls the exploration budget, we utilize different eqjgn to test for an
optimal entropy regime under identical training conditions.

Setup. To investigate the optimal entropy regime, we conduct experiments with different clip upper
bounds, including 0.28,0.315,0.35 for GRPO-TCR and keep all other settings the same and train
Qwen?2.5-7B-RA-SFT and Qwen3-4B-RA-SFT. We report the three metrics including average @ 32,
pass @32, and maj@32 to comprehensively evaluate when high entropy could improve training effi-
ciency and when high entropy would lead to the collapse of the agentic reasoning performance.

Results. As shown in fig. 6, we observe a non-monotonic relation between the clip upper bound
€high and training efficiency. Specifically, for Qwen2.5-7B-RA-SFT, modestly increasing €high (€.g.,
0.28 — 0.315) accelerates performance improvements. It also improves learning efficiency across
both models. For example, with epign = 0.315, we achieve equivalent performance 40% faster,
reaching the same results at step 60 that would otherwise require 100 steps when €pjgn = 0.28. How-
ever, pushing it further yields diminishing returns. For example, when we train Qwen3-4B-RA-SFT
with a higher epign = 0.35 it leads to worse training effectiveness despite a faster initial lift com-
pared to a lower ehigh, which is set to 0.28. In summary, a higher €pigh expands the exploration budget
and improves short-horizon progress, yet overly aggressive clipping eventually slows convergence,
introduces excessive entropy, which will lead to suboptimal agentic reasoning performance. It also
indicates that when the entropy becomes too high, it will also lead to instability in training.

e Qwen3-4B + GRPO-TCR on AIME2025 Qwen2.5-7B + GRPO-TCR on AIME2025
ad 26
R42 Sag
o o
gp 40 ¢
3 3
22
< 38 <
N N
iS) o)
© 36 © 20
4 <
34 —— clip=0.315 —— clip=0.315
—— clip=0.35 18 —— clip=0.35
32 —— clip=0.28 —— clip=0.28
0 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90
Step Step

Figure 6: The analysis of clipping strategy on AIME2025 benchmark. Left is the analysis for
Qwen2.5-7B models. Right is the analysis for Qwen3-4B models.

10


Demystifying Reinforcement Learning in Agentic Reasoning

Takeaway 4.3: Entropy as a Driver of Training Efficiency.

1. Agentic RL requires balanced policy entropy, which avoids both excessive entropy (insta-

bility) and insufficient entropy (premature convergence) for optimal training effectiveness.
2. Weaker models require larger clip upper bounds to escape the performance bottleneck,
while stronger models demand tighter bounds to prevent over-exploration.

5 REASONING MODES IN AGENTIC RL

A central question in agentic RL is how an agent should allocate its reasoning budget between
internal inference tokens and external tool calls. Should an effective agent rely on frequent tool
interactions with minimal internal thinking, or invest more inference tokens in deliberate reasoning
before acting? To address this, we characterize two regimes: (i) tool-call scaling, where the agent
engages in many short-think rounds with frequent tool usage, and (ii) internal reasoning scaling,
where the agent performs deeper reasoning before issuing fewer but more targeted tool calls. This
section empirically investigates these reasoning modes and identifies which strategy leads to more
efficient and effective agentic reasoning.

Tool Usage and Response Length Analysis
Qwen3-4B - Average Tool Calls Qwen2.5-7B - Average Tool Calls
10 —— GRPO-T —— GRPO-T
—*— GRPO-TCR —=— GRPO-TCR
—— GRPO-SCR 10) —— GRPO-SCR

Average Tool Calls
Average Tool Calls

ie} 50 100 150 200 250 300 350 400 450 0 50 100 150 200 250 300 350 400 450
Step Step

Qwen3-4B - Average Response Length per Round Qwen2.5-7B - Average Response Length per Round

—e— GRPOT
—e= GRPO-TCR
—— GRPO-SCR

w
i=)
i=)
3
a
N
S
is)

1000

rs
is)
S
=)

—e— GRPOT
= GRPO-TCR

—— GRPO-SCR

N
3°
f=)
i=)

a

°

3

b
2°
i=)
i=)

Average Response Length per Round
3S
S
Oo

Average Response Length per Round
ry
S
o

iS
So
5

i?) 50 100 150 200 250 300 350 400 450 0 50 100 150 200 250 300 350 400 450
Step Step

Figure 7: Analysis of the average number of tool calls and average response length per round in
Agentic RL.

5.1 WHEN FEWER TOOL CALLS LEAD TO BETTER TOOL USE

Setup. Based on the main experiment in section 4, we first visualize the average number of tool calls
and average response length per interaction round in the training process. To further investigate the
rationality and efficiency of tool usage, we filter out the correctly executed tool calling queries and
calculate the average success rate of the tool calls.

Result. As shown in fig. 7, we identify two distinct modes in agentic reasoning: Reactive Mode
(short-think + frequent tool calls) and Deliberative Mode (deliberate-think + fewer tool calls).
Relating these modes to overall performance (average @32 in fig. 4), we find that the strongest
models consistently adopt the Deliberative Mode, while weaker models predominantly fall into the
Reactive Mode. Tool-call efficiency further explains this performance gap. As shown in fig. 8,

11


Demystifying Reinforcement Learning in Agentic Reasoning

Deliberative Mode agents achieve over 70% success in tool usage, indicating that careful reasoning
before acting enables highly accurate and effective calls. In contrast, Reactive Mode agents exhibit
substantially lower success rates, as their rapid, frequent calls often yield ineffective or erroneous
results. Together, these findings highlight a clear quality-over-quantity principle: agents that invest
more inference tokens in deliberate reasoning ultimately make fewer but more successful tool calls,
leading to higher efficiency of tool use and superior task performance. Thoughtful, selective tool
usage thus consistently outperforms frequent but poorly targeted interactions.

Takeaway 5.1: Effective mode for scaling Agentic Reasoning.

Effective agentic reasoning follows a quality-over-quantity principle: investing more in de-

liberate internal reasoning before tool calls yields fewer but far more successful interactions,
leading to higher overall efficiency and stronger performance.

80
GRPO-TCR-Qwen3-4B
@
70 =
GRPO-SCR-Qwen3-4B
& 60
S&S
ov
A
+c
Lu
o
uv 50
2
5 GRPO-TCR-Qwen2.5-7B
FE
A
40 q  GRPO-SCR-Qwen2.5-7B
@ GRPO-TCR-Qwen3-4B
CRP TOwa 1B I GRPO-SCR-Qwen3-4B
% A. GRPO-TCR-Qwen2.5-7B
30 GRPO-T-Qwen2.5-7B @ GRPO-SCR-Qwen2.5-7B
4 GRPO-T-Qwen3-4B
% GRPO-T-Qwen2.5-7B

500 1000 1500 2000 2500
Response Length Per Round

Figure 8: Tool-use efficiency comparison across different models.

5.2 LIMITATIONS OF CURRENT LONG-COT MODELS IN AGENTIC RL

Motivation. Motivated by our findings in section 5.1 that scaling internal reasoning before tool calls
improves agentic reasoning, we explore whether incorporating Long-CoT models could further en-
hance agentic reasoning performance. Previous works combining Long-CoT with search engines
(Search-R1 (Jin et al., 2025), R1-Searcher (Song et al., 2025) and Search-ol (Li et al., 2025b)) have
demonstrated success on knowledge-intensive tasks. Building on this foundation, we investigate
whether such Long-CoT reasoning capabilities can be effectively used to benefit agentic reinforce-
ment learning with code interpreters on reasoning-intensive problems.

Setup. Here we directly utilize Long-CoT LLMs like Qwen3-4B-Thinking-2507 as the starting
point for RL, and utilize GRPO-TCR algorithm with the same training settings in section 4. We
report the average @k and the average number of tool calls throughout training.

Result. As shown in fig. 9, we observed that the model achieved strong average@32 performance
in the beginning, but it hardly call the tools. As training progressed, the average number of tool calls
gradually converged to zero, indicating that Long-CoT models tend to avoid invoking tools and
rely solely on internal reasoning when encountering reasoning-intensive tasks. This behavior

12


Demystifying Reinforcement Learning in Agentic Reasoning

varies significantly by task type. For reasoning-intensive tasks, the Long-CoT models tend to utilize
their internal reasoning capability to solve these tasks, thus focusing exclusively on the problem
rather than analyzing the user instruction or considering calling available tools. Conversely, when
confronting knowledge-intensive tasks that exceed their internal reasoning capabilities, these models
could actively utilize available tools such as search engines to complete the tasks.

Takeaway 5.2: Limitations of Current Long-CoT models in Agentic RL

Current open-source Long-CoT LLMs optimized for reasoning tasks cannot be directly ap-

plied in Agentic RL, since they over-rely on internal reasoning and avoid invoking tools
when encountering reasoning tasks.

Training Dynamics: Simultaneous Decline of Accuracy and Tool Usage

Accuracy decline: 3.7%
Tool calls decline: 81.9%
- 0.20

Start

0.655 7

0.6504

-0.15

0.645 4

- 0.10

0.640 4

0.635 7

°
°
uw
Average Number of Tool Calls

AIME2025 Accuracy Average@32

0.6304

- 0.00

0 10 20 30 40 50 60 70 80
Training Step

—@®— AIME2025 Accuracy —&— Avg Tool Calls |

Figure 9: The training dynamics of current Long-CoT with Agentic RL.

5.3. INTEGRATING LONG-COT WITH AGENTIC REASONING

Motivation Since we observe that current Long-CoT LLMs overly rely on internal reasoning and
avoid calling the tools for reasoning tasks in section 5.2, we further explore how to effectively
integrate Long-CoT with agentic reasoning.

Setup. To address the limitation that Long-CoT models often avoid tool calls in reasoning-intensive
tasks, we explicitly align them with agentic reasoning through SFT. Specifically, we leverage our
SFT dataset (as described in section 3) to initialize Long-CoT models, thereby guiding them to
balance deliberate internal reasoning with appropriate tool usage. This initialization enables the
models to enter reinforcement learning (RL) training with a prior for effective tool invocations.

Result. As shown in fig. 10, the SFT-initialized Long-CoT model actively utilizes tools while re-
taining strong internal reasoning, demonstrating significantly improved agentic RL performance
compared to the non-initialized version. However, despite this initial advantage, Long-CoT models
ultimately achieve only comparable performance to instruction-based models rather than surpassing
them. Analysis of response length evolution in fig. 10 reveals contrasting optimization dynamics.

Analysis. Instruction-based models concentrate on developing agentic reasoning capabilities from
scratch without specialized internal reasoning biases, enabling continuous growth through focused
tool-use learning. However, Long-CoT models face conflicting objectives: their ingrained inter-
nal reasoning patterns contradict agentic reasoning paradigms, forcing a scaling and pruning pro-
cess where gains in agentic reasoning are offset by the need to suppress over-thinking behaviors.
This dual optimization burden fragments learning efficiency, allowing instruction-based models to

13


Demystifying Reinforcement Learning in Agentic Reasoning

achieve superior scaling through concentrated capability improvement rather than divided attention
between acquiring new skills and unlearning incompatible reasoning paradigms. It reveals that direct
agentic RL training, where models develop reasoning and tool-use capabilities jointly from scratch,
outperforms training based on Long-CoT models with conflicting internal reasoning paradigms.

Takeaway 5.3: Aligning Long-CoT with Agentic RL.

1. SFT initialization with multi-turn tool-use trajectories is essential for Long-CoT models
to acquire effective tool-invocation priors before RL.

2. Instruction-based models are more suitable for agentic RL that scales the agentic reason-
ing ability from scratch compared to Long-CoT models with internal reasoning priors.

AIME2025 Accuracy Comparison Average Response Length per Round

—— Qwen3-4B-Thinking —— Qwen3-4B-Thinking
—— Qwen3-4B-Instruct —— Qwen3-4B-Instruct

wu
o
So
oe

4000

3000

Accuracy (%)

2000

Avg Response Length per Round

H
2°
f=)
eo

0 50 100 200 250 300 0 50 100 200 250 300

150 150
Training Step Training Step
Figure 10: Comparison between the instruction-based models and Long-CoT reasoning models.
Left is the average@32 performance on AIME2025. Right is the average response length during

training.

6 CONTRIBUTIONS AND COMPARISON ON CHALLENGING BENCHMARKS

Beyond the insights gained from our comprehensive study, we make the following contributions:

(i) A 3k high-quality end-to-end agentic SFT dataset,
(ii) A 30k diverse and effective RL dataset,

(iii) Two strong cold-start models (Qwen2.5-7B-RA-SFT and Qwen3-4B-RA-SFT) that en-
able broad downstream RL research,

(iv) A strong baseline model, DemyAgent-4B, that validates our training insights and achieves
SOTA performance against significantly larger models.

Training Recipe. DemyAgent-4B is trained using our complete 30k RL dataset with the GRPO-
TCR algorithm, applied to Qwen3-4B-RA-SFT as the base model. Following our study insights on
clip range optimization, we use a higher clip upper bound (€jgh = 0.315) to balance exploration and
constraint satisfaction.

Key Results. We evaluate on challenging reasoning benchmarks under two paradigms (table 2):
(1) Self-Contained Reasoning, where models rely solely on internal reasoning capabilities, and (2)
Agentic Reasoning, where models leverage external tools such as code interpreters and search en-
gines. As demonstrated in table 2, despite having only 4B parameters, DemyAgent-4B matches
or even outperforms much larger models (14B/32B) across challenging benchmarks. Notably,
DemyAgent-4B achieves state-of-the-art agentic reasoning performance, surpassing ReTool-
32B (Feng et al., 2025) and rStar2-Agent-14B (Shang et al., 2025), and even outperforming Long-
CoT models like DeepSeek-R1-Zero on AIME2025.

These results further demonstrate that our simple yet effective training recipe unlocks strong agentic
capabilities in compact models.

14


Demystifying Reinforcement Learning in Agentic Reasoning

Table 2: Overall results on challenging reasoning benchmarks grouped by domain. Higher is better
(%). The top two results are highlighted in bold and underlined. The results with * are our self-
evaluated results for self-contained reasoning. The prompts for agentic reasoning and self-contained
reasoning could be found in appendix B.

MATH Science Code
Method AIME2024 AIME2025 GPQA-Diamond LiveCodeBench-v6
Self-Contained Reasoning
Qwen2.5-7B-Instruct 16.7* 10.0* 31.3* 15.2
Qwen3-4B-Instruct-2507 63.3* 47 4 52.0* 35.1
Qwen2.5-72B-Instruct 18.9 15.0 49.0 -
DeepSeek-V3 39.2 28.8 59.1 16.1
DeepSeek-R1-Distill-32B 70.0 46.7 59.6 -
DeepSeek-R1-Zero (671B) 71.0 53.5 59.6 -
Agentic Reasoning
Qwen2.5-7B-Instruct 4.8 5.6 25,5 12.2
Qwen3-4B-Instruct-2507 17.9 16.3 44.3 23.0
ToRL-7B 43.3 30.0 - -
ReTool-32B 72.5 54.3 - -
Tool-Star-3B 20 16.7 - -
ARPO-7B 30.0 30.0 53.0 18.3
rStar2-Agent-14B 80.6 69.8 60.9 -
DemyAgent-4B (Ours) 72.6 70.0 58.5 26.8

7 RELATED WORK

Tool-integrated Reasoning. Tool-integrated reasoning (TIR) enables large language models
(LLMs) to leverage external tools such as code interpreters and search engines in order to overcome
the limitations of pure internal reasoning. This approach extends the computational and knowledge
capacity of LLMs and allows them to tackle tasks that are infeasible through text-only reasoning.
Previous TIR methods are based on prompting engineering, such as PoT (Chen et al.), template-
augmented reasoning paradigm like BoT (Yang et al., 2024b), and supervised finetuning (SFT)
methods. SFT-based methods like ToRA (Gou et al.), Tool-former (Schick et al., 2023), and Qwen-
Math-TIR (Yang et al., 2024a) train models on datasets containing tool invocation demonstrations,
teaching LLMs to follow predefined patterns of tool use. Similarly, works such as ReAct (Yao et al.),
MathCoder (Wang et al., 2024), and Mario (Liao et al., 2024) incorporate tool usage examples into
training data so that LLMs can interleave reasoning steps with external tool calls. However, these
SFT-based approaches have inherent limitations. Since models are compelled to use tools according
to the distribution of training data, they cannot develop adaptive strategies for tool use, such as de-
ciding when to invoke a tool, how often to call it, or how to balance tool use with internal reasoning.
As a result, previous SFT-driven TIR methods improve tool-following ability but lack the flexibility
and autonomy required for robust agentic behavior.

Agent Reinforcement Learning. Compared with SFT-based tool-integrated reasoning that merely
imitates demonstrations of tool usage, it is more promising to leverage reinforcement learning algo-
rithms (Shao et al., 2024; Guo et al., 2025; Yu et al., 2025; Liu et al., 2025a; Zhao et al., 2025) to
train more capable agents. Agent Reinforcement Learning (Agent RL) explicitly models tool invo-
cation as part of the action space and optimizes adaptive strategies through outcome-driven rewards,
enabling agents to move beyond static supervision toward more flexible and effective reasoning be-
haviors. Representative works include search-oriented approaches such as Search-R1 (Jin et al.,
2025) and R1-Searcher (Song et al., 2025), which train LLMs to interleave reasoning with search
engine queries. ToRL (Li et al., 2025d) demonstrates that RL can directly optimize tool use at scale,
enabling models to discover effective invocation strategies. ReTool (Feng et al., 2025) further high-
lights the benefit of RL by teaching models not only to use tools but also to decide when and how to
call them. More recently, ZeroTIR (Mai et al., 2025) takes a complementary perspective by analyz-
ing scaling laws in RL-based tool use, showing how strategic code invocation gradually emerges as
training progresses. More general frameworks such as ARTIST (Singh et al., 2025), Tool-Star (Dong
et al., 2025a), and Auto-TIR (Wei et al., 2025), further extend Agent RL to the setting of multi-tool

15


Demystifying Reinforcement Learning in Agentic Reasoning

integration. ARTIST focuses on multi-turn reasoning where agents autonomously decide not only
whether to call a tool but also which tool to invoke in complex reasoning chains. Together, these
frameworks highlight the frontier of Agent RL: building agents that generalize beyond single-tool
domains to flexibly coordinate multiple tools under reinforcement learning objectives. However,
current Agent RL methods are often tied to specific workflows and lack a systematic understand-
ing of how reinforcement learning can more generally improve tool-use ability. Key issues such as
algorithm design, tool-call efficiency, and reliance on synthetic trajectories remain underexplored.

Entropy Mechanism for Reinforcement Learning. Recent advances in reinforcement learning
have markedly improved LLM reasoning, yet entropy collapse—the failure to maintain exploration
ability under outcome-driven optimization, which remains a central obstacle for effective scaling
of RL. At the mechanism level, Cui et al. (2025) formalizes how entropy governs exploration and
identifies collapse as a key bottleneck; deepening this view, Wang et al. (2025b) show that a minority
of high-entropy tokens, rather than the low-entropy majority, disproportionately drives effective
learning. At the system level, He et al. (2025) provide empirical evidence that preserving entropy
is essential for stable, long-horizon reasoning. Building on these insights, Cheng et al. (2025a)
and Dong et al. (2025b) incorporate entropy-aware objectives into RL to better balance exploration
and exploitation in multi-turn reasoning. More recently, Deng et al. (2025) propose exploration
mechanisms in Reinforcement learning with verifiable rewards (RLVR), consolidating entropy as
a controllable signal, not merely a regularizer for promoting exploration, stability, and sustained
improvement in agentic RL.

8 DISCUSSION AND FUTURE WORK

In this section, we outline the challenges and potential future directions of reinforcement learning in
Agentic RL.

8.1 DATA-FUEL SCARCITY

Based on our analysis in section 3, we believe that the training data plays a crucial role in Agentic
RL, which determines both the training effectiveness and the scaling upper bound for agentic rea-
soning. However, for the SFT dataset that requires full end-to-end generated trajectories, it is still
computationally costly to collect. Works like s1 (Muennighoff et al.) and limo (Ye et al., 2025) have
demonstrated that the effectiveness of curating a small-sized but high-quality distilled dataset could
significantly enhance the internal reasoning ability of LLMs. These findings suggest that we could
also develop a recipe for how to curate small-sized high-quality SFT datasets, which could not only
alleviate the scarcity of data in Agentic RL, but it could also improve our understanding of agentic
behaviors through the insights of curating these datasets.

8.2 EFFECTIVE SCALING OF AGENTIC REASONING

As demonstrated in section 5, deliberate reasoning before tool invocation emerges as a superior mode
for agentic problem-solving, yet effectively scaling such reasoning behaviors remains challenging.
Our analysis in section 5.2 reveals fundamental limitations in current open-source LLMs for agentic
reasoning tasks. Based on that, exploring agent-specific reasoning frameworks that prioritize high-
level strategic planning and efficient tool orchestration, rather than relying heavily on the model’s in-
ternal reasoning capabilities, could be a promising direction for future research. Such agent-oriented
reasoning chains should emphasize problem decomposition into tool-executable subtasks, strategic
tool selection, and synthesis of tool outputs. This shift from reasoning-centric to high-level tool-
planning-centric approaches necessitates new training methodologies and evaluation frameworks
specifically tailored for agentic workflows, potentially leading to more capable autonomous agents
that can navigate complex multi-step scenarios with greater reliability. We look forward to future
research on exploring the compatible inference scaling methods for agentic reasoning.

8.3. ADDITIONAL APPLICATION SCENARIOS IN AGENTIC REASONING

In this work, we mainly focus on the code interpreter as a tool for agentic reasoning and find valuable
insights and recipes. But we can also generalize our insights from a static and single tool environ-

16


Demystifying Reinforcement Learning in Agentic Reasoning

ment to multi-tool and optimizable environment. For example, in this multi-tool environment, the
insights of encouraging exploration in agentic RL may still hold. In a more complex environment,
the correct solution to a problem consists of different possible combinations of tools, which re-
quires more exploration for the optimal strategy and the ability to select the most effective tools for
corresponding tasks.

9 LIMITATIONS

In this work, we investigate reinforcement learning for agentic reasoning from three key perspec-
tives: data, algorithm, and reasoning mode. However, our experiments are conducted on small-sized
models (e.g, 4B/7B). While this has already provided valuable insights into challenges and design
choices for Agentic RL, recent works (Vattikonda et al., 2025) has underscores RL’s extreme hyper-
parameter sensitivity, especially for larger-sized models. In particular, larger models may demon-
strate different sensitivities to reward signals, require different exploration strategies, or exhibit more
robust reasoning patterns that interact differently with RL training dynamics. We leave a more com-
prehensive study of RL with larger-sized models in broader agentic settings as an important future
work direction.

10 CONCLUSION

In this work, we conducted a comprehensive empirical study of reinforcement learning for agen-
tic reasoning across the axes of data, algorithm, and reasoning mode. For the perspective of
data curation, our findings highlight that real end-to-end multi-turn trajectories are indispensable for
building strong agentic SFT foundations, while diverse and model-aware RL datasets sustain explo-
ration and yield stable training. Algorithmically, we show that simple but effective design choices,
such as clip higher, reward shaping, and token-level loss, which substantially improve training effec-
tiveness, and that maintaining appropriate entropy is the key driver of effective agentic RL. On the
reasoning side, we find a quality-over-quantity principle: fewer but more deliberate tool calls lead to
superior efficiency, while Long-CoT priors often hinder tool adoption and slow down scaling. With
our recipes, we effectively improve the agentic reasoning of LLMs, and we conduct a comprehen-
sive evaluation across challenging benchmarks, including AIME2024/2025, GPQA-Diamond, and
LiveCodeBench-v6, which further validates our insights.

REFERENCES

Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effec-
tiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-
ing: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on
Machine Learning Research.

Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi.
Pass@ k training for adaptively balancing exploration and exploitation of large reasoning models.
arXiv preprint arXiv:2508.10751, 2025.

Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and
Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758,
2025a.

Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and
Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758,
2025b.

Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen

Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for
reasoning language models. arXiv preprint arXiv:2505.22617, 2025.

17


Demystifying Reinforcement Learning in Agentic Reasoning

Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yingian Min,
Yanzipeng Gao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: A
systematic analysis of Ilm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534,
2025.

Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui
Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering Ilm-brained multi-tool reasoner
via reinforcement learning. arXiv preprint arXiv:2505. 16410, 2025a.

Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia
Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization.
arXiv preprint arXiv:2507.19849, 2025b.

Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training
datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025.

Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,
Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in Ilms.
arXiv preprint arXiv:2504.11536, 2025.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen,
et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. In The Twelfth
International Conference on Learning Representations.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.

Alex Havrilla, Andrew Dai, Laura O’ Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fab-
rizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, et al. Surveying the effects
of quality, diversity, and complexity in synthetic data from large language models. arXiv preprint
arXiv:2412.02980, 2024.

Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang
Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner | technical report. arXiv preprint
arXiv:2505.22312, 2025.

Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando
Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free
evaluation of large language models for code. In The Thirteenth International Conference on
Learning Representations.

Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and
Jiawei Han. Search-r1: Training Ilms to reason and leverage search engines with reinforcement
learning. arXiv preprint arXiv:2503.09516, 2025.

Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baix-
uan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web
agent. arXiv preprint arXiv:2507.02592, 2025a.

Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and
Zhicheng Dou. Search-ol: Agentic search-enhanced large reasoning models. arXiv preprint
arXiv:2501.05366, 2025b.

Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and
Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability.
arXiv preprint arXiv:2504.21776, 2025c.

Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint
arXiv:2503.23383, 2025d.

Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code
interpreter output-a reproducible pipeline. CoRR, 2024.

18


Demystifying Reinforcement Learning in Agentic Reasoning

Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee,
and Min Lin. Understanding rl-zero-like training: A critical perspective. arXiv preprint
arXiv:2503.20783, 2025a.

Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan
Xiong, Ju Huang, Jian Hu, et al. Part i: Tricks or traps? a deep dive into rl for llm reasoning.
arXiv preprint arXiv:2508.08221, 2025b.

Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl
scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv
preprint arXiv:2505.07773, 2025.

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple test-time
scaling. In Workshop on Reasoning and Planning for Large Language Models.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in neural information processing systems, 36:53728-53741, 2023.

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Di-
rani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a bench-
mark. In First Conference on Language Modeling, 2024.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro,
Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can
teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539-—
68551, 2023.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv: 1707.06347, 2017.

Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng
Dong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. arXiv
preprint arXiv:2508.20722, 2025.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathemati-
cal reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.

Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan.
Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv
preprint arXiv:2503.22230, 2025.

Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool
integration for Ilms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025.

Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang,
and Ji-Rong Wen. RI-searcher: Incentivizing the search capability in Ilms via reinforcement
learning. arXiv preprint arXiv:2503.05592, 2025.

Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang,
Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of Ilms without
searching. arXiv preprint arXiv:2505.04588, 2025.

Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.

Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar,
Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Mufioz-Marmol, Sahar Omidi
Shayegan, Stefania Raimondo, et al. How to train your llm web agent: A statistical diagnosis.
arXiv preprint arXiv:2507.04103, 2025.

19


Demystifying Reinforcement Learning in Agentic Reasoning

Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, and Wenhu Chen. Emergent hi-
erarchical reasoning in Ilms through reinforcement learning. arXiv preprint arXiv:2509.03646,
2025a.

Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi
Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in Ilms for en-
hanced mathematical reasoning. In 12th International Conference on Learning Representations
(ICLR 2024). International Conference on Learning Representations, ICLR, 2024.

Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen,
Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive
effective reinforcement learning for IIm reasoning. arXiv preprint arXiv:2506.01939, 2025b.

Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutioniz-
ing reinforcement learning framework for diffusion large language models. arXiv preprint
arXiv:2509.06949, 2025c.

Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving Ilm coder and unit
tester via reinforcement learning. The Thirty-ninth Annual Conference on Neural Information
Processing Systems, 2025d.

Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. Autotir: Autonomous
tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.2 1836, 2025.

Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang,
Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking
agency. arXiv preprint arXiv:2505.22648, 2025.

An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jian-
hong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical
expert model via self-improvement. arXiv preprint arXiv:2409. 12122, 2024a.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint
arXiv:2505.09388, 2025a.

Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez,
and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models.
Advances in Neural Information Processing Systems, 37:113519-113544, 2024b.

Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux: Hierarchical Ilm reasoning via
scaling thought templates. arXiv preprint arXiv:2502.06772, 2025b.

Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E Gonzalez, Bin Cui, and Shuicheng
Yan. Supercorrect: Supervising and correcting language models with error-driven insights. 13th
International Conference on Learning Representations (ICLR 2025), 2025c.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan
Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International
Conference on Learning Representations.

Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more
for reasoning. arXiv preprint arXiv:2502.03387, 2025.

Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian
Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source Ilm reinforcement learning system
at scale. arXiv preprint arXiv:2503.14476, 2025.

Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shao-

han Huang, Lei Cui, Qixiang Ye, et al. Geometric-mean policy optimization. arXiv preprint
arXiv:2507.20673, 2025.

20


Demystifying Reinforcement Learning in Agentic Reasoning

Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang,

Yugiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint
arXiv:2507.18071, 2025.

Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonflux-

prm: Trajectory-aware prms for long chain-of-thought reasoning in Ilms. The Thirty-ninth Annual
Conference on Neural Information Processing Systems, 2025.

21


Demystifying Reinforcement Learning in Agentic Reasoning

A EXPERIMENT SETUP

A.1 TRAINING SETUP

We choose Qwen?2.5-7B-Instruct (Team, 2024) and Qwen3-4B-Instruct-2507 (Yang et al., 2025a) as
our base models. For datasets, we specifically curate 3k actual agentic trajectories for SFT and 30K
high-quality RL data, including math, science, and code (For more detail, please refer to section 3).
For training, we employ VeRL framework, and we conduct all our experiments on 8 x Tesla-A 100-
80G GPUs. Regarding hyperparameters for SFT, we train the base models for 5 epochs with batch
size of 32, we utilize AdamW Optimizer with an initial learning rate of 5e-5, and the max response
length is set to 32768. For RL training, we train 3 epochs with the batch size of 64 and the learning
rate of le-6; the max prompt length is set to 2560. For GRPO baseline, we set the KL loss coefficient
8 to 0.001, and the clip ratio € = 0.2 along with token-level loss aggregation, the max response
length is set to 16384.

A.2. EVALUATION SETUP

We focus on four challenging benchmarks, including AIME2024, AIME2025, GPQA-Diamond
(Rein et al., 2024), and LiveCodeBench (Jain et al.). By default, we set the temperature to 1.0
and top_p to 0.6 and the maximum response length to 16384. For each problem, we sample 32
times to comprehensively evaluate average@32, pass@32, and maj@32 for AIME2024/2025 and
GPQA-Diamond, for LivecodeBench, and we evaluate pass @ 1 and pass @5 according to its official
evaluation guideline.

A.3 SFT DATASET

For our self-curated real end-to-end trajectories, we utilize Qwen3-Coder-30B-A3B as the teacher
model and roll out multi-turn interactions via the open-source Qwen-Agent framework with Sand-
BoxFusion as the code interpreter. The SFT problems are drawn from three sources: s1-1k (Muen-
nighoff et al.), our self-curated 3k LeetCode dataset, and a 2k ReTool multi-turn SFT set, yielding 6k
problems in total. We generate trajectories for all 6k tasks and score the 3k LeetCode and 2k ReTool
subsets using ReasonFlux-PRM (Zou et al., 2025) to filter for high-quality data. We then retain the
top 1k LeetCode and 1k ReTool trajectories and keep s1-1k, resulting in a 3k real-trajectory dataset.

A.4. RL DATASET

To investigate how diversity influences the training dynamics, we construct a diverse 30k-sample RL
dataset by combining 17k DAPO-Math samples (Yu et al., 2025), 4902 math and 3586 code samples
from Skywork-or1 (He et al., 2025), and 3k science problems from MegaScience (Fan et al., 2025).

B PROMPT TEMPLATE

Since our constructed dataset encompasses diverse question types, including mathematics, program-
ming, and scientific problems, the output answer formats vary across different problems. To stan-
dardize the accuracy of model output formats while encouraging the model to think and utilize tools
simultaneously, we have designed distinct prompts for different task types and reasoning paradigms
(agentic reasoning and self-contained reasoning). In this chapter, we will present these prompts.

B.1 PROMPT FOR AGENTIC REASONING

In this section, we present the prompts we utilized during agentic RL training and evaluation for
agentic reasoning on different benchmarks.

22


Demystifying Reinforcement Learning in Agentic Reasoning

Prompt Template for verifiable Math/Science Problems

Analyze and solve the following [math/science domain] problem step by step.

Problem: [Insert problem text here]

Hint: The tool could be used for more precise and efficient calculations and could help you to verify
your result before you reach the final answer.

Note: You should first analyze the problem and form a high-level solution strategy, then utilize the
tools to help you solve the problem.

Answer Format: Do not put units of the final answer inside \boxed{}. The content of \boxed{}
should be the numerical value of the final answer only, without any units.

Remember once you make sure the current answer is your final answer, do not call the tools again and
directly output the final answer in the following text format, the answer format must be: \boxed{’The
final answer goes here.’ }.

Prompt Template for Scientific QA Problems

Analyze and solve the following [science domain] problem step by step.

Problem: [Insert problem text here]

Hint: The tool could be used for more precise and efficient calculations and could help you to verify
your result before you reach the final answer.

Note: You should first analyze the problem and form a high-level solution strategy, then utilize the
tools to help you solve the problem.

Answer Format: Remember once you make sure the current answer is your final answer, do not call
the tools again and directly output the final answer in the following text format, the answer format
must be: \boxed{’The final answer goes here.’ }. You need to put the final uppercase letter option of
this problem into \boxed{ }.

Prompt Template for Code Problems

You will be given a question (problem specification) and will generate a correct Python program that
matches the specification and passes all tests.

Problem: [Insert problem text here]

Public Examples: Here are some input and output examples of the expected code: Input: [sample
inputs] Output: [sample outputs]

Note: You should first analyze the problem and form a high-level solution strategy, then utilize the
tools to help you solve the problem.

Instruction: Read the inputs from stdin, solve the problem, and write the answer to stdout (do not
directly test on the sample inputs). Enclose your code within the delimiters shown below. Ensure that
when the Python program runs, it correctly reads inputs, executes the algorithm, and writes output to
stdout.

Submit: Before submitting your code, you can utilize tools to check its correctness. Once you make
sure the current code is correct, do not call the tools again and submit your code within the following
Python code block:

VA

python
# YOUR CODE HERE

VAN

B.2 PROMPT FOR SELF-CONTAINED REASONING

We present the prompts for evaluating the self-contained reasoning abilities of open-source models
like Qwen3-4B-Instruct-2507 in this section.

23


Demystifying Reinforcement Learning in Agentic Reasoning

Prompt Template for AIME2024

You are an expert mathematician specializing in competition mathematics. You excel at solving chal-
lenging problems from contests like AIME, AMC, and IMO.

Problem: {problem}

Instructions:

1. Read the problem carefully and identify what is being asked.

2. Plan your approach and identify relevant mathematical concepts (algebra, geometry, number theory,
combinatorics, etc.).

3. Work through the problem step-by-step, showing all your reasoning.

4. Perform all calculations carefully and check your work.

5. Simplify your final answer to match the required format.

Formating Requirements:

- AIME answers are always integers between 0 and 999.

- If the problem asks for m+n, a+b+c, or similar, compute the final sum.

- You MUST put your final numerical answer in \boxed{’verifiable answer here...’} notation.

- Example: If your answer is 123, write \boxed{123}.

- Example: If you need to find m+n where m=25 and n=8, write \boxed{33}.

Do NOT: - Put formulas or expressions in the box (like \boxed{m+n}).

- Include units or text in the box.

- Leave the answer in fraction form if an integer is requested.

Begin your solution:

Prompt Template for GPQA-Diamond

You are an expert in science with deep knowledge in physics, chemistry, and biology.
Question: {problem}

Instructions:

1. Carefully analyze the question and all provided options

2. Apply relevant scientific principles and reasoning

3. Think step-by-step through the problem

4. Consider edge cases and eliminate incorrect options

5. Provide your final answer in the format: \boxed{’The final answer of the option letter.’ }
Format Requirements:

- Your answer must be one of the given options (A, B, C, or D)

- You MUST put your final answer which is the option letter in \boxed{’The final answer of the option
letter.’ } notation.

- Example: \boxed{’B’}

Begin your analysis:

24
