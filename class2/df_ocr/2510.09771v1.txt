PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework
Using Majority Voting and Keyword Similarity for Bengali Hate Speech
Detection

Rakib Hossan
Bangladesh University of Business
and Technology, Bangladesh
rakib911hossan@gmail .com

Abstract

Content Warning: This paper contains con-
tent that some readers may find inappropriate
or disturbing.

The BLP-2025 Task 1A requires Bengali
hate speech classification into six categories.
Traditional supervised approaches need ex-
tensive labeled datasets that are expensive
for low-resource languages. We developed
PromptGuard, a few-shot framework combin-
ing chi-square statistical analysis for keyword
extraction with adaptive majority voting for
decision-making. We explore statistical key-
word selection versus random approaches and
adaptive voting mechanisms that extend clas-
sification based on consensus quality. Chi-
square keywords provide consistent improve-
ments across categories, while adaptive voting
benefits ambiguous cases requiring extended
classification rounds. PromptGuard achieves
67.61 micro-Fl, outperforming n-gram base-
lines (60.75) and random approaches (14.65).
Ablation studies confirm chi-square based key-
words show the most consistent impact across
all categories. !

1 Introduction

The widespread presence of hate speech on on-
line platforms threatens user safety worldwide.
While Natural Language Processing has advanced
through transformer models (Vaswani et al., 2017),
pre-trained models like BERT (Devlin et al.,
2019), and large language models (Brown et al.,
2020), Bengali faces unique challenges in auto-
mated content moderation due to its rich morphol-
ogy and culturally specific expressions of toxicity.
Additionally, the field suffers from a lack of large-
scale annotated datasets (Romim et al., 2021; Das
and Mukherjee, 2023). The BLP-2025 Task 1A
(Hasan et al., 2025a,b) requires classifying Ben-
gali text into six categories: none, sexism, abusive,

‘https ://github.com/Rakib911Hossan/PromptGuard

Shubhashis Roy Dipta
University of Maryland, Baltimore
County, USA
sroydip1@umbc. edu

profane, religious hate, and political hate. This
task comes with some significant challenges: se-
vere class imbalance with limited training data for
Bengali hate speech detection (Faria et al., 2024),
the scarcity of comprehensive datasets (Al Maruf
et al., 2024), and resource constraints common
to low-resource language processing (Magueresse
et al., 2020).

We introduce PromptGuard, a statistically
grounded few-shot framework combining: (1) chi-
square analysis for discriminative keyword ex-
traction (Azzahra et al., 2021), (2) systematic
two-phase example selection inspired by few-shot
learning principles (Brown et al., 2020), and (3)
adaptive majority voting (Dietterich, 2000) that ex-
tends decisions when consensus is unclear. Our ap-
proach demonstrates that few-shot methods com-
bining statistical feature selection and adaptive
voting can achieve competitive performance with-
out requiring extensive labeled datasets.

To summarize, our contributions are: (1) novel
integration of chi-square feature selection with
few-shot prompting, (2) adaptive majority voting
with dynamic decision extension, and (3) compre-
hensive ablation studies validating each compo-
nent’s effectiveness.

2 PromptGuard

PromptGuard combines chi-square keyword ex-
traction, adaptive example selection, and majority
voting with consensus extension to achieve robust
Bengali hate speech classification without exten-
sive labeled datasets. The whole method is illus-
trated in Fig. 1.

2.1 Balanced Dataset Construction

The first step creates a balanced training pool by
sampling 120 examples per category, resulting in
720 examples across 6 categories. This size was
constrained by the smallest category (sexism) to


Category Top-2 Keywords 7 Score
Profane et 2980.27
afta 1559.10
Political Hate Culp 1738.07
fester 1534.63
Religious Hate pier 1378.39
fey 1280.53
Sexism val 871.51
vata 801.57

Table 1: Top-2 discriminative keywords per hate

speech category ranked by chi-square scores. Profane
category shows the highest statistical association (x?
= 2980.27), while sexism exhibits the lowest scores
among hate categories (? = 871.51).

ensure balanced representation without data aug-
mentation. Random sampling with a fixed seed
ensures reproducibility and prevents category bias.

2.2 Statistical Keyword Extraction using
Chi-Square Testing

We have used the chi-square (7) to identify words
most statistically associated with each hate speech
category (Forman, 2003), ensuring genuine dis-
criminative power over arbitrary selection. Table 1
presents the highest-scoring keywords for each cat-
egory based on chi-square analysis. The prepro-
cessing pipeline applies Unicode filtering, mini-
mum document frequency of 5, and maximum of
95% to retain discriminative Bengali vocabulary.

Manual Keyword Refinement The statistically
extracted keywords undergo manual filtering for
cultural sensitivity while maintaining balanced
representation across categories. The refined key-
word sets include:

¢ Abusive: Wee, fofS, set, cola, Fras, M17,
Gel, AER, SPT

¢ Profane: aa, sift, aafe, Il, WH, ATI,
We, “Ne, WACOM, Pel, Gate, cate, w-
Gira

¢ Religious Hate: spiny, fey, Raf, SPIT,
Ter, af, DTT, HICHA, Wife, aA, cara,
Teg

* Political Hate: Corb, aan, ane, a,
waa, Picfor, Aa, Alay, Garry, ono, wer,
Cola, 4 Reriife

Run 3
Generations

Winner

Return Best
Result

Add Extension
Turn

Figure 1: Adaptive majority voting algorithm. The sys-
tem executes 3 parallel classification attempts with dif-
ferent examples. If no clear majority (>50%) emerges,
it iteratively adds rounds with random examples un-
til consensus is reached or 10 maximum iterations are
completed.

¢ Sexism: aval, Aafera, BICGIN Aea, faeral, fa-
CA, er, AIGA, CIC, AVI, Sars

2.3. Enhanced Prompt Engineering

The method uses a structured prompt (§A.1) tem-
plate that combines statistical keywords with few-
shot examples to improve classification accuracy.
Specifically, the prompt integrates three key com-
ponents: (1) Examples for each category, (2) Key-
words from each category, (3) step-by-step reason-
ing guidelines.

2.4 Dynamic Few-Shot Learning Strategy

The example selection method uses two phases
to balance diversity, as sample selection strate-
gies significantly impact few-shot learning perfor-
mance (Pecher et al., 2024). In the first phase (3
turns), examples are selected sequentially from a
pool of 120 instances without reuse, using 20 ex-
amples per category per turn. For future turns, if
needed, the system extends voting for up to 10 ad-
ditional turns using random selection of examples
to provide fresh perspectives on ambiguous cases.
If consensus remains unclear after maximum itera-
tions, the system selects one random winner from
the tied ones (the label with the highest vote count)
as the final decision.

2.5 Robust Majority Voting with Adaptive
Extension

The voting mechanism operates in two phases
building on self-consistency approaches (Wang
et al., 2022):


Model
micro-fl 14.65 57.60 60.75 67.61 73.62 73.45 73.40

| Random Majority n-gram PromptGuard Rank-1 Rank-2  Rank-3

Table 2: PromptGuard performance compared to base-
lines and top-performing systems on 10,000 test in-
stances. While outperforming simple baselines by 7-53
micro-F1 points, a 6-point gap remains with leading ap-
proaches.

Initial Phase: Execute no = 3 parallel vot-
ing turns {V), V2, V3}, each using distinct example
sets E; where E; E; = 0 fori F 7.

Adaptive Extension: If no clear majority
emerges, iteratively add voting turns V3+; for k =
1,2,...,10, where each V3,; uses freshly sam-
pled examples F3,;, ~ D. The process terminates
when max,|S.| > 4|$| or maximum iterations
are reached, where 5S, represents votes for candi-
date c.

3 Results

3.1 Experimental Setup

We use Qwen/Qwen3-32B with temperature=0 and
parallel processing for efficiency. Following the
official shared task, we have used micro-f1 as the
main evaluation. To give more insights into the re-
sult, we have also provided confusion matrix with
fine-grained analysis for each metric.

3.2 Discussion

As a baseline, we have provided the random, ma-
jority and n-gram baseline. To compare with other
participants’ work, we have also reported the 1“,
2™4 and 3™ results from the official leaderboard.
We have reported the results on the Table 2. While
PromptGuard achieves better scores than the orig-
inal baselines, still it lags behind compared to the
Rank-{1,2,3} models.

To perform a fine-grained analysis of class-wise
performance, we present the confusion matrix in
Fig. 2. The breakdown reveals that the model ex-
hibits a bias toward labeling instances as non-hate,
more than any other category. The poorest perfor-
mance is observed in the “Abusive” category. One
possible reason for this is that many of the key-
words associated with the Abusive class, i.e., fofs,
fa, ANeT, Gol, TSI, wifrt — can also appear in
benign, non-hateful contexts. This semantic over-
lap makes the classification of the “Abusive” cate-
gory particularly challenging.

5000

Abusive- 471 1093 521 87 120 50

4000
None- 106 5023 443 26 11 42

Political Hate- 39 241 893 22 23 2 3000

True Class

- 2000

- 1000

Sexism - 0 1 0 0 0 18

¢ CG ¢
; KR x x
SS RS Rs $ ¥
Re ¥ ¥
RS Ss
& &
Predicted Class

Figure 2: Per-category breakdown reveals uneven per-
formance across hate speech types. Non-hate content is
classified most accurately, while abusive language de-
tection proves most challenging.

4 Ablation Studies

Due to the high computation and time needed to
run on the whole test set (10k), we sampled a bal-
anced subset from the test set. In the test set, the
sexism category has the lowest number of exam-
ples (29). We therefore sampled a balanced subset
of 174 instances (29 per label).

4.1 Impact of Similar Keywords

As described in §A, our final model utilizes a well-
crafted prompt that includes both in-context exam-
ples and targeted keywords. To assess the contribu-
tion of these keywords, we compare performance
against a baseline using a basic prompt without the
keywords. Both prompt versions are provided in
SA.

On our sampled test dataset, the keyword-
free prompt achieves a micro-F1 score of 57.47,
whereas the prompt with keywords reaches 59.77,
highlighting the effectiveness and importance of
including targeted keywords in the prompt design.

4.2 Impact of Shots & Turns

In our original configuration, we used 20 examples
per label in the prompt and 3 initial turns. To eval-
uate the impact of these two parameters, we con-
ducted a controlled analysis by varying one while
keeping the other fixed. Specifically, we experi-
mented with {3, 7, 10, 16, 20} shots and {3, 7, 10,
16} turns. The results of this ablation are presented
in Fig. 3.

The results indicate that the best performance is
achieved with 3 shots and 3 turns. We hypothesize


=O= Shot
66 -t Tums

Micro F1 Score

Number of Shots/Turns

Figure 3: Impact of varying the number of examples
and turns on final evaluation performance. We observe
a negative correlation with the number of examples,
likely attributable to the “lost-in-the-middle” problem
(Liu et al., 2023).

that this is due to the “lost-in-the-middle” effect
in LLMs (Liu et al., 2023), where too many in-
context examples can reduce the model’s focus on
relevant inputs. In contrast, varying the number of
turns has minimal impact on performance. This
aligns with our expectations, as the initial turns
are primarily used to identify a clear winner; ad-
ditional turns are also invoked if the earlier ones
fail to produce a confident outcome.

4.3 Impact of Different Models

In our primary results, we used the Qwen3-32B
model. To evaluate the impact of model architec-
ture and size, we further ran our method on two
families of models: Qwen3 (Yang et al., 2025)
and GPT-OSS (OpenAI et al., 2025), across vary-
ing model sizes. The comparative results are pre-
sented in Fig. 4.

The findings show a clear positive correlation
between model size and micro-Fl score within
the Qwen3 family, indicating that larger mod-
els yield better performance. In contrast, the
GPT-OSS models display relatively smaller perfor-
mance variation across sizes and consistently lag
behind the best-performing Qwen models.

5 Related Works

Bangla Hate Speech Detection. Karim et al.
(2021) achieved Fl-scores of 78-91% using trans-
former ensembles, while Jahan et al. (2022) de-
veloped domain-specific BanglaHateBERT with
1.5M Bengali posts. Raihan et al. (2023) ad-
dressed code-mixed content through cross-lingual
transformers for transliterated text.

Model Performance Comparison

60 = 586

ad 477
, |
$30

20

10

°

¢ ¢ iS 2
5 é Rs
of & & of

Figure 4: Performance comparison across model archi-
tectures and sizes. Qwen3 models show clear scaling
benefits with size, while GPT-OSS models exhibit con-
sistent but lower performance across different scales.

FI se

Mi

é
&

oF Pod
& & &

LLM in Bengali. Hasan et al. (2023);
Roy Dipta and Vallurupalli (2024) showed that
monolingual transformers outperform general-
purpose LLMs (e.g., Flan-T5, GPT-4) on zero/few-
shot Bangla sentiment tasks, while Wang et al.
(2025) improved LLM performance through multi-
lingual prompting enriched with cultural cues. To
address reasoning limitations in LLMs, Colelough
and Regli (2025) identified major gaps in explain-
ability and meta-cognition across Neuro-Symbolic
AI literature. In parallel, Kowsher et al. (2022)
introduced Bangla-BERT with language-specific
pretraining, outperforming prior models by up to
5.3%. Gao et al. (2025) tackled prompt design
challenges with MAPS, an automated framework
achieving higher coverage via diversity-guided
search.

Social Media Challenges. Guo et al. (2024)
categorized LLM biases and proposed mitigation
strategies, while Natsir et al. (2023) examined dy-
namic language evolution in social media, identi-
fying shifts in multilingual adaptation.

6 Conclusion

We present PromptGuard, a few-shot classifica-
tion framework that addresses Bengali hate speech
detection through statistical feature selection and
adaptive decision-making. Our approach com-
bines chi-square-based keyword extraction with
majority voting to achieve robust classification.
Our work demonstrates the value of integrating sta-
tistical foundations with few-shot learning for low-
resource language tasks. Future directions include
exploring advanced feature selection methods to
improve few-shot hate speech detection.


References

Abdullah Al Maruf, Ahmad Jainul Abidin, Md Mah-
mudul Haque, Zakaria Masud Jiyad, Aditi Golder,
Raaid Alubady, and Zeyar Aung. 2024. Hate speech
detection in the bengali language: a comprehensive
survey. Journal of Big Data.

Nadhia Azzahra, Danang Murdiansyah, and Kemas
Lhaksmana. 2021. Toxic comment classification
on social media using support vector machine and
chi square feature selection. International Jour-
nal on Information and Communication Technology
(IJoICT).

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, and 1 others. 2020. Language models are
few-shot learners. Advances in neural information
Processing systems.

Brandon C Colelough and William Regli. 2025. Neuro-
symbolic ai in 2024: A systematic review. arXiv
preprint arXiv:2501.05435.

Mithun Das and Animesh Mukherjee.
Banglaabusememe: A dataset for
abusive meme classification.
arXiv:2310.11748.

2023.
bengali
arXiv preprint

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proceedings of the 2019 conference of the
North American chapter of the association for com-
putational linguistics: human language technolo-
gies, volume I (long and short papers).

Thomas G Dietterich. 2000. Ensemble methods in ma-
chine learning. In International workshop on multi-
ple classifier systems. Springer.

Fatema Tuj Johora Faria, Laith H Baniata, and Sang-
woo Kang. 2024. Investigating the predominance of
large language models in low-resource bangla lan-
guage over transformer models for hate speech de-
tection: A comparative analysis. Mathematics.

George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of machine learning research.

Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xi-
aoqian Jiao, Chun Yong Chong, Shan Gao, and
Michael Lyu. 2025. The prompt alchemist: Auto-
mated Ilm-tailored prompt optimization for test case
generation. arXiv preprint arXiv:2501.01329.

Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang,
Mengqiu Zhu, Hongfei Li, Mengyang Qiu, and
Shuo Shuo Liu. 2024. Bias in large language
models: Origin, evaluation, and mitigation. arXiv
preprint arXiv:2411,10915.

Md Arid Hasan, Firoj Alam, Md Fahad Hossain, Us-
man Naseem, and Syed Ishtiaque Ahmed. 2025a.
Llm-based multi-task bangla hate speech detec-
tion: Type, severity, and target. arXiv preprint
arXiv:2510.01995.

Md Arid Hasan, Firoj Alam, Md Fahad Hossain, Us-
man Naseem, and Syed Ishtiaque Ahmed. 2025b.
Overview of blp 2025 task 1: Bangla hate speech
identification. In Proceedings of the Second Inter-
national Workshop on Bangla Language Processing
(BLP-2025), India. Association for Computational
Linguistics.

Md Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj
Alam, Anika Anjum, Avijit Sarker, and Sheak
Rashed Haider Noori. 2023. Zero-and few-shot
prompting with llms: A comparative study with fine-
tuned models for bangla sentiment analysis. arXiv
preprint arXiv:2308.10783.

Md Saroar Jahan, Mainul Haque, Nabil Arhab, and
Mourad Oussalah. 2022. Banglahatebert: Bert for
abusive language detection in bengali. In Proceed-
ings of the second international workshop on re-
sources and techniques for user information in abu-
sive language analysis, pages 8-15.

Md Rezaul Karim, Sumon Kanti Dey, Tanhim Islam,
Sagor Sarker, Mehadi Hasan Menon, Kabir Hossain,
Md Azam Hossain, and Stefan Decker. 2021. Deep-
hateexplainer: Explainable hate speech detection in
under-resourced bengali language. In 202] IEEE
Sth international conference on data science and ad-
vanced analytics (DSAA), pages 1-10. IEEE.

Md Kowsher, Abdullah As Sami, Nusrat Jahan Prot-
tasha, Mohammad Shamsul Arefin, Pranab Kumar
Dhar, and Takeshi Koshiba. 2022. Bangla-bert:
transformer-based efficient model for transfer learn-
ing and language understanding. JEEE Access,
10:91855-91870.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023. Lost in the Middle: How Lan-
guage Models Use Long Contexts. arXiv preprint.
ArXiv:2307.03172 [cs].

Alexandre Magueresse, Vincent Carles, and Evan Heet-
derks. 2020. Low-resource languages: A review
of past work and future challenges. arXiv preprint
arXiv:2006.07264.

Nurasia Natsir, Nuraziza Aliah, Zulkhaeriyah
Zulkhaeriyah, Amiruddin Amiruddin, and Farida
Esmianti. 2023. The impact of language changes
caused by technology and social media. Language
Literacy: Journal of Linguistics, Literature, and
Language Teaching.

OpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason
Ai, Sam Altman, Andy Applebaum, Edwin Ar-
bus, Rahul K. Arora, Yu Bai, Bowen Baker, Haim-
ing Bao, Boaz Barak, Ally Bennett, Tyler Bertao,
Nivedita Brett, Eugene Brevdo, Greg Brockman,


Sebastien Bubeck, and 108 others. 2025.  gpt-
oss-120b & gpt-oss-20b model card. Preprint,
arXiv:2508.10925.

Branislav Pecher, Ivan Srba, Maria Bielikova, and
Joaquin Vanschoren. 2024. Automatic combination
of sample selection strategies for few-shot learning.
arXiv preprint arXiv:2402.03038.

Md Nishat Raihan, Umma Hani Tanmoy, Anika Binte
Islam, Kai North, Tharindu Ranasinghe, Antonios
Anastasopoulos, and Marcos Zampieri. 2023. Of-
fensive language identification in transliterated and
code-mixed bangla.

Nauros Romim, Mosahed Ahmed, Hriteshwar
Talukder, and Md Saiful Islam. 2021. Hate speech
detection in the bengali language: A dataset and its
baseline evaluation. In Proceedings of International
Joint Conference on Advances in Computational
Intelligence: ITCACT 2020. Springer.

Shubhashis Roy Dipta and Sai Vallurupalli. 2024. UM-
BCLU at SemEval-2024 task 1: Semantic textual re-
latedness with and without machine translation. In
Proceedings of the 18th International Workshop on
Semantic Evaluation (SemEval-2024), pages 1351-
1357, Mexico City, Mexico. Association for Compu-
tational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Ilia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems.

Qihan Wang, Shidong Pan, Tal Linzen, and Emily
Black. 2025. Multilingual prompting for im-
proving llm generation diversity. arXiv preprint
arXiv:2505. 15229.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. 2022. Self-consistency im-
proves chain of thought reasoning in language mod-
els. arXiv preprint arXiv:2203.11171.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Lv, Chujie Zheng, Day-
iheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao
Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41
others. 2025. Qwen3 technical report. Preprint,
arXiv:2505.09388.


A Prompts
A.1 Prompt with Keywords

Following is the prompt that we have used with

our final method.

You are an AI language model specialized in detecting
hate speech in Bengali. Your task is to classify a given
Bengali sentence into one of six categories: none, sexism,
abusive, profane, religious hate, or political hate.

First, review these examples of sentences for each
category:

<examples>

{ {EXAMPLES } }

<examples>

Now, consider these common words _ associated
with each category. Note that the presence of these words
doesn’t guarantee classification into that category, but
they can be helpful indicators:

<category_keywords>

abusive: Weta, fof, Sieg, cold, FAS, Meet, GSI, ASE,
artery

profane: <I, aif, arafes, Cal, Wa, Sool, Wer, “WT,
WMMICOM, Pel, GAC, crle, Vealla

religious hate: 3piery, fy, Ram, SPT, WI, ast,
political hate: cols, faaafa, snes, eet, Wate, fretto-,
Qiral, Alay, TAA, Ao, Wet, Cola, Aral

sexism: “eal, ales, haem, ARPA, faeral, face, fer,
CRICDAT, CCH, CTI, Satta

<category_keywords>

Here’s the Bengali sentence you need to classify:
<input_sentence>

{ {INPUT_SENTENCE} }

<input_sentence>

Before making your final classification, analyze the
sentence in detail. Consider the following:
1. Compare the sentence to the provided examples.
2. Examine the tone, specific words used, and overall
context.
3. Check if any words from the category_keywords are
present and relevant.
4. For each category (none, sexism, abusive, profane,
religious hate, political hate):

- List evidence for classifying the sentence into this
category.

- List evidence against classifying the sentence into
this category.
5. Summarize your findings and explain your final
decision.

Your final output should be the category classifica-
tion. Use only one of these exact category names: none,
sexism, abusive, profane, religious hate, or political hate.

Provide your classification inside <classification>
tags.

A.2 Basic Prompt

Following is the prompt that we used during the

ablation study of similar keywords.

You are an AI language model specialized in detecting
hate speech in Bengali. Your task is to classify a given
Bengali sentence into one of six categories: none, sexism,
abusive, profane, religious hate, or political hate.

First, review these examples of sentences for each
category:

<examples>

{ {EXAMPLES } }

<examples>

Here’s the Bengali sentence you need to classify:
<input_sentence>

{ {INPUT_SENTENCE} }

</input_sentence>

Before making your final classification, analyze the
sentence in detail. Consider the following:
1. Compare the sentence to the provided examples.
2. Examine the tone, specific words used, and overall
context.
3. For each category (none, sexism, abusive, profane,
religious hate, political hate):

- List evidence for classifying the sentence into this
category.

- List evidence against classifying the sentence into
this category.
4. Summarize your findings and explain your final
decision.
Your final output should be the category classification.
Use only one of these exact category names: none,
sexism, abusive, profane, religious hate, or political hate.

Provide your classification inside <classification>
tags.

