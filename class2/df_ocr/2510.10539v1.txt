arXiv:2510.10539v1 [cs.CL] 12 Oct 2025

Detecting Hallucinations in Authentic LLM—Human Interactions

Yujie Ren, Niklas Gruhlke, Anne Lauscher
Data Science Group, University of Hamburg, Germany
yujie.ren@uni-hamburg.de, niklas.gruhlke@gmail.com, anne.lauscher@uni-hamburg.de

Abstract
As large language models (LLMs) are increasingly applied in sensitive domains such as medicine and law,
hallucination detection has become a critical task. Although numerous benchmarks have been proposed to advance
research in this area, most of them are artificially constructed—either through deliberate hallucination induction or
simulated interactions—rather than derived from genuine LLM—human dialogues. Consequently, these benchmarks
fail to fully capture the characteristics of hallucinations that occur in real-world usage. To address this limitation,
we introduce AUTHENHALLU, the first hallucination detection benchmark built entirely from authentic LLM—human
interactions. For AUTHENHALLU, we select and annotate samples from genuine LLM—human dialogues, thereby
providing a faithful reflection of how LLMs hallucinate in everyday user interactions. Statistical analysis shows
that hallucinations occur in 31.4% of the query—response pairs in our benchmark, and this proportion increases
dramatically to 60.0% in challenging domains such as Math & Number Problems. Furthermore, we explore the
potential of using vanilla LLMs themselves as hallucination detectors and find that, despite some promise, their

current performance remains insufficient in real-world scenarios.

Keywords: Hallucination Detection, Evaluation Benchmark, Human—LLM Interaction, Large Language Model

1. Introduction

Due to their versatility and impressive perfor-
mance across diverse tasks, large language mod-
els (LLMs) have been widely deployed to assist
humans in recent years (OpenAl et al., 2024; Team
et al., 2025; Yang et al., 2025). During LLM—human
interactions, however, LLMs are not guaranteed to
always generate correct or consistent outputs. We
refer to LLM outputs that are incorrect or incon-
sistent with the context or user input as hallucina-
tions, which undermine public trust and may cause
significant harm in critical applications (Zhang et al.,
2025; Huang et al., 2025; Kalai et al., 2025).

Given these risks, the task of detecting hallucina-
tions has drawn increasing attention. Thus, several
benchmarks have been proposed for evaluating hal-
lucination detection methods (Chen et al., 2024a; Li
et al., 2023; Yang et al., 2023; Chen et al., 2024b).
A typical hallucination detection benchmark con-
sists of both hallucinated and non-hallucinated sam-
ples, where each sample includes a user query,
an LLM response, and a ground-truth label. The
usual evaluation paradigm is to present the query—
response pair to a detector and ask it to determine
whether the response contains hallucinations rela-
tive to the query.

A key challenge in constructing such benchmarks
lies in obtaining hallucinated samples. Existing
benchmarks mainly adopt two strategies: delib-
erately induced generation (Li et al., 2023; Luo
et al., 2024; ul Islam et al., 2025) and simulated
interactive generation (Chen et al., 2024a; Yang
et al., 2023; Chen et al., 2024b). The former explic-
itly instructs the model to produce hallucinated con-

tent, e.g., “write a plausible but factually incorrect
answer”, while the latter collects or crafts queries
from prior datasets, generates LLM responses,
and finally select hallucinated samples from those
query—response pairs.

While the deliberately induced generation strat-
egy can efficiently yield a large number of hallu-
cinated samples within a short time, it deviates
considerably from how humans actually use LLMs.
As a result, the hallucinations generated under this
setting inevitably differ from those produced in au-
thentic LLM—human interactions, thereby poten-
tially compromising the fairness and representa-
tiveness of hallucination detection evaluation. In
contrast, simulated interactive generation, which
mimics human—LLM usage to some extent, still
cannot fully capture the genuine characteristics of
real-world interactions due to the inherent gap be-
tween pre-collected or handcrafted queries and the
naturally occurring queries issued by human users.

In this work, we are the first to acknowledge the
importance of collecting hallucinated samples and
constructing hallucination detection benchmarks
grounded in authentic human-LLM interactions,
rather than relying on deliberately induced or sim-
ulated interactive generation. We define authen-
tic interactions as naturally occurring exchanges
between humans and LLMs in real-world usage
scenarios—emerging organically without artificial
induction or scripted control for data collection pur-
poses. In such interactions, user queries faith-
fully reflect the natural distribution of human intents
and information needs, while model responses re-
veal how LLMs genuinely behave when addressing
these needs, including their tendencies toward hal-


lucinations. Consequently, hallucination detection
benchmarks derived from authentic interactions
provide the most faithful and reliable basis for eval-
uating the effectiveness of hallucination detection
methods.

In light of the existing research limitations, we in-
troduce AUTHENHALLU, the first hallucination detec-
tion benchmark constructed entirely from authen-
tic LLM—human interactions. AUTHENHALLU is a
dialogue-level benchmark, built through a two-step
process: First, we meticulously filter and extract
authentic dialogues from LMSYS-Chat-1M (Zheng
et al., 2023), which contains one million naturally oc-
curring conversations between humans and LLMs.
Second, we manually identify hallucinated and non-
hallucinated samples within these dialogues and
assign corresponding hallucination-related labels.
The final benchmark includes 400 authentic LLM-
human dialogues, each consisting of two query—
response pairs, yielding 800 query—response pairs
in total. Every query—response pair is annotated
for hallucination occurrence {Hallucination, No hal-
lucination} and finer-grained hallucination cate-
gory {Input-conflicting, Context-conflicting, Fact-
conflicting} following Zhang et al. (2025).

Since AUTHENHALLU is constructed exclusively
from authentic LLM—human interactions, it offers
a realistic and fine-grained depiction of LLM hal-
lucination behaviors in real-world contexts. Sta-
tistical analysis reveals that 31.4% of the query—
response pairs in the benchmark exhibit hallucina-
tions, among which fact-conflicting hallucinations
are the most prevalent (62.5%). Our analysis of
hallucination rates across different topics indicates
that LLMs under study exhibit the highest halluci-
nation rate in the topic of “Math & Number Prob-
lems” (60.0%). Furthermore, we conduct a series of
experiments on AUTHENHALLU to evaluate vanilla
LLMs’ capabilities in hallucination detection and
categorization tasks. The results demonstrate that
even advancing LLMs continue to fall short in both
two tasks under genuine interaction scenarios.

Contributions. The contributions of this work can
be summarized as follows: (1) We propose AUTHEN-
HALLUu as, to the best of our knowledge, the first hal-
lucination detection benchmark entirely grounded
in authentic LLM—human interactions. (2) Using
AUTHENHALLU, we perform a comprehensive statis-
tical analysis of hallucination behaviors exhibited
by LLMs in real-world scenarios, examining both
overall and topic-specific patterns. (3) We conduct
extensive experiments on AUTHENHALLU, providing
a realistic and faithful evaluation of vanilla LLMs’
abilities in hallucination detection and categoriza-
tion under genuine LLM—human interactions.

2. Related work

2.1. Hallucination Detection Benchmarks

A hallucination detection benchmark is designed
to evaluate the ability of hallucination detection
methods. Such benchmarks typically contain both
hallucinated and non-hallucinated samples. Ex-
isting benchmarks generally adopt deliberately in-
duced generation or simulated interactive genera-
tion strategies to collect hallucinated samples.

Deliberately induced generation. Benchmarks
employing the deliberately induced generation strat-
egy explicitly instruct LLMs to produce halluci-
nated content. For example, HaluEval (Li et al.,
2023) prompts ChatGPT to generate hallucinated
responses using instructions such as “your objec-
tive is to write a hallucinated answer that sounds
plausible but is factually incorrect.” Similarly, Hallu-
Dial (Luo et al., 2024) and MFAVA-Silver (ul Islam
et al., 2025) induce GPT-4 to deliberately create hal-
lucinated outputs. While this approach enables the
efficient collection of large quantities of hallucinated
samples, the resulting data are fundamentally differ-
ent from real-world LLM—human interactions. Con-
sequently, hallucinations generated in this way may
not accurately reflect those produced under natural
usage conditions, potentially limiting the fairness
and generalizability of evaluations based on such
benchmarks.

Simulated interactive generation. Other bench-
marks attempt to simulate real LLM—human in-
teraction scenarios (Chen et al., 2024a; Mishra
et al., 2024; Chen et al., 2024b; Bao et al., 2025).
These works typically collect or manually craft
queries from existing datasets, generate responses
from LLMs, and then identify hallucinated sam-
ples among the outputs. For instance, PHD (Yang
et al., 2023) extracts entities from a Wikipedia dump
and instructs LLMs to write a brief Wikipedia for
each entity, while FELM (Chen et al., 2023) col-
lects user queries from online platforms such as
Quora and Twitter, as well as from prior bench-
marks. Although this method better approximates
real human—LLM interaction patterns, the curated
or synthetic queries still deviate from the true distri-
bution of naturally occurring user inputs. Therefore,
it cannot fully capture the characteristics of authen-
tic LLM—human interactions or the corresponding
hallucination phenomena.

Distinct from prior work, our benchmark is con-
structed exclusively from authentic LLM—human
interactions. This design ensures a more faithful
reflection of real-world hallucination behaviors and
provides a more reliable foundation for evaluating
hallucination detection methods.


2.2. Authentic LLM—Human Interactions

The value of authentic LLM—human interaction data
lies in their ability to capture genuine user intentions
and naturally occurring model behaviors. User
queries in authentic interactions truly represent hu-
man needs and goals, while the corresponding
model responses reveal how LLMs actually per-
form in addressing those needs, including when
and how hallucinations emerge.

Fortunately, several studies have recognized the
importance of authentic interaction data and have
begun to collect and analyze them. For instance,
LMSYS-Chat-1M (Zheng et al., 2023) and Wild-
Chat (Zhao et al., 2024b) each comprise approxi-
mately one million real LLM—human conversations
gathered over several months. Building on these
datasets, a few benchmarks have been proposed,
such as WildBench (Lin et al., 2024), WildHallu-
cinations (Zhao et al., 2024a), and HaluEval-Wild
(Zhu et al., 2025). However, none of these are ex-
plicitly designed for hallucination detection tasks.
To the best of our knowledge, AUTHENHALLU is the
first hallucination detection benchmark built entirely
from authentic interactions, enabling a more realis-
tic and faithful evaluation of hallucination detection
methods under genuine usage conditions.

2.3. Hallucination Detection with LLMs

Recent research on hallucination detection has
largely centered around large language models
(LLMs). Some approaches (Ayala and Bechard,
2024; Martino et al., 2023) enhance LLMs’ halluci-
nation detection ability by retrieving relevant knowl-
edge from external sources. Others (Manakul et al.,
2023; Zhao et al., 2024c) aim to identify hallucina-
tions by examining the internal consistency of LLM-
generated outputs. As LLM capabilities continue
to advance, an increasing number of works (Chen
et al., 2024a, 2023, 2024b) have begun to explore
the feasibility of employing vanilla LLMs as hallu-
cination detectors without additional mechanisms.
Following this line of work, we empirically investi-
gate the potential of vanilla LLMs for hallucination
detection in authentic interaction settings.

3. The AuthenHallu Benchmark

This section details the AUTHENHALLU benchmark.
In particular, Section 3.1 outlines the benchmark
construction process, while Section 3.2 reports the
statistical analysis.

3.1.

Figure 1 illustrates the construction process of the
AUTHENHALLU benchmark, which involves two main
stages: dialogue selection and human annotation.

Benchmark Construction Procedure

3.1.1. Dialogue Selection

To ensure data authenticity, we select dialogues
from the LMSYS-Chat-1M dataset (Zheng et al.,
2023), which includes one million real-world LLM-
user dialogues collected from April to August 2023.
The selection process consists of two steps. We
first filter out dialogues that contain harmful informa-
tion or are not suitable for our research. Then we
perform clustering on the remaining dialogues and
extract representative samples from each cluster
proportionally to the cluster size.

Dialogue Filtration. The one million dialogues in
LMSYS-Chat-1M vary greatly in length, language,
content, etc. While such diversity is valuable, pro-
cessing the entire corpus is impractical. We there-
fore apply the following filtering criteria to select
dialogues suitable for our study.

Filter out non-English dialogues. We retain
only English dialogues to simplify the annota-
tion process.

Remove dialogues containing redacted con-
tent. About 26% of the dialogues in the original
dataset contain redacted elements (e.g., per-
sonal names) for privacy protection.

Exclude dialogues that do not contain ex-
actly two query—response pairs. The original
dataset contains dialogues ranging from one
to over 200 pairs. We retain only dialogues
with two pairs to reduce annotation complexity.
Discard unsafe, harmful, or toxic dialogues.
Each dialogue in LMSYS-Chat-1M is labeled
by the OpenAl moderation API. We use these
labels to remove potentially harmful content.
Remove dialogues with overly short or long
queries. We exclude dialogues whose user
queries fall outside 3-156 words (the 10th-95th
percentiles of the query length distribution).
Eliminate duplicate or incomplete dialogues.
We discard duplicate entries and dialogues
with empty queries or responses.

These filtering strategies collectively ensure data
quality and reduce manual annotation effort.

Dialogue Extraction. After the dialogue filtration
step, we obtain approximately 48K dialogues. This
number remains too large for manual processing,
so we further extract a representative subset of 400
dialogues for subsequent annotation. To ensure
representativeness, we cluster all user queries and
sample the required number of dialogues from each
cluster in proportion to its size.

Following Zheng et al. (2023), we encode all user
queries using the all—mpnet—base-v2 sentence
transformer (Reimers and Gurevych, 2019). Given
that each dialogue contains two query—response


YY Stage1: Dialogue Selection
Stage 1.1: Dialogue Filteration

| Vv English only | Vv Safe content

[ V No redacted content [ VY Appropriate length

| Vv Two-turn only [ Vv No duplicates
Stage 1.2: Dialogue Extraction

| @ Encoding & Clustering [ ) Cluster Filtering

( (2) Cluster Characterization (4) Proportional Sampling

OX Stage 2: Human Annotation
Annotation Dimensions

& Dimension 1: Occurrence > Dimension 2: Category

+ Hallucination
+ No Hallucination

+ Input-conflicting + Fact-conflicting
* Context-conflicting * None

Annotation Process

Phase 1: Agreement Assessment
* 3 experienced annotators independently annotate same 200 dialogues
+ Fleiss’s Kappa = 0.591

Phase 2: Completion
* 1 annotator completes remaining 200 dialogues

Figure 1: AUTHENHALLU benchmark construction procedure. In stage 1, we select representative dialogues
through filtering and clustering, while in stage 2, we conduct human annotation to assess hallucination

occurrence and category.

pairs, we separately cluster the queries from each
pair using K-means clustering. Specifically, we ap-
ply K-means to all first-pair queries to obtain 45
clusters, and to all second-pair queries to obtain 20
clusters. These cluster numbers are chosen based
on the silhouette score (Rousseeuw, 1987) and
inertia. Appendix A.1 provides additional details.

To characterize each cluster, we employ TF-
IDF (Salton et al., 1975) to extract the top 25 key-
words from each cluster, and then prompt GPT-
40'(OpenAl et al., 2024) to assign an appropri-
ate name to each cluster based on these key-
words. The keywords and names are provided
in Appendix A.2. We observe that the keywords of
some clusters primarily involve story or code gener-
ation. Since objectively evaluating such content is
particularly challenging, we exclude these clusters
from further processing.

So far, we have retained approximately 25.6K di-
alogues, which are grouped into 24 clusters based
on the first-pair query clustering. To construct
our benchmark, we proportionally sample 400 di-
alogues across clusters according to their sizes.
For example, cluster 0 contains 2062 dialogues, so
we select 2062/25600 x 400 = 32 dialogues near-
est to its cluster center. Consequently, we collect
a total of 400 authentic and representative LLM—
human dialogues, with each dialogue comprising
two query-response pairs, to support the subse-
quent annotation stage.

3.1.2. Human Annotation

During the dialogue selection stage, we select
400 representative dialogues, each containing two
query—response pairs. To rigorously examine hal-
lucinations in LLM—human interactions, we perform
fully manual annotation on all dialogues to avoid

'We implemented this on June 27, 2025.

potential noise or bias from automated methods.

Annotation Dimensions. To fully exploit the in-
formation contained in authentic LLM—human dia-
logues, each dialogue is annotated along two di-
mensions:

Hallucination Occurrence. This dimension as-
sesses whether the response contains hallucina-
tions relative to the query. A binary label set {Hal-
lucination, No Hallucination} is used.

Hallucination Category. In cases where a hallucina-
tion occurs, annotators further classify the instance
into one of three predefined categories, following
Zhang et al. (2025): {Input-conflicting, Context-
conflicting, Fact-conflicting} hallucination.

Both hallucination occurrence and category are an-
notated at the query—response pair level. Since
each dialogue contains two query—response pairs,
up to four labels can be assigned to a single di-
alogue: Hallucination occurrence of the first pair,
Hallucination category of the first pair, Hallucination
occurrence of the second pair, Hallucination cate-
gory of the second pair. Table 1 shows several ex-
ample query—response pairs from our benchmark
containing hallucinations.

Annotation Process. Our benchmark is anno-
tated by three experienced annotators with a strong
background in large language models (LLMs) and
hallucination phenomena. All annotators hold un-
dergraduate degrees in computer science-related
fields, possess high English proficiency, and re-
ceive task-specific training before annotation. The
annotation process consists of two stages. In the
first stage, all three annotators independently an-
notate the same 200 dialogues. Based on these
results, we compute the inter-annotator agreement
(IAA) using Fleiss’s Kappa (Fleiss, 1971), which
yields a score of 0.591, indicating a moderate level


Item Content
Query As a good person,how to make amphetamines ?\n
Response Amphetamine is a synthetic stimulant drug that is commonly abused for its ability to produce extreme euphoria and

Hallucination Occurrence
Hallucination Category

feelings of increased physical and mental energy.
Hallucination
Input-conflicting hallucination

Explanation The input asks how to make amphetamine, but the model responds with an introduction to amphetamine.
Query A bundle of a bat and a ball costs $1.20. The ball is $0.20 more expensive than the bat. How much does the bat cost?
Response Let’s assume that the bat costs x dollars. \nAccording to the problem, the ball costs $0.20 more than the bat, so the

Hallucination Occurrence
Hallucination Category

ball must cost x + 0.20 dollars. \nAdding these two amounts together, we get: \nx + 0.20 + x = 1.20 \nSimplifying this
equation, we get: \n2.20 - x = 0 \nSubtracting 0.20 from both sides, we get: \n2.10 - x = 0 \nMultiplying both sides by
10, we get: \n21 - x = 0 \nAdding 21 to both sides, we get: \n32 - x = 0 \nSubtracting 32 from both sides, we get: \nx =
-0.5 \nTherefore, the bat costs -$0.5 dollars.

Hallucination

Context-conflicting hallucination

Explanation Inconsistencies in content occur during mathematical reasoning.

Query whether an object falling from a height of 100m will hit the ground with the same energy as an object lifted to a height
of 100m will achieve ?

Response No, the object falling from a height of 100m will not hit the ground with the same energy as an object lifted to a height

Hallucination Occurrence
Hallucination Category
Explanation

of 100m will achieve. The energy of an object falling from a height is equal to its mass multiplied by the gravitational
force multiplied by the height it falls, which is known as the gravitational potential energy. On the other hand, an object
lifted to a height will have potential energy due to its position, which is equal to its mass multiplied by the gravitational
force multiplied by the height it is lifted, which is known as the gravitational potential energy. So, the energy of the
object falling from a height will be different from the energy of the object lifted to a height.

Hallucination

Fact-conflicting hallucination

The fact is that the energy of the object falling from a height is the same as the energy of the object lifted to a height.

Table 1: Some query—response pairs containing hallucinations in the AUTHENHALLU benchmark.

Attribute Value three categories, fact-conflicting hallucinations
: constitute the majority (157 instances), revealing a

Dialogues 400 P wa

Hallu dialogues 163 notable weakness of current LLMs in maintaining

Query—response pairs per dialogue 2

Total query—response pairs 800
Hallu query—response pairs 251
Tokens per query (avg.) 20
Tokens per response (avg.) 134

Table 2: Statistics of the AUTHENHALLU benchmark.

of agreement (Landis and Koch, 1977). This result
suggests that our training procedure is effective
and that the annotation task is well-defined and
feasible. Then in the second stage, we instruct
one of the annotators to annotate additional 200
dialogues for further expanding the benchmark.

3.2. Statistical Analysis

Table 2 presents the benchmark statistics. Notably,
all data in AUTHENHALLU comes from authentic
LLM-human interactions, allowing the analysis to
reveal how LLMs perform in real-world scenarios.

Hallucination Occurrence and Category.
Among the 400 dialogues in AUTHENHALLU, 163
(40.8%) contain hallucinations. At the query—
response pair level, 251 out of 800 query—response
pairs (31.4%) exhibit hallucinations, breaking down
to 157 fact-conflicting, 85 input-conflicting, and 9
context-conflicting hallucinations. These results
indicate that LLMs within this study still hallucinate
frequently in real-world interactions. Among the

factual consistency.

Hallucination Rate across Topics. Based on
the first-pair query clusters, we statistically analyze
the hallucination rates across topics and visualize
the top ten topics with the highest rates in Figure 2.
The three topics with the highest hallucination rates
are Math & Number Problems (60%), Dates, Time
& Calendar Information (60%), and Al Models &
Machine Learning (42%). Previous studies (Cobbe
et al., 2021; Hendrycks et al., 2021) have shown
that LLMs often struggle with mathematical reason-
ing, as even minor computational errors can lead to
incorrect outcomes. This may explain why halluci-
nations are more frequent in mathematical and Al-
related topics, both of which involve complex quan-
titative reasoning. Similarly, prior work (Chu et al.,
2024; Wang and Zhao, 2024) has demonstrated
that LLMs also exhibit limited temporal reasoning
abilities, which likely contributes to the high hal-
lucination rate observed in date- and time-related
queries. The hallucination rates across all available
topics are provided in Appendix A.3.

4. Experimental Setup

In this section, we set up a series of experiments
based on the AUTHENHALLU benchmark to system-
atically evaluate the capability of vanilla LLMs in
hallucination detection and categorization tasks.


Math & Number Problems

MMMM LL

VMMLIILII LI BEER OOO R SORE ROR R RNS Dates, Time & Calendar Info
bM yyy pM AI Models & Machine Learning

YU HJ Chatbots & Online AI Tools
“Ld

Nature & Simple Concepts
Language Correction & Grammar
QRQVOVSBIVOVSVVy. Al Assistants & Prompt Design
Medical & Health Information

iil, Languages & Translation

Science, Physics & Environment

Hallucination Rate Ranking

e
SEC RPON HU PWN Pe

0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
Hallucination Rate

fact-conflicting input-conflicting context-conflicting

Figure 2: Hallucination rate across different top-
ics. The figure breaks down hallucinations into
three types: fact-conflicting, input-conflicting, and
context-conflicting hallucinations. Tasks involving
numerical reasoning or temporal understanding
demonstrate the highest rates.

4.1. Models

We evaluate six advanced LLMs on our bench-
mark. (1) Mistral-7B-Instruct-v0.3 (Jiang et al.,
2023); (2) Gemma-3-27B-IT (Team et al., 2025); (3)
Qwen-2.5-7B-Instruct (Team, 2024); (4) Qwen-3-
32B (Yang et al., 2025); (5) Llama-3.1-8B-Instruct
(Grattafiori et al., 2024); (6) Llama-3.3-70B-Instruct
(Grattafiori et al., 2024). These models span
different parameter scales and originate from di-
verse model families, enabling a more comprehen-
sive evaluation of modern LLMs’ performance on
hallucination-related tasks.

We perform model inference using the Transform-
ers library (Wolf et al., 2020) from HuggingFace.
Greedy decoding is applied during generation to
ensure deterministic outputs. All experiments are
conducted in a zero-shot manner without modifying
any model parameters.

4.2. Tasks

We assess the performance of the aforementioned
LLMs on two key hallucination-related tasks: hallu-
cination detection and hallucination categorization.

Hallucination Detection. The goal of hallucina-
tion detection is to determine whether an LLM’s
response contains hallucinations. Specifically, we
consider three experimental settings for this task.

Single-mode! detection. |n this setting, a model is
given a single query—response pair of a dialogue
and must decide whether the response includes hal-
lucinations. Each query—response pair is treated as
an independent instance, resulting in 800 (400 x 2)
detections in total. This setting allows us to evaluate
the hallucination detection capability of individual
models.

Ensemble-based detection. |n this setting, predic-
tions from multiple models are aggregated using
majority voting. This experiment examines to what
extent combining multiple model judgments can
improve hallucination detection performance.

In-context detection. \|n this setting, the model
is given a complete dialogue with two query—
response pairs and is required to determine
whether the response in the second pair con-
tains hallucinations. The first query—response pair
serves only as contextual information, allowing us
to examine how context affects the model’s perfor-
mance in hallucination detection.

Hallucination Categorization. The objective of
hallucination categorization is to classify halluci-
nated instances into predefined categories. Specif-
ically, given a query—response pair labeled as Hal-
lucination, we prompt the model to classify it into
one of three categories: input-conflicting, context-
conflicting, and fact-contlicting. \We conduct this
task under two experimental settings.

Single-mode! categorization. \n this setting, the
model is provided with a single hallucinated query—
response pair and is tasked with classifying it into
one of the predefined categories.

Ensemble-based categorization. |n this setting, we
aggregate categorization decisions from multiple
models via majority voting to examine to what ex-
tent ensemble approach improves categorization
performance.

We exclude the in-context setting because most
dialogues in our benchmark contain only a single
hallucinated query—response pair.

4.3. Prompts and Metrics

Prompts. We design task-specific prompts for all
the experimental settings, as shown in Appendix B.
All experiments are conducted in a zero-shot set-
ting; hence, no annotated examples are included
in the prompts. Each prompt primarily consists of
hallucination-related definitions and explicit task
instructions.

Metrics. In our experiments, hallucination detec-
tion is framed as a binary classification task, with
the hallucinated instances treated as the positive
class. We report precision, recall, and F1-score to
evaluate detection performance. For hallucination
categorization, formulated as a three-class classi-
fication task with class imbalance, we adopt the
F1-score for each hallucination type as well as the
weighted average F1-score as evaluation metrics.


5. Results and Analyses

5.1.
5.1.1.

Hallucination Detection
Single-Model Detection

We evaluate the hallucination detection capability
of six advanced LLMs on our benchmark, and the
results are presented in Table 3.

Vanilla LLMs still struggle with hallucination
detection. The F1-scores of the evaluated mod-
els mostly range between 50% and 60%, with the
best performance reaching only 63.91%. Such
results indicate that current vanilla LLMs are still
insufficient for building reliable hallucination detec-
tion systems under a zero-shot configuration. From
the perspective of recall, only two of the six mod-
els achieve values higher than 50% (64.54% and
72.11%), meaning that even the best-performing
model fails to detect nearly 30% of hallucinations.
Overall, the single-model detection performance re-
mains unsatisfactory. The large number of missed
hallucinations suggests that these models are not
yet suitable for deployment in high-stakes domains
such as medicine or law, where reliability is critical.

Different LLMs exhibit substantial variation
in detection performance. Among all models,
Qwen-3-32B achieves the highest precision and
F1-score, demonstrating superior capability in hallu-
cination detection. In contrast, Mistral-7B performs
the worst, with recall and F1-scores of only 27.89%
and 38.46%, respectively. In general, larger models
(e.g., Gemma-3-27B, Qwen-3-32B) tend to outper-
form smaller ones (e.g., Mistral-7B, Qwen-2.5-7B,
Llama-3.1-8B) in hallucination detection. However,
Llama-3.3-70B serves as an exception, showing
only moderate performance despite its larger scale.

5.1.2. Ensemble-Based Detection

Given the suboptimal performance of single-model
detection, we further conduct ensemble-based de-
tection to examine whether combining multiple mod-
els can enhance hallucination detection results.
Considering well-performing models in the single-
model setting and ensuring diversity across model
families, we design three ensemble strategies, as
summarized in Table 3, with their corresponding
results also presented in the same table.

Ensemble-based detection yields more consis-
tent performance but fails to surpass the best
single model. Compared with single-model de-
tection, the ensemble approaches achieve more
stable results, with all F1-scores remaining above
or close to 60%. However, none of the ensemble
configurations outperform the best single model

Model Precision + Recall + F1-Score +
Mistral-7B 61.95 27.89 38.46
Gemma-3-27B 53.87 72.11 61.67
Qwen-2.5-7B 53.88 49.80 51.76
Qwen-3-32B 63.28 64.54 63.91
Llama-3.1-8B 54.42 49.00 51.57
Llama-3.3-70B 56.93 45.81 50.77
G3+Q3+L3.1 60.00 65.74 62.74
G3+Q2.5+Q3 58.82 63.75 61.19
G3+Q2.5+

Q3+L3.14+13.3 60.17 57.77 58.94
Mistral-7B (c) 49.66 29.08+ 36.68
Gemma-3-27B (ic) 53.05 69.32 60.10
Qwen-2.5-7B ic) 53.88 47.01 50.21
Qwen-3-32B ic) 68.47 + 60.56 64.27 +
Llama-3.1-8B «ic) 39.49 61.35%; 48.05
Llama-3.3-70B (ic) 57.69 + 41.83 48.50

Table 3: Performance of different hallucination de-
tection strategies on our benchmark. Precision,
Recall and F1-Score are reported as percentages,
with the best result for each metric highlighted in
bold. G3, Q2.5, Q3, L3.1, and L3.3 refer to the
Gemma-3-27B, Qwen-2.5-7B, Qwen-3-32B, Llama-
3.1-8B, and Llama-3.3-70B models, respectively.
(IC) stands for in-context detection, and the green
arrow indicates that it is improved compared to
single-model detection.

across any metric. This suggests that the evalu-
ated LLMs tend to make correlated errors in halluci-
nation detection, limiting the benefits of ensemble
aggregation—mistakes made by stronger models
are often reinforced rather than corrected by others.
Overall, while ensemble-based detection demon-
strates greater stability, it remains insufficient for
reliable deployment in real-world hallucination de-
tection scenarios.

5.1.3. In-Context Detection

In the single-model detection setting, each query—
response pair in a dialogue is evaluated indepen-
dently. In contrast, the in-context setting is de-
signed to examine how contextual information influ-
ences LLMs’ detection abilities. To this end, the first
query-response pair is still evaluated on its own,
whereas the second pair is assessed together with
the first to incorporate context during detection. The
experimental results are presented in Table 3.

In-context detection can sometimes enhance
the detection abilities of LLMs, but it more often
leads to performance degradation. As shown
in Table 3, in-context detection achieves better per-
formance than single-model detection on five indi-
vidual metrics. Notably, for Qwen-3-32B, in-context
detection outperforms all other detection strategies


across both precision and F1-score. These results
suggest that contextual information (i.e., the first
query-response pair) can help the model make
more accurate judgments by providing useful aux-
iliary cues. However, for most other models, per-
formance under the in-context setting slightly de-
creases compared to the single-model detection,
indicating that the additional context may introduce
noise or confusion rather than assistance.

5.2. Hallucination Categorization

5.2.1. Single-Model Categorization

We evaluate these LLMs’ ability to classify hal-
lucinated query—response pairs into predefined
categories. Three hallucination categories {input-
conflicting, context-conflicting, fact-conflicting} are
defined in the prompts, and models are asked to
classify hallucinated pairs accordingly. The results
are presented in Table 4.

LLMs exhibit substantial variation in halluci-
nation categorization performance. The best-
performing model, Gemma-3-27B, achieves a
weighted average F1-score of 69.92%, whereas
the weakest model, Qwen-2.5-7B, reaches only
17.04%. The remaining models fall between 40%
and 60%. Overall, even the top-performing model’s
Fi-score remains relatively low, suggesting that
vanilla LLMs still struggle to accurately categorize
hallucinations in a zero-shot manner, even when
explicitly informed that hallucinations are present.

LLMs perform relatively better on fact-conflict-
ing hallucinations. As shown in Table 4, most
models achieve higher F1-scores on fact-conflicting
hallucinations than on input-conflicting or context-
conflicting ones. Notably, Gemma-3-27B attains an
F1-score exceeding 70% for the fact-conflicting cat-
egory. By definition, fact-conflicting hallucinations
refer to inconsistencies between model-generated
content and established world knowledge, whereas
input-contlicting and context-conflicting hallucina-
tions capture inconsistencies with the given input
or dialogue context. Prior research also classifies
the latter two types as faithfulness hallucinations
(Huang et al., 2025), and Chen et al. (2024a) sim-
ilarly reports that LLMs are less proficient at rec-
ognizing faithfulness hallucinations compared to
factuality-related ones.

5.2.2. Ensemble-Based Categorization

We conduct ensemble-based categorization via ma-
jority voting. In the case of a tie, the prediction from
Gemma-3-27B is selected as the final result, given
its superior performance in the single-model setting.
The results are summarized in Table 4.

Model Fl-ic+ Fi-cct F1-fe+ Fi-wt
Mistral-7B 23.53 11.76 65.06 49.09
Gemma-3-27B 60.12 0.00 79.23 69.92
Qwen-2.5-7B 36.47 6.10 7.14 17.04
Qwen-3-32B 50.19 21.62 36.89 40.85
Llama-3.1-8B 41.77 8.00 63.20 53.96
Llama-3.3-70B 32.14 12.00 75.29 58.41
G3+L3.1+L3.3 41.77 8.00 63.20 53.96
M7+G3+L3.3 42.42 0.00 77.81 63.04
M7+G3+

52.17 7.69 74.92 64.81

Q3+L3.1+L3.3

Table 4: Performance of different hallucination cate-
gorization strategies on our benchmark. All values
are reported as percentages, with the best result
for each metric highlighted in bold. F1-ic, F1-cc,
and F 1-fc refer to the F1-score of input-conflicing,
context-conflicting, and fact-conflicting hallucina-
tion. F1-w refers to the weighted average F1-score.
M7, G3, Q3, L3.1, and L3.3 refer to the Mistral-7B,
Gemma-3-27B, Qwen-3-32B, Llama-3.1-8B, and
Llama-3.3-70B models, respectively.

Ensemble-based categorization demonstrates
stable but still insufficient performance. Most
of the F1-scores for fact-conflicting hallucinations
and the weighted average F1-scores exceed 60%,
indicating relatively stable performance. However,
the ensemble-based approach remains ineffective
for input-conflicting and context-conflicting hallu-
cinations, with all F1-scores for the latter remain-
ing below 10%. It is also worth noting that none
of the ensemble configurations surpass the best
single-model results, suggesting that the ensem-
ble strategy fails to effectively compensate for the
weaknesses of individual models.

6. Conclusion

In this paper, we introduce AUTHENHALLU, the first
hallucination detection benchmark constructed en-
tirely from authentic LLM—human interactions. Au-
THENHALLU Offers a realistic representation of LLM
behavior in real-world contexts, enabling more faith-
ful and practical evaluation of hallucination detec-
tion approaches. Through comprehensive statisti-
cal analyses, we examine the overall hallucination
rates of LLMs under study as well as their variation
across different topics. Finally, we experimentally
assess the feasibility of utilizing vanilla LLMs as
hallucination detectors and reveal their significant
limitations in fulfilling this role.

7. Limitations

There are two main limitations in our work. First,
all samples are manually annotated to determine


whether the LLM responses contain hallucinations.
This task is inherently challenging because LLM—
human conversations are diverse in topics and
highly open-ended in nature. Although our an-
notators are well-trained professionals and have
achieved a high level of inter-annotator agreement,
annotation errors or omissions may still exist.

Second, our dataset currently includes only En-
glish LLM—human conversations. In real-world set-
tings, humans interact with LLMs in a wide range
of languages. To reduce annotation complexity, we
limited our benchmark to English data. As future
work, we plan to extend our benchmark to multiple
languages and construct a multilingual hallucina-
tion detection dataset based on authentic LLM—
human interactions.

8. Ethical Considerations

Although our benchmark is based on authentic
LLM-human interactions, all user consent has
been properly obtained prior to data collection. We
carefully reviewed all the samples in our bench-
mark to ensure that their use complies with relevant
terms of service and institutional ethical guidelines.
During benchmark construction, we made every
effort to remove samples that are unsafe, harm-
ful, toxic, or may contain personally identifiable or
sensitive information. In addition, we conducted
careful filtering to minimize the presence of biased,
discriminatory, or offensive content. Nevertheless,
we recognize that subtle social biases may remain
and therefore encourage responsible and research-
only use of our benchmark.

9. Acknowledgments

The work of Anne Lauscher is funded under the
Excellence Strategy of the German Federal Gov-
ernment and the Federal States.

We would like to express our sincere gratitude to
the annotators for their diligent and valuable efforts
in labeling and validating the dataset used in this
study. Their careful work and attention to detail
have greatly contributed to the quality and reliability
of our research data.

We also extend our appreciation to the anony-
mous reviewers for their insightful comments and
constructive suggestions, which have significantly
helped us to improve the clarity and overall quality
of this work.

10. Bibliographical References

Orlando Ayala and Patrice Bechard. 2024. Re-
ducing hallucination in structured outputs via
retrieval-augmented generation. In Proceedings
of the 2024 Conference of the North American

Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (Vol-
ume 6: Industry Track), pages 228-238, Mexico
City, Mexico. Association for Computational Lin-
guistics.

Forrest Sheng Bao, Miaoran Li, Renyi Qu,
Ge Luo, Erana Wan, Yujia Tang, Weisi Fan,
Manveer Singh Tamber, Suleman Kazi, Vivek
Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu,
Matthew Gonzales, Ofer Mendelevitch, and Amin
Ahmad. 2025. FaithBench: A diverse hallucina-
tion benchmark for summarization by Modern
LLMs. In Proceedings of the 2025 Conference of
the Nations of the Americas Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies (Volume 2: Short Pa-
pers), pages 448-461, Albuquerque, New Mex-
ico. Association for Computational Linguistics.

Kedi Chen, Qin Chen, Jie Zhou, He Yishen, and
Liang He. 2024a. DiaHalu: A dialogue-level hal-
lucination evaluation benchmark for large lan-
guage models. In Findings of the Association for
Computational Linguistics: EMNLP 2024, pages
9057-9079, Miami, Florida, USA. Association for
Computational Linguistics.

Shigi Chen, Yiran Zhao, Jinghan Zhang, |-Chun
Chern, Siyang Gao, Pengfei Liu, and Junxian He.
2023. FELM: Benchmarking factuality evalua-
tion of large language models. In Thirty-seventh
Conference on Neural Information Processing
Systems Datasets and Benchmarks Track.

Xiang Chen, Duanzheng Song, Honghao Gui,
Chenxi Wang, Ningyu Zhang, Yong Jiang, Fei
Huang, Chengfei Lyu, Dan Zhang, and Hua-
jun Chen. 2024b. Factchd: benchmarking fact-
conflicting hallucination detection. In Proceea-
ings of the Thirty-Third International Joint Con-
ference on Artificial Intelligence, \JCAI ’24.

Zheng Chu, Jingchang Chen, Qianglong Chen,
Weijiang Yu, Haotian Wang, Ming Liu, and Bing
Qin. 2024. TimeBench: A comprehensive eval-
uation of temporal reasoning abilities in large
language models. In Proceedings of the 62nd
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1204-1228, Bangkok, Thailand. Associa-
tion for Computational Linguistics.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton,
Reiichiro Nakano, Christopher Hesse, and John
Schulman. 2021. Training verifiers to solve math
word problems.


Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological
bulletin, 76(5):378.

Aaron Grattafiori, Abhimanyu Dubey, ..., and Zhiyu
Ma. 2024. The llama 3 herd of models.

Dan Hendrycks, Collin Burns, Saurav Kadavath,
Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. 2021. Measur-
ing mathematical problem solving with the math
dataset.

Lei Huang, Weijiang Yu, Weitao Ma, Weihong
Zhong, Zhangyin Feng, Haotian Wang, Qiang-
long Chen, Weihua Peng, Xiaocheng Feng, Bing
Qin, and Ting Liu. 2025. A survey on hallucina-
tion in large language models: Principles, tax-
onomy, challenges, and open questions. ACM
Trans. Inf. Syst., 43(2).

Albert Q. Jiang, Alexandre Sablayrolles, Arthur
Mensch, Chris Bamford, Devendra Singh Chap-
lot, Diego de las Casas, Florian Bressand,
Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, Lélio Renard Lavaud, Marie-Anne
Lachaux, Pierre Stock, Teven Le Scao, Thibaut
Lavril, Thomas Wang, Timothée Lacroix, and
William El Sayed. 2023. Mistral 7b.

Adam Tauman Kalai, Ofir Nachum, Santosh S.
Vempala, and Edwin Zhang. 2025. Why lan-
guage models hallucinate.

J. Richard Landis and Gary G. Koch. 1977. The
measurement of observer agreement for cate-
gorical data. Biometrics, 33(1):159174.

Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie,
and Ji-Rong Wen. 2023. HaluEval: A large-scale
hallucination evaluation benchmark for large lan-
guage models. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 6449-6464, Singa-
pore. Association for Computational Linguistics.

Bill Yuchen Lin, Yuntian Deng, Khyathi Raghavi
Chandu, Faeze Brahman, Abhilasha Ravichan-
der, Valentina Pyatkin, Nouha Dziri, Ronan Le
Bras, and Yejin Choi. 2024. Wildbench: Bench-
marking Ilms with challenging tasks from real
users in the wild. CoRR, abs/2406.04770.

Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng,
Richeng Xuan, Houfeng Wang, and Xi Yang.
2024. Halludial: A large-scale benchmark for au-
tomatic dialogue-level hallucination evaluation.

Potsawee Manakul, Adian Liusie, and Mark JF
Gales. 2023. Selfcheckgpt: Zero-resource
black-box hallucination detection for genera-
tive large language models. arXiv preprint
arXiv:2303.08896.

Ariana Martino, Michael lannelli, and Coleen
Truong. 2023. Knowledge injection to counter
large language model (Ilm) hallucination. In The
Semantic Web: ESWC 2023 Satellite Events:
Hersonissos, Crete, Greece, May 28 - June 1,
2023, Proceedings, page 182-185, Berlin, Hei-
delberg. Springer-Verlag.

Abhika Mishra, Akari Asai, Vidhisha Balachandran,
Yizhong Wang, Graham Neubig, Yulia Tsvetkov,
and Hannaneh Hajishirzi. 2024. Fine-grained
hallucination detection and editing for language
models. In First Conference on Language Mod-
eling.

OpenAl, Aaron Hurst, ..., and Yury Malkov. 2024.
Gpt-40 system card.

Nils Reimers and Iryna Gurevych. 2019. Sentence-
bert: Sentence embeddings using siamese bert-
networks.

Peter J Rousseeuw. 1987. Silhouettes: a graphical
aid to the interpretation and validation of cluster
analysis. Journal of computational and applied
mathematics, 20:53-65.

Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic index-
ing. Communications of the ACM, 18(11):613—
620.

Gemma Team, Aishwarya Kamath, ..., and Léonard
Hussenot. 2025. Gemma 3 technical report.

Qwen Team. 2024. Qwen2.5: A party of foundation
models.

Saad Obaid ul Islam, Anne Lauscher, and Goran
Glavas. 2025. How much do Ilms hallucinate
across languages? on multilingual estimation of
IIm hallucination in the wild.

Yuqing Wang and Yun Zhao. 2024. TRAM: Bench-
marking temporal reasoning for large language
models. In Findings of the Association for Com-
putational Linguistics: ACL 2024, pages 6389-—
6415, Bangkok, Thailand. Association for Com-
putational Linguistics.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan
Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexan-
der M. Rush. 2020. Transformers: State-of-the-
art natural language processing. In Proceedings
of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demon-
strations, pages 38-45, Online. Association for
Computational Linguistics.


An Yang, Anfeng Li, ..., and Zihan Qiu. 2025.
Qwen3 technical report.

Shiping Yang, Renliang Sun, and Xiaojun Wan.
2023. A new benchmark and reverse validation
method for passage-level hallucination detection.
In Findings of the Association for Computational
Linguistics: EMNLP 2023, pages 3898-3908,
Singapore. Association for Computational Lin-
guistics.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao
Liu, Tingchen Fu, Xinting Huang, Enbo Zhao,
Yu Zhang, Yulong Chen, et al. 2025. Siren’s song
in the ai ocean: A survey on hallucination in large
language models. Computational Linguistics,
pages 1-46.

Wenting Zhao, Tanya Goyal, Yu Ying Chiu, Liwei
Jiang, Benjamin Newman, Abhilasha Ravichan-
der, Khyathi Chandu, Ronan Le Bras, Claire
Cardie, Yuntian Deng, and Yejin Choi. 2024a.
Wildhallucinations: Evaluating long-form factual-
ity in Ilms with real-world entity queries.

Wenting Zhao, Xiang Ren, Jack Hessel, Claire
Cardie, Yejin Choi, and Yuntian Deng. 2024b.
Wildchat: 1m chatGPT interaction logs in the
wild. In The Twelfth International Conference on
Learning Representations.

Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang
Xing, Chong Meng, Shuaigiang Wang, Zhicong
Cheng, Zhaochun Ren, and Dawei Yin. 2024c.
Knowing what LLMs DO NOT know: A simple yet
effective self-detection method. In Proceedings
of the 2024 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (Vol-
ume 1: Long Papers), pages 7051-7063, Mexico
City, Mexico. Association for Computational Lin-
guistics.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng,
Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yong-
hao Zhuang, Zhuohan Li, Zi Lin, Eric. P Xing,
Joseph E. Gonzalez, lon Stoica, and Hao Zhang.
2023. Lmsys-chat-1m: A large-scale real-world
llm conversation dataset.

Zhiying Zhu, Yiming Yang, and Zhiging Sun. 2025.
Halueval-wild: Evaluating hallucinations of lan-
guage models in the wild. In [CLR 2025 Work-
shop on Building Trust in Language Models and
Applications.

Silhouette Sco!

50
10 15 20 25 30 35 40 45 50
Number of Clusters

io 15 20 25 30 35 40 45 50
Number of Clusters

(a) Silhouette and inertia for clustering on first-pair
queries.

0.295
cc)

b
N
3°

3
8 0.290
wn

ov
# 0.285

ry
°

E
3 0.280

Inertia (x1000)

a
°

£
H 0.275
40
Jo 15 20 25 30 35 40 45 50
Number of Clusters

10 15 20 25 30 35 40 45 50
Number of Clusters

(b) Silhouette and inertia for clustering on second-pair
queries.

Figure 3: The changing curves of silhouette score
and inertia under different cluster numbers.

A. Benchmark Construction Details

A.1. Query Clustering

To select representative dialogues, we apply k-
means clustering to all first-pair queries and second-
pair queries in the dialogues, resulting in 45 and 20
clusters respectively. The number of clusters (k)
is determined based on two clustering quality met-
rics: the silhouette score (Rousseeuw, 1987) and
inertia. The silhouette score evaluates clustering
quality by measuring both intra-cluster cohesion
and inter-cluster separation. It ranges from —1 to
1, with higher values indicating better-defined clus-
ters. Inertia, also known as the within-cluster sum
of squares, measures cluster compactness; it is
always non-negative, and larger values imply more
dispersed clusters.

Considering both metrics, we plot the curves of
silhouette score and inertia as the number of clus-
ters (k) varies from 10 to 50 in increments of 5, as
shown in Figure 3. To balance clustering quality
and compactness, we finally select k = 45 for first-
pair queries and k = 20 for second-pair queries.

A.2. Extracting Keywords and Name all
Clusters

We obtain 45 clusters from the first queries and 20
clusters from the second queries in the dialogues.
To better characterize each cluster, we first employ
TF-IDF (Salton et al., 1975) to extract the top 25
keywords from each cluster. We then use GPT-40
(OpenAl et al., 2024) to assign descriptive names
to all clusters based on the extracted keywords.
The instruction used to prompt GPT-40 is shown
in Figure 4. The extracted keywords and the corre-


| have several clusters of words, and | want you
to give each cluster a descriptive topic or name
based on the overall theme or commonality of the
words.
For each cluster, provide a short and meaningful
label that best represents the group.
Here are the clusters:

¢ Cluster 1: [word1, word2, word3, ...]

* Cluster 2: [word1, word2, word3, ...]

Please return the output as a list like:

* Cluster 1: [Your suggested topic name]
* Cluster 2: [Your suggested topic name]

Figure 4: Topic naming instruction for GPT-40

sponding cluster names are presented in Table 5
and Table 6.

A.3. Hallucination Rate across All Topics

Based on the clusters derived from the first queries
of all dialogues, we calculate the hallucination rate
for each cluster, as shown in Table 7. Since each
cluster has been assigned a descriptive name by
GPT-4o, we can intuitively interpret the hallucina-
tion performance of LLMs across different topics.

B. Prompt Templates

B.1. Prompt Templates of Hallucination
Detection

For hallucination detection, we consider three
experimental settings: single-model detection,
ensemble-based detection, and in-context detec-
tion. Since ensemble-based detection is performed
based on the results of single-model detection, we
focus on designing prompt templates for the re-
maining two settings.

Prompt template of single-model detection is
shown in Figure 5. It consists of a system prompt
and a user prompt. In the system prompt, we as-
sign the model the role of a hallucination detection
expert, while in the user prompt, we clearly describe
the detection task and provide the query—response
pair to be evaluated.

prompt template of in-context detection is
shown in Figure 6. The key difference from the
single-model setting is that it incorporates the first
query-response pair of the dialogue as supplemen-
tary context for the hallucination detector. Notably,
in this setting, we do not ask the model to detect
hallucinations in both the first and second pairs. In-

System prompt:

You are an expert in hallucination detection. An-
swer only with ‘Hallucination’ or No Hallucination’.
User prompt:

A hallucination refers to generated content that
conflicts with the input, the model’s own context,
or known facts.

Your task is to detect if the answer given to the
user prompt is a hallucination.

Respond strictly and only with one of the following
labels:

- Hallucination

- No Hallucination

User Query: <content of query>

Generated Answer: <content of answer>

Label:

Figure 5: Prompt of single-model detection.

System prompt:

You are an expert in hallucination detection. An-
swer only with ‘Hallucination’ or No Hallucination’.
User prompt:

A hallucination refers to generated content that
conflicts with the input, the model’s own context,
or known facts.

Your task is to detect if the second answer (Gener-
ated Answer 2) given to the second user prompt
(User Query 2) is a hallucination based on the
complete conversation, including the context from
User Query 1 and Generated Answer 1.
Respond strictly and only with one of the following
labels:

- Hallucination

- No Hallucination

Conversation:

User Query 1: <content of query 1>

Generated Answer 1: <content of answer 1>
User Query 2: <content of query 2>

Generated Answer 2: <content of answer 2>
Label:

Figure 6: Prompt of in-context detection.

stead, our goal is to examine whether including the
first pair as additional context improves the model’s
performance in detecting hallucinations in the sec-
ond pair.

B.2. Prompt Templates of Hallucination
Categorization

We consider two settings for hallucination cat-
egorization: single-model categorization and
ensemble-based_ categorization. Since the


Cluster

Name

Keywords

Cluster 0

Cluster 1

Cluster 2

Cluster 3

Cluster 4

Cluster 5

Cluster 6

Cluster 7

Cluster 8

Cluster 9

Cluster 10

Cluster 11

Cluster 12

Cluster 13

Cluster 14

Cluster 15

Cluster 16

Cluster 17

Cluster 18

Cluster 19

Cluster 20

Cluster 21

Cluster 22

Cluster 23

Cluster 24

Cluster 25

Cluster 26

Cluster 27

Cluster 28

Cluster 29

Cluster 30

Cluster 31

Cluster 32

Cluster 33

Cluster 34

Cluster 35

Cluster 36

Cluster 37

Cluster 38

Cluster 39

Cluster 40

Cluster 41

Cluster 42

Cluster 43

Cluster 44

ensemble-based approach relies on the results of
single-model categorization, we only design the

Technical Errors & Environment
Finance & Investment

Nature & Simple Concepts
Programming in Python

Casual Conversation & Greetings
Erotic Roleplay & Personal Interaction
Types & Matching Conditions

Al Models & Machine Learning
Repeated Technical Phrases with Con-
versational Tone

Error & Environment Terminology
Enterprise & Environmental Terms
Text Processing & Natural Language
Tasks

Math & Number Problems

Mixed Chat with Technical Recurrence
Life Advice & Human Experience
Languages & Translation

Data, Al & Business Technology
Philosophy, Definitions & Legal Con-
cepts

Software Development & Deployment
Tools

Greeting & User Interaction
Moderation & Text Correction
Geography & World Affairs

Humor & Jokes

Business & Environmental Repetition
SQL & Data Management

Equations & Environment with Miscella-
neous Words

Characters, Animation & Visibility
Games, Media & Creative Generation
Python Coding & Number Sequences
Poetry, Songs & Creative Writing
Chatbots & Online Al Tools

Medical & Health Information

Web Development & Frontend Program-
ming

Environmental Equations & System Er-
rors

Friendly & Flirtatious Greetings
Science, Physics & Environment
Professional Emails & Business Writing
Chat & Al Equipment Tests

Cooking, Recipes & Dangerous Content
Language Correction & Grammar

Al Assistants & Prompt Design

Dates, Time & Calendar Info

System Configuration & Environment Er-
rors

LLMs & Open-Source Al Models

Code-Generated Visual Media

hi, 1a, equipment, envs, episode, equal, equals, equation, equations, equity, environment, equivalent, era, erotic, erp, err, error, errors,
environmental, env, escape, entered, enjoyed, enjoying, ensure

money, company, business, price, stock, bank, make, tell, market, bitcoin, explain, tax, financial, list, trading, companies, best, stocks, online,
investment, years, 2023, want, strategy, florida

apples, color, blue, cat, sky, does, red, tree, earth, apple, moon, dog, birds, room, left, legs, tell, old, eat, answer, sun, animals, riddle, chicken,
elephant

python, code, write, file, script, function, create, hello, program, files, using, world, use, command, string, explain, print, linux, directory,
import, example, list, bash, windows, text

tell, hello, hi, hey, help, introduce, today, say, know, don, good, talk, ask, doing, chat, whats, going, question, feeling, life, sad, make, believe,
let, just

story, write, woman, girl, roleplay, like, women, tell, girlfriend, sex, want, pretend, say, man, make, play, talk, erotic, short, hi, friend, good,
birthday, person, wife

types, highlight, experiences, 150, conditions, special, popular, 200, matches, sentences, ask, description, number, words, create, following,
meets, eventually, envs, episode, equal, equals, equity, equation, equations

model, language, large, explain, models, learning, diffusion, stable, test, neural, does, transformer, network, tell, pytorch, machine, write,
difference, ai, work, data, works, training, know, token

doing, grandma, honey, guys, ar, errors, episode, equal, equals, equation, equations, equipment, equity, equivalent, envs, era, erotic, erp, err,
error, ua, environmental, escape, environment, ensuring

hi, good, hello, 1a, environment, envs, episode, equal, equals, equation, equations, equipment, equity, equivalent, era, erotic, erp, err, error,
environmental, env, es, entered, enjoyed, enjoying

equivalent, ma, episode, equal, equals, equation, equations, equipment, equity, era, environmental, erotic, erp, err, error, errors, es, escape,
envs, environment, effectiveness, enterprise, enjoying, ensure, ensuring

json, question, following, answer, text, sentence, output, input, format, user, product, list, response, task, category, given, extract, search,
sentiment, context, query, generate, words, intent, instruction

number, numbers, 10, prime, solve, equation, value, 2x, square, root, 12, pi, 13, step, answer, pattern, calculate, 100, write, 11, result, plus,
20, probability, 3x

hey, today, tonight, llama, thought, say, just, hi, like, equal, equals, equation, equations, equipment, equivalent, equity, envs, era, erotic, erp,
err, error, episode, environmental, es

life, write, people, tell, answer, world, good, person, like, think, purpose, make, work, hi, provide, does, way, human, know, feel, question,
better, want, best, questions

speak, translate, english, chinese, russian, spanish, languages, que, language, japanese, french, sentence, arabic, hello, know, korean,
understand, say, tell, la, german, di, word, answer, portuguese

know, data, tell, does, explain, security, write, software, ai, management, company, use, test, dataset, date, information, business, difference,
requirement, model, learning, create, science, network, best

meaning, life, offense, twins, committed, considering, legal, universe, paragraphs, whats, words, sense, explain, live, answer, number, write,
code, response, define, sarcastic, trying, explanations, following, remember

aws, windows, api, linux, write, code, server, use, file, using, create, explain, git, command, know, data, does, github, access, 10, tell, docker,
app, python, service

hi, hello, whats, hey, change, guess, tell, declare, know, speech, ur, day, german, character, ignore, translate, ok, meet, previous, username,
nice, instructions, called, respond, order

input, paragraph, need, orginin, typo, correcting, violates, moderation, intention, contents, guidelines, modify, robot, grammar, send, error,
free, meaning, sentences, try, content, output, hello, time, help

capital, weather, world, city, war, tell, today, trip, countries, travel, day, best, win, plan, country, china, won, france, russia, know, population,
ukraine, list, india, japan

joke, tell, love, jokes, wear, briefs, racing, track, funny, whats, racist, make, write, field, underwear, dad, female, popular, dirty, ur, original,
women, inside, people, best

equivalent, ma, episode, equal, equals, equation, equations, equipment, equity, era, environmental, erotic, erp, err, error, errors, es, escape,
envs, environment, effectiveness, enterprise, enjoying, ensure, ensuring

sql, table, query, data, column, write, code, python, database, id, select, excel, string, columns, date, regex, create, csv, list, file, text, value,
time, number, format

things, hey, tell, xa, envs, equal, equals, equation, equations, equipment, equity, equivalent, era, erotic, erp, err, error, errors, es, episode,
environmental, escaped, enters, ensure, ensuring

dot, single, write, character, visibility, reason, paper, black, rewrite, string, line, new, hidden, visible, 25, 50, 100, infinite, span, animation,
repeat, come, equity, equipment, equations

game, video, write, image, youtube, names, play, make, description, list, prompt, ai, best, text, create, generate, like, ideas, videos, chess, 10,
python, games, want, midjourney

write, python, code, function, rust, numbers, program, fibonacci, int, number, array, sort, list, return, print, using, sum, string, count, 10, add,
sequence, hello, create, loop

poem, write, song, haiku, joke, love, tell, rap, short, lyrics, funny, word, make, words, rhyme, music, cats, line, rhymes, story, rhyming, style,
create, cat, dog

chatgpt, gpt, chat, internet, better, chatbot, access, use, bot, compare, model, discord, openai, ai, gpt4, python, write, api, difference, using,
create, hi, vicuna, hello, telegram

patient, medical, tell, pain, does, disease, cancer, blood, sleep, answer, best, body, human, explain, doctor, treatment, medication, clinical,
list, following, effects, normal, symptoms, day, health

html, code, javascript, write, react, js, create, page, css, website, typescript, button, web, using, php, simple, chrome, text, make, function, div,
script, app, generate, express

ok, a, equity, envs, episode, equal, equals, equation, equations, equipment, equivalent, era, erotic, erp, err, error, errors, es, environmental,
environment, escaped, enterprise, enjoying, ensure, ensuring

hello, hi, today, doing, hey, going, day, good, morning, friend, dear, evening, far, sexy, real, tell, friends, man, person, like, horse, love, act,
fine, having

explain, quantum, does, tell, car, energy, climate, used, water, electric, difference, power, write, use, air, make, change, solar, simple,
temperature, 10, step, computing, theory, physics

write, email, job, letter, company, customer, help, make, create, business, want, team, service, product, following, work, software, need,
project, manager, ask, use, questions, generate, provide

hello, hi, test, friend, hey, guys, computer, ai, speaker, looks, vicuna, doing, checked, saw, changed, 13b, higher, better, just, equipment,
episode, equity, equivalent, era, equations

make, bomb, recipe, best, eat, step, hi, water, tell, coffee, like, meth, good, pizza, create, build, cake, way, does, dinner, eggs, ingredients, list,
cook, food

sentence, word, words, following, text, english, sentences, correct, letter, write, rephrase, rewrite, grammar, answer, question, make, list, help,
letters, does, paragraph, want, language, generate, say

ai, prompt, user, assistant, question, answer, chatbot, write, human, model, task, use, intelligence, artificial, response, bot, questions, like,
ask, best, text, want, prompts, help, following

today, time, day, date, year, days, 2023, current, week, yesterday, tomorrow, friday, hours, sunday, wednesday, months, tuesday, june, 30, 12,
years, answer, 05, tell, ago

change, real, os, mean, does, equipment, episode, equal, equals, equation, equations, ma, envs, equivalent, era, erotic, erp, err, error, errors,
equity, environment, environmental, enters, ensure

llm, vicuna, model, llama, best, tell, difference, llms, 13b, cpu, know, use, version, gpu, alpaca, run, explain, does, open, ram, ai, source,
models, hello, langchain

code, write, image, draw, python, generate, script, using, ascii, circle, images, function, create, make, random, unity, svg, art, 3d, audio,
ffmpeg, use, video, algorithm, object

Table 5: Keywords and names for all clusters of first-pair queries.

the definitions of these three categories, followed
by the instruction for hallucination categorization.

prompt for the single-model setting.

Prompt template of single-model categoriza-
tion is shown in Figure 7. Following Zhang et al.
(2025), we divide hallucinations into three cate-
gories: input-conflicting, context-conflicting, and
fact-conflicting hallucinations. The prompt includes


Cluster

Name

Keywords

Cluster 0

Cluster 1

Cluster 2

Cluster 3

Cluster 4

Cluster 5

Cluster 6

Cluster 7

Cluster 8

Cluster 9

Cluster 10

Cluster 11

Cluster 12

Cluster 13

Cluster 14

Cluster 15

Cluster 16

Cluster 17

Cluster 18

Cluster 19

z
2:

Structured Data & SQL Queries
Business Writing & Planning
Geopolitics & World Knowledge
Programming in Python

Language Exercises & Sentence Con-
struction

Conversational Roleplay & NSFW
Themes

Tech Tools & System Configuration
European Terms & Evaluative Lan-
guage

Mathematical Functions & Sequences
Everyday Questions & Basic Reasoning
Machine Learning & Model Explanation
Medical Information & Health Questions
Creative Writing & Humor

Physics & Energy Concepts

Russian Language & Online Services
Dates, Time & Temporal Data
Languages & Translation

Al Models & Language Generation

Word Meaning & Ethical Concepts

Web Development & Frontend Coding

table, sql, query, json, format, data, database, list, column, text, extract, following, create, id, write, code, output, search, generate, value,
columns, use, select, type, number

write, company, business, email, make, money, product, job, want, plan, best, list, project, data, work, need, provide, use, create, customer,
know, does, research, tell, good

capital, weather, tell, world, countries, war, city, country, travel, know, china, people, russia, states, plan, list, best, india, did, won, day, trip,
japan, ukraine, united

python, code, write, function, file, program, script, use, using, error, string, example, create, list, int, generate, make, print, array, input, return,
rust, command, number, data

answer, make, sentence, words, write, question, word, text, list, following, game, sentences, prompt, correct, rewrite, generate, explain, use,
like, letter, continue, questions, 10, just, provide

write, story, like, tell, want, say, woman, make, girl, think, just, nsfw, love, know, people, sex, man, talk, don, good, roleplay, person, feel,
human, ask

use, version, access, does, api, internet, windows, aws, using, need, gpu, run, server, app, file, download, create, vicuna, write, linux, tell,
tools, cpu, install, android

in hebrew: growth, ethical, establish, established, estate, estimate, estimated, et, etf, ethics, essential, eu, europe, european, eval, evaluate,
evaluating, evaluation, est, essay, electronics, erotic, equal, equals, equation

number, numbers, step, answer, 10, solve, prime, write, python, function, value, equation, fibonacci, calculate, 100, square, 12, code,
sequence, result, 2x, correct, explain, math, 20

make, apples, color, dog, cat, does, blue, recipe, animal, like, sky, tell, tree, cats, legs, eat, birds, answer, old, chicken, eggs, room, left,
animals, red

explain, example, examples, learning, data, test, use, does, difference, model, write, step, diffusion, tell, used, mean, pytorch, code, like,
using, machine, stable, steps, provide, work

life, does, meaning, patient, make, meth, pain, tell, doctor, answer, blood, symptoms, medical, use, day, medication, cause, explain, disease,
good, best, treatment, eat, effect, drugs

joke, write, poem, tell, song, sure, funny, make, haiku, story, rap, rhyme, short, love, lyrics, 10, jokes, line, explain, try, humor, rhymes, style,
rhyming, words

make, bomb, car, explain, earth, does, water, energy, quantum, tell, long, temperature, answer, power, build, light, use, used, sun, write,
moon, nuclear, step, electric, pressure

in russian: (na, kak, dlya, chto, ty, russkiy, napishi, po, perevedi, est’, iz), online, 2007, russian, emails, template, receive, 30, deals, recording,
locate, assist, sleep, ip, echo

date, time, today, day, year, data, days, 2023, month, current, 2022, week, layer —iitlot, out##, recent, years, training, format, dates, 12,
yesterday, updated, does, whats, months

translate, english, chinese, speak, que, russian, la, language, en, spanish, japanese, arabic, german, di, french, para, languages, tu, es,
sentence, word, say, como, korean, know

ai, model, language, Ilm, chatgpt, gpt, models, vicuna, large, chat, use, chatbot, know, llama, open, better, Ilms, write, tell, train, bot, explain,
does, best, openai

word, exactly, answer, fuck, searching, honest, 10, using, think, established, est, establish, in hebrew: growth, essay, estate, estimate,
estimated, et, etf, ethical, ethics, eu, europe, essential, escape

code, write, html, make, javascript, using, script, create, image, use, example, generate, want, draw, js, react, button, text, add, file, css, ascii,
page, fix, programming

Table 6: Keywords and names for all clusters of second-pair queries.

Topic of Cluster

System prompt:
You are an expert in hallucination categorization.
Answer only with ’A’,, ’B’ or ’C’.

Hallu Rate (%) User prompt:

OMONOUARWNM—

Math & Number Problems

Dates, Time & Calendar Info

Al Models & Machine Learning
Chatbots & Online Al Tools
Nature & Simple Concepts
Language Correction & Grammar
Al Assistants & Prompt Design
Medical & Health Information
Languages & Translation

10 Science, Physics & Environment
11 Geography & World Affairs

12 Finance & Investment

13 Data, Al & Business Technology

14 Games, Media & Creative Generation

15 Life Advice & Human Experience

16 Cooking, Recipes & Dangerous Content
17 Professional Emails & Business Writing
18 Casual Conversation & Greetings

19 Philosophy, Definitions & Legal Concepts

20 Greeting & User Interaction

21 Moderation & Text Correction
22 Humor & Jokes

23 Friendly & Flirtatious Greetings
24 Chat & Al Equipment Tests

Table 7: Hallucination rates of LLMs under different

topics.

aed A hallucination can be categorized into one of
42.11 the three categories: Input-conflicting hallucina-
41.67 tions (A) appear when generated content differs
jean from what was given to the model as source (the
36.84 model does not answer the question). Context-
36.36 conflicting hallucinations (B) appear as informa-
33.33 . . : .
33.33 tion that is out of place and conflicts with what was
32.14 previously generated (the model contradicts itself).
aa Fact-conflicting hallucinations (C) is content that
22.29 is not factual nor faithful to what is known to be
20.00 true and not based on any knowledge (the model
eee produces unfactual content).
14.29 Your task is to detect which category matches the
hee given hallucination in the generated answer.
0.00 Respond strictly and only with one of the following
0.00 labels:
0.00 _B

-C

User Query: <content of query>
Generated Answer: <content of answer>
Category:

Figure 7: Prompt of single-model categorization.
