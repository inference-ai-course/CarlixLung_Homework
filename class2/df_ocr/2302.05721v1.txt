arXiv:2302.05721vl [cs.HC] 11 Feb 2023

Synthesizing Human Gaze Feedback for Improved NLP Performance

Varun Khurana*
Adobe, IIIT Delhi

varunl9124@iiitd.ac.in

Nora Hollenstein
University of Copenhagen

nora.hollenstein@hum.ku.dk

Abstract

Integrating human feedback in models can im-
prove the performance of natural language pro-
cessing (NLP) models. Feedback can be ei-
ther explicit (e.g. ranking used in training
language models) or implicit (e.g. using hu-
man cognitive signals in the form of eyetrack-
ing). Prior eye tracking and NLP research re-
veal that cognitive processes, such as human
scanpaths, gleaned from human gaze patterns
aid in the understanding and performance of
NLP models. However, the collection of real
eyetracking data for NLP tasks is challeng-
ing due to the requirement of expensive and
precise equipment coupled with privacy inva-
sion issues. To address this challenge, we pro-
pose ScanTextGAN, a novel model for gener-
ating human scanpaths over text. We show
that ScanTextGAN-generated scanpaths can
approximate meaningful cognitive signals in
human gaze patterns. We include synthetically
generated scanpaths in four popular NLP tasks
spanning six different datasets as proof of con-
cept and show that the models augmented with
generated scanpaths improve the performance
of all downstream NLP tasks.

1 Introduction

Integrating human signals with deep learning mod-
els has been beginning to catch up in the last few
years. Digital traces of human cognitive processing
can provide valuable signals for Natural Language
Processing (Klerke et al., 2016a; Plank, 2016). Var-
ious approaches for integrating human signals have
been explored. For example, human feedback for
better decisioning (Christiano et al., 2017), NLP
tasks (Stiennon et al., 2020; Wu et al., 2021), and
most recently language modeling using reinforce-
ment learning with human feedback (RLHF) based
reward (Bai et al., 2022; Ouyang et al., 2022).
RLHF involves explicit human feedback and is

* Equal Contribution

Rajesh Kumar
Bucknell University

rajesh. kumar@bucknell.edu

Yaman Kumar Singla*
Adobe, IIIT Delhi, SUN Y-Buffalo

ykumar@adobe.com

Balaji Krishnamurthy
Adobe

kbalaji@adobe.com

(FS ~

Investors seem bullish about the stock market.

SENTIMENT: Positive

| work 40 hours a week to be this poor.

SARCASM: Present

The author is one of several defense experts expected to testify.
Spitz is expected to testify later for the defense.
Ne PARAPHRASE: True SD)

Figure 1: Generated scanpaths over text samples taken from
various natural language processing (NLP) tasks. The green
circles denote the important words characteristic of that task.
The circles’ size denotes the fixation duration, and the arrows
depict the saccadic movements. As can be seen, linguisti-
cally important words often have a higher fixation duration
and revisit. Regressions (word revisits) also appear in the
examples.

expensive and hard to scale. On the other hand, pre-
vious studies have also tried to use implicit human
feedback in the form of eyetracking signals. It has
proven to be a useful signal for inferring human
cognitive processing (Sood et al., 2020; Hollen-
stein and Zhang, 2019; Mathias et al., 2020). NLP
researchers have focused on assessing the value
of gaze information extracted from large, mostly
dis-jointly labeled gaze datasets in recurrent neu-
ral network models (Ren and Xiong, 2021; Strzyz
et al., 2019; Barrett et al., 2018a). The proposed
approaches under this paradigm include gaze as an
auxiliary task in multi-task learning (Klerke et al.,
2016b; Hollenstein et al., 2019), as additional sig-
nals (Mishra et al., 2016b), as word embeddings
(Barrett et al., 2018b), as type dictionaries (Barrett
et al., 2016a; Hollenstein and Zhang, 2019), and as
attention (Barrett et al., 2018a).

Previous studies demonstrate that human scan-


PEO OST PCOS ON

A year ago, the company posted a profit of 12 cents a share.
a. Reading Scanpath

(iN

A year ago, the company posted a profit of 12 cents a share.

Natural Language Inference

b. Natural Language Inference Scanpath

9 Saal Sy ye

There were conflicting reports about the number of casualties yesterday.

a. Reading Scanpath

a an a

There were conflicting reports about the number of casualties yesterday.

b. Sentiment Scanpath

Sentiment Analysis

Figure 2: (Intent-aware) Scanpath samples generated by con-
ditioning scanpath generation on different downstream natural
language tasks. Note that the conditioned scanpaths are heav-
ily biased to words important for that downstream task.

paths (temporal sequences of eye fixations, see
Fig. 1) gleaned from eye tracking data improve the
performance of NLP models. However, the real-
world application of these methods remains limited
primarily due to the cost of precise eye-tracking
equipment, users’ privacy concerns, and manual
labor associated with such a setup. Therefore, gen-
erating scanpaths from existing eyetracking cor-
pora would add great value to NLP research. To
the best of our knowledge, this is the first paper
to propose a model that generates scanpaths for a
given read text with good accuracy. We call the
model, ScanTextGAN.

We demonstrate the scanpath generation capa-
bility of ScanTextGAN over three eye-tracking
datasets using multiple evaluation metrics. Fur-
ther, we evaluate the utility of generated scanpaths
for improvements in the performance of multiple
NLP tasks (see Figs. 1,2) including the ones in the
GLUE benchmark (Wang et al., 2018). The gener-
ated scanpaths achieve similar performance gains
as the models trained with real scanpaths for classic
NLP tasks like sentiment classification, paraphrase
detection, entailment, and sarcasm detection.

Our contributions are threefold:

1. We propose ScanTextGAN, the first scanpath
generator over text.

2. We compare ScanTextGAN with multiple base-
lines and conduct ablation experiments with vary-
ing models and configurations. The model per-
forms well on the test sets and cross-domain gen-
eralization on two additional eyetracking datasets
belonging to different text domains.

3. We tested the usefulness of generated scanpaths
in downstream NLP tasks such as sentiment anal-

ysis, paraphrase detection, and sarcasm detection
on six different datasets. The results show that
the downstream NLP tasks benefited significantly
from cognitive signals inherent in generated scan-
paths. Further, we show how scanpaths change
when finetuning with downstream natural language
tasks (Figs.2,6) and that they lead to further im-
provements in downstream task performance ($4.3)
showing how they can act as additional controls
beyond the task architecture.

2 Related Work

When reading a text, humans do not focus on every
word and often do not read sequentially (Just and
Carpenter, 1980). A series of studies in psycho-
linguistics have shown that the number of fixations
and the fixation duration on a word depend on sev-
eral linguistic factors. The linguistic factors can
also be determined given the cognitive features
(Clifton Jr et al., 2007; Demberg and Keller, 2008).
Though advances in ML architecture have helped
bring machine comprehension closer to human per-
formance, humans are still superior for most NLP
tasks (Blohm et al., 2018; Xia et al., 2019).

It has been shown in the literature that integrat-
ing explicit (Bai et al., 2022; Ouyang et al., 2022)
and implicit (cognitive processing) human feed-
back signals in traditional ML models is expected
to improve their performance (Just and Carpen-
ter, 1980). However, the cost of explicit feedback
(e.g., using MTurk) and implicit feedback (e.g.,
eye tracking) at scale is excessively high. Similarly,
privacy-invasive eye-tracking processes limit the
scope of this idea. One way to address this prob-
lem is to use generated eye movements to unfold
the full potential of eye-tracking research. Hence,
the idea is to architect ScanTextGAN, a scanpath
generator for text reading, and test its usefulness in
downstream NLP tasks.

More precisely, this work builds upon previous
works on 1) human attention modeling and 2) gaze
integration in neural network architectures, which
are described as follows:

Human Attention Modeling: Predicting what
people visually attend to in images (saliency predic-
tion) is a long-standing challenge in neuroscience
and computer vision, the fields have seen many
data-based models (Wang et al., 2021). In contrast
to images, most attention models for eye move-
ment behaviors during reading are cognitive pro-
cess models, i.e., models that do not involve ma-


chine learning but implement cognitive theories
(Engbert et al., 2005; Xia et al., 2019). Key chal-
lenges for such models are a limited number of
parameters and hand-crafted rules. Thus, it is diffi-
cult to adapt them to different tasks and domains
and use them as part of end-to-end trained ML
architectures (Kotseruba and Tsotsos, 2020). In
contrast, learning-based attention models for text
remain under-explored. Within that, all eye track-
ing models are saliency prediction models with
non-existent work in predicting scanpaths. On the
other hand, visual scanpaths generation for image-
based eye tracking data has been recently explored
for both traditional (Assens et al., 2019) and 360°
images (Martin et al., 2022).

Matthies and Sggaard (2013) presented the first
fixation prediction work for text. They built a
person-independent model using a linear Condi-
tional Random Fields (CRF) model. Hahn and
Keller (2016) designed the Neural Attention Trade-
off (NEAT) language model, which was trained
with hard attention and assigned a cost to each fixa-
tion. Other approaches include sentence representa-
tion learning using surprisal and part of speech tags
as proxies to human attention (Wang et al., 2017).

Our work differs from previous studies as we
combine cognitive theory and data-driven ap-
proaches to predict scanpaths and further show its
application in downstream NLP tasks (Hollenstein
et al., 2021b,a).

Integrating Gaze in Network Architecture:
Integration of human gaze data into neural network
architectures has been explored for a range of com-
puter vision tasks such as image captioning, visual
question answering, and tagging (Karessli et al.,
2017; Yu et al., 2017; He et al., 2019; Boyd et al.,
2022). Hence, recent research has utilized features
gleaned from readers’ eye movement to improve
the performance of complex NLP tasks such as sen-
timent analysis (Long et al., 2017; Mishra et al.,
2016c), sarcasm detection (Mishra et al., 201 6b),
part-of-speech tagging (Barrett et al., 2016b), NER
(Hollenstein and Zhang, 2019), and text difficulty
(Reich et al., 2022).

While in recent years, eye tracking data has been
used to improve and evaluate NLP models, the
scope of related studies remains limited due to
the requirement of real-time gaze data at inference
time. Mathias et al. (2020) reported that there exists
no automated way of generating scanpaths yet in
the literature. With high-quality artificially gener-

ated scanpaths, the potential of leveraging eyetrack-
ing data for NLP can be unfolded. Additionally,
generating scanpaths that mimic human reading be-
havior will help advance our understanding of the
cognitive processes behind language understanding.
Hence, we propose ScanTextGAN; researchers can
use that to generate scanpaths over any text without
worrying about collecting them from real users.

3 Proposed Model

In this section, we define the scanpath generation
task, describe the ScanTextGAN model architec-
ture, and provide details on loss functions and
model training.

Task Definition: The task of scanpath genera-
tion is to generate a sequence S(T ) representing a
scanpath over the text T = {w, wa, ..., Wn} com-
posed of a sequence of words, can be defined as
follows:

S(T) = {.., (wi, t8), ns (wh, 0), (wh, )}

| (1)
where ¢’ represents the fixation duration over the
word we occurring at the position 7. Note that
it is not necessary to have a < b (words being
read in linear order) or that k = n (the number of
fixations being equal to the number of words). Due
to regressions, i.e., backward saccades to previous
words, words are also revisited. Hence, the same
word could appear multiple times in the sequence.

3.1 ScanTextGAN Model Architecture

Fig. 3 illustrates the proposed conditional GAN ar-
chitecture of the model. The ScanTextGAN model
is composed of two competing agents. First, a con-
ditional generator that generates scanpaths given
text prompts. The second is a discriminator net-
work, which distinguishes real human scanpaths
from the generated ones. The ScanTextGAN model
is trained by combining text content loss, scan-
path content loss, and adversarial loss (Eq. 6). The
scanpath content loss measures the difference be-
tween the predicted scanpath and the correspond-
ing ground truth scanpath. The text content loss
reconstructs the input text, and the adversarial loss
depends on the real/synthetic prediction of the dis-
criminator over the generated scanpath. We de-
scribe the losses along with the generator and dis-
criminator architectures next.

Generator: The ScanTextGAN generator
constitutes a transformer-based encoder-decoder
framework. The encoder is conditioned on


GENERATOR

Dense Text
Representations
(BERT)

Positional +
Encoding 5
ect

Transformer Encoder
Decoder

Gay.
c

Sequence Length 7 t c t :
Reconstructed f “ ‘
CLS Token Word ID | Duration | EOS :

TASK -2 |

NLP Models |
Sentiment

1@ NLP Models

augmented with
Improved

performance

Analysis

Sarcasm <—Text Investors. seem bullish about the stock market.
Detection

GENERATED SCANPATH

scanpaths

DISCRIMINATOR

1 FAKE REAL

an a."

a,

(BERT)

cua -_ Y y '
— Z Se tt BiLSTM BiLSTM :
' BatchNorm

| Investors seem  bullsh about the stock mart, |

‘ Dense Text '
: Scanpath Representations :

BatchNorm

<<

REAL SCANPATH

f

ot Multi-head
' ' Attention Fusion

Eye Tracking Data Recorded
t ri BiLSTM

Real / Fake

z=
z

3

>

if yo
3

g

a =

oe

a

Figure 3: The architecture of the proposed ScanTextGAN model. The model consists of a conditional generator
and a discriminator playing a zero-sum game. The generator is trained by two cognitively inspired losses: text
content reconstruction and scanpath content reconstruction.

BERT-based text embeddings (Devlin et al., 2019),
which are concatenated with noise to make the
generator’s output non-deterministic. The output of
the transformer encoder is supplied to the decoder,
which consists of task-specific feed-forward
networks. One branch generates the scanpath
(Task 1), while the other reconstructs the 768
dimensional CLS token embedding of the sentence
(Task 2). The scanpath is output as a temporal
sequence of word ID (fixation points) w’, fixation
duration ¢’, and end-of-sequence probability
EOS". At inference time, the length L(G) of
generated scanpath G' is determined as follows:

M otherwise

where M is the maximum scanpath length as de-
scribed in section §3.2 and r € (0, 1) is a probabil-
ity threshold. We use 7 = 0.5. The loss functions
of the two branches are described below.
Scanpath Content Loss tries to minimize the
deviation of generated scanpaths G(7,.\V) from
the ground-truth scanpaths R(T, h)) over text T

where ground-truth scanpaths are recorded from
the human h and A stands for Gaussian noise
N (0, 1). The loss function L, is given as:

1

Ls(G(TN), R(T, h)) = Oho (a(idy, — idj.)?+

k
B(t, — t,)°+y(Ey — E;)*)
(3)
which is a weighted sum of three terms. The

first term measures the error between real and
predicted fixation points given by the mean squared
difference between generated and real word-ids
(id), — idj.). It penalizes permutations of word
ids and trains the model to approximate the real
sequence of fixation points closely.

The second term measures the difference in fix-
ation durations given by the mean squared differ-
ence between generated and real duration ee —t').
Fixation durations simulate human attention over
words in the input text. Thus, a word with a
larger fixation duration is typically synonymous
with greater importance than other words in the
input text. This error term supplements the genera-
tor’s ability to learn human attention patterns over
the input text.


Finally, the third term measures the mean
squared error between the prediction of end-of-
sequence probability by real and generated distri-
butions (Ey — E'). These are weighted by the
hyperparameters a, 3, and y. Preliminary exper-
iments showed that optimizing the mean squared
error leads to better performance over the cross-
entropy loss for optimizing the EOS probability
output.

Text Content Loss: Scanpaths depend heavily
on the linguistic properties of the input text.
Therefore, to guide the generator towards near
the probable real data manifolds, we adopt
reconstruction of the CLS token embedding of the
input text (Task 2) by the generator as an auxiliary
task since the CLS token embedding encodes a
global representation of the input text. This text
content reconstruction loss L,. is given as:

L,(G(T.N), R(T, h)) = (BERT (wf, wy, vey WY
—BERT(w",, wi, ...w’,))*

(4)

where BERT (wy, w;, wh) and

BERT (wi, wi,...wgz) stand for the CLS

vector representations of real and generated text
respectively.

Discriminator: The goal of the discriminator is
to distinguish between the real and synthetic scan-
paths supplied to it. Similar to the generator, it
requires text representations to distinguish between
real and generated scanpaths. Specifically, the dis-
criminator comprises two blocks of BiLSTMs that
perform sequential modeling over the scanpaths
and BERT embeddings. The outputs of the two
branches are combined and passed to an attention
fusion module with four heads, followed by another
network of BiLSTMs. The hidden states of the last
BiLSTM layer from both forward and backward
directions are concatenated and supplied to a feed-
forward network. A Sigmoid function activates the
output of the feed-forward network. In this manner,
the discriminator classifies the input scanpaths as
either real or fake.

Adversarial Loss: The generator and discrimi-
nator networks are trained in a two-player zero-sum
game fashion. The loss is given by:

La = min max Ee paata(x) log D(a|T, h)|+

(5)

Therefore, the net generator loss becomes:

Ly =L,+L, + Eynpe(s) lL — log D(G(z|T, N))]
(6)

3.2 Dataset

For training the ScanTextGAN model, we use the
CELER dataset (Berzak et al., 2022). It contains
eyetracking data of 365 participants for nearly 28.5
thousand newswire sentences, sourced from the
Wall Street Journal Penn Treebank (Marcinkiewicz,
1994). Each participant in CELER reads 156
newswire sentences. Half of the sentences are
shared across participants, and the rest is unique
to each participant. The maximum sentence length
was set to 100 characters. Participant eyetracking
data were recorded using Eyelink 1000 tracker in a
desktop mount configuration with a sampling rate
of 1000 Hz. The ScanTextGAN model is trained to
approximate the average eye movements of all the
participants who read given sentences. The CELER
dataset was envisioned to enable research on lan-
guage processing and acquisition and to facilitate
interactions between psycholinguistics and natural
language processing. Furthering the goal, we use it
to train our conditional GAN model through which
we show human scanpath approximation capabil-
ities (§4.2). Also, we use it to show improvements
in the performance of NLP tasks (§4.3).

The data consist of tuples of participant ID,
sentence ID, and word ID corresponding to fixation
point and fixation duration. We compute the 99th
percentile of fixation durations and treat it as the
largest value. Fixations of durations longer than
this are treated as outliers and hence dropped from
the dataset. To apply the scanpath reconstruction
loss (Eq. 3), we scale all fixation durations by
the maximum value and then normalize them to
[0,1]. Similarly, word IDs in each sentence are
normalized to [0, 1] after scaling them by the
length of that sentence. For the last fixation point
in every scanpath, the binary EOS token is set to 1.
The maximum scanpath length is set to 80 fixation
points (99th percentile of the lengths). Thus shorter
scanpaths are padded while longer scanpaths are
trimmed. We use BERT to encode the sentences
and obtain their 768-dimensional embeddings,
keeping the max length parameter as 80, thus
resulting in an 80 x 768 dimensional tensor.


3.3 Parameter Settings

Sinusoidal positional encoding is applied over
the input embeddings fed to the generator. We
use a 3-layer transformer encoder with four head
attention and a hidden dimension size of 776
in the generator. In the discriminator, we use
bidirectional LSTMs over sentence embeddings
and generated scanpaths with a hidden size of
64 and a dropout ratio of 0.3, followed by batch
normalization for faster convergence. An attention
module with four attention heads is applied after
concatenating the outputs. We employ the Adam
and RMSProp optimizer to minimize generator
and discriminator losses. The batch size is set to
128, the initial learning rate of the generator to
0.0001, and that of the discriminator to 0.00001.
The model is trained for 300 epochs. Our imple-
mentation uses PyTorch, a popular deep-learning
framework in Python. All experiments are run on
an Intel Xeon CPU with Nvidia A100-SXM GPUs.

4 Performance Evaluation

We quantify the performance of ScanTextGAN in
two regimes!; first, scanpath generation with three
datasets, and second, NLP tasks with six datasets.
Similar to prior computer vision studies (Sun et al.,
2019; de Belen et al., 2022; Kiimmerer and Bethge,
2021; Jiang et al., 2016), we evaluate the ScanT-
extGAN model over the scanpath generation task.
For this, we use the test split of the CELER dataset,
Mishra et al. (2016a), and Mishra et al. (2017).
In addition, unlike the computer vision studies, we
also evaluate the ScanTextGAN model for improve-
ment in NLP tasks. The hypothesis is that the
human eyes (and consequently the brain) process
many language comprehension tasks unconsciously
and without visible effort. The next logical step is
to capture (or, in our case, generate) this mental rep-
resentation of language understanding and use it to
improve our machine-learning systems. For evalua-
tion, we use four tasks from the GLUE benchmark
and two from the tasks proposed by Mishra et al.
(2016a). While the ScanTextGAN model is trained
over news text from the CELER dataset, with the
help of the other datasets, we expand our testing to
other domains, including reviews, quotes, tweets,
and Wikipedia text.

‘Al results are calculated with five random seeds and
reported as the mean of those five runs

4.1 Evaluation Datasets

Mishra et al. (2017) comprises eye movements
and reading difficulty data recorded for 32 para-
graphs on 16 different topics, viz. history, science,
literature, etc. For each topic, comparable para-
graphs were extracted from Wikipedia* and simple
Wikipedia®. The participant’s eye movements are
tracked using an SR-Research Eyelink-1000 Plus
eye tracker. Using the ground truth scanpaths over
the text corpora, we evaluate the quality of gener-
ated scanpaths.

Mishra et al. (2016a) contains eye fixation se-
quences of seven participants for 994 text snippets
annotated for sentiment and sarcasm. These were
taken from Amazon Movie Corpus , Twitter, and
sarcastic quote websites. The task assigned to the
participants was to read one sentence at a time and
annotate it with binary sentiment polarity labels
(i.e., positive/negative). The same datasets were
used in several studies (Joshi et al., 2015; Mishra
et al., 2016b,c) to show improvements in sarcasm
and sentiment analysis. We use the datasets to
evaluate both the generation quality and potential
improvements in NLP tasks.

Furthermore, we explore the potential of includ-
ing cognitive signals contained in scanpaths in NLP
models for a range of GLUE tasks which include
Sentiment Analysis using Stanford Sentiment Tree-
bank (SST), Paraphrase Detection using Microsoft
Research Paraphrase Corpus (MRPC) and Quora
Question Pairs (QQP), Natural Language Infer-
ence using Recognizing Textual Entailment (RTE)
dataset.

Next, we cover the results of scanpath generation
and its application in NLP tasks.

4.2 Evaluation of Scanpath Generation

We evaluate the scanpath generation model on two
most commonly used metrics in image scanpath
generation studies (Sun et al., 2019; Chen and Sun,
2018; de Belen et al., 2022; Kiimmerer et al., 2022):
MultiMatch (Jarodzka et al., 2010) and Leven-
shtein Distance (Levenshtein, 1965). Multimatch
is a geometrical measure that compares scanpaths
across a comprehensive set of dimensions com-

"https: //en.wikipedia.org/

Shttps://simple.wikipedia.org/

“In the CELER dataset, there are only 78 shared sentences
amongst all the participants. Therefore, inter-subject scan-
path evaluation is done only for these sentences. In contrast,
the ScanTextGAN results are reported for the entire test set
(including these 78 sentences).


MultiMatch +

Generator Model Levenshtein Distance |
Vectort Length? Position} Durations
Inter-subject score* 0.973 0.958 0.830 0.698 0.691
LSTM Encoder-Decoder trained with scanpath content loss 0.975 0.956 0.765 0.344 0.865
ScanTextGAN — Text Reconstruction - GAN Loss 0.968 0.947 0.728 0.703 0.779
ScanTextGAN 0.983 0.972 0.787 0.733 0.769
ScanTextGAN — Text Reconstruction 0.974 Q.957 0.773 0.703 0.798
ScanTextGAN — GAN Loss 0.973 0.955 0.750 0.761 0.786
ScanTextGAN + addition of noise 0.971 0.952 0.756 0.736 0.791
ScanTextGAN — Text (CLS) Reconstruction + sentence reconstruction 0.978 0.963 0.724 0.721 0.805

Table 1: In-domain Evaluation of Scanpath Generation on the CELER dataset (Berzak et al., 2022).

REAL REAL

Time (ms)
Time (ms)

Word ID
FAKE

Word ID
FAKE

Time (ms)
Time (ms)

Word ID Word ID

Mr. Baker promises to return
if the haunting continues.

Soon she was running the office.

Figure 4: Comparison of real and synthesized scan-
paths corresponding to a few text samples. The pro-
posed ScanTextGAN model generates the latter.

posed of shape, lengths, position, and fixation du-
ration. Levenshtein Distance between a pair of
sequences measures the least number of edits (in-
serts, deletes, substitution) to transform one into the
other. More details are discussed in Appendix:A.
Further, as a top-line comparison, we use inter-
subject scanpath similarity (Sun et al., 2019). It
measures the degree of variation among real hu-
man scanpaths corresponding to each text input.
To compute this, we first calculate each subject’s
performance by treating the scanpaths of other sub-
jects as the ground truth. Then, the average value
of all subjects is used as inter-subject performance.
Baselines: Since ScanTextGAN is the first text-
based scanpath generation model, we conduct an
ablation study to compare ScanTextGAN with its
other variants. Specifically, we compare ScanT-
extGAN with the following six configurations:
(1) An LSTM-based network trained with scan-
path content loss. Sentence embeddings obtained
through BERT are concatenated with noise in this
model. The resultant is fed to an attention mod-

ule with four heads, then passed to a network of
LSTMs and Batch Normalization layers applied
in tandem. (2) ScanTextGAN model trained with
only the scanpath content loss. (3) ScanTextGAN
model without the text reconstruction loss (Task-2).
(4) ScanTextGAN model with BERT-based sen-
tence embeddings reconstruction instead of CLS
token reconstruction. (5) ScanTextGAN model
with the addition of noise instead of concatena-
tion. (6) ScanTextGAN model trained without
GAN loss.

Results: Table 1 presents the results of our scan-
path prediction model on the CELER dataset. Fur-
ther, we also compare ScanTextGAN with base-
lines on two other contemporary datasets of movie
reviews, tweets, and sarcastic quotes (Mishra et al.,
2016a), Wikipedia and simple Wikipedia para-
graphs (Mishra et al., 2017). Tables 2 and 3 present
the results of our model on those datasets. For ob-
taining results on these corpora, we use the model
trained on the CELER dataset, thus helping us eval-
uate the cross-domain performance of the model.

As can be seen in Table 1, Table 2 and Table 3,
ScanTextGAN outperforms other models for scan-
path prediction on most metrics. The performance
of ScanTextGAN even surpasses inter-subject ref-
erence on Duration and comes very close to Vector,
Length, and Position.

We observe that adopting the reconstruction of
the CLS token as an auxiliary task (Task - 2) boosts
the model performance. Reconstructing the full
sentence embeddings rather than the CLS tokens
only as an auxiliary task does not always improve
the results, despite adding a larger computational
overhead. The results also reveal that concatenating
noise with text embeddings is more rewarding than
adding it.

Further, to compare the skipping behavior of
ScanTextGAN with humans, we calculate the
weighted Fl score of the words skipped and at-


MultiMatch t

Generator Model Levenshtein Distance |
Vector} Length} Position Durationt
Inter-subject score 0.977 0.963 0.839 0.715 0.723
LSTM Encoder-Decoder trained with scanpath content loss 0.984 0.973 0.714 0.379 0.918
ScanTextGAN — Text Reconstruction —- GAN Loss 0.977 0.960 0.780 0.769 0.847
ScanTextGAN 0.966 0.945 0.791 0.771 0.836
ScanTextGAN — Text Reconstruction 0.976 0.961 0.763 0.757 0.845
ScanTextGAN — GAN Loss 0.976 0.959 0.774 0.768 0.839
ScanTextGAN + addition of noise 0.968 0.947 0.737 0.743 0.838
ScanTextGAN — Text (CLS) Reconstruction + sentence reconstruction 0.964 0.934 0.747 0.733 0.869

Table 2: Cross-domain Evaluation of Scanpath Generation on the Dataset by Mishra et al. (2016a).

Generator Model

MultiMatch t

Levenshtein Distance |

Vector} Length} Positiont Duration?

Inter-subject score 0.994 0.991 0.834 0.620 0.845
LSTM Encoder-Decoder trained with scanpath content loss 0.992 0.987 0.596 0.329 0.969
ScanTextGAN — Text Reconstruction - GAN Loss 0.990 0.984 0.729 0.705 0.951
ScanTextGAN 0.984 0.977 0.759 0.693 0.931
ScanTextGAN — Text Reconstruction 0.986 0.981 0.756 0.706 0.939
ScanTextGAN — GAN Loss 0.990 0.984 0.739 0.706 0.945
ScanTextGAN + addition of noise 0.984 0.976 0.759 0.703 0.943
ScanTextGAN — Text (CLS) Reconstruction + sentence reconstruction 0.983 0.974 0.667 0.674 0.958

Table 3: Cross-domain Evaluation of Scanpath Generation on the Dataset by Mishra et al. (2017).

tended by both model types. We find the weighted
F1 to be 64.6 between them. Fig. 4 presents a visual
comparison between real scanpaths from the avail-
able eyetracking data and scanpaths generated by
ScanTextGAN, corresponding to some randomly
chosen text samples. We can observe that the gen-
erated scanpaths resemble the real ones to a great
extent. Thus, the quantitative and qualitative re-
sults on in-domain and cross-domain settings lead
us to believe that our proposed scanpath generation
model can be deemed a good approximator of the
human scanpaths.

4.3 Application to NLP Tasks

We use them to augment various NLP models and
measure their performance to demonstrate the use-
fulness of cognitive signals hidden in the generated
scanpaths.

Sentiment Classification and Sarcasm Detec-
tion: For these tasks, we use a model consisting
of a network of two branches of BiLSTMs and
Batch Normalization layers that perform sequen-
tial modeling over text representations obtained
through BERT and scanpaths fed as input to the
model. The outputs of both branches are combined
and passed to another layer of BiLSTMs, followed
by a feed-forward network that predicts binary sen-
timent/sarcasm labels corresponding to the input
after activating with the Sigmoid function. We
follow a 10-fold cross-validation regime.

We compare the models with generated scan-

Model Configuration F1 score
Train Test Sentiment Sarcasm
w/o w/o 0.7839 0.9438
Random Random 0.7990 0.9397
Random Generated 0.7773 0.9313
Real Generated 0.8319 0.9378
Real Real 0.8334 0.9501
Generated Real 0.8402 0.9452
Generated Generated 0.8332 0.9506
Real + Generated Generated 0.8404 0.9512
Intent-Aware Intent-Aware 0.8477 0.9528

Table 4: Sentiment analysis and sarcasm detection re-
sults on the dataset by Mishra et al. (2016a). Model
configuration refers to the type of scanpath included in
train and test data.

paths, real scanpaths, and without scanpaths. Fur-
ther, to investigate whether performance gains ob-
served by adding scanpaths are due to scanpaths
and not the increase in the number of parameters,
we train a Random-Random variant in which we
send Random noise as scanpaths to the model with
an increased number of parameters. We also simu-
late the real-world case where both real and gener-
ated scanpaths are available during train time, but
only generated ones are available during test time,
for example, during user deployment.

Table 4 records the results of sentiment analysis
and sarcasm detection tasks (Mishra et al., 2016a).
We note that generated scanpaths training and test-
ing lead to similar gains for sentiment analysis and
sarcasm detection as real scanpaths. The model
with an increased number of parameters fed ran-


Dataset Model Acc Fl score
Sst w/o scanpaths 0.8090 0.8089
w/ random scanpaths 0.8059 0.8061
w/ generated scanpaths 0.8138 0.8138
w/ intent-aware scanpaths 0.8269 0.8272
w/o scanpaths 0.6902 0.6656
MRPC w/ random scanpaths 0.6623 0.6680
w/ generated scanpaths 0.6969 0.6828
w/ intent-aware scanpaths 0.7009 0.6911
RTE w/o scanpaths 0.6162 0.6080
w/ random scanpaths 0.5802 0.5794
w/ generated scanpaths 0.6211 0.6205
w/ intent-aware scanpaths 0.6293 0.6278
QQP w/o scanpaths 0.8499 = 0.8513
w/ random scanpaths 0.8491 0.8503
w/ generated scanpaths 0.8578 0.8596
w/ intent-aware scanpaths 0.8648 0.8658

Table 5: Results of training NLP models with and with-
out scanpaths on the GLUE benchmark tasks. Includ-
ing scanpaths leads to consistent improvements across
all the NLP tasks.

dom noise in place of scanpaths performs similarly
to the model trained without any scanpaths. Inter-
estingly, the best results are obtained when model
training uses both real and generated scanpaths.
We believe this is due to ScanTextGAN bringing
additional cognitive information from the news-
reading CELER corpus, which is not present in the
real scanpaths in Mishra et al. (2016a). In addition
to the intrinsic evaluation presented in §4.2, this
downstream evaluation demonstrates the high qual-
ity of the synthesized scanpaths, showing that they
contain valuable cognitive processing signals for
NLP tasks.

GLUE Tasks: To validate further, we augment
classification models (based on sequential mod-
eling using LSTMs) with generated scanpaths to
show performance improvement in downstream
NLP tasks on four GLUE benchmark datasets —
SST, MRPC, RTE, QQP as described in §4.1. Ta-
ble 5 reports the accuracy and weighted-F1 scores
of the models trained with and without scanpaths
for these tasks. We observe that in all four tasks,
the model trained with generated scanpaths outper-
forms the one without scanpaths.

Intent-Aware Scanpaths: Finally, we try to
condition scanpaths generation on the downstream
natural language task. We back-propagate gradi-
ents from the downstream NLP task to the condi-
tional generator. In this fashion, the model learns to
generate intent-aware scanpaths. The hypothesis is
that finetuning scanpath generation based on feed-

back from the natural language task will bias the
generator towards words more pertinent to that task
and thus could help further improve performance
on the downstream task. The architecture is shown
in Appendix: Fig 5. The results in Tables 4 and
5 validate the hypothesis that we observe consis-
tent improvements in all downstream tasks. Fig 2
and Appendix: Fig 6 show a few examples of scan-
paths and saliency generated for three downstream
natural language tasks.

Together these results corroborate the hypothesis
that leveraging the cognitive signals approximated
by synthetic scanpaths in NLP models leads to
performance gains.

5 Conclusion

In this work, we make two novel contributions to-
ward integrating cognitive and natural language
processing. (1) We introduce the first scanpath
generation model over text, integrating a cogni-
tive reading model with a data-driven approach to
address the scarcity of human gaze data on text.
(2) We propose generated scanpaths that can be
flexibly adapted to different NLP tasks without
needing task-specific ground truth human gaze data.
We show that both advances significantly improve
performance across six NLP datasets over various
baselines. Our findings demonstrate the feasibility
and significant potential of combining cognitive
and data-driven models for NLP tasks. Without the
need for real-time gaze recordings, the potential
research avenues for augmenting and understand-
ing NLP models through the cognitive processing
information encoded in synthesized scanpaths are
multiplied.

6 Limitations

In this work, we demonstrated artificial scanpath
generation over multiple eye-tracking datasets. Fur-
ther, our experiments build a link between cogni-
tive and natural language processing and show how
one can inform the other. However, the proposed
method has a few limitations, which we aim to
address in the future. The field needs work on big-
ger and more diverse eye-tracking datasets, which
can enable scanpath generation over longer text se-
quences and can model generating scanpaths condi-
tioned on previously read context. Besides, a better
understanding of the entire scanpath generation
process can help model the intra and inter-sentence
scanpath generation process. The understanding


would enable the integration of scanpaths to gener-
ative modeling tasks, which we intend to take up
in future work. Another parallel direction is to in-
clude both explicit (like using RLHF) and implicit
signals (like using cognitive signals) to better NLP
tasks like language modeling.

References

Marc Assens, Xavier Giro-i Nieto, Kevin McGuin-
ness, and Noel E. O’Connor. 2019. Pathgan: Vi-
sual scanpath prediction with generative adversarial
networks. In Computer Vision —- ECCV 2018 Work-
shops. Springer International Publishing.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862.

Maria Barrett, Joachim Bingel, Nora Hollenstein,
Marek Rei, and Anders S¢gaard. 2018a. Sequence
classification with human attention. In Proceedings
of the 22nd Conference on Computational Natural
Language Learning, pages 302-312, Brussels, Bel-
gium. Association for Computational Linguistics.

Maria Barrett, Joachim Bingel, Frank Keller, and An-
ders Sggaard. 2016a. Weakly supervised part-of-
speech tagging using eye-tracking data. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 579-584, Berlin, Germany. Association
for Computational Linguistics.

Maria Barrett, Ana Valeria Gonzaélez-Gardufio, Lea Fr-
ermann, and Anders Sggaard. 2018b. Unsupervised
induction of linguistic categories with records of
reading, speaking, and writing. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 2028-2038, New Orleans, Louisiana.
Association for Computational Linguistics.

Maria Barrett, Frank Keller, and Anders Sggaard.
2016b. Cross-lingual transfer of correlations be-
tween parts of speech and gaze features. In Proceed-
ings of COLING 2016, the 26th International Con-
ference on Computational Linguistics: Technical Pa-
pers, pages 1330-1339, Osaka, Japan. The COLING
2016 Organizing Committee.

Yevgeni Berzak, Chie Nakamura, Amelia Smith, Emily
Weng, Boris Katz, Suzanne Flynn, and Roger Levy.
2022. Celer: A 365-participant corpus of eye move-
ments in 11 and 12 english reading. Open Mind.

Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang
Yu, and Ngoc Thang Vu. 2018. Comparing attention-
based convolutional and recurrent neural networks:

Success and limitations in machine reading compre-
hension. In Proceedings of the 22nd Conference on
Computational Natural Language Learning, pages
108-118, Brussels, Belgium. Association for Com-
putational Linguistics.

Aidan Boyd, Kevin W Bowyer, and Adam Czajka.
2022. Human-aided saliency maps improve gener-
alization of deep learning. In Proceedings of the
IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 2735-2744.

Zhenzhong Chen and Wanjie Sun. 2018. Scanpath pre-
diction for visual attention using ior-roi stm. In JJ-
CAL.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep re-
inforcement learning from human preferences. Ad-
vances in neural information processing systems, 30.

Charles Clifton Jr, Adrian Staub, and Keith Rayner.
2007. Eye movements in reading words and sen-
tences. Eye movements.

Filipe Cristino, Sebastiaan Mathét, Jan Theeuwes, and
Tain D Gilchrist. 2010. Scanmatch: A novel method
for comparing fixation sequences. Behavior re-
search methods.

Ryan Anthony Jalova de Belen, Tomasz Bednarz, and
Arcot Sowmya. 2022. Scanpathnet: A recurrent
mixture density network for scanpath prediction. In
IEEE CVPR.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Ralf Engbert, Antje Nuthmann, Eike M Richter, and
Reinhold Kliegl. 2005. Swift: a dynamical model of
saccade generation during reading. Psychological
review.

Michael Hahn and Frank Keller. 2016. Modeling hu-
man reading with neural attention. In Proceedings
of the 2016 Conference on Empirical Methods in
Natural Language Processing, pages 85-95, Austin,
Texas. Association for Computational Linguistics.

Sen He, Hamed R Tavakoli, Ali Borji, and Nicolas
Pugeault. 2019. Human attention in image caption-
ing: Dataset and analysis. In ICCV.


Nora Hollenstein, Maria Barrett, Marius Troendle,
Francesco Bigiolli, Nicolas Langer, and Ce Zhang.
2019. Advancing nlp with cognitive language pro-
cessing signals. arXiv preprint arXiv: 1904.02682.

Nora Hollenstein, Emmanuele Chersoni, Cassandra L.
Jacobs, Yohei Oseki, Laurent Prévot, and Enrico
Santus. 2021a. CMCL 2021 shared task on eye-
tracking prediction. In Proceedings of the Workshop
on Cognitive Modeling and Computational Linguis-
tics, pages 72-78, Online. Association for Computa-
tional Linguistics.

Nora Hollenstein, Federico Pirovano, Ce Zhang, Lena
Jager, and Lisa Beinborn. 2021b. Multilingual lan-
guage models predict human reading behavior. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 106-123, Online. Association for Computa-
tional Linguistics.

Nora Hollenstein and Ce Zhang. 2019. Entity recog-
nition at first sight: Improving NER with eye move-
ment information. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume I (Long and Short Pa-
pers), pages 1-10, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Halszka Jarodzka, Kenneth Holmqvist, and Marcus
Nystrom. 2010. A vector-based, multidimensional
scanpath similarity measure. In Proceedings of the
2010 symposium on eye-tracking research & appli-
cations.

Ming Jiang, Xavier Boix, Gemma Roig, Juan Xu, Luc
Van Gool, and Qi Zhao. 2016. Learning to predict
sequences of human visual fixations. [EEE transac-
tions on neural networks and learning systems.

Aditya Joshi, Vinita Sharma, and Pushpak Bhat-
tacharyya. 2015. Harnessing context incongruity for
sarcasm detection. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 757-762, Beijing, China. As-
sociation for Computational Linguistics.

Marcel Just and Patricia A. Carpenter. 1980. A theory
of reading: From eye fixations to comprehension.

Nour Karessli, Zeynep Akata, Bernt Schiele, and An-
dreas Bulling. 2017. Gaze embeddings for zero-shot
image classification. In IEEE CVPR.

Sigrid Klerke, Yoav Goldberg, and Anders Sggaard.
2016a. Improving sentence compression by learn-
ing to predict gaze. In NAACL: Human Language
Technologies, San Diego, California. Association for
Computational Linguistics.

Sigrid Klerke, Yoav Goldberg, and Anders S¢gaard.
2016b. Improving sentence compression by learn-
ing to predict gaze. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1528-1533, San Diego,
California. Association for Computational Linguis-
tics.

Iuliia Kotseruba and John K Tsotsos. 2020. 40 years
of cognitive architectures: core cognitive abilities
and practical applications. Artificial Intelligence Re-
view.

Matthias Kiimmerer and Matthias Bethge. 2021. State-
of-the-art in human scanpath prediction. arXiv
preprint arXiv:2102.12239.

Matthias Kiimmerer, Matthias Bethge, and Thomas SA
Wallis. 2022. Deepgaze iii: Modeling free-viewing
human scanpaths with deep learning. Journal of Vi-
sion.

V Levenshtein. 1965. Leveinshtein distance.

Yunfei Long, Qin Lu, Rong Xiang, Minglei Li, and
Chu-Ren Huang. 2017. A cognition based attention
model for sentiment analysis. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 462-471, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Mary Ann Marcinkiewicz. 1994. Building a large an-
notated corpus of english: The penn treebank. Using
Large Corpora.

Daniel Martin, Ana Serrano, Alexander W. Bergman,
Gordon Wetzstein, and Belen Masia. 2022. Scan-
gan360: A generative model of realistic scanpaths
for 360° images. IEEE Transactions on Visualiza-
tion and Computer Graphics.

Sandeep Mathias, Diptesh Kanojia, Abhijit Mishra,
and Pushpak Bhattacharya. 2020. A survey on us-
ing gaze behaviour for natural language processing.
In IJCAT. Survey track.

Franz Matthies and Anders Sggaard. 2013. With blink-
ers on: Robust prediction of eye movements across
readers. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 803-807, Seattle, Washington, USA. Associa-
tion for Computational Linguistics.

Abhijit Mishra, Diptesh Kanojia, and Pushpak Bhat-
tacharyya. 2016a. Predicting readers’ sarcasm un-
derstandability by modeling gaze behavior. In AAAI.

Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kun-
tal Dey, and Pushpak Bhattacharyya. 2016b. Har-
nessing cognitive features for sarcasm detection. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1095-1104, Berlin, Germany.
Association for Computational Linguistics.


Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal
Dey, and Pushpak Bhattacharyya. 2016c. Leverag-
ing cognitive features for sentiment analysis. In Pro-
ceedings of the 20th SIGNLL Conference on Com-
putational Natural Language Learning, pages 156—
166, Berlin, Germany. Association for Computa-
tional Linguistics.

Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal
Dey, and Pushpak Bhattacharyya. 2017. Scanpath
complexity: Modeling reading effort using gaze in-
formation. AAAI.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. ‘Training language models to follow in-
structions with human feedback. arXiv preprint
arXiv:2203.02155.

Barbara Plank. 2016. Keystroke dynamics as signal for
shallow syntactic parsing. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers, pages
609-619, Osaka, Japan. The COLING 2016 Orga-
nizing Committee.

David Robert Reich, Paul Prasse, Chiara Tschirner,
Patrick Haller, Frank Goldhammer, and Lena A.
Jager. 2022. Inferring native and non-native human
reading comprehension and subjective text difficulty
from scanpaths in reading. In ETRA. Association for
Computing Machinery.

Yuqi Ren and Deyi Xiong. 2021. CogAlign: Learning
to align textual neural representations to cognitive
language processing signals. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 3758-3769, Online. As-
sociation for Computational Linguistics.

Ekta Sood, Simon Tannert, Philipp Miiller, and An-
dreas Bulling. 2020. Improving natural language
processing tasks with human gaze-guided neural at-
tention. NeurIPS.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems, 33:3008—
3021.

Michalina Strzyz, David Vilares, and Carlos Gémez-
Rodriguez. 2019. Towards making a dependency
parser see. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),
pages 1500-1506, Hong Kong, China. Association
for Computational Linguistics.

Wanjie Sun, Zhenzhong Chen, and Feng Wu. 2019. Vi-
sual scanpath prediction using ior-roi recurrent mix-
ture density network. JEEE transactions on pat-
tern analysis and machine intelligence, 43(6):2101-
2118.

Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman. 2018.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Pro-
ceedings of the 2018 EMNLP Workshop Black-
boxNLP: Analyzing and Interpreting Neural Net-
works for NLP, pages 353-355, Brussels, Belgium.
Association for Computational Linguistics.

Shaonan Wang, Jiajun Zhang, and Chengqing Zong.
2017. Learning sentence representation with guid-
ance of human attention. In JJCAI. AAAI Press.

Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing
Shen, Haibin Ling, and Ruigang Yang. 2021.
Salient object detection in the deep learning era: An
in-depth survey. [EEE T-PAMI.

Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Sti-
ennon, Ryan Lowe, Jan Leike, and Paul Christiano.
2021. Recursively summarizing books with human
feedback. arXiv preprint arXiv:2109. 10862.

Menglin Xia, Ekaterina Kochmar, and Ted Briscoe.
2019. Automatic learner summary assessment for
reading comprehension. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume I (Long and Short
Papers), pages 2532-2542, Minneapolis, Minnesota.
Association for Computational Linguistics.

Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung
Yoo, Sang-Hun Lee, and Gunhee Kim. 2017. Super-
vising neural attention models for video captioning
by human gaze data. In IEEE CVPR.


A Scanpath Evaluation Metrics

MultiMatch is a geometrical measure that models
scanpaths as vectors in 2-D space, wherein
the vectors represent saccadic eye movements.
Starting and ending coordinates of these saccades
constitute the fixation positions. It compares
scanpaths across multiple dimensions, viz. shape,
length, position, direction, and fixation duration.
Shape measures the vector difference between
aligned saccade pairs, which is then normalized by
twice the diagonal screen size. Length measures
the normalized difference between the endpoints
of real and generated saccade vectors. Direction
is the angular distance between the two vectors.
The position is the Euclidean difference in position
between aligned vectors, and duration measures
the difference in fixation durations normalized
against the maximum duration. Since our work
deals with scanpaths over text, we use 1-D space
to represent the saccade vectors where word IDs
denote the fixation positions. Thus, it is easy to
see that computing scanpath direction similarity
is redundant here (it is subsumed within position);
hence we drop it from our analysis.

Levenshtein Distance between a pair of se-
quences measures the least number of character ed-
its, 1.e., insertion, deletion, and substitution needed
to transform one sequence into the other. Specifi-
cally, we use it to gauge the degree of dissimilarity
between a pair of real R and generated G scan-
paths. To account for the fixation durations of each
word, R and G' are temporally binned using a 50 ms
bin size, similar to the computation of ScanMatch
metric (Cristino et al., 2010). The resulting se-
quences of word IDs, Ry and G'w are transformed
into character strings, Rg = {r1,72,..-,%} and
Gg = {91,92;+-:9m}, where Rg and Gg are
strings over the ASCII alphabet and n = |Ro|
and m = |Go|.

Levenshtein Distance (LD) between strings Rg
and G's is computed and then normalized by the
length of the longer string, which yields a Normal-
ized Levenshtein Distance (NLD) score, as given
below:

LD(Gs, Rs)

NID=
max(|Rs|,|Gs|)

(7)

Thus, a lower NLD score is indicative of greater
scanpath similarity.

B_Intent-Aware Scanpaths

As described in section §4.3, the generator condi-
tioned on the downstream natural language task
yields intent-aware scanpaths. Augmenting NLP
models with these scanpaths leads to higher per-
formance gains. Here, we provide more details on
intent-aware scanpath generation. Please refer to
figures 5 and 6 on the following page. Saliency
corresponding to intent-aware scanpaths are shown
in Fig. 6.


Investors seem bullish about

the stock market.

SENTENCE |

: Dense Text : : ’ ;
: Representations ' ' 6 a . Dense a '
: BERT) : REAL ' canpa ‘epresentations :
‘ { : ' Eee A (BERT) '
: Positional 4 ‘ : BiLSTM BiLSTM '
‘ Encoding » ' 1 tot a + '
: i : Ho BatchNorm BatchNorm '
! H : Investors seem bullish about the stock market! ' :
: Transformer Encoder ' ‘ REAL SCANPATH tot '
i ot : ; - Multi-head :
: ' : io Attention Fusion :

Eye tracking data recorded

t
2

BiLSTM

Sequence Length
Reconstructed .
CLS Token | Word ID | Duration

TASK - 2:

! Text Reconstruction TASK - 1: Reconstruction Loss :
ee eee Gil y TASK - 4: '
NLP Models —— Real / Fake Adversarial :
: t . q i Zero Sum Game |
Sentiment )| |e anpatir fu ae. : Human reading text it
Analysis H ‘4 :
Sarcasm [<— Text Investors seem  bullsh about the stock market.
peaaeD NLP Models

augmented with GENERATED SCANPATH
TASK - 3: scanpaths
| Intent
Improved Prediction
performance

Figure 5: The architecture of the proposed Intent-Aware ScanTextGAN model. The model consists of a condi-
tional generator and a discriminator playing a zero-sum game. Two cognitively inspired losses train the generator:
scanpath (Task-1) and text (Task-2) reconstruction, a loss from the downstream intent of the natural language task
(Task-3), and finally, the loss from the adversarial zero-sum game (Task-4). Variations of scanpaths are generated
based on the downstream natural language task.

The company also earned 14 cents a share a year earlier. The company also earned 14 cents a share a _ year earlier.

a. Reading Saliency

A year ago, the company posted a profit of 12 cents a share. A year ago, the company posted a profit of 12 cents a_ share.

a. Reading Saliency b. Natural Language Inference Saliency
There were conflicting reports about the number of casualties yesterday. There were conflicting reports about the number of casualties yesterday.

a. Reading Saliency b. Sentiment Saliency

b. Paraphrase Saliency

Figure 6: Saliency samples generated by conditioning scanpath generation on different downstream natural lan-
guage tasks. It can be observed that the conditioned saliency pays much more attention to words important for that
downstream task.
