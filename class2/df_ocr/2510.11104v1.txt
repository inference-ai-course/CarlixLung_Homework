arXiv:2510.11104v1 [cs.CL] 13 Oct 2025

Enhancing LLM Reasoning via Non-Human-Like Reasoning Path
Preference Optimization

Junjie Lu'*, Yuliang Liu”**, Chaofeng Qu°*, Wei Shen®, Zhouhan Lin”*, Min Xu'*

"University of Technology Sydney, 7Shanghai Innovation Institute, >Southeast University,
*Nanjing University, °Shanghai Jiao Tong University, “Independent Researcher
{Junjie.Lu,Min.Xu}@uts.edu.au, liuyl03181@gmail.com

Abstract

Current approaches for strengthening LLM rea-
soning tend to introduce a training bias to-
ward human-like reasoning trajectories. In
step-wise preference optimization, in particu-
lar, dependence on human or higher-capacity
model annotations for intermediate steps lim-
its exploration of alternative, non-human-like
reasoning paths and thus constrains achievable
performance. Furthermore, through a small-
scale pilot study, we observed that in approx-
imately 75% of cases, the model’s first erro-
neous step occurs after the lowest-confidence
point. This suggests that guiding the model at
its lowest-confidence point before an error pro-
vides more accurate supervision than locating
the first explicit error. In this paper, we propose
Confidence-Guided Reasoning Path Preference
Optimization (CGPO), a method that leverages
a confidence signal to identify points of maxi-
mal uncertainty in the model’s reasoning pro-
cess and applies self-generated, non-human-
like reasoning-path guidance to mitigate trajec-
tory drift. Our experiments span diverse models
applied to both code and mathematical reason-
ing tasks. The results show that, with the same
amount of training data, our method using data
generated by a small model can achieve bet-
ter performance in most cases compared with
approaches using data generated by a strong
model or human-annotated.

1 Introduction

Large language models (LLMs) have demonstrated
remarkable capabilities across various domains,
stemming from their ability to reason (Zelikman
et al., 2022; Hao et al., 2023; Lewkowycz et al.,
2022; Magister et al., 2022; Khalifa et al., 2025).
To further enhance their reasoning capabilities, re-
searchers have conducted extensive studies on both
the training and inference phases of the models.

* Equal contribution.
" Corresponding author

Training methods typically focus on teaching the
model to follow human instruction through super-
vised training (Yue et al., 2023; Luo et al., 2023),
or on preference optimization of reasoning chains
using reward models or human feedback, usually
known as RLHF (Guo et al., 2025; Zhang et al.,
2024a; Le et al., 2022). For inference methods,
researchers often employ prompt tuning (Kojima
et al., 2022; Imani et al., 2023), or intervene in
the model’s generation process by assessing consis-
tency (Wang et al., 2022; Chen et al., 2022; Tauben-
feld et al., 2025a), predicting confidence (Weng
et al., 2022; Zhu et al., 2025b), and leveraging ex-
pected outcomes (Madaan et al., 2023; Cheng et al.,
2024).

While chain-of-thought (CoT) (Wei et al., 2023)
has become the standard approach for addressing
complex reasoning tasks, early training methods
primarily focused on outcome optimization, with
limited attention to the structure and quality of rea-
soning paths (Shojaee et al., 2025). One possible
is that outcome-based approaches have yielded rea-
sonably satisfactory performance on current bench-
marks (Gao et al., 2024b; Hosseini et al., 2024),
while another is that assigning fine-grained rewards
remains costly. Nevertheless, there have been ef-
forts to optimize finer-grained aspects of the rea-
soning process, like step-wise methods (Snell et al.,
2024; Wang et al., 2024b) and token-wise meth-
ods (Zhang et al., 2024b; Lin et al., 2024).

Existing step-wise preference optimization ap-
proaches usually locate the model’s first error (Luo
et al., 2024; Lu et al., 2024) and construct prefer-
ence pairs accordingly, enabling focused and tar-
geted optimization. However, these methods often
involve substantial labeling overhead, posing a bot-
tleneck in their practical applicability. Furthermore,
our analysis of several incorrect answers in mathe-
matical reasoning tasks reveals that models often
exhibit confusion, shown by relatively low confi-
dence in generation, well before about 75% of the


p Initial reasoning steps

| Let's think step by step.

| Step t:

| First, we multiply the first two

| numbers together: 12.5 * 4.2 = 52.5.
ee eee

( We multiply this result by the third |
number: 52.5 * 8.8 = 461.25 I

ge oe ee 1
Sample until HUMAN decides to stop
_ Chosen —_-_ =e
f Then, we multiply the result by the
third number: 52.5 * 8.8 = 462.

I
i ns J

Query:
12.5*42%*88=?

. Rejected _____________
! We multiply this result by the third
1 number: 52.5 * 8.8 = 461.25. :
ee eee eee ee Ee EE a ee
HUMAN-LIKE :

Dividing by human thinking

Seca eb ei gs Secctos ci al a

Let's think step by step. Multiplying
the numbers together, we get 12.5 *
4.2* 88=525* 88 =

‘Multiplying the numbers together, |
| we get 12.5*42*88=525*88= |

Sample until MODEL decides to stop
Chosen

eo ey

OWMOedasOning™| *~~——--—-~-—-=----<------=--= °
and sampling?

_ Rejected

| 480.48. Rounding to the nearest
| whole number, the answer

NON-HUMAN-LIKE :
Dividing by model confidence

Figure 1: The difference between human-like reasoning and non-human-like reasoning. Human-like methods
emulate human cognitive processes and typically use predefined, readable anchors as delimiters to split the
reasoning path. Nevertheless, it imposes substantial constraints on the content and format of data during sampling.
In contrast, in this paper, non-human-like delineate reasoning steps are based on model confidence and provide
reward supervision for tokens where the model is most uncertain, incurring lower cost and exhibiting broader

applicability across various tasks.

actual errors occur. Examples are provided in Ap-
pendix A.1. This observation inspired us to develop
a method capable of identifying issues earlier in a
fully automated, human-free manner.

We propose Confidence-Guided Reasoning Path
Preference Optimization (CGPO), which anchors
preference optimization to model uncertainty rather
than human alignment or process-error identifica-
tion. Concretely, CGPO is grounded in two key
principles: (1) enabling the model to identify and
refine its own confusion, and (2) leveraging pref-
erence pair construction to guide optimization. By
preference optimization, our approach introduces a
thoroughly non-human-like reasoning path explo-
ration, as shown in Figure 1.

Our experiments show that CGPO yields con-
sistent gains on mathematical reasoning under the
same sample counts. The MetaMath-llama model
improves by 4.15% on GSM8K (Cobbe et al., 2021)
and by 2.54% on MATH (Hendrycks et al., 2021)
compared to the baseline. Importantly, our method
uses the training data generated by the policy model
itself, whereas the baseline uses the training data
generated by stronger models or annotated by hu-
mans. Models trained with CGPO further demon-
strate better generalization on Omni-Math. For the
code generation task, we train the DeepSeek-Coder-

Instruct-7B by using CGPO and observe a 2.1%
improvement on LiveCodeBench (Jain et al., 2024)
and 4.0% improvement on LeetCodeDataset (Xia
et al., 2025). We also examine the scalability of our
method by increasing the training data from 10k to
80k, which results in the data leading to continued
improvements.

Our ablation studies and analyses further investi-
gate the differences in effects between human-like
and non-human-like splits. Moreover, our experi-
ments reveal that, within a certain range, interme-
diate reasoning path-focused optimization leads to
more substantial enhancement than optimizing the
entire reasoning path.

Our main contributions are as follows:

1. We introduce CGPO, a method that automati-
cally constructs preference data and performs
reasoning path preference optimization for
LLMs without relying on more powerful mod-
els or human supervision. CGPO can sample
key non-human-like reasoning paths from the
model to construct preference-learning data,
therefore improving the model’s reasoning
ability.

2. We demonstrate on several mathematical rea-
soning benchmarks that CGPO improves mod-
els’ reasoning ability. We also apply CGPO to


code generation, showing that CGPO gener-
alizes to broader domains where human-like
reasoning paths are hard to define.

3. We build a simple and efficient training code-
base for reproducing CGPO, together with
multiple datasets of non-human-like reason-
ing paths across different models and the cor-
responding models trained on these data. We
will release all code, datasets, and trained
models to facilitate reproducibility and com-
parison upon acceptance of this paper.

2 Related Works

Preference Optimization for LLM: Preference
optimization methods have been investigated to
improve LLM performance on complex tasks.
PPO (Schulman et al., 2017) balances reward maxi-
mization with stability through a clipped objective,
serving as a foundational method in RLHF but with
considerable complexity. Subsequently, critic-free
RL methods (Shao et al., 2024; Ahmadian et al.,
2024; Hu et al., 2025) reduced complexity by us-
ing reward model outputs as immediate rewards
for policy gradient updates. And methods like
DPO (Rafailov et al., 2024) and SimPO (Meng
et al., 2024) directly optimize the model on pair-
wise preference signals. The reduced computa-
tional overhead, combined with sufficiently strong
performance, has enabled preference-based meth-
ods to achieve widespread success across a variety
of domains like instruction following (Wu et al.,
2024; Cheng et al., 2025), mathematical reason-
ing (Pang et al., 2024), and code generation (Zhang
et al., 2025; Zhu et al., 2025a).
Confidence-Aware Methods: Model confi-
dence, which is commonly used to quantify model
uncertainty, is widely used in LLMs (Fu et al.,
2025; Taubenfeld et al., 2025b; Kang et al., 2025;
Li et al., 2025; Singhi et al., 2025). While confi-
dence can be interpreted from multiple perspectives.
Some approaches measure confidence directly us-
ing the predicted probability of the selected to-
ken (Farquhar et al., 2024; Liu et al., 2025), which
is also the approach adopted in our work. Others
rely on the model’s outputs. Xiong et al. (2024)
proposed a systematic framework to express model
confidence by generation, Yona et al. (2024) mea-
sures model confidence by consistency of answers.
There are also several methods using the entropy
of probability distribution or probe-based methods

to measure model confidence in other tasks (Stolfo
et al., 2024; Beigi et al., 2024).

Reasoning Trajectory Optimization: LLM rea-
soning is viewed as a foundational capability for im-
proving performance on complex tasks, and numer-
ous studies have investigated how to improve this
ability (OpenAI et al., 2024; Yang et al., 2025): Su-
pervised fine-tuning methods directly align model
trajectory to annotated sequence (Yu et al., 2025a);
RLHF methods typically force the model to op-
timize the reasoning trajectory towards a higher
score (Yu et al., 2025b); and test time methods
intervene in the generation process to optimize
reasoning trajectories (Muennighoff et al., 2025).
The aforementioned training approaches, especially
RLHF, are generally concerned with outcome opti-
mization and do not explicitly enforce correctness
along the reasoning trajectories. Lai et al. (2024) di-
rectly optimizes an independent reasoning path, the
nearest one of our method, while with a significant
annotation challenge.

3 Methods

In this section, we first introduce the basic formu-
lation, then describe the construction of the CGPO
training data, as illustrated in Figure 2. Finally, we
introduce the training objective of CGPO.

3.1 Preliminaries

Given an input problem x, an initial policy model
7 can generate a reasoning sequence under the
temperature-based sample strategy. We have y =
(yi,Y2,---,¥T); Yt € VY, where V denotes the
vocabulary. Notably, the sampled token in the gen-
eration may not be the token with the top-1 proba-
bility. So that at decoding time step t, we define the
model confidence c in decoding by the sampled
token yz, as

A
ce = p(yslto, X, Yet)

3.2 Preference Pairs Construction

Step Definition. To construct non-human-like step-
wise chosen/rejected preference pairs, we first need
to clarify the definition of a step in our method.
Given x and the corresponding generated y, we
can get the confidence sequence for y, denoted by
c = (c1,¢2,...,er7). A confidence threshold is
employed to split the reasoning steps, with 7 deter-
mined based on the distribution of all confidence
values c across the dataset. The tokens with con-


CGPO Data Construction Pipeline

Step 1: Step 2:

Locate lowest confidence token Select best and worst token

" ay
pocereeees Se

| Generate : '

1 response for 1 (els Hy

| each query ! Pm

ere ! SS rf
Besser 1
ns T 1 i

i 1 ' Get reward for! '
OU ‘O) O oe) OU teach candidate | i

| token

Initial reasoning
steps
(“t=best token

{1=lowest confidence token (t=worst token

Step 3: Step 4:
Sample preference data pairs Build dataset and train
(>) (— Chosen Rejected
oe U CIOL: Chosen
AC err pe i) - Ti
{Decoding to first! l :
\low confidence! LLM _ Rejected ===>
itoken below i n Maximum
eS ee ae likelihood
(> fo | Construct data 1
(al | }{ hi I | } ) 1 pairs and train !
Z 5 utaskmodel___

“| =first low confidence token T =low confidence token threshold

Figure 2: CGPO overview. The core idea of our method fails in two parts: 1) explore the non-human-like reasoning
path by determining the model reasoning step using the initial policy model 79, and 2) construct chosen/rejected

pairs to train 7 with step-wise DPO training objective.

fidence below 7 will be the split tokens. Then we
get y = Ss = (51, 52,..., 87), where s; denotes a
reasoning step that consists of several tokens.

Initial Reasoning Trajectory Determination.
To align with the pair-wise preference optimization
paradigm, based on the step definition, we choose
the sequence before the most uncertain step of s to
serve as the shared initial reasoning trajectory sjni¢
of the chosen/rejected pairs.

Chosen/Rejected Pairs Construction. We in-
troduce a reward model R to select the first token
of the chosen/rejected pairs given (x, Siniz). The
Top-k candidate tokens, ordered by their logits, are
denoted as (v1, v2,..., Ux). The reward for each
token is obtained as R(x, Sinit, v,). Then, we se-
lect the token with the highest score and the one
with the lowest score as the starting tokens of the
chosen/rejected pairs, denoted by yt and y~ re-
spectively.

Subsequently, 7 will sample the following to-
kens of (x, Sinit, yt) and (x, Sinit, y” ) before end-
of-sequence tag <EOS> or the first token with con-
fidence below T.

Finally, for each given input, we have two rea-
soning trajectories: the chosen one (Snitz, 5*,-.. )
and the rejected one (Sinit, 5” ,---).

3.3 Training Objective

Our goal is to optimize 7g based on the triplets
(Sinit, 8', 8). Let D denote the collection of such
triples. With a fixed reference model 7;e¢ and tem-
perature 2 > 0, we optimize 79 using the DPO-like
objective

E(sinitss* 8-)~D [log o(8 (A))|
_ 1

where (3 is the inverse temperature and

A= AY) _ Avot
Ap = log 7(s* | Sinit) — log 7(s~ | Sinit),
Avet — log Tret(sT | Sanit ) _ log Tret(S” | Size.)

Sequence log-likelihoods use natural logs and
decompose autoregressively as

seq

log 1(s~ | Sinit) = S “log mye | x; Sinit),
t

where the sum starts at the branching index y*.
This objective increases the likelihood of the R-
selected high-quality trajectory st while decreas-
ing that of s~; the reference term Ayer stabilizes
the relative preference signal. We use Algorithm 1
to demonstrate the entire pipeline of CGPO.

4 Experiments and Analysis

In this section, we first introduce our experimental
setup, including datasets, models, baselines, met-
rics, and parameter settings. We then present the
experimental results, followed by an analysis of the
CGPO transferability, generalization, and scalabil-

ity.


Algorithm 1 CGPO Pipeline

Require: Dataset 1, 70, R, temp, 7, k, 8
Ensure: Trained policy 79

1: Initialize 7» < 70, preference dataset D < 0)

2: for each input x € ¥ do

3: Sample y from 7; record confidences
ce = To(Yt |X, ¥<t)

4: Split y into steps s = (s1,..., 5,7) using T

5: Find the most uncertain step; set Sinit =
concat(s1,..-, Ymine)

6: Obtain Top-k& next-token candidates
(v4, tee , Uk)

7 Score candidates 1;
choose yt =
arg min; 7;

8: Expand the chosen branch: sample s*, s~
from 7 starting at y*, y~, stopping at EOS or
the first token with confidence < 7

9: Add (x, Sinit, 8, 8) to D

10: end for

11: ford =1to Ddo

12: Update 79 by
Loss(x, ne Y7)

13: end for

14: return 79

— R(x, Sinit, Vi);
argmax;r; and y =

Step-DPO loss,

4.1 Experiments Setup

Datasets and Models: For mathematical reason-
ing tasks, we use the prompts from the Step-DPO-
10k! dataset as model inputs x to construct the
preference dataset for CGPO training. MetaMath-
Mistral-7B (Yu et al., 2024), MetaMath-LLaMA-
8B (Liu et al., 2025), Qwen2-7B-SFT”, Qwen1.5-
32B-SFT? and DeepSeekMath-Base-SFT® are em-
ployed as the policy models 79. To ensure that
the capabilities of the Reward Model (RM) align
with the policy model and can provide token-level
fine-grained reward, we adopt ASPRM-L? as R
to select candidate tokens after s;,,;,, preventing
performance gains from an overly strong RM.

For code generation tasks, we use the prompts
from the training set of the LeetCodeDataset as
model inputs x to construct the CGPO preference
dataset. The policy model 79 is Deepseek-Coder-
7B-Instruct-v1.5 (Guo et al., 2024) and we adopt

‘https://huggingface.co/datasets/xinlai/Math-Step-DPO-
10K

*https://huggingface.co/xinlai/Qwen2-7B-SFT

$https://huggingface.co/xinlai/Qwen1.5-32B-SFT

“https://huggingface.co/xinlai/DeepSeekMath-Base-SFT

*https://huggingface.co/Lux0926/ASPRM-L

ASPRM-D* as our reward model to select candi-
date tokens. The trained models are evaluated on
the test sets of LiveCodeBench’ and LeetCode-
Dataset®.

Baselines and Metrics: For the mathematics
domain, two baselines are considered: the base
model and the model trained using the Step-DPO
method. For the code domain, only the base model
serves as the baseline.

For all tasks, model performance is reported us-
ing the accuracy metric. In mathematical reasoning
tasks, a prediction is considered correct only if
model’s final answer exactly matches the ground
truth. For code-generation tasks, the generated
code is executed in a sandbox and deemed correct
if it passes all test cases.

Parameter Settings: For each base model in
the mathematical domain, we construct the training
data as follows. All 10,795 prompts from Step-
DPO-10k are fed through the model to generate
one output per prompt, yielding 10,795 outputs.
For each output, we locate the lowest confidence
point and use the reward model to score the top 8
token candidates, then branch into two reasoning
paths. Decoding proceeds until the confidence falls
below the 2% of the model’s confidence distribu-
tion or <EOS> token is reached. This produces a
preference dataset of 10,795 pairs for each base
model, equivalent in size to Step-DPO-10k. For
the code domain base model, we sample 4 times
per prompt from the 2,641 LeetCodeDataset train-
ing examples and apply the same token selection
and reward-model procedure to generate 10,564
preference pairs.

For MetaMath-Mistral-7B and MetaMath-
Mistral-8B, we follow the training configuration
released by Step-DPO, set ( to 0.4, use a learn-
ing rate of 5e-7, set the global batch size to 128,
and train for 8 epochs to reproduce the Step-DPO
method. For all other Step-DPO baselines, we use
the publicly released model checkpoints by Step-
DPO. For all CGPO models, we train for 4 epochs
using a global batch size of 128 and a learning rate
of 5e-7. For MetaMath-Mistral-7B, Qwen2-7B-
SFT and Deepseek-Coder-7B-Instruct-v1.5, we set
6 to 0.4. For other models, we set (6 to 0.3. We use
the AdamW optimizer and a cosine learning rate
schedule, with a warmup ratio of 0.1.

Shttps://huggingface.co/Lux0926/ASPRM-D
Thttps://huggingface.co/livecodebench
Shttps://huggingface.co/datasets/newfacade/LeetCodeDataset


Table 1: Performance comparison of various models on mathematical and code benchmarks. Red and Green
numbers represent the performance improvement or decline compared to the base model.

Dataset Model Base +Step-DPO +CGPO
MetaMath-Mistral-7B 771 76.7(-0.4) 79.4 (+2.3)

MetaMath-Llama-8B 81.8 84.8 (43.0) 85.3 (43.5)

GSM8K Qwen2-7B-SFT 87.9 88.4 (40.5) 88.6 (+0.7)
Qwen1.5-32B-SFT 90.0 90.3 (40.3) 90.4 (40.4)
DeepSeekMath-Base-7B-SFT 86.3 86.5 (40.2) 87.1 (40.8)

MetaMath-Mistral-7B 26.6 27.3 (40.7) 27.0 (40.4)

MetaMath-Llama-8B 39.35 40.2 (40.9) 40.3 (41.0)

MATH Qwen2-7B-SFT 54.2. 54.6 (404) 54.8 (+0.6)
Qwen1.5-32B-SFT 54.2 55.7(4+1.5) 55.2 (+1.0)
DeepSeekMath-Base-7B-SFT 51.4 52.5(41.1) 51.8 (40.4)

LiveCodeBench —Deepseek-Coder-7B-Instruct-v1.5 19.3 / 19.7(+0.4)
LeetCodeDataset Deepseek-Coder-7B-Instruct-v1.5 12.7 / 13.2(+0.5)

4.2 Overall Results

The main results are shown in Table 1. For the
mathematics reasoning task, our method outper-
forms Step-DPO on GSM8K and achieves the
greatest improvement on MetaMath-Llama-8B,
with a 4.3% increase compared to the base model.
We attribute this phenomenon to the reward model
and the policy model being trained on the same
series of foundational models. Notably, when
training MetaMath-Mistral-7B using the Step-DPO
method, the resulting model’s performance actually
declined compared to the base model. This may
be because it is difficult for the weaker model to
follow the human-like reasoning paths and learn
from them in a preference way. However, our
method still leads to an improvement compared
to the base model, which demonstrates the advan-
tage of the model learning from non-human-like
reasoning paths through self-exploration. On the
MATH dataset, our method outperforms Step-DPO
on MetaMath-Llama-8B and Qwen2-7B-SFT, but
performs less well on the other three models. That
may be because the MATH dataset is more chal-
lenging than the GSM8K dataset. The 10k data vol-
ume is insufficient for fully exploring non-human-
like reasoning paths, limiting the model improve-
ment. Additional analysis experiments are con-
ducted below to verify this.

Additionally, for the code generation task, the

Deepseek-Coder-7B-Instruct-v1.5 trained with our
method achieved 2.1% and 4.0% over the base
model on the LiveCodeBench and LeetCode-
Dataset respectively. This demonstrates that, be-
yond mathematical reasoning tasks, CGPO can also
enhance model capabilities in tasks such as code
generation, where human-like reasoning paths are
not easily defined, by enabling the model to explore
non-human-like reasoning paths.

4.3 Analysis of Scalability

To explore model performance at different explo-
ration degrees, we sampled 2, 4, and 8 times for
each prompt on the Step-DPO-10k dataset using
MetaMath-Llama-8B and Qwen2-7B-SFT, thereby
constructing training datasets of 20k, 40k, and 80k.

Table 2 presents model performance on the
GSMB8K and MATH datasets. We find that increas-
ing the exploration degree gradually improves per-
formance on both datasets, with a notable jump on
GSMB8K when the data volume increases from 40K
to 80K. These results indicate that CGPO exhibits
effective scalability.

4.4 Analysis of Out of Datasets

To analyze the out-of-distribution generalization
performance of CGPO and its performance on chal-
lenging mathematical tasks, we use the Omni-Math
dataset (Gao et al., 2024a) is used to evaluate the


Table 2: Scalability on CGPO training data: models were trained with 10k, 20k, 40k, and 80k data samples,

respectively.
Data Scal
Model Dataset ara seal
10k 20k 40k 80k

GSM8K 85.3 85.3 85.6 86.4
MetaMath-Llama-8B

MATH 40.3 40.8 41.2 41.2

GSM8K 88.6 88.7 88.9 89.5
Qwen2-7B-SFT

MATH 54.8 54.9 55.1 55.3

Table 3: Performance comparison on out-of-domain dataset (Omni-Math).

Dataset Model Base + Step-DPO + CGPO
MetaMath-Mistral-7B 8.4 7.7 (-0.7) 8.8 (40.4)
MetaMath-Llama-8B 11.4 11.1 (-0.3) 11.7 (40.3)

Omni-Math Qwen2-7B-SFT 14.3 14.0 (-0.3) 14.6 (+0.3)
DeepSeekMath-Base-7B-SFT 13.7 14.7 (+1.0) 14.2 (+0.5)
Qwen1.5-32B-SFT 14.5 15.1 (+0.6) 15.2 (+0.7)

models. Omni-Math is an evaluation benchmark
that encompasses 4,428 Olympic-level competition
problems of 10 different difficulty levels and 33
subfields.

As shown in Table 3, compared with Step-
DPO, CGPO performs slightly lower only on
DeepSeekMath-Base-7B-SFT, while outperforms
it on all other models. This shows that CGPO ex-
hibits superior out-of-distribution generalization
and can effectively enhance the performance of
weaker models on challenging benchmarks, even
without strong model annotations.

Table 4: Results of training datasets constructed by
different reward models: ASPRM and Math-Shepherd
denote the models trained with preference data guided
by these two reward models.

Model GSM8K MATH
MetaMath-Llama-8B
+ ASPRM 85.3 40.3
+ Math-Shepherd 84.1 40.2
Qwen2-7B-SFT
+ ASPRM 88.6 54.8
+ Math-Shepherd 88.2 54.7

4.5 Analysis of Reward Model

To explore the impact of different reward models
on CGPO, we adopt the process reward model pro-
posed by Math-Shepherd (Wang et al., 2024a) for
comparison with ASPRM. The comparison results
are presented in Table 4.

When Math-Shepherd is used instead of AS-
PRM, the performance of CGPO-trained model
declines, although it remains better than the
base model. Even with a weaker PRM, CGPO
still shows an improvement over the base model.
This improvement stems from precise token-level
rewards, underscoring the significance of fine-
grained evaluation.

4.6 Analysis of Stopping Threshold

We report the performance differences of the model
under different confidence thresholds at which the
sampling stops in Figure 3. For MetaMath-Llama-
8B, performance keeps improving as the thresh-
old increases. For Qwen2-7B-SFT, overall perfor-
mance also shows an upward trend as the thresh-
old rises, but the peak is achieved at a confidence
threshold of 8%.

When we increase the confidence threshold,
more samples will stop in the intermediate pro-


Qwen2-7B-SFT

89.5

89.0
88.5
88.0
87.5
87.0

86.5
72.5
72.0

715
71.0
70.5

70.0
69.5
55.5

55.0
54.5
54.0

53.5

Performance(%)

0% 2% 4% 8% 10% 12% 14%
Confidence Threshold
—@®- GSM8K —é—Average IE MATH

MetaMath-Llama-8B

Performance(%)
aAnnaa
SSBF
wouou

Ny

0% 2% 4% 8% 10% 12% 14%

Confidence Threshold

Figure 3: The relationship between the confidence
threshold and the trained model performance.

cess rather than the generation of the <EOS> tag.
This increases the proportion of non-human-like
data, thereby enhancing the model’s exploration
and learning of non-human-like reasoning paths,
which ultimately improves performance. However,
increasing the confidence threshold also introduces
shorter lengths of chosen and rejected data, as
shown in Figure 4. We argue that this is the rea-
son for the performance decline of Qwen2-7B-SFT
when the confidence threshold is greater. These
results further demonstrate the necessity of opti-
mizing the model by the intermediate reasoning
path rather than by the entire process.

4.7 Analysis of Division Strategy

We use confidence scores to reconstruct the initial
reasoning step , as well as the chosen and rejected
data in the original Step-DPO-10k dataset, in or-
der to compare human-like and non-human-like
step divisions. Specifically, we truncate the ini-
tial reasoning step in the Step-DPO-10k dataset at
the lowest confidence point, as in the dataset con-
struction of CGPO. For the chosen and rejected

| == Qwen2-7B-SFT
250 ~ =@= MetaMath-Llama-8B

200
150

100

Average Tokens

50

0% 2% 4% 8% 10% 12% 14%
Confidence Threshold

Figure 4: The relationship between the confidence
threshold and the average number of tokens per pair.

data, we apply a truncation threshold at the lowest
2% confidence. The final results are reported in
Table 5.

We find that even using 79 to split the reasoning
path on outputs generated by the strong model for
training, the resulting performance still surpasses
the baseline. However, it does not outperform train-
ing on the 79 own generated and split data. This
result suggests that the split of its own generated
outputs by the weak model is more aligned with
the model, thereby leading to improved training
performance.

Table 5: Model performance with different annotation
or split methods.

Model GSM8K MATH
MetaMath-Llama-8B
+CGPO 85.3 40.3
+ Step-DPO 84.8 40.2
+ Step-DPO-Confidence 84.4 40.2
Qwen2-7B-SFT
+CGPO 88.6 54.8
+ Step-DPO 88.4 54.6
+ Step-DPO-Confidence 88.1 54.5

5 Conclusion

In this paper, we propose CGPO, a DPO-based
step-wise reasoning path preference optimization
method. Our method emphasizes non-human-like
reasoning paths in two key ways: 1) the training
data does not rely on strong models; 2) the split
for reasoning paths does not depend on predefined
anchors. We show the effectiveness of CGPOin
both mathematics and code domains, achieving
significant improvements and generalization ability


over approaches that use data generated by strong
models. Furthermore, our experiments reveal that,
within a certain range, intermediate reasoning path-
focused optimization leads to greater performance
gains than whole reasoning path optimization.

Limitations

Due to computational constraints, we are unable to
conduct experiments on larger models (e.g.,70B)
to explore the scalability with respect to model
size. In addition, our experiments were limited
to the code and math domains, primarily because
fine-grained evaluation in the commonsense do-
main is challenging. However, we believe our
approach remains applicable as long as a general
fine-grained scoring model is available, since the
proposed methodology is domain-agnostic.

References

Arash Ahmadian, Chris Cremer, Matthias Gallé,
Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,
Ahmet Ustiin, and Sara Hooker. 2024. Back to
basics: Revisiting reinforce style optimization for
learning from human feedback in lms. Preprint,
arXiv:2402. 14740.

Mohammad Beigi, Ying Shen, Runing Yang, Zihao
Lin, Qifan Wang, Ankith Mohan, Jianfeng He, Ming
Jin, Chang-Tien Lu, and Lifu Huang. 2024. Inter-
nalinspector i?: Robust confidence estimation in Ilms
through internal states. Preprint, arXiv:2406.12053.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588.

Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu,
Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongn-
ing Wang, and Minlie Huang. 2024. Spar: Self-play
with tree-search refinement to improve instruction-
following in large language models. arXiv preprint
arXiv:2412.11605.

Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu,
Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongn-
ing Wang, and Minlie Huang. 2025. Spar: Self-play
with tree-search refinement to improve instruction-
following in large language models. Preprint,
arXiv:2412.11605.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. Preprint, arXiv:2110.14168.

Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and
Yarin Gal. 2024. Detecting hallucinations in large
language models using semantic entropy. Nature,
630(8017):625-630.

Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei
Zhao. 2025. Deep think with confidence. Preprint,
arXiv:2508.15260.

Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo
Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang
Chen, Runxin Xu, Zhengyang Tang, Benyou Wang,
Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei
Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu,
and Baobao Chang. 2024a. Omni-math: A univer-
sal olympiad level mathematic benchmark for large
language models. Preprint, arXiv:2410.07985.

Jiaxuan Gao, Shusheng Xu, Wenjie Ye, Weilin Liu,
Chuyi He, Wei Fu, Zhiyu Mei, Guangju Wang, and
Yi Wu. 2024b. On designing effective rl reward
at training time for Ilm reasoning. arXiv preprint
arXiv:2410.15115.

Daya Guo, Dejian Yang, Haowei Zhang, and | others.
2025. DeepSeek—R1 incentivizes reasoning in LLMs
through reinforcement learning. Nature, 645:633-
638.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-
feng Liang. 2024. Deepseek-coder: When the large
language model meets programming — the rise of
code intelligence. Preprint, arXiv:2401.14196.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,
Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023.
Reasoning with language model is planning with
world model. arXiv preprint arXiv:2305.14992.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
problem solving with the math dataset. NeurIPS.

Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron
Courville, Alessandro Sordoni, and Rishabh Agar-
wal. 2024. V-star: Training verifiers for self-taught
reasoners. arXiv preprint arXiv:2402.06457.

Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen.
2025. Reinforce++: An efficient rlhf algorithm
with robustness to both prompt and reward models.
Preprint, arXiv:2501.03262.

Shima Imani, Liang Du, and Harsh Shrivastava. 2023.
Mathprompter: Mathematical reasoning using large
language models. arXiv preprint arXiv:2303.05398.

Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia
Yan, Tianjun Zhang, Sida Wang, Armando Solar-
Lezama, Koushik Sen, and Ion Stoica. 2024. Live-
codebench: Holistic and contamination free evalu-
ation of large language models for code. Preprint,
arXiv:2403.07974.


Zhewei Kang, Xuandong Zhao, and Dawn Song. 2025.
Scalable best-of-n selection for large language mod-
els via self-certainty. Preprint, arXiv:2502.18581.

Muhammad Khalifa, Rishabh Agarwal, Lajanugen Lo-
geswaran, Jaekyeom Kim, Hao Peng, Moontae Lee,
Honglak Lee, and Lu Wang. 2025. Process reward
models that think. Preprint, arXiv:2504.16828.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances in
neural information processing systems, 35:22199-

22213.

Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xi-
angru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise
preference optimization for long-chain reasoning of
lms. Preprint, arXiv:2406.18629.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio
Savarese, and Steven Chu Hong Hoi. 2022. Coderl:
Mastering code generation through pretrained models
and deep reinforcement learning. Advances in Neural
Information Processing Systems, 35:21314—21328.

Aitor Lewkowycz, Anders Andreassen, David Dohan,
Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo
Gutman-Solo, and 1 others. 2022. Solving quan-
titative reasoning problems with language models.
Advances in neural information processing systems,

35:3843-3857.

Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey
Kuznetsov, and Ivan Oseledets. 2025. Confidence
is all you need: Few-shot rl fine-tuning of language
models. Preprint, arXiv:2506.06395.

Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing
Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu
Yang, and Zhaopeng Tu. 2024. Critical tokens matter:
Token-level contrastive estimation enhances I]m’s rea-
soning capability. arXiv preprint arXiv:2411.19943.

Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu,
Jason Klein Liu, Chonghan Liu, Zefan Cai, Yun-
hui Xia, Li Zhao, Jiang Bian, Chuheng Zhang, Wei
Shen, and Zhouhan Lin. 2025. Adaptivestep: Au-
tomatically dividing reasoning step through model
confidence. Preprint, arXiv:2502.13943.

Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren,
Weikang Shi, Junting Pan, Mingjie Zhan, and Hong-
sheng Li. 2024. Step-controlled dpo: Leveraging
stepwise error for enhanced mathematical reasoning.
arXiv preprint arXiv:2407.00782.

Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat
Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei
Shu, Yun Zhu, Lei Meng, and | others. 2024. Im-
prove mathematical reasoning in language models
by automated process supervision. arXiv preprint
arXiv:2406.06592.

10

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:
Empowering code large language models with evol-
instruct. arXiv preprint arXiv:2306.08568.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
and | others. 2023. Self-refine: Iterative refinement
with self-feedback. Advances in Neural Information
Processing Systems, 36:46534—46594.

Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Adamek, Eric Malmi, and Aliaksei Severyn. 2022.
Teaching small language models to reason. arXiv
preprint arXiv:2212.08410.

Yu Meng, Mengzhou Xia, and Danqi Chen. 2024.
Simpo: Simple preference optimization with a
reference-free reward. Preprint, arXiv:2405.14734.

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xi-
ang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Candés, and
Tatsunori Hashimoto. 2025. sl: Simple test-time
scaling. Preprint, arXiv:2501.19393.

OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer,
Adam Richardson, Ahmed El-Kishky, Aiden Low,
Alec Helyar, Aleksander Madry, Alex Beutel, Alex
Carney, Alex Iftimie, Alex Karpenko, Alex Tachard
Passos, Alexander Neitz, Alexander Prokofiev,
Alexander Wei, Allison Tam, and 244 others. 2024.
Openai o1 system card. Preprint, arXiv:2412.16720.

Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho,
He He, Sainbayar Sukhbaatar, and Jason Weston.
2024. Iterative reasoning preference optimization.
Preprint, arXiv:2404.19733.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2024. Direct preference optimization: Your lan-
guage model is secretly a reward model. Preprint,
arXiv:2305.18290.

John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Prox-
imal policy optimization algorithms. Preprint,
arXiv: 1707.06347.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.
Deepseekmath: Pushing the limits of mathemati-
cal reasoning in open language models. Preprint,
arXiv:2402.03300.

Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh,
Maxwell Horton, Samy Bengio, and Mehrdad Fara-
jtabar. 2025. The illusion of thinking: Understanding
the strengths and limitations of reasoning models via
the lens of problem complexity. In NeurIPS.


Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya
Grover, Kai-Wei Chang, Marcus Rohrbach, and
Anna Rohrbach. 2025. When to solve, when to
verify: Compute-optimal problem solving and gen-
erative verification for Ilm reasoning. Preprint,
arXiv:2504.01005.

Charlie Snell, Jachoon Lee, Kelvin Xu, and Aviral Ku-
mar. 2024. Scaling Ilm test-time compute optimally
can be more effective than scaling model parameters.
arXiv preprint arXiv:2408.03314.

Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Be-
linkov, Xingyi Song, Mrinmaya Sachan, and Neel
Nanda. 2024. Confidence regulation neurons in lan-
guage models. Preprint, arXiv:2406.16254.

Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder,
Ariel Goldstein, Zorik Gekhman, and Gal Yona.
2025a. Confidence improves self-consistency in Ilms.
arXiv preprint arXiv:2502.06233.

Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder,
Ariel Goldstein, Zorik Gekhman, and Gal Yona.
2025b. Confidence improves self-consistency in Ilms.
In Findings of the Association for Computational Lin-
guistics: ACL 2025, page 20090-20111. Association
for Computational Linguistics.

Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai
Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.
2024a. Math-shepherd: Verify and reinforce LLMs
step-by-step without human annotations. In Proceed-
ings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 9426-9439, Bangkok, Thailand. Associ-
ation for Computational Linguistics.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. 2022. Self-consistency improves chain
of thought reasoning in language models. arXiv
preprint arXiv:2203.11171.

Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo,
Le Hou, Hongkun Yu, and Jingbo Shang. 2024b.
Multi-step problem solving through a verifier: An
empirical analysis on model-induced process super-
vision. arXiv preprint arXiv:2402.02658.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2023. Chain-of-thought prompting elic-
its reasoning in large language models. Preprint,
arXiv:2201.11903.

Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu
He, Shengping Liu, Bin Sun, Kang Liu, and Jun
Zhao. 2022. Large language models are better
reasoners with self-verification. arXiv preprint
arXiv:2212.09561.

Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Ja-
son Weston, and Sainbayar Sukhbaatar. 2024. Think-
ing llms: General instruction following with thought
generation. Preprint, arXiv:2410.10630.

11

Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu,
Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu.
2025. Leetcodedataset: A temporal dataset for ro-
bust evaluation and efficient training of code IIms.
Preprint, arXiv:2504.14655.

Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie
Fu, Junxian He, and Bryan Hooi. 2024. Can
Ilms express their uncertainty? an empirical eval-
uation of confidence elicitation in Ilms. Preprint,
arXiv:2306.13063.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Lv, Chujie Zheng, Dayi-
heng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,
Haoran Wei, Huan Lin, Jialong Tang, and 41 oth-
ers. 2025. Qwen3 technical report. arXiv preprint
arXiv:2505.09388.

Gal Yona, Roee Aharoni, and Mor Geva. 2024.
Can large language models faithfully express
their intrinsic uncertainty in words? — Preprint,
arXiv:2405.16908.

Bin Yu, Hang Yuan, Haotian Li, Xueyin Xu, Yuliang
Wei, Bailing Wang, Weizhen Qi, and Kai Chen.
2025a. Long-short chain-of-thought mixture super-
vised fine-tuning eliciting efficient reasoning in large
language models. arXiv preprint arXiv:2505.03469.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T. Kwok, Zhen-
guo Li, Adrian Weller, and Weiyang Liu. 2024.
Metamath: Bootstrap your own mathematical
questions for large language models. Preprint,
arXiv:2309.12284.

Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,
Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,
Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin,
Zhigi Lin, Bole Ma, Guangming Sheng, Yuxuan
Tong, Chi Zhang, Mofan Zhang, Wang Zhang, and
16 others. 2025b. Dapo: An open-source Ilm re-
inforcement learning system at scale. Preprint,
arXiv:2503.14476.

Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-
hao Huang, Huan Sun, Yu Su, and Wenhu Chen.
2023. Mammoth: Building math generalist models
through hybrid instruction tuning. arXiv preprint
arXiv:2309.05653.

Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-
man. 2022. Star: Bootstrapping reasoning with rea-

soning. Advances in Neural Information Processing
Systems, 35:15476-15488.

Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue,
Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm
self-training via process reward guided tree search.
Advances in Neural Information Processing Systems,
37:64735-64772.


Kechi Zhang, Ge Li, Yihong Dong, Jingjing Xu, Jun
Zhang, Jing Su, Yongfei Liu, and Zhi Jin. 2025. Cod-
edpo: Aligning code models with self generated and
verified source code. Preprint, arXiv:2410.05605.

Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran
Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024b.
Generative verifiers: Reward modeling as next-token
prediction. arXiv preprint arXiv:2408. 15240.

Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli
Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo
Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin
Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhen-
jie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, and 4
others. 2025a. Flowrl: Matching reward distributions
for lm reasoning. Preprint, arXiv:2509.15207.

Yuqi Zhu, Ge Li, Xue Jiang, Jia Li, Hong Mei, Zhi Jin,
and Yihong Dong. 2025b. Uncertainty-guided chain-
of-thought for code generation with Ilms. arXiv
preprint arXiv:2503. 15341.

12


A Appendix

A.1 Example of Confusion vs. First Error
We present two examples where the lowest confidence appears prior to the first error in Figure 5.
Question: Question:

If $x - \\frac{1}{x} = 4$ and $y = x*2+
\\Frac{1}{x*2}$, what is $y*2 - 2$?

Qwen2-7B-SFT:

Determine the smallest integer n, greater than
1, for which n*-1 mod 1050 is defined?

MetaMath-Llama-8B:

We know that $n“{-1} \\pmod{1050}$ exists

|
|
|
|
|
| \nFirst, we square the equation $x -
| if and only if $\\gcd(n, 1050) = 1$. \n\nThe

|

|

|

|

|

|

|

|

|

|

\

(
|
|
|
|
|
|
| rn
\\frac{1}{x} = 46 to get \\[(« - \\frac{1}{x})*2
prime factorization of $1050$ is $2 \\cdot 3 | Frac{tHx) = 4$ to g K prac)
|
|
|
|
|
|
|
|
|
\

\\cdot 5*2 \\cdot 7$. \n\nTo find the
smallest integer $n$ that satisfies $\\gcd(n,
1050) = 1$, we can start with $n = 2$ and
increment by $1$ until we find a number that
is relatively prime to $1050$. \n\nThe first
few numbers we try are $2, 3,4, 5, 6, 7$.
\n\nWe see that $\\gcd(7, 1050) = 1$, so the
smallest integer $n$ for which $n*{-1}
\\pmod{1050}$ is defined is $\\boxed{7}$.
\n\nThe answer is: 7 J

= 4°2.\\]\nExpanding, we have $x*2 - 2 +
\\frac{1}{x*2} = 16$, so $x*2 + \\frac{1}{x*2}
= 18$.\nTherefore, $y = x*2 + \\frac{1}{x*2} =
18$.\nFinally, we have $y*2 - 2= 18°2 -2=
\\boxed{320}$.\nThe answer is: 320

—— ee
fe  _____~,

ee

Figure 5: Examples where the lowest confidence appears before the first error.

A.2_ Statistics of the Positional Relationship

Table 6: Statistics on the relative positions of first error step and lowest confidence point, where "Before" and
"After" denote the first error before or after the model confusion point

Position
Before After

Model

MetaMath-Llama-8B DD. 78
Qwen2-7B-SFT 28 72

We randomly select 200 samples with incorrect results, half of which are from the MetaMath-Llama-8B
generated CGPO training dataset and the other from the Qwen2-7B-SFT dataset. To identify the first
erroneous step, we use GPT5-thinking with the prompt-

"Analyze the following text and identify the first location where an error occurs, reporting the exact
substring/token, its position (line and character index), why it is an error, and why this is the first error
(i.e., why all preceding text is error-free); if no error exists, reply ‘No error found’; Text:" [Q-A pair]

-to find the first error, and manually determine the positional relationship between the first error and the
model’s lowest confidence point. We report the analysis results in Table 6.

13
