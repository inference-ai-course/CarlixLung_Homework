2510.09913v1 [cs.CL] 10 Oct 2025

arXiv

Preprint

DON’T THROW AWAY YOUR PRETRAINED MODEL

Shangbin Feng! Wenhao Yu? Yike Wang! Hongming Zhang? Yulia Tsvetkov' Dong Yu

University of Washington Tencent AI Seattle Lab
shangbin@cs.washington.edu

ABSTRACT

Alignment training has tradeoffs: it helps language models (LMs) gain in rea-
soning and instruction following but might lose out on skills such as creativity
and calibration, where unaligned base models are better at. We aim to make the
best of both worlds through model collaboration, where different models in the
training pipeline collaborate and complement each other. Since LM responses
feature interleaving skills that favor different models, we propose SWITCH GEN-
ERATION, where pretrained and aligned model versions take turns to “speak” in
a response sequence. Specifically, we train a switcher LM by learning from out-
comes of choosing different models to generate the next segment across diverse
queries and contexts. At inference time, the switcher LM guides different model
checkpoints to dynamically generate the next segment where their strengths are
most needed. Extensive experiments with 8 model collaboration baselines and
18 datasets show that 1) model collaboration consistently outperforms individual
models on 16 out of 18 tasks, and 2) SWITCH GENERATION further outperforms
baselines by 12.9% on average. Further analysis reveals that SWITCH GENERA-
TION discovers compositional skills to solve problems where individual models
struggle and generalizes to unseen models and tasks, reusing and repurposing by-

products in expensive model training pipelines that are otherwise discarded. !

1 INTRODUCTION

Alignment/RL has become an integral part in
language model (LM) training, improving mod-
els on skills such as reasoning and instruction
following (Ouyang et al., 2022; Guo et al.,
2025). However, it is not a Pareto-optimal
strategy (Lin et al., 2024): aligned models have
tradeoffs on skills such as creativity (West &
Potts, 2025), calibration (Tian et al., 2023), and
generation diversity (Yue et al., 2025; Yang &
Holtzman, 2025), where unaligned base mod-
els are better at. How to make the best of both
worlds is essential for handling complex tasks
that require compositional skills and develop-
ing AI systems that are flexible and adaptable
to diverse user needs and contexts.

To this end, we resort to model collabora-
tion (Feng et al., 2025a), where diverse model
checkpoints (e.g., pretrained and aligned ver-
sions of models) collaborate, compose, and
complement each other. Since model responses
are not monolithic and feature a wide variety of
skills favoring different models (Figure 1), we
propose SWITCH GENERATION, where differ-

@ Instruction: In which film was actor Glenn Strange

unaware he was being called to play Frankenstein's
monster until he reported to the make-up room?

Response: The actor Glenn Strange was known

for playing the Frankenstein’s monster in several =
films, but you are asking for a specific film when it |o Jpauctien
is documented that he was unaware he was to Following
play the role until arriving at the make-up room.

Glenn Strange also played the role of the monster

in several Universal Pictures’ Frankenstein

sequels, including “Son of Frankenstein” and the Recalling
original “Frankenstein.” Therefore, it is not known
if Glenn Strange was unaware he was to play the
Frankenstein’s monster until he reported to the
make-up room. However, he may havebeen
unaware that he was to play the role in the
“House of Frankenstein” film, because he was
credited as “The Monster” in these films and was

not specifically credited as Glenn Strange. The

answer is House of Frankenstein.

eReasoning

Figure 1: Model responses are not monolithic:
they switch across diverse skills which favor dif-
ferent model checkpoints in the training pipeline,
thus we introduce model-guided collaborative in-
ference to optimally use models with diverse skills
for different segments of response generation.

ent models in the training pipeline take turns to “speak” in a response sequence. Specifically, we

"Inference code and switcher LMs available at https://github.com/BunsenFeng/switch_generation.


Preprint

> Learning the switcher

. . rollout average
Query @ What is the company with the largest market cap diverging segments ktimes 62 score
that doesn’t directly sell things to customers? graphics processing units (GPUs) and system- 2 0.68
As of July 2025, the company with the largest market cap on-a-chip (SoC) units to other companies. 3°
Trace t that doesn't directly sell things to customers (i.e., is not » semiconductor components to enterprise-level
primarily a direct-to-consumer business) is likely NVIDIA. "clients, including major technology firms, data 5
While NVIDIA's chips are integral to many consumer | center operators, and automotive manufacturers. 0.4)
products, their core business is designing and supplying . — oe
eneseadvanced “_/—» computing platforms for specialized applications ~N
like artificial intelligence, data centers, and 0.57
Candidate C [ pretrained finetuned aligned professional visualization. aaa
pena netimimimmiemammomimse msmnmimineimuminvnmiminesen/mima mm/min nimienimininimiasnimmniminien sion nseinymminieimimnin.eimninimmomimimemiascminiavniniminseinie ' G3} switcher f(q,t,C) +P!
>Usingtheswitcher nnn nnaannaee nena n ae
Query 4 Please detail every property transaction in New York City from the year 2000 to the present. aa
response
In order to obtain an approximate answer, | have used the following information. First, | have used the average price
f(a,t = None,C) > per square foot for the past 20 years in New York City. According to Zillow, the average price per square foot is
| $1,200.00. Second, | have used the population of New York City from

In order to obtain an approximate answer, | have used the following information. First, | have used the average price
rt = In order to... |have used the ) per square foot for the past 20 years in New York City. According to Zillow, the average price per square foot is

> population of New York City from’ $1,200.00. Second, | have used the population of New York City from the year 2000 to the present. However, it is
impossible to gather all of the information. It is not feasible to detail every property

| In order to obtain an approximate answer, I have used the following information. First, | have used the average price
per square foot for the past 20 years in New York City. According to Zillow, the average price per square foot is
$1,200.00. Second, | have used the population of New York City from the year 2000 to the present. However, it is
impossible to gather all of the information. It is not feasible to detail every property transaction in New York City
from the year 2000 to the present. The reason is that there are millions of property transactions in New York City
over the past two decades, and there are no complete records of all property transactions.

In order to ... [have used the
f(a,t = population of New York City fromthe ,C) >
year 2000 ... detail every property

Figure 2: Overview of SWITCH GENERATION, where multiple model checkpoints in the training
pipeline (e.g., pretrained, finetuned, and aligned LM checkpoints) are dynamically selected to gen-
erate text segments in a sequence. (Up) We derive training data for the switcher LM f by rolling out
which model would lead to the best average outcome for a particular query and trace. (Down) At
inference time, multiple models are guided by the trained switcher LM to generate text segments as
part of a response when their skills and strengths are most needed.

train a (small) switcher LM to decide which model should generate the next segment based on the
query, trace (what has been generated thus far), and (model) candidates. For any (query, trace) pair,
we let each model candidate generate one more segment, randomly sample & continuations, and
evaluate which candidate has led to the best average performance: this yields supervised fine-tuning
data for the switcher LM, where it learns to predict the best model checkpoint for generating the
next text segment on diverse (query, trace) pairs. During inference, the switcher LM dynamically
selects the most suitable model checkpoint for each segment, so the final response is generated as a
sequence of turns where different models contribute under the switcher’s guidance (Figure 2).

Extensive experiments with 8 model collaboration baselines (spanning API, text, logit, and weight-
level collaboration) and 18 datasets (e.g. QA, reasoning, instruction following) demonstrate that:

¢ Don’t throw away your pretrained model: model collaboration approaches outperform all individ-
ual models on 16 out of 18 datasets (close second on the other two).

¢ SWITCH GENERATION presents a strong paradigm for collaborative inference: SWITCH GEN-
ERATION outperforms all baselines on 13 datasets with an average improvement of 12.9%.

¢ Further analysis reveals that SWITCH GENERATION generalizes to unseen tasks and model set-
tings, helps solve problems impossible for any of the models when used individually, identifies
high-quality switching patterns, and texts generated through SWITCH GENERATION can be dis-
tilled back into a single model for efficiency.

Our work put forward a new vision to reuse, recycle, and repurpose byproducts in expensive model
training pipelines that have huge potential but are currently neglected and underappreciated.

2 METHODOLOGY

We propose S WITCH GENERATION, a collaborative inference algorithm where diverse model check-
points in the training pipeline are dynamically selected to generate successive segments of the re-
sponse. SWITCH GENERATION aims to dynamically leverage the complementary strengths and


Preprint

expertise of different model checkpoints (e.g., pretrained, finetuned, and aligned’), especially for
complex problems that require compositional skills. The core of SWITCH GENERATION is deciding
“who should speak at when”, formally the Query-Trace-Candidate Problem (the QTC Problem):

where q denotes the query/instruction, t denotes the trace, i.e., what has been generated thus far,
C = {c1,--- ,€,} denotes the pool of candidate language model checkpoints, and p; denotes the
likelihood of choosing model c; to generate the next text segment. The QTC problem essentially
tackles the research question: “Given the question and what has been said thus far, which model is
best suited to speak next?”

This differs from existing routing paradigms (Ong et al., 2025; Frick et al., 2025) in that: 1) the trace
t # @; 2) each selected model c; only generates text segments instead of the full response; 3) the
switcher f is applied multiple times instead of just once. This brings novel ways of combining model
strengths, finer-grained control over model collaboration, and improved adaptability to diverse user
requests.

Parameterizing the switcher We parametrize the switching strategy f as a (small) language
model and encode its input q, t, and C with the following prompt:

Prompt |: Switcher Prompt

query (model i begins) text generated by model i (model i ends) --- (model j begins)

text generated by model j (model j ends) Which model should generate the next segment?
Please respond with a number from 0 to n-1. The answer is model

We provide the switcher f with candidate-marked trace t using special token delimiters ((model i
begins/ends)), aiming to decipher what is needed next and who might be helpful by learning from the
model-attributed generation history. The switcher f then predicts a model ID, and we take the logits
of 0 to n-1 as [pi1,--- , pn]. By making the switching strategy f compatible with natural language
and language models, we seek to leverage their language capabilities to aid in the QTC problem.

Learning the switcher Given any instruction q:

* We randomly sample a trace t (a partial response to the query) with random switching fandom =
Uniform(n), i.e., randomly choosing models to generate a segment after another. Trace t is
capped at a random threshold of 10% to 90% of the maximum response length, aiming to capture
switching behavior at different stages of response completeness.

¢ From the generated trace t we take one divergent step: different models generate one more seg-
ment following it: {t; = t||c1(q,t),--- ,tn =t||cn(q, t) }. || denotes string concatenation.

¢ We sample k continuations for each t; with frandom, aiming to roll out diverse outcomes of choos-
ing model c; at this particular (q,t). The utility for choosing c; is then:

k
1
&; = k 2 score(t,, St anaom | q)

where score uses any evaluation metric corresponding to q (accuracy, Fl-match, reward scores).
Let g = arg max; s;, then model c, should be selected at this particular (q, t). This then yields
{(q, t,C) + cy }, a supervised fine-tuning instance for training the switcher f (it should predict
the model id g after “The answer is model” in Prompt 1). By sampling such SFT data points over
diverse q € Q, we obtain a dataset for training the switcher LM f.

*Pretrained models indicate models after autoregressive pretraining on mass corpora, finetuned models
indicate models after instruction tuning, and aligned models indicate models after alignment and reinforcement
learning. They are usually successive steps in model development.


Preprint

Using the switcher At inference time, the trained switcher f guides switching patterns among
diverse model checkpoints for collaborative generation. While existing works might change models
at every token (Shen et al., 2024; Fei et al., 2024), we propose to call the switcher per patch (a fixed
set of tokens) as it: 1) scales betters (Pagnoni et al., 2024), 2) preserves the continuity of thought for
models instead of being interrupted at every token, and 3) incurs much fewer times and thus much
less cost of calling the switching strategy f.

We employ top-p (nucleus) sampling (Holtzman et al., 2020) to select a model from the distribu-
tion [p1,+-+ , Pn]: top-p(f(q,t,C)) + ¢ € C (instead of greedy selection), balancing utility and
exploration in switching generation.

At first, given the query q and no trace, we select model ce) = top-p( f(q,2,C )) , generate a patch
of tokens ce“) (q), and append to trace t) = ce) (q).

At the i-th step, we select model ec = top-p(f(q, t—),C)), generate a patch ec (q||t—))), and
append to trace t = t—) || ce (q||t@-Y),

We continue until the generation ends or the maximum amount of tokens is reached. To sum up,
SWITCH GENERATION employs diverse model checkpoints in the training pipeline to collabora-
tively generate, complement each other, and advance compositional intelligence.

3. EXPERIMENT SETTINGS

Models and Implementation We by default employ the three models in the pretrained—finetuned—
aligned pipeline of Tulu-v3 (Lambert et al., 2024) (meta-llama/Llama-3.1-SB, allenai/Llama-3.1-
Tulu-3-8B-SFT, and allenai/Llama-3.1-Tulu-3-8B) due to its transparency and experiment with dif-
ferent model checkpoints, number of models or model settings in Section 5. We employ the aligned
model (allenai/Llama-3. 1-Tulu-3-8B) to initialize the switcher f, sample 10k switcher SFT instances
for each task with k = 32, and train f for 5 epochs with 2e-4 learning rate and 32 batch size under
two settings: switch-g(lobal), where one switcher is trained on the SFT data across all tasks; switch-
t(ask-specific), where one switcher is trained on the SFT data for each task. At inference time, all
methods generate 512 new tokens at max by default; for SwITCH GENERATION, we use the aligned
model in the first and last patch, employ a patch size of 50 tokens, and top-p sampling p = 0.7 by
default.

Baselines We compare SWITCH GENERATION with 11 baselines: the pretrained, finetuned, and
aligned models employed individually, API-level collaboration (prompt-based routing (Feng et al.,
2024a) and RouteLLM (Ong et al., 2025)), text-level collaboration (collaborate (Si et al., 2023) and
debate (Du et al., 2023)), logit-level collaboration (logit merge and proxy tuning (Liu et al., 2024)),
and weight-level collaboration (greedy soup (Wortsman et al., 2022) and dare-ties (Yadav et al.,
2023; Yu et al., 2024)). These baselines cover a wide range of model collaboration protocols across
diverse levels of information exchange.

Data and Evaluation We employ 18 datasets spanning 3 categories:

Datasets where having the base model might be helpful: knowledge and factuality (WikiDYK
(Zhang et al., 2025) and TruthfulQA (Lin et al., 2022)), creativity (poem (West & Potts, 2025)
and GuessBench (Zhu et al., 2025)), pluralism (Sorensen et al., 2024), sycophancy (Cheng et al.,
2025), randomness, generation diversity (movie reviews (Wang et al., 2023b)), and uncertainty
(AbstainQA (Feng et al., 2024b)), as supported by the findings of existing literature.

Datasets where having the base model might be worse: reasoning (gsm8k (Cobbe et al., 2021),
BigBench-Hard (Suzgun et al., 2023), and NLGraph (Wang et al., 2023a)), instruction following
(Alpaca (Dubois et al., 2023)), and safety (Coconot (Brahman et al., 2024)), since these skills are
explicitly what alignment is for.

Datasets where the effect of base models is unclear: general QA (MMLU-pro (Wang et al., 2024),
AGIEval (Zhong et al., 2024), and PopQA (Mallen et al., 2023)) and scientific literature (Sci-
enceMeter (Wang et al., 2025a)).


Preprint

Table 1: Performance of individual models and model collaboration methods. Green, red, and yellow
denote category-1/2/>5 tasks in Section 3. | denotes the lower the better. Best in bold and second-
best in underline. Model collaboration approaches outperform employing models individually on
16 out of 18 tasks. SWITCH GENERATION achieves the best performance on 13 tasks with a 12.9%
relative improvement over baselines on average.

WikiDYK TruthfulQA Poem AbstainQA Pluralism Sycophancy GuessBench Numbers (1) Movie

PRETRAINED 1.70 10.37 24.55 62.92 32.20 12.80 2.00 0.26 13.76
FINETUNED 3.27 30.63 49.45 49.44 58.90 13.00 4.40 0.90 3.82
ALIGNED 3.92 29.01 71.70 44.38 50.90 16.40 6.40 0.42 0.93
PROMPT ROUTE 321 23.99 54.40 53.93 45.80 6.40 7.60 0.59 4.94
ROUTELLM 3.01 34.38 61.60 58.43 51.80 14.20 4.40 0.87 13.05
TEXT COLLAB 3.14 18.15 52.30 28.65 28.30 11.80 6.80 0.59 3.24
TEXT DEBATE 3.53 16.86 51.10 30.90 43.00 6.40 8.80 0.54 2.65
LOGIT MERGE 1.57 9.08 53.80 12.92 23.90 14.80 1.20 0.22 10.75
PROXY TUNING 1.96 2.16 72.60 26.40 10.70 15.20 2.00 0.37 19.56
GREEDY SOUP 4.58 33.06 77.95 60.67 57.80 13.20 6.80 1.11 13.91
DARE TIES 2.61 23.99 47.90 52.81 3750 12.40 1.60 031 10.60
SWITCH-GLOBAL 4.44 34.04 70.90 60.67 55.60 16.80 10.80 0.41 14.14
SWITCH-TASK 5.75 39.22 10.25 74.16 53.20 17.40 13.60 0.44 17.39
GSM8k CocoNot Alpaca BBH NLGraph MMLU-pro AGIEval PopQA Science
PRETRAINED 27.20 11.90 14.51 38.10 44.50 5.10 4.41 15.30 29.80
FINETUNED 37.20 64.00 52.43 26.70 38.33 13.10 11.94 26.10 56.30
ALIGNED 56.80 53.10 57.46 35.20 41.83 10.40 11.85 31.20 60.60
PROMPT ROUTE 48.10 43.60 42.90 43.40 44.83 5.10 11.68 29.90 51.80
ROUTELLM 48.10 57.60 43.97 45.90 48.67 10.50 12,32 31.30 59.00
TEXT COLLAB 40.70 41.80 40.39 34.60 41.33 7.10 15.74 31.30 48.10
TEXT DEBATE 46.90 40.90 52,35 39.10 44.67 8.30 15.05 29.60 52.70
LOGIT MERGE 38.30 16.70 28.46 39.00 25.67 1.50 7.01 21.60 10.50
PROXY TUNING 44.50 7.30 37.27 44.00 43.83 1.40 2.34 22,50 11.40
GREEDY SOUP 58.10 66.00 54.89 36.50 45.33 12.70 11.76 31.30 60.30
DARE TIES 22.90 19.40 40.01 30.30 42.33 6.50 7.87 25.10 46.50
SWITCH-GLOBAL 49.50 70.90 49.06 52.60 59.67 13.00 14.19 33.70 59.80
SWITCH-TASK 59.60 72.80 56.89 58.30 61.67 16.70 25.26 37.70 67.20

These datasets cover a wide range of LM capabilities that favor different model checkpoints in the
training pipeline. If SWITCH GENERATION improves on category-! tasks, slightly behind/on par
on category-2 tasks, and on par/improves on caicgory-3 tasks, it presents a promising collaboration
strategy to fuse the strengths of model checkpoints and enable them to complement each other.

4 RESULTS
We present the performance of individual models and model collaboration methods in Table 1.

Don’t throw away your pretrained model. Model collaboration among pretrained, finetuned,
and aligned language models, baselines or ours, outperforms using these models individually on
16 of 18 tasks with 31.0% relative improvement on average. This indicates that checkpoints other
than the aligned models are diamonds in the rough, successfully complementing each other and
contributing their unique strengths.

. . Table 2: Ablation study of generation patch size and switch-
Switch Generation offers a strong ing strategy. Different tasks might favor different patch sizes

collaboration strategy. SWITCH while fine-tuning the switcher f is consistently helpful.
GENERATION ~ outperforms all

individual models and model collab-

oration baselines on 13 datasets, with Setting TruthfulQA Pluralism GSM8k BBH PopQA
an average relative improvement of | SWITCH-TASK 39.22 53.20 59.60 58.30 37.70
12.9%. In addition to improving on PATCH SIZE: 100 30.31 53.30 44.70 40.40 32.00
cat-1 tasks, SWITCH GENERATION PATCH SIZE: 30 35.79 55.70 52.20 53.50 33.80
also gains 6.58 points on average PATCH SIZE: 20 32.58 56.80 47.80 48.10 31.80
across cat-2 and cat-3 tasks, where PATCH SIZE: 10 30.96 54.50 51.40 48.70 32.80
it was originally uncertain whether RANDOM SWITCH 27.07 54.80 44.70 53.10 27.90

UNTUNED SWITCH 31.12 53.10 47.90 41.80 32.10

5


Preprint

having the base model in collaboration might be helpful. This indicates that by collaborative
inference with a flexible switching strategy, our approach adapts to diverse tasks through leveraging
the strengths of candidate models and fusing their strengths.

Routing-based approaches are best for pretrained-aligned collaboration. In descending order,
routing-based, weight-based, text-based, and logit-based baselines achieve 31.15, 29.91, 26.32, and
18.97 points on average, indicating that routing-based methods are best suited for the collaboration
of aligned and unaligned models, since different tasks require different skills that favor varying
models. SWITCH GENERATION further provides a finer-grained and more flexible routing on the
segment-level, so diverse model checkpoints could dynamically contribute in the problem-solving
process when their skills are most needed.

5 ANALYSIS

Ablation study We conduct abla- i pretrained _m Finetuned = Aligned m distil(Aligned) = full ours
tion study on two key design choices  && gg

in SWITCH GENERATION: 1) gener-
ation patch size: we by default gen-
erate 50 tokens per model and call 40,00
the switcher f and we additionally
employ {10, 20, 30, 100} in this
study; 2) switcher training: we by 20.00
default train the switcher f through

supervised fine-tuning on simulated ll

switching outcomes and we addi- 9.00

tionally employ random switching © RS of $ xe 8
(frandom = Uniform(n)) and un- \S rs & RY eg
tuned switching (directly employing o

the aligned model as f without fine- _. oo, ;
tuning). Results in Table 2 demon- Figure 3: Distilling the collaboration patterns of SWITCH

strate that different tasks might ben- GENERATION back into the aligned model. Distillation re-
covers 58% of the collaboration gains with only one fourth

efit from different switching granu- ;
of the inference cost.

larity: by employing more frequent
switching on Pluralism our approach
further improves. Fine-tuning the switcher with our methodology is consistently effective as it out-
performs random and untuned switching on all five tasks.

Distillation back into a single model At in- 1.25

ference time, SWITCH GENERATION loads and e

generates texts with n+1 LMs simultaneously. 1.00

While this is fast with multiple GPUs and mul-

tiprocessing, we propose to reduce inference % 0.75

costs by distilling the switching patterns back ©

into a single model. The aligned model was 3 0.50 ° 2
once pretrained and finetuned too, so by dis- 7 e
tillation we hope that it could recover the sub- 0.25 “+, *
merged capabilities of strengths of its previous e ° .

forms. For inputs in a dataset, we 1) gener- 0.00

ate outputs with full SWITCH GENERATION, fe * °

2) fine-tune the aligned model on the generated -0.25

060 -040 -0.20 0.00 0.20

tput d luate th fe f th
outputs, and 3) evaluate the performance of the Pasetaimanes

distilled aligned model when used individually.
Results in Figure 3 demonstrate that distillation __ .
successfully helps the aligned model pick up Figure 4: Correlation between the performance
the submerged skills, recovering 57.5% of the nd helpfulness of the pretrained model. While
gains of SWITCH GENERATION with only one ot being the best individual model, it is consis-
fourth of the inference cost (one model only tently helpful in the model collaboration system.

vs. three models and a switcher LM). This suc-


Preprint

Table 3: Performance when directly employing the trained switcher model for other model settings:
PFA from another model family, PA only, PF and two versions of A, and three specialized LM
experts. The trained switcher model consistently generalizes to these settings to varying extents.

Setting | Setting 2 Setting 3 Setting 4
Truthful BBH PopQA| Truthful BBH PopQA | Truthful BBH PopQA| Truthful BBH PopQA

MODEL #1 26.09 69.50 24.10 | 10.37 38.10 15.30 | 10.37 38.10 15.30 | 40.19 48.40 20.90
MODEL #2 53.00 54.80 25.60 | 29.01 35.20 31.20 | 30.63 26.70 26.10 | 53.00 54.80 25.60

MODEL #3 63.29 70.70 23.30 / / / 32.58 36.20 31.10 | 54.29 56.20 22.20
MODEL #4 / / / / / / 29.01 35.20 31.20 / / /
OURS 66.64 73.70 27.60 | 32.58 45.60 34.60 | 35.82 48.70 31.70 | 55.27 57.60 26.90

cess sheds light on the broader potential of distilling multi-model/agent systems back into a single
model/agent for inference-time efficiency.

Correlation between individual performance and helpfulness While the pretrained base model
has many strengths, it consistently isn’t the best-performing individual model in Table 1. How-
ever, this doesn’t prevent it from being helpful in the collaboration and contributing its strengths
when needed. We quantify this phenomenon with two metrics for each task: P-performance =

eee Ri P-helpfulness = C@ tae wees , where P, F, A indicate the performance

of pretrained, finetuned, aligned models when employed individually and C'(P, F, A) indicates the
performance of their collaboration (through SWITCH GENERATION). Results in Figure 4 demon-
strate that the vast majority of tasks fall into the top-left quadrant: while the pretrained model isn’t
the best when employed individually (P-performance < 0), SWITCH GENERATION leverages its
strengths to gain in collaboration (P-helpfulness > 0). This highlights the broader potential that
weak models are not useless: they are rightfully diamonds in the rough and contribute their unique
strengths when employed in the right model collaboration system.

Generalizing to unseen models The trained switcher LM f (in switch-global) has learned from
diverse tasks, contexts, and model collaboration patterns among the Tulu-v3 suite of models. We
hypothesize that f could be employed off-the-shelf for switch generation with other model settings,
in increasing difficulty and generalization gap:

¢ Setting 1: pretrained, finetuned, and aligned models in another model family, specifically
Qwen?2.5-7B (Yang et al., 2024).

¢ Setting 2: one fewer model: only pretrained and aligned of Tulu-v3.

¢ Setting 3: one more model: pretrained, finetuned, and two versions of aligned (DPO and RLVR)
of Tulu-v3.

¢ Setting 4: three specialized LM experts (Jiang et al., 2025) that are not the aligned version of each
other.

Table 3 shows that the switcher consistently generalizes to these four settings, with an average
relative improvement of 5.8%, 14.3%, 13.1%, and 3.1%. We will release the switcher model f as an
artifact and encourage readers to employ it for switch generation with their suite of models.

Generalizing to unseen tasks We Table 4: The trained swicher model generalizes to unseen

directly employ the trained switcher tasks, outperforming baselines on most tasks.
LM f (in switch-global) and com-  ——————————————————————
pare it against two strong baselines Normad Interests MATH K-Cross ARC MedQA

on six additional tasks spanning the _ preTRAINED 29.85 43.47 27.89 7.00 16.13 20.30
three task categories (Normad (Rao FINETUNED 46.40 63.30 25.75 24.60 45.22 26.50
et al., 2025), human interests (Feng ALIGNED 48.70 66.95 28.74 20.80 46.76 28.10

TEXT DEBATE 34.90 70.10 34.55 12.00 30.97 28.50
et al., 2025b), MATH (Hendrycks  Gpeepy soup «48.85 «6735 «2941.25.50 4744 28.90

et al., 2021), Knowledge Crosswords = switcn-GLopaL 50.65 69.78 37.36 25.70 48.38 31.50


Preprint

[|] _ pretrained [] finetuned aligned 0.30
e
sequence frequency treatment sequence frequency treatment ‘
effect effect 0.20 e
0.365 0.106 LLL 0.185 0.093 F . e
ec
0.503 0.098 0.136 0.059 2 oto - :
5 . e oe.
E
0.315 0.078 0.161 0.057 3
0.646 0.115 0.113 0.061 9.00 Z
e
0.536 0.115 0.198 0.095 = - r
-0.10 e
0.611 0.094 [ 0.146 0.046 On2; Ose i088 = BEB: 0:20

frequency

Figure 5: Frequency and treatment effect of 2-length (left) and 3-length (middle) switching se-
quences, as well as their correlation across three task categories for 3-length sequences (right).
SWITCH GENERATION learns to identify helpful switching patterns and frequently leverages them.

(Ding et al., 2024), ARC-challenge (Clark et al., 2018), and MedQA (Jin et al., 2021)). Results
in Table 4 demonstrate that by learning from the switching patterns across diverse tasks, SWITCH
GENERATION generalizes to unseen tasks and outperforms baselines by 3.9% on average.

Good sequences By running Table 5: SWITCH GENERATION solves 10.7% problems

SWITCH GENERATION across 18
tasks, we accumulate valuable traces
of model collaboration and switching
patterns. Within them exist many
switching sequences (e.g. “PFA”:
pretrained generates first, followed
by finetuned, followed by aligned):

that no individual model did. Color shades denote outcomes
that discover new skills, retain existing skills, no change,
and might lose skills.

Truthful AbstainQA BBH

PFA/ours correct wrong correct wrong correct wrong

if we could identify which of these all correct 3.6% 0.3% 15.7% 2.2% 74% 0.9%
sequences are good, we could 1)  >1 correct 25.3% 7.6% 50.6% 5.1% 37.1% 8.6%
directly employ these switching all wrong (104%) 52.8% 719%) 18.5% |1318% 32.2%

patterns off-the-shelf without calling
the switcher LM for efficiency and/or 2) steer SWITCH GENERATION towards employing these
sequences more often. We define two metrics for switching sequences:

¢ Frequency: in what percentage of responses was this sequence employed?

¢ Treatment effect: performance when this sequence is employed minus when not employed.

We present results for all unique 2-length and 3-length sequences as well as their correlation in
Figure 5: the most helpful sequences (with the highest treatment effect) are also among the most
frequent, and there is a positive correlation between the two with R? = 0.017. This indicates that
SWITCH GENERATION learns to identify helpful switching patterns and more frequently leverage
them for better collaboration.

New skills We hypothesize that the perfor-  Pretrain yt siknowteaya
mance gains of SWITCH GENERATION might recall
come from two aspects: |) aggregate skills that = ean aan
one of the models already has, and 2) solving _ Finetune tH erowing

® reasoning
problems that none of the models could solve
Bs Bene d Ff & creativity
individually. We present the statistics between w feedback
single-model and multi-model correctness in Align , ie mw others
Table 5: SWITCH GENERATION successfully
answers 10.7% of problems that none of the in-

0% 25% 50% 75% 100%

dividual models could, while only losing out on
8.2% of problems that one model could individ-
ually solve, netting a benefit of 2.5% through
model collaboration.

Figure 6: Roles that each model plays in their gen-
erated segments, averaged across all datasets.


Preprint

Model Roles Model responses often feature a wide range of skills that favor different model stages
(Figure 1): we investigate under SWITCH GENERATION, what are the roles of the pretrained, fine-
tuned, and aligned langauge models in generated texts? Specifically, we first identify a suite of
five skills with high frequency (knowledge recall, instruction following, reasoning, creativity, and
feedback). We then employ LLM-as-a-judge (Zheng et al., 2023), specifically the GPT-40 model,
to annotate each generated segment for one of the skills (or “others’”) given the query, trace, and the
full generated sequence. We manually examine 100 LLM annotations and find an 86% agreement
between human-LLM judgements (with the most difference in the “others” category). We average
it across datasets and report the results in Figure 6: it shows that models are largely performing the
skills that they are good at: for example, the pretrained model is most frequently used for knowledge
recall, while the aligned model is most used for reasoning. This indicates that SWITCH GENERA-
TION and the trained switcher f learns to leverage model strengths when their skills are most needed.

6 RELATED WORK

The Tradeoffs of Alignment Alignment and reinforcement learning have become an indispens-
able part of language model training: they are credited for valuable skills in state-of-the-art LMs
such as reasoning (Guo et al., 2025), safety (Zhang et al., 2024), agentic applications (Ma et al.,
2024), and more (Ouyang et al., 2022). An increasing line of research recognizes that alignment has
tradeoffs (Lin et al., 2024), that the pretrained and unaligned base models might have advantages on
skills such as creativity (West & Potts, 2025), uncertainty (Tian et al., 2023), pluralism (Feng et al.,
2024c), knowledge (Wang et al., 2025a), or even reasoning itself (Yue et al., 2025). However, we
couldn’t directly employ the base model for these domains since they struggle to follow instructions
and lack safety guardrails. We propose to make the best of both worlds by not throwing away your
base model and instead leveraging model collaboration across diverse checkpoints in the training
pipeline to fuse model strengths and complement each other.

Model Collaboration Advancing beyond training a single, generalist language model, recent re-
search is increasingly emphasizing modularity through model collaboration, where diverse (lan-
guage) models collaborate, compose, and complement each other (Feng et al., 2025a). Model col-
laboration approaches mainly vary by the level of information exchange: API-level methods such as
routing (Ong et al., 2025; Frick et al., 2025; Feng et al., 2025c; Zheng et al., 2025) and cascading
(Chen et al., 2023; Gupta et al., 2024; Yue et al., 2024), text-level methods through collaboration
(Feng et al., 2024b; Guo et al., 2024; Zhao et al., 2025; Dang et al., 2025) or competition (Du et al.,
2023; Liang et al., 2024; Zhao et al., 2024), logit-level methods with logit fusion or contrast (Pei
et al., 2023; Li et al., 2023; Mavromatis et al., 2024; Chuang et al., 2024; Mitchell et al., 2024; Liu
et al., 2024; Huang et al., 2025), and weight-level methods such as model merging (Yadav et al.,
2023; Yu et al., 2024; Huang et al., 2024; Feng et al., 2025b; Zeng et al., 2025) and Mixture-of-
Experts (Sukhbaatar et al., 2024; Diao et al., 2023; Yadav et al., 2024; Shi et al., 2025). Since
model responses are often not monolithic, featuring a diverse set of skills that favor different model
stages (Figure 1), we propose SWITCH GENERATION for the collaborative inference of pretrained,
finetuned, and aligned LMs where they take turns to generate in a response sequence. SWITCH
GENERATION is related to various model collaboration protocols (Fei et al., 2024; Shen et al., 2024;
Wang et al., 2025b) while uniquely training a switcher LM as the switching strategy, switching by
the granularity of patches, and offers generalization to unseen models as switching candidates. Our
work also highlights that we don’t need to always train new models for collaboration: byproducts in
existing model development lifecycles could be reused and repurposed for new potential.

7 CONCLUSION

We propose SWITCH GENERATION, an inference-time model collaboration strategy where multi-
ple models in the training pipeline are dynamically selected to generate text in a single response.
By training and employing a switcher LM, multiple models dynamically generate text segments
and contribute their strengths when most needed. Extensive experiments demonstrate that SWITCH
GENERATION outperforms each individual constituent models and eight model collaboration base-
lines on 13 datasets by 12.9% on average. Further analysis reveals that SWITCH GENERATION
generalizes to unseen models and tasks, as well as identifying and frequently employing helpful col-
laboration patterns. Our work uniquely highlights the huge potential of reusing by-product models
and checkpoints in current LM training pipelines that are otherwise discarded.


Preprint

REFERENCES

Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhi-
lasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, et al. The art
of saying no: Contextual noncompliance in language models. Advances in Neural Information
Processing Systems, 37:49706-49748, 2024.

Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while
reducing cost and improving performance. Transactions on Machine Learning Research, 2023.

Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, and Dan Jurafsky. Social
sycophancy: A broader understanding of Ilm sycophancy. arXiv preprint arXiv:2505.13995,
2025.

Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R Glass, and Pengcheng He. Dola:
Decoding by contrasting layers improves factuality in large language models. In The Twelfth
International Conference on Learning Representations, 2024.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
arXiv preprint arXiv: 1803.05457, 2018.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang,
Xiaoyin Che, Ye Tian, et al. Multi-agent collaboration via evolving orchestration. arXiv preprint
arXiv:2505.19591, 2025.

Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, and Tong Zhang. Mixture-of-domain-adapters:
Decoupling and injecting domain knowledge to pre-trained language models’ memories. In Pro-
ceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pp. 5113-5129, 2023.

Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha Balachandran, Tianxing He,
and Yulia Tsvetkov. Knowledge crosswords: Geometric knowledge reasoning with large language
models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association
for Computational Linguistics: ACL 2024, August 2024.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving fac-
tuality and reasoning in language models through multiagent debate. In Forty-first International
Conference on Machine Learning, 2023.

Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. Advances in Neural Information Processing Systems,
36:30039-30069, 2023.

Yu Fei, Yasaman Razeghi, and Sameer Singh. Nudging: Inference-time alignment of Ilms via guided
decoding. arXiv preprint arXiv:2410.09300, 2024.

Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.
Knowledge card: Filling Ilms’ knowledge gaps with plug-in specialized language models. In The
Twelfth International Conference on Learning Representations, 2024a.

Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov.
Don’t hallucinate, abstain: Identifying 1lm knowledge gaps via multi-llm collaboration. In Pro-
ceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 14664-14690, 2024b.

Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, and Yulia
Tsvetkov. Modular pluralism: Pluralistic alignment via multi-Ilm collaboration. In Proceedings
of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 4151-4171,
2024c.

10


Preprint

Shangbin Feng, Wenxuan Ding, Alisa Liu, Zifeng Wang, Weijia Shi, Yike Wang, Zejiang Shen,
Xiaochuang Han, Hunter Lang, Chen-Yu Lee, et al. When one Im drools, multi-Ilm collaboration
rules. arXiv preprint arXiv:2502.04506, 2025a.

Shangbin Feng, Zifeng Wang, Yike Wang, Sayna Ebrahimi, Hamid Palangi, Lesly Miculicich, Achin
Kulshrestha, Nathalie Rauschmayr, Yejin Choi, Yulia Tsvetkov, et al. Model swarms: Collabora-
tive search to adapt Ilm experts via swarm intelligence. In Forty-second International Conference
on Machine Learning, 2025b.

Tao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: A graph-based router for Ilm selections.
In The Thirteenth International Conference on Learning Representations, 2025c.

Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N Angelopou-
los, and Ion Stoica. Prompt-to-leaderboard. arXiv preprint arXiv:2502. 14855, 2025.

Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin,
Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcee’s MergeKit: A toolkit for merging
large language models. In Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina
(eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Process-
ing: Industry Track, November 2024.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1l: Incentivizing reasoning capability in Ilms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.

Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest,
and Xiangliang Zhang. Large language model based multi-agents: a survey of progress and
challenges. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intel-
ligence, pp. 8048-8057, 2024.

Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna
Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In
The Twelfth International Conference on Learning Representations, 2024.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. In International Conference
on Learning Representations, 2021.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In International Conference on Learning Representations, 2020.

Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Ef-
ficient cross-task generalization via dynamic lora composition. In First Conference on Language
Modeling, 2024.

Chengsong Huang, Langlin Huang, and Jiaxin Huang. Divide, reweight, and conquer: A logit
arithmetic approach for in-context learning. In Workshop on Reasoning and Planning for Large
Language Models, 2025.

Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, and Yulia Tsvetkov. Sparta align-
ment: Collectively aligning multiple language models through combat. arXiv preprint
arXiv:2506.04721, 2025.

Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-
ease does this patient have? a large-scale open domain question answering dataset from medical
exams. Applied Sciences, 11(14):6421, 2021.

Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brah-

man, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers
in open language model post-training. arXiv preprint arXiv:2411.15124, 2024.

11


Preprint

Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B Hashimoto, Luke
Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimiza-
tion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 12286-12312, 2023.

Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming
Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-
agent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language
Processing, pp. 17889-17904, 2024.

Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 3214-3252, 2022.

Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang
Wang, Wenbin Hu, Hanning Zhang, et al. Mitigating the alignment tax of rlhf. In Proceedings of
the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 580-606, 2024.

Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. Tuning
language models by proxy. In First Conference on Language Modeling, 2024.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Dangi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv: 1907.11692, 2019.

Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolv-
ing with the other you: Fine-tuning Ilm with sequential cooperative multi-agent reinforcement
learning. Advances in Neural Information Processing Systems, 37:15497—15525, 2024.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.
When not to trust language models: Investigating effectiveness of parametric and non-parametric
memories. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 9802-9822, 2023.

Costas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time
via perplexity optimization. In First Conference on Language Modeling, 2024.

Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. An
emulator for fine-tuning large language models using small language models. In The Twelfth
International Conference on Learning Representations, 2024.

Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E Gonzalez,
M Waleed Kadous, and Ion Stoica. Routellm: Learning to route Ilms from preference data.
In The Thirteenth International Conference on Learning Representations, 2025.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-
low instructions with human feedback. Advances in neural information processing systems, 35:
27730-27744, 2022.

Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li,
Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, et al. Byte latent transformer: Patches
scale better than tokens. arXiv preprint arXiv:2412.09871, 2024.

Jonathan Pei, Kevin Yang, and Dan Klein. Preadd: Prefix-adaptive decoding for controlled text
generation. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 10018-
10037, 2023.

Abhinav Sukumar Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. Nor-
mAd: A framework for measuring the cultural adaptability of large language models. In Luis
Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations
of the Americas Chapter of the Association for Computational Linguistics: Human Language
Technologies (Volume 1: Long Papers), April 2025.

12


Preprint

Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. Learning to decode col-
laboratively with multiple language models. In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 12974-12990, 2024.

Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin
Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, et al. Flexolmo: Open language
models for flexible data use. arXiv preprint arXiv:2507.07024, 2025.

Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. Getting more out
of mixture of language model reasoning experts. In Findings of the Association for Computational
Linguistics: EMNLP 2023, pp. 8234-8249, 2023.

Taylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha
Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. Value kaleidoscope: Engaging ai with
pluralistic human values, rights, and duties. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 38, pp. 19937-19947, 2024.

Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Roziere,
Jacob Kahn, Shang-Wen Li, Wen-tau Yih, Jason E Weston, et al. Branch-train-mix: Mixing
expert Ilms into a mixture-of-experts Ilm. In First Conference on Language Modeling, 2024.

Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and
whether chain-of-thought can solve them. In Findings of the Association for Computational Lin-
guistics: ACL 2023, pp. 13003-13051, 2023.

Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea
Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated
confidence scores from language models fine-tuned with human feedback. In Proceedings of the
2023 Conference on Empirical Methods in Natural Language Processing, pp. 5433-5442, 2023.

Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.
Can language models solve graph problems in natural language? Advances in Neural Information
Processing Systems, 36:30840-30861, 2023a.

Heng Wang, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Qinghua Zheng, and Min-
nan Luo. Detecting spoilers in movie reviews with external movie knowledge and user networks.
In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
pp. 16035-16050, 2023b.

Yike Wang, Shangbin Feng, Yulia Tsvetkov, and Hannaneh Hajishirzi. Sciencemeter: Tracking
scientific knowledge updates in language models. arXiv preprint arXiv:2505.24302, 2025a.

Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming
Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-
task language understanding benchmark. Advances in Neural Information Processing Systems,
37:95266—95290, 2024.

Ziyao Wang, Muneeza Azmat, Ang Li, Raya Horesh, and Mikhail Yurochkin. Speculate, then col-
laborate: Fusing knowledge of language models during decoding. In Forty-second International
Conference on Machine Learning, 2025b.

Peter West and Christopher Potts. Base models beat aligned models at randomness and creativity.
arXiv preprint arXiv:2505.00047, 2025.

Mitchell Wortsman, Gabriel IIharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model
soups: averaging weights of multiple fine-tuned models improves accuracy without increasing
inference time. In International conference on machine learning, pp. 23965-23998. PMLR, 2022.

Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-merging: Re-
solving interference when merging models. Advances in Neural Information Processing Systems,
36:7093-7115, 2023.

13


Preprint

Prateek Yadav, Colin Raffel, Mohammed Mugeeth, Lucas Caccia, Haokun Liu, Tianlong Chen,
Mohit Bansal, Leshem Choshen, and Alessandro Sordoni. A survey on model moerging: Recy-
cling and routing among specialized experts for collaborative learning. Transactions on Machine
Learning Research, 2024.

An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,
Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai,
Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng
Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai
Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan
Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang
Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2
technical report. arXiv preprint arXiv:2407. 10671, 2024.

Chenghao Yang and Ari Holtzman. How alignment shrinks the generative horizon. arXiv preprint
arXiv:2506.17871, 2025.

Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Ab-
sorbing abilities from homologous models as a free lunch. In Forty-first International Conference
on Machine Learning, 2024.

Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades
with mixture of thought representations for cost-efficient reasoning. In The Twelfth International
Conference on Learning Representations, 2024.

Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does re-
inforcement learning really incentivize reasoning capacity in lms beyond the base model? arXiv
preprint arXiv:2504. 13837, 2025.

Runjia Zeng, James Chenhao Liang, Cheng Han, Zhiwen Cao, Jiahao Liu, Xiaojun Quan, Qifan
Wang, Tong Geng, and Dongfang Liu. Probabilistic token alignment for large language model
fusion. arXiv preprint arXiv:2509.17276, 2025.

Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,
and Tong Zhang. R-tuning: Instructing large language models to say ‘i don’t know’. In Proceed-
ings of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 7106-7132, 2024.

Yuwei Zhang, Wenhao Yu, Shangbin Feng, Yifan Zhu, Letian Peng, Jayanth Srinivasa, Gaowen
Liu, and Jingbo Shang. Bidirectional Ims are better knowledge memorizers? a benchmark for
real-world knowledge injection. arXiv preprint arXiv:2505.12306, 2025.

Justin Zhao, Flor Miriam Plaza-Del-Arco, and Amanda Cercas Curry. Language model council:
Democratically benchmarking foundation models on highly subjective tasks. In Proceedings of
the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 12395-12450, 2025.

Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Com-
peteai: Understanding the competition dynamics of large language model-based agents. In Forty-
first International Conference on Machine Learning, 2024.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging Ilm-as-a-judge with mt-bench and
chatbot arena. Advances in neural information processing systems, 36:46595—46623, 2023.

Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P
Xing, Hongyi Wang, and Huaxiu Yao. Citer: Collaborative inference for efficient large language
model decoding with token-level routing. arXiv preprint arXiv:2502.01976, 2025.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 2299-
2314, 2024.

14


Preprint

Zifeng Zhu, Shangbin Feng, Herun Wan, Ningnan Wang, Minnan Luo, and Yulia Tsvetkov. Guess-
bench: Sensemaking multimodal creativity in the wild. arXiv preprint arXiv:2506.00814, 2025.

LIMITATIONS

SWITCH GENERATION jointly employs multiple model checkpoints in the training pipeline for col-
laborative inference, which incurs extra cost (compared to just using the aligned version) in exchange
for compositional model strengths. The extra cost could be mitigated on several fronts: 1) by em-
ploying multiple GPUs and multiprocessing for parallel text generation over batches of instructions,
the throughput is much higher compared to using a single model; 2) by calling the switching strategy
every patch (instead of every token), the switching overhead is significantly reduced and the user
could also configure the patch size to customize the cost; 3) by distilling the collaboration patterns in
SWITCH GENERATION back into the aligned model (Figure 3), we recover part of the performance
gains while cutting inference costs back to a single model.

We observe that switching by patches works better than tokens, so we employ fixed-size patches
in SWITCH GENERATION. We also observe that different tasks might need different amounts of
generated tokens; thus, the optimal patch size might also change across tasks and contexts. We treat
it as a hyperparameter for now: future work could look into flexible and dynamic adjustments of
patch sizes and switching frequency.

REPRODUCIBILITY STATEMENT

We provide extensive experiment details such as hyperparameter settings, dataset statistics, and more
in Section 3 and Appendix B. We will release the training and inference code, switcher LMs, and
experiment logs upon acceptance.

ETHICS STATEMENT

SWITCH GENERATION is a model collaboration protocol across multiple language models, so it is
susceptible to malicious contributions: for example, if the alignment datastore is compromised and
the aligned model is malicious, when used in collaboration, the system would also be seriously im-
pacted. Safety in model collaboration systems is a critical future research question, and its findings

would have great impacts on SWITCH GENERATION.
. @ begin m middle ™ end
As the base (and finetuned) versions of models gq gg%

are mostly not safety-aligned, it is possible that
having them in collaboration might override the

safety guardrails of aligned language models. ep.oure
This might have implications for red teaming 40.00%
and adversarial language modeling.

20.00%
A ANALYSIS (CONT.)

0.00%

. ‘ : pretrained finetuned aligned
Model Locations We investigate whether

pretrained, finetuned, and aligned language
models might be used more frequently in the
beginning (first one-third), middle (one-third
to two-thirds), and the end (two-thirds to end)
patches in SWITCH GENERATION. Results in Figure 7 demonstrate that the aligned model is more
frequently employed in the beginning and the end while the pretrained/finetuned model is more em-
ployed in the middle, suggesting that the middle of the response is more suited for exploration, while
the beginning/end requires instruction following and summarization that favors the aligned model.

Figure 7: Frequency of pretrained, finetuned,
and aligned models being used in the be-
gin/middle/end of the sequence.

Switching Frequency How often does the switcher f decide that it’s time to change to another
model? We plot the switching frequency and collaboration helpfulness (P-helpfulness, Figure 4) in

15


Preprint

Figure 8: it is demonstrated that the switching frequency is consistently high, indicating that the
models are actively used in collaboration. There isn’t a consistent conclusion about whether more
or less switching is better for performance.

Qualitative Analysis We present examples
where the pretrained, finetuned, and aligned © switch frequency (%) @ P-helpfulness
models did not generate a good response in- 1.25

dividually, while SWITCH GENERATION was 499 ° =
successful in generating a good response —

through collaboration in Tables 7 to9.Itshows == °% »« , ° ° e ee ° °
that SwITCH GENERATION has better delibera- °° © ° =. —_*—s
tion and more extensive “reasoning” and expla- 9.25 . Fs «a -
nation to reach a more well-rounded response. _ 0.00 L

This also suggests that while the pretrained and 9 95 ° . .

finetuned models might not be following in- ROR SP RS eee Se SS So s
structions when used individually, if the aligned LS LK FEF FFM LG'S 8 cS
model provides good context to work on, they oe &
will be helpful.

Figure 8: Switching frequency and P-helpfulness

across tasks.
B EXPERIMENT DETAILS

Dataset Details We employ 18+6 diverse

datasets spanning multiple LM capability areas for evaluation in this work. All datasets are eval-
uated with zero-shot prompting. Sycophancy uses the original prompt in Cheng et al. (2025) and
GPT-4o for evaluation: only if a response does not fall into any of the sycophantic categories we
give a score of 1, otherwise 0. AbstainQA employs abstain accuracy (Feng et al., 2024b) as the
evaluation metric. ScienceMeter is employed as an NLI task where the passage either supports or
refutes the claim. CocoNot (Brahman et al., 2024) uses the regex in the original paper to judge con-
textual non-compliance. We employ the multiple-choice setting of TruthfulQA. We employ GPT-40
to generate a description of images in GuessBench (Zhu et al., 2025) to transform it into a language
task. Movie reviews are generated ten times for each movie, five times with the IMDB summary and
five times without, employing ROBERTa-base (Liu et al., 2019) for embeddings, and calculating the
average pairwise distance. NLGraph (Wang et al., 2023a) are evaluated with 50% connectivity and
50% shortest path problems. Statistics and statistical significance tests are presented in Table 6.

Hyperparameter Details We descibe main

hyperparameter configurations in Section 3. Dataset Source ad
We run grid search for training the switcher dev test
with epoch € {1,2,3,4,5,6,7,8} and learning “gycophancy (Cheng et al., 2025) 1000 1000
rate 1 € {le—3,5e—4,2e—4, le—4,5e—5}. — abstainQa** (Feng et al., 2024b) 178 178
We then select the switcher f that leads to the — Normad (Rao et al., 2025) 500 2000
best performance on the dev set for evaluation ScienceMeter*** — (Wang etal.,2025a) 10001000
on the fest set MATH* (Hendrycks et al.,2021) 956 956
. Human Interests (Feng et al., 2025b) 400 400
AGIEval*** (Zhong et al., 2024) 1156 1156
Baseline Details Pretrained, finetuned, and = CocoNot*** (Brahman et al., 2024) 1000 1000
aligned models used individually with top — — TruthfulQa** he 200 617
— 0.7 and 512 ‘ cad 4016 WikiDYK (Zhang et al., 2025) 6849 = =765
B= Ust all maximum generated (OKeNS — vaMLU-pro** (Wang et al., 2024) 70 1000
(same as SWITCH GENERATION); PROMPT BBH*** (Suzgun et al., 2023) 1000 1000
ROUTE employs the aligned model and the fol- — PopQA*** (Mallen et al., 2023) 1000 1000
lowing prompt to select one of the three mod- —_ K-Crosswords (Ding et al., 2024) 200 1000
. . 6 . . 7 GuessBench** (Zhu et al., 2025) 250 250
els for inference: ‘You are gies an instruc Movies (Wang et al2023b) 200 200
tion and three candidate models: a pretrained — Gsmgx (Cobbe et al., 2021) 200 1000
model, a supervised fine-tuned model, and an Alpaca (Dubois et al., 2023) 10000 1000
aligned model. Please select a model that | NLGraph*** (Wang et al., 2023a) 600 600

might best fulfill the instruction. Instruction:

[instruction] You are given an instruction and Table 6: Statistics of employed datasets. *, **,

three candidate models: a pretrained model, and *** indicate the improvement of SWITCH

a supervised fine-tuned model, and an aligned (GENERATION against baselines for this dataset is
statistically significant with p < 0.1, p < 0.05,
1204 p < 0.01 with one-tailed z-test.


Preprint

model. Please select a model that might best fulfill the instruction. Response:”; ROUTELLM em-
ploys the “causal LLM classifier” approach in Ong et al. (2025) with the aligned model; TEXT
COLLABORATE employs the prompts in Feng et al. (2024b); TEXT DEBATE employs the prompts in
Du et al. (2023); LOGIT MERGE and PROXY TUNING both employs the implementation of Liu et al.
(2024), with the former approach assigns a weight of 1/3 to every model and the second is (best +
(best - worst)); GREEDY SOUP and DARE-TIES employ the implementation at MergeKit (Goddard
et al., 2024).

17


Preprint

Instruction

Pretrained
Finetuned

Aligned

Switch Generation

What location in the West Bank has been associated with the temptation of Christ
since at least the 4th century?

The Church of the Holy Sepulchre. X
Jericho X

The location in the West Bank associated with the temptation of Christ is tradi-
tionally identified as the ’Place of the Sermon” or ’Petrine Monastery” on the
top of the Mount of Olives. It is believed that this is where Jesus was tempted by
the Devil, according to the Gospels of Matthew, Mark, and Luke. The tradition
holds that the actual place where the temptation took place was on the nearby
rocky terrain known as the Garden of Gethsemane, located just below the Mount
of Olives. The Mount of Olives is a rocky hill east of Jerusalem, and the Garden
of Gethsemane is where Jesus is traditionally believed to have prayed before his
arrest. X

The location in the West Bank associated with the temptation of Christ since at
least the 4th century is traditionally considered to be the Mount of Temptation,
also known as the Quarantine. According to Christian tradition, it is believed that
Jesus was 40 days in the desert, tempted by the devil, and fasted. This mountain
is located in the Judaean Desert, between the Dead Sea and Jerusalem. The
mountain is approximately 2,700 feet above sea level, and is a popular pilgrimage
site for Christians. The site is marked by a chapel, which dates back to the 4th
century and was built to commemorate the event. The chapel has undergone
various renovations and modifications over the centuries, but the basic structure
remains a symbol of this significant event in Christian belief. It’s important to
note that the Mount of Temptation is within the West Bank, an area that has been
a point of contention in the ongoing Israeli-Palestinian conflict. Access to and
control over religious sites such as the Mount of Temptation have been a source
of tension and conflict between the two groups. /

Table 7: Example one of individual model and SWITCH GENERATION responses.

18


Preprint

Instruction

Pretrained

Finetuned
Aligned

Switch Generation

Passage 1.At the 2007 American Geophysical Union’s.meeting in Acapulco,
Mexico, some two dozen.scientists presented multiple studies arguing that
a.comet or asteroid exploded above or on the northern.ice cap almost 13,000
years ago—showering debris.across the North American continent and caus-
ing.temperatures to plunge for the next millennium..The team argues that
its idea explains multiple.observations: not only the climate cooling and
the.disappearance of the Clovis hunters, but also the.near-simultaneous extinc-
tion of the continent’s large.mammals..Not all will be convinced. Several lead-
ing.hypotheses already explain each of these three events..A change in ocean cir-
culation is generally thought to.have brought about the onset of the millennium-
long.cooling, which is known as the Younger Dryas. This.cooling might, in
turn, have caused the Clovis.hunters to disappear. And, if they had not previ-
ously.been killed by disease or hunted to extinction, the big.prehistoric beasts
may also have been doomed by this.change in climate..The new evidence comes
in the form of.geochemical analysis of sedimentary layers at 25.archaeologi-
cal sites across North America—9 of.them Clovis. Certain features of the lay-
ers, say the.team, suggest that they contain debris formed by an.extraterrestrial
impact. These include spherules of.glass and carbon, and amounts of the el-
ement.iridium said to be too high to have originated on.Earth. In addition,
the rocks contain black layers of.carbonized material, which the team says are
the.remains of wildfires that swept across the continent.after the impact..Passage
2. (...) The team argues that when the quality.and accuracy of the dating—which
was based on.radiocarbon and other techniques—is examined.closely, only three
of the 29 sites actually fall within.the time frame of the Younger Dryas onset,
about 12,800 years ago; the rest were probably either earlier.or later by hun-
dreds (and in one case, thousands) of.years..“The supposed Younger Dryas im-
pact fails on.both theoretical and empirical grounds,” says.Meltzer, who adds
that the popular appeal of the.hypothesis is probably due to the way that it
provides.“simple explanations for complex problems.” Thus,.“giant chunks of
space debris clobbering the planet.and wiping out life on Earth has undeni-
ably broad.appeal,” Meltzer says, whereas “no one in Hollywood.makes movies”
about more nuanced explanations,.such as Clovis points disappearing because
early.Americans turned to other forms of stone tool.technology as the large
mammals they were hunting.went extinct as a result of the changing climate
or.hunting pressure..But impact proponents appear unmoved by the.new study.
“We still stand fully behind the [impact.hypothesis], which is based on more than
a.confluence of dates,” says Richard Firestone, a.nuclear chemist at the Lawrence
Berkeley National.Laboratory in California. “Radiocarbon dating is a.perilous
process,” he contends, adding that the.presence of Clovis artifacts and mammoth
bones just.under the claimed iridium, nanodiamond, and.magnetic sphere de-
posits is a more reliable indicator.that an extraterrestrial event was responsible
for their.disappearance. The authors of both passages characterize the impact hy-
pothesis as A) unsupported by reliable evidence. B) interesting but difficult to
conclusively evaluate. C) more appealing to the public than to specialists. D)
controversial in the scientific community. Please select the correct answer by
choosing the letter corresponding to your choice.

3 ABCD7. Researchers have concluded that the Younger Dryas is not the most
likely cause of the sudden extinction of the large mammals. A) true B) false C)
impossible to conclude D) not yet known Answer: | X

The correct answer is C) more appealing to the public than to specialists. X
The correct answer is B) interesting but difficult to conclusively evaluate. X

Both passages characterize the impact hypothesis as controversial within the sci-
entific community. Passage | mentions that not all will be convinced by the idea
and several leading hypotheses already explain each of the three events: the cli-
mate cooling, the disappearance of the Clovis hunters, and the extinction of large
mammals. Passage 2 directly states that the Younger Dryas impact hypothesis
has been sharply contested by some specialists in relevant fields, who either did
not detect such evidence or argued that the deposits had other causes than a cos-
mic impact The correct answer is D. D) controversial in the scientific community.
The correct answer is D) controversial in the scientific community. /

Table 8: Example two of individual model and SWITCH GENERATION responses.

19


Preprint

Instruction

Pretrained

Finetuned

Aligned

Switch Generation

What is the sum of the odd integers from 11 through 39, inclusive?

25

What is the sum of the odd numbers from 11 through 49, inclusive? Response:
275

What is the sum of the odd numbers from 11 through 49, inclusive? Response:
215

What is the sum of the odd numbers from 11 through 49, inclusive? Response:
275 X

The sum of the odd integers from 11 through 39, inclusive, is 250. X

To solve the problem of finding the sum of the odd integers from 11 through 39,
inclusive, follow these steps:

1. Identify the range of numbers to be summed: 11 through 39.

2. Determine which numbers in this range are odd. Odd numbers are integers
that are not divisible by 2.

3. Sum the identified odd numbers.

Let’s identify the odd numbers in the range 11 through 39.

- 11 is odd - 13 is odd - 15 is odd - 17 is odd - 19 is odd - 21 is odd - 23 is odd
- 25 is even (not odd) - 27 is odd - 29 is odd - 31 is odd - 33 is odd - 35 is even
(not odd) - 37 is odd

Now, sum these odd numbers: 11 + 13 +15+17419+21+23+4274+29+431
+33 + 37.

Adding these numbers together:

114+ 13=24 244 15 =39 39+ 17 =56 564+ 19=75 75 +21 = 96 96 + 23 = 119
119 + 27 = 146 146 + 29 = 175 175 + 31 = 206 206 + 33 = 239 239 + 37 = 276
So, the sum of the odd integers from 11 through 39, inclusive, is 276.

The final answer is: 276 X

To find the sum of the odd integers from 11 through 39, inclusive, we need to
first identify all the odd numbers in this range. Odd numbers are those that are
not divisible by 2.

First, list the numbers from 11 to 39, inclusive. Then, identify which of these
numbers are odd:

11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33 35, 37, 39

The sum of the odd integers from 11 through 39, inclusive, is 375. We found this
sum by adding the numbers in the list, as follows:
114+134+154+174+194+214+234+25+274+294+314+33 +35 +37+39=375
v

Table 9: Example three of individual model and SWITCH GENERATION responses.

20
