1810.01048v1 [cs.CR] 2 Oct 2018

1V

arX1

Privacy-Preserving Outsourcing of Large-Scale Nonlinear
Programming to the Cloud

tAng Li*, 8Wei Du*, Qinghua Li

tDepartment of Computer Science and Computer Engineering, University of Arkansas
SDepartment of Electrical and Computer Engineering, Michigan State University
Email: ‘{angli, qinghual}@uark.edu, 'duweil@msu.edu

Abstract. The increasing massive data generated by various sources has given birth to big
data analytics. Solving large-scale nonlinear programming problems (NLPs) is one important
big data analytics task that has applications in many domains such as transport and logistics.
However, NLPs are usually too computationally expensive for resource-constrained users.
Fortunately, cloud computing provides an alternative and economical service for resource-
constrained users to outsource their computation tasks to the cloud. However, one major
concern with outsourcing NLPs is the leakage of users private information contained in NLP
formulations and results. Although much work has been done on privacy-preserving outsourc-
ing of computation tasks, little attention has been paid to NLPs. In this paper, we for the first
time investigate secure outsourcing of general large-scale NLPs with nonlinear constraints. A
secure and efficient transformation scheme at the user side is proposed to protect users private
information; at the cloud side, generalized reduced gradient method is applied to effectively
solve the transformed large-scale NLPs. The proposed protocol is implemented on a cloud
computing testbed. Experimental evaluations demonstrate that significant time can be saved
for users and the proposed mechanism has the potential for practical use.

1 Introduction

Cloud computing has gained an increasing popularity in both academia and industry communities,
and been widely used due to its huge computing power, on-demand scalability and low usage cost
[15]. It offers many services to users, such as data storage, data management and computing re-
sources via the Internet. Besides personal uses such as data storage service represented by Dropbox,
cloud computing also has enterprise applications such as big data analytics and business intelligence.
One fundamental feature of cloud computing is computation outsourcing, allowing users to perform
computations at the resource-rich cloud side and no longer be limited by limited local resources.
Despite the tremendous benefits, outsourcing computation to the cloud also introduces security
and privacy concerns. The first concern is data privacy including both input data privacy and
output data privacy [14]20]16]. The outsourcing paradigm deprives users’ direct control over the
systems where their data is hosted and computed. The data input to the cloud may contain sensitive
information such as medical records and financial asset status. The leakage of these data will breach
users’ privacy. To protect data against unauthorized leakage, data has to be encrypted before
outsourcing. Another concern is the verifiability of results returned from the cloud. Usually users
cannot oversee all details of the computations in the cloud. There do exist some motives for the

* Ang Li and Wei Du equally contributed to this work. This work was done when Wei Du was at the
University of Arkansas.


cloud service provider to behave dishonestly and deliver incorrect results to users. One motive is
that since intensive computing resources are usually needed to perform outsoursed computation
tasks, the cloud service provider might not do all the needed computations to save computing
resources. If the cloud server is under outside attacks during the computing process or suffering
from internal software failures, the correctness of returned results will also be at risk. Consequently,
the verifiability of results returned from cloud should be provided. A third concern is that the
computation at the cloud should be efficient; otherwise there is no need for users to outsource
computations to the cloud. The time needed by the client to offload the computation to the cloud
should be much less than the time needed by the client to solve the computation task by itself
(T6[2T1516).

In this paper, we investigate privacy-preserving outsourcing of large-scale NLPs with nonlinear
constraints. NLP is a general optimization problem [2]3]. For instance, finding the optimal invest-
ment portfolio is a typical NLP optimization problem subjecting to nonlinear constraints, where
an investor wants to maximize expected return and minimize risk simultaneously for investment.
In the deep learning area, researchers are always making efforts to find the optimal solution for
loss function, which can also be formulated as an NLP with nonlinear constraints [18]. NLPs with
nonlinear constraints are also common in various industry domains, such as the minimum cost of
transport and logistics, optimal design, emission-constrained minimum fuel, and so forth [2)3]. It
is very challenging for resource-limited users to solve large-scale NLPs with nonlinear constraints,
since it requires intensive computation resources.

In this work, we propose a privacy-preserving and efficient mechanism to offload large-scale
NLPs with nonlinear constraints to the cloud. To the best of our knowledge, privacy-preserving
outsourcing of NLPs with nonlinear constraints has never been studied before and this paper is the
first. We first formulate the private NLP with nonlinear constraints as a set of matrices and vectors.
Then the user generates random vectors and matrices and performs mathematical transformation
to protect the original NLP formulation. It is proved that the transformed NLP with nonlinear
constraints is computationally indistinguishable from the original one, which means that the cloud
cannot infer any useful information about the original NLP from the transformed NLP. At the cloud
side, the generalized reduced gradient method is employed to solve the encrypted NLP, which is
experimentally demonstrated to be efficient and practical. Finally, the user can verify the correctness
of the returned solution to NLP.

The contributions of this paper can be summarized as follows:

— For the first time, we propose an efficient and practical privacy-preserving mechanism for out-
sourcing large-scale NLPs with nonlinear constraints to the cloud.

— For the proposed solution, we mathematically prove that the input privacy and output privacy
of users can be protected. The solution also provides verifiability of cloud-returned results.

— The proposed mechanism is implemented, and its performance is evaluated through experiments.
The results show high efficiency and practicality of the proposed mechanism.

The rest of the paper is organized as follows. Section[2]reviews related work. SectionB]introduces
system model and security definitions. Section [4] presents how to use transformation schemes to
protect the original NLP and formal proofs are given. Section [5] applies the generalized reduced
gradient method to solve the outsourced NLP with nonlinear constraints. Section[6]shows evaluation
results. Section [7] concludes this paper.


2 Related Work

Much work has been done on privacy-preserving outsourcing of computation-intensive tasks to the
cloud. Some work focused on outsourcing arbitrary computation functions [I1J8[i], mainly using
fully homomorphic encryption (FHE) schemes such as [10]. Although theoretical guarantees of
privacy can be achieved with FHE, current FHE schemes have very high computation cost, making
them impractical for large-scale computations such as large-scale NLPs addressed in this paper.
Other work designed secure outsourcing protocols for specific problems, such as linear programming
19], system of equations [20], distributed linear programming [17], quadratic programming [21],
and linear regression [4]. Outsourcing basic mathematical computations has also been studied, such
as matrix determinant computation [13], matrix inversion [14], and modular exponentiations [7].
However, previous work mostly focused on linear systems and some other particular problems.
Outsourcing NLPs has received little attention

Very recently, we also studied securely outsourcing NLPs in our previous work [9], but that work
only considers NLPs with linear equality constraints. Different from it, this paper addresses NLPs
with nonlinear inequality and equality constraints which are more complicated and general.

3 Problem Formulation
3.1 NLPs Formulation
The general form of NLP is expressed as follows [2)3]:

P,: Minimize f(x)

subject to g(x) = 0, t=1,---,m
hj(x) <bj, f= 1,--- (1)
ay Sp Sup, K=1,---n
where x = (#1,%2,:-+ ,%p) is an n dimensional vector of variables, f(x) is a nonlinear objective

function, gi(x) = 0 are m equality constraints, and h;(x) < bj; are J inequality constraints. In this
paper, the NLP is considered as feasible indicating that there exists at least one point x* satisfy-
ing all of the inequality and equality constraints. Also, it should be noted that the inequality and
equality constraints are both of nonlinear form in this paper. NLPs appear many practical appli-
cations, such as machine learning, finance budget allocation, and some decision-making problems.
Taking the typical support vector machine (SVM) classification as an example. It is known that
SVM consists of linear and nonlinear form according to the selection of classification functions.
A large portion of the classification tasks require using the nonlinear form of hyperplanes due to
the complexity of data. As a result, the training of the SVM classifier is transformed to solve the
nonlinear function subjecting to nonlinear constraints, where nonlinear function is the loss function
of SVM model, and nonlinear constraints are nonlinear forms of hyperplanes.

3.2 System Model

The outsourcing model has two parties, the user and the cloud server, as illustrated in Fig. 1. The
user has an NLP problem to solve. However, the user cannot solve this large-scale problem due
to his limited computation power. Thus, he outsources the NLP problem to the cloud server. In


Secret Key

User
Cloud Server

Fig. 1. System model for outsourcing large-scale NLPs with nonlinear constraints.

order to protect the original NLP problem & from being known to the cloud, the user generates a
private key K to encrypt the problem @, and sends the encrypted NLP problem ®(K) to the cloud
server. The cloud server solves (A) using the generalized reduced gradient method, and returns
the solution back to the user. During the computing process, the cloud server is supposed to learn
nothing or very little information about the original NLP problem. When the user receives the
solution for (A) from the server, the user verifies its correctness. If it is incorrect, the user will
reject it; if it is correct, the user will accept it and decrypt it with the private key K to get the
solution for the original NLP problem @.

3.3 Security Model and Goal

The security concerns and threats are mainly from the untrusted cloud server. A malicious cloud
server may try to learn about the original NLP problem. It may also not follow the correct computing
process of the problem and derive a wrong solution. As a result, the security goal is two-fold: hiding
the original NLP problem from the cloud in order to protect the users privacy, and providing a
verification mechanism for the user to check the correctness of returned result so that the cloud
server cannot cheat.

3.4 Security Requirements

This section gives a formal security definition for the outsourcing protocol. Let us first look at the
scope of private information within this context. In the original NLP problem Pj, the coefficient
matrices of the equality and inequality constraints contain sensitive information. The positions of
elements in the coefficient matrices may also contain private information, e.g. the node distribution
graph of an optimal digital circuit layout path. In addition, the solution x* of the original NLP
problem P, should also be protected.

The concept of computational indistinguishability is used in this paper to design a secure out-
sourcing protocol.

Definition 1: A randomized algorithm A satisfies computational indistinguishability if and only
if for any two databases D and D’, for every probabilistic polynomial-time adversary machine M,


there exists a negligible function neg(-) such that [12]:
|Pr[MA(D)| — Pr[MA(D')]| < neg(-) (2)

where the notation MA(D) (similarly for M4(D’)) means that adversary machines have access
to the database and try to extract private information from the data. Definition 1 measures the
information leakage level of the encryption scheme that encrypts the original NLP problem. If
computational indistinguishability is achieved, the cloud server cannot learn anything significant
about the original NLP problem.

4 NLP Transformation

4.1 Input Privacy Protection

In order to protect the coefficient matrices and vectors of the constraints as shown in the general
form of NLP Pj, they are encrypted by the users privacy key Kk.

Protecting Equality Constraints Suppose the coefficient matrix and the vector of the equality
constraints in P; are denoted as G € R™*” and b € R™*!, respectively. G and b can be effi-
ciently hidden by employing matrix multiplications. In particular, the user can protect the equality
constraint matrix and vectors as follows:

=PQG
= PQb

om >

(3)

where P € R™*" is a diagonal matrix, with the elements defined as follows:
Piz = 4

Here the value of r; comes from the uniform distribution defined as:

r -N<r<N
y= F
0 otherwise

Q € R”*™inEg.(3) is a positive constant diagonal matrix, which is expressed as:

_[@ taj
a= {i iy (6)

It can be seen from Eq. (3) that the matrices G and b are masked by the multiplying a random
diagonal matrix P and a constant diagonal matrix Q. It should be noted that the rank of matrix
G stays the same due to the full rank of P and Q. From the encryption form, one cannot extract
any useful information without releasing P and Q. We will give detailed mathematical analysis of
the above transformation in the next section.


Inequality Constraints Protection The coefficient matrix of the inequality constraints is de-
noted as H € R'*”. A similar approach can be used to encrypt H as follows:

H = TSH (7)

where T € R’™! is a diagonal matrix with elements generated following the uniform distribution
defined in Eq. (5) and S € R'™’ is set to be a diagonal constant matrix.

4.2 Output Privacy Protection

The above matrix multiplication mechanism is able to protect the input privacy, but the output
privacy has not been addressed yet. The user sends the NLP to the cloud server, and will receive a
solution x* back from the cloud. The result x* might contain sensitive information, e.g., the asset
allocation strategy in a financial company. In fact, the output privacy can be easily protected by
vector addition. We can just replace the original variable vector x with the following:

Z=x4+r (8)

where the elements of vector r are also taken from the uniform distribution defined in Eq. (5). After
completing the protection of the input and output privacy, we can rewrite the original problem P,
as the following:

P2: Minimize f(z)

subject to  g;(z) = 0, i=l1,---,m
Gp < 2m < tie, kK=1,---n

where G = PQG, b = PQb, H = TSH, z =x +r, a; = ag +rp, and dy = up tre.

4.3 Structure Privacy Protection

The above matrix transformations can protect the element values within the input and output
matrix; however, the structure of the input and output matrix (i.e., positions of non-zero elements)
still needs to be protected, which might also contain sensitive information. For example, the circuit
layout is of vital importance in the area of electronics design, and one of the common methods is to
construct matrices according to the node distribution. Thus, it is easy to recover the original circuit
layout if we know the circuit matrices. As a result, the position of the elements in a matrix sometimes
will contain sensitive and valuable information which needs to be hidden. Next we will introduce a
matrix permutation mechanism to protect the position information of the original matrix.

The permutation of a matrix starts from permuting a set S. Consider a two line notation
representing an original set S and its permutation set 5” denoted as [2]:

$1,52,53°°* ,5n (10)
Here the upper line is the elements of the original set S and the bottom line is the elements from
the permutation set S’. Note that S and S’ have the same elements but with different orders. Here


Algorithm 1 Key generation.

Input:
Input size n;

Output:
Random uniformly distributed vector r;
Random uniformly distributed matrix S;
Constant matrix M;
Random permutation matrix W;

1: Generate a uniformly distributed random vector r according to Eq. (5);
2: Generate uniformly distributed diagonal matrices S according to Eq. (5);
3: Generate constant diagonal matrix M according to Eq. (6);

4: Set S = {1,2,3,--- ,n};

5: forj7=l1ton

6: select i randomly from i € (1,7);

7: swap S(i) and S(4);

8: end for

9: fori=1lton
10: for j=1ton
11: y outputs the ith element from S with (i);
12: o outputs value with oj) 43
14: end for
15: end for

16: return r,S, M, W;

we define s; = 0(s;) to represent the function mapping from Eq. (10), indicating that for an upper
line element s; as the input, the output will be the corresponding element s; in the bottom line.
Then the permutation matrix can be obtained as follows:

where o;,; is the Kronecker delta function, which is commonly used in engineering field and defined
as [5] :
0 t#Jj
Or : vd (12)
1 t=)

and 6(2) is defined as above, outputting the ith element of the permutation set. Then we can protect
the position information of matrix G and H with the following expression:

G' = xXGY

, n (13)

H’ = XHY
where X and Y are random permutation matrices generated from Eq. (11). It can be seen that
matrices X and Y are used to randomly permute the positions of rows and columns of the matrix,
respectively. Since the permutation matrices are randomly generated, they will permute the original


matrix to random-order rows and columns. As a result, the cloud server will not be able to learn
any structure information from the reordered matrices.

Up to now, we have finished transforming the original NLP, and the problem P3 can be rewritten
as:

P3 : Minimize f(z)
subject to gi (z) = 0, i=1,---,m
b

where G’ = XGY, H’ = XHY, and b’ = Xb.
Both key generation and matrix transformation are performed by the user locally, and the
procedures are summarized in Algorithm 1 and Algorithm 2, respectively.

Algorithm 2 Transformation mechanism.
Input:
Objective function f(x);
Equality coefficient matrix G and inequality coefficient vector b;
Inequality coefficient matrix H;
Output:
Encrypted objective function f(z);
Encrypted matrix G’ and vector b’;
Encrypted matrix H’;
Generate a random vector r from Algorithm 1 to obtain z=x+r, f(z) = f(x+r);
Generate two random diagonal matrices P and T from Algorithm 1;
Generate two constant diagonal matrices Q and S from Algorithm 1;
Calculate G = PQG and b = PQb:;
Calculate H = TSH;
Generate matrices X and Y from line 4 to 15 in Algorithm 1 , corresponding to matrix W;
Calculate G’ = XGY and b’ = Xb;
Calculate H’ = XHY;
return f(z),G’,b’,H’;

4.4 Privacy Analysis

In order to show why the aforementioned transformation schemes can protect input privacy and
output privacy, next we will derive a theorem proving that the input matrix and the output vector
are computationally indistinguishable from a randomly generated matrix and vector, respectively.

Theorem 1. Let the elements of R € R™*”" and r € R"*! be generated from the uniform
distribution defined in Eq. (5). Then the matrices G = PQG and H = TSH are computationally
indistinguishable from a random matrix R, and vector z = x+r is computationally indistinguishable
from a random vector r.


Proof: Firstly, to prove the computational indistinguishability between matrices G and R, we
need to show for any probabilistic polynomial-time adversary machines M having access to database
M“, it can only tell the difference between Gi j and R;; with negligible success probability, where
Gig is the element in the ith row and jth column of G and R;,; is the element in the ith row
and jth column of R. Suppose the adversary machine M sending inquiry to database D, it will
output Pr[M“(D)], which is the probability of the element coming from a specific database D. It is
obvious that if M determines the element coming from a specific database D with full confidence,
it will output 1. Suppose the element G; j is chosen from G by adversary machine M, the success
probability to identify it from G is expressed as the following:

A 1 a
Pr(MA(G,;)| = gPrl-N < Giz < N]

1 * a
Here is brief explanation of the above expression, if the inquiry Gij by M is within the range
(—N,.N), the probability of it coming from G is 1/2 since it is possible for both R and G owing
elements falling in the range (—N, NV). However, if the inquiry G;,; by M is out of the range (—N,N),
it must be from matrix G;,; that the probability is 1. To calculate Eq. (15), we first have that

Pr(Gij > N] = PrlQwPiaGi > N]

N
= Pr|PiiGiy =a

N N
aPr[Pii = aj) * (1 —a)Pr[Pii < CG, (16)
N —N
< 2 ae = aS
< aPr[Pis = Ge] + (1 -a)PriPaa S$
OF,

where P;,,; and Q;,; is the element in 7th row and 7th column of matrix P and Q, respectively; L is

the maximum value of elements in G, C is the constant in Q defined in Eq. (6), a is the probability

for the element G;,; to be positive and 1 — a is the probability for the element G;,; to be negative.
In addition, we can similarly obtain :

2 1

Pr[G;,; <-N|=1-— 1
nlGig <-N]=1- (17)

Thus, the success probability for adversary machine M determining Gi, j chosen from matrix G
in Eq. (15) will be:

» 3
Pr(MA(G;,; 1
rMAG)) <5 - > (18)
The success probability for R;,; to be identified by the distinguisher M is expressed as follows :
B 1
Pr[MA(Ri3)] = 5 (19)


The comparison of Eq. (18) and Eq. (19) will lead to

x A CL—-1
|Pr(M4(G;;)| — Pr[MA(Rj3)]| < CL (20)
By comparing Eq. (20) and Eq. (2), we can define
CL—-1
neg(0) = 7 (21)

Since we can choose any positive constant C’, we can choose a value of C' that makes CL be close
to 1, which will make Eq. (21) be a negligible value. As such, the encrypted matrix G and random
generated R. meet the property of computational indistinguishability. Similarly, we can also prove
the computational indistinguishability between H, R, and z, r, respectively. The proof is complete.

It can be concluded from Theorem 1 that even if the adversary machines have full access to
the data, it still cannot learn any useful information. As a result, sending the encrypted data via
the transformation mechanism to the cloud will not release any private information, proving the
security of the proposed protocol.

5 Secure Outsourcing Algorithm For Encrypted NLPs

In this section, we will design an efficient outsourcing algorithm to solve the encrypted large-scale
NLPs. To solve large-scale NLPs, the generalized reduced gradient (GRG) method is employed
to get the optimal solution of Ps. The strategy of GRG is based on an iterative way to repeatedly
generate feasible improving directions optimizing NLPs.

5.1 Gradient Decent Method for Unconstrained NLPs

Before delving into details of the GRG to solve large-scale NLPs subjecting to a system of con-
straints, we first introduce how to find the optimal solution of an unconstrained objective function.
A popular and widely used algorithm for solving unconstrained problem is gradient decent method.
Suppose f(z) is convex and differentiable within the neighborhood of z9. The decent method is to
produce a sequence of z, (k = 0,1,2,...) which can continuously decrease the objective function,
expressed as:

Zk+1 = Ze + Ad (22)

where d is called search direction, k = 0,1, 2,... is iteration number, and \ is termed as step length.
The decent method means that for every iteration of the algorithm, we must have

f(Ze+1) < f (Ze) (23)

except when zx is already an optimal solution of the objective function.
The convexity of the objective function indicates that the search direction d must satisfy the
following expression:
Vi (ax)d <0 (24)

It can be seen from Eq.(24) that search direction d must form an acute angle with the negative
gradient, thus as such it is called as a decent direction. As a result, an obvious choice for d is along


the negative gradient direction —V f (zs). Once the selection of the search direction is completed,
next step is to determine step size as the following:

A = arg min,s9 f(z + sd) (25)

An exact line search method can be used to solve the one variable optimization task, just as Eq.
(25). However, the above gradient decent method cannot be applied to P3 due to the existence
of constraints. The reason is that if we directly move zz, along the negative gradient direction
Zk+1 = Ze —AVf (zx), the feasibility of the constraints may be destroyed. As a result, it occurs to
us that we have to figure out a way to generate a series of feasible directions gradually approaching
the optimal solution of the constrained large-scale NLPs, which will be shown as next section.

5.2 Generalized Reduced Gradient Method for Constrained NLPs

GRG method is robust and efficient in solving large-scale nonlinear problems practically. The con-
straints in Ps includes both equality and inequality equations. In fact, we can make all of the
inequality constraints h;(z) < 6j,j = 1,--- ,1 to equality constraints by introducing a bunch of
slack variables as follows:

26
Thus we can rewrite P3 in the following general form:
P,: Minimize f(z, s)
subject to é;(z,s)=0, i=1,---,m,---,m+l
ak < 2% Ste, K=1,---n (27)

0<sp<ow, p=1,-:-l

where €;(z, s) = 0 is the combination of equality and inequality constraints in P3. For simplicity of
notation, we can use y = (z,s) to represent the variable vector, and v < y < w to denote the range
of the variables. It should be noted that for some slack variables, the corresponding components of
w can be set as infinite.

As described before, the constraints are of nonlinear forms. To make the logic more clear and
algorithm more understandable, we will first describe how to solve the linear constraints and then
extend to the nonlinear forms of the constraints. Suppose the equality constraints in Py, are in
the linear form that Ey = c,y > 0, where E € R(™*+)*”" and ¢ € R(™*)*!, In addition, an
nondegeneracy assumption is made here that every m+ / columns of the matrix E are linearly
independent and every basic solution to the constraints has at least m+ strictly positive values.
This assumption can be easily satisfied since we can apply elementary transformation of matrix
which will reduce the matrix be composed of independent columns or rows. With this assumption,
every feasible point to the constraints will have at most n — m — I variables with values setting to
zero. For any feasible point y, it can be partitioned into two groups that y = (yg, yy), where yp
has the dimension m + / termed as basic variables, and y, with dimension n — m — I is called as
non-basic variable. Accordingly, matrix E can be decomposed as E = [Eg, Ey], where Ep and Ey
are the columns corresponding to yp and yy, respectively.



From the algebra we know that for each stage, the optimization of this problem is only dependent
on the non-basic variables yj, since basic variable vector yz can be uniquely determined from yy.
A simple modification of the gradient decent method will provide a feasible improving direction d
to optimize the objective function. A feasible improving direction d at the point y must follow:

Ed = 0, (a)

Vily"a <0, 0) ”)
where V f(y)” is the gradient vector of objective function f(y) at point y. Eq. (28a) means that if a
feasible point y moves along the direction d, the feasibility of the constraints will not be damaged.
Eq.(28b) indicates that moving along d will make the objective function f(y) approach the optimal
point. The reduced gradient method as the following will find such moving direction d that satisfies
Eq.(28).

The gradient vector corresponding to yy (also called as reduced gradient) can be found by the
following expression:

r’ =Vyf(y)’ —Vaef(y)’ Es En (29)

where Vy f(y)? is the gradient vector of Vf(y)? that corresponds to yy and Vef(y)” is the
gradient vector corresponding to yg. From above reduced gradient, we can construct the feasible
moving direction dy that will move yy + Ady in the feasible working space, where dy can be
determined as the following:
— 1% rT, <0
dui = ‘ ~ (30)

—yniri T% > 0

where dy; is the ith element of dy, r; is the ith element of r7, and yni is the 7th element of yy. Eq.
(30) provides the rules for finding improving feasible direction for non-basic variables y),. Once the
improving feasible direction for yy is determined, we can get the corresponding moving direction
dz for yp by expanding Eq. (28a):

Endy + Epdg = 0

dp = —E,' Endy BL)
Eq. (31) shows that dg can be uniquely calculated from dy, and the moving direction is composed
that d = [dg, dy]. It can be proved that d = [dg, dy] satisfies Eq.(28a) and Eq.(28b), indicating
both feasibility and improvability will be achieved for d.

The reduced gradient method dealing with the linear constraints can be generalized and extended
to address the nonlinear constraints. Similar to linear constraints, we first partition the variables
into basic and non-basic variable vector as y = (yz,yy), and the corresponding Jacobi matrix of
€(y) in Py can also be grouped into:

e e le
=) (32)
B N

oe

and a nondegeneracy assumption is made here that for any point y, ao © ROm+)x(m+) ig non-
YB

singular.


For the case of nonlinear constraints, the reduced gradient r™ with respect to yy is expressed
as:

de _, dé

T T T
=V —V — 33
fT = Vfl)" ~ Vell (5) 5 (33)
Now we specify the direction dy as follows:
0 yniri > O and yni = Vi

—ynir; otherwise

where v; is the lower bound of the variable y; and w; is the upper bound of the variable y;. However,
the difference with the linear form is that yj, moves a straight line along dy, the nonlinear form of
the constraints requires y, move nonlinearly to continuously walk in the feasible space formed by
the constraints. To address this, we can first move y,y along the direction defined by Eq. (34), then
a correction procedure is employed making y, return to working space to satisfy the feasibility
of the constraints. Once a tentative move along dy is made, the following iterative method can
be used for the correction. Supposing y, is the current feasible point, we first move the non-basic
variable vector yy(n41) = Ynx + Adwe, to return point y = (Ygx,Yn(e41)) near y;, back to the
constraint space, we can solve the following equation:

€(Y pe, Yn(e+1)) = 9 (35)

for yp, Where yy (441) 1s fixed. This is done by the following iterative procedure, which is described
in Algorithm 3 from line 11 to 14:

J€(Y pp, , Yn(k-+1))

—te 5 36
OY an ) (Yer, YNn(k-+1)) (36)

YBkji1 — VBR; — (
where yg, is the basic variable vector of yg in the jth iteration step according to Eq. (36). When
this iterative process produces a feasible point y g(,41), we have to check if following conditions are
satisfied:

F(¥Bce+1) Yn (e+1)) < [VBR Ye)

(37)
V SY B(R+H) Yn(k+1) SW

If Eq. (37) holds true, it indicates that the new point is feasible and improvable. Then we set
Yeti = (Ve(e+1):Yn(e+1)) a8 a new approaching point, otherwise we will decrease the step length
A when we first make the tentative move for yy, and repeat the above iterative process. The
procedure of generalized reduced gradient method is summarized as Algorithm 8.

Regrading verification of the correctness of the returned result, the users can apply KKT con-
ditions of P3 [8] to check if the return result is valid or not.

6 Performance Evaluation

In this section, we evaluate the performance of our proposed secure outsourcing protocol for large-
scale NLPs with nonlinear constraints. For the experimental setup, the client side is implemented on
a computer with Intel(R) Core(TM) i5-5200 U CPU processor running at 2.2 GHz, 8GB memory.


Algorithm 3 Secure outsourcing scheme for large-scale NLPs
Input:
Starting point yy that 6(y9) = 0;
Output:
Optimal result y* for P4;
1: Initialize k = 0;
2: Decompose Yo = (yg; Yno);

: Calculate Jacobi matrix of €(y,) and decompose the Jacobi matrix oe

Oy B’ OY n

w

e
ay =

) corre-

sponding to (yg0,Yno);

4: Compute dyo from Eq. (34);
6: Choose A > 0 and compute yy(441) = Yn~ + AAN«:;
ie If not V< Yn(pq1) SW:
A= 1/2);
8: go to (6) and repeat;
9: Initialize j = 0;
10: Let Ypj = Yer:
Il: while || €(¥3;,¥n(e+41)) |l1> €
12: Let B= (Fen Yer) 1,
OY Bk,
13: Yayjt1) = VB; ~ E€(¥ Bj, Yn (n41));
14: jg=ot4+]hs
15: end while;
16: If Eq. (37) holds true:
17: YB(k+1) = YB; 3
18: Yeti = (Yece+i)» Yn(e41))3
19: else:
20: go to (6) and repeat;
21: k=k+1,;
22: Calculate dy, from Eq. (34);
23: end while;

24: Let y* = (ype, Ye);
25: return y*;

For the cloud side, the experiment is conducted on a computer with Intel(R) Core(TM) i7-4770 U
CPU processor running at 3.40 GHz, 16GB memory. We implement the proposed protocol including
both the client and cloud side processed in Python 2.7. We also ignore the communication latency
between users and the cloud for this experiment, since the computation dominates running time as
demonstrated by our experiments.

We randomly generate a set of test cases that cover the small and large sized NLPs with
nonlinear constraints, where the number of variables is increased from 1000 to 16000. The objective
function here is randomly generated second-degree polynomial function, and nonlinear constraints
are randomly generated with first-degree polynomial functions for equality constraints and second-


degree polynomial functions for inequality constraints, respectively. All these test cases are carefully
designed so that there are feasible under corresponding nonlinear constraints.

For the experiments, we first solve the original NLP in the client side, then solve the encrypted
NLP in cloud side. Table [I] shows the experiment results, and each entry in this table represents
the mean of 20 trials. As illustrated in this table, the size of original NLPs is reported in the first
three columns. Besides, several parameters are adopted to evaluate the performance of proposed
protocol. Toriginal is defined as the time to solve original NLP by client side. The time to solve
encrypted NLP is divided into time for the cloud server Tyjouq and time for the client Tyrent. Tetoud
is defined as the time that cloud used to operate the encrypted NLP by the cloud server. Terjent
is the time cost to encrypt and decrypt the original NLP by the client. Furthermore, we propose
to assess the practical efficiency by two metrics calculated from Topiginat, Tetient aNd Tetoua. The

Lopiginal

speedup is calculated as , representing time savings for the client to outsource the NLP to
the cloud using proposed protocol. The speedup is expected to be greater than 1, otherwise there is
no necessity for the client to outsource NLP to the cloud server. The cloud efficiency is measured
as Foriginal indicating the time savings enabled by the cloud. It is expected that the encryption of
the problem should not introduce great overhead for solving the large-scale NLP. Moreover, due to
more powerful computation capabilities of cloud server, the cloud efficiency is expected be grater
than 1.

It can be seen that from the Table [I] that the encryption can be finished in a very short time
by the client. For instance, the time consumption of the encryption for the problem with 16000
variables is only 166.68s. However, the time cost to find the optimal solution by the cloud server
is much longer but reasonable, and increases rapidly with growing number of variables. As shown
in the penultimate column of the table, the speedup of proposed protocol increases dramatically
when the size of the problem gets larger. Hence, a substantial amount of time can be saved for
the client by proposed protocol. For example, the speedup is 49.66 for the problem with 16000
variables, indicating 97.9% of time is saved for the client. The cloud efficiency is shown in the
last column, and it can be seen that the cloud efficiency increases with the increasing size of
the problem, indicating the powerful computation capabilities of cloud server. Consequently, the
experiment results demonstrate our proposed secure outsourcing protocol is practical and efficient.

Table 1. Performance Evaluation

Test Cases Original NLP} Encrypted NLP |Speedup|Cloud Efficiency
#|# variables|# equality constraints|# inequality constraints] Toriginal(sec) |Tctoua(sec)|Tetient(sec)| Re Tgaiptocd

1 1000 300 3.17 0.09 : 1.39
a 7a ras ra
3 4000 1200 182.05 112.38 4.48 ‘ 1.62
4 8000 2400 1236.42 695.79 36.50 34.33 L.77
5} 12000 3600 2777.04 1368.55 75.32 37.02 2.03
6}| 16000 4800 4800 8245.51 3720.11 166.68 49.66 2.21

7 Conclusion

In this paper, for the first time, we design an efficient and practical protocol for securely outsourcing
large-scale NLPs with nonlinear constraints. The transformation technique is applied to protect


the sensitive input/output information. In addition, we adopt the generalized reduced gradient
method to solve the transformed NLP. A set of large-scale simulations are performed to evaluate
the performance of proposed mechanism, and results demonstrate its high practicality and efficiency.
It is expected that the proposed protocol can not only be deployed independently, but also serves
as a building block to solve more sophisticated problems in the real world.

References

10.
11.
12.
13.
14.

15.
16.

17.

18.

19.

20.

21.

. Barbosa, M., Farshim, P.: Delegatable homomorphic encryption with applications to secure outsourcing

of computation. In: Proceedings of the 12th conference on Topics in Cryptology. pp. 296-312 (2012)

. Bazaraa, M.S., Sherali, H.D., Shetty, C.M.: Nonlinear programming: theory and algorithms. John Wiley

& Sons (2013)

Bertsekas, D.P.: Nonlinear programming. Athena scientific Belmont (1999)

Chen, F., Xiang, T., Lei, X., Chen, J.: Highly efficient linear regression outsourcing to a cloud. IEEE
Transactions on Cloud Computing 2(4), 499-508 (2014)

Chen, F., Xiang, T., Yang, Y.: Privacy-preserving and verifiable protocols for scientific computation
outsourcing to the cloud. Journal of Parallel and Distributed Computing 74(3), 2141-2151 (2014)
Chen, X., Huang, X., Li, J., Ma, J., Lou, W., Wong, D.S.: New algorithms for secure outsourcing of
large-scale systems of linear equations. IEEE Transactions on Information Forensics and Security 10(1),
69-78 (2015)

Chen, X., Li, J., Ma, J., Tang, Q., Lou, W.: New algorithms for secure outsourcing of modular expo-
nentiations. IEEE Transactions on Parallel and Distributed Systems 25(9), 2386-2396 (2014)

Chung, K., Kalai, Y., Vadhan, S.: Improved delegation of computation using fully homomorphic en-
cryption. In: 30th Annual Cryptology Conference. pp. 483-501 (2010)

Du, W., Li, Q.: Secure and efficient outsourcing of large-scale nonlinear programming. In: Communi-
cations and Network Security (CNS), 2017 IEEE Conference on. IEEE (2017)

Gentry, C.: Fully homomorphic encryption using ideal lattices

Kalai, Y., Raz, R., Rothblum, R.: How to delegate computations: the power of no-signaling proofs
Katz, J., Lindell, Y.: Introduction to modern cryptography. CRC press (2014)

Lei, X., Liao, X., Huang, T., Li, H.: Cloud computing service: The case of large matrix determinant
computation. IEEE Transactions on Service Computing 8(5), 688-700 (2015)

Lei, X., Liao, X., Huang, T., Li, H., Hu, C.: Outsourcing large matrix inversion computation to a public
cloud. IEEE Transactions on Cloud Computing 1(1), 1-1 (2013)

Murugesan, S., Bojanova, I.: Encyclopedia of Cloud Computing. John Wiley & Sons (2016)

Ren, K., Wang, C., Wang, Q.: Security challenges for the public cloud. IEEE Internet Computing 16(1),
69-73 (2012)

Shen, W., Yin, B., Cui, X., Cheng, Y.: A distributed secure outsourcing scheme for solving linear
algebraic equations in ad hoc clouds. IEEE Transactions on Cloud Computing (2017)

Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization and momentum in
deep learning. In: International conference on machine learning. pp. 1139-1147 (2013)

Wang, C., Ke, R., Wang, J.: Secure and practical outsourcing of linear programming in cloud computing
Wang, C., Ren, K., Wang, J., Wang, Q.: Harnessing the cloud for securely outsourcing large-scale
systems of linear equations. IEEE Transactions on Parallel and Distributed Systems 24(6), 1172-1181
(2013)

Zhou, L., Li, C.: Outsourcing large-scale quadratic programming to a public cloud. IEEE Access 3,
2581-2589 (2015)
