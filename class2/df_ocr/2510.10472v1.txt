arXiv:2510.10472v1 [ces.CL] 12 Oct 2025

arXiv preprint

FML-BENCH: A BENCHMARK FOR AUTOMATIC ML
RESEARCH AGENTS HIGHLIGHTING THE IMPORTANCE
OF EXPLORATION BREADTH

Qiran Zou"! Hou Hei Lam*!? Wenhao Zhao! Yiming Tang! Tingting Chen!
Samson Yu! Tianyi Zhang? Chang Liu? Xiangyang Ji? Dianbo Liu *!

‘National University of Singapore Tsinghua University *University of Minnesota

giranzou@u.nus.edu, linhx22@mails.tsinghua.edu.cn
{wenhaozhao, yimingtang, tingting.c, samson.yu}@u.nus.edu
zhan9167@umn.edu, {liuchang2022,xyji}@tsinghua.edu.cn
dianbo@nus.edu.sg

ABSTRACT

Large language models (LLMs) have sparked growing interest in automatic ma-
chine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particu-
larly promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing bench-
marks tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent’s scientific capa-
bilities in machine learning research. They also suffer from limited task diver-
sity, an overemphasis on application-oriented tasks over fundamental research
problems, and limited scalability to realistic research settings. To address these
limitations, we introduce FML-bench, a benchmark designed to evaluate auto-
matic machine learning research agents on 8 diverse and fundamental machine
learning research problems. It reduces coding burden, emphasizes fundamen-
tal problems rather than specific use cases, offers high task diversity, and is ex-
tensible to real-world machine learning GitHub repositories. Furthermore, we
present a unified evaluation framework with five complementary metrics, de-
signed to comprehensively assess agent performance on our benchmark. We
evaluate state-of-the-art automatic research agents on FML-bench, and find that
agents employing broad research exploration strategies outperform those focus-
ing on narrow but deep exploration. These findings suggest that emphasiz-
ing the breadth of exploration may lead to more effective research outcomes
than focusing solely on incremental refinement. Our benchmark is available at:
https://github.com/gqrzou/FML-bench

1 INTRODUCTION

Large language models (LLMs) have catalyzed a resurgence of interest in automatic machine learn-
ing (ML) research agents that assist or carry out parts of the scientific discovery workflow. These
agents not only support hypothesis generation, coding, and experiment management, but also in-
creasingly act as collaborators in discovery by providing complementary perspectives that can ac-
celerate machine learning research across domains. Within this landscape, agents that automatically
propose ideas and run experiments are particularly compelling (Lu et al.|[2024} {Yamada et al.|[2025).
They close the loop from ideation to empirical validation to maximize automation and to speed up
research cycles. Compared to settings that only elicit ideas and then use LLMs or humans to as-
sess “novelty” and “feasibility” which often diverge from real-world utility, this approach evaluates

* Equal contribution. + Corresponding author.


arXiv preprint

Task Specification

© Codebase Cy

GitHub_Repo_Sample/
|= models/
|- train/
|= eval
|- train.py
|= eval. py

( © Task Description

You are working with the
Continual-Learning repo’s SI
baseline to improve accuracy

Baseline Results Rg
{"splitMNIST":
{"Accuracy": @.2710,

and reduce forgetting on +
splitMNIST (5 tasks x 2
classes, single-head over 10

Target Metrics
classes). Your goal is ... @ Tara

Accuracy (Precision, ..)

(@ Suggested Files for Editing © Protected Files >=] Experiment CMD List
GitHub_Repo_Sample/
|- eval/utils.py

|= eval. py

1. python train.py \
--data=splitMNIST
2. python eval.py

GitHub_Repo_Sample/
|- models/target_model.py
|-  train/trainer. py

Input to Agent

Propose Hypothesis
Update Codebase

Figure 1: Overview of FML-bench. FML-bench includes 8 fundamental machine learning re-
search tasks, designed to evaluate agents’ capabilities in solving machine learning research prob-
lems. Agents are assessed on their ability to solve machine learning problems through iterative
research.

Research Iterations

Generate Code
Modification

| Code Modification £

| Updated Codebase |
G :

agents based on actual experimental outcomes, providing objective and quantitative evidence of their
effectiveness (Wang et al.}{2024a 2024 2024).

Despite rapid progress, existing benchmarks offer an incomplete picture of research competence, as
shown in Tab.}1| Most focus on Kaggle-style, use-case-oriented tasks and emphasize engineering
execution (e.g., feature engineering, standardized model training, and optimization) while paying
limited attention to evaluating an agent’s ability to tackle fundamental machine learning research
problems, such as representation learning and generalization
2024). Moreover, some benchmarks provide only raw data without
baseline code (Chan et al.) (2024), making it difficult to systematically assess agents’
research capabilities while introducing coding barriers that can obscure academic merit (e.g., when
sound ideas fail due to engineering pitfalls). In addition, even when baseline codebases are pro-
vided, they are often handcrafted and tightly formatted (2025),
which hinders their scalability. Because adapting them to new tasks usually requires substantial
re-engineering to fit their benchmark design, rather than allowing direct use of existing codebases.

To address these gaps, we introduce FML-bench, a benchmark designed to evaluate automatic ML
research agents on fundamental ML problems. FML-bench comprises 8 diverse tasks (Fig.|1) cho-
sen to reflect bottlenecks that repeatedly surface in modern ML. The design follows four principles.
1) Fundamental ML problems. Designed tasks target core scientific challenges rather than ap-
plication products or leaderboard scoring, keeping the focus on research questions. 2) Real-world
codebases. Tasks are instantiated from existing research repositories, mirroring typical practice
where new ideas are tested by adapting prior code. 3) Extensibility by construction. The bench-
mark can easily incorporate machine learning GitHub repositories that support end-to-end training
and evaluation, requiring only minor output-format adapters. 4) Low coding barrier. Agents are
not required to build entire codebases from scratch, but can start from provided baselines. This setup
enables agents to focus on scientific advances in algorithms and architectures rather than on purely
engineering effort.

The tasks included in FML-bench span a broad set of foundational problems (Wang et al.
2021): generalization (cross-domain transfer), data efficiency (learning
from few samples), representation learning (discovery of meaningful features), continual learning
(knowledge retention over time), causality (intervention reasoning), robustness and reliability (re-
sistance to adversarial corruption), privacy (protection against information leakage) and fairness and
bias (equitable treatment across groups). Agents are expected to propose new or improved ML
methods that deliver stronger empirical results than baselines across these eight tasks.


arXiv preprint

Table 1: Comparison of ML agent benchmarks across key design goals and agent require-
ments. Repo refers to the repository, and Comp denotes Competition. *: In MLAgentBench, only
part of the tasks meet this requirement; users must prepare baseline and evaluation code even when
some tasks are based on real-world Kaggle repositories.

Criterion Ours MLE-Bench MLAgentBench ML-Dev-Bench DSBench
Design Goals

Fundamental ML Problem Focus v x x x x
Real-World Repo/Comp v v v* x v
Low Coding Barrier v x v v x
Scalability via Existing Repo/Comp v v x x v
Requirements to Agent

Understand Codebase v x v v x
Understand Data x v x x v
Execute Arbitrary CMD with Args v x v v x
Execute Multi-step CMD List v x x x x

To evaluate agents holistically, we formalize five complementary metrics that capture different facets
of research competence. Utility measures empirical performance improvement and serves as the pri-
mary objective. Diversity quantifies the variety of code modifications proposed; empirically, broader
exploration often correlates with larger gains. Academic contribution rate distinguishes academic
modifications (e.g., new losses, architectures, or training schemes) from engineering modifications
(e.g., hyperparameter tuning), rewarding agents that prioritize scientifically meaningful changes.
Cost accounts for computational and time expenditure. Step success rate captures the fraction of
runs that produce valid results without bugs, reflecting an agent’s reliability in multi-step work-
flows. Different tasks emphasize different subsets of these metrics so that the aggregate evaluation
reflects the spectrum of capabilities required for fundamental ML research.

We evaluate several state-of-the-art automatic research agents and LLMs on FML-bench. A central
finding concerns agent strategy: once the basic requirements for both exploration breadth and depth
are met, broader exploration proves more effective. Generating a wider variety of ideas more reli-
ably leads to successful methods than repeatedly refining a single one, and we observe a positive
correlation between idea diversity and performance improvement. Besides, we find that Gemini-2.5-
Pro outperforms GPT-5 under our protocol. Finally, while CLI-style agents such as Claude Code
offer general-purpose flexibility, they often fail to complete multi-step tasks due to early termination,
where the model stops despite further actions being possible. This suggests that, although flexible,
CLI-style agents are less suitable for automatic machine learning research than agents specifically
designed for it.

We summarize our contributions as follows:

e We construct FML-bench, a benchmark centered on a diversity of fundamental ML
problems instantiated in real-world codebases, closing gaps left by use-case-oriented,
engineering-heavy evaluations.

¢ We propose a five-dimensional evaluation protocol covering utility, diversity, academic
contribution rate, cost, and step success rate, jointly measuring empirical progress, research
quality, and reliability.

¢ We provide empirical insights on research strategy (breadth vs. depth of exploration), quan-
tify the role of diversity in driving gains, and report comparative results across leading agent
frameworks and LLMs, offering guidance for practical agent design.

2 RELATED WORKS

2.1 AUTOMATIC AI AGENTS

Recent advances in large language models (LLMs) have enabled research agents to support core
components of the scientific workflow. These agents are capable of generating and prioritizing
research ideas, retrieving and synthesizing literature, and simulating peer review processes. For

instance, SciMON 2024a) and Nova (Hu et al.|/2024) implemented frameworks for
generating diverse and novel research ideas. AutoSurvey (Wang et al.||2024b) presented an au-


arXiv preprint

tomated literature review framework that performs retrieval over a large arXiv corpus, followed
by outline planning and section drafting using specialized models. Meanwhile, AgentReview
employed LLM agents to simulate peer reviews, rebuttals, and committee discussions,
offering insights into the dynamics of academic decision-making.

Recent efforts are moving beyond assistance toward fully automatic research agents. These sys-
tems aim not only to support researchers but to generate ideas, implement them, run experiments,
and refine approaches without human supervision. One representative system is AIDE
(2025), a tree-search agent that optimizes user-defined metrics by iteratively editing and evaluating
code, though it executes only one file and modifies a specific target file per iteration. TheAIScien-
tist represents an independent line of work, demonstrating end-to-end autonomy
across the research process, including idea generation, implementation, experimentation, analysis,
and manuscript drafting. Its improved version further reduces reliance on
hand-crafted templates, enhancing generality across tasks. Similarly, the AgentLaboratory executes
a full pipeline for automatic research, but its evaluation is limited to relatively simple research ques-
tions. Separately, AlphaEvolve adopts an evolutionary approach, iteratively
refining and selecting promising ideas through variation and empirical evaluation. Beyond the com-
puter science domain, a growing number of research agents have been developed for other fields,
including chemistry, where they are used to investigate and optimize chemical processes

2023 2024), and biomedical science, where they have been applied to the
discovery of novel nanobodies (Swanson et al.|/2024).

2.2 BENCHMARKS FOR ML AGENTS

While recent benchmarks have begun to evaluate agents on code-intensive tasks, they remain limited
in both scope and flexibility. MLAgentBench includes 13 machine learning en-
gineering tasks, but most are implemented as single-file scripts, which is not practical for real-world
scenarios. In addition, it requires setting an individual evaluator for each task and lacks support,
limiting its scalability to support more tasks. MLE-Bench covers 75 Kaggle
competitions and assesses whether agents can function as machine learning engineers. It empha-
sizes tasks such as data pipeline management, experiment orchestration, and submission formatting,
which may shift focus away from core machine learning understanding. ML-Dev-Bench
places greater emphasis on engineering aspects such as dataset loading and API inte-
gration. It evaluates agents’ ability to improve existing baselines only in performance tests, which
are relatively simple due to narrow task scopes like classification and segmentation, and the use of
fixed starter files. In contrast, our benchmark includes tasks spanning diverse machine learning do-
mains. DSBench aggregates 466 data analysis tasks and 74 modeling tasks from
ModelOff and Kaggle, focusing on problem-solving within data science workflows. By compari-
son, our benchmark focuses on 8 diverse and fundamental machine learning research problems. It
is built on real-world codebases, thereby providing practical challenges and strong extensibility by
construction, while maintaining a low coding barrier.

3. UNIFIED EVALUATION FRAMEWORK FOR AUTOMATIC ML RESEARCH

Automatic ML research agents operate through iterative refinement cycles, where each iteration in-
volves hypothesis generation, code modification, experimental execution, and empirical evaluation.
To formalize this process, we propose a unified optimization framework that is explicitly aligned
with five evaluation dimensions.

Consider an agent conducting research over T iterations. At iteration t € {1,...,7'}, the agent
starts with codebase C;_1 and generates a hypothesis h; from a learned proposal distribution q.
This hypothesis is instantiated as a concrete code modification m;. After completing all T iterations,

the agent has produced a hypothesis set H = {h1,..., hr} with corresponding modifications M =
{m,,..., mr}. The agent’s objective over the research process is:
T
max = S>[U, + AAp — Pi] + S(M, C1) + BD(H) (1)

Ts {arm},

where U; = Ep,~q,(U (me, Cte—1)] is the expected utility, A, = A(m,) is the academic contribution
rate, Ce = Ce_1 @ mM, P; = P(mz) is the cost, S'(M, Cz) is the step success rate, and D(H)
measures diversity across all hypotheses.



arXiv preprint

Evaluation metrics Each term in Eq.[I]corresponds directly to our proposed evaluation metrics:

* Utility U(m, C): The primary objective measuring empirical performance improvement.
Specifically, V(m, C) = perf(C © m) — perf(C), where perf(-) evaluates the task-specific
metric (e.g., accuracy, AUC, error rate).

* Diversity D(H): Quantifies the variety across all proposed hypotheses H = {h,,..., hr}.
We measure this through semantic and structural variance of the resulting modifications,
capturing the agent’s exploration breadth. Empirically, D(#) strongly correlates with dis-
covering high-performing solutions.

* Academic Contribution Rate A(m): Measures the proportion of academic/algorithmic
contributions (e.g., novel architectures, loss functions, training schemes) relative to en-
gineering modifications (e.g., hyperparameter tuning, infrastructure fixes). Higher A(m)
indicates greater scientific contribution, distinguishing genuine research advances from im-
plementation optimizations.

* Step Success Rate S(M,Cj,): Captures the reliability of all code modifications M on
initial codebase Cy. This reflects the agent’s ability to produce syntactically correct, se-
mantically coherent code that successfully completes experiment iterations without errors.

* Cost P(m): Encompasses time expenditure (wall-clock time) and API usage (tokens) re-
quired to execute codebase with modification m.

Design Principles for Effective Agents. To achieve high utility while maintaining research qual-
ity, effective agents should satisfy:

T
D(H) >6, A>a, S>p, SOR<B (2)
t=1.

where A = z 1 A; is the average academic contribution rate. These principles guide agent de-
sign: maintain exploration breadth to avoid local optima (D > 4), prioritize algorithmic innovation
(A > a), ensure reliable execution (S' > p), and respect computational budgets (> P; < B).

4 BENCHMARK DESIGN

4.1 TASK DESCRIPTION

We select 8 diverse tasks that collectively span the most critical aspects of ML research, ensuring
a comprehensive assessment for ML research agents. These tasks represent core competencies that
a comprehensive and robust agent should demonstrate: the ability to generalize beyond training
distributions, learn efficiently from limited data, discover meaningful representations, retain knowl-
edge over time, reason about causal relationships, maintain reliability under adversarial conditions,
protect sensitive information, and operate fairly across different groups.

Specifically, we evaluate agent performance across 8 critical machine learning tasks. 1) General-
ization. Assessed via a cross-domain transfer task in which models train on a source domain and are
evaluated on a held-out target under distribution shift. The objective is to maximize out-of-domain
accuracy. 2) Data Efficiency. Tested through a few-shot classification task. Agents should pro-
pose approaches to improve metric-based decision rules in the embedding space to boost accuracy
with limited labels. 3) Representation Learning. Pretrain encoders in a self-supervision manner
and evaluate by linear-probe accuracy with the encoder frozen, targeting meaningful feature discov-
ery. 4) Continual Learning. Measure knowledge retention in a class-incremental sequence with a
shared output head. Agents should propose methods to mitigate catastrophic forgetting and maxi-
mize average accuracy across all tasks. 5) Causality. Estimate treatment effects under a specified
causal data-generating process and minimize absolute error in the average treatment effect (ATE).
6) Robustness and Reliability. Evaluate resilience to adversarial corruption, including poisoning
or backdoor perturbations, while preserving clean performance. The defense score balances both
objectives. 7) Privacy. Assess protection against information leakage by reducing the effective-
ness of membership-inference attacks, i.e., lowering attack AUC. 8) Fairness and Bias. Evaluate
equitable performance across groups in binary classification with sensitive attributes, aiming to im-
prove group-fairness metrics (e.g., minimizing absolute average odds difference) without sacrificing
overall accuracy.


arXiv preprint

4.2 UNIFIED INPUT-OUTPUT INTERFACE

A core design of our benchmark is to support direct utilization of machine learning research GitHub
repositories. Real-world repositories vary significantly in execution pipelines, output formats, and
evaluation protocols. We solve this through unified input-output interfaces that preserve repository
complexity.

Input Existing benchmark designs struggle to handle different GitHub repositories. Data-based
benchmarks accept only datasets and task descriptions as inputs. And benchmarks providing code-
base assume unified training script names, single-stage training, no customizable arguments, or
requiring to set an individual evaluator manually. In contrast, real repositories use different script
names, multi-stage pipelines, diverse training arguments, and include their own evaluator already.
Our solution treats the complete execution sequence (training and evaluation commands) as a single
input unit so that the agent receives a command list for running experiments. Therefore, our bench-
mark provide following resources (as shown in Fig. |1) to agent as input: 1) task description with
objectives and expected outputs, 2) complete repository code, 3) suggested files for modification, 4)
protected code segments that cannot be modified to preserve evaluation integrity, 5) command list
for running experiments, 6) baseline performance, and 7) target improvement metrics.

Output Repository outputs are various (e.g., from text files to JSON formats). However, most
outputs share a common structure: performance metrics on specific datasets. We provide a post-
processing module that converts diverse task outputs into a standardized format, enabling consistent
metric extraction across all tasks while preserving native output mechanisms.

This design bridges the gap between evaluation standardization and real-world repository diversity,
enabling rigorous assessment of agents on practical code optimization tasks.

5 EXPERIMENTS

5.1 SETTINGS

Selections of Agents As shown in Fig. |2} we explore three automatic machine learning research
agents, each adopting a distinct research strategy. TheAIScientist follows a broad exploration ap-
proach, generating and testing a wide range of hypotheses in parallel across multiple experimental
directions. AIDE employs a hierarchical, tree-based search strategy, balancing the exploration of
new possibilities with the exploitation of promising results. And we prompt Claude Code to em-
ploy a linear refinement strategy, sequentially improving its hypotheses and code implementations
to address ML tasks.

Benchmark Task Settings Our benchmark encompasses 8 fundamental machine learning chal-
lenges, each implemented using established repositories with baselines: 1) Generalization using
the DomainBed repository with ERM baseline, op-
timizing out-of-domain accuracy; 2) Data Efficiency using the Easy-Few-Shot-Learning
repository with Prototypical Networks baseline, maximizing few-shot
classification accuracy; 3) Representation Learning using the Lightly reposi-
tory with MoCo baseline, maximizing linear probing accuracy on frozen encoders;
4) Continual Learning using the Continual-Learning repository with
Synaptic Intelligence baseline, optimizing average accuracy across sequential
tasks; 5) Causality using the CausalML repository with DragonNet
baseline, minimizing absolute error in average treatment effect estimation; 6) Robustness and
Reliability using the Adversarial Robustness Toolbox (ART) repository with
dp-instahide defense baseline, optimizing defense scores against backdoor
attacks; 7) Privacy using the PrivacyMeter (Murakonda & Shokri repository with Wide-
ResNet-28-2 2016) baseline, minimizing membership inference attack
AUC; and 8) Fairness and Bias using the AIF360 repository with Adversar-

ial Debiasing baseline, minimizing absolute average odds difference while preserving classification
performance. Comprehensive details are provided in Appendix |B]

Experimental Protocol Each agent is required to execute in three independent rounds. In each
round, the agent is assigned a fixed budget of total steps = 100 (iterations). We select the best result
achieved among the three rounds based on the target metric computed on the test set.


arXiv preprint

TheAlScientist AIDE Claude Code

bf

Parallel Exploration Tree-based Exploration Linear Refinement

Breadth: Wide Depth: Shallow Breadth: Medium Depth: Medium Breadth: Narrow Depth: Deep

Figure 2: Comparison of research exploration strategies of different agents. TheAIScientist uses
parallel exploration for broad coverage, AIDE employs hierarchical tree-based search balancing ex-
ploration and exploitation, while Claude Code follows linear refinement for sequential improvement.

Evaluation Metrics We evaluate agent performance on our benchmark using the proposed met-
rics: Utility, Diversity, Academic Contribution Rate, Cost, and Step Success Rate. In addition, we
report the Step Completion Rate, which calculates the proportion of executed steps relative to the
required total, since AIDE and Claude Code may exhibit premature termination. See Sec. [C]in the
appendix for detailed calculation of metrics. We present detailed Utility results across the 8 tasks
(Tab. 2). and report averages of the other metrics over the 8 tasks (Tab. [3): full per-task results are
provided in Appendix[E]

5.2 IMPLEMENTATION DETAILS

Agents ‘To enable these agents to operate on our benchmark, several modifications were necessary.
We adapted TheAIScientist by fixing compatibility issues and extending its functionality to support
the requirements of our benchmark, such as executing experiments in real repositories and reporting
results consistently. As for AIDE, we employed its cloud-based commercial variant, Weco, as the
operational interface, adapting our benchmark to integrate with its workflow despite limited control
over its internal mechanisms. For Claude Code, we designed a prompting scheme (see Appendix|F)
that enabled it to function as an automatic research agent, capable of reading code, generating hy-
potheses, and proposing modifications grounded in experimental feedback.

LLMs adopted for agents We employ GPT-5 (2025-08-07) and Gemini-2.5-Pro (2025-06-17) for
TheAIScientist and AIDE. For Claude Code, it is constrained to its native models, and therefore, we
use Opus-4.1 (2025-08-05).

5.3. RESULTS AND DISCOVERIES

5.3.1 COMPARISON OF AGENTS WITH DIFFERENT RESEARCH STRATEGIES

As shown in Tab. [2] the combination of TheAIScientist with Gemini-2.5-Pro achieved the best per-
formance, ranking first in 4 out of 8 tasks. The combination of TheAIScientist with GPT-5 ranked
second, securing top results in 2 out of the 8 tasks. These findings suggest that TheAIScientist per-
forms better in discovering novel and effective machine learning methods, compared to AIDE and
Claude Code.

As illustrated in Fig. [2| TheAIScientist adopts a research exploration strategy that is broad but shal-
low, while AIDE maintains both medium breadth and depth. In contrast, Claude Code exhibits a
narrow yet deep exploration pattern. For a detailed explanation of the research strategies adopted by
each agent, refer to Section|5.1] Considering the research exploration strategy together, the results
suggest that a broader research exploration space adopted by TheAIScientist is more effective for
discovering promising ideas. This insight offers practical guidance for real-world research: broadly
exploring diverse ideas could be more productive than focusing on a single direction.


arXiv preprint

Table 2: Comparison of best performance (Utility) among different agents. G2.5-Pro denotes
Gemini-2.5-Pro. (+) indicates higher is better, (|.) indicates lower is better.

ML Problems Baseline TheAIScientist AIDE Claude Code
GPT-5  G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization (t) 0.2254 0.5036 0.3252 0.2254 0.2254 0.5036
Data Eff. (+) 0.6547 0.7689 0.8231 0.6547 0.6547 0.6571
Rep. Learn. (+) 0.7562 0.7796 0.8597 0.8469 0.8466 0.7725
Cont. Learn. (t) 0.2710 0.4281 0.7808 0.4369 0.3658 0.2337
Causality ({) 0.2057 0.0434 0.0058 0.0155 0.0000 0.0063
Robust. & Rel.(t) 0.6966 0.9623 0.9570 ~—-:0.9708 ~— 0.9653 0.9469
Privacy (1) 0.8114 0.4908 0.1750 0.4882 0.4814 0.4892
Fair. & Bias (|) 0.1471 0.0002 0.0053 0.0004 0.0030 0.0149

Table 3: Comparisons of different agents across diversity, academic contribution rate, cost,
and success metrics. Contrib. denotes Contribution and Cons. stands for Consumption.

F TheAIScientist AIDE Claude Code
Metrics
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Diversity (t) 24.95+9.63 20.66=109-85 20.26+9-37 18.49+14.87 12.02+8.94

Academic Contrib. Rate (ft)  0.8349-?7) 0.784974 0,84#9-29 0,65 +0-34 0.254036
Total Token Cons. (M) ({) 6.0772 = -5,33+2-55 9. g5+9-59 9. 590-90 8.32+3-10
Step Token Cons. (M) (1) 0.06=£9-02 0.05£9-03 0.03£9-03 0.04+9-03 1.77+1.32

Total Time Cons. (H) (J) 8.32+3-15 12,237.98 2.464177 8.06+°.44 1.274177
Step Time Cons. (Min) ({) —10.39#19-79—-12.73414.64 — 19,79+12-86 3,65 47-92 12.49+10.77
Step Success Rate (t) 0.92+9-08 0.87+0-12 0.65+9-25 0.73 +9-22 0.83+0-18

Step Completion Rate () 1.00=°°° —1,00*°-°9 0389-22 0,799.2 0.07*9.%

5.3.2 ANALYSIS OF DIVERSITY

As shown in Tab. |3| TheAIScientist shows the highest average diversity (GPT-5: 24.95 + 9.63;
Gemini-2.5-Pro: 20.66 + 10.85). AIDE is lower but comparable (GPT-5: 20.26 + 9.37; Gemini-2.5-
Pro: 18.49 + 14.87), while Claude Code is markedly lower (12.02 + 8.04). This pattern mirrors how
the agents explore solutions. TheAIScientist advances several ideas in parallel. AIDE grows ideas
through a tree of iterative refinements. Claude Code, by contrast, tends to proceed along a single
linear track. By comparison, parallel exploration broadens the search and produces higher measured
diversity, while linear iteration confines the search and curbs diversity.

We further analyze the relationship between code diversity and task performance using the correla-
tion coefficient r (details are shown in Appendix Fig. (4p. The results suggest that, overall, diversity
is positively correlated with performance. Specifically, among the 8 tasks, 4 tasks show strong
positive correlations, 2 weak positive correlations, and 2 negative correlations. The most notable
effects appear in Continual Learning (r = 0.96), Fairness & Bias (r = 0.86), and Generalization
(r = 0.72), with a moderate correlation also observed in Data Efficiency (r = 0.48). These findings
indicate that higher code diversity tends to be associated with improved task performance, although
the strength of this relationship varies by task.

5.3.3. ACADEMIC CONTRIBUTION RATE

The academic contribution rate provides further insights into the characteristics of each agent. It
helps disentangle the impact of academic value from other factors such as engineering effort and
diversity. According to Tab.|3} TheAIScientist generally exhibits a slightly higher academic con-
tribution rate than AIDE, whereas Claude Code consistently shows the lowest rate. This suggests
that the ideas and code modifications proposed by TheAIScientist are more closely aligned with
methodological advancements, rather than relying on engineering tricks to boost performance. Fur-


arXiv preprint

thermore, comparing GPT-5 and Gemini-2.5-Pro reveals that Gemini-2.5-Pro tends to propose more
engineering-oriented solutions than GPT-5. In summary, for automatic ML research agents, we
prioritize agents that can generate hypotheses with strong academic value while simultaneously de-
livering better Utility U (higher performance).

5.3.4 TOKEN AND TIME CONSUMPTION

Tab. [3] reports both the average token and time consumption per step, as well as the total token
and time usage for a complete experimental run. We observe that TheAIScientist consumes more
tokens than AIDE, while Claude Code, despite its lower performance, uses the highest number of
tokens among the three agents. This indicates that dedicated automatic ML research agents, such as
TheAJIScientist and AIDE, are more suitable for ML research problems in terms of both performance
and token efficiency compared to general-purpose agents like Claude Code. In terms of time per
step, all three agents show similar durations, with differences of around 2 minutes, which are not
substantial. However, AIDE and Claude Code exhibit significantly shorter total execution times for
a full experiment. This is primarily due to premature termination issues, as evidenced by the step
completion rate, which leads to reduced overall time usage.

5.3.5 ADDITIONAL OBSERVATIONS

Characteristics of Claude Code Since all actions are executed based on LLM decisions rather
than fixed procedures, Claude Code often fails to follow prompt instructions, frequently terminating
experiments prematurely. Despite this, it demonstrates a high improvement speed (refer to the im-
provement curves Fig. [3]and speed comparison Tab. [12]in the Appendix). In addition, its academic
contribution rate is low, with a strong emphasis on engineering. This may be attributed to its nature
as a general-purpose agent, rather than a specialized automatic ML research agent.

Shallow edits We found that AIDE sometimes misinterprets the structure and logic of target code-
bases. In certain cases, it generated new classes or components that were never integrated into the
actual execution pipeline, resulting in no functional improvement over the baseline. As shown in
Tab.|2| AIDE failed to improve the baseline in tasks related to Generalization and Data Efficiency.
This may stem from the fact that AIDE only supports iterative modifications on a single file. How-
ever, real-world ML research codebases are often complex and span multiple files, making AIDE
insufficient for addressing realistic research tasks.

Premature termination We encountered the issue of early termination of AIDE and Claude Code.
For AIDE, the agent sometimes terminated prematurely due to its commercial version, Weco, which
relies on cloud infrastructure that occasionally failed during execution. For Claude Code, early
stopping was often triggered by the model’s internal reasoning, where the LLM would decide not to
continue even when further actions are possible.

6 CONCLUSION

In this work, we introduce FML-bench, a benchmark that assesses automatic machine learning re-
search agents on 8 diverse and fundamental ML problems drawn from real-world codebases and
authentic research workflows. Within the benchmark, we introduce a five-dimensional evaluation
protocol that facilitates a more systematic assessment, moving beyond exclusive reliance on final
performance. Building on FML-bench, we conducted a systematic analysis of three state-of-the-
art automatic research agents. Our findings reveal that agents capable of generating and evaluating
multiple hypotheses across diverse directions, such as TheAIScientist, tend to outperform those that
focus on iteratively refining a single line of thought (e.g., Claude Code). These findings suggest that
a broader exploratory capacity contributes more significantly to overall success once a basic level
of proficiency is established. Overall, FML-bench provides a robust and practical foundation for
evaluating the capabilities of research agents and offers a pathway toward building more effective,
generalizable, and scientifically productive research agents.


arXiv preprint

REPRODUCIBILITY STATEMENT

The paper describes the benchmark design, evaluation protocol, experimental settings, and imple-
mentation details, including baseline configurations and prompt specifications. All benchmark
code, experiment prompts, and configuration files are open-sourced and available at: https:

//github.com/qrzou/FML-bench
ACKNOWLEDGEMENTS

We gratefully acknowledge Zhengyao Jiang and Weco (https: //www.weco.ai/) for their sup-

port and for providing access to their more general agent, which extended beyond the limitations of
the original AIDE and enabled us to run AIDE as a baseline on our benchmark.

REFERENCES

Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308-318, 2016.

Amina Adadi. A survey on data-efficient algorithms in big data era. Journal of Big Data, 8(1):24,
2021.

Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv: 1907.02893, 2019.

Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative
research idea generation over scientific literature with large language models. arXiv preprint
arXiv:2404.07738, 2024.

Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI:
https://doi.org/10.24432/CS5XW20.

Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya
Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar,
Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Monin-
der Singh, Kush R. Varshney, and Yunfeng Zhang. AI Fairness 360: An extensible toolkit
for detecting, understanding, and mitigating unwanted algorithmic bias, October 2018. URL

https://arxiv.org/abs/1810.01943

Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. [EEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.

Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research
with large language models. Nature, 624(7992):570-578, 2023.

Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong
Huang, Micah Goldblum, and Tom Goldstein. Dp-instahide: Provably defusing poisoning and
backdoor attacks with differentially private data augmentations. arXiv preprint arXiv:2103.02079,
2021.

Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio
Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learn-
ing agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024.

Huigang Chen, Totte Harinen, Jeong- Yoon Lee, Mike Yung, and Zhenyu Zhao. Causalml: Python
package for causal machine learning, 2020.

Zhiyuan Chen and Bing Liu. Lifelong machine learning. Morgan & Claypool Publishers, 2018.

Li Deng. The mnist database of handwritten digit images for machine learning research. [EEE
Signal Processing Magazine, 29(6):141—142, 2012.

10


arXiv preprint

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv: 1412.6572, 2014.

Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data flow. arXiv preprint arXiv:2009.08366, 2020.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp. 9729-9738, 2020.

Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili
Pan, and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty
and diversity of llm generated ideas. arXiv preprint arXiv:2410.14255, 2024.

Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents
on machine learning experimentation. arXiv preprint arXiv:2310.03302, 2023.

Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and

Yuxiang Wu. Aide: Ai-driven exploration in the space of code. 2025. URL
org/abs/2502.13138

Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang.
Agentreview: Exploring peer review dynamics with llm agents. arXiv preprint arXiv:2406.12708,
2024.

Ligiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming
Zhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents from becoming data
science experts? arXiv preprint arXiv:2409.07703, 2024.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Lightly-AI. Lightly: A python library for self-supervised learning on images.
2025,

Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scien-
tist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292,
2024.

Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe
Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelli-
gence, 6(5):525-535, 2024.

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1—35, 2021.

Sasi Kumar Murakonda and Reza Shokri. MI privacy meter: Aiding regulatory compliance by
quantifying the privacy risks of machine learning. arXiv preprint arXiv:2007.09339, 2020.

Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wis-
tuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, et al. Adversarial
robustness toolbox v1.0.0. arXiv preprint arXiv: 1807.01069, 2018.

Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects.
Biometrika, 108(2):299-319, 2021.

Alexander Novikov, Ngan Vi, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt
Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian,
et al. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint
arXiv:2506.13131, 2025.

11


arXiv preprint

Harshith Padigela, Chintan Shah, and Dinkar Juyal. Ml-dev-bench: Comparative analysis of ai
agents on ml development workflows. arXiv preprint arXiv:2502.00964, 2025.

Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Communi-
cations of the ACM, 62(3):54-60, 2019.

Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment
effects. Advances in neural information processing systems, 32, 2019.

Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can Ilms generate novel research ideas? a large-
scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024.

Sicara. Easy few-shot learning: ready-to-use code and tutorial notebooks for few-shot image classi-

fication. https://github.com/sicara/easy-few-shot-learning, 2024.

Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Ad-
vances in neural information processing systems, 30, 2017.

Kyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab: Ai agents
design new sars-cov-2 nanobodies with experimental validation. bioRxiv, pp. 2024-11, 2024.

Gido M van de Ven, Tinne Tuytelaars, and Andreas S Tolias. Three types of incremental learning.
Nature Machine Intelligence, 4:1185-1197, 2022.

Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. Advances in neural information processing systems, 29, 2016.

Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun
Zeng, and Philip S Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE
transactions on knowledge and data engineering, 35(8):8052—8072, 2022.

Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines
optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume I: Long Papers), pp. 279-299, 2024a.

Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu
Dai, Qingsong Wen, Wei Ye, et al. Autosurvey: Large language models can automatically write
surveys. Advances in neural information processing systems, 37:115119-115145, 2024b.

Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune,
and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree
search. arXiv preprint arXiv:2504.08066, 2025.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv: 1605.07146, 2016.

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International conference on machine learning, pp. 3987-3995. PMLR, 2017.

12


arXiv preprint

APPENDIX

A USE OFLLMS

In this study, we employed LLMs (ChatGPT and Claude AI) to assist with manuscript writing.
Specifically, these tools were used to polish the writing (e.g., improving fluency, grammar, and
clarity of expression), aid in formatting, and support proofreading. Additionally, we used them to
facilitate the retrieval of related work by suggesting potentially relevant literature.

B’~ TASK SETTINGS

Generalization We adopt the DomainBed (Gulrajani & Lopez-Paz}|2020) repository with Em-
pirical Risk Minimization (ERM) (Vapnik||1998) as the baseline and evaluate on the Color MNIST

dataset. The evaluation metric is accuracy on a held-out domain. The agent
is required to train on a source domain and generalize to a target domain under distribution shift.
The task objective is to improve out-of-domain generalization performance. This task evaluates the
agent’s ability to develop algorithms that transfer effectively across domains, which is critical for
robust real-world deployment.

Causality We utilize the CausalML (Chen et al.| |2020) repository with DragonNet

2019) as the baseline model, evaluated on a synthetic dataset constructed from Setup A in
(2021), Quasi-Oracle Estimation of Heterogeneous Treatment Effects. The performance met-
ric 1s the absolute error in average treatment effect (ATE) estimation. The agent’s goal is to develop
improved causal inference strategies that minimize this error. This task assesses the agent’s capacity
to reason about interventions, which is essential for decision-making in high-stakes environments.

Data Efficiency For data-efficient learning, we employ the Easy-Few-Shot-Learning

2024) repository with Prototypical Networks (Snell et al.||2017) as the baseline, evaluated on the
Mini-ImageNet (Vinyals et al.||2016) dataset. The agent must operate under a frozen backbone and

propose improved algorithms for metric-based classification in the embedding space. Accuracy is
used as the evaluation metric. This task measures the agent’s ability to enhance few-shot learning
performance by optimizing distance-based reasoning under tight data constraints.

Robustness and Reliability We adopt the Adversarial Robustness Toolbox (ART)
repository, using dp-instahide as the defense baseline on a poisoned
MNIST dataset constructed with the “pattern” backdoor method. The evaluation metric
is a defense score, defined as the harmonic mean of clean accuracy and resistance accuracy against
backdoor attacks. The agent is tasked with proposing defenses that reduce the effectiveness of
poisoning attacks while maintaining high clean performance. This task probes the agent’s ability to
improve model robustness against adversarial corruption.

Privacy This task uses the PrivacyMeter (Murakonda & Shokri| repository and Wide-
ResNet-28-2 (Zagoruyko & Komodakis} |2016) as the baseline model trained on CIFAR-10
targets membership inference attacks, with performance
measured by the area under the ROC curve (AUC). The agent must design improved defense mech-

anisms that minimize the AUC, thereby reducing information leakage. This task is critical for as-
sessing the agent’s ability to enhance privacy protections in the face of inference attacks.

Representation Learning — For self-supervised learning, we employ the Lightly (Lightly-AI)|2025)
repository with MoCo as the baseline and evaluate on CIFAR-10 (Krizhevsky et al.
(2009). The primary evaluation metric is the accuracy of a linear classifier trained on top of the frozen
encoder after pretraining. The agent is expected to devise improved representation learning methods

that yield higher linear probing accuracy. This task tests the agent’s ability to learn generalizable
and semantically meaningful features from unlabeled data.

Continual Learning We use the Continual-Learning (van de Ven et al.||2022) repository with
Synaptic Intelligence (SI) (Zenke et al.||2017) as the baseline, evaluated on splitMNIST 2012)

13


arXiv preprint

in the class-incremental learning (Class-IL) scenario. The model must learn sequentially across
multiple contexts using a shared 10-way classification head. The evaluation metric is the average
accuracy across all tasks. The agent is expected to develop algorithms that mitigate catastrophic
forgetting and maintain performance across contexts. This task evaluates long-term adaptability in
non-stationary environments.

Fairness and Bias We adopt the AJF360 (Bellamy et al.||2018) repository with Adversarial Debi-
asing as the baseline, using the Adult (Becker & Kohavi||1996) dataset. The primary fairness metric
is average odds difference, and the agent is tasked with enhancing fairness (minimizing absolute av-

erage odds difference) while maintaining or improving classification accuracy. This task measures
the agent’s ability to balance equitable outcomes with model utility across demographic groups.

C EVALUATION METRICS

Utility reflects empirical improvement within each task and is reported using the task-specific per-
formance metric.

Diversity quantifies implementation dispersion within the ‘al ]2020) and c round. Given 2 th step

code embedding e; extracted from CodeGraphBERT 2020) and centroid e = + re eis

diversity is computed as — }7;_, ||e; — €||2. Greater dispersion indicates broader exploration of

implementation choices within a single round.

Academic Contribution Rate reflects the agent’s tendency toward innovative algorithmic contribu-
tions versus conventional engineering optimizations. It is defined as Naca/Neuc, Where Naca is the
number of steps recogniezd by Qwen3 as academic modifications (e.g., new losses or architectures)
relative to the baseline, and N,,, is the number of steps whose experiments execute without errors
and yield valid results.

Step Success Rate is defined as the Nguc /Neomp- Combining the metric Step Completion Rate
(Neomp /Niotal) Will reflect the system reliability of the agent. Here, Neomp is the number of steps
actually executed and Niotai is the number of steps assigned to the agent.

Cost is reported as total token consumption (sum across steps) and step token consumption (av-
erage per step), together with total time consumption and step time consumption, to characterize
computational and temporal efficiency.

Step To Target calculation Steps To Target is measured as STT’ = min{n : performance(n) >
threshold}, representing the minimum number of iterations required to reach the target performance
level, where the threshold is set to be the worst best-run performance among the agents.

C.1 ACADEMIC MODIFICATION CLASSIFICATION

Academic Contribution Rate is computed by identifying academic steps within the best-performing
rounds. We use Qwen3 (version: 235b-a22b-—2507) to automatically classify each modification
as academic or engineering by comparing the baseline and modified code. The prompt used to guide
Qwen3 is provided below.

14


arXiv preprint

You are an experienced machine learning researcher. Your task is to analyze two pieces
of code—baseline and modified—and judge whether the modification is mostly prone to
engineering or academic modification. You will also assign one subtag to describe the
specific type(s) of modification.

Baseline code is delimited by [baseline:begin] ...[baseline:end].
Modified code is delimited by [modified:begin] ... [modified:end].

Carefully compare the two kinds of code and reason about what changed and why it matters.
Please give your judgement by ’engineering’ or academic’ and one the most related subtag
eg: [engineering, [ENG/LR_SCHED]], [academic, [ACD/LOSS NEW]].

Your answer should only select one of the two options [engineering] or [academic].

Engineering modification: Changes focused on making an existing method run more
stably, efficiently, or accurately through implementation details, system-level tuning, or
configuration, without introducing a new learning principle, objective family, or architec-
tural paradigm. Typical signals: hyperparameter tuning, training schedules, efficiency and
stability tricks, data pipeline and evaluation scripting that keep the task/method essentially
the same.

Academic modification: Changes that introduce or test a new machine learning idea: a
new objective/loss with motivation, new inductive bias or architecture, a different training or
inference paradigm, etc.

For Engineering subtags:

[ENG/LR_SCHED]: Learning-rate, momentum, scheduler (Cosine, OneCycle, Warmup)
[ENG/OPT_SWAP]: Optimizer swap as tuning (SGD to AdamW) without new update rule.
[ENG/LOSS_WEIGHTING]: Loss weights/temperature/threshold sweeps (no new loss
family).

[ENG/TRAIN_STR]: Early stopping or extend training time.

[ENG/DATA_AUG]: Data cleaning/dedup/resampling; standard/strong augmentation pa-
rameters

For Academic subtags:

[ACD/LOSS_NEW]: New loss/constraint; dual/information-theoretic objectives
[ACD/ARCH_NEW]: New layer/module/architecture (e.g., novel attention; new en-
coder—decoder coupling)

[ACD/TRAIN-PARADIGM]: New training or inference paradigm (e.g., contrastive to
masked/generative; new decoding strategy; beyond teacher forcing; supervised to self-
supervised)

Your answer should only select one of the two options [engineering] or [academic]
and one the most related subtag eg: [engineering, [ENG/LR_SCHED]], [academic,
[ACD/LOSS_NEW]].

[baseline:begin] ... baseline code ... [baseline:end]

[modified:begin] ... modified code ... [modified:end]

Compare baseline code and modified code, tell me which kind of code modification it
is to improve the baseline method, choose from engineering, academic and one most related
subtag eg: [engineering, [ENG/LR-SCHED]], [academic, [ACD/LOSS_NEW]]

D PROTECTING EVALUATION INTEGRITY

A crucial aspect of our implementation involved protecting evaluation files from inadvertent mod-
ification by the agents. We employed agent-specific protection strategies: For TheAIScientist, we
explicitly instructed the agent through prompting to avoid modifying evaluation files and imple-

15


arXiv preprint

mented a systematic refresh mechanism that restores evaluation files before each evaluation cycle.

For AIDE, protection is inherently ensured by its
tees that when the target code and evaluation cod
files remain naturally protected. For Claude Co
which the evaluation files were first restricted to

single-file modification constraint, which guaran-
e are separated into different files, the evaluation
de, we implemented a two-pronged approach in
read and execute permissions before running the

agent, and then the --disallowedTool1s argument was employed to explicitly prevent permis-

sion modification operations during execution.

E ADDITIONAL RESULTS

Accuracy

va

Step

(a) Generalization ()

Accuracy

100

40

Step

(c) Representation Learning (fT)

ATE Error

| {\ f \ {| I\
NIN al WU NAN WN

100

Step

(e) Causality (|)

40 80 100

wee
ze
£
B50
<
40
30
"5 20 60 80 100
Step
(b) Data Efficiency (7)
08
a7
>
& 06
5
Sos
<
Soa ;
Soa | | Au

40 80 100

Step

0.8

Defense Score

Step

(f) Robustness and Reliability (7)

coos eos 8
& R&B AY

Absolute Average Odds Difference
Es

Step
Step
(g) Privacy (|) (h) Fairness and Bias (|)
— TheAlIScientist (GPT-5) | ——— TheAlScientist (Gemini-2.5-Pro) ——— Claude Code
~—— AIDE (GPT-5) —— AIDE (Gemini-2.5-Pro) Baseline

Figure 3: Agents’ performance improvement curves across 8 tasks.

This section provides a comprehensive comparative analysis of automated AI research systems, fo-
cusing on both their performance and operational efficiency. We evaluate TheAIScientist, AIDE,
and Claude Code across eight core machine learning research problems, considering not only their

16


arXiv preprint

final outcomes but also their operational behavior over time. Fig. [3] depicts the step-wise perfor-
mance progression of these systems across all tasks, while Fig. 4Jhighlights the relationship between
performance and solution diversity.

Our analysis covers multiple key dimensions: solution diversity (Tab. |4p, research contribution qual-
ity (Tab. [5p, computational efficiency in terms of total token usage (Tab. (6) and step-wise token
usage (Tab. [7), runtime efficiency in terms of total time (Tab. [8) and step-wise time (Tab. ae
ational reliability measured by step success rate (Tab. and step completion rate (Tab.|11), and
improvement speed (Tab.[12). Together, these results provide detailed insights into the trade-offs be-
tween different agentic research designs, illustrating how they influence both the quality of research
outcomes and the efficiency of resource utilization.

Diversity-Performance Correlation Analysis

Pearson Correlation Coefficients Across Different Tasks

*
to 0.956

0.863

0.719

0.291
0.096
0.0 —— — —

-0.635

-0.232

Pearson Correlation Coefficient

Ki oO O° wt : >) 0?
- ver a ee a eo oe Tasks

so
Rx)
40 xO
Nia 8 : 5 Re
a he) ge? co x” ye ot
rx

@@mmm Strong Positive(r>0.7) MEM Moderate Positive (0.3 < rs 0.7) Weak (-0.3<1<0.3) MN Moderate Negative (-0.7 sr < 0.3)

Figure 4: Performance - Diversity Analysis.

Table 4: Code Diversity Comparison of Best Performance Round. G2.5-Pro denotes Gemini-
2.5-Pro. Bold numbers indicate the best (highest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1

Generalization 46.38 17.94 0.00 21.73 22.29
Data Eff. 25.43 17.70 18.33 11.06 17.98
Rep. Learn. 17.42 10.86 15.06 8.76 4.74
Cont. Learn. 21.41 45.23 30.79 18.03 5.42
Causality 19.22 18.25 17.51 8.19 7.06
Robust. & Rel. 16.65 30.13 29.88 56.05 25.90
Privacy 33.84 13.13 25.29 10.13 5.93

Fair. & Bias 19.29 12.06 25.23 13.98 6.91

Average + Std  24.95+9-63 — 20,66£10-85 29,269.37 1g.49+14.87 12.98.04

17


arXiv preprint

Table 5: Academic Contribution Rate (ACR) Comparison. G2.5-Pro denotes Gemini-2.5-Pro.
Bold numbers indicate the best (highest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization 0.90 0.95 0.33 0.63 0.33
Data Eff. 1.00 0.89 0.97 0.99 1.00
Rep. Learn. 0.93 0.93 0.98 0.99 0.00
Cont. Learn. 0.12 0.17 0.89 0.35 0.00
Causality 0.88 0.78 0.93 0.89 0.00
Robust. & Rel. 0.95 0.87 0.92 0.33 0.67
Privacy 0.90 0.83 0.80 0.07 0.00
Fair. & Bias 0.95 0.84 0.93 0.95 0.00

Average + Std 0.839?" 0.78924 0.844929 0,65#934 — 9,25#9-36

Table 6: Total Token Consumption Comparison (in millions). G2.5-Pro denotes Gemini-2.5-Pro.
Bold numbers indicate the best (lowest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization 9.34 9.88 0.18 3.80 5.98
Data Eff. 3.09 2.55 0.33 1.53 7.98
Rep. Learn. 4.69 3.99 0.40 1.51 8.89
Cont. Learn. 7.81 7.74 1.06 3.21 2.99
Causality 4.57 3.05 0.56 2.99 9.26
Robust. & Rel. 7.24 6.74 1.69 2.93 8.22
Privacy 6.64 4.45 1.68 1.61 13.88
Fair. & Bias 5.13 4.21 0.94 3.12 9.34

Average + Std 6.07$7-°° 5,33 #255 9.854059 -2.,59+0.90 8.32+3-10

Table 7: Step Token Consumption Comparison (in millions). G2.5-Pro denotes Gemini-2.5-Pro.
Bold numbers indicate the best (lowest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4. 1
Generalization 0.09 0.10 0.09 0.10 1.99
Data Eff. 0.03 0.03 0.01 0.02 0.50
Rep. Learn. 0.05 0.04 0.01 0.01 0.81
Cont. Learn. 0.08 0.08 0.04 0.05 1.49
Causality 0.05 0.03 0.02 0.03 0.77
Robust. & Rel. 0.07 0.07 0.03 0.05 1.64
Privacy 0.07 0.04 0.02 0.02 4.63
Fair. & Bias 0.05 0.04 0.02 0.03 2.34

Average + Std 0.0699? 0.05+9-°3 — 9,0349-93 0,04 £9.08 1.77#1:82

18


arXiv preprint

Table 8: Total Time Consumption Comparison (in hours). G2.5-Pro denotes Gemini-2.5-Pro.
Bold numbers indicate the best (lowest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization 9.91 17.42 0.20 6.07 0.81
Data Eff. 4.47 4.33 1.02 2.40 0.47
Rep. Learn. 7.50 23.96 4.45 14.49 5.56
Cont. Learn. 14.68 15.17 3.70 7.08 0.28
Causality 5.41 4.85 1.44 2.91 0.57
Robust. & Rel. 8.26 8.65 5.10 6.00 0.90
Privacy 9.24 20.03 2.43 21.04 1.29
Fair. & Bias 7.05 3.46 1.32 4.46 0.25

Average + Std 8.32#%15  12,23+7-93 2.46#177 g.06#644 = 1.2741.77

Table 9: Step Time Consumption Comparison (in minutes). G2.5-Pro denotes Gemini-2.5-Pro.
Bold numbers indicate the best (lowest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization 5.95 10.43 5.97 9.83 16.13
Data Eff. 2.68 2.60 2.03 1.43 1.77
Rep. Learn. 33.28 43.15 37.92 22.87 30.30
Cont. Learn. 8.80 9.08 7.93 5.98 8.33
Causality 3.25 2.90 3.08 1.73 2.87
Robust. & Rel. 4.95 5.20 4.77 6.67 10.83
Privacy 19.93 26.42 22.63 18.03 25.87
Fair. & Bias 4.23 2.07 2.02 2.63 3.80

Average + Std 10.39#10-79 12,73#1464 —10,79#12.86  gg5#7-92 12.49+10.77

Table 10: Step Success Rate Comparison. G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indi-
cate the best (highest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4. 1
Generalization 0.7700 0.9900 0.5000 0.7027 1.0000
Data Eff. 0.9600 0.6500 0.2333 0.2871 0.9375
Rep. Learn. 0.8300 0.8400 0.8000 0.6040 0.5455
Cont. Learn. 0.9600 0.9400 0.8214 0.9155 1.0000
Causality 0.8900 0.9000 0.8214 0.9406 0.8333
Robust. & Rel. 0.9900 0.9500 0.8281 0.6667 0.6000
Privacy 0.9700 0.9500 0.8714 0.9571 1.0000
Fair. & Bias 0.9500 0.7200 0.3590 0.7426 0.7500

Average + Std 0.9298 0.874912 0.65#9-25 0,739.22 0.83+9-18

19


arXiv preprint

Table 11: Step Completion Rate (SCR) Comparison. G2.5-Pro denotes Gemini-2.5-Pro. Bold
numbers indicate the best (highest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization 1 1 0.02 0.36 0.03
Data Eff. 1 1 0.30 1 0.16
Rep. Learn. 1 1 0.45 1 0.11
Cont. Learn. 1 1 0.28 0.70 0.02
Causality 1 1 0.28 1 0.12
Robust. & Rel. 1 1 0.64 0.53 0.05
Privacy 1 1 0.70 0.69 0.03
Fair. & Bias 1 1 0.39 1 0.04

Average + Std 1.0099 = 1.00#9-°° —-0.3849-22 9,799.25 0.07£9-05

Table 12: Improvement Speed Comparison. We report the number of steps used to reach the
threshold, which is defined as the best result achieved in the best run of the worst-performing agent.
Fewer steps indicate higher improvement speed. G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers
indicate the best (lowest) performance in each row.

ML Problems TheAIScientist AIDE Claude Code
GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1
Generalization 1 1 1 1 1
Data Eff. 1 2 1 1 3
Rep. Learn. 29 10 15 40 10
Cont. Learn. 6 1 1 1 2
Causality 13 38 17 6
Robust. & Rel. 26 50 5 34 2
Privacy 47 83 31 40 2
Fair. & Bias 1 25 14 17 4

Average + Std 155051895 36.95+29-40 16,639=10-06 = 17,63=17-78 3.75#2-96

F PROMPTS DETAILS

This section presents the detailed prompt specifications that form the foundation of our autonomous
research agent framework. The prompts serve as the primary interface between human researchers
and AI agents, translating high-level research objectives into actionable instructions that can guide
systematic scientific inquiry across diverse machine learning domains. The prompt design philoso-
phy centers on creating a structured yet flexible research environment that balances autonomy with
scientific rigor. Rather than providing overly prescriptive instructions that limit creative exploration,
these prompts establish clear boundaries, evaluation criteria, and operational constraints while en-
couraging the agent to develop and test novel hypotheses within established research paradigms.

This section is organized into two complementary components. First, we present the Task Descrip-
tion Prompts that define specific research challenges across 8 fundamental areas of machine learning,
each grounded in established benchmarks and methodologies. These prompts simulate realistic re-
search scenarios where an AI agent must navigate complex technical requirements while pursuing
meaningful improvements to existing methods.

20


arXiv preprint

Second, we detail the Autonomous Research Agent Framework that governs how agents interact
with research codebases to conduct iterative experimentation. This operational framework trans-
forms the conceptual research challenges into executable workflows, ensuring that agent behav-
ior follows sound scientific methodology while maintaining reproducibility and experimental in-
tegrity. Together, these prompt specifications create a comprehensive research environment where
autonomous agents can contribute meaningfully to advancing machine learning across multiple dis-
ciplines, providing both the research contexts and the methodological framework necessary for sys-
tematic scientific progress.

Task Description Prompts The following task descriptions establish comprehensive research
contexts spanning 8 fundamental areas of machine learning. Each prompt follows a structured for-
mat that defines: (1) the researcher’s identity and expertise, (2) the specific technical setup, including
datasets and baseline methods, (3) clear optimization objectives and constraints, and (4) fairness cri-
teria to ensure meaningful comparisons.

These prompts span a wide range of machine learning challenges, including generalization, data
efficiency, privacy, fairness, and robustness. Each task is anchored in established benchmarks and
frameworks, such as DomainBed for domain generalization, EasyFSL for few-shot learning, and
AIF360 for fairness-aware learning, providing realistic experimental environments that closely mir-
ror actual research workflows.

The prompts are carefully crafted to encourage both incremental improvements to existing meth-
ods and the exploration of novel algorithmic approaches, while maintaining scientific rigor through
controlled experimental conditions. This design enables systematic evaluation of how autonomous
agents can contribute to advancing the state-of-the-art across multiple ML disciplines simultane-
ously.

GENERALIZATION

System: You are an ambitious AI PhD student focused on improving the generalization
performance of machine learning methods using the DomainBed benchmark.

Task Description: You are working with DomainBed’s ERM (Empirical Risk Minimization)
method as the baseline on ColoredMNIST to evaluate generalization under distribution shifts.
Your goal is to enhance test-time domain generalization accuracy beyond standard ERM.
You should improve the algorithm based on ERM, but you may also propose entirely new
algorithms if they can better support cross-domain generalization. You are also allowed to
refine the backbone model, as long as your modifications are fair compared to the original
architecture. The priority is to improve the average accuracy on unseen test domains while
maintaining accuracy on in-domain tests, along with ensuring efficiency and low complexity.

DATA EFFICIENCY

System: You are an ambitious AI PhD student focused on data-efficient learning, specializ-
ing in few-shot learning and meta-learning.

Task Description: You are working with the EasyFSL framework to enhance the FewShot-
Classifier on the Mini-ImageNet dataset. The Mini-ImageNet dataset presents a challenging
few-shot learning scenario due to its fine-grained inter-class similarities and limited training
examples per class. Your goal is to improve the classifier’s ability to generalize to novel
classes.

REPRESENTATION LEARNING

21


arXiv preprint

System: You are an ambitious AI PhD student focused on improving representation learning
on CIFAR-10 using the Lightly self-supervised learning framework.

Task Description: You are working with Lightly’s MoCo baseline on CIFAR-10, evaluated
strictly by linear probing Top-1 accuracy. Your goal is to improve representation learning at
pretrain stage to improve linear-probe accuracy on the CIFAR-10 test set beyond standard
MoCo as much as you can under the same compute and data (no external data). You may
modify MoCo or propose new self-supervised methods if they can yield better representa-
tions, as long as your modifications are fair compared to the original architecture. You are
also allowed to refine the ResNet-18 backbone as long as parameter count and FLOPs remain
comparable to the baseline. Pretrain on the CIFAR-10 train split without labels, fit the linear
classifier on the same train split, and report Top-1 on the test split with priority on improving
representation learning performance.

CONTINUAL LEARNING

System: You are an ambitious AI PhD student focused on improving continual learning
based on Synaptic Intelligence (SI) on splitMNIST under the class-incremental scenario.

Task Description: You are working with the Continual-Learning repo’s SI baseline to im-
prove accuracy and reduce forgetting on splitMNIST (5 tasks x 2 classes, single-head over
10 classes). Your goal is to improve average accuracy over all 5 contexts on splitMNIST
without unfair model size or compute advantages. You should improve SI method, but are
also allowed to add lightweight fair components , or propose new methods, as long as your
modifications are fair (stay within fairness computation budgets). The priority is to improve
the average accuracy.

CAUSALITY

System: You are an ambitious AI PhD student focused on advancing machine learning for
causal inference, reasoning, and interpretable modeling.

Task Description: You are working with the Dragonnet framework to estimate individual
treatment effects (ITEs) in both real IHDP) and simulated data scenarios. The simulated
data follows Setup A from Nie & Wager (2018), featuring difficult nuisance functions (e.g.,
propensity scores) but simple, easily identifiable treatment effects. Your goal is to improve
the precision of treatment effect estimation across both IHDP and simulated benchmarks.

ROBUSTNESS AND RELIABILITY

System: You are an ambitious AI PhD student focused on improving robust learning under
data poisoning and privacy constraints.

Task Description: You are given the Adversarial Robustness Toolbox (ART) codebase with
a focus on the dp_instahide defense. dp_instahide mixes inputs with public data
and applies differential privacy noise to hinder inversion and poisoning. While designed for
privacy-preserving training, its structure offers headroom to harden against both clean-label
and trigger/backdoor poisons. Your goal is to improve defense performance against diverse
poisoning attacks while maintaining high clean accuracy. You may tune dp_instahide,
compose it with other defenses, or propose a new method if it outperforms baselines.

PRIVACY

22


arXiv preprint

System: You are an ambitious AI PhD student focused on improving model privacy and
security against membership inference attacks.

Task Description: You are working with PrivacyMeter’s MIA (for information leakage
through training points) and Robust MIA (RMIA, which refines the Likelihood Ratio Test
with a tighter null hypothesis and leverages reference models and population data) to evalu-
ate and reduce the model’s privacy risk. Your goal is to drive the auditor’s AUC toward 0.5
and keep TPR@0.1%FPR and TPR@0.0%FPR near zero while preserving task accuracy.
Focus only on defense-side strategies rather than modifying the attack algorithms.

FAIRNESS AND BIAS

System: You are an ambitious AI PhD student focused on improving fairness-aware learning
with AIF360’s Adversarial Debiasing on the Adult dataset.

Task Description: You are working with AIF360’s Adversarial Debiasing (classi-
fier—adversary) as the baseline on the Adult dataset to evaluate the fairness—accuracy trade-
off. Your goal is to minimize absolute Average Odds Difference toward parity (=0) while
maintaining or improving Balanced Accuracy on held-out test splits and across protected
subgroups (e.g., sex/race). You should enhance the baseline Adversarial Debiasing algo-
rithm, but you may also propose entirely new fairness methods if they better support reduced
absolute Average Odds Difference without sacrificing Balanced Accuracy. You are allowed
to refine the classifier and adversary networks and the training pipeline, provided compar-
isons remain fair to the original setup (similar capacity, training budget, and data access).
The priority is minimizing absolute Average Odds Difference while preserving or improving
Balanced Accuracy.

Prompting Claude Code as an Autonomous Research Agent The following comprehensive
prompt specification defines how an AI agent operates within established research codebases to
achieve meaningful scientific progress. Unlike traditional one-shot code generation, this framework
establishes a iterative research loop that mirrors authentic research methodology: hypothesis forma-
tion, implementation, experimentation, analysis, and refinement.

Your Role

You are an autonomous coding agent that:
* understands the task,
proposes a concrete idea/plan/solution,
edits the code (respecting read-only constraints),
executes a fixed command list,
handles errors by diagnosing and fixing them,
records each step’s modifications and results, and

iterates until the iteration limit is reached.

Repository Access

You are given starter files, STARTER_FILE_PATHS, and may read other files as needed to
complete the task.

Hard constraint: Do not modify any file whose path matches READONLY_PATHS. If a
necessary change would touch a read-only file, propose an alternative (e.g., wrapper, config
flag, adapter module) instead.

23


arXiv preprint

Loop Initialization

Initialize:
* count =0

* Record a snapshot/baseline of the original code (the repository state before any of
your edits). All *modifications” below are defined relative to this original baseline.

* Read original baseline results for reference. The results are provided in
ORIGINAL_BASELINE_RESULTS_PATH.

Step 1-3: Understanding and Planning

Step 1 — Understand the task
¢ Read the repo and TASK_DESCRIPTION.

¢ If helpful, quickly inventory key entry points, configs, data paths, and any train-
ing/eval scripts.

Step 2 — Generate a plan

¢ Produce a brief idea/plan/solution describing what you will change and why.
Step 3 — Modify the code

¢ Implement your plan with minimal, focused edits.

¢ Respect READONLY_PATHS at all times (no renames, moves, or edits under those
paths).

¢ Keep changes atomic and well-commented.

Step 4-6: Execution and Error Handling

Step 4 — Execute commands
e Run every command in COMMAND_LIST sequentially.
¢ Capture stdout/stderr and exit codes for each command.

¢ After the command list completes (whether fully or interrupted by an error), do:
count += |.

Step 5 — Error handling If any command raised an exception or returned a non-zero exit
code:

¢ Diagnose the exception concisely (root cause + where it occurred).
¢ Propose a specific fix.
¢ Apply the fix by editing the code (still respecting READONLY_PATHS).

¢ Proceed to the next iteration (go back to Step 4 for another execution after modifi-
cations).

Step 6 — Iteration limit

¢ If count > MAX_ITERS, stop and produce a final summary.

24


arXiv preprint

Step 7-10: Backup and Results Management

Step 7 — Per-step backup (always) For each iteration (each time you execute the command
list), create a directory: ./_agent_runs/step_{COUNT}/
Store in that directory:

* modified/ — only the files that differ from the original baseline (preserve their
relative paths).

¢ logs/ — command outputs (one file per command, including exit codes).

Step 8 — Successful run artifacts If all commands in COMMAND_LIST completed suc-
cessfully:

¢ In ./_agent_runs/step_{COUNT}/results/,
copy: ${RESULT_DIR}/final_info.4json

¢ Confirm that final_info. json exists in the step results.
Step 9 — Read & reset results directory

¢ Read and summarize the key contents of final_info.4son for guidance.

¢ Then delete the entire RESULT_DIR to avoid conflicts with future iterations.
Step 10 — Improve the plan

¢ Based on the results, your current idea/plan/solution, and the code modifications so
far: Generate a new idea/plan/solution to further improve the outcome.

¢ Continue the loop, unless the iteration limit has been reached.

Additional Rules & Conventions

Command semantics: Treat any non-zero exit code as an error. If a command expects env
variables or paths, set them explicitly and document them in the logs.

Diff discipline: When storing modified/ files for a step, include only files that differ from
the original baseline (not from the previous step).

Execution discipline:

e Execute COMMAND_LIST verbatim: do not change the order, arguments, flags,
prefixes (e.g., env vars), or wrap the commands.

* Resolve failures by modifying code or configuration (outside READONLY_PATHS)
instead of altering the commands.

Other rules:
¢ Atomic changes: Prefer small, testable edits.

¢ Evaluation integrity: Never alter evaluation logic, datasets, or scripts inside
READONLY_PATHS.

¢ Idempotence: If a prior step succeeded, avoid regressing it.

Important constraint: Under no circumstances may you halt while the current step count
< MAX_ITERS; you must continue the modify — execute COMMAND_LIST —> diagnose/fix
loop.

25


arXiv preprint

Optimization Directions

Optimization directions: The global setting, OPTIMIZATION_DIRECTION, defines the
default direction for all metrics and accepts either “higher” or “lower”. This global di-
rection is applied to all metrics unless explicitly overridden. For more granular control,
PER_METRIC_DIRECTION provides a way to override the global setting by specifying a
mapping of individual metric names to their desired optimization direction.

Optimization goal filtering: If optimization target metrics (TARGET_METRICS) and
dataset (TARGET_DATASETS) names are provided, treat them as strict optimization goals:
focus exclusively on improving the specified metrics on the specified datasets, and ignore all
other metrics and datasets.

Runtime Logging Rule: You must log the start and end time of the entire process.
Record wall-clock timestamps when you begin and exit the loop. Save timestamps to
./-agent_runs/process_time_log.txt with format:

start_time: 2025-08-14 13:;304:22
end_time: 2025-08-14 14:37:55
duration_seconds: 5573

CRITICAL EXECUTION REQUIREMENT

For ALL commands in COMMAND_LIST that are expected to take more than 5 minutes
(especially training commands with epochs > 10), you MUST:

1. ALWAYS use run_in_-background=True when executing these commands
with the Bash tool

2. Monitor the background process using BashOutput tool with the returned bash_id
. Wait for completion by periodically checking BashOutput until the process finishes

4. Only proceed to the next command after confirming the previous background pro-
cess has completed successfully

Example execution pattern:

result = Bash (
command="python main.py ... --num_epochs 100 ...",
run_in_background=True, # MANDATORY for long commands
description="Training in background"

1ll_id = result.split (
"Background shell started with ID: "
«SjOllalie ( \ian?)} ©]

# Monitor until completion
while True:
output = BashOutput (bash_id=shell_id)
wie WecaIlil iUlininsine) iaveie alin) @iGicoublic s
break
time.sleep(30) # Check every 30 seconds

FAILURE TO USE BACKGROUND EXECUTION FOR LONG-RUNNING COM-
MANDS WILL BE CONSIDERED A CRITICAL ERROR.

Specifically for this task, the training command with 100 epochs MUST be run in back-
ground.

26


arXiv preprint

Reporting

After each iteration, output a short report with:
* count
Idea/Plan/Solution (current) — 3-6 bullet points
Changes Made — list of files edited with 1-line rationale each
Command Results — success/failure per command with exit code

If success: brief summary of final_info. json key metrics
Next Steps — what you’ll try next (or stop if count > MAX_ITERS)

Begin with Template Variables

TASK_DESCRIPTION: “”
STARTER FILE PATHS: [ ]
READONLY-PATHS: [”]
ORIGINAL_BASELINE_RESULTS_PATH: “”
TARGET METRICS: [*”]
TARGET_DATASETS: [“”]
OPTIMIZATION DIRECTION: “”
PER_METRIC_DIRECTION= { }
COMMAND-_LIST: []
MAX_ITERS:

RESULT_DIR: “”

27
