2510.09988v1 [cs.CL] 11 Oct 2025

e
e

arXiv

Unifying Tree Search Algorithm and Reward
Design for LLM Reasoning: A Survey

Jiaqi Wei!*, Xiang Zhang**, Yuejin Yang**, Wenxuan Huang, Juntai Cao”, Sheng Xu*, Xiang
Zhuang!, Zhangyang Gao', Muhammad Abdul-Mageed’, Laks V.S. Lakshmanan?, Chenyu You‘,
Wanli Ouyang”, Siqi Sun®

! Zhejiang University, * University of British Columbia, ? Fudan University,
4 Stony Brook University, ° The Chinese University of Hong Kong, * Equal Contribution

Abstract:

Deliberative tree search is a cornerstone of modern Large Language Model (LLM) research, driving the pivot
from brute-force scaling toward algorithmic efficiency. This single paradigm unifies two critical frontiers: Test-
Time Scaling (TTS), which deploys on-demand computation to solve hard problems, and Self-Improvement,
which uses search-generated data to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous role of the reward signal—is
it a transient heuristic or a durable learning target? This paper resolves this ambiguity by introducing a unified
framework that deconstructs search algorithms into three core components: the Search Mechanism, Reward
Formulation, and Transition Function. We establish a formal distinction between transient Search Guidance
for TTS and durable Parametric Reward Modeling for Self-Improvement. Building on this formalism, we
introduce a component-centric taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

€) Github Repository: https://github.com/More2Search/Awesome-Search-LLM

Training-Time Scaling Test-Time Scaling

Q. @ First Step
ac 0 80B @ Second Step 10 CoT Steps

F ob i" Ff Result
oO — rob)
(&) — Parameters — (1 CoT if! 3Bi
c 0.0 (= 1007 Tokens = | an ( > a ae
= OO000 = a grt
o one) F al = Result —
L &B £ 1 CoT Step “
(7 —_— Training Data Oo Base Models eave
oO 1T Tokens ai Le 500,000 Flops oO 1 Inference If BR >, Betton
ig x Model PQQ
a a ® ; J Sood
a i a - P > Searching

\ Training Time Minimal ety

S:000; lens ae Exploration ./° ig

O Training Budget O Inference Budget
Objective Signal Scalable Training Techniques Objective Signal Scalable Inference Techniques
Gradient Reward
if | @ Second Step... RES

(This Survey's Focus)
Chain of Thoughts Tree-Search Method

Non-Scalable Training Techniques ABP 5: Non-Scalable Inference Techniques

Latent Space gl Task Space “st

Objective Space Objective Space

Recurrent Networks Beam Search Decoding


Contents

1 Introduction

2 Foundational Search Paradigms in General AI
2.1 Uninformed Search: Blind Exploration ............-. 002 eee eee eee eee
2.2 Informed Search: Heuristic-Guided Exploration ............ 0-00 ee eee eeeee
2.3 Monte Carlo Tree Search: Learning from Experience .........--.-++0 eee ee ee

24 ‘Comparisonel ExplorauornSwaregies px i ve tw ee Ew ew Ew

3 Test-time Scaling via Search
3.1 A Tale of Two Optimizations for LLM Scaling: Training-Time vs. Test-Time ..........
3.2 Operationalizing Search in the Objective Space ......... 0.00.

3.3 Decomposing the Objective Space: Prompt and Answer Spaces. .......-.--++2405

4 Reward as a Unified Signal for RL and Search : One Objective, Two Optimizers
4.1 RLvia Policy Shaping: Internalizing Rewards for Generalization ................
4.2 Search via Deliberative Planning: Externalizing Rewards for Specificity ............

4.5 ASymloue Fraewe nk ERR we

5 Monte Carlo Tree Search (MCTS)
5.1 Unified Notation and Problem Formation. ..........-. 000 ee eee eee eens
5.2 A 15-Minute Walkthrough of Core Designs with Unified Notation. ...............
5.3 Advanced Topics and Hybrid Approaches for MCTS ..........-.00 000 eee eee
5.3.1 Multi-Agent and Collaborative Search ..............20.0000000004
5.3.2 Reward Model Design and Optimization ...................2..000.
5.3.3 Search Efficiency and Dynamics ............. 2.000000 002 eee eee
5.4 MCTS for Direct Test-Time Enhancement. .........-.- 000 eee eee eee eee
5A1 General Reasoning & Problem Solvig ..8 cee eet mie eee me em Em
5.4.2 Mathematical Reasoning ........... 2... 0.00002 eee eee eee eee
5.4.3 Code Generation & Software Engineering ...........0. 00 ee eee eens
5.4.4 LLM Agents & Interactive Environments ..............--. 2.0022 eee

5.4.5 Retrieval-Augmented Generation (RAG) & Knowledge-Intensive Tasks .........


5.4.6 Multimodal Reasoning .......... ee ee

54.7 Specialized and Novel Applications . 2... 6 wa eee RE wR RR Ew

5.5 MCTS for Self-Improvement via Data Generation ..........0. 000+ e eee tees

5.5.1 Foundational Self-Improvement Frameworks..............2.2.2.020005

5.5.2 General Capabilities & Alignment .................0 2.0000 00000-

5.5.3 Scientific & Specialized Domains .................0..02.00.0000-

5.54 WiiltmedalAppueeems 2. ik i ee KR ERM Rw

Informed Search Based Method

Search In Prompt Space

7.1 Search Complexity in Prompt and Answer Spaces ..... 2... 2... eee eee ee ee ee

7 Diserete (Text-Based) Prompt Search 2. ci cw tw ee ew eww

7.3 Soft (Latent) Prompt Search... 2... ee ee ee

Challenges and Future of Tree-Search Methods

Conclusion

42
42
43

45
46
47
50

50

52


1. Introduction

As the scaling laws governing Large Language Models (LLMs) reach a regime of diminishing returns [94,
76, 207, 80, 243], the research frontier is shifting from brute-force growth in data and parameters toward
algorithmic efficiency and new forms of reasoning. Two interrelated paradigms have emerged at the core of this
transition: Test-Time Scaling (TTS) [168, 16, 218, 208, 275]—allocating adaptive, auxiliary computation at
inference time to improve problem-solving, akin to the human capacity for deliberate System 2 reasoning [91,
50, 276, 238, 243]—and Self-Improvement via Data Generation, where models construct and refine their
own training signals through high-fidelity reasoning traces [64, 188, 65]. The synergy between these two
paradigms defines a crucial direction for advancing the effectiveness and autonomy of modern LLMs.

Within this landscape, deliberative search algorithms, particularly those structured around trees, have
become the connective methodology unifying TTS and self-improvement. Tree search offers a principled
mechanism to transcend the limitations of greedy, single-path decoding such as standard Chain-of-Thought
(CoT) prompting [206]. By systematically exploring and evaluating multiple reasoning branches, whether
through explicit search [238, 68], heuristic expansion [10, 79], re-ranking [143, 113], or iterative refine-
ment [139, 159], these methods yield substantial gains on complex, multi-step reasoning tasks across diverse
domains [200, 126, 84]. This paradigm, often termed search-augmented reasoning, forms the conceptual
foundation of modern test-time enhancement.

Concurrently, these same search mechanisms serve as engines for durable, parametric Self-Improvement.
Algorithms such as Monte Carlo Tree Search (MCTS) [36, 180] excel at navigating vast solution spaces to
uncover optimal reasoning trajectories that an LLM might produce only stochastically. By distilling these
trajectories into synthetic training data, researchers can fine-tune the base model or train a specialized
reward function, effectively internalizing high-quality reasoning behaviors. This process transforms costly
inference-time deliberation into generalizable parametric knowledge, thereby realizing a self-evolutionary

WA sourbaki ©) cvar-mcts (®) swe-ebate Mila VLAPS === prism
(©) Agentswift 2 Mcts-Refined CoT fas AniMaker % MC-DML (@J DyFo BME MES
“) sigma (CO) mmc === MM-PRM (4) SENATOR S) SELT ((&)) IDEA Ee © PRA & on
NvIDIA.
3 ox
Pz ; a Mas ASTRO e
=] MCTS-RAG \¥) MT-RewardTree © I-MCTS Ez AirRAG () Astar aan Mirage-1 oO ® Pro-Med
WY waster ' cts-tudge @_ wosa ©) star @) meds3 (®) pits oa GJ TRANS-ZERO (=) SE-Agent ® R2-LLMs
- % MCTS-AHD 2 =
i 7 == —= ChemAgent
B tot @ toolchain @ im-planning-eval WA witex @ cts Keon 8 ® TreePO © AIRL-S
Za

2025 6 Apeieh Ez MCTS-HR

©) ats 83 cop-zero A! GiF-mcts $%p svpo @ SAPIENT

& Agent q if y
BB tects () Halusearch @® sramcrs (¥ sam () rare

Be star 22-12 rm ca ae
MD) AR-MCTS Ez Marco-o1 ((®)) ol-coder TOA OD peptune

rex W pro-mcts  ¥% Alphamath “> peop

Bo sa
me XOT @ KNor-mcts © omegaPrm ) ppa

@D sea oo ae oa
RAP | “UCL ERSIOU wa Mindstar QD ares @ Mulberry ® DRPO  () HiAR-ICL ‘© SG-MCTS |" RAG-Star
oa & - 8-10
0 KcTts @ LUM-McTS >| search-agents
. @®) rethinkmcts () swe-search (i!) DAWN-ICL KAIST wzaa  Rects BB cPL
1-7 =
Prompt Agent ie
pt ag PlanSearch (@) LlaMA-Berry (y> 1DEA-McTS [RJEB webpilot EE sc-mcts* \% mcot
2024
2023 ) MCTS-DPO tencenr ALPHALLM seats @) mctsr TEAL at BF vermcrs €) LiteSearch () Rest-mcts*

Figure 1: Landscape of research on tree search algorithms and reward design for LLMs.


loop of continual improvement [172, 258, 65, 251, 82, 23, 222, 183, 159].

Despite rapid and decentralized progress across both TTS and self-improvement research, the field remains
highly fragmented. The diversity of search paradigms (e.g., Tree-of-Thought [238], MCTS variants [68, 222],
Graph-of-Thought [10]), inconsistent notation, and heterogeneous evaluation protocols have made systematic
comparison and cumulative progress difficult (see Figure 1). Moreover, the central concept of reward or value
estimation—often implemented as Process or Outcome Reward Models (PRMs/ORMs) [121, 145, 188] which
are vital for guiding the search—plays fundamentally different roles in transient test-time reasoning versus
persistent parametric optimization, yet this distinction remains underdefined. This conceptual ambiguity
hinders the emergence of general design principles and impedes theoretical synthesis.

To address this gap, we present a cohesive conceptual and mathematical framework that unifies the
growing body of work on search-based reasoning in LLMs. Our framework aims to clarify core mechanisms,
formalize the roles of key components, and establish a rigorous basis for comparing methods across both
TTS and self-improvement paradigms. Our main contributions are as follows:

¢ A Deconstructive Formalism for Deliberative LLMs: We introduce a unified mathematical framework
that dissects and compares diverse tree search algorithms through their core components: Search
Mechanism, Reward Formulation, and Transition Function. We further distinguish the transient role
of Search Guidance (for TTS) from the enduring objective of Parametric Reward Modeling (for
Self-Improvement).

¢ A Systematic, Component-Based Taxonomy: We propose a novel taxonomy that organizes existing
and emerging algorithms along three orthogonal axes—the Search Mechanism (e.g., guided vs. unguided
exploration), the Reward/Value Estimation method, and the overarching Application Paradigm (Test-Time
Enhancement vs. Self-Improvement).

¢ Synthesis and Future Research Agenda: We synthesize key advances across both paradigms, highlight
open challenges in scaling search complexity and designing effective reward signals, and outline a
forward-looking research agenda toward truly self-evolving, deliberate LLMs.

Survey Organization. This survey is organized to build a cohesive narrative from foundational search
principles to their cutting-edge application in augmenting LLM reasoning. We first lay the groundwork by
reviewing classical AI search paradigms (Section 2) and contextualizing them within the modern framework
of Test-Time Scaling (TTS) for LLMs (Section 3). The intellectual core of our survey is a unified perspective
on reward modeling (Section 4), which reconciles the objectives of deliberative search and reinforcement
learning. This unified lens allows for a systematic examination of the two dominant families of search
algorithms employed today: Monte Carlo Tree Search (MCTS) based methods (Section 5) and other informed
search strategies (Section 6). Venturing into an emerging frontier, we then explore a conceptual shift from
search over reasoning paths to search over the conditioning context itself in Section 7, a paradigm we term
“Search in Prompt Space.” We conclude by synthesizing the current state of the field, outlining critical open
challenges and promising future directions in Section 8, before summarizing our findings in Section 9.

2. Foundational Search Paradigms in General AI

Solving complex problems can be formalized as a search task: finding an optimal path from an initial state
to a goal state within a state-action space, conventionally represented as a tree Tg. While classical AI has
developed a rich toolkit for navigating such trees, the state spaces implicit in language model reasoning
present unique challenges. They are not merely large; they are combinatorially vast [61], high-dimensional,


and semantically structured, rendering exhaustive exploration computationally infeasible. This section
revisits three foundational paradigms of tree search—uninformed, informed, and Monte Carlo-based—to
establish a conceptual vocabulary for understanding their modern adaptations for LLM-based reasoning,
where the goal is to identify optimal reasoning paths efficiently.

2.1. Uninformed Search: Blind Exploration

Traditional search algorithms, such as Breadth-First Search (BFS) [141], Depth-First Search (DFS) [182],
and Uniform Cost Search (UCS), a generalization of Dijkstra’s algorithm [41], are uninformed search
algorithms that operate with minimal knowledge about the goal. These algorithms can recognize the goal
state when reached but lack any additional information to guide them toward it efficiently [161, 153]. While
some uninformed search algorithms, like UCS, consider the cost of the path taken so far, none can estimate
the remaining distance to the goal or determine which paths are more promising.

The key characteristic of uninformed search is that it must rely solely on the problem’s basic definition—the
available actions, their costs, and the goal recognition criteria—to systematically explore the search space.
As a result, these algorithms differentiate between possible solution paths primarily through their order of
exploration and accumulated costs. Each algorithm offers different guarantees: BFS finds the shortest path
in terms of steps, while UCS finds the lowest-cost path. Additional variants like Depth-Limited Search (DLS)
and Iterative Deepening Search (IDS) [101] address memory limitations of basic DFS while maintaining
completeness. The choice between these algorithms often depends on the problem’s characteristics and
computational constraints, particularly memory requirements.

2.2. Informed Search: Heuristic-Guided Exploration

In contrast to their uninformed counterparts, informed search, or heuristic search, algorithms leverage
additional knowledge about the goal’s location through domain-specific hints [161, 150]. These hints are
encoded in a heuristic function, denoted h(n) [153]:

h(n) = estimated non-negative cost of the cheapest path from node n to a goal state (1)

Let c(n,n') denote the cost of the path between nodes n and n’. By incorporating heuristics, informed
search algorithms can make educated decisions about which paths are most promising to explore, potentially
reducing the computational resources required to find a solution. The effectiveness and properties of these
algorithms depend critically on the quality of their heuristic functions. A heuristic is considered admissible if it
never overestimates the true cost to the goal, and consistent (or monotone) if it satisfies the triangle inequality
h(n) < c(n,n') +h(n’) for any successor n’ of n [72, 150]. The choice of heuristic function significantly
impacts performance. A heuristic i; is considered more informed than hp if hy(n) > h(n) for all nodes n
and h,(n) > h2(n) for some nodes. More informed heuristics generally lead to more efficient search, as they
provide better guidance toward the goal.

However, there is often a trade-off between the computational cost of calculating the heuristic and the
savings it provides in search efficiency. Common informed search algorithms include Greedy Best-First Search
(BeFS), A* Search [72], Weighted A* Search [152], Iterative Deepening A* (IDA*) [101], Beam Search [176],
and Recursive Best-First Search (RBFS) [102]. These algorithms vary in how they balance the heuristic
estimates with path costs, leading to different trade-offs between optimality and efficiency. For instance, A*
search, when used with an admissible heuristic, guarantees finding an optimal solution if one exists. The
success of these algorithms in practical applications often depends on designing effective problem-specific


heuristics. Common techniques for developing heuristics include relaxing problem constraints, using pattern
databases [38], and learning from experience [163, 161]. While informed search algorithms generally
outperform uninformed search in practice, their effectiveness relies heavily on the quality of their heuristic
functions and the specific characteristics of the problem domain.

2.3. Monte Carlo Tree Search: Learning from Experience

Monte Carlo Tree Search (MCTS) was first introduced by Coulom [36] in the context of computer Go and
later gained prominence as a core component of AlphaGo [172]. It is an adversarial search algorithm, which
aims to maximize winning probability against an optimal opponent. While adversarial MCTS alternates
between players and models opponent responses, the MCTS variant used in LLM’s inference-time search is a
single-agent formulation, where the algorithm explores different action sequences without modeling opposing
players [18]. This adaptation maintains MCTS’s core strengths in balancing exploration and exploitation
through statistical sampling, while refocusing the objective from competitive game-playing to finding optimal
sequences of actions in a non-adversarial environment.

Inference-time MCTS (hereafter referred to simply as MCTS) retains the four fundamental phases of the
original algorithm: selection, expansion, simulation, and backpropagation. During selection, the algorithm
traverses the tree using the Upper Confidence bounds applied to Trees (UCT) policy, which balances exploration
and exploitation by selecting actions that maximize:

re ie

a” = arg max lows +c Nia)

acA(s)

where Q(s,a) estimates the expected future reward of taking action a in node s, N(s) is the number of times
node s has been visited, N(s,a) is the number of times action a has been selected in node s, c is an exploration
constant, and A(s) is the set of available actions at node s [98]. In the expansion phase, new nodes sampled
by LLMs (e.g. subsequent steps in reasoning) are added to the tree to gradually build a model of the search
space. The simulation phase performs rollouts from leaf nodes using a default policy to estimate long-term
rewards, replacing the win/loss outcomes of adversarial MCTS with domain-specific reward measures.

Unlike traditional uninformed search algorithms such as BFS or DFS that systematically explore the state
space, MCTS offers a statistical sampling approach that can handle much larger search spaces. Compared to
informed search algorithms like A*, which rely on pre-defined heuristics, MCTS builds its evaluation function
through experience. This makes it particularly suitable for LLM inference where defining accurate heuristics
is challenging. The algorithm’s ability to balance between exploration and exploitation, combined with its
flexibility in handling large state spaces, makes it a powerful tool for guiding LLM inference, though its
effectiveness depends on carefully managing the trade-offs between computational resources and search
depth.

2.4. Comparison of Exploration Strategies

Figure 2 provides a conceptual illustration of these distinct exploration strategies. Uninformed algorithms like
BFS and DFS are governed by rigid, topology-driven expansion protocols. Informed search, exemplified by
A*, introduces goal-directedness by prioritizing search based on a heuristic cost-to-go estimate, h(-), allowing
it to focus on promising regions irrespective of tree topology. Finally, MCTS replaces the static heuristic
with a dynamically learned value function, estimated via statistical sampling. This adaptive, self-correcting
mechanism allows it to focus computational resources on the most promising regions of the search space


A* MCTS

| . alee aertion Wr \
\
OO0ODd0

a dng "

~

x
AyWouUd youees @&

fe)
=

Highest h( - ys «Lowest ht - )

Figure 2: A visual comparison of four fundamental tree search algorithms, where node color intensity
represents search priority. BFS explores exhaustively level by level, while DFS commits to a single path until
a leaf is reached. In contrast, informed search like A* uses a heuristic function /(-) to prioritize nodes with
the lowest estimated total cost, regardless of their depth. MCTS introduces a statistical approach, using
simulated rollouts from leaf nodes and backpropagating the outcomes to dynamically guide the search
toward high-reward regions of the tree.

without requiring prior domain knowledge encoded in a heuristic. This very property makes it the preeminent
search paradigm for navigating the vast and ill-defined reasoning spaces of large language models.

3. Test-time Scaling via Search

As the scaling of model parameters and training data yields diminishing returns [93, 75, 207], a new frontier
has emerged: test-time scaling [35]. This paradigm investigates how to optimally allocate computational
resources during inference to enhance a model’s effective reasoning capabilities [238, 68, 243, 200]. Unlike
training-time scaling, which refines a global, amortized policy by encoding knowledge into a model’s weights,
test-time scaling performs instance-specific optimization for a given problem Q. This section provides
a detailed, mathematically-grounded analysis of these two orthogonal paradigms, contrasting how they
operate in fundamentally different optimization landscapes: the latent parameter space for training versus
the task-defined objective space for inference [268].

3.1. A Tale of Two Optimizations for LLM Scaling: Training-Time vs. Test-Time

The figure referenced illustrates two distinct approaches for improving model performance, each defined by
its unique objective signal and the space over which it optimizes.

Training-Time Scaling: Optimization in Latent Parameter Space. During training, the primary goal is
to learn a set of parameters 6* that minimizes an expected loss function £ over a data distribution D [17].
The optimization problem is formally stated as:

o* = arg min E(i)~p[L (fe(i),0)],

where © C RN is the high-dimensional latent parameter space. The objective signal in this paradigm is the
gradient of the loss with respect to the parameters, Vg£. Optimization proceeds via iterative updates, such


as stochastic gradient descent. The result is a static artifact—a trained model 7-—that implicitly represents a
posterior distribution over solutions.

Test-Time Scaling: Optimization in Task-Defined Objective Space. Given a fixed, pretrained model 7,
test-time scaling seeks to find an optimal reasoning trace p* for a specific problem instance Q. This process
constitutes a second, distinct optimization loop [172, 112]. The search occurs in a discrete, structured
task-defined objective space, the solution space P(Q), which consists of all possible reasoning traces. The
objective signal is a scalar reward or value that evaluates the quality of a trace. The optimization problem
at inference is therefore:
m
_ peA(7 0,Citer) VP)

where A(71, Q, Cinfer) is the search algorithm that explores a subset of P(Q) guided by the model’s prior 7 and
constrained by the inference compute budget Cinfer, and V(p) is a function evaluating the final trace. Scalable
inference techniques, such as tree search, use intermediate rewards r; or partial trace values v; to dynamically
allocate compute to more promising regions [238, 68]. The evaluation function V(p) providing these signals
can range from heuristics to learned Process or Outcome Reward Models (PRMs/ORMs) [121, 188, 258, 65].

3.2. Operationalizing Search in the Objective Space

The conceptual shift from gradients in latent space to rewards in objective space necessitates a different
class of optimization algorithms. While training relies on gradient-based methods, test-time scaling is
operationalized by search procedures that can navigate complex, non-differentiable solution spaces [162].

Tree Search as a Scalable Inference Optimizer. Tree search methods, particularly Monte Carlo Tree
Search (MCTS) [36, 99], provide a principled framework for this optimization. They build a search tree
Tg where each node C; corresponds to a partial reasoning trace p;. At each node, an action selection
policy balances exploiting known high-reward paths and exploring novel ones. For LLM-based search, this
policy often uses a PUCT-style rule that incorporates the policy network’s prior, as popularized by AlphaGo
[172, 172]. The next action a* is selected by choosing the action that leads to the most promising child node:

a* = argmax (qj + U(Ci,C;)),
acA(s;)

where s; is the state at the parent node C;, and action a leads to the child node C; with quality value q;. The
uncertainty bonus U(C;,C;) is formulated as:

nj
U(CisC}) = comp (Alpi Q)- gn
J

Here, n; and nj; are the visit counts of the parent and child nodes, respectively. The policy 7t provides a prior
probability for taking action a given the history p;, and cexp is an exploration hyperparameter. This synthesis
allows the algorithm to scale reasoning performance effectively with the allocated inference compute budget
[68, 188, 258, 65].

3.3. Decomposing the Objective Space: Prompt and Answer Spaces

The task-defined objective space, over which test-time search operates, is not monolithic. It can be productively
decomposed into two distinct, hierarchically-related search spaces: the Prompt Space (details in Section 7)


and the Answer Space (details in Section 5 and Section 6). This decomposition clarifies the mechanisms
of Chain-of-Thought (CoT) reasoning [206] and reveals the limitations of many current test-time search
methods. The overall optimization problem is thus a search for an optimal reasoning trace, which involves
finding both the right algorithm and its correct execution.

The Prompt Space (7): Searching for an Algorithm. The prompt space, ?, encompasses the set of all
possible reasoning structures or “step templates” an LLM can adopt to solve a problem. Each template p € P
represents a specific strategy for externalizing and manipulating information from the model’s latent state h
into its textual output space [268]. In essence, selecting a template p is equivalent to selecting an algorithm.
For example, one template for a complex arithmetic task might involve explicitly tracking a running total,
while another might only verbalize intermediate calculations without a canonical state representation. The
search for effective prompts or reasoning structures has itself become an active area of research [282, 230].

The choice of template is paramount because it dictates the computational graph the model simulates
through its autoregressive generation. While theoretical work suggests that a CoT-augmented Transformer
can be Turing-complete [116], this potential is contingent on generating the correct computational trace. An
suboptimal template can lead to an inefficient or even intractable search by failing to surface the necessary
state information for subsequent steps, effectively breaking the simulated recurrence. The search for an
optimal p* € P is therefore a meta-level optimization: discovering the most effective procedure for solving
the task instance.

The Answer Space (S): Searching for a Solution. For any given prompt template p, there exists a
corresponding answer space, S,, which contains all possible reasoning traces (i.e., potential solutions) that
can be generated by adhering to that template’s structure. The complexity of navigating this space is critically
conditioned on the choice of p. An effective template p* dramatically prunes the answer space, simplifying
the path to a correct solution. Conversely, a poorly chosen template p’ can render the answer space vast and
unstructured, making the search computationally infeasible even with a large compute budget.

Many contemporary test-time compute methods operate primarily within this second level of the hierarchy.
They typically fix a single, heuristically-defined prompt template (¢.g., via a generic instruction like “think
step by step”) and then explore the resulting answer space S,. These approaches can be broadly categorized.
Some, like Tree-of-Thought [238] and Reasoning as Planning [68], employ formal search algorithms. Others
utilize tree-like or graph-based branching structures but rely on heuristics, aggregation, or sorting rather than
explicit search procedures, such as Graph-of-Thought [9], Tree-Planner [79], and Boost-of-Thoughts [26].
Distinct from these incremental exploration methods are frameworks that first generate multiple complete
solution candidates and then either re-rank them [113, 143] or iteratively revise them [139, 159]. While all
these approaches excel at mitigating execution errors and exploring diverse solution paths within a fixed
algorithmic strategy, they do not address the foundational challenge of selecting the algorithm itself. If the
governing template p is flawed, even an exhaustive search of Sy, is unlikely to yield a correct solution.

A Unified View of Test-Time Search. A comprehensive framework for test-time search must therefore
account for the joint optimization over both spaces. The ultimate objective is to discover a solution trace s*
that maximizes the value function V(-), where the search spans all possible traces allowed by all possible
templates:
*
s* =arg max_ V(s
5 pEPsESp ( )

This formulation highlights a critical gap in current research. While significant effort has been invested in

optimizing search algorithms within a given answer space Sy», the systematic exploration of the prompt space

10


Reward as RL Signal Reward as Search Signal

Va
(A) (B) (C) | think there’s a

ae a ae ah eek

§ Next Encounter ¥ Next Encounter Next Prout

4

External
Oracle

Figure 3: Reward Design: Search vs. RL. (A) In RL, a positive reward updates the agent’s policy, making it
more likely to repeat the action. (B) A negative reward also updates the policy, discouraging the behavior.
The change is durable. (C) In search, an external oracle provides a reward signal to guide the current
decision process without altering the agent’s underlying parameters.

P remains a largely open challenge [144, 282]. The true potential of test-time scaling lies not merely in
executing a known algorithm more robustly, but in dynamically discovering the most effective algorithm for
the specific problem at hand.

4. Reward as a Unified Signal for RL and Search : One Objective, Two Optimizers

In advanced AI systems, a reward signal is the fundamental currency for guiding behavior. However, its role
bifurcates into two distinct yet complementary functions depending on the temporal scope of the objective:
shaping a durable, long-term policy versus guiding a transient, short-term plan. This distinction is not one
of paradigm but of application—whether the reward is used to permanently update the model’s internal
parameters (RL learning) or to direct a temporary search with fixed parameters (planning).

4.1. RL via Policy Shaping: Internalizing Rewards for Generalization

When a reward signal is coupled with a learning algorithm, such as in Reinforcement Learning (RL) [180], its
purpose is to be internalized. The feedback from the reward directly modifies the model’s weights, creating
lasting changes in its behavior. This process is analogous to skill acquisition, where experience is distilled
into a robust, general-purpose policy that governs the agent’s "instincts" across all future tasks. Formally,
this involves optimizing policy parameters 0 to maximize an objective Zp, that integrates task rewards with
adherence to a set of universal principles P, a technique central to modern Reinforcement Learning from
Human Feedback (RLHF) [34, 145, 7].

The optimization objective can be expressed as finding the optimal parameters 6* that balance expected
cumulative rewards G(T) over trajectories Tt with a regularization term that enforces alignment with a foun-
dational policy prior 77p. This is commonly implemented using algorithms like Proximal Policy Optimization
(PPO) [166]:

0° = argmaxE rn (G(r) —A f Dax (mols) |h"r(-l8)) as

where Dx, is the Kullback-Leibler divergence, measuring the "cost" of deviating from the ingrained principles,
and A is a hyperparameter controlling the strength of this alignment imperative. Because this learning is

11


permanent, the reward function is designed to instill universal, foundational principles—for example,
promoting logical consistency, ensuring truthfulness, or encouraging methodical, step-by-step reasoning
[5, 121]. The objective is not to solve a single problem but to forge a broadly capable and aligned agent. The
reward here acts as a long-term teacher, shaping the agent’s intrinsic character for future, unseen challenges.
Architecturally, this is often achieved by adapting a pretrained LLM into a reward model—such as a Process
Reward Model (PRM) or Outcome Reward Model (ORM)—by replacing its final unembedding layer with a
multi-layer perceptron (MLP) that outputs a scalar value, a practice standard in RLHF [145, 121].

4.2. Search via Deliberative Planning: Externalizing Rewards for Specificity

Conversely, during test-time search, the reward signal functions as an external, ephemeral guide. It directs
a deliberative process, like Monte Carlo Tree Search (MCTS) [36, 99], to navigate the solution space for a
single, immediate task. The reward evaluates candidate action sequences (plans), allowing the system to
identify a high-quality solution for the specific problem at hand. This approach, famously demonstrated by
AlphaGo [172], uses a fixed model to provide powerful heuristics. This iterative, step-by-step evaluation
of intermediate actions distinguishes search-based methods from other test-time compute frameworks like
re-ranking, which first generate multiple complete solutions and then score them [143, 113], or sequential
revision, which iteratively refines a complete solution [139]. For a given task with a specific external reward
function Rext, the goal is to find an optimal plan p* that maximizes a combination of this external signal and
an internal, path-dependent heuristic H, provided by the frozen model.

The optimal plan p* for a state sequence sg, s1,...,S7 resulting from the plan’s actions is found by solving:

T-1
p =arg agen Ss Y' Rext(St, at) + Ho(sr, »]
plan t=0

where the heuristic Hg is not just a simple state evaluation but a complex function of the final state s7 and
the path p taken, potentially incorporating penalties for path irregularity or deviation from the model’s
learned priors. This can be viewed through the lens of planning as inference [6, 184, 12], where desirable
paths have lower energy or higher probability:

Ho(sr,p) = Vo(sr) — B- log ( | e0/vap)
P

N(p)

Here, Vg(sr) is the model’s intrinsic value estimate, while the second term acts as a complexity penalty
based on the "free energy" over a neighborhood of paths \’(p), discouraging overly surprising or convoluted
solutions. Crucially, this feedback is discarded once the task is complete; the model’s underlying parameters
6 remain untouched. This makes the reward an ideal tool for task-specific, localized objectives without the
risk of corrupting the model’s general-purpose policy.

4.3. A Symbiotic Framework

Ultimately, policy shaping and deliberative planning are not competing methodologies but two integrated
components of a sophisticated decision-making architecture [172]. This mirrors the dual-process theory
in human cognition, where a fast, intuitive "System 1" (the policy) is complemented by a slow, deliberate
"System 2" (the search) [91]. The RL-trained policy provides the foundational intuition, offering high-quality,
pre-compiled heuristics that make the search space tractable. Search then provides the focused deliberation

12


needed to refine these intuitions into a precise plan for the current context. This symbiotic relationship can be
captured in a single, bi-level optimization objective, where the outer loop learns the policy parameters 6 by
anticipating the outcome of the inner-loop search process over a distribution of tasks Z € D, as exemplified
by models like MuZero [165]. More recent work has explicitly fine-tuned LLMs to improve their synergy
with search, training them to function as better policy, value, and reward models within these deliberative
frameworks [188, 258, 65]. This fine-tuning paradigm also extends to other test-time compute methods,
such as training models specifically for sequential revision [159].

The overarching goal is to find policy parameters 6* that maximize the true, ground-truth reward Rtrue
of the plans generated by the search algorithm:

T1
6* = arg max Ezwp | Rerue | arg aes Ss" + Rextz (St, ar) + He(st, p)
plan t=0

This formulation reveals the deep connection between the two processes. The outer optimization (learning)
seeks to create a model whose internal heuristic, Hg, is maximally useful for the inner optimization (planning),
which in turn must produce plans that score well on the final, external metric Rtrue. In essence, one process
builds the artist’s foundational skill over a lifetime, while the other guides the brushstrokes for the single
masterpiece they are creating now.

5. Monte Carlo Tree Search (MCTS)

Q | Given x+2 = 4, solve the value for x Note: We use t; = (a;,s;) here

terminal
(foo Mog» Too)

‘terminal
A. | Cog a

Figure 4: Unified Notations for MCTS-Based Methods in LLM.

5.1. Unified Notation and Problem Formation

We adopt the notation conventions introduced in ReST-MCTS* [257] to formalize MCTS in the context of
LLM reasoning in a unified manner. This approach ensures that all the articles surveyed adhere to a consistent

13


notation system (with minor adjustments to accommodate unique designs), allowing for a clear comparison
of their methods without the reader having to navigate the discrepancies in notation.

We first introduce the table of notations in Table 1 and Figure 4.

Table 1: Unified Notations for MCTS-Based Methods in LLM Reasoning

Symbol Definition
Q Input question or problem for which reasoning is being performed
C User prompt or conditioning input used to bias the reasoning traces
Aj Reasoning action at step i generated by the LLM (policy network), where a; € A
Sj Reasoning state at step i resulting from action a;
Pi Partial reasoning trace up to step i, defined as p; = [s1,52,...,5;|
Is; Single-step reward for state s;, measuring its quality independent of previous states
Vi Value of partial solution p;, indicating its potential to reach a correct final answer
To Search tree for problem Q, where each node uniquely identifies a reasoning trace
7 Policy model (LLM) used to generate reasoning steps during tree search
Vo Value model that computes partial trace values: vj; = Vo(p;)
Ro Reward model that generates single-step rewards: r,, = Rg(s;)
A Action space available at state s;, representing all possible next actions
C; Search tree node, represented as C; = (t;,1;,q;) where:

e t;: tree node that identifies C;
e n;: Visit count of node C;, tracking exploration frequency

e gj: Quality value of the partial solution at node C;, indicating its potential to lead to
a correct answer

With this set of notations defined, a search problem in LLM based reasoning can be generalized as finding
the correct solution or the optimal reasoning trace p’ = [s/,,s4,--- ,s/,] for a given problem Q.

We categorize approaches for finding correct final solution (a specific terminal state s’) as goal-driven.
Goal-driven methods focus primarily on arriving at the correct final answer for given reasoning problems,
paying less attention to the reasoning trace that leads to it. In contrast, approaches that aim to identify good
or optimal reasoning steps for a given problem are categorized as step-driven. Step-driven methods not
only seek to find the correct solution but also emphasize discovering high-quality intermediate steps that
contribute meaningfully to the reasoning process and minimize the reasoning distance.

In the search process, the reasoning LLM acts as a policy network 7r(-|Q,c). where it generates a sequence
of reasoning steps or actions to solve the problem Q, under a given instruction prompt c. The sequence of
generated state-action pairs by 71(-|Q,c) is denoted as [s1, 41, 52,42, 83,43, ++ ,Sn], Where sj is the initial state
(often a dummy answer or system prompt) and s,, is the terminal state. The terminal state s,, is reached
when [eos] (i.e. end of sequence) token is produced, which may signify the generation of a final answer
(correct or incorrect) or the exhaustion of the step limit (e.g. max context length) [266, 267].

14


Note that, unlike most other reinforcement learning (RL) problems, where an action a; leads to different
states s;,; based on a state transition probability, a reasoning action a; in LLM-based reasoning determinis-
tically leads to a fixed next reasoning state. This deterministic nature is due to the structure of reasoning
(with rare exceptions). As a result, we clarify the usage of certain notations, which may differ from those in
typical RL formulations:

¢ A reasoning trace, or partial solution, p;, can be expressed in two equivalent forms:

Pi = [S1, 41, 52,42, 53,43, .. ., Si

or
Pi = [S1,82,83,..+,Si]-

The first form treats actions as distinct from states, while the second combines actions and resulting
states into s. There is no inherent difference between the two representations, as LLM outputs both s;
and a; into a sentence in each reasoning step during Chain of Thought. Some looks at it separately
(such as RAP) while others take a joint view (such as ReST-MCTS*).

* Unlike traditional RL, where the reward is calculated based on the state-action pair, denoted as R(a,s),
and depends on the different state transitions resulting from action a, the reward of a single LLM
reasoning step can be evaluated based on either the action a; or the resulting state s;;, or even on state
action pairs (s,a), due to the deterministic nature of reasoning (each a deterministically determines s).

For simplicity, we typically consider s; to be a natural language sentence generated as one chain-of-thought
(CoT) reasoning step. Consequently, p; = [s1,52,53,...,s;] represents a CoT trace consisting of i sentences
generated in i sequential steps by LLMs.

During reasoning, a given reasoning state s; can transition to different next reasoning states s;11, deter-
ministically, depending on the different action a; that is chosen (from the action space A) by the LLM policy
7, forming a tree structure, denoted as To.

Monte Carlo Tree Search (MCTS) optimizes the search for the reasoning trace [s1,52,...,8,] in Tg to
find correct answers. Each partial solution trace p; = [s1,52,...,s;] forms a unique path (or even node) in
this tree, associated with its estimated value v; and visit count n;. The value v; defines how promising such
partial trace is to reach the correct answer. MCTS process is guided by this promising indicator vj.

Unsurprisingly, the design and computation of v; become one of the most critical challenges in search
algorithm design for LLM reasoning. Our survey places particular emphasis on the methods used to design
the value function V(-) in each of the surveyed papers.

All of the search to be discussed here is done in Answer Space of problem Q, for the discussion of searching
in Prompt Space of LLM, refer to Section.

5.2. A 15-Minute Walkthrough of Core Designs with Unified Notation
ReST-MCTS*

ReST-MCTS* [257] adopts a step-driven approach, emphasizing the discovery of high-quality reasoning
traces and the optimization of intermediate steps. Its novelty lies in integrating MCTS with process
reward guidance to automatically generate high-quality traces without manual annotation. These traces

15


Table 2: Comparison of MCTS Node Representations and Evaluations

Model Name Tree Node Node Evaluation Evaluation Need
Current reasoning trace
ReST-MCTS* 9; = (S1,S2,...,8) v; = V, ji
Pi = (S182 i) i = Vim (pi) = CrpSipeees 8)
RAP (aye8}) f= R(aySi) = rey ? as" Current state-action
LLaMA-Berry gterminal 1; = aR, 1 (sterminal,y +4 (1 _ a)R, jk 1 (sterminal y Current and all previously
: cea gona" t explored solution nodes in Tg
. Fash i RI (sterminaly 42h soe R (terminal ) ;
MCTSr geminal "2 \ ic evaluating n times LLM”! neal current solution s‘e™minal
re-evaluating 1 times to improve robustness.
0; = Vum(pi = (41,51,--+ ,4i,8;)), ifs; € solution node, All history states
TS-LLM (ai, i) fernibeal terminal “ s : z = i nueey..Be
ri = Rorm(a} MC *), if s; = solution node. Pi = Bi, S25 Si)
vj = VeNre(n; = (a4,81,-+- ,4;,5;), if s; A solution node,
current state 5;
ALPHALLM (ai, si) 17 = Reem (si), ifs; # solution node, history p; of a
ptenmunal = Rorm(ste™ina!), if s; = solution node. : ‘
0, if s; A solution node, = s B6288
PG-TD (ities) He ifs; # Bi = (41,81, 42, 82,° >» $i)
test cases pass rate: TEST(p; = (a1,51,---,si) ), if s; = solution node. and test cases provided
0, if s; € solution node, Current state-action
rStar (aj, 8;) i ae ;
mutual agreement rate Ryoting(sj"""""), if s; = solution node (4i,8;)
test cases pass rate: TEST(p; = (41,51,---,5;)), if 0 < TEST <1, i
RethinkMCTS (a;, 8) = Pp (pi = (41,51 si) ) TOs (pi) History trace p; and
a- TEST(p;) + B - Vitm(pi), if TEST(p;) = 1, public test cases
HiAR-ICL (PRM) (4j,5;) 1; = Rppm(s;) —— a)
ir Si
: 0, if s; € solution node, -acti
HiAR-ICL (ORM) (03 _ ay iF i Current state-action
Rorm(s;"™""), if s; = solution node (4i,8i)
- - 0, if s, solution node, i i
HiAR-ICL (Self-Consistency) (a;,8;) n= ae sit : Terminal solution
VOTING(s/"™""), if s; = solution node 8;

Agent-R

Average reward Q(s;) from rollouts;
8; based on final environmental
reward r(t) € [0,1]

Final reward r(t) fora
complete MCTS rollout

Complete trajectory continuation

. ‘ ) = yN-i #
Reopens "4 Vie ease) from step s; to final answer
. _ - Current agent’s full context
MASTER Si (ro,i,C0,) = Vitm-sett-evat (si) (Solution + Validation)
. P(rl{Tn } descendants) History of scores from
ABEMCTS Fout (Posterior predictive dist.) descendant nodes
ocontaitinp states Current answer s and
SELT whictcs cheremoning path A = Scoreyio (5; Srepresent) representative answers
from clusters S;epresent
Original source text Xsr¢ and
TRANS-ZERO Yi r(yi) = Maxwe{x,}(S(Xw, Xsre)) the set of reconstructions {x,,}
from multilingual rollouts
CMCTS ri = Q(si-1,4;) + V(si) Current state s; and

where Q and V are from PRM

previous state-action (s;_1,4;)

are then used to iteratively train improved reward and policy (LLM) models. This approach significantly
enhances reasoning trace quality, achieving superior performance on datasets such as MATH, GPQA, and
CEval compared to baselines that do not leverage MCTS.

Evaluator-Modeling: A separate LLM (Mistral), distinct from the reasoning policy LLM, is used as the

value model V(p;) to evaluate partial reasoning traces p; = [s1,52,...

, Si]. The model is fine-tuned on

DFS-searched reasoning data with automatically labeled quality scores, capturing the likelihood of p; leading
to a correct solution. During MCTS rollouts, V(-) evaluates each p; and this value is stored in each node

directly.

16


Table 3: Comparison of MCTS value Q update and visit count n update

Model Name Tree Node t; Update (back-propagate) Q; using node value Update n;
nj-0;
" ft jeChildren(p;)
ReST-MCTS* Pi= (s1,S2, ang ,8i) OF nl <> —_ sa, he—n-ed
jeChildren(p;)
update
RAP (aj, 8;) Qi — nj = nj + il
max % ave (ri, Vidds-- oy)
each roll out §;,4j,1j,-.-,S],4),1n seuminal
; for j € Children(st!™) ;
LLaMA-Berry gpermainal J areiitic G; )
Qa = (1-9) 49-8;
MCTSr sieminal Qypdate = ; (o + max "|
j€Children(s;)
update
TS-LLM (MCTS-a) Qo =
(MCTS-Rollout) (ai, )) Q;+ vn if rollout final s,, 4 solution node,
Q;+ rn if rollout final s,, = solution node
update + Qi + By: 0; + Berm: rj +
. terminal
ALPHALLM 4j,S; Bor * Egrerminat yyy (s)) [im
roll-out to terminal node s‘e"™"! n times
to estimate expected reward of ORM values.
update ,
PG-TD 4, 5; Q; < max(Qi, Tm)
where s,, is terminal roll-out state
update
rStar i, $i Q; = Qi + tm
where s,, is terminal roll-out state
RethinkMCTS Aj, 8; Qrpeate ¢ max (Qi,r;)
j € Children((a;,s;)
HiAR-ICL (PRM) Qj, 8; Ox ae ew - Qj) + (1— &) - min(Q;, 741) nj<nt+i
update
HiAR-ICL (ORM) 83 Qe &- Qi + (1-4) +m neds pack I
where s,, is terminal roll-out state
update
HiAR-ICL (Self-Consistency) Qj, Sj Qi oe Qi - (1 =) - rm ni nj+1
where s,, is terminal roll-out state
Agent-R 8; Qrpene Genitals) nj=nj+d
Greedy trajectory replacement, not value backprop:<
Retro-Search Sj Replaces if V(Snew) > V(Sola)- N/A
: Only on backpropagation:
MASTER Si Qi = coi +o; + (1— co’) > se Depa tn I, wh vt Propag
AB-MCTS ' Update posterior parameters. E.g., for Beta dist: Implicitly tracks number of
° cue &R-—-K+ rn, B— B+ OI — rn) observations for posterior update
SELT v containing states Q(v)<+ Q(v) +A N(v) + N(v) +1
TRANS-ZERO Yi Qypaate < Q; +1; (where 7; isfromchildnode) nj =n; +1
os
CMCTS Sj update __ ee nj=nj+1

i nj

Evaluation-Function Design: ReST-MCTS* introduces a weighted reward w;, instead of standard 1;,
to better reflect the quality and contribution of a single reasoning step (state) s;. The weighted reward is
defined as:

1— 0j-1

1 —2r;

Wi =

where v;_, is the value of the previous trace, and m; is a heuristic measure of the remaining reasoning

17


Table 4: Reward Model Training: Input Generation, Label Construction,

Reward Model

Reward Label

and Datasets

Model Name Input Generation Generation Reward Reward Model
P Model Train Dataset
4, SeiinserierOuestion For correct reasoning traces,
co-training both answers and 2 GUC OS GATASE
eT aE oT is assigned to each partial
ming 7 trace p;,, computed as
2. Dataset with only answers; Oo. = WK, where bisthe
reasoning traces are collected ou ent ae and Kis the
ReST-MCTS* using DFS-based reasoning P, ; V(p) MATH,

: : total number of steps in the : :
traces (with Mistral-7B : ScilnstructQuestion
model) gener tedinranel correct reasoning trace. For
ree, ath. free on dente incorrect traces, v;, values are
search on verified search penalized sere 0, a
Hees reflecting the trace’s inability

. to lead to a correct solution.
Target labels are calculated
using Temporal Difference
Rollouts generated using a (TD-A) or Monte Carlo (MC)
supervised fine-tuned policy methods.
(LLaMA2-7B). TD-A uses a weighted sum of
TS-LLM (Value Model) Inputs are partial trajectories _n-step returns and bootstraps V(p) GSMBK,

TS-LLM (ORM)

pi = (S1,82,...,8;) from
sampled reasoning traces
across tasks like GSM8K and
Game24.

Terminal states s‘e™minal

generated from sampled
rollouts of a fine-tuned policy
model.

with the predicted reward
V(sr) at the terminal state.
MC estimates the full
cumulative return directly as
the sum of rewards from s; to
ST.

Labels are assigned using a
binary reward based on
solution correctness and
quality. Correct solutions
receive +1, while incorrect
ones are penalized with —1.
Rewards are derived from a
task-specific reward function.

R ( geemminal)

Game24 Rollouts

GSM8K,
Game24 Rollouts

Reasoning traces are sampled

Reward 7; is computed using
Temporal Difference (TD) or
Monte Carlo (MC) methods

ALPHALLM (Value Model) : by reaching solution nodes On GSM8k Game24
froaxpaltey ELM (Lava, and assign scores based on and PrOntoQA
expected correctness of
roll-out trace from each node.
: Immediate rewards pe are
reasoning traces are reused : : :
from the value function Ses glee uli ens
ALPHALLM (PRM) ; sampling with textual PRM GSM8k Game24
Inputs are sampled node : :
templates for intermediate and PrOntoQA
values (s¢, ar).
correctness assessments.
‘ F Labels r°8™ are binary (+1
terminal Ty
ALPHALLM (ORM) ‘Tseirtinel states s cata for correct, —1 for incorrect ORM GSM8k Game24
sampled trajectories are used. :
solutions). and PrOntoQA
Uses a pre-trained PRM. Uses a pre-trained PRM.
CMCTS Input generation is external Label generation is external PRM for External to framework

to the CMCTS framework.

to the CMCTS framework.

Q(s,a) and V(s)

(depends on pre-trained
PRM, e.g., from [58]).

distance. This approach prioritizes steps closer to the solution, improving exploration of promising paths.

MCTS Design: ReST-MCTS* primarily follows standard MCTS design but introduces a self-critic mecha-
nism during the MCTS expansion phase. Self-critic is also used in deterning termination node in Tg.

18


RAP

RAP [69] (Reasoning as Planning with World Models) adopts a goal-driven approach, focusing on efficiently
navigating reasoning paths to directly achieve correct solutions. This work innovates by re-purposing the
same LLM as both a reasoning policy (producing action a;) and a world model to simulate state transitions
(producing s; and reward returning (R(a;) model). By combining MCTS with state-action rollouts guided
by the world model, RAP demonstrates improved efficiency and accuracy across tasks, achieving strong
performance on benchmarks like Blocksworld and logical reasoning datasets while reducing reliance on
pretraining or additional reward models.

Evaluator-Modeling: A repurposed LLM (LLaMA in this work), which is the same model as policy
(reasoning) LLM, is used to generate per-step reward. This model generates not only next state s; but also
returns reward for each newly derived state action pair (s;,a;). This world model (same as reward model R)
capture both action likelihood and task-specific progress when calculating reward r;. Evaluation occurs at
each node ((s;,a;) in this case) and focuses solely on the current state-action pair, rather than the full trace,
ensuring efficient and lightweight evaluation.

Evaluation-Function Design: RAP introduces a novel per-step reward r; = R(s;,a;) that combines action
likelihood and task-specific evaluations to assess the quality of each reasoning step. The reward is calculated
using a weighted geometric mean of two components:

Tt
— rey Th. (3)

where 1; is the log-probability of the action a; as predicted by the policy model, reflecting its confidence in
the chosen action. r; is a task-specific score evaluating how well the predicted next state s;,; (generated by
the world model) aligns with the task’s objectives. This is often derived using heuristics or domain-specific
metrics.

The value Q of current node is calculated by using the max value of average future reward of each
rolled-out future trace starting from this node :

— max ave(r:,...,17).
Qi St At Vt y---/S] ALT] S141 8( a ’ 1) (4)
where 81, 41,1t,--+,S],41,11,$141 is one specific rollout trace starting from current node (s;,a;). this value v
will be used in MCTS selection.

MCTS-design: RAP uses this v value for selection process in MCTS and sticks with standard MCTS design
for expansion and backprogration.

LLaMA-Berry

LLaMA-Berry [260] is strong goal-driven approach, emphasizing the refinement of complete reasoning
solutions. It changes the way of building the reasoning tree Tg by using only final result s*°*™*"*? as each
node, and each child node is a refinement of parent solution. It also innovates the evaluation framework
by usingPairwise Preference Reward Model (PPRM), using global win-loss matrices and local adjacency
comparisons to rank these solutions. By leveraging critiquing and rewriting during tree expansion, LLaMA-
Berry ensures accurate self-refinement of solutions, achieving robust performance across explored solution
paths.

19


Evaluator-Modeling: A fine-tuned small LLM (Gemma2-2B-Instruct) serves as the Pairwise Preference
Reward Model (PPRM). PPRM, which is Rg(s*®*™"*") in out notation system, predicts the quality (r;) of
single state solutions by comparing directly with other solutions. It is trained using ranked reasoning traces
and produces a win-loss matrix M(i,j), where M(i,j) encodes the preference between two solutions s; and
s;. During evaluation, the PPRM gives a ranked score when each new solution is derived.

Evaluation Function Design: LLaMA-Berry introduces an evaluation function that combines a local
reward (09a) and a global reward (gjopa1) of a derived solution s;, defined as:

Q; _— Riocat ( gierminaly et Retoval ( gferminaly

The local value evaluates the quality of s5°""'"*" based on its immediate neighbors in the reasoning

tree. Pairwise comparisons are performed with the preceding solution gpeaimal and the subsequent solution

sferminal Using the Pairwise Preference Reward Model (PPRM), the local value is calculated as:
Rica (Sern) = : (PPRM(sie™n* gyecininal + PPRM (sterminal sien) )

The global value assesses s‘¢"™! based on its position within the entire set of explored solutions in Tg.
A win-loss matrix M(i,/) is constructed using pairwise comparisons between all solutions:

1 if s; is preferred over s;,
M(i,j) = 4 —1_ ifs; is preferred over s;,
0 if they are equally preferred.

The global value is computed using the Enhanced Borda Count (EBC), which aggregates the win-loss scores

for each solution:
Raa) _ S- M(i,{)
j

where the sum is taken over all other solutions s; in the reasoning tree.

MCTS design: LLaMA-Berry follows standard MCTS principles with significant enhancements in its
expansion, evaluation, and backpropagation processes. During expansion, it employs a critiquing and rewriting
mechanism that iteratively refines each solution at the node, improving the quality and accuracy of expanded
results. The evaluation phase integrates both global win-loss matrix calculations and local adjacency
comparisons to determine the combined quality Q; of a solution node. For backpropagation, LLaMA-Berry
introduces an updated formula to refine Q; by aggregating the values from subsequent child nodes in the
tree. This ensures that the backpropagated Q; reflects both the cumulative utility of explored paths and the
quality of individual solutions.

MCTSr

MCTSr [259] is also a strong goal-driven approach that emphasizes generating and refining complete
reasoning solutions. It structures the reasoning tree Tg where each node represents a terminal solution
sterminal and edges denote iterative refinement attempts. The key innovation in MCTSr lies in its evaluation
framework, where rewards for nodes are computed using a combination of minimum and average reward

20


values from multiple resampling attempts. This approach ensures robustness and fairness in evaluating
solutions, significantly improving performance in complex reasoning tasks.

Evaluator-Modeling: MCTSr uses the same LLM (LLaMA) for reasoning and evaluation, where the
evaluation model assigns rewards R(s*®*™"@+) to each terminal solutions through a resampling process.
The evaluator repeat its rewarding process n times for each s*®*™!"@", assigning scores between -100 and
100, which are then aggregated to calculate the final node value. This integration avoids the need for a
separately trained reward model, instead leveraging the inherent capabilities of the LLM.

Evaluation Function Design: MCTSr introduces an evaluation function for terminal solutions that
combines the minimum reward from resampling and the average reward over all resampled attempts:

1 : _
Q; _— . min R(ef™") t. = S- R(gpestinal)
j=l

Here, R(s*°""'"") represents the reward assigned to sF°*"""#" for a single evaluation, and n is the number
of sampling of rewards attempts. This formula balances the robustness of worst-case performance (min R)
with the overall quality (avgR) of the solution.

MCTS Design: MCTSr follows the standard MCTS structure with refinements across the selection,
evaluation, and backpropagation phases. Each child node represents a refined version of its parent solution,
generated by iteratively rewriting st°*™'"*" using the same LLM during expansion phase. MCTSr employs an
updated backpropagation formula that aggregates the values from child nodes, refining the parent node’s
value Q;:

1
‘(a == si) + max Sj
Qs) = 5 (2 i) rots i)
This formula ensures that parent nodes reflect the best potential of their children while retaining their
intrinsic value. Dynamic pruning is applied to remove unpromising nodes based on evaluation scores and
exploration criteria, improving computation

TS-LLM

TS-LLM [52] is both goal-driven and Step-driven, leveraging an AlphaZero-inspired MCTS framework
to optimize reasoning solutions and better steps to re-train policy LLM. A key innovation in TS-LLM is
its dual-model evaluation strategy, where the value model evaluates intermediate reasoning paths based
on the entire trajectory p; = (s1,52,--- ,8;), while the Outcome Reward Model (ORM) scores terminal
solutions. This design ensures that the reasoning process balances exploration of promising paths with
rigorous evaluation of terminal solutions.

Evaluator-Modeling: TS-LLM employs two separate models for evaluation: a) A value model, which
predicts V(p;), the cumulative reward potential of an intermediate trajectory p; = (s1,52,--- ,s;); b) An
Outcome Reward Model (ORM), which assigns a reward Rorm (sterminal ) to terminal nodes based on solution
quality.

Both models are trained using supervised learning on collected data from MCTS rollouts. The value
model focuses on evaluating reasoning trajectories, while the ORM specializes in scoring terminal solutions.

Evaluation Function Design: The evaluation function for a node s; is determined by whether it is an

21


intermediate or terminal node:

Q; _ Vealue-model (Pi = (s1, S2,°°° +84) )y if Sj # gpemminal
to Rorm(ste™inal) ifs; = gterminal
; , i .

i i

This case-based design ensures that intermediate nodes are evaluated based on their cumulative trajectory,
while terminal nodes are directly evaluated for correctness and quality.

MCTS Design: TS-LLM follows closely standard MCTS design, with a additional pruning mechanism. It
dynamically removes unpromising nodes based on updated Q; values and visit counts, ensuring efficient
resource usage.

ALPHALLM

ALPHALLM [183] is a strong goal-driven framework that integrates an AlphaZero-inspired MCTS with
multi-critic evaluation to optimize reasoning paths and terminal solutions. Each node in its tree represents a
reasoning step paired with an action, (s;,a;), and edges denote transitions between steps. Its core innovation
lies in the weighted integration of a value model, process reward model (PRM), and outcome reward model
(ORM) to evaluate both intermediate and terminal nodes. ALPHALLM demonstrates superior reasoning
capabilities across tasks such as GSM8K and Game24, leveraging its multi-critic evaluation to achieve
consistent improvements.

Evaluator-Modeling: ALPHALLM employs three distinct evaluation modeling: The value model predicts
viuture (y;), capturing the potential reward of intermediate trajectories p; = (s1,82,...,8;). This model
is trained using temporal difference (TD-A) learning and Monte Carlo rollouts to evaluate intermediate
states. The Process Reward Model (PRM) provides immediate feedback Rprm(s;) for each reasoning step
s;, focusing on local step quality. PRM is fine-tuned on prefixes of reasoning traces using step-level rewards.
The Outcome Reward Model (ORM) assigns r =Rorm(stermina!) to terminal states based on the correctness

1
and quality of solutions. ORM is trained using solution-specific labels derived from task outcomes.

This multi-critic design allows ALPHALLM to evaluate reasoning steps both locally and globally, ensuring
robust guidance during the search.

Evaluation Function Design: The node value Q; in ALPHALLM combines contributions from all three
critics as :

Q; = By i yiniue (py, ) + Perm : Rpro (si) + Bor : E,gterminal 714 94(5;) [ORM(ste™inal))

Where expected values for ORM is calculated by monte carlo sampling to reach terminal (solution) state
from current state s; using policy LLM.

This formula ensures that the evaluation balances trajectory-level exploration with terminal state quality,
facilitating robust reasoning and exploration.

MCTS Design: ALPHALLM sticks with standard MCTS process except for evaluation phase as we have
discussed above.

PG-TD

PG-TD [262] adopts a goal-driven approach, leveraging a novel integration of Monte Carlo Tree Search
(MCTS) and large language models to improve code generation quality. Its primary innovation lies in using

22


test case execution as the evaluation metric during the generation process, rather than relying on deep
learning models. By combining the Transformer’s beam search probabilities with P-UCB selection for planning,
PG-TD achieves significant improvements in code correctness and efficiency compared to standard decoding
methods. This framework has shown strong performance across multiple benchmarks, including APPS and
CodeContests, particularly in tasks requiring executable and syntactically valid code.

Evaluator-Modeling: PG-TD does not use a separate deep learning model for evaluation. Instead, it
evaluates nodes during MCTS rollouts by executing test cases on the final generated programs. The outcome
(pass rate) of these test cases directly determines the reward. This approach simplifies evaluation while
aligning it closely with the end goal of generating functional code. Since the evaluation relies purely on test
results, no additional value or reward model training is required.

Evaluation-Function Design: PG-TD evaluates each node in the reasoning tree by running test cases on
final complete program represented by the node. The reward for s‘*"™"@! is average pass rate on all test cases.

MCTS Design: PG-TD introduces several innovations to standard MCTS design. The selection process
uses a P-UCB algorithm, which incorporates the probabilities provided by the Transformer’s beam search to
balance exploration and exploitation effectively. During expansion, the tree grows by selecting the top-k
most probable tokens suggested by the Transformer, reducing the likelihood of syntax errors. Additionally,
caching mechanisms (tree structure caching and sequence caching) significantly improve efficiency by reusing
previously computed paths and evaluations.

rStar

rStar [157] introduces a novel goal-driven framework to enhance the reasoning capabilities of small lan-
guage models (SLMs) by employing a self-play mutual reasoning paradigm. The core idea is to combine
a generation-discrimination process, where a target SLM generates reasoning trajectories via MCTS, and
another SLM verifies the quality of these trajectories through mutual agreement. This approach is particularly
effective in overcoming the limitations of SLMs, such as poor exploration and unreliable self-rewarding.
Experiments demonstrate significant performance gains across reasoning tasks, with accuracy improvements
on benchmarks like GSM8K, MATH, and StrategyQA, surpassing many fine-tuned models.

Evaluator-Modeling: rStar employs two SLMs for a collaborative evaluation process. The generator SLM
performs MCTS-based trajectory generation, while the discriminator SLM verifies the quality of trajectories.
The discriminator applies a mutual reasoning consistency mechanism: it is given partial reasoning traces
and asked to complete the remaining steps, validating trajectories based on whether the generator and
discriminator agree on the solutions. This unsupervised approach eliminates the need for fine-tuned value
models or external supervision.

Evaluation-Function Design: As SLM does not perform well in partial solution evaluation, rStar applys
a AlphaGo-like evaluation framework, where intemediate solutions p; with s; gets reward of 0 for simplicity,
and reward only assigned if s‘™™"@! is reached, based on the mutual agreement with discriminator SLM.

MCTS Design: rStar enhances MCTS with a diverse action space inspired by human reasoning processes.
Actions include proposing single steps, generating sub-questions, rephrasing problems, and re-answering sub-
questions, enabling broader and deeper exploration of solution trajectories. The P-UCB algorithm balances
exploration and exploitation, while mutual consistency during node selection ensures robust trajectory
validation.

RethinkMCTS

23


RethinkMCTS [111] adopts a goal-driven approach, enhancing reasoning-to-code performance by
leveraging fine-grained feedback and refining erroneous thoughts during the search process. Its novelty lies in
combining Monte Carlo Tree Search (MCTS) with a dual evaluation mechanism and introducing a "rethink"
operation, which corrects reasoning errors based on execution feedback. This framework significantly
improves the quality of search paths and achieves state-of-the-art performance in code generation tasks, with
notable gains on benchmarks like APPS and HumanEval.

Evaluator-Modeling: RethinkMCTS employs a dual evaluation framework that uses: 1) Public Test
Cases (denoted as TEST(p;)): The pass rate of public test cases is calculated to assess the correctness of the
generated code. 2) LLM Self-Evaluation (Viz): When all public test cases are passed, the LLM provides a
self-assessment score to further evaluate the likelihood of correctness for private test cases.

Both components are integrated into a unified evaluation system, which ensures more robust assessments
for selecting high-quality nodes during tree exploration.

Evaluation-Function Design: The evaluation combines scalar and self-assessment rewards to compute
the node value 7;:
n= (ee if 0 < Utes: < 1,
X + Vtest + p *Ollm, if Otest = 1,
where « and f are weighting parameters (€.g., « = 0.8, B = 0.2).

The reward (r;) is assigned to each node. (for p; that’s incomplete or incorrect, TEST(p;) will always be
O so there’s no need to differentiate the terminal node and intermediate node)

MCTS Design: RethinkMCTS incorporates several innovations into the MCTS framework. During the
selection phase, the P-UCB algorithm is used to balance exploration and exploitation, where verbal feedback
stored at nodes influences subsequent thought refinements. In the expansion phase, nodes that fail public test
cases incorporate block-level verbal feedback into the prompts, enabling the LLM to propose new thoughts
and assign reasonableness scores to each. The rethink operation is employed for leaf nodes that fail public
test cases, refining the current thought based on verbal feedback to correct erroneous paths and improve
overall search quality. Finally, during backpropagation, node values Q; are updated using the maximum
reward from child nodes, ensuring that the best paths are prioritized for future exploration. Verbal feedback
is stored separately and utilized in the next expansion phase but is not directly incorporated into the scalar
reward.

HiAR-ICL

HiAR-ICL [212] adopts a versatile and hybrid goal-driven and step-driven approach to enhance in-
context learning (ICL) by refining reasoning trajectories and leveraging three evaluation strategies: Process
Reward Model (PRM), Outcome Reward Model (ORM), and Self-Consistency. This framework introduces
hierarchical context construction for iterative refinements and integrates multiple evaluation paradigms
to balance intermediate and terminal solution rewards. HiAR-ICL demonstrates significant improvements
across reasoning and coding benchmarks, showcasing its adaptability to diverse tasks.

Evaluator-Modeling: HiAR-ICL employs three evaluation mechanisms tailored for different stages of
the reasoning process. The Process Reward Model (PRM) evaluates intermediate reasoning steps using a
pre-trained language model fine-tuned to assign rewards based on the quality of each step in the trajectory.
The Outcome Reward Model (ORM) evaluates terminal solutions using a reward model fine-tuned specifically
for outcome-based reasoning tasks. Finally, the Self-Consistency mechanism uses majority voting across

24


multiple reasoning trajectories sampled from the language model to assess terminal solutions. PRM focuses
on partial states as inputs and produces scalar intermediate rewards, while ORM and Self-Consistency directly
assess terminal states and produce outcome-based rewards.

Evaluation-Function Design: Each evaluation mechanism employs unique reward aggregation strategies.
PRM utilizes a Min-based aggregation method where the Process Reward Model evaluates all intermediate
steps leading to a state s; and calculates the minimum reward across all steps. This emphasizes the weakest
link in the trajectory. The reward is defined as:

i
Rprm (si) = ay TPRM, jv

where rpry,; represents the reward for the j-th intermediate step. In contrast, ORM and Self-Consistency
directly evaluate terminal solutions using either the Outcome Reward Model or majority voting across
sampled traces, respectively, with rewards Rogm(sts'"™!™#!) and Rvoting (ste™™!).

MCTS Design: HiAR-ICL sticks to standard MCTS designs and made changes during backprobgation
stages by using a min based value backpropgating approach and a product based reward backpropgating
approach.

Agent-R

The Agent-R [250] framework is an iterative self-training method designed to improve an agent’s ability
to recover from errors in interactive environments. Unlike approaches such as RAP that employ an LLM as a
learned world model to predict dense, per-step rewards and next states, Agent-R performs post-hoc analysis
over complete trajectories to generate corrective training data, turning failure cases into targeted revision
examples that refine the underlying policy.

Evaluator-Modeling: Agent-R dispenses with a separately trained reward/evaluator and instead relies
on two signals. First, a sparse, terminal environment reward r(t) € [0,1] supplies the ground-truth success
label for a trajectory t. Second, the agent’s own actor model is repurposed as a critic through “model-guided
critique construction,” where the policy is prompted to examine a failed trajectory step-by-step, labeling
actions as good, bad, or uncertain and identifying the first erroneous step t’. This self-critique localizes the
source of failure without external experts or a dedicated reward model, in contrast to RAP’s dense reward
prediction; Agent-R leverages the actor’s current competence to diagnose where the trajectory went wrong
rather than to score every step.

Evaluation-Function Design: Full trajectories collected via MCTS are categorized as “good” (T%) or “bad”
(t”) by thresholding their terminal reward, with parameters « and f satisfying r(t’) < B < r(t8) <1anda
progressively tightened high-quality bar « (so that a < r(t’) = r(t") for accepted/revision-worthy traces).
The actor-derived transition point t’ on a bad trajectory pinpoints the earliest faulty action; Agent-R then
splices the correct prefix of t? with the aligned suffix of a good trajectory to form a revision trajectory tT’, e.g.,
= es 1) 0 (.,,)- In this way, the evaluation function yields both a coarse trajectory-level judgment and
a precise edit location that together produce training pairs emphasizing how to repair failures into successes.

MCTS Design: Agent-R uses Monte Carlo Tree Search not as an inference-time planner but as a data
collection engine that systematically explores the action space to yield diverse good/bad trajectories for
offline training. The search follows the standard stages (selection, expansion, simulation, backpropagation),

25


with selection guided by the UCT criterion

log N,(s)

UCT(s) = Q(s) + cuct Nis)”

where Q(s) is the average return for state s, N(s) its visit count, and N,(s) the parent’s visits; rollouts
proceed under a default policy until a terminal state, whose environment reward r(T) is then backpropagated
to update Q(s) by averaging and to increment N(s) along the traversed path. This usage of MCTS prioritizes
breadth and difficulty of experience, furnishing the raw material from which Agent-R constructs revision
data that teaches the policy to self-correct.

Retro-Search

Retro-Search [131] is an MCTS-inspired iterative path revision algorithm rather than a traditional tree
search method.

Evaluator-Modeling: The evaluation does not rely on a separately trained reward model. Instead, it uses
a “revision model” M, which can either be the original reasoning model (in a self-improvement setting) or a
weaker student model (in a weak-to-strong setting). This model’s role is generative; it produces alternative
reasoning trajectories (rollouts) from specific points in an existing path for subsequent evaluation.

Evaluation-Function Design: The quality of a reasoning path is determined by a deterministic value
function applied to each step s; of the trajectory. The function is defined as V(s;) := yN~'R(a(s;),a*), where
a(s;) is the final answer produced from the trajectory starting at step s;, R is a binary function verifying if
this answer matches the ground truth a*, N is the total number of steps in the path, and ¥ is a decay factor
that penalizes longer paths. A new path replaces an old one only if its value is strictly greater, effectively
prioritizing shorter, correct solutions.

MCTS Design: The algorithm deviates from standard MCTS by not building an explicit tree or using
selection heuristics like UCB. It performs a sequential, greedy revision of a given reasoning trace. The
process identifies points where the original model switched its line of thought (e.g., using keywords like
“Alternatively”). At these points, it generates new rollouts by constraining the revision model M to continue
the current thought rather than switching. The resulting trajectory is evaluated, and if it proves more efficient
(i.e., has a higher value), it replaces the original path from that point onward. This process repeats for the
next thought-switch in the (potentially updated) trajectory. There is no backpropagation of values; decisions
are final and greedy.

MASTER

Evaluator-Modeling. MASTER [55] does not train a separate evaluation model. Instead, it repurposes
the base Large Language Model (LLM) to perform self-evaluation through a structured, multi-step prompting
process. For each generated agent (node), the LLM first executes a Validation step, where it is prompted to
verify the key facts within the agent’s proposed solution. Following this, it performs an Assessment step,
where it is prompted to generate both a numerical score (rg) indicating progress and a confidence level (cg)
for that score. This approach leverages the in-context reasoning capabilities of the LLM itself to serve as the
evaluator, avoiding the need for model training and external datasets. A final Evaluation step is applied only
to terminal agents to determine if their solution is correct, which can trigger backpropagation.

26


Evaluation-Function Design. The evaluation function in MASTER is not a simple reward function but
a composite procedure that yields two key values for each agent: an initial reward (ro) and a confidence
score (co). These are extracted from the LLM’s textual output during the Assessment phase. The design is
intended to make the reward more reliable by first having the LLM explicitly validate the reasoning steps
before assigning a score. The confidence score (cg) is a crucial component, as it is used to modulate the
influence of both the initial reward and the exploration term in the system’s modified UCT formula. For
terminal agents that fail the final evaluation, a reward is generated and backpropagated to penalize the
preceding reasoning path.

MCTS Design. MASTER introduces a novel adaptation of MCTS tailored for LLMs. The core modification
is the complete elimination of the simulation step, which is traditionally used to estimate long-term
rewards. Instead, rewards are derived directly from the LLM’s self-assessment at each expansion step. The
framework retains the other three MCTS procedures:

¢ Selection: An agent (node) is chosen for expansion based on a modified UCT formula that incorporates
the LLM’s confidence (co) to dynamically weigh the initial reward and adjust the exploration term.

¢ Expansion: The selected agent generates a set number of child agents to explore different reasoning
paths.

¢ Backpropagation: This step is retained but is only triggered when a terminal agent’s final solution fails
the evaluation. The reward from the failed agent is then used to update the Q-values of its ancestors,
allowing the system to correct for initially misallocated rewards.

This design shifts the computational resources from numerous, costly simulations to a series of refined
self-evaluation steps within each node.

AB-MCTS

Evaluator-Modeling: AB-MCTS [86] does not train or model an evaluator. Instead, it presupposes
the existence of an external scoring function, r = R(to,+), which provides direct feedback on a complete,
LLM-generated solution candidate t,,;. This function is treated as a black box that returns a score, often
normalized to [0,1], based on task-specific criteria, such as the fraction of passed test cases in a coding
challenge. The core method is designed to leverage this external feedback signal for search, rather than
modeling the evaluation process itself.

Evaluation-Function Design: The method evaluates actions at a node (either exploring deeper into
an existing child’s subtree or widening by generating a new child) by modeling the posterior predictive
distribution of scores for future nodes. This is implemented in two ways: (1) AB-MCTS-M uses a Bayesian
mixed-effects model where each child’s subtree is a "group", sharing statistical strength to inform the score
distribution of generating a new, unseen child. (2) AB-MCTS-A simplifies this by aggregating all existing
children under a ‘CONT‘ node and representing new generation with a ‘GEN‘ node, modeling the score
distribution for each with independent Bayesian models using conjugate priors .g., a Beta distribution for
scores in |0,1]) for efficient updates. The choice of action is then made via Thompson sampling from these
distributions.

MCTS Design: The central innovation is a novel framework that dynamically decides whether to "go
wider" (exploration) or "go deeper" (exploitation), enabling adaptive and theoretically unbounded branching.
Unlike standard MCTS with a fixed branching factor, AB-MCTS introduces a special ‘GEN‘ node at each
level of the tree, which represents the action of generating a new child candidate from the current node.

27


The selection policy is not based on UCT but on Thompson Sampling, which naturally balances the choice
between selecting an existing child node and selecting the ‘GEN‘ node to expand the tree’s width based on
the Bayesian posterior distributions of expected scores.

SELT Introduction

The SELT (Self-Evaluation LLM Tree Search) [215] framework introduces a novel approach to enhance the
reasoning capabilities of Large Language Models (LLMs) by integrating a modified Monte Carlo Tree Search
(MCTS). Its primary innovation is the elimination of external, pre-trained reward models by leveraging the
intrinsic self-evaluation abilities of the LLM itself. By decomposing complex problems into atomic subtasks
and employing semantic clustering to guide the evaluation, SELT aims to create a more robust, generalizable,
and efficient reasoning process without the need for task-specific fine-tuning.

Evaluator-Modeling: For its evaluator, SELT repurposes the foundational LLM as an intrinsic, unsu-
pervised Scorer, a core design choice that circumvents the dependency on external reward models. This
self-evaluation is not performed in a vacuum; its effectiveness is enhanced through a dynamic, reference-based
system. At each node in the search tree, SELT performs unsupervised semantic clustering on all previously
generated answers to identify distinct, high-quality reasoning paths. From each cluster, a representative
answer is selected, and these representatives serve as a contextual benchmark for the LLM Scorer to assess
the quality of newly simulated answers.

Evaluation-Function Design: The evaluation function in SELT produces a reward score, denoted as A,
which is generated directly by the LLM Scorer during the simulation phase. Unlike methods that rely on
a fixed reward function, SELT’s evaluation is dynamic and context-aware. The score A for a new answer
is determined by the LLM’s assessment of that answer against the set of representative answers curated
through the semantic clustering process. This approach allows the evaluation to be grounded in the diverse
and high-quality solutions discovered during the search itself, rather than an abstract or pre-trained notion
of correctness. The final score A is then used in the backpropagation step to update the value of all parent
nodes in the traversed path.

MCTS Design: SELT introduces several significant modifications to the traditional MCTS algorithm.
The Selection phase utilizes a custom Upper Confidence Bound for Trees (UCT) formula. The exploitation
term, Sri Exploit, is redesigned using Bayesian Averaging to better handle the uncertainty inherent in LLM
self-evaluation scores. The exploration term, S;7 1 Explore, is also adjusted to encourage deeper, more focused
searches within the reasoning tree. During Expansion, the framework builds out a binary search tree. The
Simulation step involves the LLM acting as a ‘Reasoner‘ to complete a reasoning path, followed by the
clustering and self-evaluation process to generate the reward A. Finally, the Backpropagation phase follows
a standard procedure, where the visit count N(v) and the total reward Q(v) of each node along the path are
updated with the newly calculated score A.

TRANS-ZERO

TRANS-ZERO [287] operates as a goal-driven system. The entire search process is optimized to identify
the single highest-quality translation for a given source input. Rewards are calculated for complete translation
candidates after a comprehensive, multi-step simulation process, rather than being assigned to intermediate
steps. The final output of the search is the node (translation candidate) with the highest cumulative utility,
reinforcing the framework’s focus on achieving a final, high-quality output.

28


Evaluator-Modeling TRANS-ZERO does not train a dedicated evaluation model. Instead, it leverages the
inherent multilingual capabilities [264] of the base Large Language Model (LLM) for generating translation
variations and employs a pre-trained, off-the-shelf text generation metric, BLEURT, to function as the evaluator.
This approach bypasses the need for training a separate critic or reward model by defining evaluation as a
direct measurement of semantic consistency derived from round-trip translations, making the framework
self-contained and reliant only on monolingual data.

Evaluation-Function Design The evaluation function computes a reward, r(y), based on the principle
of multilingual semantic consistency. For any given translation candidate node, y, the system performs a
simulation by rolling out a temporary sub-tree. This involves translating y through a series of randomly
sampled pivot languages and then back to the original source language, generating a set of reconstructions
{xw}. The final reward is the maximum semantic similarity score, calculated via BLEURT, between these
reconstructions and the original source text, effectively measuring how well the meaning is preserved across
multiple translation steps.

MCTS Design The framework introduces a novel Genetic Monte-Carlo Tree Search (G-MCTS), where each
node in the tree represents a complete translation candidate. The primary innovation is in the tree expansion
phase, which uses two genetic operators to foster diverse exploration. The Merge operator combines the
current best-utility node and the best-UCB node as few-shot examples to guide an in-context translation of
the original source text. The Mutate operator promotes creative exploration by translating a semantically
similar variant of the source text-specifically, a reconstruction generated during a previous simulation-instead
of the original input.

CMCTS

Complementing the constrained action space, CMCTS [123] integrates a set of human-like partial order
rules during the simulation phase to ensure logical coherence in the reasoning chain. These rules impose
constraints on the sequence [269] of actions, such as mandating an "understand" action at the beginning
and a "summary" action at the end. Further rules govern action diversity, the necessity of reflection, and
the strategic use of coding actions based on reasoning depth. These rules can be used independently or in
combination with the PRM to guide the search, preventing illogical or redundant state transitions. The overall
MCTS process follows the standard selection, expansion, simulation, and back-propagation phases, but with
these novel constraints and guidance mechanisms integrated to produce higher-quality, long-chain-of-thought
reasoning.

Evaluator-Modeling The CMCTS framework utilizes a pre-trained Process Reward Model (PRM) as its
primary evaluator, forgoing the need for training a new model. This PRM, specifically the Qwen2.5-Math-PRM,
is designed to assess the quality of intermediate reasoning steps. It functions by taking a given state s; and a
potential subsequent action a;,,, which are concatenated with a specialized prompt template. The model
then processes this combined input to produce logits for "positive" and "negative" outcomes. This mechanism
allows the PRM to provide nuanced, context-aware evaluations of reasoning quality without relying on the
base Large Language Model (LLM), which is often an unreliable reward signal. The framework uses this
external, specialized model to guide the search process toward more rational and effective reasoning paths.

Evaluation-Function Design The evaluation function in CMCTS is bifurcated into two components:
an action-value function Q(s;,4;,,) and a state-value function V(s;), both derived from the PRM. The
action-value Q(s;, 4141) is calculated by applying a softmax function to the PRM’s output logits, yielding the
probability of a "positive" assessment for taking action a;,, in state s;. Similarly, the state-value V(s;) is

29


computed in an action-agnostic manner, representing the intrinsic quality of a given reasoning state s;. During
back-propagation, the reward for a specific transition is defined as the sum r; = Q(s+_1,4:) + V(s;), which
combines the value of the chosen action and the resulting state. This dual-evaluation approach provides a
comprehensive signal for updating the cumulative rewards of nodes in the search tree.

MCTS Design The MCTS design in CMCTS introduces two primary innovations to the standard algorithm:
a constrained action space and partial order rules. Unlike traditional methods where the LLM generates
subsequent actions, CMCTS samples actions from four predefined, disjoint sets: A™dertand, Areflect {code
and Asmmary_ This action space constraining, applied during the expansion phase, forces the model to explore
diverse and semantically rich reasoning states that are otherwise difficult to sample, such as self-correction
and code-based verification. This directly addresses the issue of state-space homogenization common in
other MCTS applications with LLMs.

Table 5: Comparison of MCTS-based Methods.

Model Name

ReST-MCTS*

LLaMA-Berry

MCTSr

TS-LLM

ALPHALLM

PG-TD

rStar

RethinkMCTS

Policy Model
Self-Train

Yes

No

No

Yes

Yes

No

No

No

Evaluation-Model
Training

Yes

No

No

No

No

MCTS-eval Algorithm

Per-step Process
Reward Model (PRM).

Per-step evaluation
based on learned policy
and dynamics model.

Pairwise preference
comparisons, combining
global (win-loss matrix)
and local (adjacency)
evaluations.

Evaluation combines
minimum and average
rewards sampled
reward.

Case-based evaluation
depending on node
type.

Evaluation combines
weighted contributions
from value model,
process reward model
(PRM), and outcome
reward model (ORM).

Test Case Running.

Evaluation uses mutual
reasoning consistency,
where another SLM acts
as a discriminator to
validate reasoning
trajectories.

Dual evaluation
combining public test
cases (scalar reward)
and LLM self-evaluation
(for fully passing
nodes).

MCTS-eval Model Base

Separately trained LLM
(Mistral); input p;,
returns v;; using
collected reasoning
traces.

Repurposes the same
LLM (LLaMA) as a
world model; input
state-action (s;,d+),
returns r; and s;41.

Fine-tunes a small LLM
(Gemma2-2B-Instruct)
as PPRM; evaluates
pairwise preferences
between reasoning
traces. input state s;
and returns score r;

Same LLM €.g., GPT-4)
used for reward. Input
sj, Output rj.

Separately trained value
model and Outcome

Reward Model (ORM) .
Input state s;, output 7;.

Shared LLM backbone
(LLaMA-2-7B) for
(multi-critic
framework); V(p;),
PRM, ORM for
state-action and
terminal evaluation.

Python/C++

Reuses the same or
similar SLM (e.g.,
Phi3-Mini or
LLaMA2-7B); input
trajectory, validates
partial and full
completions.

Reuses the same LLM
(e.g., GPT-3.5) for
self-evaluation; input
thought trace p;,
returns combined score.

Additional MCTS
Selection Algorithm

Self-critic mechanism
during the selection
process.

State exploration
guided by predictive
heuristics from the
dynamics model,
focusing on promising
states.

Critiquing and rewriting
during expansion to
allow more accurate
self-refined expanded
results.

Dynamic pruning of
unpromising nodes
using updated reward
backpropagation
formula.

AlphaZero-inspired
selection process with
combined value model
and terminal
evaluation.

Combines option-level
refinement, and
multi-critic evaluation
for selecting promising
nodes.

P-UGB selection
mechanism with
caching to optimize
efficiency.

P-UGB selection
augmented with diverse
reasoning actions
inspired by human
reasoning, including
sub-question generation
and rephrasing.

P-UGB selection
enhanced with verbal
feedback during
expansion and rethink
operations.

Goal-Driven or
Reasoning Step-Driven

Step-driven

Goal-driven

Goal-driven

Goal-driven and
Step-driven

Goal-driven

Goal-driven with
multi-level reward
integration.

Step-driven

Goal-driven

Goal-driven

30


Table 5 - continued from previous page

Model Name Policy Model Evaluation-Model MCTS-eval Algorithm MCTS-eval Model Base Additional MCTS Goal-Driven or
Self-Train Training Selection Algorithm Reasoning Step-Driven
HiAR-ICL No Yes (PRM, ORM), No Process Reward Model Separately trained P-UCB with hierarchical Goal-driven
(Self-Consistency) evaluates partial LLM-based PRM context construction

reasoning states (PRM); (math-shepherd- (PRM, ORM); no

Outcome Reward Model mistral-7b-prm) and additional mechanism

evaluates final ORM (Llama3.1-8B for Self-Consistency.

reasoning results -ORM-Mistral-Data;

(ORM); Majority voting reuses the same LLM for

over sampled traces Self-Consistency.

(Self-Consistency).

Agent-R Yes No Final trajectory reward External environment’s Standard UCT Goal-driven (MCTS
r(t) from environment reward function and the exploration) &
after MCTS rollout. policy model itself Step-driven

(Llama-3.1-8B) as a (self-correction)
self-critic for revision.

Retro-Search Yes No Length-discounted Uses the policy LLM (or Suppresses Goal-driven
binary outcome reward. a weaker one) as a thought-transition

“revision model” to keywords during rollout
generate rollouts. generation to encourage
deeper reasoning paths.

MASTER No No Per-step multi-stage Repurposes the same Modified UCT with Step-driven
self-evaluation using base LLM (GPT-4); dynamic exploration
Validation and input agent state s;, weight based on LLM’s
Assessment prompts. returns score ry; and confidence (1/co).

confidence cp j.

AB-MCTS No No Bayesian posterior On-the-fly statistical Thompson Sampling to Goal-driven
predictive modeling of model €e.g., decide between "go
node scores. mixed-effects or wider" (new branch)

conjugate prior models) and "go deeper" (refine
fitted to observed scores existing).
from the tree search.

SELT No No Intrinsic self-evaluation. Same foundational LLM Modified UCT with Step-driven
LLM scores a new (Llama-3.1) used for Bayesian Averaging for
answer against reasoning. exploitation and
representative answers adjusted exploration
from semantic clusters. term.

TRANS-ZERO Yes No Multilingual Semantic Pre-trained metric Genetic Expansion Goal-driven
Consistency via rollouts. (BLEURT); input (Merge/Mutate

sentence pair, returns operators).
similarity score.
CMCTS No No Per-step Process Uses a pre-trained LLM Constrained Action Step-driven

Reward Model (PRM).

5.3. Advanced Topics and Hybrid Approaches for MCTS

(Qwen2.5-Math-PRM);
input (s;,4;41) returns
Q-value; input s+
returns V-value.

Space (during
expansion) and Partial
Order Rules (during
simulation).

As the field of LLM-driven search matures, research is advancing beyond the core MCTS paradigm to explore
more sophisticated techniques. These efforts focus on refining the search process through collaborative
multi-agent systems, designing more effective reward signals, and enhancing computational efficiency and

adaptability.

5.3.1. Multi-Agent and Collaborative Search

Rather than relying on a single large language model (LLM) to conduct search, recent approaches employ
multiple LLM agents that collaborate, debate, or assume specialized roles to address complex problems
more effectively. This paradigm shift—from a monolithic searcher to a coordinated team—enables greater
robustness, diversity, and adaptability in problem-solving. Some frameworks leverage MCTS to orchestrate
multi-agent interactions, dynamically adjusting agent numbers and communication strategies according
to task complexity [55, 240, 271]. Others adopt hierarchical structures comprising specialized agents for
high-level planning, role assignment, and low-level execution [77, 108].

31


MCTS for

Self-Improvement
via Data Generation

Taxonomy)

Advanced Topics and
Hybrid Approaches

Discriminator-based Tree Search [29], Interpretable Contrastive MCTS [58],
RoT [85], LiteSearch [189], MindStar [92],
MARCO-O1 [277], Everything of Thoughts [43], CoAT [146]

General Reasoning

& Problem Solving

MCTS Self-Refine [259], Energy Function MCTS [228], OVM [244],

Mathematical Reasoning LLaMA-Berry [261], Automated Process Supervision [134],

Constrained MCTS [123], Markov Chain of Thought [234]

RTL Code Gen MCTS [40], RethinkMCTS [111], Verified Multi-step Synthesis [15],
VerMCTS [14], Code World Models MCTS [39], Planning in NL [191],

O1-Coder [273], SRA-MCTS [226], SWE-Search [3],

SWE-Debate [109], MCTS-Judge [201], APRMCTS [78], MCTS-Refined CoT [201]

Code Generation

& Software Engineering

LLM Agents LATS [280], SeLa [32], BIDA [247], WebPilot [271], Prompt-based MCTS [248],
& Interactive Environments | MASTER [55], SE-Agent [122], WebSynthesis [57], AgentSwift [114], HALO [77]

KNOT-MCTS [211], Search-in-the-Chain [229],

Contrastive RAG [62], RARE [185], CoRaG [205], RITEK [82],

AirRAG [51], KBQA-O1 [133], MCTS-KBQA [225], Hierarchical RAG [45],
Explainable MCTS [103], KCTS [33], RAG-Star [87], FREESON [96]

: , Mulberry [236], Progressive Multimodal Reasoning [44],
Multimodal Reasoning MCTS-Automated Structured Thinking [213], Dyfo [107]

rStar-Math [63], Alphazero-like tree-search [52],

Curriculum Preference Learning [198], Agent Q [156],
Self-Improvement Iterative Preference Learning [222], Imagination, Searching, and Criticizing [183],
Foundational Frameworks Mutual Reasoning [157], AlphaMath Almost Zero [23], CPL [196],

Step-level Value Preference Optimization [24], TreeRPO [235],

Data Influence-Oriented Tree Search [170], MCTS-Refined CoT [201], ASTRO [97],

Knowledge-Intensive
& RAG Tasks

PPL-MCTS [20], Value-Guided MCTS [125], ARGS [95],
Reflective Tree Search [249], PromptAgent [197], STAIR [272],
Dynamic Rewarding [173], Evolutionary Space Search [106], APRMCTS [78]

General Capabilities
& Alignment

Self-Play Approach [66], Named Entity Matching [187],

Synthetic Data Generation [128], Monte Carlo Thought Search [175],

STRATEGIST [119], Fast and Slow Thinking [31], Peptune [181],

Multi-Agent Sampling [240], Step-level [138], Process Reward-guided Tree Search [149],
Think&Cite [110], SAPIENT [46], Automatic Heuristic Design [279], Trans-Zero [287],
Prompt-Based MCTS [49], MedS? [88], MCTSr-Zero [130], K-MSE [283]

Stepwise Domain Knowledge-Driven Reasoning [124] IRIS [59], ChemAgent [214]

Scientific
& Specialized Domains

Multimodal Applications MCTS-guided Sample Selection [199], MMC [127], MM-PRM [47]

Reflective Tree Search [249], Multi-agent sampling [240],
Process reward-guided tree search [149], Webpilot [271], MASTER [55],

Data Influence-Oriented Tree Search [170], Multi-LLM collaborative search [233],
KompeteAI [104], SWE-Debate [109], AniMaker [169], HALO [77]

Multi-Agent and
Collaborative Search

rStar-Math [63], AlphaZero-like tree-search [52],

Iterative preference learning [222], Step-level q-value models [255],
Curriculum Preference Learning [198], Interpretable contrastive MCTS [58],
Energy function guided MCTS [228], AlphaMath Almost Zero [23],

F CPL [196], OVM [244], Value-Guided MCTS [125],
Reward Model Design ee,
and OpGizadon Step-Level reward model [136], Step-level value preference optimization [24],
P Automated process supervision [134], MCTS-boosted mathematical reasoning [138],

Think&Cite [110], STAIR [272], MT-RewardTree [53],
Hierarchical Multi-Step Reward Models [195], ProMed [42],
TreeRPO [235], Unifying RL and Search-Based TTS [89],
Re-ranking Reasoning Context with Tree Search [232]

Tree search for agents [100], Discriminator-dependent tree search [29],
Information Directed Tree Search [22], RoT [85], LiteSearch [189], BoostStep [256],
Everything of Thoughts [43], Dual process of fast and slow thinking [31],
Search Efficiency T-SCEND [263], BFS-Prover [223], MCTS-Judge [201],
and Dynamics Adaptive branching tree search [86], Retro-Search [131],
Bilevel MCTS [4], Test-Time Depth Adaptation [117],
Abstraction dropping methods [164], SIGMA [160], AgentSwift [114],

FREESON [96], Structural Entropy Guided Agent [209]

Figure 5: A comprehensive taxonomy of MCTS.

32


A notable strategy is self-play or mutual reasoning, in which one agent generates solutions while another
critiques or refines them. This adversarial or cooperative dynamic enhances reasoning quality and filters
erroneous paths. In competitive domains such as software issue resolution, debate-based frameworks foster
diverse reasoning trajectories and promote more convergent, high-quality outcomes [109, 48, 3]. For
instance, the rStar framework uses one LLM to produce reasoning trajectories via MCTS and another as a
discriminator to ensure mutual consistency [157]. Similarly, Reflective MCTS (R-MCTS) integrates debate
mechanisms to improve state evaluation reliability during the search process [249].

Collaborative search can also be achieved through ensemble-style designs. The Mixture-of-Search-Agents
(MOSA) framework aggregates independent explorations from multiple LLMs and iteratively refines them,
mitigating the limitations of any single model [233, 19, 224, 275]. This idea extends to process-level
ensembling, where MCTS operates over reasoning traces produced by different LLMs to assemble the most
coherent and accurate chain of thought [149]. In multimodal contexts, Collective MCTS (CoMCTS) enables
models to cooperatively explore reasoning spaces across modalities, thereby enriching datasets with diverse,
interpretable reasoning steps [236].

5.3.2. Reward Model Design and Optimization

The effectiveness of a search algorithm fundamentally depends on the quality of its reward function. Current
research emphasizes designing and training reward models that can precisely guide search toward desirable
outcomes. A prominent shift has been from coarse, outcome-level rewards to fine-grained, step-level feedback.
Process-Supervised Reward Models (PRMs) provide such detailed supervision, enhancing reasoning in
domains like mathematics and code generation [136, 120]. These models often build upon Reinforcement
Learning from Human Feedback (RLHF), which aligns models using preference-based supervision [145, 177].
Since step-level annotation is expensive, recent work employs MCTS to automatically generate large-scale
supervision data efficiently [134].

Beyond direct supervision, several self-improving frameworks draw inspiration from AlphaZero. Here,
an LLM uses MCTS to generate reasoning trajectories, which are then used to iteratively fine-tune itself
or an associated reward model [63, 183, 226]. Other studies combine MCTS-guided data generation
with preference optimization techniques such as Direct Preference Optimization (DPO) to improve policy
alignment [222, 156]. Extensions also explore integrating value models that autonomously provide feedback
signals during search, eliminating the need for explicit process annotations [23]. These iterative loops
enable continuous self-improvement by leveraging both successful and failed trajectories [52, 190, 255, 24].
Furthermore, value networks learned during policy optimization (e.g., PPO) can be repurposed at inference
time to guide MCTS decoding, bridging the training—inference gap [125].

Alternative formulations avoid explicit reward modeling. Outcome Value Models (OVMs), for instance,
learn value estimation solely from final outcomes yet effectively evaluate partial reasoning paths [244].
Other methods employ external tools as implicit reward sources—logical verifiers for program synthesis [14],
knowledge retrievers for factual grounding [33, 211, 87, 31]—or repurpose the LLM itself as a world model
or zero-shot reward estimator [278, 69, 39, 248]. Analytical studies reveal several counterintuitive findings:
the overall accuracy of planning heavily depends on discriminator quality [29], and step-level reward models
exhibit superior sensitivity to mathematical logic compared to natural language coherence [138].

5.3.3. Search Efficiency and Dynamics

The principal challenge in tree-based reasoning lies in its computational expense. Recent work aims to
enhance the efficiency, adaptivity, and scalability of MCTS while preserving exploration depth and reasoning

33


quality.

One line of research focuses on structural and algorithmic improvements. Techniques such as LiteSearch
employ dynamic node selection and adaptive exploration budgets guided by learned value networks to
minimize redundant computation [189]. Although not strictly MCTS, related paradigms like Tree-of-Thoughts
(ToT) have underscored the value of structured reasoning over purely greedy generation [238, 129]. These
ideas have been generalized into Graph-of-Thoughts (GoT), where reasoning paths form graph structures
that support merging, feedback loops, and hierarchical composition [10]. Other algorithmic innovations,
such as bilevel MCTS, achieve amortized O(1) node-selection time, greatly improving efficiency in deep
search trees [4]. Information Directed Tree Search (IDTS) introduces a Bayesian mechanism for quantifying
information gain across feedback types, thereby prioritizing more informative expansions [22].

A complementary research direction seeks to make search dynamics adaptive. Frameworks such as
Reasoning via Planning (RAP) employ an LLM as a world model to simulate and anticipate outcomes,
achieving a more balanced exploration—exploitation trade-off [69]. Adaptive Branching MCTS (AB-MCTS)
adaptively decides whether to expand breadth or depth at each node, generalizing repeated sampling
strategies [86]. Some variants embed reflection mechanisms directly into the search process: RethinkMCTS
incorporates execution feedback to iteratively correct reasoning errors [111], while Reflection on Trees (RoT)
distills prior search experiences from stronger LLMs into reusable heuristics for weaker ones [85].

Finally, MCTS has been extended to broader and higher-level applications, including natural language
planning for diverse code generation [262, 191], web-based decision-making [100], conversational policy
optimization [115, 46, 73], and automated machine learning pipeline design [32]. In some cases, search
operates at a meta-level—optimizing prompts [197] or abstract reasoning schemas [212]. To facilitate
standardized benchmarking and comparison across these diverse applications, unified toolkits such as LLM
Reasoners have been developed [70].

5.4. MCTS for Direct Test-Time Enhancement

This category includes methods that use Monte Carlo Tree Search (MCTS) primarily to improve the quality of
the LLM’s output for a single, given prompt at inference time, without updating the model’s weights. These
approaches treat the generation of a solution as a sequential decision-making problem, where the MCTS
algorithm explores a tree of possible reasoning steps or text segments to find an optimal path. The core idea
is to leverage lookahead planning to overcome the greedy, left-to-right nature of standard autoregressive
decoding, thereby enhancing the model’s performance on tasks that require strategic thinking, exploration,
or backtracking.

5.4.1. General Reasoning & Problem Solving

This research direction seeks to develop domain-agnostic frameworks that augment the general reasoning
capabilities of LLMs. Efforts focus on making MCTS-based inference more efficient, interpretable, and robust.
A key principle is to represent text generation not as a linear sequence but as a structured search over a tree
of possibilities, facilitating exploration, lookahead, and revision—an idea popularized by Tree-of-Thoughts
(ToT) [238, 129] and extended by Graph-of-Thoughts (GoT) [10].

Subsequent research has advanced both the efficiency and conceptual sophistication of this search
paradigm. Some works design lightweight search algorithms and dynamic resource allocation strategies to
reduce the computational burden inherent in tree expansion [189], for instance by using adaptive branching
to decide whether to explore wider or deeper [86]. Others introduce novel MCTS components, such as

34


interpretable reward models based on contrastive decoding, to improve both accuracy and computational
efficiency [58]. A key insight is to repurpose LLMs as world models capable of simulating future states to
guide planning, exemplified by the Reasoning via Planning (RAP) framework [69] and extended to virtual
web environments [57]. Further analyses highlight that the success of MCTS-based reasoning often hinges
on the fidelity of the reward model or discriminator used to evaluate intermediate steps [29, 70], leading to
the development of more robust hierarchical reward models [195] and frameworks that unify reinforcement
learning (RL) and search by using the learned RL reward function as a dynamic process reward model (PRM)
[89].

Inspired by classical AI paradigms, several frameworks have introduced reinforcement-style or self-play
mechanisms. For example, TS-LLM employs an AlphaZero-like structure, where a learned value function
drives decoding [52]. The rStar framework adopts mutual self-play between a reasoner and a discriminator to
refine search quality [157]. Meta-cognitive extensions such as reflection further enhance robustness, allowing
models to learn from past trajectories to avoid repeated errors [85]. This principle has been formalized in
frameworks like Agent-R, which uses MCTS to construct training samples that recover correct trajectories
from erroneous ones [250], and ASTRO, which teaches models to reason like search algorithms by explicitly
reflecting and backtracking [97]. Other self-improvement strategies leverage MCTS to retrospectively revise
and shorten reasoning paths for more efficient distillation [131] or use sibling-guided augmentation to
reintegrate valuable insights from non-optimal search branches [160].

MCTS has become a cornerstone of multi-agent systems, where it orchestrates collaboration and debate.
Frameworks like MASTER [55], HALO [77], and MoSA [233] use MCTS to coordinate agent recruitment,
task decomposition, and communication, leveraging the collective expertise of multiple LLMs to solve
complex problems. This paradigm extends to specialized domains like software engineering, where agents
competitively debate to localize faults [109]. Efficiency in such systems is enhanced by using data influence
scores to guide tree search, prioritizing data synthesis that most effectively improves model training [170].

The versatility of MCTS is evident in its application across a growing array of domains. In Retrieval-
Augmented Generation (RAG), it guides a dynamic interplay between reasoning and information retrieval,
activating the model’s intrinsic reasoning capabilities [51] and ensuring factual accuracy [81], even enabling
retriever-free models that traverse a corpus directly [96]. In knowledge base question answering (KBQA),
MCTS navigates large knowledge graphs to generate logical forms or structured queries [133, 225, 202].
The paradigm has been extended to multimodal reasoning, enhancing performance on tasks combining
vision and language [213, 199, 127, 232] and even for specific applications like multimodal misinformation
detection [37]. Further applications include ensuring safety alignment by guiding introspective reasoning
[272], mitigating hallucinations [49], and tackling open-ended scientific discovery [59, 1] and specialized
tasks like heuristic design [279, 193], automated machine learning [118], and text-based game playing
[171].

5.4.2. Mathematical Reasoning

Mathematics serves as an ideal testbed for MCTS because its problems have unambiguous, verifiable solutions,
facilitating the design of precise reward functions. This property enables fine-grained evaluation of both
intermediate reasoning steps and final outcomes. Research in this area primarily targets improving reasoning
quality and search efficiency. For instance, MCT Self-Refine (MCTSr) incorporates a self-correction mechanism
within the MCTS loop, enabling the LLM to iteratively refine its reasoning during exploration [259]. This
concept of learning from errors is further explored in frameworks like LEMMA, which explicitly constructs
training data connecting incorrect solutions to correct ones to improve the model’s reflective capabilities

35


[147].

Efficiency and scalability remain core challenges. To mitigate computational costs associated with
long reasoning chains, Markov Chain of Thought (MCoT) compresses previous steps into concise state
representations [234]. Others focus on refining the search process itself. For example, Constrained MCTS
(CMCTS) restricts the action space to semantically plausible steps, enhancing efficiency and rationality [123],
while lightweight energy-based path verifiers guide tree exploration without additional fine-tuning [228].
Retrieval augmentation has also been integrated, with hierarchical systems retrieving abstract templates and
analogous solution steps to guide the MCTS process [45].

Several studies reduce supervision overhead by training value models solely on final outcomes [244] or by
using MCTS to autonomously collect high-quality process supervision data [134, 132]. Frameworks such as
rStar-Math [63] and AlphaMath [23] adopt self-evolutionary approaches in which policy and reward models
are iteratively refined through MCTS-guided data synthesis without human intervention. In automated
theorem proving, MCTS is used to guide proof assistants like Lean [231] and to solve complex problems
by generating and pursuing subgoals within a structured search [285]. However, some research challenges
the necessity of complex search, demonstrating that simpler methods like Best-First Search can achieve
competitive performance when properly scaled [223]. The principles of process supervision have also been
extended to multimodal mathematical reasoning, enhancing the logical robustness of vision-language models
[47].

5.4.3. Code Generation & Software Engineering

In software engineering, MCTS is employed to navigate the immense and combinatorial search space of
possible code implementations. A key advantage in this domain is the availability of immediate, objective
feedback from tools such as compilers, unit tests, and formal verifiers, which provide powerful and inter-
pretable reward signals. Numerous studies leverage this feedback to guide the search toward correct, efficient,
and verifiable code. For instance, RethinkMCTS explores the reasoning process underlying code generation
and uses execution feedback to iteratively refine erroneous reasoning paths [111]. Similarly, Planning-Guided
Transformer Decoding (PG-TD) integrates MCTS into token-level decoding, using test cases as reward signals
to improve correctness [262]. Frameworks like Adaptive Branching MCTS have also shown that dynamically
deciding whether to explore new code candidates ("wider") or refine existing ones ("deeper") based on
feedback can outperform standard MCTS [86].

MCTS has also been adopted for structured query generation and formal program synthesis. In Text-
to-SQL, frameworks like SQL-o1 [135] and Alpha-SQL [105] use MCTS for multi-step exploration with
self-reward mechanisms. For formal synthesis, VerMCTS integrates logical verification at every node in the
search tree, ensuring partial correctness and delivering strong guarantees of soundness [15]. The versatility
of MCTS extends beyond program synthesis to hardware design, where it optimizes register-transfer-level
(RTL) code for power, performance, and area trade-offs [40].

At a larger scale, MCTS supports repository-level software engineering. Multi-agent frameworks such
as SWE-Search [3] and SWE-Debate [109] coordinate self-improvement and patch generation through
deliberative search and reasoning. MCTS has been applied to automated program repair (APR-MCTS) [78]
and even to code evaluation, where MCTS-Judge explores reasoning trajectories to assess code correctness
in an LLM-as-a-Judge setting [201]. These approaches are often used not only for inference but also to
generate high-quality fine-tuning data. For instance, SE-Agent uses a self-evolutionary mechanism to optimize
interaction trajectories for solving GitHub issues [122], and other work uses a refined MCTS process to
generate high-quality Chain-of-Thought data for fine-tuning models on issue resolution [201]. Collectively,

36


these methods demonstrate MCTS’s potential to emulate human-like, deliberative reasoning in software
engineering, bridging the gap between symbolic search and data-driven learning.

5.4.4. LLM Agents & Interactive Environments

MCTS naturally complements large language model (LLM) agents operating in interactive environments
by enabling strategic planning, exploration of action sequences, and adaptive decision-making based on
environmental feedback. The Language Agent Tree Search (LATS) framework exemplifies this paradigm,
unifying reasoning, acting, and planning within a reflective MCTS structure to enhance deliberative control
[280]. This approach has proven effective across a wide array of agentic tasks, including web navigation,
complex reasoning, and software engineering.

In web navigation and GUI interaction, where agents must execute multi-step tasks in dynamic settings,
tree search provides a robust mechanism for managing vast action spaces and uncertain state transitions.
Methods such as WebPilot employ dual-level optimization strategies—combining high-level planning with
MCTS-guided subtask execution—to achieve strong performance [271, 100]. This is further extended by
frameworks that learn world models to simulate web environments, allowing MCTS to perform efficient,
reversible planning for large-scale trajectory synthesis [57]. Similarly, agents can be augmented with
hierarchical skill libraries, which MCTS can leverage to prune the action space during online exploration
[220]. The paradigm is also effective in classic interactive environments like text-based games, where
LLM-guided MCTS with dynamic memory enhances planning and learning [171].

MCTS is also a cornerstone for automated problem-solving in specialized domains. In software engi-
neering, it guides agents in resolving complex GitHub issues and performing automated program repair
by globally evaluating explored patches and identifying promising refinement paths [109, 201, 78]. For
tasks requiring formal logic, such as automated theorem proving, MCTS helps navigate large proof spaces
[223, 285]. This principle extends to scientific and heuristic discovery, where agents use MCTS to search
for optimal AutoML pipelines [32, 118], design heuristics for optimization problems [279, 193], or even
generate novel hypotheses for scientific research [1, 59].

A significant line of research focuses on enhancing the agent’s core reasoning and self-improvement
capabilities. Iterative self-training frameworks like Agent-R use MCTS to construct training samples that
teach agents to recover from errors [250], while others like AgentQ combine MCTS with self-critique and
offline fine-tuning [156]. The search process itself can be improved through introspective reflection and
multi-agent debate to refine state evaluation [249, 55, 233]. Furthermore, MCTS is pivotal for generating
high-quality training data to instill sophisticated reasoning abilities. Frameworks like Retro-Search and
ASTRO use MCTS-like algorithms to distill optimal reasoning paths from stronger models or teach search-like
behaviors, which are then used to fine-tune smaller models [131, 97]. This self-evolution paradigm, where
MCTS explores reasoning trajectories that are then used to train policy and reward models, has proven
effective in domains ranging from medicine to general problem-solving [88, 122].

5.4.5. Retrieval-Augmented Generation (RAG) & Knowledge-Intensive Tasks

Ensuring factual accuracy and adherence to constraints such as safety, style, and alignment remains a central
challenge for LLMs. MCTS provides a principled mechanism to steer generation toward desired attributes
during inference by integrating external knowledge sources and reward models. Early work demonstrated
that discriminator-guided MCTS can effectively control textual style and toxicity [20]. This has been extended
by safety-aware frameworks like STAIR, which use a specialized MCTS to find a balance between helpfulness
and safety [272], and by systems like SEM-CTRL, which enforce rich syntactic and semantic constraints

37


during decoding [2].

In knowledge-intensive settings, MCTS helps mitigate hallucination and enhance factual grounding.
Frameworks such as KNOT-MCTS, KCTS, and prompt-based MCTS variants integrate external retrieval or
dynamic strategies into the search process to align content with verified sources [211, 33, 49]. HaluSearch
conceptualizes generation as a deliberative "slow thinking" process, using a self-evaluation reward model
to identify reliable reasoning pathways [31]. This structured exploration is critical for complex tasks like
Knowledge Base Question Answering (KBQA) and Text-to-SQL, where MCTS-driven agents can explore the
knowledge base or schema to construct accurate queries [133, 225, 202, 135, 105].

The integration of MCTS into Retrieval-Augmented Generation (RAG) represents a significant advance-
ment. Instead of static retrieval, MCTS enables dynamic, interleaved retrieval and reasoning. Systems like
RAG-Star, RARE, and AirRAG incorporate retrieval and verification within the search loop, using evidence
consistency or strategic planning as a reward signal [87, 185, 51]. Similarly, Think&Cite treats attributed
generation as a search problem, rewarding factually supported progress [110], while CORAG optimizes
retrieval composition [205]. This paradigm transforms retrieval into an adaptive reasoning process, as
demonstrated by frameworks like SearChain, MCTS-RAG, and R2-LLMs, which decide when and what to
retrieve at each step, incrementally building verifiable solutions [229, 87, 81, 45]. This tight coupling
between search and knowledge retrieval is further advanced by innovations like FREESON, which allows
LLMs to autonomously traverse a corpus via MCTS, learning to retrieve without an explicit retriever [96].
These methods significantly augment the deliberative capacity of LLMs, enabling smaller models to perform
competitively on complex knowledge-intensive tasks [81, 45].

5.4.6. Multimodal Reasoning

The tree search paradigm has recently been extended to multimodal large language models (MLLMs),
enhancing their ability to perform complex reasoning across text, images, and video. By structuring the
reasoning process, MCTS decomposes a multimodal problem into a sequence of interpretable steps, where
each step can involve grounding textual concepts in visual evidence or actively seeking new information.

A key application is active retrieval, where models dynamically acquire supporting evidence. The AR-
MCTS framework exemplifies this by integrating retrieval of both textual and visual information within the
MCTS loop, ensuring that generated explanations are well-supported by facts from a hybrid-modal corpus
[44]. This idea of exploring and re-ranking multimodal contexts is further refined by methods that use tree
search to select the most relevant examples for in-context learning, improving response consistency and
quality [232]. Other approaches, such as DyFo, use MCTS to simulate human-like visual search, enabling
models to dynamically focus on key image regions and reduce hallucinations without additional training
[107].

MCTS is also instrumental in generating high-quality training data for multimodal self-improvement.
For instance, MCTS can be repurposed to measure sample difficulty, allowing for the selection of a highly
effective, condensed dataset for reinforcement fine-tuning, as demonstrated by ThinkLite-VL [199]. Similarly,
frameworks like MMC and MM-PRM use MCTS to automatically construct large-scale critique datasets and
step-level annotations for training powerful multimodal reward models, which in turn guide the model’s
reasoning at inference time [127, 47].

Beyond reasoning, MCTS powers creative and specialized multimodal tasks. In automated storytelling,
frameworks like AniMaker use a multi-agent system where an MCTS-driven strategy intelligently generates
and selects high-quality video clips to form a coherent narrative [169]. MCTS is also used to create

38


comprehensive benchmarks for tasks like video captioning by iteratively generating diverse and detailed
descriptive sentences [246]. Approaches like AStar abstract high-level reasoning patterns, or "thought cards,"
using MCTS and retrieve them at inference time to scaffold solutions for new problems [213]. These methods
substantially enhance the robustness and compositional reasoning capabilities of vision-language models
[232].

5.4.7. Specialized and Novel Applications

Beyond general reasoning and language understanding, MCTS-guided inference has been increasingly adopted
in specialized domains and for novel applications.

Game Playing and Strategic Reasoning: While MCTS is traditionally renowned for its success in board
games such as Go, its integration with LLMs has opened new frontiers. LLMs can function as action pruners
and value function approximators within an MCTS loop for deterministic games like chess, achieving effective
play without additional training [66]. For games with partial observability, the STRATEGIST framework
employs a bi-level search process in which an LLM explores high-level textual strategies, while a low-level
MCTS refines and executes them [119].

Scientific Discovery: MCTS has also been applied to accelerate scientific reasoning in large combinatorial
spaces. Notably, it has been used to discover novel catalysts by balancing multiple chemical property trade-
offs [175] and to guide de novo therapeutic peptide generation by jointly optimizing for multiple biological
objectives [181].

Prompt and Hyperparameter Optimization: At a meta-reasoning level, MCTS can be applied not
to solve downstream tasks directly, but to optimize the instructions or prompts that guide an LLM. The
PromptAgent framework formulates prompt engineering as a strategic planning problem, using MCTS to
explore and refine the vast space of possible instructions based on model feedback [197]. Similar search-based
strategies have also been applied for tuning-free self-alignment, allowing models to autonomously generate
optimal alignment instructions without any gradient updates [173].

Reasoning over Structured Data: For tasks that involve reasoning over knowledge graphs, Relational
MCTS automatically extracts relevant relational paths from textual knowledge graphs, supplying structured
evidence to the LLM and enhancing its ability to answer complex, compositional queries [82].

5.5. MCTS for Self-Improvement via Data Generation

This paradigm leverages MCTS not merely to identify a single optimal solution but to generate high-quality
reasoning trajectories. These trajectories serve as synthetic data for fine-tuning large language models (LLMs)
or reward models, forming a virtuous cycle of iterative self-improvement.

5.5.1. Foundational Self-Improvement Frameworks

Recent works establish the foundations for employing MCTS within self-training loops, drawing inspiration
from reinforcement learning frameworks such as AlphaZero and preference optimization. The central idea is
to construct a self-evolutionary cycle where both a policy model (the LLM) and a value or reward model are
iteratively enhanced. For example, frameworks including rStar-Math, AlphaLLM, TS-LLM, and ReST-MCTS*
utilize MCTS to perform extensive rollouts that yield large volumes of verified, step-by-step reasoning
data. This data is subsequently used to train both the LLM and an auxiliary process preference model
[63, 183, 188, 258]. In an AlphaZero-like manner, the model learns from its own structured explorations

39


and adapts across diverse tasks and model scales. A learned value function guides the search more effectively
than reliance on the pretrained LLM’s intrinsic priors alone [52]. Some approaches explicitly unify these
paradigms, demonstrating that a reward function learned via reinforcement learning can serve as the ideal
process reward model (PRM) for guiding search, eliminating the need for labeled process data [89]. Others
refine the learning process with novel policy optimization techniques, such as using segment-level advantage
estimation to balance credit assignment and estimation accuracy [235].

The data generated from MCTS rollouts is typically converted into preference pairs—comparisons
between superior and inferior reasoning steps—and employed with algorithms such as Direct Preference
Optimization (DPO) to refine the model’s policy [222]. This self-contained process, exemplified by AlphaMath,
autonomously generates process supervision and step-level evaluation signals without requiring human or
teacher-model annotations [23]. To improve data quality and diversity, some methods augment MCTS data
with hierarchical node compression [195] or by reintegrating discarded sibling nodes from the search tree to
provide comparative feedback for refining optimal trajectories [160]. Other frameworks like ASTRO and
Agent-R focus on learning from both successful and failed trajectories to improve generalization and develop
robust recovery mechanisms [97, 250]. This includes weak-to-strong generalization, where trajectories from
a weak model, including failures, are organized into a tree to efficiently elicit a strong model’s optimal policy
[241]. Data generation can be further targeted by using metrics like structural entropy to guide MCTS
toward a model’s knowledge gaps [209] or by using influence scores to select the most impactful data for
model training [170].

The self-improvement principle also underpins distillation, where MCTS is used to generate or refine
reasoning paths from a large teacher model to create higher-quality training data for a smaller student
model. For example, Retro-Search uses an MCTS-like algorithm to retrospectively revise reasoning paths to
discover shorter, more efficient traces [131], while other work constructs tree-based Chain-of-Thought data
from scratch to widen the distillation bottleneck [242]. Beyond data generation, novel search frameworks
enhance the reasoning process itself. For instance, the Chain-of-Associated-Thoughts (CoAT) framework
integrates MCTS with a dynamic associative memory to expand the search space [146], while others use
self-evaluation to guide tree search without external reward models [215] or adaptively decide whether
to broaden or deepen the search tree based on external feedback [86]. At a meta-level, frameworks like
AgentSwift employ hierarchical MCTS to search for optimal agentic system designs, including workflows and
functional components [114].

5.5.2. General Capabilities & Alignment

MCTS also serves as a powerful mechanism for generating synthetic data that enhances LLM capabilities
and improves alignment with human values. In prompt optimization, for example, frameworks such as
PromptAgent formulate prompt engineering as a strategic planning task, using MCTS to explore diverse
instruction spaces and learn from failures to produce expert-level prompts [197, 245]. Similarly, Dynamic
Rewarding with Prompt Optimization (DRPO) applies MCTS-based search for tuning-free self-alignment,
generating optimal alignment instructions at inference time without costly retraining [173].

In safety and factuality contexts, MCTS can synthesize fine-grained reasoning data to train models
in identifying and mitigating risk. Frameworks such as HaluSearch, KNOT-MCTS, and others employ
self-evaluation-guided search to minimize hallucinations by favoring more reliable reasoning pathways
[31, 211, 49]. The STAIR framework specifically integrates safety-informed MCTS (SI-MCTS) to generate
step-level reasoning data that balances helpfulness and safety, which is then used to train a process reward
model for improved test-time responses [272]. This principle extends to red-teaming, where MCTS-based

40


fuzzing frameworks like AgentXploit or adaptive stress testing systematically search for prompt perturbations
and contextual manipulations to automatically discover and exploit agent vulnerabilities [203, 21]. Some
approaches use MCTS to enforce syntactic and semantic constraints during decoding [2] or guide search
using value models inherited from previous alignment stages like PPO [125, 95]. Additionally, multi-agent
variants of MCTS coordinate distributed sampling processes to synthesize higher-quality alignment data
through collective exploration and collaborative problem-solving [240, 55, 233, 77].

5.5.3. Scientific & Specialized Domains

The self-improvement paradigm has been increasingly adopted across a diverse range of specialized domains.
In software engineering and code generation, frameworks such as 01-Coder and SRA-MCTS explicitly employ
MCTS to generate extensive code samples with intermediate reasoning traces, which are subsequently used
to iteratively fine-tune the model’s programming proficiency [273, 226]. This approach has been extended to
automated program repair [78] and repository-level issue resolution, where multi-agent debate frameworks
or MCTS-refined Chain-of-Thought data are used to generate high-quality patches and fine-tuning data
[109, 201, 3]. Other systems use MCTS to improve test-time evaluation of code correctness [201]. In
knowledge-intensive tasks, MCTS enhances retrieval-augmented generation (RAG) by planning reasoning
actions [51, 81] and enables agents to traverse a corpus without a separate retriever model [96]. For
structured data, MCTS is used to generate high-quality annotations for knowledge base question answering
(KBQA) [133, 225, 202] and to build experience memory for text-to-SQL tasks [135, 105, 62].

At a meta-level, this paradigm underpins agent-based systems in Automated Machine Learning (AutoML),
where frameworks such as SELA, I-MCTS, and KompeteAI utilize MCTS to optimize the discovery and
configuration of machine learning pipelines [32, 118, 104]. MCTS has also been applied to optimize
hyperparameters for fine-tuning models in tasks like named entity matching [187]. Beyond these, MCTS
facilitates the generation of high-quality synthetic tabular data [128] and the development of domain-specific
models through self-evolution, including clinical reasoning systems in medicine [88, 42], legal reasoning
systems [124], and psychological counseling dialogues [130]. In chemistry and materials science, search-
driven agentic frameworks like ChemAgent have been employed to plan and execute intricate synthesis
procedures and integrate specialized tools [13, 11, 214]. In strategic settings such as game-playing, MCTS
continues to guide the learning of high-level strategies via self-play simulations in traditional [66, 119]
and text-based games [171]. This principle extends to molecular structure elucidation, where K-MSE [283]
integrates MCTS with a knowledge base to markedly improve chemical reasoning. Other emerging applications
include automated scientific discovery [59, 192, 1], web and GUI agent training [57, 220], synthetic data
generation for multilingual translation [287, 53], personalized educational content creation [217], heuristic
discovery for optimization problems [279, 265, 193], and automated theorem proving [285].

5.5.4. Multimodal Applications

The self-improvement paradigm has also expanded into multimodal domains, where MCTS enhances the
reasoning capabilities of Vision-Language Models (VLMs). To address the scarcity of fine-grained super-
vision, MCTS-based pipelines can autonomously generate millions of step-level annotations for training
process reward models (PRMs) without human labeling [47]. For example, MulBerry adopts a Collective
MCTS (CoMCTS) strategy that coordinates multiple models to collaboratively search for effective reasoning
trajectories, subsequently constructing large-scale multimodal datasets for training advanced Multimodal
Large Language Models (MLLMs) [236]. Other approaches use MCTS to automate structured thinking by
retrieving optimal high-level reasoning patterns [213] or re-rank retrieved reasoning context to improve
consistency [232].

41


Another approach, AR-MCTS, integrates active retrieval mechanisms into the search process to generate
step-wise annotations, which are used to train progressive reward models that verify reasoning chains [44].
This results in a self-improving feedback loop, enabling VLMs to ground their reasoning more effectively
in both visual and textual modalities. Recent extensions introduce multimodal actor-—critic frameworks,
where MCTS directs an actor model to explore diverse reasoning paths, while an annotator model compares
successful and failed trajectories to produce critique data that guide self-correction [127]. Data-efficient
variants leverage MCTS to quantify the difficulty of visual reasoning tasks by estimating the number of
search iterations required for solution discovery, thereby selecting a compact yet highly informative subset of
challenging samples for reinforcement fine-tuning [199]. The paradigm has also been applied to generate
data for vision-language-action (VLA) policies [142], create animated stories [169], generate fine-grained
video captions [246], and detect multimodal misinformation [37]. These principles are even being extended
to other generative modalities, such as using MCTS to enhance diffusion models for solving complex reasoning
tasks like mazes [263] or for steerable scene generation [151].

6. Informed Search Based Method

To enhance the reasoning capabilities of Large Language Models beyond simple sequential generation,
researchers have increasingly turned to informed search algorithms. This paradigm structures problem-
solving as a tree traversal, where heuristic guidance helps navigate vast and complex solution spaces efficiently.
Early frameworks such as Tree-of-Thoughts (ToT) adapted classical algorithms like Breadth-First Search
(BFS) and Depth-First Search (DFS), using the LLM itself to evaluate intermediate ‘thoughts’ and prioritize
promising reasoning paths. Building on this, more recent approaches have implemented A* search, a more
sophisticated heuristic method, to further optimize exploration. Methods like LLM-A*, ToolChain*, and
Q* exemplify this trend by designing intricate cost and heuristic functions that incorporate memory, self-
consistency, and learned value estimates to guide the search for optimal solutions. This section explores
these key informed search strategies, detailing how they formalize and direct the LLM’s reasoning process.

6.1. Informed BFS/DFS

The Tree-of-Thoughts (ToT) framework [239] enables Large Language Models (LMs) to systematically
explore multiple reasoning paths by formulating problem-solving as a tree search. Each node represents a
state s = [x,2Z},,;], where x is the input and z,, ; is the sequence of generated thoughts. ToT comprises four
key components: problem structuring, thought generation, state evaluation, and a search strategy.

In this framework, the problem is first decomposed into intermediate reasoning steps. At each step
i+1,a generator G(p»,s,k) produces k candidate thoughts from the current state using the LM pg. Thought
generation can occur via (1) sampling i.i.d. thoughts from a Chain-of-Thought (CoT) prompt—effective
for broad search spaces—or (2) proposing thoughts sequentially through a “propose prompt” to reduce
redundancy in more constrained tasks. An evaluation function V(p g,S) then assesses progress across
candidate states S, either through a value-based mode, assigning scores to each state, or a voting-based
mode, selecting the most promising candidate via LM judgment.

Informed search strategies operationalize these components through two primary algorithms. The
informed Breadth-First Search (BFS) algorithm functions like a beam search, maintaining a beam of b states
per level to control exponential growth, making it suitable for problems of bounded depth T. Conversely, the
informed Depth-First Search (DFS) algorithm follows a single reasoning path until its value score drops
below a threshold, at which point the path is pruned.

42


BFS-based approaches have proven particularly versatile. Frameworks such as Beam-LLM [221] and
PathFinder [60] extend ToT’s beam search, respectively using LLM-based value functions and similarity
metrics to select top candidates. Beyond text reasoning, this paradigm underlies methods like Think-on-
Graph [178, 137], which navigate knowledge graphs through SPARQL-based expansion and LLM-guided
pruning. In causal discovery, BFS-style search has been adapted to reduce query complexity from quadratic
to linear [90], integrate observational data into prompts [179], and incorporate dynamic, LLM-informed
scoring for identifying fairness-critical bias paths [252].

Recent work pushes these ideas toward greater autonomy and adaptivity. The Autonomous Tree-Search
(ATS) paradigm internalizes BFS-like exploration within the LLM itself using a fixed system prompt, cutting
API costs compared to externally controlled search loops [274]. Similarly, LLM-First Search (LFS) allows
the model to dynamically adjust its exploration depth and width, achieving a more flexible and efficient
search process [74]. At the architectural level, the Coconut (Chain of Continuous Thought) framework
reimagines reasoning in a continuous latent space, where a single “continuous thought” vector implicitly
represents multiple parallel reasoning paths, effectively enabling BFS-like exploration in vector space [71].

Finally, a related but distinct family of approaches employs Best-First Search, which greedily expands
the single most promising node from an open set. Best-LLM [100] demonstrates this for web navigation,
maintaining a priority queue of states scored by an LLM-based heuristic. In formal reasoning, BFS-Prover
[223] combines Best-First Search with expert iteration and Direct Preference Optimization (DPO), achieving
state-of-the-art theorem-proving results by normalizing proof lengths and encouraging deeper exploration. To-
gether, these methods highlight how informed search—whether breadth-first, depth-first, or best-first—forms
a principled foundation for scalable, LLM-guided reasoning.

6.2. A*

To mitigate the computational overhead associated with methods like Monte Carlo Tree Search (MCTS),
recent work has explored A*-based search algorithms. These methods guide exploration using a specialized
cost function f(n) = g(n) + h(n), which prioritizes nodes that appear to be on the most promising path to a
solution. This function balances the cost of the path taken so far, g(n), with an estimated cost to reach the
goal, h(n).

A prominent application of A* in robotics and path-finding is LLM-A* [140], which synergistically combines
the precise pathfinding capabilities of classic search algorithms with the global, commonsense reasoning of
LLMs. This framework enhances traditional A* search for tasks like maze-solving by incorporating LLM-based
heuristics. While the cost-so-far, g(n), remains the standard path distance, the heuristic estimate, h(n), is
innovatively formulated. It combines the geometric distance from a candidate node to the goal with the
distance to a waypoint suggested by an LLM. This hybrid heuristic grounds the search in physical reality
while intelligently biasing it with the LLM’s high-level understanding of the problem space. Furthermore,
the framework supports a human-in-the-loop approach, where human feedback on intermediate results can
refine the planning process, making it transparent and interactive.

Beyond path-finding, A* has been adapted to navigate the vast action spaces of LLM-based agents.
ToolChain* [284] addresses the challenge of selecting a correct sequence of tools (e.g., API calls) by
formulating the problem as a tree search. In this formulation, each node represents a possible API function
call, and the A* algorithm efficiently explores this decision tree. By incorporating a task-specific cost function,
ToolChain* prunes high-cost branches corresponding to incorrect or inefficient action sequences, allowing it
to identify an optimal plan while effectively balancing exploration and exploitation.

43


Table 6: Compact Overview of A* Heuristic Components for LLMs

Heuristic A* Component Mechanism (and Signal Source)

Process-Based Rewards g(n) Aggregates step-wise rewards from execu-
tion feedback (e.g., logits, rule checks).

Statistical Consistency g(n) Favors steps that are frequently proposed
across multiple generation samples.

Memory-Based Comparison g(n), h(n) Scores path similarity against a repository
of high-quality examples (€.g., using LCS).

Learned Future Value h(n) Estimates the cost-to-goal using a trained
proxy model €.g., a Q-function).

Similarly, A* search can guide the internal reasoning process of an LLM. The Q* framework [190]
casts multi-step reasoning as a heuristic search problem to improve the model’s decoding process. Instead
of generating text purely auto-regressively, the model uses A* to explore different reasoning steps. Its
key innovation is a learned, plug-and-play Q-value model that serves as the heuristic function h(n). This
function estimates the expected future reward of a reasoning path, guiding the LLM towards more promising
intermediate steps and enhancing performance without requiring costly, task-specific fine-tuning of the base
model. The primary innovation in these methods lies in constructing composite heuristics for g(n) and h(n)
from diverse, LLM-relevant signals, as summarized in Table 6.

ToolChain*

In ToolChain*, the cost function for a node n is the standard A* formulation, f(n) = g(n) + h(n), where
g(n) is the cumulative cost from the start node to n, and h(n) is a heuristic estimate of the future cost to
the goal. The cumulative cost g(n) is the sum of single-step costs over all ancestors of n, denoted an(n). Each
single-step cost is derived from two value functions, g;; and g;2, whose outputs are bounded in [0,1]. The
cost is formulated as the geometric mean of the complements of these values. The cumulative cost is thus:

g(n)= SP (1 gia)" A geal), (5)

i€{an(n),n}
where the hyperparameter « weights the contribution of each value function.

The first value function, g;1(1), is task-specific and draws from a long-term memory M, which is
initialized with seed demonstrations and augmented with successful plans discovered during search. Each
memory entry m; is a plan sequence (sj, 4j,1,--- /4j,T;). This function evaluates the current plan s, by
computing its maximum longest common subsequence (LCS) score against all plans in memory: g1(n) =

LCS (s11,111;) : 3 F
MAXmieM mn(L(5,) Lm)? where L is the sequence length. The second value function, g;2(n), is based on

self-consistency frequency. It measures the frequency with which node n is proposed as the next step across
k independently sampled reasoning paths, reflecting its reliability.

The future cost h(n) is formulated analogously to g(n):

n(n) = SD (1 haa (i) - (= ali) 8, 6)
i€{an(n),n}

44


where f is the geometric mean weight. The first heuristic, h(n), leverages the long-term memory M.
For an action node 1, it finds the action a in each memory plan m; with the highest lexical similarity to
pos(a,m;)
ay
heuristic, h;2(), is an LLM imagination score. An LLM generates a plausible future plan toward a target
node nr, and the heuristic value is the ratio of the current path length to the total imagined path length:
hiyo(n) = lan()| where |an(-)| is the number of ancestors. A higher score signifies closer proximity to the

~— fan(nr)|?
goal.

n. The heuristic is the sum of these actions’ relative positions: h;1(n) = yimje M Aaem, . The second

Q e
.

In Q*, the cost function is f(n) = g(n) +Ah(n), where A is a weighting hyperparameter. The accumulated
cost g(n) is an aggregation of process-based rewards for the current node and its ancestors: g(n) =
Agg({R(s) | s € an(n) U {n}}). The reward function ® can be derived from human feedback, ground-
truth labels, predefined rules, or LM logit scores. The aggregation function, Agg, can be chosen from
{max, min, >>, |—1]}, where |—1] indicates selecting the reward of the last node.

The heuristic cost h(n) is a Q-function that estimates the expected future reward. As an exhaustive search
over subsequent steps is intractable, the heuristic is approximated by taking the maximum Q-value among
the top-k actions proposed by the LLM policy 79: h(n) = max,,ctop-k(z9(-\n)) Q(7, 4). A primary challenge is
estimating optimal Q-values when the frozen policy 7tg is suboptimal. The authors propose three methods for
learning a proxy Q-value model: (1) offline reinforcement learning on curated data, (2) learning from MCTS
rollouts, or (3) distillation from a stronger LLM. However, this approach may have limited generalization,
and the anticipated computational savings are not guaranteed.

7. Search In Prompt Space

While most current test-time scaling methods, including all we have covered above, conduct search in answer
space—directly exploring candidate reasoning paths or intermediate solutions—an emerging line of research
shifts the optimization process to the prompt space [268]. Rather than modifying the generated content,
these methods iteratively refine or search over prompts that steer the model’s reasoning trajectory. This
perspective treats the prompt itself as a control variable in a high-dimensional discrete space, where each
candidate prompt induces a distinct reasoning policy over the same base model.

Recent work such as Automatic Prompt Optimization (APO) [155] focus on the search in prompt space 6.
Given a task and a validation set, prompt search begins with an initial handcrafted prompt and employs
large language models to propose, evaluate, and refine candidate prompts automatically. The framework
decomposes the optimization loop into three stages: (i) prompt generation, where variants are produced
via LLM-driven mutation or rewriting; (ii) evaluation, where each prompt’s downstream performance (.g.,
accuracy or reward) is measured; and (iii) selection, where high-performing candidates are retained or
combined to guide the next iteration. This process embodies a meta-level search, optimizing the policy that
governs how the model reasons rather than the reasoning steps themselves.

Conceptually, search in prompt space can be interpreted as meta-inference—a higher-order optimization
that adapts the inference procedure without altering model weights. Compared to answer-space search,
which explores reasoning trajectories within a fixed prompting scheme, prompt-space search operates one
level higher, learning to generate better reasoning algorithms through prompt adaptation. The optimization

45


(a) Prompt Space --> Answer Space

Prompt

(b) Chain of thoughts and its variants in large language model

Chain of Thoughts Tree of Thoughts

Search via Planing

State

Se (Greedy Search) (BFS or DFS) (MCTS) Transition
om — =
Answer as _| SS = SS =
Space 5
[ CL] =
Ao = Ej @
Stock it on yellow | a soa arr WA | ay
4 - E44 AR @
- q _ i Ss. 5 Rollout a,
’
— 5 —~E6 B.S e
Reward Model

Figure 6: Search in prompt space v.s. search in answer space. Search allows for complicated reasoning
beyond single path as seen in CoT.

objective is therefore external to the reasoning process, focusing on eliciting the most effective reasoning
strategy rather than the most accurate solution.

Such prompt-space optimization methods naturally complement search in answer space. Whereas the
latter focuses on inference-time deliberation conditioned on a fixed query formulation, the former performs
structural adaptation over the query formulation itself. As prompt optimization techniques mature—ranging
from evolutionary strategies to reinforcement learning over discrete prompt tokens—they offer a scalable
and model-agnostic route toward improved test-time performance, potentially serving as the outer loop to
guide answer-space search policies.

7.1. Search Complexity in Prompt and Answer Spaces

Test-time reasoning with large language models can be understood as a search process unfolding across two
coupled spaces: the prompt space, which defines how reasoning is initiated and structured, and the answer
space, which represents the trajectories of reasoning or solutions produced under a given prompt. Each space
possesses distinct search characteristics and computational bottlenecks, and their interaction fundamentally
shapes the efficiency and scalability of inference-time optimization.

Prompt Space Search. Search in the prompt space operates at a meta level, seeking to identify a high-level
control program that guides the model’s reasoning behavior (Table 7). This search exhibits high semantic
branching complexity: each candidate prompt can induce a qualitatively different reasoning distribution,
leading to non-smooth objective landscapes. Because prompt evaluation requires full model inference for each
candidate, the computational cost scales linearly with the number of candidates but exponentially with the
diversity of the prompt modifications considered. The effective search depth is thus bounded by evaluation
cost, prompting the use of heuristic or LLM-driven mutation operators to navigate the combinatorial space
efficiently.

Answer Space Search. Conversely, search in the answer space—exemplified by Monte Carlo Tree Search
(MCTS), beam search, and self-refinement—unfolds within the reasoning trajectories conditioned on a fixed
prompt. Its branching factor corresponds to the number of possible next-step continuations, while its depth
corresponds to the number of reasoning steps or thought expansions. The complexity here arises from the

46


exponential growth of possible paths as reasoning deepens, making it crucial to incorporate value estimates
or pruning strategies to maintain tractability. In this sense, answer-space search optimizes the inference
path given a static reasoning policy defined by the prompt.

Coupled Search Dynamics. Prompt and answer spaces interact hierarchically: the prompt determines
the policy that shapes exploration in the answer space, while the observed outcomes in the answer space
provide feedback signals for prompt adaptation. This coupling forms a bi-level optimization problem:

na Ex.p|[L(fo(x;p))], (7)

where p € P represents a prompt sampled from the prompt space and f9(x; p) denotes the model’s reasoning
trajectory in answer space under parameters 0. The outer loop optimizes over prompts, while the inner loop
executes inference-time search within each induced reasoning policy. In practice, these two searches are not
independent: better prompts shape smoother answer landscapes, reducing search depth and variance, while
richer answer-space exploration yields more reliable reward signals for prompt optimization.

7.2. Discrete (Text-Based) Prompt Search

Discrete prompt search methods aim to optimize natural language prompts directly in the token space without
gradient updates or parameter tuning. Unlike soft prompt tuning, which learns continuous embeddings,
these methods treat prompts as symbolic sequences and explore their combinatorial structure through
heuristic or metaheuristic search. This makes them model-agnostic, accessible via APIs, and highly suitable
for closed-source large language models (LLMs). The general framework can be summarized as: (i) generate
new candidate prompts through mutation, rewriting, or planning; (ii) evaluate their performance using a
task-specific metric or feedback signal; and (iii) retain or recombine the top-performing candidates. Below,
we review representative methods and categorize them by their underlying search paradigm.

Evolutionary and Population-Based Search. GPS (Genetic Prompt Search) [227] and EvoPrompt [67]
instantiate evolutionary algorithms for prompt optimization. GPS introduces a genetic algorithm that evolves
prompts via mutation and crossover operators guided by task accuracy on a small validation set. EvoPrompt
extends this idea by integrating an LLM into the evolutionary loop—LLMs are used to perform semantically
coherent mutations and recombinations while classical evolutionary operators (selection, mutation, crossover)
drive convergence. Both methods are gradient-free and exhibit strong data efficiency, outperforming manual
and tuning-based baselines. Their reward function is typically the task-specific performance score (.g.,
accuracy or BLEU) evaluated on a held-out set. These approaches demonstrate that population diversity and
linguistic mutation can jointly enhance prompt robustness and transferability.

Promptbreeder [54] extends evolutionary search into a self-referential paradigm, where the system
co-evolves both task prompts and mutation prompts. The LLM learns to iteratively refine its own mutation
strategy, forming a meta-evolutionary loop. This introduces a novel self-improvement dynamic where the
mutation operator itself adapts to the optimization landscape, leading to improved generalization and
domain-specific expertise.

Gradient-Inspired and Beam Search Methods. ProTeGi (Prompt Optimization with Textual Gradi-
ents) [155] proposes a hybrid search procedure inspired by numerical gradient descent. It constructs
natural-language “gradients” that describe directions for prompt improvement, which are then propagated
back into the text of the prompt. Beam search and multi-armed bandit selection are used to explore the
most promising gradient edits efficiently. This approach combines human-readable textual updates with

47


Table 7: Comparison of representative methods for search in prompt space. Each approach optimizes prompts
as discrete or continuous control variables, differing in search paradigm, reward design, and interpretability.

Method
GPS [227]

EvoPrompt [67]

PromptBreeder [54]

ProTeGi [155]

GrIPS [154]

APE [282]

PromptAgent [197]

MPA [216]

BPO [30]

HPME [210]

InstructZero [25]

Adversarial Soft
Search [286]

Paradigm

Evolutionary

Evolutionary

Meta-
Evolutionary

Gradient-

inspired

Local Search

LLM-as-
Search-
Engine

Planning /
Tree Search
Adversarial
MCTS
Preference-
based
Gradient-
based
Latent /

Bayesian

Gradient-
guided

Search Type

Discrete

(Population-based)

Discrete (Hybrid)

Discrete

Discrete (Textual
Gradients)

Discrete
(Edit-based)

Discrete

Discrete
(Sequential)

Discrete
(Tree-based)
Discrete
(Black-box)

Soft / Hybrid

Soft (Latent)

Soft (Continuous)

Reward Signal

Task accuracy

Task accuracy

Task accuracy

Loss reduction

Task accuracy

Task score (e.g.,
truthfulness)

Composite
reward

Attack success
rate

Human
preference

Task accuracy

Task
performance

Unsafe output
likelihood

Key Characteristics

Genetic algorithm with muta-
tion and crossover over natural-
language prompts; gradient-free
and data-efficient.

Integrates LLM-driven semantic
mutation with classical evolution-
ary operators for robust prompt
evolution.

Co-evolves both task prompts and
mutation prompts; self-referential
optimization of mutation strategy.
Constructs natural-language
“gradients” to iteratively improve
prompts via beam search and
bandit selection.

Applies inser-
tion/deletion/substitution edits
to seed prompts; efficient for
API-limited settings.

Two-model setup where one LLM
generates and another evaluates
prompts; self-evaluative optimiza-
tion loop.

Models prompt editing as sequen-
tial decision-making with MCTS-
based exploration and rollout eval-
uation.

Uses MCTS to discover adversarial
suffixes for jailbreak attacks; high-
lights vulnerability exploration.
Aligns LLM outputs with hu-
man preferences via black-box
search; alternative to PPO/DPO
fine-tuning.

Learns discrete textual prompts via
gradient relaxations; bridges hard
and soft prompts.

Optimizes latent soft prompts via
Bayesian optimization for black-
box models; transferable instruc-
tion generator.

Learns continuous adversarial suf-
fix embeddings transferable across
aligned models.

48


algorithmic rigor, bridging the gap between discrete prompt editing and continuous optimization. The
reward signal corresponds to a task-specific loss reduction across mini-batches, aligning the search direction
with performance improvement.

Edit-Based Local Search. GrIPS (Gradient-free, Edit-based Instruction Search) [154] performs local
edits (insertions, deletions, and substitutions) to improve task instructions. It searches within a constrained
neighborhood around a seed instruction, using validation performance as the fitness measure. This method
is efficient for API-constrained models, requiring minimal evaluations. GrIPS highlights that even small,
syntactically simple edits can yield substantial semantic gains, demonstrating that local search in prompt
space can approximate global optimization when guided by strong reward signals.

LLM-as-Search-Engine Paradigm. APE (Automatic Prompt Engineer) [282] reframes prompt search
as a two-model optimization loop. One LLM (the “engineer”) proposes candidate prompts, while another
LLM (the “executor”) evaluates their zero-shot performance. The search proceeds by ranking and selecting
candidates based on a scoring function that may incorporate truthfulness, informativeness, or accuracy. This
black-box, self-evaluative process effectively treats the LLM as both generator and judge. The evaluation
metric functions as a reward, and the system converges toward human-level prompt engineering performance
without explicit supervision.

Planning and Tree Search. PromptAgent [197] and MPA (Monte Carlo Tree Search-Based Prompt
Autogeneration) [216] apply strategic planning and tree-based exploration to the prompt optimization
problem. PromptAgent models prompt design as a sequential decision process, where each editing or
expansion step forms a node in a search tree. Monte Carlo Tree Search (MCTS) guides exploration using
simulated rollouts and reward backpropagation from model feedback. The reward is defined as a composite of
task success, coherence, and informativeness, enabling the discovery of expert-level prompts that rival human-
crafted ones. MPA adapts this MCTS framework to adversarial prompt generation, where nodes represent
attack suffixes and rewards correspond to successful jailbreak attempts. Both highlight the flexibility of
search-based reasoning applied to prompt optimization.

Preference-Guided Optimization. Black-Box Prompt Optimization (BPO) [30] optimizes prompts to
align LLM outputs with human preferences, rather than fixed task metrics. The search process treats prompt
editing as a preference-based black-box optimization problem, where feedback from human annotators or
reward models serves as the fitness signal. This enables alignment without model retraining and achieves
results comparable to reinforcement learning—based methods such as PPO and DPO. The approach thus
reframes alignment as prompt-level optimization in the discrete space, demonstrating that search-based
prompt tuning can substitute for costly fine-tuning.

Comparative Analysis. Across these methods, the search complexity depends on whether the algorithm
explores global (population-based or tree search) or local (edit-based or gradient-inspired) neighborhoods.
Evolutionary and MCTS-based methods are computationally intensive but robust to local minima, while
edit-based and bandit-guided methods offer efficiency at the cost of search coverage. The reward design
also varies: task-level accuracy for supervised settings (GPS, GrIPS, ProTeGi), preference alignment for
human-feedback-driven setups (BPO), and adversarial success for safety probing (MPA). Together, these
discrete prompt search methods reveal a growing consensus that textual optimization—driven by structured
search and LLM-in-the-loop evaluation—provides a scalable, interpretable, and model-agnostic pathway to
enhance reasoning performance and alignment in large language models.

49


7.3. Soft (Latent) Prompt Search

While discrete prompt search operates directly in the textual space, soft prompt search explores a continuous
or latent representation of prompts. These methods treat prompts as trainable embeddings in the model’s
input layer or in a lower-dimensional latent space, enabling differentiable optimization and gradient-based
adaptation. The advantage lies in their efficiency and fine-grained controllability: small movements in the
latent space can correspond to semantically rich changes in model behavior. However, the resulting prompts
are often uninterpretable and may require additional mechanisms to map latent representations back to
natural language.

Gradient-Based Optimization and Discrete Projection. Hard Prompts Made Easy (HPME) [210]
introduces a hybrid optimization framework that bridges soft and hard prompts. The method applies
gradient-based discrete optimization to directly learn textual prompts that are robust across modalities,
such as text-to-image and text-to-text tasks. By estimating gradients through continuous relaxations of the
discrete token space, the algorithm discovers effective hard prompts automatically, without requiring manual
engineering. The optimized prompts outperform human-crafted ones in both generative and classification
settings, showing that latent search with gradient feedback can serve as a bridge between interpretable
text prompts and continuous embeddings. HPME also establishes that latent search can yield interpretable,
transferable prompts through differentiable decoding mechanisms.

Latent Instruction Optimization for Black-Box Models. InstructZero [25] extends latent prompt op-
timization to black-box LLMs where backpropagation is unavailable. The approach trains a soft prompt
on an open-source model to generate explicit natural-language instructions for a target black-box model
(e.g., GPT or ChatGPT). Each iteration converts the learned soft prompt into text using the open-source
LLM, evaluates its zero-shot performance on the target model, and updates the latent representation using
Bayesian optimization. This process decouples gradient estimation from model access, effectively turning
prompt optimization into a surrogate learning problem. The reward signal is the performance score (accuracy
or preference alignment) on the target model’s outputs, allowing indirect gradient-free updates in latent
space. This framework demonstrates that latent prompts can act as transferable “instruction generators”
across models.

Gradient-Guided Adversarial Prompt Search. Universal and Transferable Adversarial Attacks on
Aligned Language Models [286] explores soft prompt search from a safety and robustness perspective. The
authors use a combination of greedy and gradient-based search to optimize continuous adversarial suffix
embeddings that induce objectionable behavior in aligned models. These suffixes, though discovered in latent
embedding space, are decoded back into natural language to serve as universal adversarial prompts. The
optimization objective maximizes the probability of eliciting affirmative (unsafe) responses, and the learned
suffixes demonstrate strong transferability across model families, including both open- and closed-source
LLMs (e.g., Vicuna, ChatGPT, Bard, and Claude). This highlights that continuous search techniques can
capture global vulnerabilities shared across models, emphasizing the dual use of latent prompt search for
alignment and red-teaming studies.

8. Challenges and Future of Tree-Search Methods
Search Efficiency and Intelligence. Tree search algorithms, despite their power, often require significantly

greater computational resources than greedy decoding. As noted by [189], resource demands can exceed 10
times that of greedy approaches, and other studies show that tree search can be 10-20 times slower than

50


methods like iterative refinement, particularly if the evaluation model lacks high discrimination accuracy
[28]. This high computational and memory overhead presents a substantial barrier to practical deployment.
Algorithms like MCTSr and LLaMA-Berry, which generate multiple solutions sequentially at each node,
exacerbate these resource demands. To mitigate these limitations, future research could prioritize improving
the efficiency of tree search algorithms. This includes investigating trade-offs between policy and reward
models, incorporating dynamic control mechanisms, employing effective pruning techniques to optimize
tree expansion, and developing more efficient memory solutions like tree-structured Key-Value (KV) caches
to reduce I/O overhead in transformer models [237].

Overthinking Issues in Simple Queries. Task complexity is closely related to the length of reasoning chains,
highlighting the need for extended cognitive processing in more difficult problems [158, 83]. However,
[27] and [253] observe that tree-search models often overanalyze simple questions, dedicating excessive
computational resources to tasks that have clear answers. For instance, a query like "3-2=?" does not require
complex reasoning, yet these models may engage in unnecessary computations. This not only consumes
valuable resources but can also degrade performance. Indeed, empirical studies show that in some scenarios,
complex search frameworks like ToT and RAP perform even worse than simpler methods like Chain-of-
Thought (CoT) or self-consistency [148]. For tasks where the base LLM can already produce reasonable
answers, simple sequential revision may be a more efficient and effective alternative to complex search [174].
Future work should focus on methods for dynamically allocating computational resources, enabling models
to quickly recognize and handle straightforward queries while reserving deliberate search for genuinely
complex problems.

Self-play Between Policy Models and Reward Models. Certain tree-search algorithms encounter challenges
due to limited parallelism, which constrains their search speed, especially in resource-intensive settings. As
detailed in Section 5, various tree-search techniques can generate traces that are then employed to iteratively
refine reward and policy models, such as ReST-MCTS and rStar-Math. This self-play paradigm is crucial
for internalizing the reasoning system into the policy model, thereby endowing LLMs with sophisticated
reasoning abilities [219]. By internalizing tree-search reasoning into LLMs, the search process can be
structured within a CoT framework, facilitating sequential reasoning. This not only enhances reasoning
efficiency but also mitigates parallelism limitations, thereby improving scalability. Future research should
investigate strategies to optimize this self-play paradigm further, facilitating more efficient problem-solving.

Reward Modeling and Reward Model Training. Section 5 examines various MCTS-based evaluation
strategies. A central element of the search strategies is the reward or evaluation model, which provides
essential supervision to guide search processes effectively [120, 167, 219]. Reward models are broadly
categorized into two types: the Outcome Reward Model (ORM) and the Process Reward Model (PRM).
Unlike outcome rewards, which deliver feedback only at the task’s conclusion, process rewards provide signals
at both intermediate steps and the final outcome, enabling finer-grained and more frequent supervision.
Nevertheless, learning process rewards present significant challenges. For example, [186, 120] relies on
human annotators for process supervision, a costly and inherently unscalable method. While automated
methods for constructing process rewards have been proposed [194, 134, 204], they are predominantly
designed for specialized areas such as mathematics and programming. These approaches struggle to
generalize to broader domains, such as scientific reasoning and complex problem-solving, where human
evaluation remains essential. Overcoming these limitations necessitates the development of more efficient
methods to generate high-quality fine-grained rewards and scalable techniques to advance reward model
capabilities, which remain open and pressing research challenges.

Reward Model Quality and Its Effect on Search. The performance and efficiency of test-time search depend
critically on the quality of the reward model [167, 219]. An imperfect reward model can give rise to inverse

51


inference scaling, where expanding the search space negatively impacts performance due to a distribution
shift between the reward and policy models [56, 254]. Furthermore, the value functions produced by LLMs
often act as heuristics based on world knowledge rather than being rigorously defined by the Bellman
equation, which is the foundation of traditional reinforcement learning [180, 238]. This deviation means
that fundamental properties, such as the optimality guarantees in A* search, can be compromised if the
LLM-based heuristic is inadmissible [8]. These findings underscore the critical need to bridge the performance
gap between oracle and learned reward models. Understanding how scaling laws for process supervision
models influence their effectiveness in large-scale search tasks remains a pivotal challenge [219].

Applicability to Irreversible Environments. A significant limitation of many current search frameworks is
their reliance on the ability to undo actions. Processes like backtracking in DFS [238], selecting an alternative
node in Best-First Search [100], or simulating rollouts in MCTS [281] all presuppose that the agent can
return to a previous state to explore a different path. This restricts their applicability primarily to virtual or
simulated environments (e.g., solving puzzles, web navigation) where state transitions are fully reversible
and carry no real-world consequences. However, in dynamic, interactive environments, many actions are
irreversible—such as sending an email, executing a financial trade, or administering a medication. A crucial
direction for future research is to devise new frameworks capable of planning and acting effectively in
environments with irreversible actions, which will be essential for deploying these agents in real-world
scenarios.

Balancing Test-Time Search with Foundational Model Capabilities. Finally, the field must consider the
broader trade-off between enhancing LLMs through intensive test-time computation versus improving their
innate, built-in reasoning capabilities. While complex search algorithms can significantly boost performance
on difficult tasks, an alternative and complementary research direction focuses on training LLMs to internalize
tree-like reasoning directly. Optimization frameworks such as Chain of Preference Optimization [270] aim
to distill the deliberative process of search into the model’s parameters, reducing the need for expensive
real-time computation. Future work should continue this parallel investigation: enhancing the power and
efficiency of test-time search while simultaneously advancing training methods that endow LLMs with more
autonomous, multi-path thinking capabilities from the outset.

9. Conclusion

This survey has charted the evolution of tree search-based algorithms as a pivotal strategy for scaling the
reasoning capabilities of Large Language Models at inference time. By establishing a unified framework, we
have systematically compared a diverse landscape of methods-from foundational search techniques to the more
sophisticated Monte Carlo Tree Search-highlighting their distinct approaches to node representation, reward
formulation, and algorithmic adaptation. Our analysis underscores that while these methods successfully
overcome the single-path limitations of approaches like Chain-of-Thought, their practical application is
constrained by two primary challenges: substantial computational overhead and the critical bottleneck of
designing high-quality reward models. As imperfect reward signals can lead to "inverse inference scaling",
where more search degrades performance, the future of the field hinges on developing more intelligent,
resource-efficient search dynamics and, most importantly, creating scalable techniques for generating high-
fidelity process rewards. Addressing these open problems is essential to unlocking the full potential of tree
search for advancing general-purpose AI reasoning.

52


References

[1]

[2]

[3

pa)

L4

a

[5

HW

6

a

L7

—

[8
[9]

faa)

[10]

[11]

[12]

[13]

Dhruv Agarwal, Bodhisattwa Prasad Majumder, Reece Adamson, Megha Chakravorty, Satvika Reddy
Gavireddy, Aditya Parashar, Harshit Surana, Bhavana Dalvi Mishra, Andrew McCallum, Ashish Sab-
harwal, et al. Open-ended scientific discovery via bayesian surprise. arXiv preprint arXiv:2507.00310,
2025.

Mohammad Albinhassan, Pranava Madhyastha, and Alessandra Russo. SEM-CTRL: Semantically
controlled decoding. arXiv preprint arXiv:2503.01804, 2025.

Antonis Antoniades, Albert Orwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. SWE-
search: Enhancing software agents with monte carlo tree search and iterative refinement. arXiv
preprint arXtv:2410.20285, 2024.

Masataro Asai. Bilevel MCTS for amortized O(1) node selection in classical planning. arXiv preprint
arXiv:2508.08385, 2025.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,
Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for
alignment. arXiv preprint arXiv:2112.00861, 2021.

Hagai Attias. Planning by probabilistic inference. In International workshop on artificial intelligence
and statistics, pages 9-16. PMLR, 2003.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Richard Bellman. Dynamic programming. science, 153(3731):34-37, 1966.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi,
Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts:
Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 38, pages 17682-17690, 2024.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi,
Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts:
Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 38, pages 17682-17690, 2024.

Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research
capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.

Matthew Botvinick, Ari Weinstein, Alec Solway, and Andrew Barto. Reinforcement learning, efficient
coding, and the statistics of natural tasks. Current opinion in behavioral sciences, 5:71-77, 2015.

Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller.
Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376,
2023.

53


[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

David Brandfonbrener, Simon Henniger, Sibi Raja, Tarun Prasad, Chloe Loughridge, Federico Cas-
sano, Sabrina Ruixin Hu, Jianang Yang, William E Byrd, Robert Zinkov, et al. VerMCTS: Synthesiz-
ing multi-step programs using a verifier, a large language model, and tree search. arXiv preprint
arXiv:2402.08147, 2024.

David Brandfonbrener, Sibi Raja, Tarun Prasad, Chloe Loughridge, Jianang Yang, Simon Henniger,
William E Byrd, Robert Zinkov, and Nada Amin. Verified multi-step synthesis using large language
models and monte carlo tree search. arXiv preprint arXiv:2402.08147, 2024.

Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia
Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. In arXiv,
2024. URL https://arxiv.org/abs/2407.21787.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,
Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
1457c0dé6ébfcbh4967418bfb8acl42f64a-Paper.pdf.

Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp
Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of
monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4
(1):1-43, 2012.

Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Chenyu You, Shafiq Joty, and Giuseppe Carenini.
Multi2: Multi-agent test-time scalable framework for multi-document processing. arXiv preprint
arXiv:2502.20592, 2025.

Antoine Chaffin, Vincent Claveau, and Ewa Kijak. Ppl-mcts: Constrained textual generation through
discriminator-guided mcts decoding. arXiv preprint arXiv:2109.13582, 2021.

Neeloy Chakraborty, John Pohovey, Melkior Ornik, and Katherine Driggs-Campbell. Adaptive stress
testing black-box LLM planners. arXiv preprint arXiv:2505.05665, 2025.

Yash Chandak, HyunJi Nam, Allen Nie, Jonathan Lee, and Emma Brunskill. Information directed
tree search: Reasoning and planning with language agents. In NeurIPS 2024 Workshop on Bayesian
Decision-making and Uncertainty.

Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision
without process. Advances in Neural Information Processing Systems, 37:27689-27724, 2024.

Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimization for
mathematical reasoning. arXiv preprint arXiv:2406.10858, 2024.

Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient
instruction optimization for black-box large language models, 2023. URL https://arxiv.org/
abs/2306.03082.

54


[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

Sijia Chen, Baochun Li, and Di Niu. Boosting of thoughts: Trial-and-error problem solving with large
language models. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=qBL04XXex6.

Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu,
Mengfei Zhou, Zhuosheng Zhang, et al. Do NOT think that much for 2+ 3=? on the overthinking of
ol-like LLMs. arXiv preprint arXiv:2412.21187, 2024.

Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search useful
for LLM planning? it depends on the discriminator. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,
editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pages 13659-13678, Bangkok, Thailand, August 2024. Association for Computational
Linguistics. doi: 10.18653/v1/2024.acl-long.738. URL https://aclanthology.org/2024.
acl—Long. 738.

Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search
useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402. 10890, 2024.

Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie
Huang. Black-box prompt optimization: Aligning large language models without model training. In
Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pages 3201-3219, Bangkok,
Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.
176. URL https://aclanthology.org/2024.acl-long.176/.

Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Think more, hallucinate less: Mitigating
hallucinations via dual process of fast and slow thinking. arXiv preprint arXiv:2501.01306, 2025.

Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi
Pang, Jacky Kwok, Ceyao Zhang, et al. Sela: Tree-search enhanced Ilm agents for automated machine
learning. arXiv preprint arXiv:2410.17238, 2024.

Sehyun Choi, Tianging Fang, Zhaowei Wang, and Yangqiu Song. KCTS: knowledge-constrained tree
search decoding with token-level hallucination detection. arXiv preprint arXiv:2310.09044, 2023.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing systems,
30, 2017.

Ho-Lam Chung, Teng-Yun Hsiao, Hsiao-Ying Huang, Chunerh Cho, Jian-Ren Lin, Zhang Ziwei, and
Yun-Nung Chen. Revisiting test-time scaling: A survey and a diversity-aware method for efficient
reasoning. arXiv preprint arXiv:2506.04611, 2025.

Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Computers
and Games, 2006. URL https: //api.semanticscholar.org/CorpusID:16724115.

Xing Cui, Yueying Zou, Zekun Li, Peipei Li, Xinyuan Xu, Xuannan Liu, Huaibo Huang, and Ran He.
T*Agent a tool-augmented multimodal misinformation detection agent with monte carlo tree search.
arXiv preprint arXiv:2505.19768, 2025.

Joseph C Culberson and Jonathan Schaeffer. Pattern databases. Computational Intelligence, 14(3):
318-334, 1998.

55


[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

Nicola Dainese, Matteo Merler, Minttu Alakuijala, and Pekka Marttinen. Generating code world models
with large language models guided by monte carlo tree search. arXiv preprint arXiv:2405.15383,
2024.

Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri,
Siddharth Garg, and Jeyavijayan Rajendran. Make every move count: Llm-based high-quality rtl code
generation using mcts. arXiv preprint arXiv:2402.03289, 2024.

Edsger W Dijkstra. A note on two problems in connexion with graphs. In Edsger Wybe Dijkstra: his
life, work, and legacy, pages 287-290. 2022.

Hongxin Ding, Baixiang Huang, Yue Fang, Weibin Liao, Xinke Jiang, Zheng Li, Junfeng Zhao, and
Yasha Wang. ProMed: Shapley information gain guided reinforcement learning for proactive medical
LLMs. arXiv preprint arXiv:2508.13514, 2025.

Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan
Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose
triangle for thought generation. arXiv preprint arXiv:2311.04254, 2023.

Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen.
Progressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024.

Alex ZH Dou, Zhongwei Wan, Dongfei Cui, Xin Wang, Jing Xiong, Haokun Lin, Chaofan Tao, Shen
Yan, and Mi Zhang. Enhancing test-time scaling of large language models with hierarchical retrieval-
augmented MCTS. arXiv preprint arXiv:2507.05557, 2025.

Hanwen Du, Bo Peng, and Xia Ning. SAPIENT: Mastering multi-turn conversational recommendation
with strategic planning and monte carlo tree search. arXiv preprint arXiv:2410.09580, 2024.

Lingxiao Du, Fanging Meng, Zongkai Liu, Zhixiang Zhou, Ping Luo, Qiaosheng Zhang, and Wendi
Shao. MM-PRM: Enhancing multimodal mathematical reasoning with scalable step-level supervision.
arXiv preprint arXiv:2505.13427, 2025.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality
and reasoning in language models through multiagent debate. In Forty-first International Conference
on Machine Learning, 2023.

Zhihua Duan and Jialin Wang. Prompt-based monte carlo tree search for mitigating hallucinations in
large models. arXiv preprint arXiv:2501.13942, 2025.

Jonathan Evans. Heuristic and analytic processes in reasoning. British Journal of Psychology, 75(4):
451-468, 1984.

Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. AirRAG: Activating intrinsic
reasoning for retrieval augmented generation using tree-based search. arXiv preprint arXiv:2501.10053,
2025.

Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun
Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv
preprint arXtv:2309.17179, 2023.

56


[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

Zhaopeng Feng, Jiahan Ren, Jiayuan Su, Jiamei Zheng, Zhihang Tang, Hongwei Wang, and Zuozhu
Liu. MT-RewardTree: A comprehensive framework for advancing LLM-based machine translation via
reward modeling. arXiv preprint arXiv:2503.12123, 2025.

Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel.
Promptbreeder: Self-referential self-improvement via prompt evolution, 2023. URLhttps://arxiv.
org/abs/2309...16797.

Bingzheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Yusu Li, Shu Xian Teo, Changwang Zhang, and
Wei Shi. MASTER: A multi-agent system with LLM specialized MCTS. arXiv preprint arXiv:2501.14304,
2025.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In
International Conference on Machine Learning, pages 10835-10866. PMLR, 2023.

Yifei Gao, Junhong Ye, Jiaqi Wang, and Jitao Sang. WebSynthesis: World-model-guided MCTS for
efficient WebUI-trajectory synthesis. arXiv preprint arXiv:2507.04370, 2025.

Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen.
Interpretable contrastive monte carlo tree search reasoning. arXiv preprint arXiv:2410.01707, 2024.

Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, and Arman Cohan. Iris: Interactive research
ideation system for accelerating scientific discovery. arXiv preprint arXiv:2504.16728, 2025.

Olga Golovneva, Sean O’Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam
Fazel-Zarandi, and Asli Celikyilmaz. PATHFINDER: Guided search over multi-step reasoning paths.
In RO-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023. URL

=

https://openreview.net/forum?id=5TsfE

EwRsu.

Mingyang Gong, Jing Fan, Guohui Lin, Bing Su, Zihan Su, and Xiang Zhang. Multiprocessor scheduling
with testing: improved online algorithms and numerical experiments. Journal of Scheduling, 28(5):
513-527, 2025.

Jiawei Gu, Ziting Xian, Yuanzhen Xie, Ye Liu, Enjie Liu, Ruichao Zhong, Mochi Gao, Yunzhi Tan, Bo Hu,
and Zang Li. Toward structured knowledge reasoning: Contrastive retrieval-augmented generation
on experience. arXiv preprint arXiv:2506.00842, 2025.

Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.
rStar-Math: Small LLMs can master math reasoning with self-evolved deep thinking. arXiv preprint
arXiv:2501.04519, 2025.

Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training
(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong
Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement
learning. arXiv preprint arXiv:2501.12948, 2025.

Hongyi Guo, Zhihan Liu, Yufeng Zhang, and Zhaoran Wang. Can large language models play games?
a case study of a self-play approach. arXiv preprint arXiv:2403.05632, 2024.

57


[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

[76]

[77]

[78]

[79]

Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu
Yang. Evoprompt: Connecting Ilms with evolutionary algorithms yields powerful prompt optimizers,
2025. URLhttps://arxiv.org/abs/2309.08532.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning
with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali,
editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
pages 8154-8173, Singapore, December 2023. Association for Computational Linguistics. URL
https://aclanthology.org/2023.emnlp-main.507.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.
Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992,
2023.

Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi
Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of
step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.

Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian.
Training large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769,
2024.

Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of
minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100-107, 1968.

Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, and Bing Qin. Planning like human:
A dual-process framework for dialogue planning. arXiv preprint arXiv:2406.05374, 2024.

Nathan Herr, Tim Rocktaschel, and Roberta Raileanu. Llm-first search: Self-guided exploration of the
solution space. arXiv preprint arXiv:2506.05213, 2025.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-
optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric
Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,
Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal
large language models. In arXiv, 2022. URL https://arxiv.org/abs/2203.15556.

Zhipeng Hou, Junyi Tang, and Yipeng Wang. HALO: Hierarchical autonomous logic-oriented orches-
tration for multi-agent LLM systems. arXiv preprint arXiv:2505.13516, 2025.

Haichuan Hu, Congqing He, Hao Zhang, Xiaochen Xie, and Quanjun Zhang. APRMCTS: Improving
LLM-based automated program repair with iterative tree search. arXiv preprint arXiv:2507.01827,
2025.

Mengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wendi Shao, Qiguang
Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large
language models. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=Glcsog6z0e.

58


[80]

[81]

[82]

[83]

[84]

[85]

[86]

[87]

[88]

[89]

[90]

[91]
[92]

[93]

[94]

Ming Hu, Chenglong Ma, Wei Li, Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang,
Jiaqi Liu, Yingzhou Lu, et al. A survey of scientific large language models: From data foundations to
agent frontiers. arXiv preprint arXiv:2508.21148, 2025.

Yunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan. Mcts-rag: Enhancing retrieval-augmented
generation with monte carlo tree search. arXiv preprint arXiv:2503.20757, 2025.

Jiatan Huang, Mingchen Li, Zonghai Yao, Zhichao Yang, Yongkang Xiao, Feiyun Ouyang, Xiaohan Li,
Shuo Han, and Hong Yu. RiTeK: A dataset for large language models complex reasoning over textual
knowledge graphs. arXiv preprint arXiv:2410.13987, 2024.

Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei
Liu, and Xiaofan Zhang. O1 replication journey—part 3: Inference-time scaling for medical reasoning.
arXiv preprint arXiv:2501.06458, 2025.

HuggingFace. Open rl: A fully open reproduction of deepseek-r1, January 2025. URL https:
//github.com/huggingface/open-rl.

Wenyang Hui, Yan Wang, Kewei Tu, and Chengyue Jiang. Rot: Enhancing large language models
with reflection on search trees. arXiv preprint arXiv:2404.05449, 2024.

Yuichi Inoue, Kou Misaki, Yuki Imajuku, So Kuroki, Taishi Nakamura, and Takuya Akiba. Wider or
deeper? scaling LLM inference-time compute with adaptive branching tree search. arXiv preprint
arXiv:2503.04412, 2025.

Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and
Tao Zhang. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and
refinement. arXiv preprint arXiv:2412.12881, 2024.

Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, and Yu Wang. MedS?: Towards
medical small language models with self-evolved slow thinking. arXiv preprint arXiv:2501.12051,
2025.

Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei
Hong, Tong Che, and Dimitris N Metaxas. Your reward function for RL is your best PRM for search:
Unifying RL and search-based TTS. arXiv preprint arXiv:2508.14313, 2025.

Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, and Yoshua Bengio. Efficient causal
graph discovery using large language models. arXiv preprint arXiv:2402.01207, 2024.

D. Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011. ISBN 9781429969352.

Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan
He, Feng Wen, et al. Mindstar: Enhancing math reasoning in pre-trained LLMs at inference time.
arXiv preprint arXiv:2405.16265, 2024.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361, 2020.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. In arXiv,
2020. URL https://arxiv.org/abs/2001.08361.

59


[95]

[96]

[97]

[98]

[99]

[100]

[101]

[102]

[103]

[104]

[105]

[106]

[107]

[108]

[109]

[110]

Maxim Khanoy, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. arXiv
preprint arXiv:2402.01694, 2024.

Chaeeun Kim and Seungone Kim. FREESON: Retriever-free retrieval-augmented reasoning via corpus-
traversing MCTS. arXiv preprint arXiv:2505.16409, 2025.

Joongwon Kim, Anirudh Goyal, Liang Tan, Hannaneh Hajishirzi, Srinivasan Iyer, and Tianlu Wang.
ASTRO: Teaching language models to reason by reflecting and backtracking in-context. arXiv preprint
arXiv:2507.00417, 2025.

Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European Conference on
Machine Learning, 2006. URL https: //api.semanticscholar.org/CorpusID:15184765.

Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference on
machine learning, pages 282-293. Springer, 2006.

Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language
model agents. arXiv preprint arXiv:2407.01476, 2024.

Richard E Korf. Depth-first iterative-deepening: An optimal admissible tree search. Artificial intelligence,
27(1):97-109, 1985.

Richard E Korf and David Maxwell Chickering. Best-first minimax search: Othello results. In AAAI,
pages 1365-1370, 1994.

Jakub Kowalski, Mark HM Winands, Stanislaw Reda, Anna Wilbik, et al. Towards explaining monte-
carlo tree search by using its enhancements. arXiv preprint arXiv:2506.13223, 2025.

Stepan Kulibaba, Artem Dzhalilov, Roman Pakhomov, Oleg Svidchenko, Alexander Gasnikov, and
Aleksei Shpilman. KompeteAI: Accelerated autonomous multi-agent system for end-to-end pipeline
generation for machine learning problems. arXiv preprint arXiv:2508.10177, 2025.

Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, and Yuyu Luo. Alpha-sql: Zero-shot
text-to-sql using monte carlo tree search. arXiv preprint arXiv:2502.17248, 2025.

Chenglin Li, Qianglong Chen, Zhi Li, Feng Tao, Yicheng Li, Hao Chen, Fei Yu, and Yin Zhang.
Optimizing instruction synthesis: Effective exploration of evolutionary space with tree search. arXiv
preprint arXtv:2410.10392, 2024.

Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo: A training-free dynamic focus visual search
for enhancing LMMs in fine-grained visual understanding. In Proceedings of the Computer Vision and
Pattern Recognition Conference, pages 9098-9108, 2025.

Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Commu-
nicative agents for" mind" exploration of large language model society. Advances in Neural Information
Processing Systems, 36:51991-52008, 2023.

Han Li, Yuling Shi, Shaoxin Lin, Xiaodong Gu, Heng Lian, Xin Wang, Yantao Jia, Tao Huang, and
Qianxiang Wang. SWE-Debate: Competitive multi-agent debate for software issue resolution. arXiv
preprint arXiv:2507.23348, 2025.

Junyi Li and Hwee Tou Ng. Think&Cite: Improving attributed text generation with self-guided tree
search and progress reward modeling. arXiv preprint arXiv:2412.14860, 2024.

60


[111]

[112]

[113]

[114]

[115]

[116]

[117]

[118]

[119]

[120]

[121]

[122]

Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, and Weinan
Zhang. Rethinkmcts: Refining erroneous thoughts in monte carlo tree search for code generation.
arXiv preprint arXiv:2409.09584, 2024.

Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,
Ekin Akyiirek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu.
Pre-trained language models for interactive decision-making. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=FWMOYjFso-a.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making
language models better reasoners with step-aware verifier. In Anna Rogers, Jordan Boyd-Graber, and
Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 5315-5333, Toronto, Canada, July 2023. Association for
Computational Linguistics. doi: 10.18653/v1/2023.acl-long.291. URL https://aclanthology.
6rg/ 2023 .ael—Leng.291/.

Yu Li, Lehui Li, Zhihao Wu, Qingmin Liao, Jianye Hao, Kun Shao, Fengli Xu, and Yong Li. AgentSwift:
Efficient LLM agent design via value-guided hierarchical search. arXiv preprint arXiv:2506.06017,
2025.

Zhigen Li, Jianxiang Peng, Yanmeng Wang, Tianhao Shen, Minghui Zhang, Linxi Su, Shang Wu,
Yihang Wu, Yugian Wang, Ye Wang, et al. Planning with large language models for conversational
agents. arXiv preprint arXiv:2407.03884, 2024.

Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to
solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.

Ziyue Li, Yang Li, and Tianyi Zhou. Skip a layer or loop it? test-time depth adaptation of pretrained
LLMs. arXiv preprint arXiv:2507.07996, 2025.

Zujie Liang, Feng Wei, Wujiang Xu, Lin Chen, Yuxi Qian, and Xinhui Wu. I-MCTS: Enhancing agentic
AutoML via introspective monte carlo tree search. arXiv preprint arXiv:2502.14693, 2025.

Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, and
Ziniu Hu. Strategist: Learning strategic skills by LLMs via bi-level tree search. arXiv preprint
arXiv:2408.10635, 2024.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth
International Conference on Learning Representations, 2024. URL https: //openreview.net/
forum?id=v8LOpN6EOi.

Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang,
Binxing Jiao, Chen Hu, et al. SE-Agent: Self-evolution trajectory optimization in multi-step reasoning
with LLM-based agents. arXiv preprint arXiv:2508.02085, 2025.

61


[123]

[124]

[125]

[126]

[127]

[128]

[129]
[130]

[131]

[132]

[133]

[134]

[135]

[136]

Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, and Ruichu Cai. Leveraging constrained
monte carlo tree search to generate reliable long chain-of-thought for mathematical reasoning, 2025.

Chengyuan Liu, Shihang Wang, Lizhi Qing, Kaisong Song, Junjie Cao, Jun Lin, Ji Zhang, Ang Li,
Kun Kuang, and Fei Wu. Towards stepwise domain knowledge-driven reasoning optimization and
reflection improvement. arXiv preprint arXiv:2504.09058, 2025.

Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli
Celikyilmaz. Don’t throw away your value model! generating more preferable text with value-guided
monte-carlo tree search decoding. arXiv preprint arXiv:2309. 15028, 2023.

Puyuan Liu, Xiang Zhang, and Lili Mou. A character-level length-control algorithm for non-
autoregressive sentence summarization. Advances in Neural Information Processing Systems, 35:
29101-29112, 2022.

Shuhang Liu, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Jun Du, Qing Wang, Jianshu Zhang, Quan Liu,
Jianqing Gao, and Feng Ma. MMC: Iterative refinement of VLM reasoning via MCTS-based multimodal
critique. arXiv preprint arXiv:2504.11009, 2025.

Leonardo Locowic, Alessandro Monteverdi, and Eleazar Mendoza. Synthetic data generation from
real data sources using monte carlo tree search and large language models. Authorea Preprints, 2024.

Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291, 2023.

Hao Lu, Yanchi Gu, Haoyuan Huang, Yulin Zhou, Ningxin Zhu, and Chen Li. MCTSr-Zero: Self-
reflective psychological counseling dialogues generation via principles and adaptive exploration. arXiv
preprint arXtv:2505.23229, 2025.

Ximing Lu, Seungju Han, David Acuna, Hyunwoo Kim, Jaehun Jung, Shrimai Prabhumoye, Niklas
Muennighoff, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, et al. Retro-search: Exploring
untaken paths for deeper and efficient reasoning. arXiv preprint arXiv:2504.04383, 2025.

Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng
Li. Mathcoder2: Better math reasoning from continued pretraining on model-translated mathematical
code. arXiv preprint arXiv:2410.08196, 2024.

Haoran Luo, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu,
Luu Anh Tuan, et al. Kbqa-o1: Agentic knowledge base question answering with monte carlo tree
search. arXiv preprint arXiv:2501.18922, 2025.

Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu,
Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process
supervision. arXiv preprint arXiv:2406.06592, 2024.

Shuai Lyu, Haoran Luo, Ripeng Li, Zhonghong Ou, Jiangfeng Sun, Yang Qin, Xiaoran Shang, Meina
Song, and Yifan Zhu. SQL-o1: A self-reward heuristic dynamic search method for text-to-sql. arXiv
preprint arXtv:2502.11741, 2025.

Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang.
Let’s reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint
arXiv:2310.10080, 2023.

62


[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

[146]

[147]

[148]

Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian
Guo. Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledge-guided
retrieval augmented generation. In The Thirteenth International Conference on Learning Representations,
2025. URL https://openreview.net/forum?id=oFBu7qaZpsS.

Yiran Ma, Zui Chen, Tiangiao Liu, Mi Tian, Zhuo Liu, Zitao Liu, and Weiqi Luo. What are step-level
reward models rewarding? counterintuitive findings from mcts-boosted mathematical reasoning. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 24812-24820, 2025.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative
refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=S37hOerQLB.

Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, and Kai-Wei Chang. Llm-a*: Large language
model enhanced incremental heuristic search on path planning. arXiv preprint arXiv:2407.02511,
2024.

E.F. Moore. The Shortest Path Through a Maze. Bell Telephone System. Technical publications.
monograph. Bell Telephone System., 1959. URL https://books.google.ca/books?id=
IVZBHAAACAAJ.

Cyrus Neary, Omar G Younis, Artur Kuramshin, Ozgur Aslan, and Glen Berseth. Improving pre-trained
vision-language-action policies with model-based search. arXiv preprint arXiv:2508.12211, 2025.

Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I. Wang, and Xi Victoria
Lin. Lever: learning to verify language-to-code generation with execution. In Proceedings of the 40th
International Conference on Machine Learning, ICML’23. JMLR.org, 2023.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for
intermediate computation with language models. 2021.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan
Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh
Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing
Systems, 2022. URL https://openreview.net/forum?id=TG8KACXxEON.

Jianfeng Pan, Senyou Deng, and Shaomang Huang. Coat: Chain-of-associated-thoughts framework
for enhancing large language models reasoning. arXiv preprint arXiv:2502.02390, 2025.

Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H Vicky Zhao, Conghui
He, and Lijun Wu. Lemma: Learning from errors for mathematical advancement in LLMs. arXiv
preprint arXtv:2503.17439, 2025.

Shubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, and
Shuiwang Ji. Inference-time computations for Ilm reasoning and planning: A benchmark and insights,
2025, URL Bteps: //arxiv.org/abs/2502.12521.

63


[149]

[150]

[151]

[152]

[153]

[154]

[155]

[156]

[157]

[158]

[159]

[160]

[161]

[162]
[163]

Sungjin Park, Xiao Liu, Yeyun Gong, and Edward Choi. Ensembling large language models with
process reward-guided tree search for better complex reasoning. arXiv preprint arXiv:2412.15797,
2024.

Judea Pearl. Heuristics: intelligent search strategies for computer problem solving. Addison-Wesley
Longman Publishing Co., Inc., 1984.

Nicholas Pfaff, Hongkai Dai, Sergey Zakharov, Shun Iwase, and Russ Tedrake. Steerable scene
generation with post training and inference-time search. arXiv preprint arXiv:2505.04831, 2025.

Ira Pohl. Heuristic search viewed as path finding in a graph. Artificial intelligence, 1(3-4):193-204,
1970.

David L. Poole and Alan K. Mackworth. Artificial Intelligence: Foundations of Computational Agents.
Cambridge University Press, 3 edition, 2023.

Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction
search for prompting large language models, 2023. URL https://arxiv.org/abs/2203.07281.

Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt
optimization with “gradient descent” and beam search. In Houda Bouamor, Juan Pino, and Kalika Bali,
editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages
7957-7968, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/
v1/2023.emnlp-main.494. URL https: //aclanthology.org/2023.emnlp-main.494/.

Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and
Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous AI agents. arXiv preprint
arXiv:2408.07199, 2024.

Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning
makes smaller LLMs stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024.

Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector
Liu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report—part 1. arXiv preprint
arXiv:2410.18982, 2024.

Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language
model agents how to self-improve. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems, 2024. URL https://openreview.net/forum?id=DRC9pPZwBwR.

Yanwei Ren, Haotian Zhang, Fuxiang Wu, Jiayan Qiu, Jiaxing Huang, Baosheng Yu, and Liu Liu.
SIGMA: Refining large language model reasoning via sibling-guided monte carlo augmentation. arXiv
preprint arXiv:2506.06470, 2025.

Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach (4th Edition). Pearson,
2020. ISBN 9780134610993. URL http://aima.cs.berkeley.edu/.

Stuart J Russell and Peter Norvig. Artificial intelligence a modern approach. London, 2010.

Arthur L Samuel. Some studies in machine learning using the game of checkers. IBM Journal of
research and development, 3(3):210-229, 1959.

64


[164]

[165]

[166]

[167]

[168]

[169]

[170]

[171]

[172]

[173]

[174]

[175]

[176]

[177]

Robin Schmocker, Lennart Kampmann, and Alexander Dockhorn. Time-critical and confidence-based
abstraction dropping methods. arXiv preprint arXiv:2507.02703, 2025.

Julian Schrittwieser, loannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go,
chess and shogi by planning with a learned model. Nature, 588(7839):604—609, 2020.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh
Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers
for LLM reasoning. arXiv preprint arXiv:2410.08146, 2024.

Rajaram Sharma. Revisiting the role of ct evaluation in persistent truncus arteriosus. International
Journal of Cardiovascular Medicine, 2(6), 2023.

Haoyuan Shi, Yunxin Li, Xinyu Chen, Longyue Wang, Baotian Hu, and Min Zhang. AniMaker:
Automated multi-agent animated storytelling with MCTS-driven clip generation. arXiv preprint
arXiv:2506.10540, 2025.

Wentao Shi, Zichun Yu, Fuli Feng, Xiangnan He, and Chenyan Xiong. Efficient multi-agent system
training with data influence-oriented tree search. arXiv preprint arXiv:2502.00955, 2025.

Zijing Shi, Meng Fang, and Ling Chen. Monte carlo planning with large language model for text-based
game agents. arXiv preprint arXiv:2504.16855, 2025.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354-359, 2017.

Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, and Eric P Xing. Dynamic
rewarding with prompt optimization enables tuning-free self-alignment of language models. arXiv
preprint arXiv:241 1.08733, 2024.

Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling test-time compute optimally
can be more effective than scaling LLM parameters. In The Thirteenth International Conference on
Learning Representations, 2025. URL https: //openreview.net/forum?id=4FWAwZtd2n.

Henry W Sprueill, Carl Edwards, Mariefel V Olarte, Udishnu Sanyal, Heng Ji, and Sutanay Choudhury.
Monte carlo thought search: Large language model querying for complex scientific reasoning in
catalyst design. arXiv preprint arXiv:2310.14420, 2023.

Volker Steinbiss, Bach-Hiep Tran, and Hermann Ney. Improvements in beam search. In ICSLP,
volume 94, pages 2143-2146, 1994.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in
neural information processing systems, 33:3008-3021, 2020.

65


[178]

[179]

[180]
[181]

[182]

[183]

[184]

[185]

[186]

[187]

[188]

[189]

[190]

[191]

[192]

Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-
Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model
on knowledge graph. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=nnVO1PvbTv.

Yuni Susanti and Michael Farber. Can Ilms leverage observational data? towards data-driven causal
discovery with llms. arXiv preprint arXiv:2504.10936, 2025.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Sophia Tang, Yinuo Zhang, and Pranam Chatterjee. Peptune: De novo generation of therapeutic
peptides with multi-objective-guided discrete diffusion. ArXiv, pages arXiv—2412, 2025.

Robert Tarjan. Depth-first search and linear graph algorithms. SIAM journal on computing, 1(2):
146-160, 1972.

Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-
improvement of Ilms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253,
2024.

Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the
26th annual international conference on machine learning, pages 1049-1056, 2009.

Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, and Hong Yu. RARE: Retrieval-
augmented reasoning enhancement for large language models. arXiv preprint arXiv:2412.02830,
2024.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-
based feedback. arXiv preprint arXiv:2211.14275, 2022.

Teon Volkova, Evander Delacruz, and Thaddeus Cavanaugh. A novel approach to optimize large
language models for named entity matching with monte carlo tree search. Authorea Preprints, 2024.

Ziyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-
search can guide large language model decoding and training, 2024. URL https: //openreview.
net/forum?id=fLO9VaAb3B.

Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu.
Litesearch: Efficacious tree search for LLM. arXiv preprint arXiv:2407.00320, 2024.

Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*:
Improving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406. 14283,
2024.

Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean
Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code
generation. arXiv preprint arXiv:2409.03733, 2024.

He Wang and Liang Zeng. Automated algorithmic discovery for gravitational-wave detection guided
by LLM-informed evolutionary monte carlo tree search. arXiv preprint arXiv:2508.03661, 2025.

66


[193]

[194]

[195]

[196]

[197]

[198]

[199]

[200]

[201]

[202]

[203]

[204]

[205]

Hui Wang, Xufeng Zhang, and Chaoxu Mu. Planning of heuristics: Strategic planning on large
language models with monte carlo tree search for automating heuristic optimization. arXiv preprint
arXiv:2502.11422, 2025.

Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.
Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 9426-9439, 2024.

Teng Wang, Zhangyi Jiang, Zhenqi He, Shenyang Tong, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan
He, and Hailei Gong. Towards hierarchical multi-step reward models for enhanced reasoning in large
language models. arXiv preprint arXiv:2503.13551, 2025.

Tianlong Wang, Junzhe Chen, Xueting Han, and Jing Bai. CPL: Critical plan step learning boosts LLM
generalization in reasoning tasks. arXiv preprint arXiv:2409.08642, 2024.

Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing,
and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt
optimization. arXiv preprint arXiv:2310.16427, 2023.

Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, and Dong
Yu. Towards self-improvement of LLMs via MCTS: Leveraging stepwise knowledge with curriculum
preference learning. arXiv preprint arXiv:2410.06508, 2024.

Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong
Huang, and Lijuan Wang. Sota with less: MCTS-guided sample selection for data-efficient visual
reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=1PL1NIMMrw.

Yibo Wang, Zhihao Peng, Ying Wang, Zhao Wei, Hai Yu, and Zhiliang Zhu. MCTS-refined CoT: High-
quality fine-tuning data for LLM-based repository issue resolution. arXiv preprint arXiv:2506.12728,
2025.

Yingxu Wang, Shiqi Fan, Mengzhu Wang, and Siwei Liu. Dynamically adaptive reasoning via LLM-
guided MCTS for efficient and context-aware KGQA. arXiv preprint arXiv:2508.00719, 2025.

Zhun Wang, Vincent Siu, Zhe Ye, Tianneng Shi, Yuzhou Nie, Xuandong Zhao, Chenguang Wang,
Wenbo Guo, and Dawn Song. Agentxploit: End-to-end redteaming of black-box AI agents, 2025.

Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. Multi-
step problem solving through a verifier: An empirical analysis on model-induced process supervision.
arXiv preprint arXiv:2402.02658, 2024.

Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, and Feifei Li. Corag: A cost-constrained retrieval
optimization system for retrieval-augmented generation. arXiv preprint arXiv:2411.00744, 2024.

67


[206]

[207]

[208]

[209]

[210]

[211]

[212]

[213]

[214]

[215]

[216]

[217]

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V

Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large lan-
guage models. Advances in neural information processing systems, 35:24824-24837,
2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/

9d5609613524ec£4E15af0F7b3labca4-Abstract—-Conference.html.

Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou,
Guangshuai Wang, Zhiqiang Gao, Juntai Cao, et al. From ai for science to agentic science: A survey
on autonomous scientific discovery. arXiv preprint arXiv:2508.14111, 2025.

Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi
Sun. Alignrag: Leveraging critique learning for evidence-sensitive retrieval-augmented reasoning.
arXiv preprint arXiv:2504.14858, 2025.

Yifan Wei, Xiaoyan Yu, Tengfei Pan, Angsheng Li, and Li Du. Structural entropy guided agent for
detecting and repairing knowledge deficiencies in LLMs. arXiv preprint arXiv:2505.07184, 2025.

Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard
prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery, 2023.
URL https://arxiv.org/abs/2302.03668.

Chung-Wen Wu, Guan-Tang Huang, Yue-Yang He, and Berlin Chen. KNOT-MCTS: An effective
approach to addressing hallucinations in generative language modeling for question answering. In
Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023),
pages 215-221, 2023.

Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, and Jianhua Tao. Beyond
examples: High-level automated reasoning paradigm in in-context learning via mcts. arXiv preprint
arXiv:2411.18478, 2024.

Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che, Zengqi Wen, and Jianhua Tao.
Boosting multimodal reasoning with MCTS-automated structured thinking, 2025.

Mengsong Wu, YaFei Wang, Yidong Ming, Yuqi An, Yuwei Wan, Wenliang Chen, Binbin Lin, Yuqiang
Li, Tong Xie, and Dongzhan Zhou. ChemAgent: Enhancing LLMs for chemistry and materials science
through tree-search based tool learning. arXiv preprint arXiv:2506.07551, 2025.

Mengsong Wu, Di Zhang, Yuqiang Li, Dongzhan Zhou, and Wenliang Chen. SELT: Self-evaluation
tree search for LLMs with task decomposition. arXiv preprint arXiv:2506.07557, 2025.

Suhuang Wu, Huimin Wang, Yutian Zhao, Xian Wu, Yefeng Zheng, Wei Li, Hui Li, and Rongrong
Ji. Monte Carlo tree search based prompt autogeneration for jailbreak attacks against LLMs. In
Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven
Schockaert, editors, Proceedings of the 31st International Conference on Computational Linguistics,
pages 1057-1068, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL
https] /aclankhelegy .erdg/2025. a0 Ling-main. TL/.

Tao Wu, Jingyuan Chen, Wang Lin, Jian Zhan, Mengze Li, Kun Kuang, and Fei Wu. Personalized
distractor generation via MCTS-guided reasoning reconstruction. arXiv preprint arXiv:2508.11184,
2025.

68


[218]

[219]

[220]

[221]

[222]

[223]

[224]

[225]

[226]

[227]

[228]

[229]

[230]

Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference com-
putation: Compute-optimal inference for problem-solving with language models. In Workshop on
Mathematical Reasoning and AI at NeurIPS’24, 2024. URL https: //openreview.net/forum?
id= j7DZWSc8qu.

Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung,
Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in LLMs: Learning
how to think with meta chain-of-thought. arXiv preprint arXiv:2501.04682, 2025.

Yuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Dongmei Jiang, and
Liqiang Nie. Mirage-1: Augmenting and updating GUI agent with hierarchical multimodal skills. arXiv
preprint arXiv:2506. 10387, 2025.

Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Self-
evaluation guided beam search for reasoning. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023. URL https: //openreview.net/forum?id=Bw82hwg5Q3.

Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi, and
Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv
preprint arXiv:2405.00451, 2024.

Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai
Shen. Bfs-prover: Scalable best-first tree search for LLM-based automatic theorem proving. arXiv
preprint arXiv:2502.03438, 2025.

Fei Xiong, Xiang Zhang, Aosong Feng, Siqi Sun, and Chenyu You. Quantagent: Price-driven multi-agent
Ilms for high-frequency trading. arXiv preprint arXiv:2509.09995, 2025.

Guanming Xiong, Haochen Li, and Wen Zhao. MCTS-KBQA: Monte carlo tree search for knowledge
base question answering. arXiv preprint arXiv:2502. 13428, 2025.

Bin Xu, Yiguan Lin, Yinghao Li, and Yang Gao. SRA-MCTS: Self-driven reasoning augmentation with
monte carlo tree search for code generation. arXiv preprint arXiv:2411.11053, 2024.

Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. GPS:
Genetic prompt search for efficient few-shot learning. In Yoav Goldberg, Zornitsa Kozareva, and Yue
Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
pages 8162-8171, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.emnlp-main.559. URL https://aclanthology.org/2022.
emnlp-main.559/.

Haotian Xu. No train still gain. unleash mathematical reasoning of large language models with monte
carlo tree search guided by energy function. arXiv preprint arXiv:2309.03224, 2023.

Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. Search-in-the-chain: Inter-
actively enhancing large language models with search for knowledge-intensive tasks. In Proceedings
of the ACM Web Conference 2024, pages 1362-1373, 2024.

Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan,
Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint
arXiv:2309.10305, 2023.

69


[231]

[232]

[233]

[234]

[235]

[236]

[237]

[238]

[239]

[240]

[241]

[242]

[243]

[244]

Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J
Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented lan-
guage models. Advances in Neural Information Processing Systems, 36:21573-21612, 2023.

Qi Yang, Chenghao Zhang, Lubin Fan, Kun Ding, Jieping Ye, and Shiming Xiang. Re-ranking reasoning
context with tree search makes large vision-language models stronger. arXiv preprint arXiv:2506.07785,
2025.

Sen Yang, Yafu Li, Wai Lam, and Yu Cheng. Multi-LLM collaborative search for complex problem
solving. arXiv preprint arXiv:2502.18873, 2025.

Wen Yang, Minpeng Liao, and Kai Fan. Markov chain of thought for efficient mathematical reasoning.
arXiv preprint arXiv:2410.17635, 2024.

Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. TreeRPO:
Tree relative policy optimization. arXiv preprint arXiv:2506.05183, 2025.

Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin
Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering MLLM with o1-like reasoning and
reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024.

Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, and Tao Lin. DeFT:
Decoding with flash tree-attention for efficient tree-structured LLM inference. In The Thirteenth
International Conference on Learning Representations, 2025. URL https: //openreview.net/
forum?id=2c7pfOquY9k.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: deliberate problem solving with large language models. In Proceedings
of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, Red Hook, NY,
USA, 2024. Curran Associates Inc.

Hai Ye, Mingbao Lin, Hwee Tou Ng, and Shuicheng Yan. Multi-agent sampling: Scaling inference com-
pute for data synthesis with tree search-based agentic collaboration. arXiv preprint arXiv:2412.17061,
2024.

Ruimeng Ye, Zihan Wang, Xiao Yang, Zinan Ling, Manling Li, and Bo Hui. Weak-to-strong generaliza-
tion with failure trajectories: A tree-based approach to elicit optimal policy in strong models. arXiv
preprint arXtv:2507. 18858, 2025.

Huifeng Yin, Yu Zhao, Minghao Wu, Xuanfan Ni, Bo Zeng, Hao Wang, Tianqi Shi, Liangying Shao,
Chenyang Lyu, Longyue Wang, et al. Towards widening the distillation bottleneck for reasoning
models, 2025.

Chenyu You, Haocheng Dai, Yifei Min, Jasjeet S Sekhon, Sarang Joshi, and James S Duncan. Uncovering
memorization effect in the presence of spurious correlations. Nature Communications, 16(1):5424,
2025.

Fei Yu, Anningzhe Gao, and Benyou Wang. OVM, outcome-supervised value models for planning in
mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023.

70


[245]

[246]

[247]

[248]

[249]

[250]

[251]

[252]

[253]

[254]

[255]

[256]

[257]

[258]

Fei Xu Yu, Gina Adam, Nathaniel D Bastian, and Tian Lan. Optimizing prompt sequences using monte
carlo tree search for LLM-based optimization. arXiv preprint arXiv:2508.05995, 2025.

Linhao Yu, Xinguang Ji, Yahui Liu, Fanheng Kong, Chenxi Sun, Jingyuan Zhang, Hongzhi Zhang,
Fuzheng Zhang, Deyi Xiong, et al. Evaluating multimodal large language models on video captioning
via monte carlo tree search. arXiv preprint arXiv:2506.11155, 2025.

Liyang Yu, Tianyi Wang, Junfeng Jiao, Fengwu Shan, Hongqing Chu, and Bingzhao Gao. BIDA: A
bi-level interaction decision-making algorithm for autonomous vehicles in dynamic traffic scenarios.
In 2025 IEEE Intelligent Vehicles Symposium (IV), pages 1209-1214. IEEE, 2025.

Xiao Yu, Maximillian Chen, and Zhou Yu. Prompt-based monte-carlo tree search for goal-oriented
dialogue policy planning. arXiv preprint arXiv:2305.13660, 2023.

Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu.
Improving autonomous AI agents with reflective tree search and self-learning, 2024.

Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-R: Training
language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425, 2025.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and
Jason E Weston. Self-rewarding language models. In Forty-first International Conference on Machine
Learning, 2024.

Khadija Zanna and Akane Sano. Uncovering bias paths with llm-guided causal discovery: An active
learning and dynamic scoring approach. arXiv preprint arXiv:2506.12227, 2025.

Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo,
Xuanjing Huang, and Xipeng Qiu. Scaling of search and learning: A roadmap to reproduce 01 from
reinforcement learning perspective. arXiv preprint arXiv:2412.14135, 2024.

Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo,
Xuanjing Huang, and Xipeng Qiu. Scaling of search and learning: A roadmap to reproduce 01 from
reinforcement learning perspective, 2024. URL https://arxiv.org/abs/2412.14135.

Yuanzhao Zhai, Tingkai Yang, Kele Xu, Dawei Feng, Cheng Yang, Bo Ding, and Huaimin Wang.
Enhancing decision-making for LLM agents via step-level q-value models. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 39, pages 27161-27169, 2025.

Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao,
Dahua Lin, and Jiaqi Wang. Booststep: Boosting mathematical capability of large language models
via improved single-step reasoning. arXiv preprint arXiv:2501.03226, 2025.

Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm
self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024.

Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*:
LLM self-training via process reward guided tree search. In The Thirty-eighth Annual Conference
on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=
8rcFOgEud5.

71


[259]

[260]

[261]

[262]

[263]

[264]

[265]

[266]

[267]

[268]

[269]

[270]

[271]

Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yugiang Li, and Wanli Ouyang. Accessing gpt-4 level
mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint
arXiv:2406.07394, 2024.

Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang,
Marco Pavone, Yugiang Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level
mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024.

Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang,
Marco Pavone, Yuqiang Li, et al. LLaMA-Berry: Pairwise optimization for olympiad-level mathematical
reasoning via Ol-like monte carlo tree search. In Proceedings of the 2025 Conference of the Nations of
the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies
(Volume 1: Long Papers), pages 7315-7337, 2025.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan.
Planning with large language models for code generation. arXiv preprint arXiv:2303.05510, 2023.

Tao Zhang, Jia-Shu Pan, Ruiqi Feng, and Tailin Wu. T-SCEND: Test-time scalable MCTS-enhanced
diffusion model, 2025.

Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. Don’t trust chatgpt when
your question is not in english: a study of multilingual abilities and types of Ilms. arXiv preprint
arXiv:2305.16339, 2023.

Xiang Zhang, Ning Shi, Bradley Hauer, and Grzegorz Kondrak. Bridging the gap between babelnet and
hownet: Unsupervised sense alignment and sememe prediction. In Proceedings of the 17th Conference
of the European Chapter of the Association for Computational Linguistics, pages 2789-2798, 2023.

Xiang Zhang, Muhammad Abdul-Mageed, and Laks VS Lakshmanan. Autoregressive+ chain of
thought= recurrent: Recurrence’s role in language models’ computability and a revisit of recurrent
transformer. arXiv preprint arXiv:2409.09239, 2024.

Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, and Chenyu You. Tokenization constraints in lms: A
study of symbolic and arithmetic reasoning limits. arXiv preprint arXiv:2505.14178, 2025.

Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, and Dujian Ding. Why prompt design matters and
works: A complexity analysis of prompt search space in Ilms. arXiv preprint arXiv:2503. 10084, 2025.

Xiang Zhang, Tianze Ling, Zhi Jin, Sheng Xu, Zhiqiang Gao, Boyan Sun, Zijie Qiu, Jiaqi Wei, Nanqing
Dong, Guangshuai Wang, et al. 7-primenovo: an accurate and efficient non-autoregressive deep
learning model for de novo peptide sequencing. Nature Communications, 16(1):267, 2025.

Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization:
Improving chain-of-thought reasoning in LLMs. In The Thirty-eighth Annual Conference on Neural In-
formation Processing Systems, 2024. URL https: //openreview.net/forum?id=2cczgOfMP4.

Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and
autonomous multi-agent system for web task execution with strategic exploration. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 39, pages 23378-23386, 2025.

72


[272]

[273]

[274]

[275]

[276]

[277]

[278]

[279]

[280]

[281]

[282]

[283]

[284]

Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong
Yan, Yinpeng Dong, and Jun Zhu. Stair: Improving safety alignment with introspective reasoning.
arXiv preprint arXiv:2502.02384, 2025.

Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang.
ol-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024.

Zheyu Zhang, Zhuorui Ye, Yikang Shen, and Chuang Gan. Autonomous tree-search ability of large
language models. arXiv preprint arXiv:2310.10686, 2023.

Zhilin Zhang, Xiang Zhang, Jiaqi Wei, Yiwei Xu, and Chenyu You. Postergen: Aesthetic-aware
paper-to-poster generation via multi-agent Ilms. arXiv preprint arXiv:2508.17188, 2025.

Haokun Zhao, Xiang Zhang, Jiaqi Wei, Yiwei Xu, Yuting He, Siqi Sun, and Chenyu You. Time-
seriesscientist: A general-purpose ai agent for time series analysis. arXiv preprint arXiv:2510.01538,
2025.

Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,
and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint
arXiv:2411.14405, 2024.

Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for
large-scale task planning. Advances in neural information processing systems, 36:31967-31987, 2023.

Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, and Bryan Hooi. Monte carlo tree search for comprehensive
exploration in LLM-based automatic heuristic design. arXiv preprint arXiv:2501.08603, 2025.

Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Lan-
guage agent tree search unifies reasoning acting and planning in language models. arXiv preprint
arXiv:2310.04406, 2023.

Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language
agent tree search unifies reasoning acting and planning in language models, 2024. URL https:
//openreview.net/forum?id=6LNTSrdjjBe.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and
Jimmy Ba. Large language models are human-level prompt engineers, 2023. URL https://arxiv.
org/abs/2211.01910.

Xiang Zhuang, Bin Wu, Jiyu Cui, Kehua Feng, Xiaotong Li, Huabin Xing, Keyan Ding, Qiang Zhang, and
Huajun Chen. Boosting LLM’s molecular structure elucidation with knowledge enhanced tree search
reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar,
editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pages 22561-22576, Vienna, Austria, July 2025. Association for Computational
Linguistics. ISBN 979-8-89176-251-0. URL https://aclanthology.org/2025.acl-long.
1100/7;

Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel,
and Chao Zhang. Toolchain*: Efficient action space navigation in large language models with A*
search. arXiv preprint arXiv:2310.13227, 2023.

73


[285] Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, and Haitham Bou Am-
mar. Bourbaki: Self-generated and goal-conditioned MDPs for theorem proving. arXiv preprint
arXiv:2507.02726, 2025.

[286] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal
and transferable adversarial attacks on aligned language models, 2023. URLhttps://arxiv.org/
abs/2307.15043.

[287] Wei Zou, Sen Yang, Yu Bao, Shujian Huang, Jiajun Chen, and Shanbo Cheng. Trans-Zero: Self-play
incentivizes large language models for multilingual translation without parallel data. arXiv preprint
arXiv:2504.14669, 2025.

74
