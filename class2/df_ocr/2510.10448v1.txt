arXiv:2510.10448v1 [cs.CL] 12 Oct 2025

RECON: Reasoning with Condensation for Efficient
Retrieval-Augmented Generation

Zhichao Xu!*, Minheng Wang?*, Yawei Wang*, Wengqian Ye*,
Yuntao Du*, Yunpu Ma’, Yijun Tian’
' University of Utah ? University of Washington * George Washington University
* University of Virginia > Shandong University
® Ludwig Maximilian University of Munich ’ University of Notre Dame

zhichao. xu@utah. edu

Abstract

Retrieval-augmented generation (RAG) sys-
tems trained using reinforcement learning (RL)
with reasoning are hampered by inefficient con-
text management, where long, noisy retrieved
documents increase costs and degrade perfor-
mance. We introduce RECON (REasoning
with CONdensation), a framework that inte-
grates an explicit summarization module to
compress evidence within the reasoning loop.
Our summarizer is trained via a two-stage pro-
cess: relevance pretraining on QA datasets, fol-
lowed by multi-aspect distillation from propri-
etary LLMs to ensure factuality and clarity. In-
tegrated into the Search-R1 pipeline, RECON
reduces total context length by 35%, leading
to improved training speed and inference la-
tency, while simultaneously improving RAG
performance on downstream QA benchmarks.
Notably, it boosts the average EM score of
the 3B model by 14.5% and the 7B model by
3.0%, showing particular strength in multi-hop
QA. RECON demonstrates that learned context
compression is essential for building practical,
scalable, and performant RAG systems. Our
code implementation is made available’.

1 Introduction

Reinforcement learning (RL, Sutton et al., 1998)
has recently emerged as a powerful approach
for improving the reasoning capabilities of large
language models (LLMs) (Jaech et al., 2024;
Guo et al., 2025). As LLMs are increasingly
deployed in domains such as biomedical ques-
tion answering and scientific discovery (Mallinar
et al., 2025; Gottweis et al., 2025), their effec-
tiveness often hinges on the ability to integrate
evidence across distributed sources. By inter-
leaving text generation with retrieval, retrieval-
augmented generation (RAG) systems can itera-
tively acquire and synthesize evidence to support

“Equal Contribution
‘https ://github.com/allfornancy/RECON

meetyijun@gmail.com

complex decision-making (Lewis et al., 2020a;
Trivedi et al., 2023). Knowledge-intensive tasks
are thus a natural proving ground: solving them
requires chaining and reconciling information from
multiple documents rather than relying solely on
parametric memory (Mallen et al., 2023; Asai et al.,
2024). Recent work has demonstrated this poten-
tial, with methods such as ReSearch (Chen et al.,
2025), R1-Searcher (Song et al., 2025), DeepRe-
searcher (Zheng et al., 2025) and Search-R1 (Jin
et al., 2025c,b) showing that training RAG agents
with reasoning-aware RL substantially improves
performance on knowledge-intensive benchmarks
that require complex, multi-turn reasoning.

However, RL-based RAG systems face persis-
tent efficiency challenges. Retrieved documents
are often verbose, redundant, or only loosely re-
lated to the query. Passing them directly into
the model inflates context length, drives up to-
ken costs (Jin et al., 2025a,c), and can even de-
grade accuracy (Sun et al., 2025; Chang et al.,
2025). These inefficiencies manifest at two lev-
els: (1) Cost efficiency: longer contexts increase
token usage, latency, and serving costs; (2) Cogni-
tive efficiency: diluted or noisy evidence weakens
performance (Liu et al., 2024; Levy et al., 2025)
and raises hallucination risk (Huang et al., 2025).
In multi-turn interaction settings, these problems
compound as context accumulates across retrieval
steps, posing memory challenges in training and
inference (Cao et al., 2025; Feng et al., 2025).

We address these challenges with a general
framework for learned context compression in RL-
based RAG systems. At its core is an explicit sum-
marizer that condenses retrieved documents into
concise, human-readable text, filtering out irrele-
vant or repetitive content while preserving essen-
tial evidence. Unlike prior RAG summarization ap-
proaches which operate as a postprocessing step for
single-pass retrieval (Xu et al., 2024), our frame-
work applies summarization after each retrieval


Rollout Module

Search Engine

Policy Model

Value Model
Reward Model
Reference Model

Figure 1: Training pipeline of our method. In the rollout module, instead of directly using retrieval results from the
search engine (dashed line), an additional summarization model is used to condense the retrieved information and
remove noises from document sources. This way we reduce the context length and achieve efficient and effective
rollout in both training and inference. Refer to § C for algorithmic details.

step but keeps it separate from the RL policy. This
disentangled design provides the policy with con-
densed yet human-readable evidence at every step,
reducing token consumption and keeping a trans-
parent reasoning chain (Lanham et al., 2023; Baker
et al., 2025; Korbak et al., 2025).

In this work, we instantiate the framework
within the Search-R1 pipeline and denote the re-
sulting method as REasoning with CONdensation
(RECON), where “condensation” highlights the
learned compression of retrieved evidence to sup-
port multi-turn reasoning. The summarizer is
trained in two stages: first, via relevance pretrain-
ing on MS MARCO (Bajaj et al., 2016) to discrim-
inate useful from non-useful documents; then, via
distillation of multi-aspect summaries from GPT-
4o0-mini (Hurst et al., 2024), aligning the model
with human-preferred properties such as factuality,
completeness, and clarity (Pagnoni et al., 2021; Liu
et al., 2023; Ryu et al., 2024).

Extensive experiments across seven public QA
benchmarks confirm that RECON effectively im-
proves accuracy, with especially strong gains in
multi-hop reasoning. Further, RECON achieves
faster training speed (5.2%) and lower inference la-
tency (30.9%) measured in wall-clock time, demon-
strating that active context compression is critical
toward practical, RL-augmented RAG systems.

2 Related Works

RAG and Agentic RAG. Retrieval-Augmented
Generation (RAG, Lewis et al., 2020b) enhances
large language models (LLMs) by retrieving rel-
evant document chunks from external knowledge
bases (Xu et al., 2025). RAG helps mitigate hal-
lucination by grounding generation in retrieved
content, and has become a foundational technique
in developing chatbots and real-world LLM ap-

plications (Gao et al., 2023b). Beyond the stan-
dard retrieve-then-generate pipeline (often referred
to as Naive RAG), numerous studies have pro-
posed techniques to improve performance and ef-
ficiency. IRCoT (Trivedi et al., 2023) interleaves
retrieval with steps in chain-of-thought prompting,
enabling more effective reasoning for knowledge-
intensive tasks that require multi-hop inference.
Self-RAG (Asai et al., 2024) introduces super-
vised fine-tuning to train the generator LLM to au-
tonomously decide whether to retrieve or generate
at each step. Leveraging large-scale synthetic data,
Self-RAG achieves state-of-the-art performance
across various QA and long-form generation bench-
marks (Gao et al., 2023a; Min et al., 2023).

Motivated by recent advances in reasoning-
aware reinforcement learning (Guo et al., 2024;
Hurst et al., 2024; Guo et al., 2025), several concur-
rent works explore training RAG agents via rein-
forcement learning (Chen et al., 2025; Song et al.,
2025; Zheng et al., 2025; Jin et al., 2025c). Among
these, we focus on Search-R1 (Jin et al., 2025c).
However, RECON’s plug-and-play design allows
it to be easily integrated into other agentic RAG
frameworks as well.

Context Compression for RAG. A core chal-
lenge in RAG is managing the limited context win-
dow of generator LLMs (Jin et al., 2025a). Re-
trieved documents are often long, noisy, and may
not contain information directly relevant to the
query. While increasing the number of retrieved
passages can improve recall, it also lengthens the
input, which may degrade generation quality (Liu
et al., 2024) and increase inference costs. This chal-
lenge becomes even more pronounced in agentic
RAG systems trained with reinforcement learning,
where multi-turn retriever-generator interactions


cause context length to grow rapidly, making RL
training both inefficient and costly.

To address this challenge, prior works have pro-
posed a range of context compression techniques.
MAIN-RAG (Chang et al., 2025) uses a separate
judge model to filter out irrelevant documents.
Other methods train the generator LLM to be more
robust to noisy or irrelevant context (Yoran et al.,
2024; Fang et al., 2024). A separate line of research
focuses on teaching language models to compress
their own context (Rae et al., 2020; Chevalier et al.,
2023). The most closely related work to RECON
is RECOMP (Xu et al., 2024), which trains a dedi-
cated summarization module for language model-
ing and QA tasks. A concurrent work ReSum (Wu
et al., 2025) focuses on leveraging performant sum-
marizer for long-horizon web browsing deep re-
search agent. However, our work is among the first
ones to apply explicit summarization-based context
compression in an RL-based agentic RAG setting.
By integrating a trained summarizer into the agent’s
decision loop, we demonstrate improved perfor-
mance while significantly reducing both training
and inference costs.

3 Proposed Method

Fig. | illustrates the training pipeline of RECON.
The central idea is to compress retrieved documents
into concise summaries before passing them to the
policy model, thereby reducing context length and
improving efficiency during both training and in-
ference. We achieve this goal by introducing a
dedicated summarizer, trained in two stages, and
integrating it seamlessly into an RL-based RAG
system built on the Search-R1 framework.

3.1 Overview

Our framework consists of two main components:
(1) a summarizer trained to filter and condense
noisy retrieval results, and (2) an integration strat-
egy that replaces raw document concatenation with
summaries inside the Search-R1 pipeline. Together,
these components enable efficient multi-turn re-
trieval and reasoning, while keeping context length
under control.

3.2. Summarizer Training

The summarizer is trained in two stages. The first
stage provides relevance detection ability, while
the second aligns the model with high-quality sum-
marization preferences distilled from a stronger
teacher model.

Stage 1: Supervised fine-tuning on MS MARCO.
Given a query x; and ten candidate passages D; =
{di1,...,dj19}, the task is to predict the relevant
passage. In MS MARCO question answering
dataset (Bajaj et al., 2016), at most one passage is
labeled as relevant. We initialize with a lightweight
model M, (Qwen2.5-3B-Instruct) with a linear
classification head to score each dj; conditioned on
xj, trained using a cross-entropy loss. This equips
the summarizer with a strong relevance prior for
later generative training.

Stage 2: Distillation from GPT-40-mini. We fur-
ther fine-tune the summarizer using multi-aspect
summaries generated by GPT-40-mini. Training
data is drawn from NQ and HotpotQA queries,
consistent with RECON ’s training distribution to
avoid leakage. For each source question x;, we
collect query sets X; and associated document sets
D; from Search-R1’s rollouts. We prompt GPT-
4o0-mini for a multi-document query-focused sum-
mary (Xu and Lapata, 2020) under six guiding as-
pects: factual correctness, completeness, cover-
age, coherence, clarity, and logicality (prompt tem-
plates in § B.3). Finally, we obtain 468k HotpotQA
and 1.0M NQ summaries, forming the distillation
mixture (more details in § B.2).

The summarizer, initialized from Stage 1 with
the classification head discarded, is trained to repro-
duce the teacher outputs via teacher forcing at the
logit level. This distillation stage transfers multi-
aspect supervision into a smaller, efficient model
suitable for integration into RL-based RAG.

3.3 Integrating the RECON Framework

We implement RECON by augmenting the Search-
R1 pipeline with an explicit summarization step. In
the original Search-R1 system, retrieved documents
are concatenated into the context after each search
turn. In RECON, retrieved documents are instead
routed through the summarizer, which condenses
them into concise yet human-readable summaries
before they are passed to the policy model (Fig. 1).
For experiments, we primarily use the clarity set-
ting, which produces concise but faithful evidence
that helps the agent reason more effectively.

A key feature of our proposed framework is its
flexibility: the summarizer can also operate under
other trained settings (e.g., factuality, complete-
ness) without retraining, making RECON easily
adaptable to a variety of RAG tasks.

Training. We train the policy model on a mixture
of NQ and HotpotQA using the same hyperparam-


Table 1: Performance comparison of different methods on selected QA datasets. denotes in-domain trainset and *
denotes OOD dataset. Best overall numbers on each dataset are bold, best in-category numbers are underlined.

Methods General QA Multi-Hop QA Ave,
NQ' TriviaQA* PopQA* HotpotQA' 2wiki* Musique* Bamboogle*
Baseline methods
Direct Inference 0.134 0.408 0.140 0.183 0.250 0.031 0.120 0.181
CoT 0.048 0.185 0.054 0.092 0.111 0.022 0.232 0.106
IRCoT 0.224 0.478 0.301 0.133 0.149 0.072 0.224 0.239
Search-ol 0.151 0.443 0.131 0.187 0.176 0.058 0.296 0.206
RAG 0.349 0.585 0.392 0.299 0.235 0.058 0.208 0.304
SFT 0.318 0.354 0.121 0.217 0.259 0.066 0.112 0.207
R1-base 0.297 0.539 0.202 0.242 0.273 0.083 0.296 0.276
R1-instruct 0.270 0.537 0.199 0.237 0.292 0.072 0.293 0.271
Rejection Sampling 0.360 0.592 0.380 0.331 0.296 0.123 0.355 0.348
Qwen2.5-3B-Base + PPO
Search-R1 0.406 0.587 0.435 0.284 0.273 0.049 0.088 0.303
RECON 0.440 0.612 0.425 0.326 0.319 0.082 0.226 0.347
Qwen2.5-7B-Base + PPO
Search-R1 0.480 0.638 0.457 0.433 0.382 0.196 0.432 0.431
RECON 0.493 0.675 0.454 0.445 0.392 0.206 0.446 0.444

eters as Search-R1, but with two key modifications:
(1) the number of retrieved documents per turn is in-
creased from 3 to 5, and (2) the maximum number
of turns is extended from 3 to 5. Since the summa-
rization step keeps the context length manageable,
the system can accommodate deeper retrieval and
reasoning. As in the baseline, we optimize the
policy with Proximal Policy Optimization (PPO,
Schulman et al., 2017). Additional algorithmic
details are provided in § C.

Outcome. By condensing evidence before rea-
soning, RECON produces shorter contexts for the
policy model, requires fewer search turns, and
achieves lower inference latency. The summarizer
also reduces distraction from irrelevant informa-
tion, improving accuracy on multi-hop questions.
Beyond Search-R1, the modular design of RECON
makes it applicable to other RL-based RAG sys-
tems where context efficiency is a bottleneck. In
the next section, we evaluate the benefits of RE-
CON against the original Search-R1 baseline.

4 Experiments

4.1 Experimental Setup

Our setup follows the Search-R1 pipeline to ensure
fair comparisons.

Baselines. We primarily compare against Search-
R1, the state-of-the-art RL-based RAG framework.
For completeness, we also include representative

RAG baselines; see § D for details.

Backbones and datasets. We use Qwen?.5-{3B,
7B }-Base (Yang et al., 2024) as backbone policy
models. In line with prior works, Base models
paired with PPO yield the strongest performance
among available variants. Evaluation spans seven
QA benchmarks: NQ (Kwiatkowski et al., 2019),
TriviaQA (Joshi et al., 2017), PopQA (Mallen et al.,
2023), HotpotQA (Yang et al., 2018), 2Wiki (Ho
et al., 2020), Musique (Trivedi et al., 2022), and
Bamboogle (Press et al., 2023). These cover open-
domain QA and multi-hop QA, providing a com-
prehensive assessment of factual accuracy and rea-
soning depth.

Evaluation. We report exact match (EM) as the
primary metric. To assess efficiency, we addition-
ally track average context length, inference time,
and number of reasoning turns. This allows us to
disentangle gains from accuracy versus efficiency.
Implementation. Our system is built on PyTorch,
veRL-v0.1 (Sheng et al., 2024), vLLM (Kwon et al.,
2023), Huggingface Transformers, and LlamaFac-
tory (Zheng et al., 2024). Hyperparameter details
are provided in § B.4.

4.2 Results and Analysis

Performance. Table 1 shows that RECON con-
sistently outperforms Search-R1 across backbones
and datasets. With Qwen2.5-7B-Base, RECON
raises the average EM score from 0.431 to 0.444.


Efficiency Comparison: RECON vs. Search-R1 Baseline

14 Method
l™@™@™ Search-R1 (Baseline)
12 |. mm RECON

Normalized Cost

Inference Time Turns Used

Context Length

Evaluation Metric

Figure 2: Inference efficiency of RECON vs. Search-
R1, with Qwen2.5-7B-Base + PPO. We report average
context length (|), inference wallclock time per query
(|) and number of search turns (|) over 7 datasets.

Improvements are even more pronounced on the
3B model, where EM increases from 0.303 to 0.347
(a 14.5% relative gain), underscoring the value of
context compression for compact models.

The largest gains appear in multi-hop QA bench-
marks (HotpotQA, 2Wiki, Musique, and Bam-
boogle), where condensing retrieved evidence
yields cleaner contexts that better support multi-
step reasoning. Notably, RECON also improves
performance on single-hop QA (e.g., TriviaQA:
0.675 vs. 0.638), indicating benefits extend beyond
multi-hop tasks.

Efficiency. Fig. 2 compares inference efficiency,
normalized to the Search-R1 baseline (1.0x). RE-
CON reduces context length and filters irrelevant
evidence, which lowers the number of required
search turns. These factors significantly reduce in-
ference latency measured by wall-clock time. RE-
CON also improves training time. Using Qwen2.5-
3B-Base + PPO on 4x H200 GPUs, we track a 13.9
hours training time for RECON for a total of 500
steps, compared to Search-R1’s 14.7 hours, mark-
ing a 5.2% speedup. Overall, our results suggest
that RECON improves both training speed and in-
ference latency compared to Search-R1, noting the
efficacy of context compression despite adding the
additional summarization step.

Backbone variants. We further evaluate RECON
with the Instruct model backbone. With Qwen2.5-
3B-Instruct + PPO, RECON achieves an average
EM of 0.336 compared to 0.325 for Search-R1.
Although the margin is smaller than with Base
models, the gains confirm that RECON provides
consistent improvements across different backbone
families (see § F for details).

5 Conclusions and Future Works

We presented RECON, a framework that tackles the
efficiency and performance challenges of RL-based
RAG systems through active context compression.
By introducing a dedicated summarization module
trained via two complementary stages — relevance
pretraining and multi-aspect distillation — RECON
condenses verbose retrieval outputs into concise,
factual evidence that is better aligned with down-
stream reasoning. Integrated into the Search-R1
pipeline, RECON yields consistent improvements
across seven QA benchmarks: it effectively reduces
context length, leading to improved training speed
and inference latency, while boosting reasoning
accuracy, particularly on multi-hop tasks that de-
mand evidence synthesis. To summarize, our re-
sults demonstrate the potential of learned context
compression as a key design for the next genera-
tion of RL-based RAG agents. We highlight poten-
tial future works as extensions to diverse domains
where efficient and trustworthy reasoning over re-
trieved knowledge is essential.

Limitations and Risks

While RECON demonstrates consistent improve-
ments in accuracy and efficiency across a range of
QA benchmarks, we note a few limitations.
Domain generalization. Our summarizer is
trained on open-domain QA datasets (NQ, Hot-
potQA, MS MARCO), and its performance may
not generalize to highly specialized domains such
as biomedical literature, legal documents, or finan-
cial reports, where relevance signals and summa-
rization norms can differ substantially. Adapting
RECON to such domains may require domain-
specific retraining or distillation from expert
teacher models.

Dependency on teacher quality. The summa-
rizer’s performance is bounded by the quality and
biases of the teacher model (GPT-40-mini). AI-
though multi-aspect distillation aims to align with
human-preferred qualities, any systematic errors
or stylistic tendencies in the teacher summaries
may propagate to the student model. Additionally,
teacher-forced training may overfit to the specific
phrasing or compression style of GPT-40-mini or
other teacher models.

We note that these points should constitute inter-
esting future directions for RAG systems extending
beyond this paper.

All of our experiments are based on public


datasets and models licensed for academic usage.
To the best of our knowledge, this work does not
raise ethical concerns.

References

Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui
Zhang, and Wenpeng Yin. 2024. Large language
models for mathematical reasoning: Progresses and
challenges. In Proceedings of the 18th Conference
of the European Chapter of the Association for Com-
putational Linguistics: Student Research Workshop,
pages 225-237. Association for Computational Lin-
guistics.

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2024. Self-rag: Learning to re-
trieve, generate, and critique through self-reflection.

Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-
drew McNamara, Bhaskar Mitra, Tri Nguyen, and
1 others. 2016. Ms marco: A human generated ma-
chine reading comprehension dataset. arXiv preprint
arXiv: 1611.09268.

Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou,
Melody Y Guan, Aleksander Madry, Wojciech
Zaremba, Jakub Pachocki, and David Farhi. 2025.
Monitoring reasoning models for misbehavior and
the risks of promoting obfuscation. arXiv preprint
arXiv:2503.11926.

Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler
Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao
Wang, Akshay Malik, Graham Neubig, Kourosh
Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei
Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025.
Skyrl-v0: Train real-world long-horizon agents via
reinforcement learning.

Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh,
Menghai Pan, Chin-Chia Michael Yeh, Guanchu
Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Ma-
hashweta Das, and Na Zou. 2025. MAIN-RAG:
Multi-agent filtering retrieval-augmented generation.
In Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2607-2622, Vienna, Austria.
Association for Computational Linguistics.

Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze
Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang,
Jeff Z Pan, Wen Zhang, Huajun Chen, and | oth-
ers. 2025. Learning to reason with search for
Ilms via reinforcement learning. arXiv preprint
arXiv:2503.19470.

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
Danqi Chen. 2023. Adapting language models to
compress contexts. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing, pages 3829-3846, Singapore. Associa-
tion for Computational Linguistics.

Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiao-
jun Chen, and Ruifeng Xu. 2024. Enhancing noise
robustness of retrieval-augmented language models
with adaptive adversarial training. In Proceedings
of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 10028-10039, Bangkok, Thailand. Association
for Computational Linguistics.

Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An.
2025. Group-in-group policy optimization for Ilm
agent training. arXiv preprint arXiv:2505.10978.

Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
2023a. Enabling large language models to gener-
ate text with citations. In The 2023 Conference on
Empirical Methods in Natural Language Processing.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang
Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun,
Haofen Wang, and Haofen Wang. 2023b. Retrieval-
augmented generation for large language models: A
survey. arXiv preprint arXiv:2312.10997, 2(1).

Juraj Gottweis, Wei-Hung Weng, Alexander Daryin,
Tao Tu, Anil Palepu, Petar Sirkovic, Artiom
Myaskovsky, Felix Weissenberger, Keran Rong, Ryu-
taro Tanno, and 1 others. 2025. Towards an ai co-
scientist. arXiv preprint arXiv:2502.18864.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-
rong Ma, Peiyi Wang, Xiao Bi, and | others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
Ilms via reinforcement learning. arXiv preprint
arXiv:2501.12948.

Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Yu Wu, YK Li, and | others. 2024. Deepseek-
coder: When the large language model meets
programming-the rise of code intelligence. arXiv
preprint arXiv:2401.14196.

Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing a multi-
hop QA dataset for comprehensive evaluation of
reasoning steps. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics,
pages 6609-6625, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.

Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2025. A survey on hallucination in large lan-
guage models: Principles, taxonomy, challenges, and
open questions. ACM Trans. Inf: Syst.

Aaron Hurst, Adam Lerer, Adam P Goucher, Adam
Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,
Akila Welihinda, Alan Hayes, Alec Radford, and 1
others. 2024. Gpt-40 system card. arXiv preprint
arXiv:2410.21276.


Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-
son, Ahmed El-Kishky, Aiden Low, Alec Helyar,
Aleksander Madry, Alex Beutel, Alex Carney, and |
others. 2024. Openai ol system card. arXiv preprint
arXiv:2412.16720.

Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O
Arik. 2025a. Long-context Ilms meet rag: Overcom-
ing challenges for long inputs in rag. In The Thir-
teenth International Conference on Learning Repre-
sentations.

Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O
Arik, and Jiawei Han. 2025b. An empirical study
on reinforcement learning for reasoning-search inter-
leaved Ilm agents. arXiv preprint arXiv:2505.15117.

Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon,
Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei
Han. 2025c. Search-r1: Training IIms to reason and
leverage search engines with reinforcement learning.
arXiv preprint arXiv:2503.09516.

Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang,
and Zhicheng Dou. 2024. Flashrag: A modular
toolkit for efficient retrieval-augmented generation
research. CoRR, abs/2405.13576.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
Billion-scale similarity search with gpus. [EEE
Transactions on Big Data, 7(3):535-547.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

Tomek Korbak, Mikita Balesni, Elizabeth Barnes,
Yoshua Bengio, Joe Benton, Joseph Bloom, Mark
Chen, Alan Cooney, Allan Dafoe, Anca Dragan, and
1 others. 2025. Chain of thought monitorability:
A new and fragile opportunity for ai safety. arXiv
preprint arXiv:2507.11473.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Ilia Polosukhin, Jacob Devlin, Ken-
ton Lee, and | others. 2019. Natural questions: a
benchmark for question answering research. Trans-
actions of the Association for Computational Linguis-

tics, 7:453—466.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, and Jon Stoica. 2023. Efficient
memory management for large language model serv-
ing with pagedattention. In Proceedings of the 29th
symposium on operating systems principles, pages

611-626.

Tamera Lanham, Anna Chen, Ansh Radhakrishnan,
Benoit Steiner, Carson E. Denison, Danny Her-
nandez, Dustin Li, Esin Durmus, Evan Hubinger,
John Kernion, Kamil.e Lukovsiut.e, Karina Nguyen,
Newton Cheng, Nicholas Joseph, Nicholas Schiefer,
Oliver Rausch, Robin Larson, Sam McCandlish,

Sandipan Kundu, and 11 others. 2023. Measuring
faithfulness in chain-of-thought reasoning. arXiv
preprint arXiv:2307.13702.

Shahar Levy, Nir Mazor, Lihi Shalmon, Michael Hassid,
and Gabriel Stanovsky. 2025. More documents, same
length: Isolating the challenge of multiple documents
in rag. arXiv preprint arXiv:2503.04388.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rock-
taschel, Sebastian Riedel, and Douwe Kiela. 2020a.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. In Advances in Neural Informa-
tion Processing Systems, pages 9459-9474. Curran
Associates, Inc.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rock-
taschel, and 1 others. 2020b. Retrieval-augmented
generation for knowledge-intensive nlp tasks. Ad-
vances in neural information processing systems,

33:9459-9474.

Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,
Yujia Zhou, Yutao Zhu, Peitian Zhang, and
Zhicheng Dou. 2025. Search-o1l: Agentic search-
enhanced large reasoning models. arXiv preprint
arXiv:2501.05366.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2024. Lost in the middle: How language mod-
els use long contexts. Transactions of the Association
for Computational Linguistics, 12:157-173.

Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Hal-
faker, Dragomir Radev, and Ahmed Hassan Awadal-
lah. 2023. On improving summarization factual con-
sistency from natural language feedback. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 15144-15161, Toronto, Canada. Association
for Computational Linguistics.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume I: Long Papers), pages 9802-9822, Toronto,
Canada. Association for Computational Linguistics.

Neil Mallinar, A Ali Heydari, Xin Liu, Anthony Z
Faranesh, Brent Winslow, Nova Hammerquist, Ben-
jamin Graef, Cathy Speed, Mark Malhotra, Shwetak
Patel, and 1 others. 2025. A scalable framework for
evaluating health language models. arXiv preprint
arXiv:2503.23339.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. FActScore:


Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, pages 12076-12100, Singa-
pore. Association for Computational Linguistics.

Artidoro Pagnoni, Vidhisha Balachandran, and Yulia
Tsvetkov. 2021. Understanding factuality in abstrac-
tive summarization with FRANK: A benchmark for
factuality metrics. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 4812-4829, Online. As-
sociation for Computational Linguistics.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah Smith, and Mike Lewis. 2023. Measuring and
narrowing the compositionality gap in language mod-
els. In Findings of the Association for Computational
Linguistics: EMNLP 2023, pages 5687-5711, Singa-
pore. Association for Computational Linguistics.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-
mar, Chloe Hillier, and Timothy P. Lillicrap. 2020.
Compressive transformers for long-range sequence
modelling. In International Conference on Learning
Representations.

Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Lee, and
Jungseul Ok. 2024. Multi-dimensional optimization
for text summarization via reinforcement learning.
In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 5858-5871, Bangkok, Thailand.
Association for Computational Linguistics.

John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv: 1707.06347.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, YK Li, Y Wu, and | others. 2024. Deepseek-
math: Pushing the limits of mathematical reason-
ing in open language models. arXiv preprint
arXiv:2402.03300.

Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin
Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin
Lin, and Chuan Wu. 2024. Hybridflow: A flex-
ible and efficient rlhf framework. arXiv preprint
arXiv:2409. 19256.

Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen,
Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-
Rong Wen. 2025. Rl-searcher: Incentivizing the
search capability in llms via reinforcement learning.
arXiv preprint arXiv:2503.05592.

Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, and Jiawei
Han. 2025. Dynamicrag: Leveraging outputs of large
language model as feedback for dynamic reranking
in retrieval-augmented generation. arXiv preprint
arXiv:2505.07233.

Richard S Sutton, Andrew G Barto, and 1 others. 1998.
Reinforcement learning: An introduction. MIT press
Cambridge.

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. Musique: Multi-
hop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics, 10:539-554.

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2023. Interleaving retrieval
with chain-of-thought reasoning for knowledge-
intensive multi-step questions. In Proceedings of
the 61st Annual Meeting of the Association for Com-
putational Linguistics (Volume I: Long Papers),
pages 10014-10037, Toronto, Canada. Association
for Computational Linguistics.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. 2022. Text embeddings by weakly-
supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
and 1 others. 2022. Chain-of-thought prompting elic-
its reasoning in large language models. Advances
in neural information processing systems, 35:24824—
24837.

Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu
Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang,
Pengjun Xie, Fei Huang, and 1 others. 2025.
Resum: Unlocking long-horizon search intelli-

gence via context summarization. arXiv preprint
arXiv:2509. 13313.

Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RE-
COMP: Improving retrieval-augmented LMs with
context compression and selective augmentation. In
The Twelfth International Conference on Learning
Representations.

Yumo Xu and Mirella Lapata. 2020. Coarse-to-fine
query focused multi-document summarization. In
Proceedings of the 2020 Conference on empirical
methods in natural language processing (EMNLP),
pages 3632-3645.

Zhichao Xu, Fengran Mo, Zhiqi Huang, Crystina Zhang,
Puxuan Yu, Bei Wang, Jimmy Lin, and Vivek Sriku-
mar. 2025. A survey of model architectures in infor-
mation retrieval. arXiv preprint arXiv:2502.14822.

Zhichao Xu, Fengran Mo, Zhiqi Huang, Crystina Zhang,
Puxuan Yu, Bei Wang, Jimmy Lin, and Vivek Sriku-
mar. 2025. Distillation versus Contrastive Learn-
ing: How to Train Your Rerankers. arXiv preprint
arXiv:2507.08336.

Zhichao Xu, Jinghua Yan, Ashim Gupta, and Vivek
Srikumar. 2025. State Space Models are Strong Text
Rerankers. In Proceedings of the 10th Workshop


on Representation Learning for NLP (RepL4NLP-
2025), pages 152-169, Albuquerque, NM. Asso-
ciation for Computational Linguistics. https: //
aclanthology.org/2025.repl4nlp-1.12/.

An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jian-
hong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,
Jingren Zhou, Junyang Lin, Kai Dang, and 22 oth-
ers. 2024. Qwen?2.5 technical report. arXiv preprint
arXiv:2412.15115.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. arXiv preprint arXiv: 1809.09600.

Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Be-
rant. 2024. Making retrieval-augmented language
models robust to irrelevant context. In The Twelfth
International Conference on Learning Representa-
tions.

Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan
Ye, and Zheyan Luo. 2024. LlamaFactory: Unified
efficient fine-tuning of 100+ language models. In
Proceedings of the 62nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 3:
System Demonstrations), pages 400-410, Bangkok,
Thailand. Association for Computational Linguistics.

Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai,
Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.
Deepresearcher: Scaling deep research via reinforce-
ment learning in real-world environments. arXiv
preprint arXiv:2504.03160.


Table 2: QA Dataset statistics. | denotes in-domain trainset and * denotes OOD dataset.

Dataset #Train #Val #Test Corpus Task License
Natural Questionst (Kwiatkowski et al.,2019) 79,168 8,757 3,610 Wikipedia QA Apache 2.0
TriviaQA* (Joshi et al., 2017) 78,785 8,837 11,313 Wikipedia & Web QA Apache 2.0
PopQA* (Mallen et al., 2023) - - 14,267 Wikipedia QA MIT
HotpotQAt (Yang et al., 2018) 90,447 7,405 - Wikipedia Multi-hop QA = CC BY-SA 4.0
2WikiMultihopQA* (Ho et al., 2020) 15,920 1,986 1,996 Wikipedia Multi-hop QA — Apache 2.0
MuSiQue* (Trivedi et al., 2022) 19,938 2,417 - Wikipedia Multi-hop QA = CC BY-SA 4.0
Bamboogle* (Press et al., 2023) - - 125 Web Adversarial QA MIT

A Formulation of Search-R1

The training of Search-R1 (Jin et al., 2025c) is formulated as a reinforcement learning (RL) problem
where the policy model 7g alternates between (i) generating intermediate reasoning spans and (ii) issuing
search queries to an external engine R, with the objective of producing a factually correct final answer.
Formally, for each input question x, the policy generates a trajectory

y = { 51, d1, 52,d2,..., 87, a},

where s; is a reasoning span, d; is the set of documents retrieved from 7 at step 2, and a is the final answer.
The training objective is

max EywD, ywmo(-| 2) [re(z,y)] — 8 Dxx(mo(y|2;R) || Mree(y| 23 R)),

where rg(x, y) is the task reward defined by the exact-match (EM) accuracy of the final answer, and the
KL term keeps the learned policy close to a reference model 7 ¢¢ (e.g., an SFT model) to stabilize training.

This formulation bears two main advantages: (1) it enables multi-turn interaction with the search engine
(via an interaction budget), allowing iterative reasoning and evidence acquisition; and (2) by explicitly
incorporating retrieved documents into the trajectory, the policy is encouraged to ground its reasoning in
external evidence, improving reliability over purely generative approaches.
Implementation note. Only tokens generated by the policy (reasoning, queries, answers) are optimized;
tokens from retrieved evidence are masked out of the loss. Empirically, this loss masking mechanism has
been shown to improve performance and training instability.

B_ Experiment Details

B.1 Dataset Statistics and Licenses

For relevance “pre-training’” on MS MARCO QA dataset (Bajaj et al., 2016), we use the trainset split
released by (Jin et al., 2024). The original dataset is licensed for non-commercial usage.

The datasets to train and evaluate RECON are presented in Table 2. The datasets we used and the
experimental settings are consistent with Search-R1.

B.2 Distillation Data Construction

We collect the training data of our summarizer from Search-R1 model’s rollouts. We use the official
checkpoint PeterJinGo/SearchR1-nq_hotpotqa_train-qwen2.5-7b-em-ppo. For each query x; in
NQ and HotpotQA dataset, the model will conduct multiple query-retrieval-reasoning turns before arriving
at the final answer. We collect all intermediate queries 7;; € X;, and remove duplicate ones, as sometimes
the model issues multiple identical queries in one rollout. We then run retrieval inference with E5-base-v2
retriever (Wang et al., 2022) to retrieval top-5 passages for each query and construct corresponding
{(xij, Dij)} query-documents pairs. For each (x;;,Dj;) pair, we instruct GPT-40-mini to generate a
multi-document query-focused summary focusing on each aspect a; of six aspects. Together, we collect
468,547 (xij, Dij, ax) triplet from the HotpotQA dataset and 1,002,329 (x;;,Di;, az) triplet from the NQ
dataset to be used in the summarizer training.

“https ://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/viewer/msmarco-qa


B.3. Prompt Templates

The six aspect and their aspect explanation with the prompt template is shown below.

Clarity Write in a clear, accessible manner that is easy to understand. Use simple, direct language and
avoid jargon or overly complex sentences. Present information in a straightforward way that makes the
key points immediately apparent to the reader. Ensure that each statement is unambiguous and easy to
follow.

Coherence Create a logically coherent and well-structured summary that flows naturally from one point
to the next. Use clear transitions, logical connections, and a consistent narrative structure. Ensure that
ideas are presented in a logical sequence that makes sense to the reader, with each piece of information
building upon the previous one.

Completeness Make sure that the support context includes all major facts from the retrieved documents
that are needed to answer the user’s question. Do not omit important information, even if it seems
implicit. The summary should be as thorough as possible within a compact form.

Coverage Ensure comprehensive coverage of all relevant information from the retrieved documents.
Include all key facts, data points, examples, and supporting details that could be useful for answering the
question. Avoid omitting important information even if it seems redundant, as comprehensive coverage
is prioritized over brevity.

Factual Correctness Ensure that every statement in the support context is factually accurate and directly
supported by the retrieved documents. Do not include any information that is inferred, assumed, or
fabricated. Avoid hallucinations, exaggerations, or unsupported claims.

Logicality Present information in a logically sound manner with clear reasoning and valid conclusions.
Ensure that cause-and-effect relationships are properly established, that arguments are well-structured,
and that conclusions follow logically from the presented evidence. Avoid logical fallacies and ensure
that the information flows in a way that makes logical sense.

B.4 Hyperparameters

We train the summarizer using LlamaFactory (Zheng et al., 2024). For both stages of relevance “pre-
training” and summarization distillation, we use an effective batch size of 16 (per-device batch size = 1
with gradient accumulation = 16 on 1 x H100), learning rate 1 x 10-4, and 5% warmup for 1 epoch. Our
SFT training is conducted in bf16 with LoRA (rank = 8, a = 32, target_modules=all-linear) and max
sequence length = 2048. In RECON training, the summarizer is frozen. During rollout, we use sampling
hyperparameters top-p = 0.9, top-k = 40, and temperature = 0.7.

For training of RECON, we use the following important hyperparameters (consistent with the original

Search-R1 implementation).

¢ Batching. Global batch size 512; mini batch size 256; micro batch size 128 (actor), 16 (critic). Training
is distributed across 4 H100 GPUs for the 3B model and 8 H100 GPUs for the 7B model. Gradient
accumulation is used to reach the global batch from per-device micro-batches.

PPO algorithm. GAE discount y = 1.0, GAE = 1.0; clip ratio=0.2; KL control coefficient=0.001
(fixed); entropy coefficient=0.001; value loss clipping (cliprange_value)=0.5; PPO epochs per
update=1.

Optimizer and training schedule. We train both 3B and 7B models using AdamW optimizer at actor
LR 1e-6, critic LR le-5, gradient clipping value 1.0, for 1005 steps with linear learning rate warmup
ratio 0.285 for actor and 0.015 for critic.

Decoding and lengths. Temperature=1.0; Max prompt length=4096 tokens; max response length=500
tokens.

Precision and memory. Precision=bfloat16; gradient_checkpointing=enabled. FSDP configuration:
no parameter/gradient/optimizer offloading.

Retrieval. Dense retriever: intfloat/e5-base-v2 (768-dim) (Wang et al., 2022) with FAISS GPU
acceleration (Johnson et al., 2019); retriever is frozen during RL training. We retrieve top-5 passages
(baseline: top-3).


¢ Randomization. Random seed=1.

¢ Backbone and system. Backbone: Qwen2.5-Base at 3B and 7B scales. We use XFORMERS as
attention backend and PyTorch FSDP for distributed training.

You are a helpful assistant in a retrieval-augmented question-answering system.

You will be given:
e A user question
¢ A search prompt (query) used to retrieve information
¢ A set of documents retrieved using that query

Your task is to generate a support context — a concise, well-structured summary that captures all
the key facts from the documents which are relevant to answering the user question.

Important Instructions:
¢ Do not answer the question directly.
¢ Do not add external knowledge or hallucinate any content.
¢ Use only the information found in the retrieved documents.
¢ Rephrase and compress where appropriate, but preserve factual meaning.
¢ Maintain consistent tone and structure throughout.
¢ Focus on maximizing the following aspect in your output:

Focus Aspect: {ASPECT}
— {ASPECT EXPLANATION}

User Question:
{question }

Search Query (used by the retriever):
{query}

Retrieved Documents:

[Doc 1] {doc_1}

[Doc 2] {doc_2}

[Doc 3] {doc_3}

Please write the support context for me. Make it clear, factual, and optimized for the aspect defined
above.

C_ Algorithmic Details

We show the detailed algorithm in Algorithm. 1. Colored text denotes modifications from the original
Search R1 algorithm.

For completeness, we present the PPO objective used for policy optimization with the summarizer-
augmented retriever Rum.

1 ly|

Wl S-1(y) -min (ri(O) Ar, clip (r:(@), 1-e, L+e) Ar)
t=1

Tero(9) = Egap, ywro(-|asReum) |

79 (ye | Li, U<ts Resum)
0) = : 1
70) Told (ve | @, Yet} Renn) Hl

Here, 7 and 7oiq denote the current and previous policy models, I(y;) = 1 if y, is an LLM-generated
token and 0 if retrieved (token-level loss masking), A; is the advantage estimate, and «€ is the clipping
parameter.


Algorithm 1 LLM Response Rollout with Multi-Turn Search Engine Calls (with Summarizer)

Require: Input query x, policy model 7, search engine 7, summarizer S, maximum action budget B.
Ensure: Final response y.

1: Initialize rollout sequence y + @

2: Initialize action count b < 0

3: while b < B do

4 Initialize current action rollout y, ~— @

5: while True do

6: Generate token y; ~ 79(- | x,y + yp)

7 Append y; to rollout yy <— yp + Ye

8 if y, € [</search>, </answer>, <eos>] then
9

: break
10: end if
11: end while
122 yy ty
13: if <search> </search> detected in y, then
14: Extract query g < Parse(yp, <search>, </search>)
15: Retrieve docs d <— R(q)
16: Summarize docs d’ + S(d)
17: Insert d’ into rollout y <— y + <information>d’</information>
18: else if <answer> </answer> detected in y, then
19: return final response y
20: else
21: y + y+ “My action is not correct. Let me rethink.”
22: end if

28 be b+1
24: end while
25: return final response y

D_ Baselines

Original Search-R1 (baseline, PPO+GRPO). The original Search-R1 framework trains reasoning-and-
search interleaved LLMs with reinforcement learning using both PPO and its variant GRPO (Schulman
et al., 2017; Shao et al., 2024). All models are trained using a mixture of Natural Questions (NQ)
(Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018), using Wikipedia-18 dump as the knowledge
corpus, with a unified preprocessing pipeline.

Other baselines. Baseline methods in Table 1 include (1) prompting-based method: Direct Inference
and Chain-of-Thought prompting (Wei et al., 2022). (2) Retrieval-based method: RAG (Lewis et al.,
2020b), IRCoT (Trivedi et al., 2023), Search-o1 (Li et al., 2025). (3) Training-based methods: SFT, RL
w/o search engine (Guo et al., 2025), rejection sampling w/o search engine (Ahn et al., 2024). All baseline
results are from Jin et al. (2025c).

E_ Detailed Efficiency Results

We report detailed inference-time efficiency results in Table 3.

F_ Base vs. Instruct Models

We report ablation studies of Base vs. Instruct models in Table 4.


Table 3: Comparison of context length (average sequence length), average inference time (s), and turns used (average
search count) for tested Search-R1 and proposed RECON. We evaluate Qwen2.5-7B-Base + PPO, with the official
checkpoint from (Jin et al., 2025c).

Method NQ_ TriviaQA PopQA_ HotpotQA 2Wiki Musique Bamboogle Avg.

Context Length (average sequence length) |
Search-R1 928.5 831.6 542.9 1087.3 1123.5 1321.0 803.1 948.3

RECON 620.7 554.4 380.5 696.0 725.3 825.6 535.4 619.7
Average Inference Time (s) |

Search-R1 31.2 21.9 13.9 30.6 29.9 46.8 27.2 28.8

RECON 22.4 15.6 10.0 21.2 19.8 31.1 19.2 19.9
Turns Used (average search count) |

Search-R1 2.10 1.82 1.41 2.12 2.15 2ST 2.12 LAB

RECON 1.96 1.70 1.35 1.94 1.98 2.34 1.61 1.84

Table 4: Ablation studies of Base vs Instruct using 3B scale models. Best overall numbers on each dataset are bold,
best in-category numbers are underlined.

Method NQ_ TriviaQA PopQA HotpotQA 2Wiki Musique Bamboogle Avg.
Qwen2.5-3B-Base + PPO

Search-R1 0.406 0.587 0.435 0.284 0.273 0.049 0.088 0.303
RECON 0.440 0.612 0.425 0.326 0.319 0.082 0.226 0.347
Qwen2.5-3B-Instruct + PPO

Search-R1I 0.341 0.545 0.378 0.324 0.319 0.103 0.264 0.325

RECON 0.353 0.551 0.381 0.359 0.292 0.109 0.304 0.336

