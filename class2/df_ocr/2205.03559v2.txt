arX1v:2205.03559v2 [cs.CL] 18 Sep 2022

Improving Downstream Task Performance by Treating Numbers as
Entities

Dhanasekar Sundararaman’, Vivek Subramanian’,
Guoyin Wang’, Liyan Xu°, Lawrence Carin‘
' Duke University
2 Amazon Alexa AI
3 Emory University
dhanasekar.sundararaman@duke.edu

Abstract

Numbers are essential components of text, like
any other word tokens, from which natural lan-
guage processing (NLP) models are built and
deployed. Though numbers are typically not
accounted for distinctly in most NLP tasks,
there is still an underlying amount of numer-
acy already exhibited by NLP models. In this
work, we attempt to tap this potential of state-
of-the-art NLP models and transfer their abil-
ity to boost performance in related tasks. Our
proposed classification of numbers into enti-
ties helps NLP models perform well on sev-
eral tasks, including a handcrafted Fill-In-The-
Blank (FITB) task and on question answer-
ing using joint embeddings, outperforming the
BERT and RoBERTa baseline classification.

1 Introduction

Named entity recognition (NER) is the task of clas-
sifying nouns within a document into categories to
which they belong (Sang and De Meulder, 2003).
Deep learning approaches for NER have been suc-
cessful recently because of their ability to under-
stand the underlying semantics of sentences (Shen
et al., 2019) and incorporate this knowledge into
tagging. Deep architectures for NER (Lample et al.,
2016) include recurrent neural networks and at-
tention based models including the Transformer
(Vaswani et al., 2017).

While NER has been performed on a number
of datasets, tagging types of numbers, or number
entities, has not been a principal focus of NER.
This is despite the fact that numbers are prevalent
in most texts, comprising roughly 5% of tokens
in a sentence (Naik et al., 2019). Numbers can
take on several categories, including years, ages,
phone numbers, dates, etc. Figure 1 shows sam-
ple sentences in which numbers are classified into
entity types. Distinguishing these different types
of numbers can greatly enhance performance on
downstream tasks (Sundararaman et al., 2020; Sun-
dararaman and Subramanian, 2022).

Super bow! COUNT was an American football game to
determine the champion of the NFL for the 2016 YEAR
season.

The league announced on October 16 DATE, 2012
YEAR, that the two finalists were Sun Life’s stadium
and Levi’s stadium.

Figure 1: Sequence classification of numbers, or num-
ber entity recognition.

In this paper, we present several methods for
addressing number entity recognition (NuER). We
begin by demonstrating that state-of-the-art NLP
models are able to classify types of numbers that
appear in text (Wallace et al., 2019; Zhang et al.,
2020). We study this by utilizing BERT (Devlin
et al., 2018) to perform NuER on a custom ver-
sion of the Stanford Question Answering Dataset
(SQuAD) (Rajpurkar et al., 2016), pruned to con-
tain roughly 10K sentences for which the answers
consist of numbers, and annotated in-house for
numbers of six different entity types (year, per-
centage, date, count, age, and size) (Sundararaman
et al., 2020). We hereby refer to this as the SQUAD-
Num dataset and the BERT classifier used to pre-
dict entities as the NuER model. We discover that
BERT model performs well at this task on most
of the categories listed above. The success of this
model is made possible by the highly contextual-
ized embeddings (e.g., the appearance of the name
of a month, say March, next to a year, say 2003)
inferred by BERT.

We then verify the effectiveness of BERT fine-
tuned on SQUAD-Num to tag numbers that appear
in a out-of-domain, human-annotated, 2K-sentence
subset of the Numeracy 600K dataset (Chen et al.,
2019), which we call HA-Numeracy 2K. We chose
this dataset because of the abundance and diver-
sity of numbers present. The subset was sampled
in such a way as to maintain the distribution of
number magnitudes that are originally present. We


C S) Y P DJA
1800 | 447 | 4355 | 291 | 418 | 82

Table 1: Number of entities in our annotation for each
type in SQUAD-Num. The table headers represent the
following entity types respectively: Count, Size, Year,
Percentage, Date, Age.

find that contextual embeddings are again benefi-
cial in this setting, as surrounding tokens in both
the source and target domains tend to be related.
After verifying the quality of the annotations on
the small sample, we annotated the full Numeracy
600K dataset using this model, which we hereafter
call ENT-Numeracy 600K. Table 1 shows the num-
ber of samples in each entity type in SQUAD-Num.
We share these annotations with the NLP commu-
nity to further enhance research in this direction.

Next, we jointly train embeddings for both to-
kens and number entities on the SQUAD-Num
dataset. Answers to questions involving num-
bers are predicted by combining together token
embeddings with trainable number entity embed-
dings. We find that this addition (Sundararaman
et al., 2019, 2021a; Subramanian and Sundarara-
man, 2021) enables BERT and RoBERTa models to
better answer questions in SQUAD involving num-
bers. We later analyze the effectiveness of NuER
through a handcrafted Fill-In-The-Blank (FITB)
classification task. Our proposed NuER model
helps the models classify numbers better when pre-
sented with the entity type information.

In summary, our contributions are as follows:

1. We develop a BERT-based model for NuER
which is used to tag numbers that appear in
the SQUAD-Num dataset and in the out-of-
domain HA-Numeracy 2K.

2. We release datasets containing number entity
annotations for the NLP community.

3. We observe the effectiveness of NuER
through a handcrafted Fill-In-The-Blank
(FITB) task to predict a masked number in
a sentence.

4. We then jointly train embeddings for tokens
and number entities, and show that numeri-
cal question answering performance improves
when both are fed into the model.

2 Related Work

Classification of numbers in plain text to entities
has not been addressed methodologically and de-
serves considerable attention. (Naik et al., 2019)
highlighted the importance of developing special-
ized embeddings for numbers such that basic prop-
erties including comparison and summation would
be preserved. Since then In addition, (Sundarara-
man et al., 2020) designed a novel loss function
that captured these properties and demonstrated
improved performance on a variety of NLP tasks.
Unlike their method which uses static embeddings,
we propose to supplement the entity information
of numbers in a contextual fashion to enable mod-
els to perform well on a number of tasks (Geva
et al., 2020). There are existing methods on nu-
merical word embeddings (Berg-Kirkpatrick and
Spokoyny, 2020; Spithourakis and Riedel, 2018),
but they didn’t consider numbers as entities for
the task of NER. NLP models have shown to use
supplemental knowledge to its advantage (Tanwar
et al., 2022; Sundararaman et al., 2019; Subrama-
nian and Sundararaman, 2021). Existing works
have also shown that NLP models are intrinsically
capable of numerical reasoning. This potential can
be tapped using simple modifications like those
proposed by (Andor et al., 2019), in which the
BERT model was used to perform arithmetic oper-
ations. Like the above-mentioned works, we seek
to leverage the potential of existing NLP models
to classify numbers into different entity types and
transfer such ability across related tasks (Sundarara-
man et al., 2021b) to boost performance.

3. Preliminaries

For each corpus, we index sentences by n =
1,2,...,.N. Each token in a given vocabulary
Y is assigned a one-hot vector embedding v €
{0, iy. Tokens in a sequence are indexed by i =
1,2,...,Z,, where T), is the length of sequence
n (after padding). During embedding lookup, to-
kens are mapped to embeddings e,; € R*% (d
refers to the dimension of embedding). For all
tasks, we use pre-trained models including BERT
and RoBERTa to form latent representations of in-
puts; token embeddings are summed with position
embeddings p,; € R¢ and segment embeddings
Sni € R® prior to being fed into the Transformer en-
coder. The output consists of a classification token
cy, € R% anda sequence of contextualized em-
beddings 0,,; € IR@, one for each token. For each


model, we use an Adam optimizer with 7 = 0.001,
8, = 0.9, 82 = 0.999, and € = 10~“ to minimize
the empirical loss across samples in each batch.
More experimental settings are detailed in their re-
spective sections. We train models on Tesla K80
GPUs and TPUs.

4 Methods

4.1 Number entity recognition

BERT, introduced by (Devlin et al., 2018), has
been shown to exhibit state-of-the-art performance
for NER. Token embeddings are passed through
a Transformer encoder, generating contextualized
representations for each subword token, which are
classified by the attention-based BERT classifier.
We fine-tune the BERT model on the SQUAD-Num
dataset. Both questions and context for each se-
quence are input to the NuER model, and the model
outputs a sequence of tags denoting whether each
token is one of the six types of numbers or “other”
(seven total classes). The outputs of the NuER
model for each element in the sequence, given by
the o;, are mapped through a linear layer (we drop
the n subscript for clarity), and the resultant scalars
are fed into a softmax function:

. exp( Wo;
vi==E ( i) (1)
dk=1 CXP(W;, O:)
where w € R?4, W = [Wi,--.,Wk---sWe) | E

R**¢ are the parameters of the linear layer, k =

., 4 indexes the entity labels, and the exp op-
erator is applied elementwise. The model is trained
with categorical cross-entropy loss:

c-¥

a=

Me

—Yik log (Yik) (2)

>
ll

1

where y;x € {0, 1} denotes whether the ith element
belongs to class k. We divide SQUAD-Num into
75% training, 10% validation, and 15% testing.
We fine-tune the NuER model for 20 epochs on
SQuAD-Num and evaluate our model on the out-
of-domain HA-Numeracy 2K.

4.2 Jointly trained embeddings

We experiment with training embeddings for tokens
jointly with number entities, referred as Jointly
trained Embeddings (JEM) (Wang et al., 2018).
Specifically, we compare the performance of nu-
merical question answering on SQUAD-Num, with

and without entity embeddings. As input to the pre-
trained model, we feed in pairs (v;,z;) of tokens
v; and number entity labels z;. 2; € {0, 1ys4t is
a one-hot vector, where K = 6 indicates the six en-
tities of numbers in the corpus. During embedding
lookup, the pairs of embeddings for each token are
summed and fed as inputs:

where h; denotes the trainable number entity em-
bedding for token 7. We compare the performance
of this model after fine-tuning for two epochs
against the baseline, which is only fed the token
identities as input. As is standard for SQUAD, the
model is trained with cross-entropy loss in order
to predict the probability of any token in the con-
text being the start/end of a span that answers the
numerical question:

T
L= J) —y log(gi) (4)

where here, y; € {0,1} identifies the start/end
of a span and 4%; is the corresponding probability
predicted by our model.

4.3 FITB

In this subsection, we measure the effectiveness of
NuER on classification tasks. We handcraft a Fill-
In-The-Blank (FITB) task of predicting a masked
number in a given input sentence. This is challeng-
ing as there are several thousand unique numbers
in the corpus to which a masked number could
be classified. It measures the true capability of
BERT to fill in a number given the context. Our
model which we refer to as BERT ny ER consists
of BERT tokenizer with six special tokens namely
<size>, <age>, <count>, <date>, <year>, and
<percentage>. These special tokens are added next
to the CLS as an indication to the model about their
respective number entity types. On the classifica-
tion side, a linear projection layer of size M x D
is created to learn about the six different entity
types (M = 6, D = 768). These projections are
then concatenated with the BERT hidden states of
768 dimensions to obtain final set of dimensions
(768 « 2) x M which is projected again to a classi-
fication layer of V output vocabulary items. Here,
the V output vocabulary items represent the unique
numbers that are found in the corpus.


Year | Count | Percentage | Age Size Date | Total
Precision || 98.61 | 88.66 95.08 94.12 | 87.84 | 100.00 | 95.38
SQuAD-Num Recall 98.99 | 95.91 96.67 88.89 | 84.42 | 100.00 | 97.24
Fl 98.8 | 92.15 95.87 91.43 | 86.09 | 100.00 | 96.3
Precision || 97.96 | 92.00 91.67 75.00 | 54.55 | 96.88 | 94.26
ENT-Numeracy 600K | Recall 96.48 | 81.01 95.65 7.14 | 30.00 | 85.94 | 84.14
Fl 97.21 | 86.16 93.62 13.04 | 38.71 | 91.09 | 88.91
Table 2: Performance metrics for NuER on SQUAD-Num and ENT-Numeracy 600K for each of the six entity
types.
Year | Count Model Version | Exact match Fl
Precision || 87.09 | 76.27 BERTp Baseline 90.66 90.71
Few-shot Recall 97.32 | 89.59 JEM 91.17 91.33
Fl 91.92 | 82.39 Baseline 95.83 95.88
Precision || 91.15 | 80.53 RoBEKNap JEM 96.84 96.95
Full finetuning | Recall 98.49 | 95.33 BERT, Baseline 93.19 93.28
Fl 94.68 | 87.31 JEM 94.83 95.01

Table 3: Few-shot transfer vs fine-tuning performance
on NuER entity types.

5 Results and Discussion

5.1 Number entity recognition

Table 2 shows the performance of our NuER model
on each dataset. We find that the model performs
well across all categories on SQUAD-Num and
in most categories of ENT-Numeracy 600K. Per-
formance drops are observed for the “Age” and
“Size” categories, partially because there are not
enough training instances of these two categories in
SQuAD-Num. For “Age,” precision is high while
recall is low. This is a limitation of our work and
could be because the numbers that are classified as
“Age” can typically be confused with other entities
including “Size,” suggesting that the model is only
able to capture age accurately when it is quite obvi-
ous in the dataset. For “Size,” both precision and
recall are low, perhaps because, unlike the other
types of numbers, size may have many different
units (e.g., feet, meters, acres, etc.), which make
it more difficult to associate with the numerical
value. Table 3 shows that the classification perfor-
mance when only few of the samples are used in
fine-tuning is still comparable to the full fine-tuning
performance on two classes. Though it works well
for few classes, the F1 scores drop significantly for
classes like Age and Size.

Table 4: Improvements on SQUAD-Num dataset us-
ing Jointly trained Embedding (JEM) technique with
BERT=B (base), BERT, (large), and ROBE RTap
(base). All numbers are tested for significance and have
a standard deviation of 0.05.

5.1.1 Training settings

For Table 2 experiments, the learning rate was set to
2x 10~°. For few-shot used in Table 3, only first 20
samples (using grid search, fewer samples to retain
maximum accuracy compared to full finetuning)
from Year and Count, while full fine-tuning uses
all the samples.

5.2 Jointly trained embeddings

Table 4 compares the performance of our model
trained on numerical question answering against
the BERT baseline. Our model demonstrates gains
in both the exact match and F1 scores, suggest-
ing that the additional information imparted by
knowledge of the type of numbers that are present
in the question and context aids the model in lo-
calizing the number within the context. A vari-
ety of models including BERT (base), BE. RT,
Uarge) and RoBERTag (base) (Devlin et al.,
2018; Liu et al., 2019) are used to understand if
these improvements are consistent. We found that
in terms of overall performance, ROBERT ap >
BERT, > BERTz, with RoBERTag being
the highest, while in terms of performance improve-
ments, BERT, > RoBERTag > BERTRZ
with BERT, JEM obtaining an improvement of


‘dancing with the stars’ : results for november
16, [MASK], season 11, week 9
Answer: 2010
Predictions: [2010, 2011, 2012, 2013, 2009,

13, 11, 10, 14, 12]

iphone [MASK] to begin production in July
Answer: 5
Predictions: [5, 4, 6, 7, 10, 3, 9, 2, 11, 8]

Toronto slides down on top Ontorio investment
cities 2010 — [MASK} list
Answer: 2016
Predictions: [2013, 2011, 2012, 13, 2014,
14, 2010, 11, 15, 10]

[MASK] plus degrees projected for this weekend 's
tournament and night
Answer: 100
Predictions: [50, 100, 30, 90, 60, 40, 80, 21, 98, 75]

nelson garrett ' s antique show at the citadel
september [MASK] in charleston , sc
Answer: 2013
Predictions: [13, 11, 25, 21, 10,
22, 15, 17, 18, 14]

national auto dealers association names honda 's
[MASK] cr - z ' car of the month !
Answer: 2011
Predictions: [2011, 2010, 2012, 2013, 2009, 2014,
2000, 2008, 2007, 2015]

Figure 2: Qualitative comparison of predictions of numbers on the FITB task between the BERT base and
BERTwnuer. The left column shows BERT base predictions, while the right columns shows BERT yuer pre-
dictions. BE RTyuzR predictions align with a number entity more than BERT base.

1.64 exact match and 1.7 F1 points respectively.

5.2.1 Training settings

BERT for QA consists of a pretrained model along
with classification heads for start and ends of spans.
The model was trained for 2 epochs with a learning
rate of 2 x 10~°. During training, embeddings for
numbers and their entities are learned in parallel.

5.3. FITB

Table 5 shows the performance improvements of
BERTwnueEpR over the BERT base for FITB. The
top-k performance tells us that BERTyuer is
able to predict the correct answer within the top-k
entries. We see that the gap in performance im-
provement increases as k increases. This tells us
that the enriched model is more accurate in its pre-
dictions. The dist metric quantifies the average
absolute distance between pairs of actual value and
the predicted values. This ensures the quality of
numeral predictions. For example, as shown in the
first column of Figure 2, when the ground truth is a
Year, we expect all the other predictions to also be
years (as opposed to Count or other classes). The
dist metric calculates the spread of numbers while
top-k calculates the accuracy itself. We see that the
dist is significantly lower than the baseline model.
Figure 2 shows that BE RT; eR generates top-k

| Top-1 | Top-2 Top-5 Top-10
| Baseline | 36.88 | 52.51 71.57 79.84
| Dist 624 | 1279.06 | 3275.22 | 8240.56
| Ours | 37.69 | 53.9 73.1 | 81.47
| Dist 554 1110 | 2829.73 | 6435.14

Table 5: Predictive performance on the FITB task. All
numbers are tested for significance and have a standard
deviation of 0.01

predictions whose elements are more consistent
with the desired class label.

5.3.1 Training settings

For the FITB experiment, DistiIBERT (Sanh et al.,
2019) with an Adam optimizer and a learning rate
of 2 x 10~* is used. The model is trained for 3
epochs with a batch size of 64. On the architec-
tural side, adding a linear layer to the BERT hidden
states yielded marginal improvements. But, con-
catenating the linear layer responsible for learning
number entity onto the BERT hidden states fol-
lowed by classification lead to significant improve-
ments.

6 Conclusions

We have defined number entities and find that this
aids multiple NLP tasks, via joint training, and clas-


sification. We also introduce a dataset representing
number entities and a methodology to annotate un-
seen numbers. In the classification objective, we
find that the addition of entities provides a consid-
erable boost in FITB evident through the quality of
predictions. Through our datasets and methods, we
hope to incite further research at the intersection of
numeracy and NER.

Acknowledgments

The authors would like to thank Vishwanath Sesha-
giri for helping with the annotations on the SQUAD-
Num dataset.

References

Daniel Andor, Luheng He, Kenton Lee, and Emily
Pitler. 2019. Giving bert a calculator: Finding op-
erations and arguments with reading comprehension.
arXiv preprint arXiv: 1909.00109.

Taylor Berg-Kirkpatrick and Daniel Spokoyny. 2020.
An empirical investigation of contextualized number
prediction. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 4754-4764.

Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura,
and Hsin-Hsi Chen. 2019. Numeracy-600k: learn-
ing numeracy for detecting exaggerated information
in market comments. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 6307-6313.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv: 1810.04805.

Mor Geva, Ankit Gupta, and Jonathan Berant. 2020.
Injecting numerical reasoning skills into language
models. arXiv preprint arXiv:2004.04487.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv: 1603.01360.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Dangi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv: 1907.11692.

Aakanksha Naik, Abhilasha Ravichander, Carolyn
Rose, and Eduard Hovy. 2019. Exploring numeracy
in word embeddings. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 3374-3380.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv: 1606.05250.

Erik F Sang and Fien De Meulder. 2003.  Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. arXiv
preprint cs/0306050.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv: 1910.01108.

Dinghan Shen, Pengyu Cheng, Dhanasekar Sundarara-
man, Xinyuan Zhang, Qian Yang, Meng Tang, Asli
Celikyilmaz, and Lawrence Carin. 2019. Learning
compressed sentence representations for on-device
text processing. arXiv preprint arXiv: 1906.08340.

Georgios P Spithourakis and Sebastian Riedel. 2018.
Numeracy for language models: Evaluating and
improving their ability to predict numbers. arXiv
preprint arXiv: 1805.08154.

Vivek Subramanian and Dhanasekar Sundararaman.
2021. How do lexical semantics affect trans-
lation? an empirical study. arXiv preprint
arXiv:2201.00075.

Dhanasekar Sundararaman, Shijing Si, Vivek Subra-
manian, Guoyin Wang, Devamanyu Hazarika, and
Lawrence Carin. 2020. Methods for numeracy-
preserving word embeddings. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 4742-4753.

Dhanasekar Sundararaman and Vivek Subramanian.
2022. Exploring gender bias in retrieval models.
arXiv preprint arXiv:2208.01755.

Dhanasekar Sundararaman, Vivek Subramanian,
Guoyin Wang, Shijing Si, Dinghan Shen, Dong
Wang, and Lawrence Carin. 2019. Syntax-infused
transformer and bert models for machine translation
and natural language understanding. arXiv preprint
arXiv:1911.06156.

Dhanasekar Sundararaman, Vivek Subramanian,
Guoyin Wang, Shijing Si, Dinghan Shen, Dong
Wang, and Lawrence Carin. 2021a. Syntactic
knowledge-infused transformer and bert models. In
CIKM Workshops.

Dhanasekar Sundararaman, Henry Tsai, Kuang-Huei
Lee, Iulia Turc, and Lawrence Carin. 2021b. Learn-
ing task sampling policy for multitask learning. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 4410-4415.

Ashwani Tanwar, Jingging Zhang, Julia Ive, Vibhor
Gupta, and Yike Guo. 2022. Unsupervised numeri-
cal reasoning to extract phenotypes from clinical text
by leveraging external knowledge. arXiv preprint
arXiv:2204. 10202.


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998-6008.

Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,
and Matt Gardner. 2019. Do nlp models know num-
bers? probing numeracy in embeddings. arXiv
preprint arXiv: 1909.07940.

Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe
Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo
Henao, and Lawrence Carin. 2018. Joint embedding
of words and labels for text classification. arXiv
preprint arXiv: 1805.04174.

Xikun Zhang, Deepak Ramachandran, Ian Tenney,
Yanai Elazar, and Dan Roth. 2020. Do lan-
guage embeddings capture scales? arXiv preprint
arXiv:2010.05345.
