arX1v:2510.10936v1 [cs.CL] 13 Oct 2025

END-TO-END SEQUENCE LABELING VIA BI-DIRECTIONAL
LSTM-CNNS-CRE:
A REPRODUCIBILITY STUDY

Anirudh Ganesh Jayavardhan Reddy
The Ohio State University The Ohio State University
Columbus, OH Columbus, OH
ganesh.48@osu.edu peddamail.1@osu.edu

ABSTRACT

We present a reproducibility study of the state-of-the-art neural architecture for sequence labeling
proposed by Ma and Hovy (2016)[5]. The original BiLSTM-CNN-CRF model combines character-
level representations via Convolutional Neural Networks (CNNs), word-level context modeling
through Bi-directional Long Short-Term Memory networks (BiLSTMs), and structured prediction
using Conditional Random Fields (CRFs). This end-to-end approach eliminates the need for hand-
crafted features while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces the key results,
achieving 91.18% Fl-score on CoNLL-2003 NER and demonstrating the model’s effectiveness across
sequence labeling tasks. We provide a detailed analysis of the architecture components and release an
open-source PyTorch implementation to facilitate further research.

Keywords Machine Learning - Natural Language Processing - Named Entity Recognition - Sequence Labeling -
BiLSTM - CNN - CRF

1 Introduction

Sequence labeling is a fundamental task in natural language processing that involves assigning labels to each token in a
sequence. Traditional approaches to sequence labeling tasks such as named entity recognition (NER) and part-of-speech
(POS) tagging rely heavily on hand-crafted features and domain-specific preprocessing pipelines. These methods are
often labor-intensive, require expert knowledge, and may not generalize well across different domains or languages.

The seminal work by Ma and Hovy (2016) introduced an end-to-end neural architecture that addresses these limitations
by combining three key components. The architecture uses character-level representations learned through Convolutional
Neural Networks (CNNs) to capture morphological information, employs word-level context modeling with Bi-
directional Long Short-Term Memory networks (BiLSTMs), and applies structured prediction via Conditional Random
Fields (CRFs) to ensure coherent label sequences.

This architecture has significantly impacted the NLP community, inspiring numerous follow-up works and establishing
itself as a standard baseline for sequence labeling tasks. The model achieved state-of-the-art results on multiple
benchmarks, attaining 91.21% Fl-score on the CoNLL-2003 NER dataset and 97.55% accuracy on Penn Treebank
WSJ POS tagging.

In this paper, we present a comprehensive reproducibility study of the BILSTM-CNN-CRE architecture. Our work
provides a detailed analysis of each architectural component and its role in the overall system. We present a complete
PyTorch implementation that reproduces the original results and conduct extensive experimental validation on standard
benchmarks. We also discuss implementation details and hyperparameter sensitivity while releasing our open-source
code to facilitate further research.


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

2 Related Work

Traditional sequence labeling approaches typically employ feature-based methods combined with probabilistic models
such as Hidden Markov Models (HMMs) or Conditional Random Fields [4]. These methods require extensive feature
engineering, including orthographic features, dictionaries, and hand-crafted rules.

The introduction of neural approaches began with feedforward networks[1] and evolved to include recurrent architectures.
Huang et al. (2015)(3] first demonstrated the effectiveness of BiLSTM-CRF models for sequence labeling, while Santos
and Zadrozny (2014){[11] showed the utility of character-level CNNs for capturing morphological information.

The BiLSTM-CNN-CRE architecture builds upon these foundations by integrating character and word-level representa-
tions in an end-to-end framework. Subsequent work has extended this architecture with attention mechanisms (Rei et al.,
2016) 10], contextualized embeddings (Peters et al., 2018) [8], and transformer-based models (Devlin et al., 2019)[2].

3 Model Architecture

The BiLSTM-CNN-CRF model consists of three main components that operate in sequence. These components include
character-level CNN encoding, word-level BiLSTM encoding, and CRF-based structured prediction.

3.1 Character-level CNN Representation

For each word, character-level features are extracted using a CNN to capture morphological patterns such as prefixes,
suffixes, and capitalization. Given a word w with characters c1,C2,...,Cm, each character is first embedded into a
d--dimensional vector space using a character embedding matrix E° € R!©!*%, where |C| is the character vocabulary
size.

The character embeddings are then fed into a 1D convolutional layer with kernel size k and 7 filters. For a window of
characters ¢;.;;,%—1, the convolution operation produces:

where W° € R"s*(*4e) and b° € R"/. Max-pooling is applied over the entire word to obtain the final character-level
representation:

r°(w) =

~ 1<igre kd he (2)
3.2 Word-level BiLSTM Encoding

Each word is represented by concatenating its pre-trained word embedding with its character-level CNN representation:

a = [r* (wr); (we) (3)
where r“’ (wz) is the word embedding for word w; and [;] denotes concatenation.

The concatenated representations are fed into a bidirectional LSTM to capture contextual information from both
directions:

hi, = LSTM(a;, hy) (4)

The final hidden representation is obtained by concatenating the forward and backward states:

hy = [his hu] (5)

These hidden states are then passed through a linear layer to produce tag scores:

where s; € R!7! and |T| is the number of possible tags.


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

3.3. CRF Layer

While the BiLSTM produces local tag scores, it doesn’t consider dependencies between consecutive tags. The CRF
layer addresses this by modeling the conditional probability of the entire tag sequence.

For a sentence x = (#1, %2,...,@n) and corresponding tag sequence y = (y1, y2,---, Yn), the CRF defines a global
score:
n n-1
Score(x,y) = 5° silyl] + 55 Tlyi, vita] + bls] + elyn] (7)

i=1 j=l
where s;[¥y;] is the emission score for tag y; at position 7, T[y;, yi+1] is the transition score from tag y; to y;+1, and b, e
are begin and end tag scores.

The conditional probability is then:

exp(Score(z, y))

¥,, exp(Seore(r, 7) sis

P(y|x) =

During training, we maximize the log-likelihood of the correct tag sequence. During inference, we use the Viterbi
algorithm to find the most probable tag sequence.

4 Experimental Setup

4.1 Datasets

We evaluate our model on two standard sequence labeling benchmarks. The CoNLL-2003 NER English dataset contains
four entity types (PER, LOC, ORG, MISC) with 14,987 training sentences, 3,466 development sentences, and 3,684
test sentences. The Penn Treebank WSJ POS dataset contains 45 POS tags with 39,832 training sentences, 1,700
development sentences, and 2,416 test sentences.

4.2 Data Preprocessing

Following the original paper, we apply various preprocessing steps. First, we convert the BIO tagging scheme to BIOES
for finer granularity. We then replace all digits with ’0’ to reduce sparsity. Words are lowercased when controlled by the
hyperparameter setting. Finally, character sequences are padded to the maximum word length for batch processing.

The BIOES scheme extends BIO by adding explicit end (E) and single (S) tags, providing better boundary information
for multi-token entities.

4.3 Model Configuration

Our model configuration matches the original paper. The character embedding dimension is set to 30, while the word
embedding dimension uses 100-dimensional GloVe 6B vectors. The character CNN employs 30 filters with a kernel
size of 3. The BiLSTM hidden dimension is 200 (100 for each direction). We apply a dropout rate of 0.5 and use SGD
optimization with a learning rate of 0.015 and momentum of 0.9. Gradient clipping is applied at 5.0, with a batch size
of 10 and a maximum of 50 training epochs.

4.4 Training Procedure

We train the model using Stochastic Gradient Descent (SGD) with momentum. The negative log-likelihood serves as
the loss function:

£ = —log P(y |x) 9)

Early stopping is applied based on development set Fl-score, with a patience of 10 epochs. Gradient clipping prevents
exploding gradients during training.


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

5 Results

5.1 Named Entity Recognition

Table[I]shows our results on CoNLL-2003 NER compared to the original paper and other baseline methods.

Method F1

CREF 84.04
BiLSTM 88.83
BiLSTM-CRF 90.10
CNN-BiLSTM-CRF 90.94

BiLSTM-CNN-CREF (original) 91.21
BiLSTM-CNN-CREF (ours) 91.18
Table 1: Fl-scores on CoNLL-2003 NER test set

Our implementation achieves 91.18% Fl-score, which closely matches the original 91.21% result. The slight difference
can be attributed to random initialization and implementation details.

5.2 Part-of-Speech Tagging

On Penn Treebank WSJ POS tagging, our model achieves 97.52% accuracy compared to the original 97.55%, confirming
successful reproduction.

5.3 Ablation Study

To understand the contribution of each component, we perform an ablation study:

Configuration Fl

Word embeddings only 85.23
+ Character CNN 89.67
+ BiLSTM 90.83
+ CRF 91.18

Table 2: Ablation study on CoNLL-2003 NER

Each component provides substantial improvements. The CRF layer contributes 0.35 F1 points by ensuring label
consistency.

6 Implementation Details

6.1 Technical Challenges

Multiple implementation challenges arose during reproduction. The original paper does not specify batching details,
leading us to implement dynamic batching with padding for efficient GPU utilization. Our CNN-based approach
for character-level representations required careful padding and masking for variable-length words. The CRF layer
implementation also demanded efficient computation of the partition function using the forward algorithm and Viterbi
decoding for inference.

6.2 PyTorch Implementation

Our PyTorch implementation offers multiple advantages through its modular design that allows easy experimentation.
The implementation includes GPU support for efficient training and provides comprehensive evaluation metrics. We
also provide pretrained model checkpoints along with detailed documentation and examples.

The complete implementation is available athttps://github.com/TheAnig/NER-LSTM-CNN-Pytorch


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

7 Discussion

7.1 Model Analysis

The BiLSTM-CNN-CRE architecture demonstrates notable strengths. The character-level modeling through the CNN
component effectively captures morphological patterns, proving particularly beneficial for handling out-of-vocabulary
words and languages with rich morphology. The contextual encoding via the bidirectional LSTM captures long-range
dependencies and contextual information crucial for disambiguation. The structured prediction capabilities of the CRF
layer ensure globally coherent predictions, preventing impossible tag transitions such as I-PER following I-ORG.

7.2 Limitations

Despite its effectiveness, the model has notable limitations. The computational complexity increases with sequence
length, and the model has limited ability to handle very long sequences due to LSTM constraints. The architecture
requires careful hyperparameter tuning and may struggle with domain transfer without fine-tuning.

7.3 Impact and Future Directions

The BiLSTM-CNN-CRE architecture has significantly influenced subsequent research in sequence labeling. Modern
approaches build upon this foundation by incorporating transformer-based models such as BERT and RoBERTa,
utilizing contextualized embeddings like ELMo and FLAIR, implementing multi-task learning frameworks, and
exploring cross-lingual transfer learning techniques.

8 Reproducibility Considerations

Our reproducibility study highlights important factors that affect result consistency. Minor differences in implementation
details such as initialization, batching, and optimization can significantly affect results, which is why we provide
comprehensive implementation details to aid reproduction. The model exhibits sensitivity to learning rate and dropout
settings, and we include hyperparameter sweep results in our repository to document this behavior. Evaluation
consistency also plays a crucial role, as different evaluation scripts can yield slightly different results; therefore, we use
the standard CoNLL evaluation script for consistency. Finally, hardware dependencies between GPU and CPU training
can introduce minor variations due to numerical precision differences.

9 Conclusion

We have successfully reproduced the BiLSTM-CNN-CRE architecture for sequence labeling, achieving results that
closely match the original paper. Our implementation demonstrates the effectiveness of combining character-level
CNNs, word-level BiLSTMs, and CRF structured prediction in an end-to-end framework.

The model’s strong performance on NER and POS tagging tasks, combined with its conceptual simplicity, explains its
widespread adoption in the NLP community. Our open-source implementation and detailed analysis facilitate further
research and applications in sequence labeling.

Future work may explore integrating modern contextualized embeddings while maintaining the architectural principles
that make this model effective. This reproducibility study underscores the importance of comprehensive implementation
details and evaluation consistency in neural NLP research.

Acknowledgments

We thank the original authors for their groundbreaking work and the open-source community for developing the tools
that made this reproduction possible. We also acknowledge the computational resources provided by our institution.

References

[1] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural
language processing (almost) from scratch. Journal of machine learning research, 12(ARTICLE):2493-2537,
2011.


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pages 4171-4186, 2019.

[3] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint
arXiv:1508.01991, 2015.

[4] John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the eighteenth international conference on machine
learning, pages 282—289. Morgan Kaufmann Publishers Inc., 2001.

[5] Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1064-1074. Association for Computational Linguistics, 2016.

[6] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of
english: The penn treebank. Computational linguistics, 19(2):3 13-330, 1993.

[7] Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages
1532-1543, 2014.

[8] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers), pages 2227-2237, 2018.

[9] Lev Ratinov and Dan Roth. Design challenges and misconceptions in named entity recognition. In Proceedings
of the thirteenth conference on computational natural language learning, pages 147—155. Association for
Computational Linguistics, 2009.

[10] Marek Rei, Gamal KO Crichton, and Sampo Pyysalo. Attending to characters in neural sequence labeling models.
In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical
Papers, pages 309-318, 2016.

[11] Cicero D Santos and Bianca Zadrozny. Learning character-level representations for part-of-speech tagging. In
Proceedings of the 31st International Conference on Machine Learning, pages 1818-1826. PMLR, 2014.

[12] Erik F Tjong Kim Sang and Fien De Meulder. Introduction to the CONLL-2003 shared task: Language-independent
named entity recognition. arXiv preprint cs/0306050, 2003.

[13] Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 173-180.
Association for Computational Linguistics, 2003.


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

Appendix

To aid researchers in replicating the current state-of-the-art, this appendix contains comprehensive documentation and
additional data, allowing for a more complete understanding of the experimental setup.

Data Statistics

CoNLL-2003 NER Dataset

Split Sentences Tokens Entities

Train 14,987 203,621 23,499
Dev 3,466 51,362 5,942
Test 3,684 46,435 5,648

Table 3: CoNLL-2003 dataset statistics

Entity Type Train Dev’ Test Total

PER 6,600 1,842 1,617 10,059
LOC 7,140 1,837 1,668 10,645
ORG 6,321 1,341 1,661 9,323
MISC 3,438 922 702 5,062

Table 4: Entity distribution in CoNLL-2003

Penn Treebank WSJ POS Dataset

Split Sentences Tokens

Train 39,832 950,028
Dev 1,700 40,117
Test 2,416 56,684

Table 5: Penn Treebank WSJ dataset statistics

Implementation Details

Character CNN Implementation

Listing 1: Character CNN layer implementation

class CharCNN(nn. Module ):
def __init__ (self, char_vocab_size , char_emb_dim,
char_hidden_dim , dropout =0.5):
super(CharCNN, self ).__init__()
self .char_emb_dim = char_emb_dim
self.char_hidden_dim = char_hidden_dim

# Character embeddings
self .char_embeds = nn.Embedding(char_vocab_size , char_emb_dim)

# CNN layers with different kernel sizes

self.char_cnn = nn.Conv2d(in_channels=1,
out_channels=char_hidden_dim ,
kernel_size=(3, char_emb_dim),
padding=(2, 0))

self.dropout = nn. Dropout(dropout )


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

def forward(self , chars):
# chars: (batch_size , max_word_len)
char_embeds = self.char_embeds(chars ). unsqueeze(1)
# char_embeds: (batch_size, 1, max_word_len, char_emb_dim)

conv_out = self .char_cnn(char_embeds )
# conv_out: (batch_size, char_hidden_dim, new_len, 1)

# Max pooling over sequence length

pooled = F.max_pool2d(conv_out ,
kernel_size=(conv_out.size(2), 1))

# pooled: (batch_size, char_hidden_dim, 1,

char_repr = pooled.squeeze(-1).squeeze(-1)
# char_repr: (batch_size, char_hidden_dim)

return self .dropout(char_repr)

BiLSTM Implementation

Listing 2: BiLSTM layer implementation

class BiLSTM(nn. Module ):
def __init__(self , input_dim, hidden_dim, num_layers=1,
dropout =0.5):
super (BiLSTM, self ).__init__ ()
self .hidden_dim = hidden_dim
self.num_layers = num_layers

self.lstm = nn.LSTM(input_dim, hidden_dim // 2,
num_layers=num_layers ,
bidirectional=True,
dropout=dropout if num_layers > 1 else 0,
batch_first=True )

self.dropout = nn. Dropout(dropout )

def forward(self , embeddings, lengths ):
# Pack padded sequences
packed = nn. utils .rnn. pack_padded_sequence (
embeddings, lengths , batch_first=True,
enforce_sorted=False )

# BiLSTM forward pass
packed_output, (hidden, cell) = self.Istm(packed)

# Unpack sequences
output, _ = nn.utils.rnn.pad_packed_sequence (

packed_output, batch_first=True)

return self .dropout(output)

CRF Implementation

Listing 3: CRF layer implementation

class CRF(nn. Module ):
def __init__(self , num_tags, batch_first=True):


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

super(CRF, self ).__init__()
self .num_tags = num_tags
self. batch_first = batch_first

# Transition parameters
self.transitions = nn. Parameter(torch.randn(num_tags ,

# Start and end transitions

num_tags))

self.start_transitions = nn. Parameter(torch.randn(num_tags ))
self.end_transitions = nn. Parameter(torch.randn(num_tags ))

self .reset_parameters ()

def reset_parameters (self):

nn.init.uniform_(self.transitions , -0.1, 0.1)
nn.init.uniform_(self.start_transitions , -0.1, 0.1)
nn.init.uniform_(self.end_transitions , -0.1, 0.1)

def forward(self , emissions, tags, mask=None):

"""Compute,_the, conditional log, likelihood _of_tag, sequences

if mask is None:
mask = torch.ones_like(tags , dtype=torch.bool)

if self.batch_first:
emissions = emissions.transpose(0, 1)
tags = tags.transpose(0, 1)
mask = mask.transpose(0, 1)

# Compute normalization constant (partition function)

won

numerator = self._compute_score(emissions , tags , mask)
denominator = self._compute_normalizer(emissions , mask)
return torch.sum(numerator — denominator)

def decode(self , emissions , mask=None ):

"""Bind the most, likely tag, sequence, using, Viterbi, algorithm

if mask is None:
mask = torch.ones(emissions.shape[:2], dtype=torch
device=emissions. device)

if self.batch_first:
emissions = emissions.transpose(0, 1)
mask = mask.transpose(0O, 1)

return self. _viterbi_decode(emissions , mask)

won

.bool,


Enabling Reproducibility in Machine Learning MLTrain @ ICML 2018

Training Algorithm

Algorithm 1 BiLSTM-CNN-CRF Training

Require: Training data D = {(a,y)}%_,
Require: Hyperparameters: learning rate 7, batch size B, epochs E
Initialize model parameters 6
Load pre-trained word embeddings
for epoch = | to EF do
Shuffle training data D
for each batch B; in D do
L=0
for each example (x, y) in B; do
// Character-level representation
r© = CharCNN (‘chars )
// Word-level representation
r’ = WordEmbedding (words)
h = BiLSTM([r”; r°])
// Emission scores
s = Linear(h)
/! CRF loss
£+ = —log P(y|a;@)
end for
£=L/\By|
0=0—NnVol
Clip gradients if ||VoL|| > 5.0
end for
Evaluate on development set
Apply early stopping if no improvement
end for

Code Availability

The complete implementation is available at: https: //github. com/TheAnig/NER-LSTM-CNN-Pytorch

10
