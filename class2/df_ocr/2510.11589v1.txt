2510.11589v1 [cs.IR] 13 Oct 2025

arXiv

QDER: Query-Specific Document and Entity Representations for
Multi-Vector Document Re-Ranking

Shubham Chatterjee
Missouri University of Science and Technology
Department of Computer Science
Rolla, Missouri, United States
shubham.chatterjee@mst.edu

Abstract

Neural IR has advanced through two distinct paths: entity-oriented
approaches leveraging knowledge graphs and multi-vector mod-
els capturing fine-grained semantics. We introduce QDER, a neu-
ral re-ranking model that unifies these approaches by integrating
knowledge graph semantics into a multi-vector model.

QDER’s key innovation lies in its modeling of query-document re-
lationships: rather than computing similarity scores on aggregated
embeddings, we maintain individual token and entity represen-
tations throughout the ranking process, performing aggregation
only at the final scoring stage—an approach we call “late aggrega-
tion.” We first transform these fine-grained representations through
learned attention patterns, then apply carefully chosen mathemati-
cal operations for precise matches. Experiments across five standard
benchmarks show that QDER achieves significant performance gains,
with improvements of 36% in nDCG@20 over the strongest baseline
on TREC Robust 2004 and similar improvements on other datasets.
QDER particularly excels on difficult queries, achieving an nDCG@20
of 0.70 where traditional approaches fail completely (nDCG@20 =
0.0), setting a foundation for future work in entity-aware retrieval.

CCS Concepts

- Information systems — Information retrieval; Document
representation; Retrieval models and ranking;

Keywords
Multi-Vector Entity-Oriented Search; Query-Specific Embedding

ACM Reference Format:

Shubham Chatterjee and Jeff Dalton. 2025. QDER: Query-Specific Document
and Entity Representations for Multi-Vector Document Re-Ranking. In
Proceedings of the 48th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’25), July 13-18, 2025, Padua,
Italy. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3726302.
3730065

1 Introduction

Background. Information retrieval (IR) systems face a fundamental
challenge: effectively modeling the complex semantic relationships
between queries and documents. Traditional retrieval methods rely-
ing on lexical matching often suffer from vocabulary mismatch and

This work is licensed under a Creative Commons Attribution 4.0 International License.
SIGIR °25, Padua, Italy

© 2025 Copyright held by the owner/author(s).

ACM ISBN 979-8-4007-1592-1/2025/07

https://doi.org/10.1145/3726302.3730065

Jeff Dalton
The University of Edinburgh
School of Informatics
Edinburgh, Scotland, United Kingdom
jeff.dalton@ed.ac.uk

fail to capture deeper semantic connections. These challenges are
particularly acute for complex queries that require understanding
specific entity relationships and broader contextual relevance. For
example, answering a query like “What was Steve jobs’ role in the
development of the iPhone?” requires not only recognizing mentions
of “Steve Jobs” and “iPhone” but also accurately interpreting their
relationship, rather than treating them as independent concepts.

One promising direction has been the development of entity-
oriented neural IR models, which enrich text representations with
knowledge from Knowledge Graphs (KGs). The Entity-Duet Rank-
ing Model [34] follows this approach by deriving entity vectors from
KG descriptions and types, and matching them to queries using a
word-entity duet framework [57]. Tran and Yates [51] organized
document entities into clusters, providing multiple perspectives
that improved document understanding and retrieval performance.

In parallel, neural IR has shifted toward multi-vector document
representations that maintain token-level embeddings for fine-
grained matching between query and document components. Mod-
els such as CoIBERT [27] and ME-BERT [35] exemplify this ap-
proach. We hypothesize that multi-vector representations are par-
ticularly beneficial for entity-oriented search, where preserving
distinct entity contexts can significantly improve retrieval accu-
racy. For example, a query like “What was Steve jobs’ role in the
development of the iPhone?” may be better handled by multi-vector
models that separate Jobs’ various roles across companies, whereas
single-vector models risk conflating them.

These advances raise a fundamental research question: Can the
integration of entity representations into multi-vector neural frame-
works further improve retrieval performance? In this work, we ex-
plore this direction, proposing a model that unifies entity-aware
and fine-grained matching capabilities. We focus on the re-ranking
phase of retrieval systems. Given a query Q anda set of candidate
documents D retrieved by an initial retriever (e.g., BM25 [48]), the
goal is to re-rank D according to relevance to Q, using computa-
tionally intensive but more precise matching.

Research Gap. Current neural IR models face two key limita-
tions. First, traditional matching methods based on dot product or
cosine similarity [3, 26, 27, 46, 63] often fail to capture the complex
semantic relationships required for accurate relevance assessment.
We hypothesize that effective retrieval demands modeling how dif-
ferent components of query and document embeddings interact, as
each encodes distinct semantics. Second, most systems rely on static,
query-agnostic document representations [27, 29, 34, 36]. However,
prior work [4, 5] shows that such static embeddings significantly
underperform on nuanced, multifaceted queries.


SIGIR °25, July 13-18, 2025, Padua, Italy

To address these challenges, we propose Query-specific Document
and Entity Representations (QDER), a novel entity-oriented multi-
vector neural re-ranking model.! At its core, QDER treats documents
as collections of tokens and entities, deriving distinct representa-
tions for each. While prior work has analyzed documents through
both textual and entity lenses [34, 56, 57], our key innovation lies
in making these representations query-specific.

QDER employs two complementary mechanisms to generate query-
specific representations. First, it uses attention to dynamically
reweight document tokens and entities based on their relevance
to the query, transforming static document representations into
query-focused ones. Second, building on research showing that
elements within relevant documents are themselves often relevant
[11, 42], we incorporate initial retrieval scores to further refine
these representations. Together, these mechanisms highlight the
document elements most pertinent to the query while mitigating
noise from imperfect entity linking. Through this integration of
fine-grained multi-vector modeling, entity knowledge, and query-
specific dynamic refinement, QDER establishes a new state-of-the-art
for entity-aware neural re-ranking.

Our model introduces several key innovations:

e Late Aggregation: Building on the late interaction idea from
CoIBERT [27], we propose late aggregation. We preserve and
jointly model token and entity representations throughout the
ranking process, and aggregate them only at final scoring.

Attention-guided Query-specific Representations: We ex-

tend the late interaction paradigm by introducing entity-aware
attention that dynamically reshapes token and entity represen-
tations based on query context. These refined representations
undergo addition and multiplication with query embeddings to
model complementary and exact semantic matches. Unlike pre-
vious work that relied solely on token matching or static entity
characteristics [34, 51, 58], our unified framework preserves and
jointly models fine-grained token and entity signals throughout
the ranking process.

Bilinear Interaction Modeling: To effectively combine these
rich interaction signals, we replace traditional similarity met-
rics with bilinear projections, addressing the limitations of dot

product and cosine similarity scoring used in prior work.

e Noise-robust Entity Incorporation: Entity linking is inher-
ently noisy, but prior work [34, 57] shows that attention can
mitigate this by emphasizing query-relevant entities. Building
on this insight, QDER uses query-guided attention to dynamically
refine token and entity representations, promoting relevance and
suppressing linking errors.

This paper makes the following contributions:

e We propose QDER, a multi-vector neural re-ranking architecture
that uses dual text and entity channels with attention-guided
interaction modeling to create query-specific document repre-
sentations. QDER significantly outperforms existing methods on
multiple benchmark datasets.

e We provide compelling evidence challenging the prevalent use
of static embeddings in neural IR. Our experiments show that
query-specific embeddings (QDER) dramatically outperform static

1Github: https://github.com/shubham526/SIGIR2025-QDER.

Shubham Chatterjee and Jeff Dalton

embeddings (SentenceBERT) in both clustering quality and rank-
ing performance, suggesting a fundamental need to shift away
from pre-computed document representations.

e We show that Addition and Multiplication operations are nearly
statistically independent in capturing query-document relation-
ships and exhibit superior noise robustness compared to Subtrac-
tion, offering valuable insights for future architecture design.

e We show that entity-attention mechanisms are vital for diffi-
cult queries, with QDER achieving an nDCG@20 of 0.70 where
traditional methods fail (nDCG@20 = 0.0). This stems from its
ability to dynamically prioritize relevant entities across multiple
dimensions while filtering out irrelevant ones.

2 Related Work

Entity-Oriented Search. The field of entity-oriented search has
evolved significantly over time. Initially, researchers like Meij et al.
[38] and Dalton et al. [9] focused on using entities for query expan-
sion. The field then progressed to treating entities as a latent layer,
with notable works including Explicit Semantic Analysis [14] and
the Latent Entity Space model [32]. EsdRank [54] further advanced
this approach by creating entity-based connections between queries
and documents. The next wave of research integrated entities as
explicit elements in retrieval models. Raviv et al. [45] and Ensan and
Bagheri [12] developed entity-based language models, while Xiong
et al. [54-61] pioneered the "bag-of-entities" approach alongside
traditional "bag-of-words" methods.

Single-Vector Neural IR. In neural IR, pre-BERT developments
fell into two categories: representation-based models [20, 39, 40, 49]
and interaction-based models [8, 15, 21, 22, 58]. The introduction of
BERT [10] and its variants [6, 16, 24, 33, 66] revolutionized the field,
leading to innovations like Birch [1], BERT-MaxP [7], CEDR [36],
and PARADE [30]. The Transformer Kernel model [18, 19], later
enhanced with local self-attention [17], represented another sig-
nificant architectural innovation. Bi-encoders emerged as a crucial
development, with DPR [26] and ANCE [62] leading the way. To ad-
dress bi-encoders’ limitations in capturing term-level interactions,
ColBERT [27] introduced “late interactions,’ while TCT-ColIBERT
[31] explored knowledge distillation. There is a growing shift to-
ward leveraging embeddings for pseudo-relevance feedback.

CEQE [41] selects expansion terms from top-ranked feedback
documents that are closest to the query in the BERT embedding
space. CoIBERT-PRF [53] uses BERT embeddings for retrieval, thus
avoiding topic drifts for polysemous words.

Multi-Vector Neural IR. Recent neural IR advancements lever-
age multi-vector representations to capture rich query-document
interactions beyond single-vector models. Humeau et al. [23] in-
troduced the poly-encoder, which generates multiple document
representations using learned “context codes” and aggregates them
via an attention mechanism with the query vector for relevance
scoring. Luan et al. [35] proposed ME-BERT, where the document
encoder produces m representations from the first m tokens. The
final score is the maximum inner product between the query vector
and these m representations, enabling efficient nearest neighbor
search. Khattab and Zaharia [27] extended this idea with ColBERT,
which creates dense token-level representations for documents. Its
“late interaction” mechanism uses the MaxSim operator to compute


QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking

relevance by aggregating the maximum cosine similarity between
query and document tokens, allowing for fine-grained matching
with scalable indexing techniques.

3 Approach

The following sections detail our approach: Section 3.1 outlines
the framework’s key concepts and guiding hypotheses, while Sec-
tion 3.2 describes the model architecture, scoring mechanisms, and
integration strategies for effective relevance modeling.

3.1 QDER: Conceptual Foundation

At its core, QDER introduces a novel paradigm for document re-
ranking, called Attention-Guided Multi-Granular Interaction
Decomposition. It is based on the hypothesis that document rele-
vance stems from two semantic relationships: direct alignment and
complementary context. For example, a query like “Steve Jobs’ role
in the development of the iPhone” requires capturing both explicit
matches (e.g., “iPhone” development) and related context (e.g., Jobs’
leadership at Apple), which together shape relevance.

We hypothesize that learned attention patterns can help iden-
tify and prioritize semantically relevant aspects of both queries
and documents, leading to improved retrieval performance. A core
innovation of QDER is its “late aggregation” approach: Instead of col-
lapsing token and entity information early, we maintain individual
representations throughout the matching process and aggregate
them only at final scoring. Unlike prior work [5, 47] that applies
interaction operations to static or aggregated embeddings, QDER
uses attention-weighted embeddings, allowing the query context
to dynamically emphasize the most relevant parts of the document.
This attention-guided refinement transforms raw token and entity
embeddings into query-specific representations tailored to capture
complex semantic relationships.

To effectively model these relationships, QDER employs a dual-
channel architecture that captures complementary semantic pat-
terns at different granularities: A text-based channel that focuses
on fine-grained linguistic alignment and complementarity, and an
entity-aware channel that models higher-level semantic relation-
ships. Within each channel, attention scores act as dynamic gates,
reshaping document representations to emphasize aspects most
relevant to the query. Based on our analysis of attention patterns,
we observe that in the text channel, higher attention weights tend
to correspond with document tokens that appear relevant to query
terms. Similarly, in the entity channel, our experiments suggest
attention-based adjustments help identify entities that may have
semantic relationships with query concepts.

After attention-based matching, the model captures rich interac-
tion patterns using two fundamental operations carefully chosen
to model different aspects of relevance: (1) Semantic alignment
(via multiplication) identifies precise matches between query and
document aspects, and (2) Semantic complementarity (via addition)
reveals how document content enriches and expands upon the
query intent. As these operations are applied to attention-weighted
embeddings, they capture more nuanced relationships than tradi-
tional static approaches. For instance, multiplication on attention-
weighted representations can identify semantic alignments that are
specifically relevant to the query’s information need, rather than
generic term matches. Similarly, addition operates on embeddings

SIGIR °25, July 13-18, 2025, Padua, Italy

that already emphasize query-relevant aspects, enabling the model
to measure complementarity in the context of the query’s specific
intent. This attention-guided approach to interaction modeling en-
sures that each operation focuses on aspects that matter most for
relevance assessment. We hypothesize that interactions on query-
specific embeddings would enable QDER to better capture alignment
and complementarity, essential for understanding document-query
relationships. Unlike methods relying on static embeddings, QDER
dynamically reshapes embeddings with attention and targeted in-
teractions, ensuring relevance is tailored to each query.

3.2 QDER: Technical Architecture

3.2.1. Multi-Channel Representation Processing. To capture both
linguistic and semantic signals from documents, QDER processes
information through two parallel channels:

Text Channel: The text channel captures fine-grained linguistic
patterns through contextualized embeddings:

¢ Query text representation: Q' = Encoder(q) € R'a*”
e Document text representation: D’ = Encoder(d) € Rlaxde

where J, and Jy denote sequence lengths, and d, is the embedding
dimension. We use BERT [10] as encoder here.

Entity Channel: Operating in parallel, the entity channel pro-
cesses higher-level semantic concepts:

Q* € R"@*4e and D® € R"4*4e (1)

where ng, ng are entity counts and d, is the embedding dimension.
To obtain query entities, we follow prior work [38]: we pool entities
from candidate documents to form a set &. We transfer the rele-
vance labels from documents to their entities, assuming relevant
documents contain relevant entities [42]. We then train a BERT-
based entity ranker using entity descriptions from DBpedia to map
entities in & to queries. Document entities are obtained using entity

linking.

3.2.2. Dynamic Attention-Guided Interaction. A key innovation in
QDER is its attention-first approach to modeling interactions. Tra-
ditional methods often compute interactions on query-agnostic
(static) representations, potentially diluting important matching
signals. Instead, QDER first establishes attention-based matching to
identify critical alignments between query and document elements.

Our working hypothesis is that attention weights may correlate
with relevance, with higher attention values potentially indicating
tokens or entities that are more relevant to the query. Section 5.4
provides empirical analysis of this relationship. By aligning query
and document elements through learned attention patterns, QDER
ensures that subsequent interaction modeling focuses on the most
semantically relevant components.

Text Channel Attention: We compute attention weights that
highlight relevant document tokens for each query token:

A! = softmax(Q'D‘") € R'a*/a (2)
D' =A'D! (3)
Entity Channel Attention: Similarly for entities:
A® = softmax(Q°D°") € R'9*"4 (4)
De = A°D® (5)


SIGIR °25, July 13-18, 2025, Padua, Italy

3.2.3 Multi-Granular Interaction Modeling. After obtaining attention-
weighted representations, QDER models two fundamental types of
semantic relationships that together provide a comprehensive view
of query-document relevance:

1. Semantic Alignment (via element-wise multiplication):

M =Q' © D! and M® = O° 0 D® (6)
Multiplication emphasizes areas of strong alignment, highlighting
strong semantic similarities. This operation is particularly effective
at identifying documents that precisely address specific query as-
pects, capturing both token-level matches in the text channel and

entity-level correspondence in the entity channel.

2. Semantic Complementarity (via addition operations):

Ci =Q' + D' and C’ = Q° + D® (7)
Addition captures how query and document representations com-
plement each other, revealing overlapping themes and shared con-
texts. This operation is especially valuable for identifying docu-
ments that provide additional relevant context beyond exact matches,
helping to model topical relevance and thematic alignment.

Our focus on multiplication and addition operations stems from
their complementary roles in capturing relevance signals. Exten-
sive experimentation (Section 5.2) shows that these two operations
effectively capture semantic similarity, achieving optimal document
ranking performance.

3.2.4 Relevance Signal Integration. QDER combines multi-granular
interaction patterns with external relevance signals to create a
unified and effective scoring mechanism. We aggregate each in-
teraction pattern through mean pooling (hi = MeanPool(*) for
* € {C', M‘} and h€ = MeanPool(«) for * € {C®, M°}), and enhance
them with external relevance scores (At =s-h!' and he =s-hg),
where s represents an external score like BM25. Our experiments
suggest this approach creates more discriminative query-specific
representations. By incorporating traditional retrieval scores as
scaling factors, we aim to create query-focused embeddings that
potentially better reflect the query’s information need. Our ex-
periments in Section 5.3 suggest this approach helps create more
discriminative query-specific representations, though the precise
mechanism requires further investigation.

3.2.5 Cross-Pattern Interaction Scoring. Consider the query: “Steve
Jobs’ role in the development of the iPhone.” Documents may show
strong entity alignment (Jobs without iPhone), strong text align-
ment (iPhone without Jobs), or ideally, both. While element-wise
operations in QDER can capture individual interactions, they can-
not determine how best to combine these signals. For instance,
when should broad topical relevance outweigh specific matches?
We hypothesize that modeling relationships between interaction
patterns enables the model to prioritize balanced evidence and rank
documents with complementary text and entity strengths higher.
We propose a two-stage interaction approach distinct from tra-
ditional methods that use direct query-document matching (e.g.,
cosine similarity [26, 46]). QDER first computes meaningful interac-
tion patterns through element-wise operations, then learns optimal
combinations for ranking. Given the feature vector concatenating
the previously defined alignment and complementarity patterns:

= [Pigs hes Mins he]

Shubham Chatterjee and Jeff Dalton

where hf, h¢, capture precise matching through multiplication in
text and entity space, and Af, h£ model broader contextual relation-
ships through addition, we compute the final relevance score using

a bilinear interaction:
dd
score = h"Mh = » hiMy jh; (8)

i=1 j=l

Here, M € R?* is a learnable interaction matrix modeling cross-
pattern relationships. For example, M;,3 might capture how text
and entity alignments reinforce each other, while M24 could learn
how complementarity patterns indicate comprehensive coverage.
This approach enables QDER to learn sophisticated relevance strate-
gies that adapt to different query types, learning optimal pattern
combinations directly from data rather than using predefined rules.

3.2.6 End-to-End Training. QDER is trained end-to-end using binary
cross-entropy loss:
1x

b= — 5 D[uilog@o + (1 - y) log — 90] (9)
where y; is the ground truth relevance label and 9%; is the predicted
relevance probability. The entire model is optimized end-to-end
using back-propagation. This unified training approach allows the
model to learn optimal attention patterns and interaction weights
simultaneously, ensuring that all components work together effec-
tively to assess document relevance.

3.2.7 Final Hybrid Scoring. To further enhance retrieval perfor-
mance, following previous work [43, 65], we combine the strengths
of lexical matching provided by BM25 with the semantic richness of
QDER scores through a linear interpolation strategy. Given a query
q and a document d, the final hybrid score is computed as:

scorehybrid(q, d) = A+ scoregmas(q, d) + (1 — A) - scoregner(q, d)

where scoregy25(q,d) is the relevance score assigned by BM25,
scoregper(q, d) is the relevance score computed by QDER, and A €
(0, 1] is a parameter that controls the relative contribution of the
two components. We learn A using Coordinate Ascent optimized
for Mean Average Precision.

3.2.8 Complementary Integration of Relevance Signals. While QDER
uses BM25 scores in both the relevance signal integration (Section
3.2.4) and final hybrid scoring (Section 3.2.7), these serve distinct
roles. In the integration phase, BM25 scores weight interaction
patterns, enhancing relevance signals in the learned representa-
tions to guide semantic matching. In contrast, the hybrid scoring
phase combines QDER’s semantic matching with BM25’s exact lexi-
cal matching, capturing both semantic and lexical relationships.

4 Experimental Methodology

4.1 Datasets

We evaluate the efficacy of our approach using the following large-
scale, established benchmarks for document and passage ranking.
Our evaluation spans diverse query sets, two distinct retrieval tasks—
document ranking and passage ranking—and two types of collec-
tions: news articles and complex answer topics.


QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking

CODEC. This benchmark [37] is designed for complex research
topics in social sciences. It includes 42 topics, and an entity linked
corpus with 729,824 documents with focused content across finance,
history, and politics. The corpus contains an average of 159 entities
per document. It provides expert judgments on 6,186 documents
derived from diverse automatic and manual runs.

TREC Complex Answer Retrieval (CAR) 2017. The dataset [11]
is derived from Wikipedia. The queries correspond to article head-
ings and subheadings. In this work, we focus on the page-level
queries, i.e., the title of the Wikipedia page as the query and ad-
dress the paragraph ranking task. The documents are paragraphs
extracted from Wikipedia articles. We use the BenchmarkY1-Train
subset from the TREC CAR v2.1 data release. This subset is based
on a Wikipedia dump from 2016. The ground truth is automatic
with 117 page-level queries, and 4862 positive passage assessments.

TREC Robust 2004. The TREC Robust 2004 track [52] focuses
on poorly performing topics. The track provides 250 topics with
short “titles” and longer “descriptions”; we report results on both
title and description queries. The collection consists of 528,024 doc-
uments (containing an average of 116 entities per document) taken
from TREC disks 4 and 5 excluding the Congressional Record. The
track provides 311,409 graded relevance judgments for evaluation.

TREC News 2021. The TREC News track [50] focuses on search
tasks within the news domain. We focus on the background linking
task, which involves retrieving news articles that provide relevant
context or background information for a given news story. There
are 51 topics, each with a title, description, and narrative; in this
work, we use all three fields for query formulation. The track uses
the TREC Washington Post (v4) collection, encompassing 728,626
documents containing 131 entities per document on average. The
track provides 12,908 graded relevance assessments for evaluation.

TREC Core 2018. The track [2] offers 50 topics, each with titles,
descriptions, and narratives. For this work, we utilize all three
components. It uses the TREC Washington Post (v2) collection,
encompassing 595,037 documents with 123 entities per document
on average. 26,233 graded relevance judgements are available.

4.2 Evaluation Paradigm

Candidate Ranking. We retrieve a candidate set of 1000 docu-
ments per query using BM25+RM3 (Pyserini default). Evaluation
Metrics. (1) Precision at k = 20, (2) Normalized Discounted Cumu-
lative Gain (nDCG) at k = 20, (3) Mean Average Precision (MAP),
and (4) Mean Reciprocal Rank (MRR). We use the official trec_eval
tool from NIST (with the -c flag) to evaluate each system. We con-
duct significance testing using paired-t-tests.

Entity Embeddings. We use pre-trained Wikipedia2Vec [64]
entity embeddings provided by Kamphuis et al. [25].

Entity Linking. We use WAT [44] entity linker in this work.

Train and Test Data. As positive examples during training, we
use documents that are assessed as relevant in the ground truth
provided with the dataset. Following the standard [26], for neg-
ative examples, we use documents from the candidate ranking
(BM25+RM3) which are either explicitly annotated as negative or
not present in the ground truth. We balance the training data by
keeping the number of negative examples the same as the number
of positive examples. These examples are then divided into 5-folds
for cross-validation. We create these folds at the query level.

SIGIR °25, July 13-18, 2025, Padua, Italy

Baselines. We compare our proposed re-ranking approach QDER
to the following supervised state-of-the-art neural re-rankers: (1)
RoBERTa [33], (2) DeBERTa [16], (3) ELECTRA [6], (4) Con-
vBERT [24], (5) RankT5 [67], (6) ERNIE [66], (7) EDRM [34].
We include the following multi-vector models for re-ranking:
(1) CoIBERT [27], (2) ME-BERT [35], and (3) PolyEncoders [23].
On TREC Robust 2004, we include the following additional (full-
retrieval) baselines: (1) CEDR [36], (2) EQFE [9], (3) SPLADE [13],
and (4) ANCE-MaxP [62] All baselines are fine-tuned on the target
datasets via 5-fold cross-validation using the binary cross-entropy.

4.3 Implementation Details

We implemented our model using PyTorch and the HuggingFace
Transformers library, leveraging the bert-base-uncased model as
the base encoder with a maximum sequence length of 512 tokens.
The model was fine-tuned using the CrossEntropyLoss function
provided by PyTorch. For optimization, we employed the Adam
optimizer [28] with a learning rate of 2e — 5 and a linear warmup
schedule over the first 1,000 steps. The training process used a
batch size of 20 and was carried out for 10 epochs, with the model
evaluated after each epoch. Early stopping was implemented based
on the validation MAP score, calculated using pytrec_eval, and
the best-performing checkpoint was saved. To enhance inference
efficiency, document embeddings and entity links were precom-
puted and cached offline. All experiments were conducted on a
single NVIDIA A6000 Ada GPU with 48GB of memory.

5 Results and Analysis

In this section, we present the results of comprehensive experiments
evaluating the effectiveness of QDER in document re-ranking. We be-
gin by summarizing the overall performance of QDER on the datasets
described in Section 4.1. Detailed experiments exploring specific
aspects of QDER’s performance are then presented in Sections 5.1
to 5.5. Unless otherwise specified, all analyses is conducted
on the TREC Robust 2004 dataset using title queries due to
lack of space.

5.1 Overall Results

From Tables 1 through 3, we observe that QDER outperforms all
baseline models across all evaluation metrics and datasets. For in-
stance, on the TREC Robust 2004 collection with title queries, QDER
achieves an nDCG@20 of 0.75, representing a significant 36% im-
provement over the strongest baseline, CEDR (aDCG@20 = 0.55). To
contextualize this advancement, CEDR improved the nDCG@20 of
the initial BM25+RM3 candidate set by 25% (0.44 to 0.55). In contrast,
QDER achieves a remarkable 70% improvement (0.44 to 0.75).
Query-Level Analysis. To further explore these substantial
gains, we conduct a fine-grained query-level analysis. By stratify-
ing queries based on their difficulty (measured by BM25+RM3 perfor-
mance), we uncovered a particularly notable insight: QDER excels in
addressing the most challenging queries, where improvements are
most critical. This is illustrated in Figure 1. The most pronounced
improvements are observed in the bottom quartile (0-25%), with
the most dramatic gains in the extremely challenging 0-5% bin.
For these queries, where traditional methods fail to place any rel-
evant documents in the top-20 positions (aDCG@20 = 0.0), QDER


SIGIR °25, July 13-18, 2025, Padua, Italy

Shubham Chatterjee and Jeff Dalton

Table 1: Overall results TREC Robust 2004. Candidate Ranking (BM25) on the top. Best results in bold. 4 denotes significant
improvement and Y denotes significant deterioration compared to * using a paired-t-test at p < 0.05.

TREC Robust 2004 (Title)

TREC Robust 2004 (Description)

MAP nDCG@20 P@20 MRR | MAP nDCG@20 P@20 MRR
BM25* 0.2915 0.4354 0.3839 0.6693 0.2779 0.4247 0.3681 0.6607
RankT5 (Enc) 0.30284 0.49354 0.42934 0.74554 0.3271a 0.53984 0.45584 0.81774
BERT 0.29664 0.4789a 0.40944 0.73864 0.3190 0.52324 0.45004 0.78714
RoBERTa 0.2899¥ 0.47364 0.41004 0.73104 0.3341a 0.5411a 0.46024 0.81364
DeBERTa 0.29314 0.48564 0.42174 0.7379 0.3411a 0.54654 0.46734 0.80584
ELECTRA 0.2680¥ 0.44614 0.38694 0.68824 0.2924 0.49044 0.41414 0.76544
ConvBERT 0.32124 0.51854 0.45064 0.76484 0.3498 a 0.56654 0.48394 0.83174
ColIBERT v2 0.2915 0.47304 0.41024 0.70954 0.28804 0.48064 0.41204 0.74464
ME-BERT 0.07744 0.0946¥ 0.09764 0.16604 0.05754 0.0749¥ 0.07334 0.14764
PolyEncoder 0.07534 0.0984¥ 0.0974¥ 0.20034 0.0623¥ 0.0820¥ 0.0825¥ 0.1858¥
BERT-MaxP 0.31744 0.48534 0.42054 0.7331 0.31494 0.49164 0.41754 0.7697
CEDR 0.37014 0.54754 0.47694 0.7879 0.40004 0.59834 0.51854 0.84574
SPLADE 0.2243¥ 0.4202 0.3588V 0.68274 0.2314¥ 0.4309 0.3600¥ 0.7319
ANCE-MaxP 0.1325¥ 0.30874 0.2496¥ 0.6095¥
ERNIE 0.28949 0.47534 0.41164 0.71354 0.32784 0.53734 0.45464 0.80584
EDRM 0.06714 0.1028¥ 0.0940¥ 0.2413¥ 0.04864 0.06874 0.0679¥ 0.1721¥
EQFE 0.3278 0.4307¥ 0.37970 0.6499¥
ExactMatch 0.20304 0.4759a 0.39884 0.91524 0.20294 0.46174 0.38134 0.4617¥
TREC Best 0.33384 0.42754 0.33384 0.42754
QDER 0.60824 0.76944 0.73614 0.97514 | 0.58554 0.75164 0.71204 0.97274

Table 2: Overall results on TREC News 2021 and TREC Core 2018. Only best performing baselines shown.

TREC Core 2018

TREC News 2021

MAP nDCG@20 P@20 MRR | MAP nDCG@20 P@20 MRR
BM25* 0.3151 0.4470 0.4590 0.6518 0.4680 0.4785 0.5775 0.7767
RankT5 (Enc) 0.2163¥ 0.3255¥ 0.3340¥ 0.5182 0.27124 0.3124 0.3608¥ 0.6288¥
BERT 0.3024¥ 0.46864 0.46104 0.72714 0.46114 0.51124 0.57844 0.85184
RoBERTa 0.2596¥ 0.3612 0.3880¥ 0.4969¥ 0.4721a 0.52154 0.58434 0.89714
DeBERTa 0.34614 0.51944 0.50704 0.73854 0.4318V 0.4742” 0.5578¥ 0.78674
ELECTRA 0.2400¥ 0.3531¥ 0.3610¥ 0.4962¥ 0.4129¥ 0.4619¥ 0.52944 0.84864
ConvBERT 0.32144 0.49614 0.48904 0.7599a 0.4441¥ 0.4738¥ 0.55000 0.78724
ColBERT v2 0.26694 0.45524 0.4510¥ 0.67604 0.4039¥ 0.4624 0.5353¥ 0.79314
ERNIE 0.3401a 0.51954 0.50704 0.77044 0.48084 0.52844 0.60004 0.87364
EDRM 0.0924¥ 0.0984¥ 0.1343¥ 0.3283 0.0924¥ 0.0984” 0.1343¥ 0.3283
ExactMatch 0.1540¥ 0.3482 0.3400¥ 0.72734 0.1827¥ 0.2793¥ 0.34220 0.8879
TREC Best 0.43034 0.6111a 0.4319¥ 0.4688¥ 0.59024
QDER 0.54494 0.65624 0.6894 0.88244 | 0.68214 0.58024 0.7484 0.92994

achieves an impressive nDCG@20 of 0.70, doubling CEDR’s perfor-
mance (nDCG@20 = 0.35). QDER improves performance for 211 of
250 queries, compared to 160 by CEDR, demonstrating its robust-
ness across query difficulty levels. Notably, it excels on both “easy”
queries and the most “difficult” ones, making it highly effective for
real-world applications with diverse query challenges.

Case Study. A case study further highlights QDER’s effectiveness.
Consider the query “Transportation Tunnel Disasters,” an example
from the most challenging 0-5% bin. Notably, NIST has classified
this query as a Category 5 difficult topic, defined as one requiring
multiple aspects that systems often handle inconsistently [52]. QDER
demonstrates exceptional capability in surfacing relevant content
for this difficult query: It promotes a highly relevant document
(as assessed by NIST) from position 734 in the BM25+RM3 ranking

to position 6, significantly outperforming CEDR’s improvement to
position 85. We will revisit this case and analyze in greater depth
the mechanisms behind this performance later in Section 5.4.

5.2 Role of Interaction Operations

The effectiveness of QDER fundamentally depends on how its inter-
action operations transform document representations in response
to query context. Our analysis, grounded in comprehensive ablation
studies and stability experiments, reveals key insights into why
certain operations outperform others, providing both empirical
evidence and theoretical validation for their roles.

Overall Results. Ablation studies (Table 4) show that using
only Addition and Multiplication operations (No-Subtract) yields


QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking

SIGIR °25, July 13-18, 2025, Padua, Italy

Table 3: Overall results on TREC CAR and CODEC. Only best performing baselines shown.

TREC CAR Y1Train (Page-Level) CODEC

MAP nDCG@20 P@20 MRR MAP nDCG@20 P@20 MRR
BM25* 0.1586 0.2812 0.2415 0.5637 0.3627 0.3795 0.4321 0.6774
RankT5 (Enc) 0.22254 0.51284 0.40134 0.88954] 0.38044 0.43744 0.45244 0.81544
BERT 0.2919 0.48634 0.41204 0.8117A| 0.38164 0.44004 0.46314 0.81634
RoBERTa 0.29194 0.48724 0.41154 0.81194] 0.35940 0.3773¥ 0.43334 0.70434
DeBERTa 0.30634 0.51254 0.42994 0.8411a| 0.38974 0.44884 0.46314 0.7698
ELECTRA 0.26174 0.44744 0.37054 0.78484 | 0.32324 0.3174¥ 0.3845¥ 0.5878
ConvBERT 0.31734 0.52034 0.43554 0.85974] 0.38164 0.44074 0.46194 0.76704
CoIBERT v2 0.27454 0.45384 0.38594 0.79624 | 0.37454 0.44604 0.45604 0.79214
ERNIE 0.30174 0.49674 0.42014 0.76974 | 0.39064 0.45744 0.47384 0.79524
EDRM 0.01764 0.0182¥ 0.01840 0.0634¥ | 0.2981¥ 0.2892¥ 0.3524¥ 0.6211
ExactMatch 0.1463¥ 0.39034 0.28164 0.80024 | 0.26744 0.41204 0.46194 0.89684
QDER 0.36384 0.58194 0.49404 0.87684 | 0.52104 0.50354 0.58104 0.95224

1.0- Runs

oes BM25+RM3
s° “o° s e s
& ae ae oe ae
3 Bs s o

CEDR
jem ConvBERT
Difficulty Percentile

jem ROBERTa
RankTS
ERNIE
DER
s°

RS
sie
S$

nDCG@20

Figure 1: Difficulty test on Robust04 title queries. 5% most
difficult queries for BM25+RM3 to the left and the 5% easiest
ones to the right. Performance reported as macro-averages
across queries. For the most difficult queries (0-5%), relevant
documents are promoted to the top of the ranking by QDER.

Table 4: Ablation Study for Interaction Operations in QDER.
Results reported on TREC Robust 2004 (Title).

Configuration MAP nDCG@20 P@20 MRR
No-Add 0.5289 0.7347 0.7008 0.9630
No-Multiply 0.5492 0.7433 0.7090 0.9594
Only-Add 0.4703 0.6682 0.6289 0.9104
Only-Subtract 0.5230 0.7202 0.6855 0.9559
Only-Multiply 0.5325 0.7275 0.6892 0.9726
No-Interactions 0.2593 0.3420 0.5011 0.3239
All-Interactions 0.5091 0.7176 0.6807 0.9598
No-Subtract (Ours) 0.5667 0.7610 0.7313 0.9714

the best performance (nDCG@20 = 0.76), surpassing No-Add (0.73),
No-Multiply (0.74), and single-operation variants. Removing all

operations drastically reduces performance (nDCG@20 = 0.34),
confirming their importance for query-document matching.

Correlation Analysis. The superiority of the Add-Multiply
combination motivates a deeper investigation into the unique roles
these operations play. Our correlation analysis reveals that Addi-
tion and Multiplication exhibit near-zero (Spearman’s) correlation
(—0.022), suggesting they capture orthogonal and largely indepen-
dent aspects of query-document relationships.

Addition combines query and document features directly, effec-
tively identifying exact matches between query terms and docu-
ment terms. For example, when a user searches for “python pro-
gramming”, Addition ensures that documents explicitly containing
these terms are prioritized. Multiplication, on the other hand, serves
as an importance-weighting mechanism, capturing interaction ef-
fects between terms. In a query like “python web development”, a
document containing “Django web framework” might be more rele-
vant than one containing just “python” and “web” separately, even
though both contain the query terms. Together, these operations
allow QDER to balance the identification of relevant terms with the
nuanced understanding of their interactions.

Noise Sensitivity Analysis. While the Subtraction operation
might initially appear complementary, with weak negative correla-
tion to Addition (—0.218) and negligible correlation to Multiplica-
tion (0.011), our analysis reveals its redundancy. We conducted a
systematic noise sensitivity analysis by injecting controlled Gauss-
ian noise (o ranging from 0.001 to 0.1) into the input embeddings.
We measured three key stability metrics: (1) Angular deviation:
How much the embeddings’ directions changed (in degrees) when
noise was added, (2) Noise amplification ratio: How much the oper-
ation magnified the input noise, calculated as the ratio of output
noise to input noise, and (3) Ranking stability: How well the relative
ordering of documents was preserved under noise, measured using
Kendall’s rt correlation.

This analysis reveals stark contrasts in operational stability. Ad-
dition and Multiplication demonstrate remarkable robustness, with
minimal angular deviations (2.55° and 1.66° respectively) and con-
trolled noise amplification ratios (1.85 and 1.89). Subtraction, how-
ever, exhibits pathological behavior with a 9.97° mean angular devi-
ation—nearly six times higher than Multiplication—and an excessive


SIGIR °25, July 13-18, 2025, Padua, Italy

noise amplification ratio of 5.56. Although Subtraction achieves the
highest ranking stability (r = 0.87) compared to Addition (rt = 0.63)
and Multiplication (r = 0.77), this result must be interpreted along-
side its instability. The high r stems from Subtraction consistently
exaggerating existing patterns rather than preserving meaningful
and robust feature relationships, making its stability counterpro-
ductive to effective query-document matching.

5.3 Query-Specific Embeddings

To investigate the impact of attention-guided interactions on docu-
ment representation quality, we compare query-dependent embed-
dings produced by QDER with static document embeddings produced
by SentenceBERT [47] (SBERT), a widely used embedding model
known for its strong performance in IR. Our evaluation focuses on
two key aspects: clustering and ranking performance.

Clustering Analysis. First, we examine relevance-based clus-
tering, which groups documents according to their relevance to
specific queries. The quantitative metrics from this analysis (Figure
2) show QDER achieves significantly better Davies-Bouldin Index
(DBI) (3.36 vs. 24.99) and Silhouette Coefficient (SC) (0.31 vs. 0.002)
compared to SBERT, indicating tighter and better-separated rele-
vance clusters. Second, we analyze topic-wise clustering, where doc-
uments are grouped based on their NIST topic assignments. Here,
we observe an intriguing pattern that validates our query-specific
approach: while QDER shows higher cluster overlap (reflected in
topic-wise DBI of 9.19 vs 4.94 and SC of -0.09 vs 0.03), it achieves
substantially stronger topic separation as measured by the Calinski-
Harabasz Index (201.10 vs 51.42). This seemingly contradictory
behavior aligns with QDER’s design—documents relevant to multi-
ple topics adapt their representations based on the query context.
This dynamic adjustment allows QDER to capture more nuanced
and effective relationships between documents and queries.

These analyses demonstrate the power of QDER’s query-aware
document representations: while maintaining clear separation be-
tween relevant and non-relevant documents for specific queries,
the model also captures the multi-faceted nature of document-topic
relationships, ultimately enhancing retrieval quality.

Rank Position Analysis. A detailed evaluation of ranking per-
formance provides compelling evidence of QDER’s superiority over
SBERT in surfacing relevant content. The improvements are par-
ticularly striking in top-rank positions: QDER more than doubles
the number of relevant documents in the top-10 positions (99 ver-
sus 46), while concentrating the majority of relevant documents
within the top-50 positions. This is further shown by only 20 rele-
vant documents remaining beyond rank 100, highlighting QDER’s
effectiveness in elevating relevant content. The analysis becomes
even more revealing when examining performance across NIST rel-
evance grades. For documents rated as highly relevant (NIST grade
2), QDER achieves substantial improvements, raising their average
rank from 588.52 (BM25+RM3) to 138.48 and successfully positioning
13 of 21 such documents within the top-50 positions. The impact is
even more pronounced for documents rated as relevant (NIST grade
1), where QDER dramatically improves the average rank from 686.78
to 31.47. In this category, QDER demonstrates remarkable precision,
promoting 95 documents to top-10 positions and an additional 84
to top-50 positions, achieving a 78.5% improvement rate.

Shubham Chatterjee and Jeff Dalton

Take-Away. These findings challenge the prevalent practice of
using pre-computed document embeddings in neural IR and sug-
gests a critical architectural shift: future IR systems may need to
move beyond static representations to dynamically adapted, query-
specific document representations. While this shift introduces new
computational challenges, our results demonstrate that the poten-
tial gains in ranking quality make this a promising direction for
advancing the state-of-the-art in neural IR. QDER’s ability to priori-
tize relevant documents is particularly valuable for RAG systems,
which often rely on the top retrieved documents. By concentrating
relevant content in top positions, QDER can significantly enhance
the accuracy and context of generated responses.

5.4 Entity Attention Analysis

As discussed in Section 3.1, the entity channel in QDER leverages
attention-based adjustments to emphasize entities that align with or
complement the query’s concepts. To illustrate, we revisit the query
“Transportation Tunnel Disasters” from Section 5.1 and analyze how
these mechanisms drive QDER’s superior performance.

A closer examination of the entity attention patterns reveals the
mechanisms driving this improvement. The model’s entity-aware
attention identifies and prioritizes relevant entities across multiple
dimensions, creating a nuanced understanding of relevance. Safety-
critical entities like “blind curve” receive strong attention scores
(0.33—-0.55), highlighting their importance to the query. Location
entities such as “Mission Hills, Los Angeles” anchor the query to
specific incidents with maximum attention scores (1.0). Addition-
ally, transportation-related entities (e.g., “Sprinter? “101 Freeway,’
“tunnel junctions”) and safety features (e.g., “oxygen system”) ex-
hibit coherent attention patterns that align with the query’s intent.
Notably, the mechanism’s strong discriminative power ensures that
relevant entities are prioritized, while less pertinent entities receive
minimal attention despite frequent appearances in the document.

Our analysis of attention patterns suggests the entity-aware
attention mechanism may serve as a contextual filter that helps pri-
oritize certain document sections. For example, we observed high at-
tention scores for entities like “Mission Hills, Los Angeles” in docu-
ments containing specific incident descriptions related to the query
This filtering effect is particularly valuable for long documents
(such as in Robust04) where traditional models might struggle to
identify the most relevant passages. Moreover, the entity-aware
attention mechanism helps the model build a more comprehensive
understanding of document relevance by capturing meaningful
entity relationships across different aspects of relevance (infras-
tructure, safety, location). This multi-faceted enhancement of the
relevance matching process explains QDER’s ability to dramatically
improve rankings for challenging queries. The entity attention pat-
terns effectively guide the model’s other components, enabling it to
identify and appropriately weight relevant documents even when
surface-level matching signals are weak.

This analysis underscores a fundamental difference in the use of
entity attention between QDER and prior work [34, 57]. In EDRM [34],
for example, “attention” is based on static, precomputed similarities
between query and document entities. In contrast, QDER introduces
query-specific entity attention that dynamically learns and adjusts
entity weights during training, optimizing their relevance to the
specific query. By delaying aggregation and maintaining separate


QDER: Query-Speci

QDER Embeddings
Relevant vs Non-relevant

ic Document and Entity Representations for Multi-Vector Document Re-Ranking

SIGIR °25, July 13-18, 2025, Padua, Italy

SBERT Embeddings
Relevant vs Non-relevant

© Non-relevant (8416, 80.1%)
@ Relevant (2087, 19.9%)
=p
bd

Dimension 2
°

Dimension 2

75

e@ Non-relevant (8416, 80.1%)
@ Relevant (2087, 19.9%)

—100 100

Dimension 1

—50 -25 0 25 50 75

Dimension 1

Figure 2: Visualization of document embeddings using t-SNE for QDER (left) and SBERT (right) on TREC Robust 2004 (Title).
Relevant documents (red, 19.8%) and non-relevant documents (gray, 80.1%) are shown. QDER produces clear, coherent clusters
of relevant documents, while SBERT embeddings display significant mixing. This highlights QDER’s ability to create query-
specific representations that naturally separate by relevance, supporting its superior retrieval performance.

Table 5: Ablation Study for Architectural Choices in QDER.
Results reported on TREC Robust 2004 (Title).

Model Variant MAP nDCG@20 P@20 MRR
Relevance Signal Integration: None 0.4286 0.5748 0.5586 0.7791
Score Method: Linear 0.3222 0.5022 0.4496 0.8063
Entity Importance: Entity Ranking 0.4543 0.6554 0.6215 0.9071
No Entities 0.2098 0.3519 0.3042 0.6011

entity representations throughout the pipeline, QDER enables truly
dynamic entity attention that adapts to each query’s specific in-
formation needs. For example, QDER can adjust the importance of
entities like “blind curve,’ and “Mission Hills” specifically for a
transportation disasters query, achieving nuanced, query-aware
reasoning beyond the capabilities of static scoring.

5.5 Architectural Choices

As a final analysis, we conduct ablation studies to validate QDER’s
key architectural components. The results reveal three critical
insights. First, bilinear scoring is essential—on Robust04 (title),
the linear variant achieves MAP = 0.32 compared to QDER’s 0.56,
demonstrating that simple linear combinations cannot capture com-
plex query-document relationships. Second, entity modeling proves
crucial-removing entities causes MAP to drop to 0.20, while re-
placing entity attention with direct entity ranking (MAP = 0.45) still
significantly underperforms the full architecture. Our ablation stud-
ies suggest that entity attention mechanisms, which adjust entity
representations based on query context, contribute significantly to
model performance compared to direct entity ranking alone. This
may indicate that dynamic entity representations capture more
nuanced relationships between queries and documents.

These findings collectively validate QDER’s architectural deci-
sions. The consistent performance improvements across all metrics
demonstrate that our approach successfully balances multiple forms

of evidence when determining document relevance. While each
component contributes meaningfully, it is their careful integration
that enables QDER to achieve superior ranking effectiveness.

6 Conclusion

We present QDER, a state-of-the-art neural document re-ranking
model that leverages attention-guided interaction modeling to cre-
ate query-specific representations. Our results definitively answer
the question (raised in Section 1) of whether integrating entities
into multi-vector frameworks can advance IR performance: QDER’s
substantial improvements over both entity-based and multi-vector
baselines demonstrate that unifying these approaches yields pow-
erful synergies. Our comprehensive analysis reveals several key
insights: (1) performing semantic matching on attention-weighted
embeddings rather than raw representations leads to dramatic im-
provements in ranking quality, (2) Addition and Multiplication
operations exhibit mathematical complementarity in capturing dif-
ferent aspects of relevance, and (3) dynamically adapting document
representations to query context is crucial, particularly for difficult
queries where traditional approaches may fail. On TREC Robust
2004, QDER doubles the relevant documents in top-10 positions com-
pared to strong baselines and achieves a 78.5% improvement in
positioning highly relevant content.

Our work suggests that neural IR systems may need to rethink
how query-document relationships are processed—moving away
from static representations towards query-aware ones. QDER’s strong
performance on challenging queries—those requiring understand-
ing of multiple aspects or complex relationships—marks significant
progress toward IR systems capable of handling real-world informa-
tion needs. The improvement in average rank for highly relevant
documents (from 588.52 to 138.48) demonstrates our approach’s
effectiveness in identifying relevant content. Overall, we believe
that future work can build on these insights by exploring deeper
integration of query-aware mechanisms and more sophisticated
entity-aware approaches to further enhance ranking effectiveness.


SIGIR °25, July 13-18, 2025, Padua, Italy

References

[1]

[10

{11

[13]

[18]

[19]

Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.
Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China,
3490-3496. doi:10.18653/v1/D19- 1352

James Allan, Donna Harman, Evangelos Kanoulas, Dan Li, Christophe Van Gysel,
and Ellen M Voorhees. 2017. TREC 2017 Common Core Track Overview. In
TREC.

Zhang Bingyu and Nikolay Arefyev. 2022. The Document Vectors Using Co-
sine Similarity Revisited. In Proceedings of the Third Workshop on Insights from
Negative Results in NLP, Shabnam Tafreshi, Jodo Sedoc, Anna Rogers, Aleksandr
Drozd, Anna Rumshisky, and Arjun Akula (Eds.). Association for Computational
Linguistics, Dublin, Ireland, 129-133. doi:10.18653/v1/2022.insights- 1.17
Shubham Chatterjee and Laura Dietz. 2022. BERT-ER: Query-Specific BERT
Entity Representations for Entity Ranking. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval
(Madrid, Spain) (SIGIR ’22). Association for Computing Machinery, New York,
NY, USA, 1466-1477. doi:10.1145/3477495.3531944

Shubham Chatterjee, Iain Mackie, and Jeff Dalton. 2024. DREQ: Document Re-
Ranking Using Entity-based Query Understanding. In Proceedings of the 46th
European Conference on Information Retrieval (ECIR 2024) (Glasgow, Scotland)
(Lecture Notes in Computer Science). Springer.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
CoRR abs/2003.10555 (2020). arXiv:2003.10555 https://arxiv.org/abs/2003.10555
Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Con-
textual Neural Language Modeling. CoRR abs/1905.09217 (2019). arXiv:1905.09217
http://arxiv.org/abs/1905.09217

Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional
Neural Networks for Soft-Matching N-Grams in Ad-Hoc Search. In Proceedings
of the Eleventh ACM International Conference on Web Search and Data Mining
(Marina Del Rey, CA, USA) (WSDM ’18). Association for Computing Machinery,
New York, NY, USA, 126-134. doi:10.1145/3159652.3159659

Jeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion
using knowledge base links. In Proceedings of the 37th international ACM SIGIR
conference on Research & development in information retrieval. ACM, 365-374.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
CoRR abs/1810.04805 (2018). arXiv:1810.04805 http://arxiv.org/abs/1810.04805
Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC
Complex Answer Retrieval Overview.. In Proceedings of Text REtrieval Conference
(TREC).

Faezeh Ensan and Ebrahim Bagheri. 2017. Document Retrieval Model Through
Semantic Linking. In Proceedings of the 10th ACM International Conference on Web
Search and Data Mining (Cambridge, United Kingdom) (WSDM ’17). Association
for Computing Machinery, New York, NY, USA, 181-190. doi:10.1145/3018661.
3018692

Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE:
Sparse Lexical and Expansion Model for First Stage Ranking. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval (Virtual Event, Canada) (SIGIR ’21). Association for
Computing Machinery, New York, NY, USA, 2288-2292. doi:10.1145/3404835.
3463098

Evgeniy Gabrilovich and Shaul Markovitch. 2009. Wikipedia-based Semantic
Interpretation for Natural Language Processing. Journal of Artificial Intelligence
Research 34 (2009), 443-498.

Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance
Matching Model for Ad-Hoc Retrieval. In Proceedings of the 25th ACM Interna-
tional on Conference on Information and Knowledge Management (Indianapolis,
Indiana, USA) (CIKM ’16). Association for Computing Machinery, New York, NY,
USA, 55-64. doi:10.1145/2983323.2983769

Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DeBERTa:
Decoding-enhanced BERT with Disentangled Attention. CoRR abs/2006.03654
(2020). arXiv:2006.03654 https://arxiv.org/abs/2006.03654

Sebastian Hofstatter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Allan
Hanbury. 2020. Local Self-Attention over Long Text for Efficient Document
Retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (Virtual Event, China) (SIGIR
°20). Association for Computing Machinery, New York, NY, USA, 2021-2024.
doi:10.1145/3397271.3401224

Sebastian Hofstatter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@
TREC Deep Learning’ 19-Simple Contextualization for Re-ranking. arXiv preprint
arXiv:1912.01385 (2019).

Sebastian Hofstatter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable

and Time-budget-constrained Contextualization for Re-ranking. arXiv preprint
arXiv:2002.01854 (2020).

[20]

[22]

[23]

[24]

[25]

28

29

30

31

32

33

34

[36]

[37]

Shubham Chatterjee and Jeff Dalton

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning Deep Structured Semantic Models for Web Search Using
Clickthrough Data. In Proceedings of the 22nd ACM International Conference on
Information & Knowledge Management (San Francisco, California, USA) (CIKM
*13). Association for Computing Machinery, New York, NY, USA, 2333-2338.
doi:10.1145/2505515.2505665

Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. PACRR: A
Position-Aware Neural IR Model for Relevance Matching. In Proceedings of the
2017 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, Copenhagen, Denmark, 1049-1058. doi:10.18653/
v1/D17-1110

Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2018. Co-PACRR:
A Context-Aware Neural IR Model for Ad-Hoc Retrieval. In Proceedings of the
Eleventh ACM International Conference on Web Search and Data Mining (Marina
Del Rey, CA, USA) (WSDM ’18). Association for Computing Machinery, New
York, NY, USA, 279-287. doi:10.1145/3159652.3159689

Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020.
Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate
Multi-sentence Scoring. In 8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
https://openreview.net/forum?id=SkxgnnNFvH

Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and
Shuicheng Yan. 2020. ConvBERT: Improving BERT with Span-based Dynamic
Convolution. CoRR abs/2008.02496 (2020). arXiv:2008.02496 https://arxiv.org/
abs/2008.02496

Chris Kamphuis, Aileen Lin, Siwen Yang, Jimmy Lin, Arjen P. de Vries, and
Faegheh Hasibi. 2023. MMEAD: MS MARCO Entity Annotations and Dis-
ambiguations. In Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR
°23). Association for Computing Machinery, New York, NY, USA, 2817-2825.
doi:10.1145/3539618.3591887

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP). Association for Computational
Linguistics, Online, 6769-6781. doi:10.18653/v1/2020.emnlp-main.550

Omar Khattab and Matei Zaharia. 2020. CoIBERT: Efficient and Effective Passage
Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Virtual Event, China) (SIGIR ’20). Association for Computing Machinery,
New York, NY, USA, 39-48. doi:10.1145/3397271.3401075

Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-
mization. arXiv preprint arXiv:1412.6980 (2014).

Hrishikesh Kulkarni, Sean MacAvaney, Nazli Goharian, and Ophir Frieder. 2023.
Lexically-Accelerated Dense Retrieval. In Proceedings of the 46th International
ACM SIGIR Conference on Research and Development in Information Retrieval
(Taipei, Taiwan) (SIGIR ’23). Association for Computing Machinery, New York,
NY, USA, 152-162. doi:10.1145/3539618.3591715

Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2020. PA-
RADE: Passage Representation Aggregation for Document Reranking. CoRR
abs/2008.09093 (2020). arXiv:2008.09093 https://arxiv.org/abs/2008.09093
Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Rep-
resentations for Ranking using Tightly-Coupled Teachers. CoRR abs/2010.11386
(2020). arXiv:2010.11386 https://arxiv.org/abs/2010.11386

Xitong Liu and Hui Fang. 2015. Latent entity space: a novel retrieval approach
for entity-bearing queries. Information Retrieval Journal 18, 6 (2015), 473-503.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).
arXiv:1907.11692 http://arxiv.org/abs/1907.11692

Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2018. Entity-
Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in
Neural Information Retrieval. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, Melbourne, Australia, 2395-2405. doi:10.18653/
v1/P18-1223

Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse,
Dense, and Attentional Representations for Text Retrieval. Transactions of the
Association for Computational Linguistics 9 (2021), 329-345. doi:10.1162/tacl_a_
00369

Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:
Contextualized Embeddings for Document Ranking. CoRR abs/1904.07094 (2019).
arXiv:1904.07094 http://arxiv.org/abs/1904.07094

Jain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean MacAvaney,
and Jeffrey Dalton. 2022. CODEC: Complex Document and Entity Collection.
In Proceedings of the 45th International ACM SIGIR Conference on Research and
Development in Information Retrieval (Madrid, Spain) (SIGIR ’22). Association for
Computing Machinery, New York, NY, USA, 3067-3077. doi:10.1145/3477495.


QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking

[42

[43

[44

[45

[46

[47

[48

[49

[50

[51

[52

[53

[54

3531712

Edgar Meij, Dolf Trieschnigg, Maarten de Rijke, and Wessel Kraaij. 2010. Con-
ceptual Language Models for Domain-Specific Retrieval. Inf: Process. Manage. 46,
4 (jul 2010), 448-469. doi:10.1016/j.ipm.2009.09.005

Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage
Re-ranking. CoRR abs/1903.07666 (2019). arXiv:1903.07666 http://arxiv.org/abs/
1903.07666

Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving
Document Ranking with Dual Word Embeddings. In Proceedings of the 25th
International Conference Companion on World Wide Web (Montréal, Québec,
Canada) (WWW ’16 Companion). International World Wide Web Conferences
Steering Committee, Republic and Canton of Geneva, CHE, 83-84. doi:10.1145/
2872518.2889361

Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, and James Allan. 2021. CEQE:
Contextualized Embeddings for Query Expansion. In Advances in Information
Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March
28 — April 1, 2021, Proceedings, Part I. Springer-Verlag, Berlin, Heidelberg, 467-482.
doi:10.1007/978-3-030-72113-8_31

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. CoRR abs/1611.09268 (2016). arXiv:1611.09268
http://arxiv.org/abs/1611.09268

Rodrigo Frassetto Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with
BERT. CoRR abs/1901.04085 (2019). arXiv:1901.04085 http://arxiv.org/abs/1901.
04085

Francesco Piccinno and Paolo Ferragina. 2014. From TagME to WAT: A New Entity
Annotator. In Proceedings of the First International Workshop on Entity Recognition
& Disambiguation (Gold Coast, Queensland, Australia) (ERD °14). Association for
Computing Machinery, New York, NY, USA, 55-62. doi:10.1145/2633211.2634350
Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document Retrieval Using
Entity-Based Language Models. In Proceedings of the 39th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Pisa,
Italy) (SIGIR ’16). Association for Computing Machinery, New York, NY, USA,
65-74. doi:10.1145/2911451.2911508

Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. arXiv:1908.10084 [cs.CL] https://arxiv.org/abs/
1908.10084

Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. CoRR abs/1908.10084 (2019). arXiv:1908.10084
http://arxiv.org/abs/1908.10084

Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Frame-
work: BM25 and Beyond. Now Publishers Inc.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. Entity Linking with a Knowledge
Base: Issues, Techniques, and Solutions. IEEE Transactions on Knowledge and
Data Engineering 27, 2 (2015), 443-460. doi:10.1109/TKDE.2014.2327028

Ian Soboroff, Shudong Huang, and Donna Harman. 2018. TREC 2018 News Track
Overview. In TREC, Vol. 409. 410.

Hai Dang Tran and Andrew Yates. 2022. Dense Retrieval with Entity Views. In
Proceedings of the 31st ACM International Conference on Information & Knowl-
edge Management (Atlanta, GA, USA) (CIKM ’22). Association for Computing
Machinery, New York, NY, USA, 1955-1964. doi:10.1145/3511808.3557285

Ellen M Voorhees et al. 2003. Overview of the TREC 2003 robust retrieval track..
In Trec. 69-77.

Xiao Wang, Craig MacDonald, Nicola Tonellotto, and Iadh Ounis. 2023. CoIBERT-
PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document
Retrieval. ACM Trans. Web 17, 1, Article 3 (jan 2023), 39 pages. doi:10.1145/
3572405

Chenyan Xiong and Jamie Callan. 2015. EsdRank: Connecting Query and
Documents Through External Semi-Structured Data. In Proceedings of the
24th ACM International Conference on Information and Knowledge Manage-
ment (Melbourne, Australia) (CIKM °15). ACM, New York, NY, USA, 951-960.
doi:10.1145/2806416.2806456

[55]

58

59

60

61

62

63

64

[65]

[66]

[67]

SIGIR °25, July 13-18, 2025, Padua, Italy

Chenyan Xiong and Jamie Callan. 2015. Query Expansion with Freebase. In
Proceedings of the 2015 International Conference on The Theory of Information
Retrieval (Northampton, Massachusetts, USA) (ICTIR ’15). Association for Com-
puting Machinery, New York, NY, USA, 111-120. doi:10.1145/2808194.2809446
Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2016. Bag-of-Entities Rep-
resentation for Ranking. In Proceedings of the 2016 ACM International Con-
ference on the Theory of Information Retrieval (Newark, Delaware, USA) (IC-
TIR ’16). Association for Computing Machinery, New York, NY, USA, 181-184.
doi:10.1145/2970398.2970423

Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-Entity Duet Repre-
sentations for Document Ranking. In Proceedings of the 40th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Shinjuku,
Tokyo, Japan) (SIGIR ’17). Association for Computing Machinery, New York, NY,
USA, 763-772. doi:10.1145/3077136.3080768

Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.

2017. End-to-End Neural Ad-Hoc Ranking with Kernel Pooling. In Proceedings
of the 40th International ACM SIGIR Can ference on Research and Development

in Information Retrieval (Shinjuku, Tokyo, Japan) (SIGIR *17). Association for
Computing Machinery, New York, NY, USA, 55-64. doi:10.1145/3077136.3080809
Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Eduard Hovy. 2017. JointSem:
Combining Query Entity Linking and Entity Based Document Ranking. In Pro-
ceedings of the 2017 ACM SIGIR Conference on Information and Knowledge Manage-
ment (Singapore, Singapore) (CIKM ’17). Association for Computing Machinery,
New York, NY, USA, 2391-2394. doi:10.1145/3132847.3133048

Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie-Yan Liu. 2018. Towards
Better Text Understanding and Retrieval through Kernel Entity Salience Modeling.
In The 41st International ACM SIGIR Conference on Research and Development in In-
formation Retrieval (Ann Arbor, MI, USA) (SIGIR ’18). Association for Computing
Machinery, New York, NY, USA, 575-584. doi:10.1145/3209978.3209982
Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit Semantic Ranking
for Academic Search via Knowledge Graph Embedding. In Proceedings of the
26th International Conference on World Wide Web (Perth, Australia) (WWW ’17).
International World Wide Web Conferences Steering Committee, Republic and
Canton of Geneva, CHE, 1271-1279. doi:10.1145/3038912.3052558

Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett,
Junaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neighbor
Negative Contrastive Learning for Dense Text Retrieval. CoRR abs/2007.00808
(2020). arXiv:2007.00808 https://arxiv.org/abs/2007.00808

Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett,
Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Neg-
ative Contrastive Learning for Dense Text Retrieval. In International Conference
on Learning Representations. https://openreview.net/forum?id=zeFrfgyZln
Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda,
Yoshiyasu Takefuji, and Yuji Matsumoto. 2020. Wikipedia2Vec: An Efficient
Toolkit for Learning and Visualizing the Embeddings of Words and Entities from
Wikipedia. In Proceedings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations. Association for Computational
Linguistics, Online, 23-30. doi:10.18653/v1/2020.emnlp-demos.4

Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma.
2020. RepBERT: Contextualized Text Embeddings for First-Stage Retrieval.
arXiv:2006.15498 [cs.IR] https://arxiv.org/abs/2006.15498

Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.
2019. ERNIE: Enhanced Language Representation with Informative Entities.
In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, Florence, Italy, 1441-1451.
doi:10.18653/v1/P19- 1139

Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni,
Xuanhui Wang, and Michael Bendersky. 2023. RankT5: Fine-Tuning T5 for
Text Ranking with Ranking Losses. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Taipei,
Taiwan) (SIGIR ’23). Association for Computing Machinery, New York, NY, USA,
2308-2313. doi:10.1145/3539618.3592047
