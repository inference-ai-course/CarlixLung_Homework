2510.09882v1 [cs.CL] 10 Oct 2025

arXiv

iBERT: Interpretable Style Embeddings via Sense Decomposition

Vishal Anand!, Milad Alshomary”, Kathleen McKeown”

"Microsoft, Washington, USA
*Columbia University, New York, USA

Correspondence: vishal.anand@microsoft.com

Abstract

We present iBERT (interpretable-BERT), an
encoder to produce inherently interpretable
and controllable embeddings - designed to
modularize and expose the discriminative cues
present in language, such as stylistic and se-
mantic structure. Each input token is repre-
sented as a sparse, non-negative mixture over k
context-independent sense vectors, which can
be pooled into sentence embeddings or used di-
rectly at the token level. This enables modular
control over representation, before any decod-
ing or downstream use.

To demonstrate our model’s interpretability, we
evaluate it on a suite of style-focused tasks.
On the STEL benchmark, it improves style
representation effectiveness by ~8 points over
SBERT-style baselines, while maintaining com-
petitive performance on authorship verification.
Because each embedding is a structured com-
position of interpretable senses, we highlight
how specific style attributes - such as emoji
use, formality, or misspelling can be assigned
to specific sense vectors. While our experi-
ments center on style, iBERT is not limited to
stylistic modeling. Its structural modularity is
designed to interpretably decompose whichever
discriminative signals are present in the data —
enabling generalization even when supervision
blends stylistic and semantic factors.

1 Introduction

Neural encoders increasingly serve as the back-
bone for tasks that rely on nuanced linguistic vari-
ation—ranging from authorship attribution and
tone-controlled generation to stylistic retrieval and
moderation. Yet, most popular encoders, such as
SBERT (Reimers and Gurevych, 2019) or SimCSE
(Gao et al., 2021), produce dense vector representa-
tions that offer no clear control over how style and
meaning are encoded. This limits their reliability in
domains where representational transparency and
stylistic control are essential. These challenges are

An input sequence with variation in expression
Tokenized

»Sparse k- senses (2,—2),) per token|

iBERT “|

PRA RCD ROCA

Controlled ait]
DEA CAE ~ al Py YO x

Mean Pooling Top-Sense Pooling
(r > &) (t > 0)

v1 v2

Interpretable+Editable Embedding

Figure 1: iBERT encodes tokens via k interpretable
senses, producing editable and composable sense ac-
tivations that are used either individually at the token
level or pooled via configurable strategies into a global,
interpretable embedding suitable for NLP pipelines.

especially acute in stylistic tasks, where disentan-
gling content from style is not optional but required.
Without structured representations, it becomes dif-
ficult to attribute stylistic effects, audit model be-
havior, or intervene in generation pipelines. Prior
approaches often rely on proxy supervision and
post-hoc explanations, which yield entangled em-
beddings and incomplete attribution (Danilevsky
et al., 2020; John et al., 2019; Elazar et al., 2021).

In this work, we ask: Can we design an encoder
where the semantic and stylistic representations are
made explicit and controllable within the embed-
ding space—by design, not post-hoc? To this end,
we develop iBERT, an encoder architecture with
representations that are interpretable and control-
lable by design. Each token is expressed as a sparse,
non-negative mixture over k context-independent
sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This
design enables modular control over specific dimen-
sions of meaning and style that are learned during
the training phase, enabling analysis, attribution,


and targeted edits in embedding space. Our ar-
chitecture builds on the Backpack formulation of
Hewitt et al. (2023), which modeled autoregressive
decoding using sparse token-level senses. We adapt
this formulation to an encoder-only setting with
global pooling, allowing for sentence-level compo-
sitionality, bidirectional input, and plug-and-play
use across classification and retrieval pipelines.

We train iBERT with a masked language model-
ing objective on a web corpus to produce sparse, in-
terpretable sense embeddings. We evaluate iBERT
on style analysis tasks—benchmarks that test a
model’s ability to isolate, attribute, and manipulate
stylistic variation without entangling semantic con-
tent. In particular, by training iBERT on the style
analysis task (Patel et al., 2025; Wegmann et al.,
2022), we find that it improves style representation
accuracy (STEL) by +8 points over SBERT-style
baselines while remaining competitive on Style-
or-Content (SoC) and PAN authorship verification.
Because each embedding is a transparent mixture
of sense vectors, we can directly identify stylistic
axes (e.g., emoji use, sarcasm, lexical choices) and
apply targeted edits in the embedding space.

Though evaluated on stylistic tasks, iBERT is
not a style model. It is a general-purpose encoder
with decomposable representations—suitable for
domains where interpretability and control are pre-
requisites rather than afterthoughts. We will make
the code publicly available upon acceptance.

Contributions.

¢ We present iBERT, an encoder with control-
lable decomposable representations, enabling
interpretable token and sentence embeddings.

e We show strong performance across three
benchmarks: STEL, SoC, and PAN - achiev-
ing up to +8% STEL gains over baselines.

¢ We demonstrate novel interpretability capabil-
ities: probing sense vectors for stylistic traits,
ablating senses to identify attribution, and edit-
ing embeddings for controlled style transfer.

2 Background and Related Work

Sentence encoders and __ interpretability.
Transformer-based encoders such as SBERT
(Reimers and Gurevych, 2019) and SimCSE
(Gao et al., 2021) perform well on semantic
retrieval and sentence similarity tasks, but their
dense embeddings offer little visibility into what
linguistic attributes are being captured. Efforts

to explain these representations have focused
on post-hoc methods such as_ probing-based
explanations (Wallace et al., 2019), saliency and
attribution techniques (Jacovi and Goldberg, 2020),
and influence functions. As noted by Jacovi
and Goldberg (2020), such methods are often
applied to representations that were not designed
for interpretability, and can yield incomplete or
unfaithful explanations. The survey by Danilevsky
et al. (2020) further emphasizes the need for
architecturally grounded interpretability—where
structure and transparency are embedded into the
model itself.

Stylistic representation learning. Most prior
work on stylistic representation relies on proxy su-
pervision, typically using authorship identity as
a surrogate for stylistic consistency. For example,
Wegmann et al. (2022) construct contrastive triplets
using same-author, different-topic texts to encour-
age content-invariant representations, and evaluate
model performance using metrics such as STEL
and Style-or-Content (SoC). While effective, these
approaches still produce dense, entangled embed-
dings with no inherent interpretability or mecha-
nism for stylistic control.

Patel et al. (2025) introduce a new paradigm: su-
pervising style directly, independent of topic, by
generating contrastive triplets with controlled lex-
ical and syntactic variation. Their StyleDistance
model, along with the SynthSTEL benchmark, en-
ables direct evaluation along 40 distinct content-
controlled style-axes. We adopt this direct-style
supervision setup, and pair it with our new archi-
tectural formulation: an encoder that produces de-
composable, interpretable embeddings by design.

Multi-sense representations and Backpack.
Backpack-GPT (Hewitt et al., 2023) introduced
sparse token-level representations via mixtures
over static senses, enabling interpretable and lo-
cally steerable generation in autoregressive models.
While their formulation supports tokenwise con-
trol, it operates within a decoder-centric setup and
focuses on in-loop manipulation during generation.

iBERT repositions sparse sense mixtures as a
representation-level mechanism, producing glob-
ally composable embeddings that enable inter-
pretability and control across tasks. This allows
for interpretable, task-agnostic control at the repre-
sentation level—prior to any decoding—making
iBERT compatible with retrieval, classification,
and generation pipelines alike. Rather than serving


as a generation-side control mechanism, iBERT re-
frames sense-based modeling as a general-purpose
representational interface for controllable NLP.

3 Method

We present iBERT, a multi-sense encoder that pro-
duces interpretable embeddings by design. iBERT
can operate as a token-level encoder (pretrained as
MLM) or as a sentence encoder (v1—v3) trained
with contrastive supervision. Each token is en-
coded as a sparse mixture over static sense vectors,
which are then pooled to form globally composable
sentence embeddings. This design enables explicit
analysis, control, and manipulation of embeddings.

3.1 iBERT Architecture

Figure 1 shows a high-level overview of our ar-
chitecture; a full schematic is provided in the Ap-
pendix (Figure 5). Each token 2; is first mapped
to a standard embedding e; = Ex; € R?. A feed-
forward layer projects this into & senses:

In parallel, a transformer encoder produces con-
textual states H = [hi,...,hn] € R"*4. These
are projected into sense-specific queries and keys:
QM, KM © R/*)x4, allowing us to compute
mixture weights ay ;,;—the contribution of sense
¢ from token x; to the contextual embedding of
token x;. k

n
a ~> XL i,5 * (xj )e, OE R¢.

j=l l=1

The resulting 0; is a convex mixture of context-
independent sense vectors. These representations
can be inspected or edited directly. We pretrain
this architecture as iBERT-MLM with k=8 senses
using masked language modeling, yielding a 171M-
parameter encoder, comparable in scale to BERT-
base. At k=1, the sense construction block reduces
to an embedding matrix, since each token is now
mapped to a traditional one dimensional vector.

3.2. iBERT Sentence Embeddings

As a general idea, we take the two-dimensional
sense vectors per token and construct a weighted
structure to condense the senses together. We apply
structured pooling over each token; representation
0;, resulting in three variants: v1 (uniform averag-
ing), v2 (top-sense selection), and v3 (soft blending
senses wired-into the network by a sense composi-
tion function).

iBERT-v1: Mean pooling. The sentence embed-
ding is computed via uniform averaging:

n
1
sz ) Oj.
n «
a1

This produces a decomposable embedding,
where each dimension reflects contributions from
token-level sense activations—supporting interpre-
tation and attribution (Figure 5b).

iBERT-v2: Top-sense pooling. For an input se-
quence, we compute the total activation per-sense:

= >|

We identify the dominant sense (per sequence) as
é* = arg maxy S¢, and retain only that sense across
all tokens to compute the sequence embedding:

1 n
i=1

This encourages stricter top-1 alignment between
input sequences and individual senses. The process
is repeated independently for each input sequence
(Figure 5c).

o(0

(2) _
|,» where o| = ) aga g (Xs )e
j

iBERT-v3: Softmax-weighted pooling. We de-
fine a general family of models to interpolate behav-
iors between v1 and v2 using softmax over sense
norms, controlled by a sense composition variable
7. Once 7 is set, the v3 trained model is fixed with
the pooling structure and is not replaced afterwards.

s= >>|
1 n k

i (©

Sp = — mel) -O;".

i=1 f=1
This general formulation defines v3, where:

a0

,° to(T) = softmax(S?/T),

* 7 — oo: recovers v1 (uniform averaging)
¢ 7 — 0: recovers v2 (hard top-sense selection)

* 0 <7 < oo: soft blending across senses

This generalized framework allows us to study
how the sharpness of sense composition affects in-
terpretability and performance (§4, Appendix A.3).

3.3 Training Pipeline

We train iBERT in two stages. Unlike prior work
(Patel et al., 2025; Wegmann et al., 2022) which
fine-tuned off-the-shelf encoders, our decompos-
able design requires MLM pretraining to instantiate
meaningful sense vectors.


3.3.1 Stage 0: MLM Pretraining.

We pretrain iBERT-MLM from scratch on 5% of
FINEWEB (Penedo et al., 2025), a 15T-token web-
scale corpus. This 750B-token slice offers broad
linguistic and stylistic diversity while remaining
computationally efficient. We use standard masked
language modeling to learn token-level sense acti-
vations, producing a 171M-parameter encoder. For
comparison, we also train a standard BERT model
using identical settings and data.

3.3.2 Stage 1: Style Representaion Learning

To train iBERT on the task of style analysis, we use
contrastive triplets (s,s*,s~) targeting stylistic
variation and create iBERT-vl-v3. Training data
is drawn from StyleSynth / StyleDistance by Patel
et al. (2025): content-controlled, synthetic style
triplets; and from author-labeled triplets with topic
control by Wegmann et al. (2022).

We apply the InfoNCE loss to encourage close-
ness between s and s*, while pushing s away from
s_. Pooling weights 7¢(7) are shared between the
anchor and positive, encouraging consistent sense-
axis usage for stylistically similar inputs.

To ensure architectural parity, we also train
Vanilla-BERT and then Vanilla-SBERT baselines
using the two-stage setup: a BERT-base model pre-
trained on the same 5% FineWeb slice, followed by
contrastive fine-tuning on anchored triplets. This
controls for pretraining variance and isolates the
contribution of our multi-sense architecture.

Model size. iBERT-vl—v3 has 171.4M parame-
ters and Vanilla-SBERT has 149.7M. Inference is
approximately 2% slower owing to sense composi-
tion. Tradeoffs are discussed in section 8.

4 Experimental Setup

4.1 Pretraining and Fine-tuning

We reuse iBERT-MLM and Vanilla-BERT models
from Stage 0 (§3.3.1), trained from scratch on 5%
of FINEWEB. Sentence-level variants—iBERT-
vl—v3 are fine-tuned on contrastive triplets to learn
style representation using either:
¢ StyleDistance - SD (Patel et al., 2025):
50k synthetic triplets (SynthSTEL) with con-
trolled lexical and syntactic variation. Median
token-size is 19 (Appendix Table 7).
¢ Wegmann - WG (Wegmann et al., 2022): 40k
author-anchored Reddit triplets with similar
topics. Median token-length is 24.

Training follows the loss function of each dataset:
for StyleDistance we use the InfoNCE objective as
in Patel et al. (2025), while for Wegmann triplets
we apply the margin-based contrastive loss used
in Wegmann et al. (2022). In both cases, pooling
weights 7¢(7) are shared between the anchor and
positive to enforce consistent sense usage. Addi-
tional details are in Appendix A.4.

Baselines. To isolate the effect of our multi-sense
architecture, we compare against contrastively
trained versions of the same Vanilla-BERT encoder
from Stage 0. We fine-tune this model to create:

¢ Vanilla-BERTsp: trained on StyleDistance
triplets with InfoNCE loss (as in Patel et al.
(2025));

¢ Vanilla-BERTwo: trained on Wegmann
triplets with the margin-based triplet loss (as
in Wegmann et al. (2022));

¢ Vanilla-BERTwosp: trained on both datasets
with InfoNCE loss (reflecting the strongest
setup reported by Patel et al. (2025)).

Vanilla-BERTwe corresponds to the SBERT-style
model of Wegmann et al. (2022), while Vanilla-
BERT woe+sp mirrors the best-performing StyleDis-
tance setup of Patel et al. (2025). All settings share
the same backbone, initialization, and optimizer
for a controlled comparison.

Backbone Consistency. Prior work used
DistilRoBERTa-based encoders (e.g., Patel et al.
(2025), Wegmann et al. (2022)). Since iBERT
is trained from scratch, we also train a new
BERT-base encoder using the same setup as
iBERT-MLM. This serves as the backbone for
Vanilla-BERTsp;wo/wo+sp, replacing Distil-
RoBERTa in their respective setups and enabling
a fair performance comparison. Full training
protocol is detailed in Appendix A.4.

4.2 Evaluation Benchmarks

We evaluate all models across four direct style tasks
(STEL-40sp, SoC-40sp, STEL-5 we, SoC-5we)
and one proxy-style task (Authorship Verification -
PAN), using both 128- and 512-token variants of
the backbone BERT models.

Direct-style evaluation.

¢ STEL: multi-class classification over 40 style
labels (StyleDistance) and 5 style classes
(Wegmann), reported as weighted accuracy.


* SoC: average binary classification accuracy
between positive/negative style polarities per
class (e.g., formal vs. informal).

Proxy-style evaluation. PAN authorship verifi-
cation: binary classification of same vs. different
author pairs (PAN 2011/13/14/15), reporting AUC.
Treated as a proxy for style consistency, though not
grounded in specific stylistic dimensions.

5 Results

5.1 STEL Performance

Table 1 reports style representation effectiveness
(STEL) across training regimes (Wegmann and
Nguyen, 2021). Across the board, iBERT models
outperform their Vanilla counterparts, underscor-
ing the utility of structured, interpretable pooling
for capturing stylistic signals. Under SD-ONLY
training, iBERT-v3-1 achieves the highest STEL
scores, yielding 38.3% (128 tokens) and 38.2%
(512 tokens)—a substantial gain of +6.7 and +7.6
points over Vanilla. Close contenders include
iBERT-v1 and iBERT-v3-10, both of which ex-
hibit similarly strong performance, suggesting that
soft, sense-weighted pooling is especially effective
when guided by direct-style supervision.

Interestingly, iBERT-v2 consistently lags behind
vl and v3 across all settings. This is expected:
its hard top-sense selection mechanism introduces
sharp sparsity that likely harms generalization by
collapsing compositional diversity. In contrast,
iBERT-v3-10 is not only among the top performers
but also the most stable across settings—offering a
smoother inductive bias that bridges the extremes
of v1 (uniform blend) and v2 (hard top-1 sense).

Even under joint supervision from WG+SD,
where noisy author-level triplets may dilute sense-
specific stylistic signal, iBERT variants maintain
their edge. iBERT-v1 secures the best STEL at 128
tokens (38.6%), while iBERT-v3-1 leads at 512
tokens (36.4%), outperforming Vanilla by approxi-
mately 6 points in both cases.

Finally, in the WG-ONLY setting, where labels
reflect author identity rather than explicit styles, all
models see degraded performance. Nevertheless,
iBERT-v3 and iBERT-v1 is at par with Vanilla, sig-
naling that even without style supervision, iBERT
retains competitive structure-aware generalizability.
These results affirm that iBERT’s compositional
inductive bias enables robustness without compro-
mising direct-style effectiveness.

5.2 SoC Performance

For the SoC metric—evaluating separability of
stylistic polarity—iBERT-v3-1 achieves the high-
est overall scores: 92.8% (128 tokens) and 92.0%
(512 tokens) under SD-ONLY, outperforming all
baselines. These results show that iBERT can pre-
serve or even enhance polarity sensitivity while
maintaining interpretability.

In contrast, iBERT-v2 underperforms across set-
tings (e.g., 88.7% under SD-ONLY), often lagging
behind both v1 and Vanilla. We attribute this to
its top-1 sense pooling: by forcing each sequence
to align with a single dominant sense, iBERT-v2
severely restricts the number of senses that can
specialize for the same data-points observed. This
architectural bottleneck reduces expressive capac-
ity, especially for fine-grained polarity distinctions
that require distributed compositionality.

Under WG-ONLY training—where supervision
is provided weakly via author labels rather than
direct stylistic polarity—all models see a dramatic
drop in SoC performance. Vanilla, for example,
falls from 91.5% (SD-ONLY) to just 27.5%, while
iBERT variants drop even further (e.g., 8.0% for
vl). Despite this collapse, Vanilla appears to out-
perform iBERT in this setting. We hypothesize
this is due to content leakage: Vanilla implicitly
entangles semantic and stylistic attributes, using
topic or lexical cues as proxies for style. While this
can yield fragile SoC signal in weakly supervised
settings, it sacrifices interpretability and modular-
ity. In contrast, iBERT’s explicit disentanglement
fails to recover polarity distinctions without true
stylistic supervision—a tradeoff aligned with its
core design goals.

Taken together, these results affirm that iBERT
excels when trained with style-specific signals,
achieving strong polarity separability without com-
promising compositional control or interpretability.
Further analysis of the pooling sharpness mecha-
nism (§3.2) translating to performance and stability
is discussed in Appendix A.3.

5.3. Authorship Verification (PAN)

To assess generalizability of learning style to
solve the authorship attribution task, we use the
PAN11-15 benchmarks, a suite of proxy-style tasks
requiring models to distinguish between same-
author and different-author text pairs. Unlike STEL
or SoC, PAN tasks blend stylistic and semantic
variation, providing a rich probe of compositional


128 tokens 512 tokens
Data MODELS

STELt SoCt PANT STELt SoCt PANT

Vanilla 3164078 91.5+0.56 60.57 | 30.6+0.86 91.7+0.33 63.52

5 iBERT-v1 37.5+£041 92.3+0.33 60.88 | 38.9+081 92.0+0.29 59.49
9 iBERT-v2 28.541.13 88.7+0.68 58.95 | 3144146 91.1+0.68 59.41
&  iBERT-v3-1 | 38.3 +002 92.8 + 0.48 58.15 | 3824075 92.040.32 59.39
iBERT-v3-10 | 38.7+044 92.5+0.30 58.29 | 36.0+0.21 92.0+030 59.41
Vanilla 30.6+0.93 88.9+0.53 58.96 | 30.4+0.94 89.6+0.38 62.76

A  iBERT-v1 38.6 + 1.57 90.040.22 58.96 | 35.7+1.76 89.5+0.70 61.53
3  iBERT-v2 33.440.54 89.14050 58.47 | 32.341.13 89.040.54 59.54
= iBERT-v3-1 | 380+0.85 90.2+0.30 58.56 | 36.4+071 90.140.58 61.43
iBERT-v3-10 | 38.0+0.88 89.6+0.65 60.96 | 34.8+0.93 89.8+0.32 60.34
Vanilla 27.140.70 27.541.18 58.20 | 27.3+0.97 28.9+1.53 61.92

5 iBERT-vl 28.5+086 804062 59.29 | 27.2+074 8040.73 60.74
3 iBERT-v2 25.99+0.76 6.74065 57.77 | 2684056 6340.75 58.63
= iBERT-v3-1 2814059 744042 59.47 | 27.14073 7440.50 60.87
iBERT-v3-10 | 28.5+0.58 7.34041 59.87 | 265+096 7.4+0.68 61.07

Table 1: Direct-style: STEL and SoC reported on the 45 style groups (40 of StyleDistance + 5 of Wegmann STEL).
Proxy-style: PAN reports average AUC% on PAN11/13/14/15 authorship verification tasks. The models are trained
on corresponding DATA triplets for 5 runs. Median token-lengths: 19 for SD, and 24 for WG (Appendix Table 7).

generalization. The results are in Tables 1 and 4.

WG training dataset is a mixed dataset, contain-
ing author identity supervision with intertwined se-
mantic and stylistic signals. Unlike Vanilla, which
entangles all available cues, iBERT is architec-
turally designed to tease apart these differentials in
an interpretable fashion. This enables it to maintain
competitive performance even when the supervi-
sion is noisy or not explicitly style-specific—while
still excelling on clean, style-focused tasks like
STEL and SoC.

At 128 tokens, all iBERT variants match or
slightly outperform Vanilla, with iBERT-v3-10
achieving the highest AUC (59.87%) for WG. This
is notable not because iBERT ignores semantic in-
formation, but because it selectively organizes it in
disentangled form—generalizing well even when
stylistic cues are weakly defined.

At 512 tokens, Vanilla slightly outperforms
iBERT (61.92% vs. 61.07% for v3), with the
largest margins on PAN13 and PANIS, that are
characterized by semantic drift and cross-topic vari-
ation. This suggests that Vanilla benefits more from
extended context, due to its architecture’s tight cou-
pling of semantic and stylistic features. In contrast,
iBERT’s modular structure—designed to isolate
stylistic axes—does not directly exploit semantic
continuity. Still, the gap stays slim, highlighting
the semantic signals acquired in MLM stage remain
accessible even in disentangled representations.

Interestingly, even iBERT-v2, which lagged on

STEL and SoC, performs competitively on PAN.
This underscores that semantic signals are pre-
served, even when stylistic specialization is weak.
Looking across datasets, iBERT performs best on
PAN11 and PAN14, where stylistic consistency
is high and maintains near-parity on PAN13 and
PAN15, where semantic variation dominates.

While WG provides author identity supervision,
its examples entangle stylistic and semantic cues:
offering iBERT no clean signals to isolate. In con-
trast, SD supplies disentangled stylistic supervi-
sion but lacks authorial labels entirely. This creates
a supervision-task mismatch for iBERT, which is
architecturally primed to extract interpretable sig-
nals. Yet it adapts: from WG, it organizes con-
flated authorial traits (e.g., tone, topic, lexical pat-
terns) into structured axes; from SD, it learns clean
stylistic subspaces that can approximate the mixed
style-content clusters found in PAN’s test sets.
That iBERT performs strongly across PAN bench-
marks—despite this misalignment—demonstrates
its ability to decompose and generalize whichever
discriminative signals are available, even when
style and semantics are blended.

5.4 Interpretability Analysis of iBERT

As highlighted before, the main goal of developing
iBERT is to encode inputs as mixtures of inter-
pretable sense vectors. To examine whether spe-
cific senses specialize in capturing coherent stylis-
tic structure, we analyze the alignment between


Top-ALIGNED STYLE AXES EMERGENT THEME

With Emojis / No Emojis
£=0 Frequent / Infrequent Conjunctions
Frequent / Infrequent Personal Pronouns

Surface-level
markers

All Upper Case / Proper Capitalization

0=1 Text Emojis / No Emojis Orthographic and
~ Long / Short Average Word Length visual style
With / Without Number Substitution
Humorous / Non-Humorous
0=2 Sarcastic / Non-Sarcastic Affect and
a Metaphoric / Literal expressive tone
Offensive / Non-Offensive
0=3 All Lower Case / Proper Capitalization Textual correctness
_ With Misspellings / Normal Sentence and noise
e=4 More / Less Frequent Function Words FinSCOHaLSRaRRnRE
a With / Without Nominalizations §
0=5 Active / Passive Syntactic voice
“= Contracted / Non-Contracted and register
0=6 Frequent / Infrequent Pronouns Pronoun and
~— More / Less Frequent Verbs verbal focus
0=7 With / Without Determiners Grammatical

Certain / Uncertain commitment

Table 2: Representative sense-style alignments in
iBERT-v3-10, based on sense activation (Table 6).

All listed axes are the top-aligned style for their respec-
tive sense (i.e., highest probing activation).

sense dimensions and styles via probing and con-
trolled ablations. Table 2 groups the style pairs
most strongly aligned with each sense for iBERT-
v3-10, identified by first finding the highest acti-
vation senses per style, and then performing com-
binatorial optimization to group them (Appendix
Table 6). Clear thematic clustering is observed: i.e.,
sense (=2 aligns with affective and expressive tone
(sarcasm, metaphor), while (=0 captures surface-
level markers like emoji use and personal pronouns.
These patterns suggest that individual senses struc-
turally specialize in distinct stylistic attributes.

To validate these groupings, Table 3 reports the
change in cosine distance between opposing style
centroids before and after removing each sense
contribution in final representation. When ablating
a style-aligned sense, the separability between style
polarities drops substantially (e.g., 66% for 0 =
3), confirming that key stylistic information are
isolated within that sense. In contrast, most non-
target styles exhibit low or negligible change in
distance, indicating minimal collateral disruption:
a hallmark of localized specialization.

Interestingly, for 2=6 and f=7, ablating these
senses harms separation among non-aligned styles
(i.e., negative ADist), suggesting they may en-
code semantic content that generalizes across style
boundaries. This aligns with the style groups they
capture (e.g., pronoun and verb usage, determin-
ers, certainty), which plausibly reflect broader dis-
course semantics. These observations highlight a

nuanced balance between stylistic and semantic
organization within iBERT’s modular space.

Taken together, these results demonstrate that
sense dimensions in iBERT naturally organize
around interpretable stylistic clusters, enabling ex-
plicit, localized editing of style without disrupting
unrelated attributes.

5.5 Visualizing Targeted Editing via Ablation

To visualize iBERT’s ability to localize and disen-
tangle style, we conduct controlled ablations over
its sense vectors. Figure 2 shows t-SNE projections
for three representative styles ablated along their
most aligned senses (Table 2). In all cases, ablating
the target sense causes the positive samples (red)
to move toward the negative centroid (gray), with
relative distance dropping up to 84%. These reflect
localized edits validating iBERT’s controllability.

For ablations on unrelated (non-target) senses,
see Appendix A.6, which confirm that unrelated
styles remain stable under targeted edits, affirming
local modularity of learned sense vectors.

6 Discussion

Interpretability without performance tradeoff.
iBERT delivers inherently interpretable embed-
dings, while maintaining competitive performance
across both style-intensive and mixed-style tasks.
Because each input is represented as a sparse mix-
ture of sense vectors, users can inspect, edit, or
ablate specific dimensions with explicit attribution
to stylistic or semantic features. Unlike post-hoc at-
tribution methods, these controls are embedded in
model’s structure that directly alters representation.

Disentanglement through structure and signal.
Our results confirm that architectural inductive bias
alone is insufficient: meaningful disentanglement
emerges only with high-quality supervision (e.g.,
SynthSTEL). Under proxy-style signals (e.g., WG-
only), sense specialization weakens, and style po-
larity collapses. This co-dependence of structure
and supervision underscores a key design insight:
iBERT is not style-specific, but learns to modular-
ize whichever axes are discriminative in the data.

Localized edits, global control. Ablating a sin-
gle sense (e.g., €=1 for emoji, or 2=4 for nominal-
ization) shifts the representation toward the nega-
tive style without disrupting unrelated styles. These
transformations are visible both numerically (Ta-
ble 3) and spatially (Fig. 2), confirming that iBERT


TARGET-ALIGNED STYLES

NON-TARGET STYLES

SENSE
#STYLES ORIG Epir ADIstT(%) ORIG EpiIT ADIST(%)

£=0 3 0.812 0.459 38.5 0.405 0.278 1.2
g=1 4 0.725 0.321 58.0 0.390 0.250 15.2
t=2 4 0.563 0.310 40.4 0.412 0.299 -2.2
f=3 2 1.333. 0.469 65.7 0.414 0.324 4.3
g=4 2 0.336 0.168 46.3 0.392 0.232 77
€=5 2 0.326 0.237 26.1 0.452 0.271 8.7
£=6 2 0.531 0.395 255 0.403 0.272 -10.7
f=7 2 0.353 0.328 7.0 0.488 0.347 -2.6

Table 3: Impact of sense-level ablation in iBERT-v3-10, using style groups from Table 2. We report mean cosine
distance to the opposite style centroid, before and after ablating sense @. Left: styles aligned with @; Right: all others.
ORIG = original; EDIT = post-ablation; ADIST = relative distance reduction (ft = stronger disentanglement).

= e gt
eg 8 ps i'
ag
‘Ay
Bats 3,
i) of

(a) Text Emojis / No Emojis
(€ = 1, ADist: 84 %)

(b) With/Without Nominaliza- (c) All Lowercase / Proper
tions (€=4, ADist: 67%)

Caps (€ = 7, ADist: 70%)

Figure 2: Style-edit t-SNEs for iBERT-v3-10, ablating the most aligned sense @ for a given style. Red: original
positive samples; Blue: edited ablated positive samples; Gray: negative samples. Arrows are from original-positive-
centroid, to: edited-positive-centroid. Edits control semantics along: (a) visual form, (b) syntactic function, and (c)
grammatical commitment. ADist: relative decrease in mean distance of positive samples to the negative centroid.

learns axis-aligned subspaces that can be controlled
with precision. This kind of intervention is infeasi-
ble in dense encoders like SBERT or Vanilla BERT.

Semantic resilience in modular space. Despite
modularizing stylistic signals, iBERT retains gen-
eralization capacity in semantically entangled tasks
(e.g., PAN13/15). Even sense vectors associated
with stylistic axes (e.g., =6, 7) appear to encode
semantic scaffolding, since ablating them harms
non-target style separability. This indicates that the
modularity does not rigidly isolate content, but sup-
ports a soft balance between style and semantics.

Bridging representation learning and sociolin-
guistics. Emergent sense specializations align
with sociolinguistic constructs such as formality,
register, modality, and expressive tone. This align-
ment arises without explicit annotation, suggesting
iBERT’s decomposition mirrors meaningful axes of
human communication. These findings open path-
ways for interpretable modeling in domains like
forensics, social analysis, and fairness auditing.

Applications. iBERT is not a style model, but
a general architecture to tease out discriminative
signals into interpretable senses, and it supports
a range of feature—conditioned retrieval, classi-
fier debiasing via sense ablation, latent data aug-

mentation, and embedding-level control in RAG
pipelines. See Appendix A.1 for more details.

7 Conclusion

We introduced iBERT (interpretable-BERT), a
modular encoder that produces sparse, inter-
pretable, and controllable embeddings without
compromising performance. By structuring each
input as a mixture over reusable sense vectors, iB-
ERT supports inspection, editing, and attribution
of linguistic properties within the representation.
iBERT matches or outperforms dense baselines
across stylistic and semantic tasks like STEL, SoC,
and PAN, while enabling: (a) disentangled encod-
ing of style and semantics via sense-probing, (b)
precise edits (e.g., emoji, formality), and (c) align-
ment of emergent senses with sociolinguistic axes.

Rather than relying on post-hoc interpretabil-
ity methods, iBERT is designed to be explainable
by construction. Its structure reveals how repre-
sentations evolve and interact—making it a viable
backbone for transparent, modular NLP.

We release our models to encourage further ex-
ploration of compositional embeddings, zero-shot
control, and sense-aware generation. iBERT offers
a step toward embedding models that are not only
powerful, but inherently understandable.


8 Limitations

While iBERT enables modular, interpretable repre-
sentations with strong empirical performance, a few
limitations remain. The model’s design introduces
a tradeoff between disentangled structure and tight
semantic coupling: in tasks requiring deep contex-
tual integration, this can lead to slight performance
gaps compared to dense encoders.

In addition, sense specialization can be sensitive
to sequence length. Our training datasets: SynthS-
TEL (median 19 tokens) and WG (24 tokens) offer
limited context, which may constrain the model’s
ability to fully activate or separate sense subspaces.
This could partially explain weaker gains in tasks
involving longer-form semantics.

Lastly, while we focused on English, extending
sense-level modularity to morphologically rich or
low-resource languages remains an open direction.

These limitations offer promising opportunities
to expand iBERT into a broader, language-agnostic
foundation for interpretable and controllable repre-
sentation learning.

9 Ethics Statement

iBERT is developed as a general-purpose encoder
that produces interpretable, controllable sentence
representations by decomposing inputs into struc-
tured mixtures over sense vectors. This modular
design supports greater transparency in how lin-
guistic features—whether semantic, syntactic, or
stylistic—contribute to downstream decisions.

We believe such structured interpretability fos-
ters accountability in language technologies. How-
ever, we recognize potential misuse: modular rep-
resentations that expose linguistic traits may be
leveraged for tasks like profiling or authorship at-
tribution, which can carry privacy risks in sensitive
settings. These risks are pronounced when models
are applied without consent, or outside the distribu-
tion of the data used during training.

iBERT is trained on public data, including syn-
thetic edits (StyleDistance) and large-scale web
corpora. As with all pretrained models, underlying
biases in this data may influence the dimensions
that emerge. We encourage careful evaluation be-
fore deployment in applications with societal or
demographic impact.

We release our models and code for research
use, with the goal of encouraging safe, transpar-
ent, and interpretable alternatives to opaque neural
representations.

Acknowledgments

We thank John Hewitt (Columbia University) for
helpful discussions during experiment and model
design. We thank Dr. Cheng Wu (Microsoft) for
his insights during modeling and experiment de-
sign, and supporting our research. We thank Ajay
Patel (University of Pennsylvania), Anna Wegmann
(Utrecht University), Nicholas Andrews (Johns
Hopkins University), Joel Tetreault (Dataminr),
and Sudha Rao (Microsoft Research) for grant-
ing access to relevant datasets. We also thank
David M. Rothschild (Microsoft Research), Vishal
Chowdhary (Microsoft Office AID, Yasaman Ameri
(Visa Inc.), and Aparna Balagopalan (MIT) for
early conversations that helped shape the first au-
thor’s thinking around research problem formula-
tion and scoping.

References

Marina Danilevsky, Kun Qian, Ranit Aharonov, Yan-
nis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A
survey of the state of explainable AI for natural lan-
guage processing. In Proceedings of the Ist Confer-
ence of the Asia-Pacific Chapter of the Association
for Computational Linguistics and the 10th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 447-459, Suzhou, China. Association
for Computational Linguistics.

Tri Dao. 2024. FlashAttention-2: Faster attention with
better parallelism and work partitioning. In Jnter-

national Conference on Learning Representations
(ICLR).

Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav
Goldberg. 2021. Amnesic probing: Behavioral expla-
nation with amnesic counterfactuals. Transactions of
the Association for Computational Linguistics, 9:160-
175.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894-6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

John Hewitt, John Thickstun, Christopher Manning, and
Percy Liang. 2023. Backpack language models. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 9103-9125, Toronto, Canada.
Association for Computational Linguistics.

Alon Jacovi and Yoav Goldberg. 2020. Towards faith-
fully interpretable NLP systems: How should we
define and evaluate faithfulness? In Proceedings


of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 4198-4205, On-
line. Association for Computational Linguistics.

Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga
Vechtomova. 2019. Disentangled representation
learning for non-parallel text style transfer. In Pro-
ceedings of the 57th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 424—434,
Florence, Italy. Association for Computational Lin-
guistics.

Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz,
Marianna Apidianaki, Kathleen McKeown, and Chris
Callison-Burch. 2025. StyleDistance: Stronger
content-independent style embeddings with synthetic
parallel examples. In Proceedings of the 2025 Con-
ference of the Nations of the Americas Chapter of the
Association for Computational Linguistics: Human
Language Technologies (Volume 1: Long Papers),
pages 8662-8685, Albuquerque, New Mexico. Asso-
ciation for Computational Linguistics.

Guilherme Penedo, Hynek Kydliéek, Loubna Ben Al-
lal, Anton Lozhkov, Margaret Mitchell, Colin Raffel,
Leandro Von Werra, and Thomas Wolf. 2025. The
fineweb datasets: decanting the web for the finest
text data at scale. In Proceedings of the 38th Interna-
tional Conference on Neural Information Processing
Systems, NIPS °24, Red Hook, NY, USA. Curran
Associates Inc.

Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982-3992, Hong Kong, China. Association for Com-
putational Linguistics.

Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Sub-
ramanian, Matt Gardner, and Sameer Singh. 2019.
AllenNLP interpret: A framework for explaining
predictions of NLP models. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP): System Demonstrations, pages
7-12, Hong Kong, China. Association for Computa-
tional Linguistics.

Benjamin Warner, Antoine Chaffin, Benjamin Clavié,
Orion Weller, Oskar Hallstr6m, Said Taghadouini,
Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom
Aarsen, Griffin Thomas Adams, Jeremy Howard, and
Iacopo Poli. 2025. Smarter, better, faster, longer:
A modern bidirectional encoder for fast, memory
efficient, and long context finetuning and inference.
In Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2526-2547, Vienna, Austria.
Association for Computational Linguistics.

Anna Wegmann and Dong Nguyen. 2021. Does it cap-
ture STEL? a modular, similarity-based linguistic

10

*) Mon

o) 4

ul aay
a
oa

(a) Vanilla-(S)BERT:
Sentences with Misspellings

(b) iBERT-v2:
Sentences with Misspellings

(c) Vanilla-(S)BERT:
Emoji Usage

(d) iBERT-v2:
Emoji Usage

Figure 3: t-SNE projections of sentence embeddings for
SynthSTEL. iBERT consistently separates contrastive
style variants (e.g., misspelled sentences, emoji usage)
better than Vanilla-SBERT, showing clearer margins
and lower entanglement. Blue points represent positive
samples and gray crosses represent negative samples.
Despite iBERT-v2 being the most underperforming iB-
ERT variant it still matches Vanilla’s performance, and
the separation of positive and negative styles is cleaner.

style evaluation framework. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, pages 7109-7130, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Anna Wegmann, Marijn Schraagen, and Dong Nguyen.
2022. Same author or just same topic? towards
content-independent style representations. In Pro-
ceedings of the 7th Workshop on Representation
Learning for NLP, pages 249-268, Dublin, Ireland.
Association for Computational Linguistics.

A Appendix

A.1_ Broader Applications

iBERT enables embedding-level interpretability
and control, unlocking several downstream capa-
bilities beyond benchmarking.

Style-Conditioned Retrieval. Rather than rely-
ing on keyword overlap or dense similarity, queries
can be modified in latent space to match stylis-
tic attributes (e.g., formality, emoji use). This en-
ables retrieval that is both semantically relevant
and stylistically aligned—particularly useful in as-
sistant and RAG pipelines where tone coherence


128 TOKENS 512 TOKENS
MODELS
AvG PANI1 PANI13 PANI4 PANI5 |) AvG PANI1 PANI3 PANI4 PANI5

> Vanilla 60.57 79.34 56.61 52.84 53.47 63.52 78.48 69.89 56.40 49.31
> iBERT-v1 60.88 87.64 53.50 51.37 50.99 59.49 79.19 55.95 52.52 50.31
© iBERT-v2 58.95 83.49 48.70 51.03 52.57 59.41 = 72.83 57.88 52.86 54.08
a iBERT-v3-1 58.15 80.42 50.56 50.98 50.64 59.39 = 78.54 57.04 52.60 49.37

iBERT-v3-10 | 58.29 80.73 49.61 50.47 52.36 59.41 77.30 56.19 53.62 50.51

Vanilla 58.96 68.82 63.04 52.55 51.43 62.76 70.35 71.46 57.00 I2.23
A iBERT-v1 58.96 63.75 66.87 54.45 50.77 61.53 71.64 67.92 55.91 50.64
3 iBERT-v2 58.47 70.25 59.64 53.10 50.88 59.54 66.60 65.32 54.75 51.50
= iBERT-v3-1 58.56 69.08 61.33 53.90 49.92 61.43 70.01 66.15 56.82 52.72

iBERT-v3-10 | 60.96 79.02 60.72 52.91 51.19 60.34 71.03 63.72 55.95 50.65
> Vanilla 58.20 65.59 62.03 53.55 51.62 61.92 65.05 70.18 58.19 54.27
> iBERT-v1 59.29 64.69 67.99 55.80 48.69 60.74 67.14 66.89 57.77 51.17
9 iBERT-v2 57.77 66.93 62.27 54.17 47.74 58.63 62.41 65.75 56.58 49.78
S iBERT-v3-1 59.47 66.12 66.98 55.68 49.11 60.87 65.92 68.87 58.29 50.38

iBERT-v3-10 | 59.87 69.70 65.15 55.23 49.40 61.07 68.02 67.73 57.92 50.59

Table 4: AUC (%) on PAN 11/13/14/15 authorship verification tasks for 128 vs. 512 token-size models. Rows
are grouped by training setup (SD, WG, WG+SD). Bold indicates the best mean per group-token combination.
Underline highlights global best per token. /talics mark values reasonably close to the best per group.

(a) £ = 0 ablated that aligns
with More/Less conjunctions

(b) 2 = 2 ablated that aligns
with Sarcasm / No sarcasm

Figure 4: All Upper Case/Proper Capitalization (¢=1)
maintains separation on ablating other styles. Colors as
in Fig. 2- Red: original, Blue: edited, Gray: negatives.

Model 7 Aggregation STELt SoCt
vl co. =>. Mean (uniform blend) 37.5 92.3
v3-10 10 Peaked blend 38.1 92.8
v3-1 1 Softmax blend 38.3 92.8
v2 0 Top-sense only 28.5 88.7

Table 5: Effect of aggregation sharpness 7 on iBERT,
trained on StyleDistance triplets (128 token size).

matters.

Classifier Debiasing via Sense Ablation. Mask-
ing specific sense dimensions (e.g., sarcasm, punc-
tuation cues) neutralizes stylistic noise before clas-
sification. This supports robust models that focus
on core semantics—offering interpretability by con-
struction.

Latent Data Augmentation. Controlled sense
edits allow creation of style-shifted variants with-

11

out decoding. These augmentations improve ro-
bustness and style-invariance for downstream clas-
Sifiers.

Style-Aware Personalization. Sense-level edits
adapt outputs to user preferences (e.g., concise vs.
elaborate tone) without retraining—enabling per-
sonalization in retrieval and generation pipelines.

RAG and Retrieval Robustness. iBERT sup-
ports query normalization, document style filtering,
and attribution for retrieval decisions—addressing
failure modes like tone mismatch or prompt drift
in RAG systems.

Forensic and Monitoring Applications. iBERT
enables tracking tone shifts, authorial style evo-
lution, or discourse change over time—useful for
moderation, attribution, or sociolinguistic analysis.

Controller for Generation. Sense vectors from
iBERT can guide decoder-based models (e.g.,
T5, GPT) as structured, interpretable control
signals—supporting style-conditioned generation
without prompt engineering.

Representation Auditing. Because embeddings
decompose into interpretable senses, iBERT sup-
ports bias audits, attribution analysis, and fairness
enforcement via projection or adversarial masking.

Zero-Decode Semantic Transfer. Sense-level
edits enable style transfer, paraphrasing, or domain
adaptation directly in latent space—without requir-
ing any generation.


STYLE £=0 @€=1 @€=2 ¢€=3 €=4 €=5 %€=6 €=T7 BEST
Active / Passive 0.6222 0.6222 0.5278 0.6278 0.6111 0.6444 0.5944 0.6333 5
Affective process / Perceptual process 0.6389 0.5444 0.6500 0.5889 0.6222 0.5833 0.6444 0.6278 2
Affective processes / Cognitive processes 0.6722 0.5833 0.6833 0.6111 0.6833 0.6389 0.7000 0.6722 6
All Lower Case / Proper Capitalization 0.9778 0.7611 0.9667 0.8444 0.9667 0.8778 0.9833 0.9833 7
All Upper Case / Proper Capitalization 0.9278 0.9444 0.9278 0.9333 0.9333 0.9389 0.9167 0.9222 1
Certain / Uncertain 0.6444 0.6056 0.6167 0.6444 0.6389 0.6500 0.6833 0.6944 7
Cognitive process / Perceptual process 0.6444 0.6167 0.5500 0.6500 0.6500 0.6500 0.6444 0.6667 7
Complex / Simple 0.5944 0.5444 0.6278 0.5611 0.6000 0.5556 0.6333 0.5944 6
Fluent sentence / Disfluent sentence 0.6278 0.5889 0.5111 0.6167 0.6111 0.6167 0.6444 0.6000 6
Formal / Informal 0.7833 0.7389 0.8167 0.7667 0.8167 0.7667 0.7667 0.7778 2.
Long average word length / Short average word length 0.8389 0.8444 0.7944 0.8444 0.8389 0.8333 0.8278 0.8389 1
Offensive / Non-Offensive 0.9111 0.7389 0.9444 0.7611 0.9056 0.7722 0.9111 0.9000 2
Polite / Impolite 0.6389 0.4167 0.7333 0.4833 0.6667 0.4889 0.6444 0.5833 2
Positive / Negative 0.5944 0.5111 0.6944 0.5222 0.5944 0.5111 0.6056 0.5556 2
Present-focused / Future-focused 0.7000 0.6278 0.5556 0.6556 0.6833 0.6556 0.6778 0.7000 7
Present-focused / Past-focused 0.6778 0.5778 0.6333 0.5944 0.6278 0.5833 0.6944 0.6667 6
Self-focused / Audience-focused 0.8111 0.5778 0.7944 0.6444 0.7889 0.6389 0.8278 0.8111 6
Self-focused / Inclusive-focused 0.7500 0.5222 0.7556 0.5611 0.7000 0.5667 0.7333 0.7222 2
Self-focused / Third-person singular 0.6333 0.6056 0.5833 0.6167 0.6167 0.6111 0.6222 0.6444 7
Self-focused / You-focused 0.7944 0.5500 0.7778 0.6278 0.7556 0.6333 0.7889 0.7667 0
Sentence With a Few Misspelled Words / Normal Sentence 0.8889 0.8722 0.8389 0.8889 0.8944 0.8889 0.9111 0.9056 6
Text Emojis / No Emojis 0.9278 0.9333 0.9167 0.9278 0.9333 0.9333 0.9167 0.9167 1
With Emojis / No Emojis 0.9389 0.9389 0.9222 0.9389 0.9389 0.9389 0.9278 0.9389 0
With Humor / Without Humor 0.7556 0.6944 0.8000 0.7167 0.7667 0.7167 0.7778 0.7222 2;
With articles / Less frequent articles 0.6889 0.6889 0.6000 0.6889 0.6833 0.6833 0.7278 0.7167 6
With common verbs / Less frequent common verbs 0.7889 0.7278 0.7611 0.7111 0.7444 0.7111 0.8111 0.7444 6
With conjunctions / Less frequent conjunctions 0.7944 0.7667 0.7944 0.7500 0.7833 0.7500 0.7833 0.7444 0
With contractions / Without contractions 0.8167 0.5778 0.7056 0.6722 0.7722 0.7000 0.8333 0.8278 6
With determiners / Less frequent determiners 0.6778 0.7222 0.6389 0.7056 0.6889 0.7000 0.7167 0.7278 7
With digits / Less frequent digits 0.8889 0.8389 0.8889 0.8556 0.8833 0.8667 0.8667 0.8778 0
With frequent punctuation / Less Frequent punctuation 0.6667 0.6167 0.7111 0.6389 0.6778 0.6389 0.7278 0.7000 6
With function words / Less frequent function words 0.7056 0.6944 0.6944 0.7056 0.7111 0.7056 0.7056 0.6944 4
With metaphor / Without metaphor 0.7278 0.6889 0.8222 0.6944 0.7667 0.7056 0.7389 0.7222 2
With nominalizations / Without nominalizations 0.8167 0.8056 0.8222 0.8111 0.8222 0.8222 0.7889 0.8000 4
With number substitution / Without number substitution 0.9278 0.9111 0.9389 0.9167 0.9222 0.9167 0.9222 0.9167 2
With personal pronouns / Less frequent pronouns 0.7389 0.7389 0.7111 0.7111 0.7167 0.7056 0.7222 0.7000 0
With prepositions / Less frequent prepositions 0.6333 0.6722 0.6111 0.6889 0.6611 0.6833 0.6667 0.6778 3
With pronouns / Less frequent pronouns 0.7611 0.7222 0.7444 0.7111 0.7278 0.7056 0.7889 0.7222 6
With sarcasm / Without sarcasm 0.7778 0.7278 0.8167 0.7278 0.7889 0.7333 0.7833 0.7278 2
With uppercase letters / Without uppercase letters 0.6556 0.5333 0.6722 0.5444 0.6611 0.5444 0.6500 0.6278 2;

Table 6: [iBERT-v3-10 (128-tokens)] Probing activations for style classes across all senses (¢ = 0 to 7). The BEST
column indicates the latent sense with the highest activation for each style. Style names in this table reflect the
original labels from the StyleDistance dataset. In the main text, shorter style names are used for presentation clarity.

DATASET SPLIT MEDIAN TOKENS >128 TOKENS
, Train 19 0.00%
StyleDistance Test 10 0.00%
Ww Train 24 6.78%
eemann Test 24 6.59%

Table 7: Token-statistics across datasets and splits.

A.2 Effect of Aggregation Sharpness

As described in Section 3, 7 controls how sharply
the model aggregates across its k=8 sense vec-
tors—ranging from uniform averaging (T—>00) to
top-sense-only pooling (r=0). Intermediate values
apply softmax pooling over sense magnitudes.
Sharper but soft aggregation version of v3 cre-
ated by 7=10, v3-10 yields the best STEL and
SoC performance, as reported in the full results

12

(Table 1). Appendix Table 5 highlights a repre-
sentative subset across 7 values, to draw attention
to how aggregation sharpness affects performance
and stability. This trend suggests that axis-selective
pooling supports better alignment with stylistic di-
mensions. In addition to strong performance, v3-10
also exhibits consistently lower variance across
runs (see Table 1), indicating more stable conver-
gence during training. In contrast, hard top-sense
selection degrades SoC, suggesting that some de-
gree of sense blending is beneficial for generaliza-
tion. These trends mirror regularization analogies:
peaked pooling acts like L1, while smoother blend-
ing behaves L2-like.


FROZEN PHASE | UNFROZEN PHASE | — TOTAL
VARIANT
Time Loss / Epochs | Time Loss / Epochs | Time Epochs
ee iBERT 23h 1.6912/57 | 3d6h  1.3365/85 4d 5h 142
™ BERT 2h 1.3607 /5 Id2h = 1.2228/31 1d Sh 36
a iBERT 2d9h 1.4418/46 | 1d7h  1.2941/16 | 3d 16h 62
‘© BERT 14h 1.1876 /6 3d 8h 1.0940/46 | 3d 22h 52

Table 8: Training runtimes, epochs, and final losses for iBERT-MLM and Vanilla-BERT (MLM) under 128- and
512-token limits. Epoch counts exclude early-stopping patience rounds.

A.3 Effect of Pooling Sharpness 7

To study the impact of pooling sharpness, we eval-
uated iBERT-Sentence ({SBERT) under four val-
ues of 7 € {0,1, 10, co} referred to as iBERT-v1,
iBERT-v2, iBERT-v3-1 and iBERT-v3-10.

We find that r=0 (v2, top-sense pooling) yields
the highest sense-level sparsity, with sharper sense
specialization and more effective style edits ($5.5).
T=1 (v3, soft pooling) balances performance and
interpretability. 7=10 leans towards uniform
weighting (v1) across senses. T—0o (vl, mean
pooling) exhibits minimal sense-selectivity and
lower editability. This confirms that sharper pool-
ing enhances sense-disentanglement, critical for
controllable edits and style probing.

A.4_ Training and Implementation Details

All iBERT models use k=8 senses, 768-dim hid-
den states, and FlashAttention-2 (Dao, 2024) for
fast attention computation. AdamW optimizer with
learning rate of (2x10~°), a batch size of 99 is used.
The models have an early stopping with 3-epoch
patience over validation loss and Flash-Attention-
2 is used for faster compute. We perform both
masked language modeling and sentence embed-
ding training at input lengths of 128 and 512 tokens.
MLM uses 5% of FineWeb (750B tokens), and
contrastive fine-tuning is performed using anchor-
positive-negative triplets from StyleDistance (SD)
and Wegmann (WG) datasets.

Training. All models are initialized with encoder
weights from ModernBERT (Warner et al., 2025),
replacing into either a Vanilla (standard) BERT
or our proposed iBERT architecture. Due to the
larger Context Sense Block in iBERT, we freeze
the encoder weights in an initial warmup stage (for
both baseline and iBERT to maintain parity) and
allow the heads to converge. Once stabilized and
converged, we unfreeze the full model and resume
end-to-end training until convergence. This process
is used in both MLM and contrastive phases. The

13

details of the MLM stage are listed in Table 8.

Contrastive training is supervised with either In-
foNCE or triplet loss, depending on the dataset.
Triplets are grouped as (anchor, pos, neg) and pro-
cessed using sentence encoders that implement
SBERT-style wrappers over both BERT and iB-
ERT backbones. Our training code supports sense
ablation via -sense_gain, and different SBERT
pooling strategies (v1, v2, v3-1, v3-10).

Training is run on 4xNVIDIA A100 (80GB)
GPUs using mixed-precision (torch. bfloat16).
We use AdamW with a learning rate of 2 x 107~,
batch size 99, and early stopping with 3-epoch
patience on validation loss. All training stages are
fully configurable via CLI or YAML using a shared
interface, and can resume from checkpoints.

Software. Our implementation stack includes Py-
Torch (2.7.0), Transformers (v4.51.3), Sentence-
Transformers (5.0.0), FlashAttention-2, Datasets
(3.6.0), and Accelerate (1.7.0). Tokenization is
handled via HuggingFace Tokenizers (0.21.1), and
logging is supported through Terminal, JSONL,
TensorBoard, and Weights-and-Biases (W&B).

Data Statistics For FINEWEB, we split the 5%
slice into a 90:10 train:dev split, without needing
a test set, given it is used in an MLM. For the
sentence-embedding training stages, we reuse the
predefined train/test split provided by the dataset
authors, and use 90:10 split of original train dataset
for training and validation correspondingly.

A.4.1_ Sentence Embedding Training and
Replications

For sentence-level contrastive training, we train
a total of five distinct sentence encoder models
(per data configuration): a baseline SBERT built
on top of Vanilla-BERT, and four sentence en-
coders built on iBERT-MLM checkpoints using
different pooling frameworks. These correspond
to mean-pooling (vl, T=o0), top-sense (v2, T=0),
softmax-blend (v3-1, 7=1), and peaked-blend (v3-
10, T=10). Each is treated as a separate model, not


as a parameter sweep. All models are trained at
two input lengths (128 and 512) and evaluated on
StyleDistance and Wegmann datasets.

To account for training variability, we perform 5
independent training replications for each sentence
encoder (Vanilla-SBERT and all iBERT variants) at
both token lengths (128 and 512). Reported results
reflect mean and standard deviation across the runs.

model types:

Vanilla, iBERT-v1, v2
iBERT-v3-1, v3-10
datasets:

SD, WG+SD, WG

token lengths:
128, 512
replications per
configuration

Training runs = 5

A.5 Style Contrast Visualization

We visualize the separation of contrastive style
pairs (e.g., number substitution, personal pronoun
frequency) using t-SNE projections. For each ex-
ample, we compare Vanilla-SBERT embeddings
(left) and iBERT-v2 embeddings (right). In both
cases, sentence embeddings were obtained after
training on StyleDistance.

Figure 3 shows that iBERT exhibits stronger
axis-wise separation between stylistic variants,
with clearer clusters and lower embedding over-
lap, consistent with our claims about modularity
and disentanglement.

A.6 Non-Target Style Ablation Visualization

To ensure that iBERT’s sense editing visualizations
(subsection 5.5) are not generic artifacts of pertur-
bation, Figure 4 ablates unrelated senses for unre-
lated (non-target) style contrasts. In all cases, pos-
itive and negative samples remain well-separated,
even as the positive cluster slightly shifts. This
affirms that sense vectors act locally, without broad
entanglement across unrelated styles.

Note that t-SNE primarily preserves local topol-
ogy rather than absolute displacement; thus, these
visualizations should be interpreted qualitatively.
The clear contrast between targeted collapse (Fig-
ure 2) and stable separation in control (Figure 4)
supports our claims of modularity and editability.

14

A.7 Global t-SNE for Style Clustering

We visualize sentence embeddings across all 40
style contrasts using t-SNE, comparing Vanilla-
SBERT and iBERT-v3 in Figure 6. The iBERT
model exhibits sharper geometric structure, with
more linear and segregated style clusters. This
supports our hypothesis that modular pooling (v3)
induces interpretable and axis-aligned representa-
tions, in contrast to the entangled embedding space
of Vanilla-SBERT.

A.8 Measuring Locality of Sense-Based
Editing

For a given style x (e.g., Text Emojis / No Emojis),
we identify the sense * with the highest probing
activation and ablate it during encoding by zeroing
the corresponding sense gain. Let P, denote the
positive examples for style x, and Pr"*, Pet be
their embeddings before and after ablating ¢*, re
spectively. We compute cosine distance between a
set of embeddings A and a centroid pu as:

7"

acA

A, 1) —cos(a,p)). (1)

~ [Al
The target editability (reported in Table 3) is then:

d( PE", pix) — a(PS", ti.)

Az = ori
d( Pr “ Lx)

» (2)

where ji, is the centroid of negative examples for
style x.

To quantify whether sense ablation incurs un-
wanted shifts on other style dimensions, we repeat
this computation for every y # x (with centroid

Hy):

_ d( Pe", pty) — PrP ., fy)
y ori ,
d(P; * fiby)

(3)

We report the following aggregate locality metrics:

1
Avg. Other Shift = ———_—— y Ay,
liy A x}| .
yFL (4)
Max Other Shift = max A,.
yA

Intuitively, a useful and disentangled iBERT sense
should yield a large, positive A, while keep-
ing Avg. Other Shift and Max Other Shift close to
zero, indicating minimal collateral influence on
unrelated style axes.


n k
= oiC(x;
* » » mej Ga), Project into vocab-space

Sense-construction-block,

Sense-construction-block,

These o, are the final

: - Objective:
2 sense-representations Sestod Language Model
Sense-construction-block,., 15% tokens masked
Sense-construction-block,
(—)
C(X qn) E R2XkX4 nxd
Token, Independent per-token (ra) oER Each Q, €R®* (@/0
K-countofientiies Qp j,j > Haw much sense 2 of j-th
—— ai, d
Tokens (7 token contrbute to representation
6 —S=—* Q= H.(Q™) at i-th position
5 kxnx
3 Fa QMERAxdk Qo= H.(Q@)T geRExnxn
Ee (—————)) —w ———
(= =E (a ae ee : kK; a
Tokenn1 SS Ex hy Q@eRexak > i ki 1
Standard Standard : Qa= H. (Q@D)T >
Token, e1= Ex; Transformer Transformer h, : eee QK," a
— : Encoder Encoder : QHER IMA Q=H. (QT
Xn EVE i block-1 block-L i = ‘ an —
Tokens os " KO € RAxd/k K,= H. (K@)? Oaks” | can
= Ex, Ke Kya
—————— L count of Encoder-Blocks we —_—
(or Decoder Blocks without KO ERX K,= H. (K@)?
e;:= Ex, i h, QuKyt Oy
autoregression-masks) In H .
—— : Wi: ——
T
Cin ERA Transformer H=h,, ER®™4|| K®eRexa Ka=H.(KeD)t || QeKe eRe ERD
t :n
Token embeddings i =
ne Hidden:leyer output — (nyt Pre-normalized
Multiplied by Ki= H. (K™) Softmax

a) activation a
corresponding Q®), K() (k-count) normalization
matrices foreachsense EachK, € R®* (4/4)

k-count of entries

(per k-sense)

(a) iBERT (MLM) overview (§3). Each input token is projected to k sense vectors, contextualized using attention-weighted
combinations across all positions. This is a drop-in replacement of BERT for easy adoption.

Coe gh ERE Cheng) € RA
Arg-max on senses during STS
Simple mean-poolon o; learning, for granular control of senses
$10 2* = arg max ,x || 0; ||
eR! (= J 3, = 0
XnEV" | Lo 3} Xin EV"
Tokens A " Tokens
went eR
, ,; + How much sense 2 of i, > How much sense 2 of
‘j-th token contribute to i-th token contribute to
representation at i-th position representation at i-th position
(b) iBERT-v1: simple mean pooling over token represen- (c) iBERT-v2: a single dominant sense index £* is selected,
, :
tations 0;. and only the corresponding components o} ) are pooled.

C(x), € R¢
C(x), € R¢

Che R

ExeR* FF: Rd = Rkxa
xEeVv
E

mbedding Matrix
Convertor
Ee R¢x!M

Feed-forward layer

C(x) € RKX4

FF(E x) C(x) € REX¢

k-sense-vectors each

with d-dimension, per token
(d) Sense Construction Block: each token is mapped via a

feedforward layer to k sense vectors of dimension d.

Figure 5: (a) shows the detailed technical iBERT architecture; (b—c) illustrate two pooling strategies used in sentence
encoding (vl and v2). The softmax-blend pooling variant (v3) lies between these and is described in Section
subsection 3.2. (d) details the sense construction block.

15


Active / Passive
Affective process / Perceptual process

Affective processes / Cognitive processes

All Lower Case / Proper Capitalization

All Upper Case / Proper Capitalization

Certain / Uncertain

Cognitive process / Perceptual process

Complex / Simple

Fluent sentence / Disfluent sentence

Formal / Informal

Long average word length / Short average word length
Offensive / Non-Offensive

Polite / Impolite

Positive / Negative

Present-focused / Future-focused

Present-focused / Past-focused

Self-focused / Audience-focused

Self-focused / Inclusive-focused

Self-focused / Third-person singular

Self-focused / You-focused

Sentence With a Few Misspelled Words / Normal Sentence
Text Emojis / No Emojis

With Emojis / No Emojis

With Humor / Without Humor

With articles / Less frequent articles

With common verbs / Less frequent common verbs
With conjunctions / Less frequent conjunctions

With contractions / Without contractions

With determiners / Less frequent determiners

With digits / Less frequent digits

With frequent punctuation / Less Frequent punctuation
With function words / Less frequent function words
With metaphor / Without metaphor

With nominalizations / Without nominalizations

With number substitution / Without number substitution
With personal pronouns / Less frequent pronouns

With prepositions / Less frequent prepositions

With pronouns / Less frequent pronouns

With sarcasm / Without sarcasm

With uppercase letters / Without uppercase letters

—404

@eeooeeceveeoveceecneoceeececoeecoeoeeeceoeeooeoeeene

—40 20 0 20 40

(a) Vanilla-SBERT (128 tokens)

Active / Passive
Affective process / Perceptual process

Affective processes / Cognitive processes

All Lower Case / Proper Capitalization

All Upper Case / Proper Capitalization

Certain / Uncertain

Cognitive process / Perceptual process

Complex / Simple

Fluent sentence / Disfluent sentence

Formal / Informal

Long average word length / Short average word length
Offensive / Non-Offensive

Polite / Impolite

Positive / Negative

Present-focused / Future-focused

Present-focused / Past-focused

Self-focused / Audience-focused

Self-focused / Inclusive-focused

Self-focused / Third-person singular

Self-focused / You-focused

Sentence With a Few Misspelled Words / Normal Sentence
Text Emojis / No Emojis

With Emojis / No Emojis

With Humor / Without Humor

With articles / Less frequent articles

With common verbs / Less frequent common verbs
With conjunctions / Less frequent conjunctions

With contractions / Without contractions

With determiners / Less frequent determiners

With digits / Less frequent digits

With frequent punctuation / Less Frequent punctuation
With function words / Less frequent function words
With metaphor / Without metaphor

With nominalizations / Without nominalizations

With number substitution / Without number substitution
With personal pronouns / Less frequent pronouns

With prepositions / Less frequent prepositions

With pronouns / Less frequent pronouns

With sarcasm / Without sarcasm

With uppercase letters / Without uppercase letters

40

20

-20

-40

—60
T T
—40 —20 0 20 40

@eecoeoeeceeveeceeoeecveeceeeoeoceoeocoeeoeeeeon enone eeeee

(b) iBERT-v3-10 (128 tokens)

Figure 6: t-SNE projections of sentence embeddings for all 40 style contrast pairs. Each point represents a sentence,
color-coded by its style label. Vanilla-SBERT shows more dispersed and overlapping clusters, while iBERT-v3-10
shows tighter, aligned groupings — indicating improved style modularity and disentanglement. Both these models
were trained on StyleDistance triplets.

16
