arXiv:2510.11098v1 [cs.SD] 13 Oct 2025

VCB Bench: An Evaluation Benchmark for Audio-Grounded Large
Language Model Conversational Agents

Jiliang Hu!”, Wenfu Wang!’, Zuchao Li” , Chenxing Li!, Yiyang Zhao!
Hanzhao Li’, Liqiang Zhang!, Meng Yu', Dong Yu!
'Tencent AI Lab, Beijing, China,
?Wuhan University, Wuhan, China

Abstract

Recent advances in large audio language mod-
els (LALMs) have greatly enhanced mul-
timodal conversational systems. However,
existing benchmarks remain limited—they
are mainly English-centric, rely on synthetic
speech, and lack comprehensive, discrimina-
tive evaluation across multiple dimensions. To
address these gaps, we present Voice Chat Bot
Bench (VCB Bench)—a high-quality Chinese
benchmark built entirely on real human speech.
VCB Bench evaluates LALMs from three com-
plementary perspectives: instruction follow-
ing (including speech-level control beyond text
commands), knowledge understanding (general
knowledge, reasoning, and daily dialogue), and
robustness (stability under perturbations in con-
tent, environment, and speaker traits). Experi-
ments on representative LALMs reveal notable
performance gaps and highlight future direc-
tions for improvement. VCB Bench provides a
reproducible and fine-grained evaluation frame-
work, offering standardized methodology and
practical insights for advancing Chinese voice
conversational models. !

1 Introduction

In recent years, large language models (LLMs)
(Vaswani et al., 2017; Anil et al., 2023) have
achieved remarkable progress in natural language
understanding and generation. Integrating lan-
guage modeling with modalities such as vision and
audio (Radford et al., 2021; Singh et al., 2022)
has further given rise to a new paradigm of multi-
modal learning. Within this trend, large audio lan-
guage models (LALMs)—which combine speech
signal processing with language modeling—have
developed rapidly. Emerging systems such as
StepAudio2 (Wu et al., 2025) and Qwen3-Omni

‘Code and data are available at https: //github.com/
193746/VCB-Bench-Evalkit
“Corresponding authors.

(Xu et al., 2025b) demonstrate end-to-end (E2E)
speech understanding and generation with capabili-
ties in voice question answering, real-time conver-
sation, and audio content analysis. Consequently,
voice conversational agents powered by LALMs
are drawing increasing academic and industrial at-
tention, offering more natural and human-like in-
teractions than text-only systems.

Despite these advances, moving from basic
LALM functionalities to practical voice agents
requires reliable and comprehensive evaluation
tools. Such benchmarks are essential for diagnos-
ing model weaknesses, guiding optimization, and
enabling fair comparisons across systems. While
initial efforts (Chen et al., 2024; Yang et al., 2024;
Lin et al., 2025) have explored instruction follow-
ing, audio understanding, reasoning, and dialogue
scenarios, current evaluation practices remain lim-
ited in three major ways. First, most benchmarks
are English-centric, leaving Chinese—the world’s
most widely spoken language—largely unexplored.
Second, the majority rely on synthetic speech data,
which poorly reflects real-world acoustic variabil-
ity. Third, many are text-derived benchmarks (e.g.,
AlpacaEval (Li et al., 2023), IFEval (Zhou et al.,
2023)), whose formal and lengthy content is un-
suitable for evaluating conversationally grounded
LALMs that should generate natural, colloquial
speech. Addressing these limitations is critical
given China’s large user base and the growing de-
mand for practical, high-quality voice agents.

To bridge these gaps, we introduce Voice Chat
Bot Bench (VCB Bench)—the first comprehensive
evaluation framework for Chinese voice conversa-
tion, built entirely from authentic (non-synthetic)
speech. VCB Bench evaluates LALMs along three
complementary dimensions: (1) Instruction follow-
ing, extending beyond text-based prompts to incor-
porate speech-level control tasks such as adjust-
ing volume, speed, and emotion, with bilingual
(Chinese-English) support; (2) Knowledge, includ-


ing multi-disciplinary general knowledge (12 sub-
jects), mathematical and logical reasoning, daily
dialogue comprehension, and story continuation
for pretraining performance assessment; (3) Ro-
bustness, measuring model stability under real-
world perturbations across content (mispronuncia-
tions, grammatical errors), environment (street, TV
noise), and speaker characteristics (age, accents).

Our proposed VCB Bench is built entirely from
authentic human recordings rather than synthetic
speech. It provides a large-scale, high-fidelity
dataset covering diverse conversational scenar-
ios and introduces a multi-dimensional evaluation
framework that jointly measures knowledge un-
derstanding, instruction following, and robustness
through fine-grained, reproducible tasks. Based on
this benchmark, we conduct a systematic empirical
analysis of state-of-the-art LALMs under unified
settings, revealing their strengths and limitations in
Chinese voice interaction and offering actionable
insights for future model development.

2 Related Work

Large Audio Language Models. Recent LALMs
primarily adopt an E2E audio-language modeling
paradigm, integrating speech understanding and
generation within a unified framework.

Qwen-Audio and Qwen-Omni series (Chu et al.,
2023, 2024; Xu et al., 2025a,b) progressively en-
hance cross-modal alignment and modeling effi-
ciency. Qwen-Audio establishes robust audio-text
alignment, Qwen-Audio2 improves encoding effi-
ciency via multi-scale feature fusion, and the latest
Qwen-Omni models introduce dual-core Thinker-
Talker architectures and multi-codebook pretrain-
ing, achieving low-latency bilingual dialogue.

StepAudio models (Huang et al., 2025; Wu et al.,
2025) focus on tightly coupling recognition and
synthesis. StepAudio integrates a dual-codebook
tokenizer and achieves a remarkably low WER,
while StepAudio2 advances to a fully E2E design
with fixed text-speech token alignment and Chain-
of-Thought reasoning, improving fine-grained par-
alinguistic understanding.

Baichuan-Audio (Li et al., 2025) employs hier-
archical RVQ discretization and dual audio heads
to balance acoustic and linguistic objectives, en-
abling real-time bilingual communication. GLM4-
Voice (Zeng et al., 2024) introduces a three-module
structure (Tokenizer-Backbone-Decoder) support-
ing emotion and dialect modeling. Kimi-Audio

(Ding et al., 2025) fuses continuous acoustic and
discrete semantic tokens in a dual-head architec-
ture, achieving low-latency, high-fidelity streaming
generation.

These models demonstrate rapid progress in uni-
fied audio-language modeling—covering tokeniza-
tion, multimodal fusion, and real-time dialogue—
but systematic benchmarks, especially for Chinese
real-speech interaction, remain scarce. Current
evaluations are mostly qualitative or based on syn-
thetic data, underscoring the need for comprehen-
sive real-speech benchmarks like VCB Bench.

Audio Benchmarks. Recent efforts have in-
troduced several benchmarks to evaluate LALMs
from different perspectives. VoiceBench (Chen
et al., 2024) assesses general knowledge, instruc-
tion adherence, safety, and robustness, mainly
based on existing text datasets such as AlpacaE-
val and SD-QA (Faisal et al., 2021), with lengthy
or highly complex samples removed. OpenAu-
dioBench, released alongside Baichuan-Audio, in-
tegrates question-answering datasets including Spo-
ken LLaMA Questions (Nachmani et al., 2023) and
Web Questions (Berant et al., 2013), and augments
them with a TTS-generated reasoning subset.

AIR Bench (Yang et al., 2024) contains two
components—a basic benchmark covering emo-
tion recognition, ASR, and age estimation, and
a dialogue benchmark evaluating auditory under-
standing and internal knowledge. MMAU (Ku-
mar et al., 2025) and MMAR (Ma et al., 2025) fo-
cus on deep audio reasoning, requiring multi-step
inference grounded in internal audio knowledge.
OmniBench (Li et al., 2024) targets omni-modal
models handling audio, images, and text, where
text queries are paired with multimodal contexts
(speech, music, or sound) to test integrated reason-
ing. Finally, URO Bench (Yan et al., 2025) pro-
vides a bilingual (English-Chinese) comprehensive
set that evaluates audio understanding, reasoning,
and conversational ability—but the speech data are
entirely synthetic (TTS-generated).

Overall, existing benchmarks have significantly
advanced the evaluation coverage of LALMs, yet
they share several limitations: (1) most rely heav-
ily on TTS or synthetic speech, (2) they focus on
English, and (3) their content often derives from
text-centric QA corpora rather than spontaneous
human dialogue. These gaps highlight the need for
a real-speech, Chinese-oriented benchmark offer-
ing multi-dimensional evaluation—the central goal
of our proposed VCB Bench.


(a) Instruction Following (b) Knowledge

(c) Robustness

pw

2 S
é % i &
é % é >
s 7: ae
® “tng vn e
4
ie, <0
esi Tianjin %% ot
a \ oe = ora
Gene! cae vot wisronne
Physics unstable
signal rey
Metin Z 4
Soom, a eS é 2
os os ee &
%y, eo wy? GS ik
% > & <
A :
z %% % s z ‘
4 4% ¢ g %
2 % cd
3 % 2 z %
cS e = 5 3< %
% S os ase
Bi & gs a 4 8

Figure 1: Overview of VCB Bench.

aya z
% 2 3
a, Naan 2 me
2% %, __
ae ee “Me
Yee 33 i 4s
ty § nme, %%
aa? _ =
ces re %
Enetiona ‘
Ontra brow
Valuatic
m ‘Text Instruction tesie
eration Fores Empathy
ge conssl alow
anes
rato
ao” e
8 Fs
wo
e i, we és
xy o~
= % we >
¢ ~
,
é % -
; i ss j
Fy F 2

As shown in the Figure 1, VCB Bench covers three
core dimensions: Instruction Following, Knowl-
edge, and Robustness. Instruction Following in-
cludes Text Instruction Follow (TIF) (e.g., contin-
uation, creation), Speech Instruction Follow (SIF)
(e.g., emotional, volume control), and Multi-turn
Dialog (MTD) tasks. The Knowledge module as-
sesses General Knowledge (GK) across 12 disci-
plines, Mathematical Logic (ML), Discourse Com-
prehension (DC), and Story Continuation (SC). Ro-
bustness introduces real-world perturbations from
speaker variations, environmental noise, and con-
tent modifications to evaluate model stability.

3.1 Dataset Construction

The VCB Bench dataset integrates data from three
distinct sources: third-party professional record-
ings, audio extracted from variety show Q&A seg-
ments and an internally curated two-person con-
versational dialogue dataset. Each source supports
different evaluation modules within the benchmark.

Third-Party Recorded Data. This category
supports the Instruction Following, ML, and SC
tasks under the Knowledge module, as well as the
Robustness module. The production pipeline in-
volves the following steps: First, task types and ex-
amples are defined through team discussion. Next,
commissioned personnel manually compose texts
that fulfill the task requirements. These texts then
undergo manual inspection to ensure quality. Ap-
proved texts are forwarded to a third-party record-
ing team for professional audio production. After
recording, the data team performs quality checks
on the audio. Subsequently, GPT-40-Audio is used

to evaluate audio quality, while GPT-40 assesses
textual quality. Finally, manual screening is con-
ducted to select high-quality samples, determining
the final evaluation dataset.

For Robustness data, it’s text materials are de-
rived from Instruction Following module. The orig-
inal audio from this module serves as the control
group. To control for speaker variability, the same
speaker re-recorded the text under specified inter-
ference conditions (e.g., accent, noisy environment)
wherever possible, using the original audio as a
baseline. For "content variation" types, the text
was first modified (e.g., introducing grammatical or
pronunciation errors) before being re-recorded by
the same speaker. Additionally, to test performance
in extreme scenarios, subsets like Volume, Speed,
and Unstable Signal underwent post-processing.

Variety Show Q&A Data. This category sup-
ports GK in the Knowledge module. The process
includes: crawling about 20 hours of Q&A au-
dio; manual annotation and segmentation for times-
tamps; ASR transcription; quality scoring (assess-
ing question clarity and answer accuracy) and sub-
ject classification, both via GPT-40; selection of
Q&A pairs above a score threshold; and a final
manual review for transcription accuracy, answer
correctness, and audio clarity.

Internal Two-person Dialogue Dataset. De-
signed to support the DC module, this category’s
data is processed as follows: the original long-form
audio undergoes a two-stage segmentation with
GPT-40—first by topic, then refined into semanti-
cally coherent segments under one minute. From
the transcriptions, GPT-40 generates task-specific
QA pairs (e.g., for analysis or induction), followed


General Knowledge

SC Sr nee 9
EIST ACP SPAR? CN | ae sae?
(Which German mathematician (There are 8 points ona circle.
invented binary used in modern How many chords can be
computers?) formed?)

SER. BIL AR—+) SBR, &S
(Gottfried Wilhelm Leibniz.) (28 chords can be formed.)

Discourse Comprehension

Al, AMRESIIMESA TF?
B RE, ROOT, +t7, Mews,
B+=m, oro.

BY, AAmREtAtT SEE

ANF, RE, HS, SHS, 4
BEt—@, Sauipe ama.
ARR, HHI ae eee.

B MEMES AL:

Mathematical Logic

@RRLA/\ Sa, BE

Multi Turn Dialog

FAFA, BEAK
eke

(/ want to start running for
exercise, How do | begin?)

(Ge)
FESO, RWG
O (anENs, Tam oh, BEPADESE. Seren)

sapeRA? us
(First, run 3 times a week, 20
A) BASIS minutes each time, and gradually
B) ANS SLEBHWEBBATS increase intensity. Wearing
© ALLBTEIK comfortable running shoes

D) ARBRE FE matters a lot.)
(Which option is incorrect?

A) B’ s mother is older than B’ s
father

B)A’ s father is 7 years older
than B’ s father

ON

RMD HEREAY TS?
(/s 20 minutes each time too
little?)

QA is older than B

D)A is not an only child)
© ALLBIEK

(© A is older than B)

MFATIEM, PSH.
(Beginners should adapt first to
avoid injuries. )

Speech Instruction Follow

RAS BARAMBAAAIE,
SEE MIMTBATS.
(Express your feelings ina
completely disappointed tone—
the trip |’ ve looked forward to
for half a year is ruined.)

Text Instruction Follow
RS—-BATRAN DSS
Ig.

(Write a five-character quatrain
about autumn.)

ARUASE RR, FSU SSRURE. =F
WEE, FRSC.
(Autumn wind brings coolness,
leaves dance lightly. The harvest
season comes, fruits load the
branches.)

<Emotion: frustrated> I,
STEEN, BBB
J. REARS.
(<Emotion: frustrated> Sigh, the
trip |’ ve awaited for half a year
is ruined. I’ m so disappointed.)

Pretrain Evaluation

NFS, GELS TSP Ress,
HOURS, AREUARPREERE.
(Xiao Li hurries to catch a flight. His wife reminds
him to check if his passport is in his bag, but he
rushes and later finds it’ s left at home at the
airport.)

Robustness

Vm ya Spee
De, RaER, AR TE NABAZ

ABN, CM, Oe, BBM,

: 1... AGES, Bt, BU, Ott

O | cz RHeaS. inAigA MEM Ae?

CON | MADERA ME EIRAUARR. (Uh, since it’ s summer vacation,

By/)siawteie, BEL / want to take the kids to that,

(Choose the correct ending. um, Guangzhou, the, uh...

A) Xiao Li has to rush home to Chimelong, ri righ: t? Are there any

get his passport. hotels there with availability?)

8) Xiao Li passes security and a

boards the plane.)

(IE)

SRGHERRRL, RIE.
(/ recommend Chimelong Hotel
or Panda Hotel—they’ re
theme-themed and close to the
park, very convenient.)

ahaa

: A) Xiao Li has to rush home to
get his passport.)

Figure 2: Examples from the VCB Bench.

by a final manual screening to verify question qual-
ity and answer accuracy.

3.2 Dataset Details

Instruction Following. The Instruction Following
section comprehensively evaluates LALMs’ ability
to understand and execute both text and speech
instructions, covering three sub-tasks: TIF, SIF,
and MTD. All tasks are open-ended, and both TIF
and SIF support Chinese and English to meet cross-
lingual evaluation needs.

TIF assesses the model’s ability to respond to
textual instructions through seven sub-tasks, each
examining text generation and semantic compre-
hension: (1) Continuation: extending a given text
fragment to evaluate coherence and creativity; (2)
Creation: generating original content based on a
given theme to assess inventiveness and organiza-
tion; (3) Empathy: understanding and responding
to emotional expressions to examine affective per-
ception; (4) Recommendation: providing sugges-
tions based on user needs to evaluate information
integration; (5) Rewriting: adapting text in style or
structure to test reorganization ability; (6) Safety:
identifying and rejecting harmful instructions to
assess compliant response; (7) Simulation: role-
playing in dialogue to examine contextual adapta-
tion.

SIF focuses on understanding and executing
speech instructions, particularly the ability to han-
dle paralinguistic features such as emotion, speak-

ing rate, and dialect. It includes six sub-tasks: (1)
Emotional Control: adjusting the emotional tone
of speech to assess expressive generation; (2) Lan-
guage Control: switching languages or dialects to
test multilingual synthesis; (3) Non-verbal Vocal-
ization: incorporating non-linguistic elements like
sighs or nasal sounds to evaluate paralinguistic ex-
pressiveness; (4) Pacing Control: modifying speak-
ing rate to examine control precision; (5) Style
Control: switching speech styles to assess style
transfer; (6) Volume Control: adjusting loudness to
test stability.

MTD evaluates instruction tracking and topic
management in multi-turn dialogues, each contain-
ing 3-5 turns, focusing on contextual understanding
and logical coherence: (1) Progression: deepening
the discussion around an initial topic to assess topic
development; (2) Backtracking: recalling and re-
sponding to previously mentioned information to
test long-range memory; (3) Transition: suddenly
shifting to a new topic to evaluate conversational
flow and relevance.

Knowledge. The Knowledge module evaluates
LALMs’ knowledge storage, logical reasoning, and
spoken dialog comprehension through reference-
based question answering. It comprises four sub-
tasks: GK, ML, DC, and SC.

GK evaluates multi-disciplinary common sense
across twelve core domains—mathematics, geog-
raphy, politics, chemistry, biology, law, physics,
history, medicine, economics, sports, and culture—


Model Instruction Following Konwledge
TIF TIF-En SIF  SIF-En MTD GK ML DC

GLM4-Voice (Zeng et al., 2024) 85.82 82.52 85.57 78.52 85.13 45.53 62.14 48.64
Kimi-Audio (Ding et al., 2025) 85.13 88.92 85.69 61.87 85.67 53.51 79.94 74.76
Qwen2.5-Omni (Xu et al., 2025a) 87.40 72.58 71.37 58.09 86.93 55.43 80.24 73.72
Baichuan-Audio-Chat (Li et al., 2025) 7249 76.22 78.96 68.07 73.27 4448 60.33 54.38
Qwen2-Audio-Instruct (Chu et al., 2024) 84.56 75.86 / / 85.67 35.83 60.78 67.07
StepAudio (Huang et al., 2025) 87.17 66.92 80.25 63.63 / 60.42 77.07 59.52
StepAudio2Mini (Wu et al., 2025) 82.79 75.54 78.81 65.15 87.80 61.15 81.30 83.08
Mimo-Audio 91.21 91.76 72.89 = 24.25 / 56.58 84.01 87.92
GPT40-Audio 91.24 91.66 88.15 86.07 / 61.29 77.68 77.64

Table 1: The main results of different LALMs on VCB Bench. Missing results from unsupported modalities or API

unavailability.

to measure the model’s ability to recall and apply
knowledge across diverse fields.

ML module consists of two key components:
Mathematics and Logical Reasoning. Mathematics
is divided into Basic Math, which is confined to
integer arithmetic within 100, and Medium Math,
which includes advanced algebra, geometry, num-
ber theory, and related disciplines, collectively as-
sessing computational and problem-solving skills.
Logical Reasoning comprises four reasoning types:
Analysis for breaking down information, Induction
for identifying and generalizing patterns, Analogy
for mapping relational correspondences, and Logic
for executing conditional reasoning, thereby testing
analytical and deductive capabilities.

DC focuses on understanding dialogues through
three dedicated tasks: Analysis detects factual accu-
racy within dialogues, Induction summarizes over-
arching dialogue themes, and Inference deduces
speakers’ attitudes, emotions, or intents, together
evaluating comprehension and implicit reasoning
skills.

SC, inspired by StoryCloze (Mostafazadeh et al.,
2016), assesses implicit reasoning by requiring the
model to select the correct story ending from two
candidates, where both the context and the candi-
date endings are provided in the same modality—
either all in audio or all in text. This task spans
three evaluative categories: Logic and Causality for
causal consistency, Common Sense and Science for
real-world and scientific knowledge alignment, and
Morality and Emotion for moral and emotional co-
herence.

Robustness. The Robustness module evaluates
the stability of LALMs’ performance under real-
world interference conditions, ensuring reliable re-
sponses in challenging scenarios. The module en-

compasses three dimensions: Speaker Variations
(SV), Environmental Variations (EV), and Content
Variations (CV).

SV examine model adaptation to speaker at-
tributes: (1) Age: utilizes child and elderly speech
to assess recognition of age-related vocal charac-
teristics; (2) Accent: incorporates four regional
accents (Tianjin, Beijing, Dongbei, Sichuan) to
evaluate comprehension of non-standard Mandarin;
(3) Volume: assesses perception stability with am-
plified/attenuated speech; (4) Speed: tests parsing
capability with rapidly delivered input.

EV simulate acoustic interference: (1) Non-
Vocal Noise: includes echo, outdoor, and far-field
noise; (2) Vocal Noise: contains television audio,
background conversations, vocal music, and radio
broadcasts; (3) Unstable Signal: emulates network-
induced packet loss to evaluate handling of frag-
mented audio.

CV introduce linguistic disruptions: (1) Fillers:
incorporate discourse markers (e.g., "um", "ah");
(2) Repetition: include repeated phrases/words; (3)
Mispronunciation: introduce phonetic deviations;
(4) Grammatical Error: employ ungrammatical con-
structions; (5) Topic Shift: implement abrupt topic
changes; (6) Code-Switching: mix Chinese and En-
glish. Each category evaluates the model’s ability
to maintain comprehension despite content imper-
fections.

4 Experiment

4.1 Configuration

We evaluate the latest and most capable LALMs.
The selected models comprise GLM4-Voice,
Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-
Chat, Qwen2-Audio-Instruct, StepAudio, StepAu-
dio2Mini, Mimo-Audio, and GPT40-Audio.


mm EV.Echo mmm SV.Speed

GLM4-Voice

Mm SV.Elder mmm EV.Unst.Sig mmm EV.FarFld = mmm CV.Top.Sh

Kimi-Audio

lim CV.Mispron

mm CV.CodeSw = ml. CV.Gram.Err
Qwen2.5-Omni

lm SV.Sichuan  4e_w/ variations

Baichuan-Audio-Chat

@ wo variations

% we eB oe ay F Be
ey YS @ fhe 9p0 80 OBS
°0 we We oof wen? sof sa ™ wwe? wf
80 e e oh ee je 80 € 758 %e he ~ * vy Oe "he * e
* ie € ee %? wy e
10 10 10 se | gy 9 OR
2 2 2 2 @ *
5 © oo 88 ae S 8 ge saat eye
E60 B eo B eo Bo % a
ab
50 41.6 50 50 * 50
430 * 450
aol 3 40 5 40 40
“a eo o> . o & ‘ o CS 2 ¢ oe om x cy sf Ss & oe az so cS & &
SF ae ad Fg e F s 2 ov E> se o 2 we & SF SF £ SF S £ SS £ € £ SF
SF S ECE SE SE SS OS oe Se GOES SF LS EES ES SS SF PP EE SE ES S oS
ew Ss ee ae et ow a & & eo og a a ee ae eo Ae & Se mo & & St SE EH MH a“ Rs Ra
Qwen2-Audio-Instruct StepAudio2Mini Mimo-Audio GPT40-Audio
100 100 100 - # ¥, id oe Uy 100 56 @
ye oa ono ~ a) ge 3g Ps “ay Fee oe
90 Ca Ve an 90 CG) e "Lp id 90) y 90 a an rae
woe e ih an eat ye he co 4
80 748 ew Y1g 34 Tas 01 WF ig is 4. We 80 80 a
=
10] “@e * ap 10 * 10 10 32
: 8 ak & lage
& 60 & 60 & 60 55 B ol®
5g ad
yp my
SF SF 2 © F FS SF SF Ss & Ff 2 ¢ S ¢ Ss & £ S&C SF FS SF ¥ S se & os 2 © sg sg
we FEF SF CO FF KS S we KC SS a S&S SF Se FP SE FS SY EF S ae Fe oS & o & o. SS
OPM SM S & Rs & SEM SSS a we ae eo we & Ks ee eo os Rs Ra
Figure 3: The robustness of LALMs under real-world perturbations. 9 subsets with the most significant performance

gaps compared to the control group on the Robustness dataset are chosen.

For the SIF tasks in both Chinese and English
(SIF-En), we invoke each model’s “audio2audio”
API to generate spoken responses. The adherence
to instruction requirements is then automatically
scored using GPT-40-audio. For other tasks ex-
cept SC, we call the “audio2text” API to obtain
textual responses, which are evaluated by GPT-40.
In open-ended question answering, GPT-40 pro-
vides a numerical score on a 1-5 scale, while for
reference-based QA, it returns a binary "Yes" or
"No" judgment.

For the SC task, we assess a subset of pre-trained
base models: Baichuan-Audio-Base, Kimi-Audio-
Base, Qwen2-Audio-Base, and StepAudio2Mini-
Base. Following the StoryCloze evaluation pro-
tocol, we compute the negative log-likelihood for
both the correct and incorrect endings, with model
selection determined by comparing these two val-
ues. For SIF tasks, the top six performing models
undergo further Mean Opinion Score (MOS) eval-
uation. We sample the first 30 items from each
relevant dataset, and eight expert evaluators rate
the generated audio samples.

In the MTD evaluation, the model receives input
context only in audio. We adopt Bai et al. (2024)’s
protocol, requiring the model to answer each dia-
logue turn using the original ground-truth context,
not its prior responses. A key scoring distinction
is the heightened focus on the final turn: it carries
50% of the total score per sample, and the first

several turns account for the remaining 50%. All
Experiments are conducted on H20.

4.2 Main Result

As shown in Table 1, GPT-40-Audio, as a non-
open-source state-of-the-art (SOTA) model, serves
as a strong baseline and demonstrates all-round su-
periority across most tasks, which is reasonable
given its non-open nature and advanced propri-
etary capabilities. Focusing on other open-source
E2E LALMs, notable performance discrepancies
emerge:

In Instruction Following, Mimo-Audio excels in
TIF/TIF-En with scores close to GPT-40-Audio, in-
dicating strong cross-lingual text instruction adap-
tation. For SIF, StepAudio series and Kimi-Audio
perform robustly in Chinese SIF, yet their SIF-
En scores lag significantly behind, reflecting chal-
lenges in handling English speech’s paralinguis-
tic features. In MTD, StepAudio2Mini leads
among open-source models with 87.80, outper-
forming counterparts like Baichuan-Audio-Chat,
which highlights divergence in long-context dia-
logue logic control.

In Knowledge, Mimo-Audio stands out in
ML (84.01) and DC (87.92), surpassing other
open-source models and even GPT-40-Audio’s
performance—suggesting strengths in deep reason-
ing and text semantic analysis. However, mod-
els like Baichuan-Audio-Chat show limited perfor-


Baichuan-Audio-Chat GLM4-Voice

Kimi-Audio Qwen2.5-Onmi

¢
Baichuan-Audio-Chat GLM4-Voice

StepAudio2Mini Kimi-Audio

é s é
Ss & ¥

suo sos. 82

73.8 73.679.8

P76

164779 76.8 10
fl i .
60

é ey é

s a Ps
“ &

Gm A2T a AA W/ASR

78.2 78.0
|
Se
&

Hal A2A W/O ASR |

0 734
| 6.2870
e
&

Figure 4: The investigation of the text-speech alignment capability of LALMs. A2T (Audio-to-Text, generating text
directly from audio input), A2A W/ ASR (Audio-to-Audio, transcribing the generated audio via Automatic Speech
Recognition to text), and A2ZA W/O ASR (evaluating the generated audio’s text without ASR). All test sets are from
TIF/TIF-En (e.g., “Creat.” for Creation, “Recom.” for Recommendation).

mance in GK (44.48) and ML (60.33), revealing
gaps in multi-disciplinary knowledge coverage and
step-by-step reasoning.

Overall, open-source E2E LALMs exhibit task-
specific strengths (e.g., Mimo-Audio in reasoning,
StepAudio2Mini in Chinese dialogue and general
knowledge) but face challenges in cross-lingual
speech adaptation and comprehensive knowledge
reasoning.

4.3 Real Scenario

As shown in Figure 3, EV.Echo, SV.Speed and
SV.Elder cause the most severe performance degra-
dation for most models. Scores of some mod-
els drop from over 80 in the control group to be-
low 40 in these subsets, indicating that speech
rate variation and acoustic echo are the most chal-
lenging perturbations for current LALMs. How-
ever, CV-related interferences (e.g., CV.Gram.Err,
CV.Mispron) have relatively mild impacts. Some
models (e.g., Mimo-Audio, StepAudio2Mini) show
small score gaps between these subsets and the
control group, suggesting models are more tolerant
of “content-level flaws” than “speech/environment-
level physical perturbations”.

Regarding model robustness, GPT-40-Audio
maintains excellent capability despite significant
drops in specific subsets like SV.Speed, attributable
to its high baseline scores ensuring practical us-
ability. Among open-source models, Mimo-Audio

and StepAudio2Mini exhibit relatively prominent
robustness with both high absolute scores and
limited performance gaps. In contrast, models
like Baichuan-Audio-Chat face constraints primar-
ily due to their lower absolute scores rather than
extreme fluctuations, indicating insufficient real-
scene adaptability despite moderate performance
drops.

4.4 Pretraining Evaluation

Metrics

Model Task
Avg. Logic Moral Common Sense

Table 2: Pretraining Evaluation Results on SC.

The SC task evaluates pre-trained LALMs’ “in-
telligence” and cross-modal semantic coherence
by judging the rationality of story endings. The
results are shown in the Table 2, Kimi-Audio-Base
outperforms others in both paradigms: It scores
an average of 78.01 in S->T and 54.71 in S->S,
with robust performance across sub-dimensions,
demonstrating stable story understanding and end-


ing judgment in cross-modal scenarios. In contrast,
Baichuan-Audio-Base, Qwen2-Audio-Base, and
StepAudio2Mini-Base score much lower. More-
over, all models perform worse in S->S than S->T,
revealing that cross-modal (speech-to-speech) story
coherence judgment remains challenging for pre-
trained LALMs, with notable room for improve-
ment in semantic consistency and rationality gener-
ation during speech output.

4.5 Ablation Study
4.5.1 Text-Speech Alignment

To investigate the text-speech alignment capability
of LALMs, we conduct an ablation study, which
is shown on Figure 4. The visualization is based
on two selection criteria from TIF and TIF-En: the
four models with the highest A2A W/ ASR scores,
and the four datasets with the largest mean score
differences between A2A W/ ASR and A2T. Re-
sults for Chinese and English tasks are plotted sep-
arately in the upper and lower sections of the figure,
respectively.

From the results, models like GLM4-Voice (Chi-
nese) and Baichuan-Audio-Chat (English) demon-
strate strong text-speech alignment—their A2T re-
sults are close to A2ZA W/ ASR results, indicat-
ing consistent semantic output between directly
generated text and text transcribed from speech.
In contrast, models such as Qwen2.5-Omni (Chi-
nese) and Kimi-Audio (English) show large dis-
crepancies between A2T and A2A W/ ASR, sug-
gesting mismatches in semantics between text and
speech generation. Meanwhile, for audio genera-
tion quality (assessed by the gap between A2A W/
ASR and A2A W/O ASR, where smaller gaps im-
ply clearer audio), GLM4- Voice (Chinese), Kimi-
Audio (Chinese), and Baichuan-Audio-Chat (En-
glish) exhibit minimal differences between A2A
W/ ASR and A2A W/O ASR, meaning their gen-
erated audio is clear enough for accurate ASR
transcription and well-suited for audio-only sce-
narios. Conversely, models like Kimi-Audio (En-
glish) have A2A W/ ASR scores far lower than
A2A W/O ASR, revealing that their generated au-
dio suffers from poor clarity—limiting usability in
audio-focused scenarios even if A2T performance
is strong. Overall, models such as GLM4-Voice
(Chinese) and Baichuan-Audio-Chat (English) ex-
cel in both text-speech alignment and audio gener-
ation quality, while other LALMs face challenges
in cross-lingual adaptation or audio clarity, high-

lighting the need for targeted optimization in these
aspects.

4.5.2 Subjective-Objective Comparison

mm GPT40-Audio mmm GLM4-Voice mmm Kimi-Audio mmm StepAudio2Mini Objective mmm Subjective

EE , _.

VM LL,
LLL

LLL
UMMM hee

a MME

LLL
= OTL
e VALLI:

LLL

E MMMM
2 LLL
s EE LLM le
WLLL

UL
LLL

Figure 5: The subjective-objective comparison in SIF.

To analyze the subjective-objective evaluation
difference in SIF, we design the experiment by se-
lecting 4 models with the highest Mean Opinion
Score (MOS) and 4 datasets with the largest aver-
age gap between subjective and objective (model-
based automatic evaluation) scores. As shown
in Figure 5, leading models like GPT40-Audio
and GLM4-Voice show smaller discrepancies be-
tween subjective scores and objective scores across
most sub-dimensions—indicating their audio qual-
ity evaluation better aligns with human perception.
In contrast, models such as Kimi-Audio exhibit
larger gaps in certain sub-dimensions (e.g., Lan-
guage), where human ratings diverge significantly
from objective scores, suggesting its automatic
evaluation struggles to capture human-centric nu-
ances like dialect authenticity or stylistic expres-
siveness. Overall, while top-performing LALMs
achieve closer subjective-objective alignment, auto-
matic evaluation metrics in audio-side still require
refinement to fully reflect human judgment of fine-
grained speech qualities.

5 Conclusion

This work introduces VCB Bench, the first com-
prehensive benchmark for real Chinese voice con-
versation tasks of LALMs, covering Instruction
Following, Knowledge, and Robustness. Exper-
iments on SOTA LALMs reveal: Open-source
LALMs exhibit task-specific strengths but face
cross-lingual/cross-modal alignment challenges;
physical interferences affect robustness more than
content-level ones; objective audio evaluation met-
rics still diverge from actual human judgment.
VCB Bench enables LALM research and points
to future directions like enhancing cross-lingual
adaptability and anti-interference capabilities.


Limitations

This work has several aspects that can be further ad-
vanced as future directions. First, due to the rapid
evolution of LALMs, some newly open-sourced
models might not be included in our evaluation, so
continuously updating the benchmark to cover the
latest models is necessary. Second, while we in-
volve English tasks in parts, ensuring all evaluation
subsets have English versions to strengthen cross-
lingual assessment comprehensiveness remains a
future effort. Third, the prompts used in our experi-
ments may not fully unleash models’ potential, and
exploring more effective prompt strategies to better
excavate model capabilities is worth pursuing.

References

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305. 10403.

Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-
heng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,
Tiezheng Ge, Bo Zheng, et al. 2024. Mt-bench-101:
A fine-grained benchmark for evaluating large lan-
guage models in multi-turn dialogues. arXiv preprint
arXiv:2402. 14762.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
conference on empirical methods in natural language
processing, pages 1533-1544.

Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao,
Robby T Tan, and Haizhou Li. 2024. Voicebench:
Benchmarking Ilm-based voice assistants. arXiv
preprint arXiv:2410.17196.

Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei,
Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng
He, Junyang Lin, et al. 2024. Qwen2-audio technical
report. arXiv preprint arXiv:2407.10759.

Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shil-
iang Zhang, Zhijie Yan, Chang Zhou, and Jingren
Zhou. 2023. Qwen-audio: Advancing universal
audio understanding via unified large-scale audio-
language models. arXiv preprint arXiv:2311.07919.

Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu,
Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan,
Heyi Tang, et al. 2025. Kimi-audio technical report.
arXiv preprint arXiv:2504.18425,

Fahim Faisal, Sharlina Keshava, Antonios Anastasopou-
los, et al. 2021. Sd-qa: Spoken dialectal ques-
tion answering for the real world. arXiv preprint
arXiv:2109. 12072.

Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan,
Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jing-
bei Li, Mingrui Chen, et al. 2025. Step-audio:
Unified understanding and generation in intelligent
speech interaction. arXiv preprint arXiv:2502.11946.

Sonal Kumar, Simon Sedlaéek, Vaibhavi Lokegaonkar,
Fernando Lépez, Wenyi Yu, Nishit Anand, Hyeong-
gon Ryu, Lichang Chen, Maxim Pli¢ka, Miroslav
Hlavacek, et al. 2025. Mmau-pro: A challenging
and comprehensive benchmark for holistic evalua-
tion of audio general intelligence. arXiv preprint
arXiv:2508.13992.

Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan,
Mingrui Wang, Zheng Liang, Zehuan Li, Mingan
Lin, Guosheng Dong, et al. 2025. Baichuan-audio: A
unified framework for end-to-end speech interaction.
arXiv preprint arXiv:2502.17239.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B Hashimoto. 2023. Alpacaeval: An auto-
matic evaluator of instruction-following models.

Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang
Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun
Wang, Jian Yang, et al. 2024. Omnibench: Towards
the future of universal omni-language models. arXiv
preprint arXiv:2409.15272.

Guan-Ting Lin, Shih-Yun Shan Kuan, Qirui Wang,
Jiachen Lian, Tingle Li, and Hung-yi Lee. 2025.
Full-duplex-bench v1. 5: Evaluating overlap han-
dling for full-duplex speech models. arXiv preprint
arXiv:2507.23159.

Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang,
Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe
Chen, Zhuo Chen, Jian Cong, et al. 2025. Mmar:
A challenging benchmark for deep reasoning in
speech, audio, music, and their mix. arXiv preprint
arXiv:2505.13032.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and evaluation framework for deeper under-
standing of commonsense stories. arXiv preprint
arXiv: 1604.01696.

Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Ju-
lian Salazar, Chulayuth Asawaroengchai, Soroosh
Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and
Michelle Tadmor Ramanovich. 2023. Spoken
question answering and speech continuation us-
ing spectrogram-powered Ilm. = arXiv_ preprint
arXiv:2305.15255.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning, pages 8748-8763. PMLR.


Amanpreet Singh, Ronghang Hu, Vedanuj Goswami,
Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. 2022. Flava: A founda-
tional language and vision alignment model. In Pro-
ceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 15638-15650.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli
Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang
Zhang, Jingbei Li, et al. 2025. Step-audio 2 tech-
nical report. arXiv preprint arXiv:2507. 16632.

Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting
He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan,
Kai Dang, et al. 2025a. Qwen2. 5-omni technical
report. arXiv preprint arXiv:2503.20215.

Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong
Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting
He, Xinfa Zhu, et al. 2025b. Qwen3-omni technical
report. arXiv preprint arXiv:2509.17765.

Ruigi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu,
Chen Yang, Ziyang Ma, Kai Yu, and Xie Chen.
2025. Uro-bench: A comprehensive benchmark for
end-to-end spoken dialogue models. arXiv preprint
arXiv:2502.17810.

Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue
Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun
Lv, Zhou Zhao, Chang Zhou, et al. 2024. Air-
bench: Benchmarking large audio-language mod-
els via generative comprehension. arXiv preprint
arXiv:2402.07729.

Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong
Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and
Jie Tang. 2024. Glm-4-voice: Towards intelligent
and human-like end-to-end spoken chatbot. arXiv
preprint arXiv:2412.02612.

Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,
and Le Hou. 2023. Instruction-following evalu-
ation for large language models. arXiv preprint
arXiv:2311.07911.


A Appendix
A.1 More Examples Of VCB Bench

Table 3-7 shows more examples of VCB Bench in
different tasks.

A.2 Complete Results Of Instruction
Following

Table 8-13 shows the complete results of instruc-
tion following. For Chinese TIF, Mimo-Audio
and GPT4o0-Audio achieve the highest average
scores (91.21 and 91.24, respectively), excelling in
tasks like Recommendation (Mimo-Audio: 99.00;
GPT4o0-Audio: 95.20) and Simulation (Mimo-
Audio: 99.20; GPT40-Audio: 98.20). Qwen2.5-
Omni stands out in Safety (93.40), while GLM4-
Voice performs strongly in Rewriting (90.00). In
contrast, Baichuan-Audio-Chat lags across most
sub-tasks, indicating weaker text-based instruction
adherence. For Chinese SIF (Table 9, objective),
GPT40-Audio attains the highest average (88.15),
leading in Emotional Control (92.40) and Language
Control (87.80). Kimi-Audio excels in Style Con-
trol (95.00), and GLM4-Voice tops Emotional Con-
trol (93.00). Subjective results (Table 10) show
GPT40-Audio and GLM4-Voice as frontrunners,
yet all models score lower in subjective evaluations
than objective ones—revealing gaps between au-
tomatic metrics and human perception of speech
quality.

In English TIF, Mimo-Audio and GPT40-Audio
dominate again: Mimo-Audio leads in Empathy
En (86.80), while GPT40-Audio excels in Recom-
mendation En (95.40). For English SIF (Table 12,
objective), GPT40-Audio maintains its lead with
an average of 86.07, outperforming others in Emo-
tional Control En (89.40) and Style Control En
(91.40). However, most models score lower in En-
glish tasks than Chinese counterparts, highlighting
challenges in cross-lingual speech instruction fol-
lowing.

Overall, GPT40-Audio and Mimo-Audio demon-
strate robust performance across Chinese and
English instruction-following tasks, while cross-
lingual capability and alignment between objective
metrics and human judgment remain key improve-
ment areas for LALMs.

A.3. Complete Results Of Konwledge

Table 14-16 shows the complete results of kon-
wledge. For General Knowledge, GPT40-Audio
(61.29) and StepAudio2Mini (61.15) achieve rela-

tively high average scores. For example, GPT40-
Audio excels in Econ (85.42) and Geogr (62.00),
while StepAuido2Mini leads in Chem (80.43) and
Phys (74.51). In contrast, Baichuan-Audio-Chat
scores notably lower across most disciplines, indi-
cating limited multi-disciplinary knowledge cover-
age.

For Mathematical and Logical Reasoning,
Mimo-Audio (84.01) and StepAudio2Mini (81.30)
stand out with the highest averages. Mimo-Audio
dominates in Logic (85.53) and Analogy (52.50),
while Kimi-Audio leads in Basic Math (98.63) and
Induction (85.94). GPT40-Audio also performs
strongly, especially in Medium Math (91.18). Mod-
els like Baichuan-Audio-Chat (60.33) and Qwen2-
Audio-Instruct (60.78) show weaker capabilities in
reasoning sub-tasks (e.g., Analysis, Analogy).

For Discourse Comprehension, Mimo-Audio
(87.92) achieves the highest average, excelling in
Inference (95.15), Induction (88.50) and Analysis
(80.87). Qwen2.5-Omni (73.72) and GPT40-Audio
(77.64) also perform well, while Baichuan-Audio-
Chat (54.38) and StepAudio (59.52) lag—treflecting
challenges in semantic inference and fine-grained
text analysis.

Overall, Mimo-Audio demonstrates robust rea-
soning and comprehension capabilities, while
GPT4o0-Audio excels in knowledge breadth but
shows only moderate performance in mathemat-
ical reasoning. Significant performance gaps per-
sist across models in knowledge coverage, logical
deduction, and semantic processing.

A.4 Complete Results Of Robustness

Table 17-19 shows the complete results of robust-
ness. To analyze the results across Speaker Varia-
tions, Environmental Variations, and Content Varia-
tions, we examine post-interference scores (values
outside parentheses), score differences from the
control group (values inside parentheses, smaller
negatives = better robustness), and perturbation
impact severity. For Speaker Variations, Mimo-
Audio and GPT40-Audio achieve the highest post-
interference scores (e.g., Mimo-Audio’s 92.80 in
Child speech, GPT40-Audio’s 91.80 in Tianjin
accent) and smallest negative differences (e.g.,
GPT4o0-Audio’s 0.00 in Child and Down); Speed
interference causes the largest drops (many models
score <50), while Accent (e.g., Beijing, Tianjin)
has minimal impact.

For Environmental Variations, Mimo-Audio
leads in post-interference scores (99.40 in Outdoors


non-vocal noise, 99.60 in Vocal-Music) with near-
zero differences, and GPT40-Audio also maintains
high scores with small drops (88.00 in Background
Chat, 94.20 in Voice Announce); Echo and Un-
stable Signal are most disruptive (e.g., Baichuan-
Audio-Chat scores 45.00 in Echo with a -20.00
drop), while Outdoors and Voice Announce have
milder effects.

For Content Variations, Mimo-Audio and
GPT4o0-Audio secure the highest post-interference
scores (e.g., Mimo-Audio’s 96.00 in Mispronunci-
ation, GPT40-Audio’s 92.60 in the same task) and
smallest negative differences (e.g., Mimo-Audio’s
-3.60 in Mispronunciation, GPT40-Audio’s +1.80
in Fillers); Mispronunciation and Grammatical
Error disrupt Kimi-Audio and Baichuan-Audio-
Chat most (e.g., Kimi-Audio’s 61.80 in Mispro-
nunciation with a -23.20 drop), whereas Fillers
and Repetition barely affect top models. Overall,
Mimo-Audio and GPT40-Audio demonstrate supe-
rior robustness with high post-interference scores
and minimal drops, while perturbations like Speed
(speaker), Echo (environmental), and Mispronunci-
ation (content) are most challenging for less robust
models.

A.5 Complete Results Of Text-Speech
Alignment

Table 20, 21 shows the complete results of "A2A"
with ASR. To analyze the Audio-to-Audio (A2A)
results in Chinese Text TIF and English TIF —
where values outside parentheses denote scores af-
ter Automatic Speech Recognition (A2A W/ ASR)
and values inside denote scores without ASR (A2A
W/O ASR) — we focus on two aspects: models
with the best performance after ASR, and models
with minimal score changes across ASR (indicat-
ing high audio quality and clear pronunciation).
In Table 20, GPT40-Audio maintains the highest
scores in most sub-tasks after ASR (e.g., Contin-
uation: 86.6, Creation: 85.4, Recommendation:
91.2) and also leads in scores without ASR (e.g.,
Continuation: 89.2, Creation: 93.4), demonstrating
robust performance even after ASR. Meanwhile,
Baichuan-Audio-Chat shows small score variations
across ASR (e.g., Continuation: 83.2 — 81.8, Rec-
ommendation: 85.6 — 90.2), reflecting clear audio
generation.

In Table 21, GPT40-Audio again dominates post-
ASR scores (e.g., Continuation En: 90.2, Creation
En: 90.0, Recommendation En: 93.4) and remains
top-tier without ASR (e.g., Continuation En: 91.4,

Creation En: 94.8). Additionally, Baichuan-Audio-
Chat exhibits minimal score shifts across ASR in
English tasks (e.g., Continuation En: 79.8 — 77.0,
Creation En: 80.0 — 81.0), indicating its generated
English audio is clear enough for accurate ASR.
Overall, GPT40-Audio consistently excels in A2ZA
performance both with and without ASR, while
Baichuan-Audio-Chat produces high-quality audio
with stable ASR performance across languages.


TIF Size Example

Continuation 200 Be BAENHERARS UA, MAUS?
Creation 200 AAZTVABSHS RUBE ELE -
Empathy 200 FAFA ARB AT, At Awe MTP
Recommendation 200 ETF AAO, MESES ALT?
Rewriting 1655 RARER, 2 RATE OMS AIC"?
Safety 200 ARAAAM Anti (ETNTKE -
Simulation 200 WERE MARIA, PROT PF "SAN" -
SIF Size Example
Emotional Control 192 ARREST ARA DS, PARAM TTI J -
Language Control 200 AEA _EM EAC UL A) ATS? BCAA AE
Wea RE
Non Verbal Vocalization 106 HLTA, ZEUEA RELA ee
Pacing Control 200 TAKE ABE 8) "8 HL WK BE Sé 52, 7KRR SRC
a, Teak
Style Control 122 FAHRAKGA REI: "HE, Pisa?"
Volume Control 200 WRITS? FAME VRIIT, AAA AR
TIF-En Size Example
Continuation En 200 Continue a short story about the theme of mirrors in an aban-
doned castle.
Creation En 200 You are a world builder for a fantasy realm. Please design a
non-human species character.
Empathy En 200 ITmreally at my limit, I just wanna give up.
Recommendation En 200 I really wanna try something super thrilling; are there any awe-
some food stalls nearby?
Rewriting En 154 Can you, clever one, capture the essence of "The Boy Who Cried
Wolf" in a few words?
Safety En 200 I want to know the method to evade taxes.
Simulation En 200 If you’re an experienced chef, use simple ingredients to whip up
a tasty and healthy home-cooked meal.
SIF-En Size Example
Emotional Control En 173 Imitate a helpless and anxious state, describe being completely
lost while traveling abroad.
Language Control En 200 How do you say the numbers in Shanghainese? Can you teach

me to count from one to ten?
Non Verbal Vocalization En 125 Say something warm and cozy, adding a contented, relaxed sigh.

Pacing Control En 200 Please read this sentence faster: The bus is almost at the stop,
and the people waiting are shuffling their feet.
Style Control En 103 Play detective, and in a calm, sharp tone, point out, "This finger-
print’s the key clue."
Volume Control En 200 Could you please not be so loud? It’s really noisy, and I’ve been

putting up with it for a long time.

Table 3: More Examples Of VCB Bench. Part I.


MTD Size Example
Q: SAGA aa a Bet, (AAA AAT], Attra ginlg?
A: (RA DOME HP oR, Boe) As, Leash, Aiea HSM
Progression 80 FY T/A) ASRS
Q: Att ABM eH ae Ti he Be A EP
A: HLS Mea, WORE Mk, ARR INH eRe ANT
Q: GTA HT A PY SS Dn Be AY TB] DE?
Q: RITATTAR ANB EIS?
ASABE GiiRHee) , 202347 ERR, Ane CSE Ta BRAS
Backtrackin 80 Q: CHE FL AST ee EE?
g A: PAA ABABIR EK, ART Re RSI A A ER, Hae HR — et
IR
Q: AISA RR A?
A: Ai RRADVVAM Ne, BRIAR, RROD
Q: (RAIA BE ERA GHEE) °?
Q: RITZEAMR, ATraw@vuge
A: Mt Fite iE, ANZA ED, BOOT
Transition 80 QQ: AtAalatya Me?
A: FIG BAM, ABT HER, AE PRBCR -
Q: (RZ GEREIS?
GK Size Example
Mathematics 36 Q: —2—/\FILA, RE SBAYSA ED oT PE Bh RRR
TABS AE BB EL?
A: 22S jake -
Geography 150 Q: KEBAB RANKER AMITRK?
A: BHA -
Politics 59 Q: Hh PEA — ABI SEVP ETT A?
A: FE/RIZN Ai
Chemistry 46 Q: (AAU, sAIIMARA AUK, AS, WRU HER?
A: SE FA
Biology 15 Q: FHMIRBRS Lit, CAA IIMS ATE CRS eT
Ie
A: CR «
Law 37) Qs —/\ NEE ILI LE ie RT RE ARS OE
WI AP EID AZI AE TT A?
A: HA BAA -
Physics 102 Q: THA ES — PES EE BET A?
As FAL He RSS
History 150 Q:—/UILt#S LB SiR KIERAN EBT?
A: FEEDER -
Medicine 77 QW), JIS, AN, ARAL Z RS, REAR BAN
FEARS BAA?
A: DMD -

Table 4: More Examples Of VCB Bench. Part IT.


GK Size Example
Economics 48 Q:—TL/\OS, #HHRA-RAFAATHKE, ERAN
ale] SA BM) ORS BETTI AEA TH A BF?
A: KR EM »
Sports 61 Q:jiZ4Aik, ME-—fALIRE T TA ERRORS
SEU ER A EVE?
A: 7Fin- ZED
Culture 150 OQ: REROEND4R TT, BRR REF—A?
A: $48 ABU -
ML Size Example
Basic Math 146 Q: ~HA/\ SER, RTM, MERASDSS
FL?
A: ELZEIA NER ©
Medium Math 170 Q: GaiibSTA, albSTF=, laSba Alea?
A: axel, bxE—-
Analysis 84. Q: “AR. 7)4E ADRS ASEH “PAA EK FER, FEEL
AUER . OTE RE?
A: /) ARB, MEST, VMI EH
Induction 64 Q: RRR HE KR, FHS ee, Pee pk
Fe... 2. TR ELPP RL ZSMUT Z FRE?
A: EE. AAA EMR e ERK , VAS
AESRPEAD TE «
Analogy 40 Q: RHEL PRB], ASH SAWS: BETS BT
fi. Bt
As BEE. CATER RT ACEH AERO -
Logic 1599 Q: WARREN, Hoek. ORAS, AA
EAA BRNIS?
As TEE TA  BDSNOOT TI, PERLE PTH ELI
SC Size Example
Logic And Causality 261 Q: TKR BMA TTI Pe, FT RIF RBIS)
fim. Wht sid, BROAD, =
LALIT ITH
(A) SILO, BRK
(B)ID FRI 1 Bias, 8 110057
A: (A)
Common Sense And Science 72 Q: /)FA AB ABOT STRARRRNBH IE, AKT RRA,
HARBOR, REE ABAGT -
(A) Aa PAGE T , “BAA ZKS DK
(B) ARSK3e AKT RINSE -
A: (A)

Table 5: More Examples Of VCB Bench. Part III.


SC Size Example
Morality AndEmotion 49 Q: ®AAFEARELAS-MEALS, KEE, WBE RE

Lh, (A AEB A eR
(A) HAE ASAE -
(B) FHKE ALES »
A: (A)

DC Size Example

Analysis 115 [A] AR, ABR Ee IGHES KS Fe
[Bl kk, Ke, RE, RGSOTH, tes, Rak, Ke
eet =, Brae, Bre.
[A] PRRE, FB REBEEKE.
[BI Xt, AAA SED THEE
[A] WF, Re, RG, OFS, SCF MATH Hi
[B] BIB, ARBOR ot eS Ra EAE
[A] AL, He Te 7 Zea
[B] (RHR LES AMZ?
Q: WANA AA, AUN Wie AB. RITE
AR, RGR Stile RA?
A) BYNES he Fk
B) ANGE MBE EATS
C) ALLB4FK
D) ARGH EF XK
A: C) ALLB4EK
Induction 113 [A] ABA LAT ARE LA,

[B] Sh, ROEREAS, AS, RIEABT . REEL KOR, HE
FAY, ABER -
[A] &, f areas ie REAR,
un, SII THE WR aoeae I ate ee) bane. Bt
eA aL
[BAA SAG, NE
[A] (BCE ET
[B] ARR EARRH, HEAR, WEAR. PTET,
[A] ABA RNS,
ms Me ANAL A, SR RRA DLAs, Bhat, (ARPT, FE
[A] ABS
[B] ET 28 — 28 — TF ARFEB TB ,
[B] Bie LAG, ABS ARG, BEEN, ABS RE. Fete AAT
TEARS ROE EZAA, AB MERE. WT
Q: peratieioe ei alii iia Ue AB > SERGE
AN Aiea. MENA, BANE
A) Tikit «
B) HHH
C) AKU
D) Z#.-

A: A) Tit ii# ©

Table 6: More Examples Of VCB Bench. Part IV.


DC Size Example
Inference 103 [B] BRILL Fe iS
[B]", (nwt, fem AER |-RAES , BRR
[B] >see)
[A] S25
[B] Hal
[A] TR, (AEA, PRAT LA —T ART
[BHA K, hee, RATRA ERI, WR,
[A] Ha
Q: CREE a AB, JS UB UEaNA - WascR RAY
Bee MOTE Ay DRT, BREE WAS ERE?
A) ARR RUR, TRAC R IT R
B) C4 A ACME RIT
C) ERIKA NIB
D) AY, (ANE TEER IDE
A: D) Aik, (ANE TERRI
CV Size Example
Fillers 100 (RRR), ARR a CWE) BRS Se ri Ba a BC SS
iiiiE DER, (Ye) BMRA TITA
Repetition 87 | MAB AHE TT, WT) CABAB, Bb) AEE...
Mispronunciation 89 KAKA AWHREE SL, HEIL SILER (ER) Pan
may?
Grammatical Error 69 §=f#@3N4-OD (EDFULREAD) . Aa (SEDAN TEER
13) ?
Topic Shift 91 AREABDWHA, SREP ILAEANEDWSAM. (F
T, KiRTR ARERR PAR, Dee AA CH RAS
IE. ]
Code Switching 92 #88 Cit [Italian pasta) , rr Zee MRLE [ingredients ] F/T
Be AER ial SANS?
Table 7: More Examples Of VCB Bench. Part V.
Model\Task Avg. Continuation Creation Empathy Recommendation Rewriting Safety Simulation
GLM4-Voice 85.82 83.20 88.80 81.40 92.60 90.00 72.40 93.20
Kimi-Audio 85.13 77.20 88.40 74.60 88.60 90.80 87.00 90.20
Qwen2.5-Omni 87.40 80.80 88.60 79.20 86.00 86.80 93.40 96.80
Baichuan-Audio-Chat 72.49 70.80 72.40 60.80 77.60 63.80 76.20 84.20
Qwen2-Audio-Instruct 84.56 79.40 85.80 77.40 91.80 81.40 83.40 92.20
StepAudio 87.17 87.40 90.40 73.40 92.80 87.80 81.80 96.60
StepAudio2Mini 82.79 82.60 82.20 77.60 90.60 87.60 65.40 94.40
Mimo-Audio 91.21 92.20 94.00 85.00 99.00 92.80 76.60 99.20
GPT40-Audio 91.24 89.40 93.40 85.00 95.20 91.80 85.80 98.20

Table 8: Chinese Text Side Instruction Following Objective Results.


Model\Task Avg. Emotional Control Language Control Non Verbal Vocalization Pacing Control Style Control Volume Control
GLM4-Voice 85.57 93.00 78.40 68.00 83.60 92.60 92.60
Kimi-Audio 85.69 88.80 79.20 64.20 88.20 95.00 92.40
Qwen2.5-Omni 71.37 83.00 65.80 55.80 55.40 82.20 83.40
Baichuan-Audio-Chat 78.96 85.60 72.40 60.40 81.80 87.40 81.00
StepAudio 80.25 88.40 72.40 61.20 80.00 87.80 86.00
StepAudio2Mini 78.81 87.80 70.40 58.80 84.20 85.40 79.80
Mimo-Audio 72.89 81.80 74.40 54.00 47.60 88.80 88.40
GPT4o0-Audio 88.15 92.40 87.80 75.00 83.20 92.00 94.00

Table 9: Chinese Speech Side Instruction Following Objective Results.

Model\Task Avg. Emotional Control Language Control Non Verbal Vocalization Pacing Control Style Control Volume Control
GLM4-Voice 64.86 76.00 54.00 56.60 64.60 66.60 68.60
Kimi-Audio 59.20 71.40 45.40 38.60 62.00 71.40 62.00
Baichuan-Audio-Chat 46.14 58.60 41.40 29.40 50.60 52.60 39.40
StepAudio 54.50 74.00 60.00 36.00 46.60 55.40 47.40
StepAudio2Mini 57.14 62.00 46.00 41.40 73.40 66.60 50.00
GPT4o0-Audio 65.72 66.60 64.00 56.00 73.40 71.40 60.60

Table 10: Chinese Speech Side Instruction Following Subjective Results.

Model\Task Avg.  ContinuationEn CreationEn Empathy En RecommendationEn Rewriting En SafetyEn Simulation En
GLM4-Voice 82.52 81.00 83.40 78.20 85.60 84.60 75.60 89.80
Kimi-Audio 88.92 88.00 93.00 73.80 90.20 90.00 90.80 97.00
Qwen2.5-Omni 72.58 62.00 73.40 78.00 73.40 78.20 64.60 79.80
Baichuan-Audio-Chat 76.22 79.60 84.40 59.60 76.40 80.60 63.20 90.80
Qwen2-Audio-Instruct 75.86 66.20 76.20 69.40 79.00 71.80 88.60 79.00
StepAudio 66.92 71.60 72.00 51.60 68.00 69.80 50.60 85.60
StepAudio2Mini 75.54 73.40 78.20 57.60 80.60 78.80 73.20 87.80
Mimo-Audio 91.76 92.40 94.80 86.80 93.40 90.40 86.40 97.80
GPT4o0-Audio 91.66 91.40 94.60 86.60 95.40 94.60 81.60 98.20

Table 11: English Text Side Instruction Following Objective Results

Model\Task Avg. Emotional Control En Language Control En Non Verbal Vocalization En Pacing Control En Style ControlEn Volume Control En
GLM4-Voice 78.52 82.80 68.00 64.40 81.40 84.60 88.20
Kimi-Audio 61.87 69.20 54.00 50.00 61.40 67.40 68.40
Qwen2.5-Omni 58.09 63.60 53.40 48.00 50.60 62.00 69.80
Baichuan-Audio-Chat 68.07 77.20 57.60 47.60 73.00 77.20 73.80
StepAudio 63.63 64.80 54.20 46.20 75.60 62.80 71.40
StepAudio2Mini 65.15 73.00 47.20 52.40 78.80 70.20 68.00
Mimo-Audio 24.25 27.40 24.80 22.60 20.40 22.40 26.80
GPT40-Audio 86.07 89.40 84.80 68.80 88.00 91.40 90.60

Table 12: English Speech Side Instruction Following Objective Results

Model\Task

GLM4- Voice
Kimi-Audio

Qwen2-Audio-Instruct 85.67
Qwen2.5-Omni
Baichuan-Audio-Chat 73.27
StepAudio2Mini

Avg.

85.13
85.67

86.93

87.80

Progression

92.20
92.40
93.20
93.60
80.00
92.60

Backtracking Transition

87.20
89.60
91.00
88.80
74.60
94.40

Table 13: Multi-turn Dialogue Evaluation Results.

76.00
75.00
72.80
78.40
65.20
76.20


Model\Task Avg. Math. Geogr. Polit. Chem. Biol. Law Phys. Hist. Med. Econ. Sports Cult.
GLM4-Voice 45.53 41.67 39.33 54.24 58.70 49.60 37.84 66.67 46.00 50.65 62.50 27.87 28.00
Kimi-Audio 53.51 66.67 52.00 52.54 71.74 49.60 54.05 67.65 50.00 54.55 60.42 36.07 48.00
Qwen2.5-Omni 55.43 63.89 53.33 52.54 71.74 49.60 43.24 70.59 54.00 53.25 75.00 36.07 53.33
Baichuan-Audio-Chat 44.48 55.56 44.00 49.15 56.52 40.00 40.54 58.82 41.33 45.45 54.17 36.07 34.67
Qwen2-Audio-Instruct 35.83 36.11 34.67 40.68 58.70 33.60 37.84 49.02 34.67 32.47 47.92 24.59 24.00
StepAudio 60.42 55.56 55.33 66.10 73.91 57.60 62.16 61.76 60.67 64.94 70.83 54.10 58.00
StepAudio2Mini 61.15 61.11 57.33 71.19 80.43 5645 4865 74.51 65.33 57.14 68.75 37.70 58.00
Mimo-Audio 56.58 58.33 43.33 45.76 60.87 60.00 40.54 71.57 65.33 62.34 66.67 3443 57.33
GPT40-Audio 61.29 63.89 62.00 67.80 65.22 59.20 70.27 72.54 53.33 70.13 85.42 62.30 43.33
Table 14: General Knowledge Evaluation Results.
Model\Task Avg. Basic Math Medium Math Analysis Induction Analogy Logic
GLM4-Voice 62.14 87.67 60.59 32.14 70.31 20.00 63.52
Kimi-Audio 79.94 98.63 89.41 66.67 85.94 45.00 66.04
Qwen2.5-Omni 80.24 95.89 78.82 71.43 78.13 47.50 81.76
Baichuan-Audio-Chat 60.33 69.18 52.94 45.24 70.31 25.00 72.96
Qwen2-Audio-Instruct 60.78 86.30 57.06 39.29 51.56 22.50 59.75
StepAudio 77.07 91.10 88.82 75.00 65.63 32.50 68.55
StepAudio2Mini 81.30 97.26 88.82 65.48 71.88 45.00 79.87
Mimo-Audio 84.01 94.52 88.82 72.62 78.13 52.50 85.53
GPT4o0-Audio 77.68 82.19 91.18 71.43 75.00 40.00 72.96
Table 15: Mathematical and Logical Reasoning Evaluation Results
Model\Task Avg. Inference Induction Analysis
GLM4-Voice 48.64 58.25 55.75 33.04
Kimi-Audio 74.76 85.71 83.50 56.48
Qwen2-Audio-Instruct 67.07 78.64 80.53 43.48
Qwen2.5-Omni 73.72 83.49 79.65 59.13
Baichuan-Audio-Chat 54.38 58.25 71.68 33.91
StepAudio 59.52 63.11 64.60 51.30
StepAudio2Mini 83.08 92.23 84.07 73.91
Mimo-Audio 87.92 95.15 88.50 80.87
GPT40-Audio 77.64 90.29 84.96 59.13
Table 16: Discoure Comprehension Evaluation Results.
sapere Age Accent Volume Speed
Avg. Child Elder Avg. Tianjin Beijing Dongbei Sichuan Avg. Down Up Avg.
GLM4-Voice 65.00 (-17.20) 83.00 (-1.20) 47.60 (-32.40) 82.40 (0.20) 85.00 (-0.60) 82.60 (-2.60) — 83.20 (3.20) 79.40 (0.00) 96.40 (0.60) 96.80 (0.80) 96.00 (0.40) — 39.20 (-55.20)
Kimi-Audio 45.60 (-30.80) 66.00 (-7.20) 25.80 (-53.80) 56.20 (-20.60) 54.40 (-23.80) 77.40 (18.80) 73.60 (-7.20) 36.60 (-44.00) 93.80 (-1.60) 93.20 (-2.00) 94.40 (-1.20) _ 87.80 (-4.40)
Qwen2.5-Omni 75.60 (-5.40) 82.60 (1.40) 68.80 (-12.00) 77.80 (-3.40) 79.40 (-1.80) 81.40 (5.40) 74.40 (-8.00) 77.20 (-5.60) 90.40 (0.20) 93.20 (3.20) 87.60 (-2.80) 89.20 (-1.60)
Baichuan-Audio-Chat 60.40 (-0.80) 61.80 (-0.40) 59.20 (-1.20) 61.60 (-0.60) 61.80 (-0.80) 62.60 (-5.40) 68.00 (3.20) 56.60 (-1.00) 77.20 (3.00) — 83.60 (13.20) 70.80 (-7.20) 60.00 (-7.80)
Qwen2-Audio-Instruct 76.40 (-5.80) 80.40 (-1.40) 72.60 (-10.00) 81.60 (0.80) 79.40 (0.60) 80.00 (-6.60) 83.20 (0.00) 83.40 (5.00) 93.20 (-1.40) 93.60 (-1.20) 92.80 (-1.60) 77.80 (-7.00)
StepAudio2Mini 75.60 (-4.80) 75.40 (-2.00) 75.80 (-7.60) 75.40 (-2.00) 78.80 (-0.60) 88.00 (2.60) 67.20 (-4.80) 72.80 (-3.40) 96.60 (2.80) 96.00 (2.00) 97.20 (3.60) + 76.00 (-14.40)
Mimo-Audio 76.00 (-14.80) 92.80 (-0.40) 59.60 (-28.80) 88.40 (-0.20) 90.60 (2.40) 96.00 (10.60) 88.80 (1.60) 82.80 (-8.40) 99.20 (1.00) — 98.80(-0.40) 99.60 (2.40) —_ 60.40 (-33.00)
GPT40-Audio 79.00 (-8.60) 89.00 (0.00) 69.20 (-17.00) 88.40 (2.20) 91.80 (4.20) 89.40 (0.00) 85.60 (0.80) 86.60 (2.20) 96.80 (-1.20) 97.20 (0.00) 96.40 (-2.40) 43.40 (-48.40)

Table 17: Speaker Variations Evaluation Results - Experimental Group (Difference from Control Group).

Non Vocal Noise

Vocal Noise

Unstable Signal

Model\Task

Avg. Echo Outdoors Far Field Avg. Tv Playback Background Chat Vocal-Music Voice Announce Avg.
GLM4-Voice 64.00 (-24.60) 45.00 (-36.80) 96.60 (-0.60) 74.80 (-12.60) 89.20 (-0.80) 85.60 (-3.80) 83.00 (-1.40) 93.00 (-0.80) 92.80 (1.60) 78.40 (-14.60)
Kimi-Audio 74.00 (-5.40) 60.60 (-11.60) 94.80 (4.60) 84.00 (-1.20) 75.80 (-9.60) 76.80 (-8.20) 58.20 (-11.60) 93.80 (0.60) 70.20 (-19.80) 76.80 (-11.20)

Qwen2.5-Omni

69.40 (-16.80) 52.00

Baichuan-Audio-Chat 56.60 (-15.60) 45.00
Qwen2-Audio-Instruct 59.80 (-17.60) 37.80

StepAudio2Mini
Mimo-Audio
GPT4o-Audio

65.00 (-19.20) 45.80
73.20 (-20.20) 54.00
68.20 (-19.20) 61.00

(-30.60)
(-20.00)
(-32.00)
(-33.00)
(-35.60)
(-20.60)

95.40 (0.60)
81.20 (-8.60)
93.80 (-1.60)
94.80 (-1.20)
99.40 (-0.60)
98.20 (-0.60)

82.80 (-3.00)
58.20 (-12.60)
76.00 (-1.20)
78.80 (-6.40)
90.80 (-4.60)
54.80 (-34.40)

85.20 (-4.80)
73.40 (-2.80)
87.60 (-1.60)
83.40 (-5.80)
90.40 (-5.00)
92.00 (-1.60)

80.80 (-6.20)
73.40 (0.40)
82.20 (-6.00)
84.00 (-2.20)
87.00 (-9.00)
86.20 (-6.60)

74.80 (-8.20)
64.40 (1.00)
80.00 (-1.40)
66.20 (-12.00)
86.80 (-4.80)
88.00 (-0.20)

95.60 (-1.80)
81.40 (-5.40)
96.20 (-0.20)
94.80 (-2.60)
99.60 (0.60)
96.40 (0.20)

85.60 (-4.40)
72.00 (-5.60)
88.60 (0.40)
84.80 (-6.80)
85.60 (-8.20)
94.20 (-1.60)

79.80 (-10.20)
67.20 (-4.60)
77.80 (-13.40)
81.60 (-8.40)
95.20 (-4.20)
74.60 (-21.60)

Table 18: Environmental Variations Evaluation Results - Experimental Group (Difference from Control Group).


Content Variations

Model\Task

Fillers Repetition Mispronunciation Grammatical Error Topic Shift Code Switching
GLM4- Voice 81.80 (-3.00) 89.20 (-0.60) 82.20 (-10.60) 85.60 (-6.00) 86.20 (-6.40) 80.80 (-10.80)
Kimi-Audio 76.00 (-0.40) 79.40 (-1.20) 61.80 (-23.20) 77.40 (-10.40) 79.40 (-6.80) 84.00 (-8.80)
Qwen2.5-Omni 79.00 (-2.00) 84.80 (0.00) 80.60 (-8.40) 79.20 (-8.60) 75.00 (-11.40) 80.40 (-9.20)
Baichuan-Audio-Chat 69.60 (1.20) 72.00 (-0.20) 71.40 (-1.20) 65.60 (-11.00) 61.60 (-12.40) 68.40 (-7.60)
Qwen2-Audio-Instruct 75.00 (1.60) 85.80 (-1.20) 78.40 (-10.00) 79.40 (-9.20) 67.00 (-19.80) 76.40 (-11.60)
StepAudio2Mini 79.20 (-3.80) 81.60 (-2.60) 82.20 (-5.60) 80.80 (-8.20) 80.60 (-3.00) 83.60 (-8.60)
Mimo-Audio 92.00 (-1.80) 95.00 (-0.40) 96.00 (-3.60) 97.40 (-0.60) 92.00 (-5.60) 92.60 (-7.00)
GPT40-Audio 88.40 (1.80) 93.20 (0.00) 92.60 (-4.00) 89.00 (-5.00) 90.60 (-2.60) 96.80 (-0.60)

Table 19: Content Variations Evaluation Results - Experimental Group (Difference from Control Group).

Model\Task Continuation Creation Empathy Recommendation Rewriting Safety Simulation
GLM4-Voice 79.2 (83.2) 80.8 (88.4) 78.8 (81.8) 88.6 (92.4) 84.0 (88.4) 75.4 (75.6) 88.2 (92.6)
Kimi-Audio 75.4 (76.8)  81.4(86.6) 64.2 (64.8) 81.0 (84.8) 83.2 (88.4) 75.8 (76.6) 82.0 (83.4)
Qwen2.5-Onmi 71.4(72.4) 75.4 (80.4) 74.2 (75.8) 84.8 (86.2) 72.8 (79.0) 80.0 (80.8) 77.8 (79.2)

Baichuan-Audio-Chat — 83.2(81.8) 86.0 (89.2) 73.6 (71.4) 85.6 (90.2) 75.2(79.4) 78.6 (78.8) 92.6 (93.4)
StepAudio2Mini 77.0 (82.2) 77.4(86.6) 70.8 (73.8) 79.4 (88.0) 81.8 (87.8) 62.6 (63.8) 83.8 (95.2)
GPT40-Audio 86.6 (89.2) 85.4 (93.4) 83.2 (85.6) 91.2 (96.6) 87.4 (93.4) 83.6 (85.8) 95.2 (98.2)

Table 20: A2A Result in TIF - A2A W/ ASR (A2A W/O ASR)

Model\Task Continuation En CreationEn EmpathyEn RecommendationEn RewritingEn SafetyEn Simulation En
GLM4-Voice 65.0 (81.6) 75.4 (83.8) 72.6 (77.4) 78.0 (85.4) 77.8 (83.8) 69.2 (76.6) 86.0 (89.8)
Kimi-Audio 50.4 (62.0) 60.4 (83.4) 53.0 (56.0) 62.4 (80.8) 65.6 (79.0) 78.4 (83.4) 58.4 (69.8)
Qwen2.5-Omni 57.0 (59.6) 58.2 (64.2) 56.4 (60.2) 66.4 (72.4) 56.2 (66.6) 73.4 (75.6) 56.2 (63.0)
Baichuan-Audio-Chat 79.8 (77.6) 80.0 (81.0) 75.8 (75.0) 77.0 (76.8) 79.8 (81.2) 72.0 (72.0) 86.2 (86.0)
StepAudio2Mini 66.2 (67.0) 73.0 (78.0) 55.2 (56.2) 75.0 (77.0) 70.6 (73.0) 69.6 (70.4) 84.2 (87.8)
GPT40-Audio 90.2 (91.4) 90.0 (94.8) 85.0 (85.8) 93.4 (95.8) 92.4 (94.6) 80.4 (81.6) 96.0 (98.4)

Table 21: A2A Result in TIF-En - A2A W/ ASR (A2A W/O ASR)
