arX1v:2510.10060v1 [cs.LG] 11 Oct 2025

Technical report

TRANSLUTION: UNIFYING SELF-ATTENTION AND
CONVOLUTION FOR ADAPTIVE AND RELATIVE MOD-
ELING

Hehe Fan Yi Yang

Zhejiang University Zhejiang University

hehefan@zju.edu.cn yangyics@zju.edu.cn

Mohan Kankanhalli Fei Wu

National University of Singapore Zhejiang University

mohan@comp.nus.edu.sg wufei@zju.edu.cn
ABSTRACT

When modeling a given type of data, we consider it to involve two key aspects:
1) identifying relevant elements (e.g., image pixels or textual words) to a cen-
tral element, as in a convolutional receptive field, or to a query element, as in
self-attention, and 2) encoding these tokens effectively. Self-attention can adap-
tively identify these elements but relies on absolute positional embedding for
structural representation learning. In contrast, convolution encodes elements in
a relative manner, yet their fixed kernel size limits their ability to adaptively
select the relevant elements. In this paper, we introduce Translution, an oper-
ation that unifies the adaptive identification capability of self-attention and the
relative encoding advantage of convolution. However, this integration leads to
a substantial increase in the number of parameters, exceeding most currently
available computational resources. Therefore, we propose a lightweight variant
of Translution, named a-Translution. Experiments on computer vision and nat-
ural language processing tasks show that Translution (including a-Translution)
achieves superior accuracy compared to self-attention. The code is available at

https://github.com/hehefan/Translution

1 INTRODUCTION

Recent evidence suggests that directly scaling up deep neural networks, particularly Transform-
additional data and parameters is encountering diminishing returns. Leading Artificial Intelligence
(AI) labs have similarly noted slower-than-anticipated improvements in next-generation models,
despite extensive training efforts. Given the saturation of available data and limitations imposed by
current scaling laws, it is crucial now to reflect on past successes and pursue the design of innovative
neural networks to sustain future progress in deep learning.

When employing deep neural networks to model a specific type of data, the process can be de-
composed into two key aspects: 1) identifying relevant data elements and 2) eee these el-
ements into effective ABR Sie When using TS Bes neural networks era LeCun et al.|
(998) rizhevsky et al 2012) Simonyan & Zisserman|2015|Szegedy eta, [2013[Fe etal, 2016
to process images, ia ae en 1s pixel. When ne “te ormers, the element is ware or
natural language processing and patch for visual tasks.

1.1 IDENTIFICATION OF RELEVANT ELEMENTS

In convolution, as shown in Figure[I](a), the relevant element identification step is handled by con-
volutional filters (kernels) with a fixed local receptive field. This fixed kernel defines a neighborhood
that is considered relevant to the center. For visual data like images, such local focus is often ef-


Technical report

0:03 0.08 0.06 0.08| 0.04 § 0:07|0.16 0.05)|0.07 |0.03

0.04 0.05 0.13 0.05|0.04 0.06 |0.06 0.08|0.04 | 0.03

0.05 |0.07)0.05)05 0.05 |0.07|0.04 |0:

b0 0) 0| of 10.04 0.04 0.03| 0.03
|

0.04 0.03|0.03 |0.03

(a) Convolution | (b) Self-attention

Figure 1: Difference between convolution and self-attention in identifying relevant elements (blue
patches) for the kernel center or query element (yellow patch). Here, convolution is assumed to
operate on image patches. 1) Convolution utilizes a fixed kernel size to define a neighborhood of
elements considered relevant, inevitably including some irrelevant regions, particularly near object
boundaries or within background areas inside the window. The fixed receptive field in convolution
can be interpreted as a special case of attention, where the attention score is set to 1 within the
receptive field and 0 outside it. 2) Self-attention adaptively identifies relevant elements by assigning
greater attention scores to areas with higher relevance, thereby mitigating the inclusion of noisy or
irrelevant information.

fective because spatially adjacent pixels tend to be related (e.g., forming parts of the same object).
However, the rigid nature of a fixed-size kernel makes convolution inevitably cover irrelevant pixels,
especially near object boundaries or in background areas that fall inside the window.

In contrast, as shown in Figure[I](b), self-attention 2017) can adaptively identify

relevant regions. Instead of being limited to a predetermined locality, it allows the model to dy-
namically attend to relevant regions. This means that self-attention can focus on important features
regardless of their physical distance. This capability provides greater flexibility compared to the
convolution’s fixed receptive field.

1.2 ENCODING OF RELEVANT ELEMENTS

When it comes to encoding the structure from these relevant elements, convolution and self-attention
employ different strategies. As shown in Figure[2](a), a convolutional kernel learns distinct param-
eters {W5, 5, } for each relative direction and distance within its receptive field. In other words,
the filter has separate parameters W5,.,5y for each offset 5, 5, from the center. This design enables
convolution to encode local structure relatively — capturing orientation and distance relationships.

In contrast, as shown in Figure [2|(b), self-attention uses three shared sets of parameters W4, W*
and W’” to process inputs for all positions. Consequently, the query, key and value of self-attention
do not encode whether one patch is to the left or right of another. To introduce positional informa-
tion, Transformer incorporates absolute positional embeddings into the input features at the outset.
Although these embeddings enable Transformer to infer order or spatial relationships, they intro-
duce noise into each token’s representation. The absolute position information becomes part of the
input features. Consequently, when the same object moves to a different location, Transformer may
struggle to recognize it.

1.3. UNIFICATION OF CONVOLUTION AND TRANSFORMER

In summary, convolution encodes structure through fixed local filters with position-specific weights,
whereas self-attention relies on adaptive global attention and requires absolute positional encoding
to capture order or spatial structures.

In this paper, we introduce Translution, a new type of operation that unifies the adaptive identifica-
tion capability of self-attention with the relative encoding advantage of convolution. Specifically,
Translution employs a convolution-style approach that assigns separate parameters (matrices) to
each distance and direction when computing the query, key and value. This design enables Translu-
tion to effectively encode relative structures.


Technical report

{W_.-1,W_10, W_41)Wo-1,Wo0,Wo1,W1-1,Wi0, Wii h1 We {W4, wk, w’}
l

lwiwilwlw|w y12%

| |

l

Vwlw)w|w|w N

lwlwilwlwilw Ava

| traitN———7 x
\

lwiwlwiwilw [ |

| | \ 1

| \ /

Nwiwlwl|wlw 3} 24 |

i ol

|

(a) Convolution | (b) Self-attention

Figure 2: Difference between convolution and self-attention in encoding relevant elements: consider
the scenario where convolution and self-attention are capturing the structure of a circle. 1) Convo-
lution learns separate parameters {W5,,.5, } for each offset, where 6,,6, € [—1, 1], from the kernel
center, allowing it to effectively encode relative local structures. Thus, when the circle appears in
a different location, it is still readily recognized due to this relative awareness. 2) Self-attention in-
corporates absolute position into each token’s representation and uses position-irrelevant parameters
W « {W4,W*, W”} across all tokens for computing query, key and value, respectively. While
this method facilitates general processing, the inclusion of absolute positional embeddings makes it
more challenging to recognize the circle when it is moved to a different location.

However, this unification leads to a significant increase in the number of parameters and exceeds
most currently available computational resources. Therefore, we propose a lightweight variant of
Translution, named a-Translution, which significantly reduces the number of parameters. This vari-
ant achieves lower accuracy than the “ideal” (original) Translution but better accuracy than self-
attention.

As a fundamental operation, we investigate whether Translution can outperform self-attention.
We conduct experiments on two widely-used Transformer architectures: Vision Transformer
(ViT) (Dosovitskiy et al.|/2021) for computer vision tasks and Generative Pre-trained Transformer
(GPT) (Radford et al. for natural language processing tasks. Exper-

iments demonstrate that Translution and a-Translution surpass self-attention in terms of accuracy.

2 RELATED WORK

Transformer (Vaswani et al.|/2017}|Radford et al.|/2018 2019} |Dosovitskiy et al.}/2021

Liu et al. Touvron et al.|/2021) eschews recurrence (as used in recurrent neural networks) and
kernel size (as used in convolutional neural networks), instead employing self-attention for relevant
region identification. Because it has no built-in notion of order, Transformer incorporates explicit
absolute positional embeddings into token embeddings, enabling the model to utilize sequence order.

Subsequent work has explored “relative attention” (Shaw et al.|/2018 2019
2019 2019 2019} [Raffel et al] [2020 2021), which inte-

grates relative position information into self-attention. They can be categorized into three families:
1) Relative positional vector. Shaw et al.enhanced Transformer for language modeling by adding
learnable relative positional vectors into the key and value computations, respectively (Shaw et al.
2018). BoTNet and HaloNet (Vaswani et al. extended this approach
to two dimensions for image processing by adding learnable relative positional vectors into key.
2) Relative positional scalar. Swin Transformer (Liu et al.} [202 1), CoAtNet (Dai et al.| (2021), and
ConViT{d’ Ascoli et al.|(2021) incorporate a learnable relative positional bias (a scalar) into the atten-

tion score. In these methods, the original self-attention can be regarded as content attention, which
measures relationships from the token-feature perspective, while the additional relative positional
bias can be regarded as position attention, which measures relationships from the token-position
perspective. 3) Rotary position embedding. RoFormer introduces a rotary posi-
tion embedding mechanism, which encodes relative positional information by applying a rotation
operation in the Query and Key representation space. Unlike these existing methods, Translution
employs a convolution-style approach that uses relative positional matrices for query, key and value
computation. Section|[D]provides a formal comparison of these methods.


Technical report

Convolutional neural networks (LeCun et al.||1998}|Krizhevsky et al.| |2012}|Simonyan & Zisser-
2015}|/Szegedy et al.|/2015 2016) have been the backbone of deep learning for years.

By using small, shared kernels and pooling, convolutional neural networks efficiently capture local
patterns. Recent architectural developments integrate self-attention with convolution. For instance,

Conformer (Gulati et al.|/2020) combines convolution layers and self-attention layers to capture both

local and global dependencies in audio sequences. Similarly, CeiT 2021) uses convo-
lutions to extract low-level features and self-attention to model long-range dependencies. Unlike

these architectural methods, Translution operates at the basic module or layer level, blending the
advantages of self-attention and convolution into a unified fundamental operation.

3. PRELIMINARY: CONVOLUTION AND SELF-ATTENTION

3.1 CONVOLUTION

Suppose fz € IR! denotes the feature or representation at location (x, y) in an image of height
HT and width W, where C’ is the number of the input feature channels. Convolution is designed to
capture the local structure centered at (a, y) with a fixed kernel size h x w,

[h/2| [w/2]
fey= > SS) fr+5,y+5, * W5,,5y1
b2=—|h/2] dy=—-|w/2]

where W5,5, © RCC’ denotes the learnable parameters corresponding to the displacement
(5x, 5y), C’ indicates the output feature dimension, and - denotes matrix multiplication. By as-
signing a set of parameters for each offset within the receptive field, convolution is able to discern
direction and distance, and capture the local structure relatively. This means that when the absolute
location of an object changes, it can still capture the same structure. However, convolution employs
a rigid method to identify relevant regions, i.e., using a fixed-size window, making it inevitably
include irrelevant pixels or regions — particularly near object boundaries or in background areas
within the window.

3.2 SELF-ATTENTION

Suppose 2; € R!*° represents the feature or representation of the i-th patch at location (2;, y;).

Transformer 2017) first incorporates the embedding of absolute position into the

input x;, as follows,
input positional embedding: f; = x; + Embed(2;, y;).

Then, self-attention performs two separate linear projections on the feature to generate query q; €
RX’ and key kj € R!*C", where C’ is the dimension for query or key,

query encoding: q; = fi: W%,

key encoding: k; = f;- wt,
where W2/W* € ROXC’, Subsequently, scaled dot-product attention is computed for each query,
and a softmax function is applied to normalize the attention weights for a query across all positions,
qi: ky ets
where N = H x W. Next, self-attention conducts another linear projection on the input feature to
generate value v; € RIxc" as follows,

value encoding: v; = f;-W”,

attention: aj; =

where W, € ROX“. Finally, the output is computed as a weighted sum of the values, i.e.,

N
weighted sum: f/ = So aij xX Vj,
j=l
where f/ € R!*C", In this way, self-attention can adaptively search for related regions, providing
greater flexibility than methods that use local fixed-size windows. However, unlike convolution,
which learns a feature encoding for every direction and distance, self-attention does not encode the
structure in a relative manner.


Technical report

(a) Self-attention

output F’ € c!

(b) Translution output F’ € RY

attention matrix ot value tensor
Ac IRNXNx1

‘A Ve RYxNxc!

elementwise multiplication
Pare |

= »
vi; = fi W585,

image size: N = HxW

attention
matrix
AE IRNXN

matmul: a; ; = qij° Kj;

t :

_ k
9ij = fi: W355 kg =P W538,

| osition a ea t 7

Figure 3: Comparison of self-attention and Translution. 1) Self-attention employs three shared
sets of weights, i.e., W4%, W*, and W”, across all patches to compute query, key, and value,
respectively. 2) Translution uses separate parameters for each offset (direction and distance), i.e.,
{Wi 5st Wh 5,} and {W3’ ; }, to encode relative structures.

!
l
I
I
l
!
|
l
l
I
I
I
I
=fi-w’|
|
|
l
I
I
!
|
!
l
l
I
I
!

3.3. TRANSLUTION

Translution is designed to integrate the adaptive related region identification capabilities of self-
attention with the relative encoding strengths of convolution. Specifically, as shown in Figure 3]
Translution employs a convolution-style formulation by assigning different parameters to compute
query, key, and value, respectively, as follows:
relative query encoding: qi; = fi: Wi. sy be = 24 — Dy, Oy = Yi —Y
relative key encoding: k;; = f; - Ws
T .
. . Gij °K; . eins
relative attention: a;,; = —~—", ai; = ,
Translution VC’ a1 eu" (1)

relative value encoding: vj,; = fj -W3,s,;

n=

N
weighted sum: f/ = s Qi5 X Vij,
j=l

where wy by / wr by /Ws. oy © ROxC represent the learnable parameter matrices for the query,
key, and value corresponding to the displacement (dz, dy).

Translution unifies convolution and self-attention.

The fixed receptive field in convolution can be interpreted as a special case of attention, where the
attention score is set to | within the receptive field and 0 outside it, as shown in Figure[2| The weights
W141, W*, and W” in self-attention serve as shared linear projections that are uniformly applied
across all spatial directions and distances. Consequently, Translution integrates the functionalities
of convolution and self-attention, as follows,

Convolution: fi) = ea a3 X f7-Ws,,6,. where aij = { 7 Gordy) © kennel,

otherwise.
Self—attention: ff =>, ai; x f;-W” where a,,= "2, aj = —@ 4
. yo gal 4) a ’ td / Gr? 7 we, erin *
r =
‘ . — N . . Vv oe Gig KRG an ettJ5
Translution: f; = Ve j=1 ig X f; -W Sx i5y? where aij = Vor? ij = yen

In other words, convolution and self-attention can be viewed as specific instances of Translution,
where convolution simplifies the attention mechanism and self-attention omits the encoding of di-
rection and distance.


Technical report

3.4 a-TRANSLUTION

Suppose there are H x W input image patches. The relative encoding method in Translu-
tion requires (2H — 1) x (2W — 1) x C x C” parameters. Specifically, it requires one pa-
rameter matrix WE os, wr oy OF Ws. x, © RCxC’ for each relative position (dz, 6,), where
dz € {-(H —1),--- ,0,---,H—1} and 46, € {-(W — 1),--- ,0,--- ,W— 1}. This approach
leads to excessive parameter demands, making it impractical for most computational devices cur-
rently. For instance, in the ViT/16 architecture (Dosovitskiy et al.||2021) with tap resolution
224 x 224, we have H = W = 224 = 14, resulting in posers ata — a ee x (2W — 1) = 729 distinct
weight matrices for query, key or value, To reduce the number of parameters, we propose a vari-

ant of Translution, i.e., a-Translution, which decreases both the input dimension C and the output
dimension C’ of each wi 5° Wk 5? and Wy’ 5,? 48 follows:
xwiOy Ee x)

qd q qd k k k v v v v
Ws, by =>W;- Ws, 5° Ws.5, =>W;.-: W5.5,> W3.5, =>W,- W3.5, -Wy,

where Wi'/WE/W? e ROXO—=, Wi 5 /WE 5, /W3. 5, ©RO*, We © ROX, and Cl <
C, C? < C’. Smaller values of C+ and C? will significantly reduce the number of parameters.

However, setting Ct and C? too small may overly compress the query, key and value information,
negatively impacting performance. To preserve the information, we incorporate the query, key and
value computation mechanism of self-attention into a-Translution. Specifically, the updated com-
putation is defined as follows:

query encoding: qi,; = fi: W/'- WS. is, gi = fi- W",
key encoding: kjyi = fj;-Wi-W*5, 5, kj =F) W*,

attention: a;; = —~ ach di I aj; = _ et
‘ . tJ 4,9 N ;
o-Translution VC! , SoM erin , (2)

value encoding: uj; = fj; (Wy - Ws) 5,-W +W"),
weighted sum: f/ = So ai X Vij.

In this way, a-Translution not only possesses relative modeling capability but also reduces the num-
ber of parameters.

4 EXPERIMENT

In this section, as a fundamental operation, our primary objective is to compare Translution with
self-attention, rather than to achieve state-of-the-art performance through specialized network ar-
chitectures or extensive training techniques. To this end, we conduct experiments using two widely
adopted Transformer architectures:

¢ Vision Transformer (ViT) ‘osovitskiy et al] for computer vision tasks.
* Generative Pre-trained Transformer Rad aa et al.||2 O18] DOTS) [Brown scal] 2020)

for natural language processing tasks. Section|C}demonstrates how to apply Translution to
text modeling.
Table [I|provides an overview of various architecture configures. We substitute self-attention in ViT
and GPT with Translution, while maintaining the remaining architecture unchanged.
Table 1: Specifics of architecture configures used in this paper.

Architecture | Depth (#Layers) | Embedding Dim (Hidden size) #Heads MLP Dim (Feedforward)
A 6 192 3 768
B 12 192 3 768
C 12 384 6 1,536

Due to limited computational resources, our evaluation is primarily conducted on small- and
medium-scale architectures. Large-scale evaluation can be performed when single-GPU memory
capacities approach approximately 2 ~ 3 TB. All training starts from scratch. The default compres-
sion dimensions for the relative encoding in a-Translution are set as C! = C? = 8.


Technical report

4.1 IMAGE CLASSIFICATION WITH VIT
4.1.1 DYNAMIC MNIST

To evaluate the capability of modeling relative structure, we synthesize a dynamic MNIST

dataset 2015} [Fan & Yang] [2019), where digits (originally sized 28 x 28 pix-

els) move within a 84 x 84 pixel area, as illustrated in Figure [4] For comparison, we also create a
static MNIST dataset of the same size, where digits remain fixed at the center of each image.

Static
, 6
nse *“|s5 7 ,
By 3 7

Figure 4: Examples of static and dynamic MNIST. Static MNIST digits are fixed at the center of
images, whereas dynamic MNIST digits are randomly positioned within the images.

Table 2: Top-1 accuracy (%) on different MNIST settings with the ViT-A architecture. A — B
denotes that models are trained on dataset A and evaluated on dataset B.

Arch. Method #Params|Static— Static] Dynamic— Dynamic | Static Dynamic
Self-attention 27M| 98.48 92.64 18.18
ViT-A/12| a-Translution (relative dim = 8 46M 98.48 97.31 34.90
Translution 116.2M 98.60 97.35 36.40
Self-attention 27M] 98.52 93.90 19.94
ViT-A/7 | a@-Translution (relative dim =8 8.3 M 98.81 98.57 40.05
Translution 355.0 M 98.91 98.60 48.07

As shown in Table[2] all models achieve high accuracy when trained and evaluated on static MNIST.
However, when digit locations vary, the self-attention’s accuracy significantly decreases, whereas
Translution (including a-Translution) still maintains high accuracy. This is because absolute po-
sitional embedding makes digit locations part of its representation. Consequently, when digits
shift positions, networks may become confused and fail to recognize digits accurately. In con-
trast, Translution employs relative encoding, effectively capturing digit structures independently of
their absolute locations. This significantly reduces sensitivity to location variability, demonstrating
Translution’s superior capability in modeling relative structures. However, when training on static
MNIST, the uniformly black image background causes some Wj, 5, not to be well trained. As a
result, when evaluated on dynamic MNIST, Translution fails to achieve very high accuracy.

4.1.2 IMAGENET

ImageNet-1K (2009) is a widely used dataset for computer vision research, particularly
in the area of image classification. It contains 1,000 object categories (classes), each with approx-

imately 1,300 training images and 50 validation images, amounting to about 1.28 million training
images and 50,000 validation images in total. Images are resized to 224 x 224. As shown in Table[3]

compared to self-attention (Vaswani et al.|2017), Translution and a-Translution effectively improve

ImageNet classification.

We compare Translution with existing positional encoding strategies, which typically represent po-
sitional information by introducing additional positional biases, as scalars|Liu et al.|(2021);
(2021) or vectors 2017 2018). The formal differences between

these approaches are detailed in Section|D] As shown in Table |4| compared to existing relative
encoding methods, Translution achieves a notable improvement in accuracy.

4.1.3. ABLATION STUDY

1) Is the improvement of Translution (including a-Translution) caused by the introduction of addi-
tional parameters or the proposed modeling approach based on relative encoding?


Technical report

Table 3: Accuracy (%) on the ImageNet-1K dataset with patch sizes of 56 and 32. Training is
conducted from scratch without pretraining on external datasets, with a batch size of 256.

Architecture Method #Parameters Top-1 Top-5
Self-attention (Vaswani et al.) 2017) 4.7M 46.28 71.17

ViT-A/56 a-Translution (relative enc dim = 8 5.3M 48.36 73.31
Translution 38.5 M 52.41 76.50

Self-attention (Vaswani et al.) 2017) T4M 53.75 77.59

ViT-B/56 a-Translution (relative enc dim = 8 8.7M 55.87 79.16
Translution 75.0 M 59.51 81.97

Self-attention (Vaswani et al. 2017) 25.3 M 64.15 84.95

ViT-C/56 a-Translution (relative enc dim = 8 30.5 M 66.54 86.49
Translution 296.0 M 68.05 88.62

Self-attention (Vaswani et al. 2017) 3.5M 57.63 80.96

ViT-A/32 a-Translution (relative enc dim = 8 5.3M 60.26 83.07
Translution 116.9M 66.03 86.01

Self-attention (Vaswani et al. 2017) 6.1M 66.13 86.87

ViT-B/32 a-Translution (relative enc dim = 8 9.9M 67.63 87.96
Translution 223.1 M 70.63 90.10

Translution runs out of memory under the following architectures.

WTC Self-attention 22.9M 73.62 91.12
a-Translution (relative enc dim = 8 38.0 M 74.19 91.52

ViT-A/16 Self-attention (Vaswani et al. 2017) 3.0M 64.71 86.25
a-Translution (relative enc dim = 8 10.7M 69.28 89.24

ViT.B/16 Self-attention (Vaswani et al. 2017) 5.7M 73.51 91.89
a-Translution (relative enc dim = 8 21.1M 76.20 93.04

ViT-C/16 Self-attention (Vaswani et al. 2017) 22.0 M 78.91 94.10
a-Translution (relative enc dim = 8 85.4 M 79.70 94.52

Table 4: Comparison of different positional encoding strategies. Results are reported on ImageNet-
1K with ViT-A/56, trained from scratch (no external pretraining) using a batch size of 256.

Method #Parameters Top-1 Top-5
Self-attention w/o Pos Emb 4.69 M 42.49 67.39
Self-attention w/ Pos Emb 2017) 4.69 M 46.28 TAT
4.74M 46.39 71.25
4.74M 46.35 71.04
4.69 M 46.36 71.31
4.69 M 46.39 71.44
4.69 M 46.65 71.51
a-Translution 5.33 M 48.36 73.31
Translution 38.53 M 52.41 76.50

Compared to self-attention, which employs three parameter matrices W?, W*, W” to compute
query, key and value, Translution uses three groups of parameter matrices {W¢ ; }, {Wf 5, })
aidy .

{W? . } for relative encoding, thus introducing more parameters.
Sx Oy

To investigate whether the improvement arises from the increased parameter count or from the rela-
tive encoding method itself, we conducted the following experiment:

relative encoding: Ws, W5.5,, Wé.5, = absolute encoding: Wi,, Wi;, W2;,

where 6, € {—(H —1),--- ,0,---,H — 1}, dy € {-(W — 1),--- ,0,--- ,W — 1}, and indices
i € [1,H x W] andy € [1,H x W]. Specifically, for each pair of patches (i, 7), a distinct
parameter matrix is employed to calculate query, key or value, rather than using the shared offset-
based matrices. Under this modification, Translution transitions to absolute modeling. Moreover,
this adjustment significantly increases the number of parameter matrices from (2H — 1) x (2W — 1)
to (H x W)?.

As shown in Table[5| although absolute encoding involves significantly more parameters, it achieves
lower accuracy than relative encoding. Therefore, simply increasing the number of parameters does
not lead to performance improvements.

2) Impact of relative encoding dimension on the performance of a-Translution.


Technical report

Table 5: Investigation of whether the improvement of Translution arises from the additional param-
eters or the proposed relative encoding method (We by? ws by? ws by ). Because the absolute

encoding method (Wii, wy, , W;”,) consumes a large number of parameters, Translation with ViT-
A/7 encounters the out- /of-memory 1 issue. Therefore, experiments are conducted using ViT-A/12.
Method Encoding | #Parameters | Static—Static | Dynamic—+Dynamic | Static+-Dynamic
o-Translution relative 4.6M 98.48 97.31 34.90
absolute 28.7 M 98.42 96.18 25.37
Transiudon relative 116.2M 98.60 97.35 36.24
. absolute 1660.9 M 98.55 53.79 11.23

Table 6: Impact of relative encoding dimension on the performance of a-Translution with ViT-A/56.

Relative Enc Dim | #Params | Top-1 | Top-5 Relative Enc Dim | #Params | Top-1 | Top-5
Cci=c7=0 47M | 42.49 | 67.39 || C'=C07=8 5.3M | 48.36 | 73.31
Cci=C?=2 48M | 46.10 | 71.29 || C!=C?=16 7.0M | 48.91 | 73.65
Cl=C?=4 49M | 47.61 | 72.18 || Ct =C? =32 13.8M | 50.07 | 74.84

To reduce parameter usage, a-Translution employs smaller input (C') and output (C7) dimensions
for{Wg ; }.{Wé 5,t and {WY by \. In our experiments, we set the relative encoding dimensions

as C' = OC? = 8. This section investigates the impact of varying C! and C? on performance. As
shown in Table|6} increasing the relative encoding dimension improves accuracy but results in more
parameters. Therefore, the relative encoding dimension presents a trade-off between efficiency and
effectiveness for a-Translution. (When C! = C? = 0, it reduces to self-attention without positional
embedding.)

4.2 NATURAL LANGUAGE MODELING WITH GPT

To compare Translution and Transformer for natural language processing, we conduct experiments
using the OpenWebText dataset (Gao et al.| TES | at 2020), an apeely available reproduction of OpenAl’s
proprietary WebText dataset used for GP'T-2 (Rad Radford et al, et al.||2019). OpenWebText contains 9 billion
training tokens and 4 million validation token wih aoe with a ay size of 50K. We use perplexity,
defined as the exponentiation of the cross-entropy loss, as the evaluation metric, where a lower per-
plexity indicates stronger language modeling performance. Since the most powerful GPU available
to us has 80GB memeory, Translution can handle at most a text sequence of length 160 with the
GPT-A architecture. Therefore, we conduct the Translution experiment with sequences of length

160. As shown in Table[7] Translution achieves lower perplexity compared to Transformer, demon-
strating its effectiveness in natural language modeling.

Table 7: Perplexity on OpenWebText using a batch size of 8 and a sequence length of 160.

Architecture Method #Parameters Perplexity |
Self-attention (Vaswani et al.) 2017 22.0M 60.40
GPT-A-160 a-Translution (relative enc dim = 8 23.7M 57.97
Translution 127.5M 56.26
Translution runs out of memory under the following architectures.
Self-attention (Vaswani et al.) 2017 24.7M 54.82
RB -iee a-Translution Ne swan enc y im = i 28.2 M 52.72
Self-attention (Waswani et al] por} 60.0 M 39.88
GPT 160 a-Translution (relative enc dim = 8 74.0 M 39.25

5 CONCLUSION

In this paper, we introduce Translution, a new operation that unifies self-attention and convolution
for adaptive and relative modeling. Experiments on computer vision and natural language processing
tasks demonstrate the effectiveness of Translution.

However, due to current limited computational resources, the validation in this paper is prelimi-
nary. We encourage the community to further evaluate Translution using larger-scale frameworks
and datasets in diverse scenarios to verify its broader applicability, particularly when single GPUs
equipped with over 2 ~ 3 TB of memory are available.


Technical report

Given Translution’s substantial parameter consumption, it is worthwhile to explore optimized vari-
ants, such as a-Translution. For instance, certain relative positions may share the same parameter,
especially when the distance between elements is too long. At the same time, extending Translution
to 3D, video, molecule, and other modalities of processing holds significant promise.

As a fundamental operation, Translution can be employed beyond the ViT and GPT architectures.
More effective and efficient architectures for Translution merit further exploration in future.

REFERENCES

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Conference on
Neural Information Processing Systems (NeurIPS), 2020.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a fixed-length context. In Conference of
the Association for Computational Linguistics (ACL), pp. 2978-2988, 2019.

Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. In Conference on Neural Information Processing Systems (NeurIPS),
pp. 3965-3977, 2021.

Stéphane d’ Ascoli, Hugo Touvron, Matthew L. Leavitt, Ari S. Morcos, Giulio Biroli, and Levent
Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In In-
ternational Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine
Learning Research, pp. 2286-2296, 2021.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 248-255, 2009.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
deep bidirectional transformers for language understanding. In Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 4171-4186, 2019.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representations (ICLR), 2021.

Hehe Fan and Yi Yang. Pointrnn: Point recurrent neural network for moving point cloud processing.
arXiv, 1910.08287, 2019.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:
An 800gb dataset of diverse text for language modeling. arXiv, 2101.00027, 2020.

Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented
transformer for speech recognition. In Interspeech, pp. 5036-5040, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778,
2016.

Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam
Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music
transformer: Generating music with long-term structure. In International Conference on Learning
Representations (ICLR), 2019.

10


Technical report

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In Conference on Neural Information Processing Systems (NeurIPS),
pp. 1106-1114, 2012.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE, 86(11):2278—2324, 1998. doi: 10.1109/5.726791.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows. In JEEE International
Conference on Computer Vision (ICCV), pp. 9992-10002, 2021.

Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon
Shlens. Stand-alone self-attention in vision models. In Conference on Neural Information Pro-
cessing Systems (NeurIPS), pp. 68-80, 2019.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. Technical report, OpenAI, 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res., 21:140:1—140:67, 2020.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-
tions. In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 464-468, 2018.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2015.

Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.
Bottleneck transformers for visual recognition. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 16519-16529, 2021.

Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video rep-
resentations using Istms. In International Conference on Machine Learning (ICML), volume 37,
pp. 843-852, 2015.

Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi:
10.1016/J.NEUCOM.2023.127063.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, 2015.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
International Conference on Machine Learning (ICML), volume 139, pp. 10347-10357, 2021.

Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: An unified understanding for transformer’s attention
via the lens of kernel. In Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 4343-4352, 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Ilia Polosukhin. Attention is all you need. In Conference on Neural In-
formation Processing Systems (NeurIPS), pp. 5998-6008, 2017.

11


Technical report

Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake A. Hechtman, and
Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12894-12904, 2021.

Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating con-
volution designs into visual transformers. In JEEE International Conference on Computer Vision
(ICCV), pp. 559-568, 2021.

12


Technical report

A DEFAULT NOTATION

a,A A scalar a A vector
A A matrix A A tensor
x Scalar multiplication . Matrix multiplication

B GENERAL TRANSLUTION

The calculation of the query, key and value in Translution, i.e., Eq. (Ip. assumes that element po-
sitions (e.g., image patches or textual words) are discrete. In this setting, it is feasible to assign a
different set of parameters for each direction and distance. However, if the positions are continuous
variables, e.g., in point clouds, it becomes impractical to assign individual weights for each direction
and distance, as there are infinitely many possible variations in continuous space. In this case, it may
be necessary to design new functions for the relative encoding.

Suppose p; denotes the position of the 7-th element. For language, p; can represent the index of the
i-th word in the text. For images, p; corresponds to the row and column indices of the 7-th patch. For
point clouds, p; refers to the 3D coordinates of the z-th point. A more general version of Translution
can be formulated as follows,

N
General Translution: f/ = So a(pi — pj, fi, f;,) x v(pi — vj, f;),
721

where a € [0, 1] denotes the attention score measuring the relevance of the j-th element to the i-th
element, and v : R¢+C — RC’ is a function that encodes relative positional information into the
element features (d denotes the dimensionality of the position, C' is the number of input feature
channels, and C’ is the number of output feature channels). When applying Translution to a new
type of data, the key is to develop effective a and v functions.

C 1D TRANSLUTION FOR NATURAL LANGUAGE PROCESSING

In the main text, we demonstrate how to apply Translution for image modeling. That Translution can
be viewed as a 2D operation because the relative encoding involves two spatial directions. However,
in natural language, relative encoding operates along a single dimension, which makes Translution
a one-dimensional model when applied to text.

Suppose f; € R!*° denotes the embedding (or representation) of the i-th token within a text se-
quence of length N, where C represents the embedding dimension. As shown in Figure |5} 1D
Translution is designed to integrate adaptive identification of related tokens with relative structural
encoding for language modeling. Specifically, Translution retains the self-attention mechanism of
the Transformer but employs distinct parameters for computing the Query, Key and Value represen-
tations, as follows,

relative query encoding: qi; = fi:Ws, 6 =i- 9,
relative key encoding: kj = fj; - Wws,,

T .;
. . dij * ki etd
relative attention: a;,; = ee) Hj = SH
1D Translution VC ye et
relative value encoding: v;,; = f; - W;’,
N
weighted sum: fj) = s Qj X Vig;
j=l

where W/W /W? € R°*C" denotes the learnable parameters for displacement 6.

13


Technical report

at
output F’ € R*¢

attention matrix value tensor
AE RNXNx1 Ve RNxNxc!
= ——— mul |
=fi-Ws

ree

query tensor : = T key tensor
Q E IRVxNxc! matmul: Qj, j > 4i.j ’ kj Ke IRVxNxc!
k =i1-jJ
j=fi Wi kji = fi-W-s le a

q q q K i v
CASS CESS GSCOSCee

input F € RN*¢

Figure 5: When modeling text, Translution operates in a 1D setting. For a sequence of
length N, it employs separate parameters for each positional offset (considering both direction
and distance), ic, {W"(y_4),°7° ,Wo,-+, Wai} {Wey ay Wo, Wa} and

{We y_ayst+ »Wo,-+> , Wx_1}, to encode relative language structure.

Causal 1D Translution

For autoregressive tasks, such as language modeling in GPT, a causal variant is typically required to
ensure future tokens remain unseen during inference. In causal 1D Translution, each token attends
only to itself and preceding tokens, guaranteeing that predictions rely exclusively on past context,
as follows,

relative query mneoing Gj = fi: Ws, 5 =i-3J,

relative key encoding: =f;- W*;,
T
relative attention: a;,; = —~—",
i lou

Causal 1D Translution ¢ causal attention: aj; = { :

. Qj =
—oo, otherwise, wd yo etn
rw L

7 =i-j>
relative value encoding: v;,; = { Si Ws, te 0,
b) r

weighted sum: fj = S O45 X 5,5.

As shown in Figure 6] compared to the original variant, causal 1D Translution reduces by half the
number of parameters needed to compute the query, key and value representations.

14


Technical report

at

output F’ € RV*¢

attention matrix =n value tensor

AE€ RY*Nx1 L) Ve IRNxNxc!

4
a
aa

vi; = fi Ws

query tensor : key tensor
Q € RNxNxc’ matmul: aj; = K € RNxNxc'
dij =fi'W kji = fi W" 6=i-j

ws)| w

input F € RY*¢

Figure 6: Illustration of causal 1D Translution. For a sequence of length NV, it employs N parameter
matrices to encode relative language structure. Compared to the original 1D Translution, the causal
variant reduces the number of parameters required to compute Query, key and Value by half.

D MEMORY-EFFICIENT IMPLEMENTATION OF a-TRANSLUTION:
OPTIMIZING RUNTIME MEMORY USAGE

Recall that a-Transformer is defined as follows,
N
a-Translution: fj = So ais x fj-(W°+w"'. Ws. oy” Ww),
j=l
where WY € ROX”, Wet © ROXO" We 5 e ROXC, We? E ROXC’, and C! < C,

C? < C’. Although this variant significantly reduces the number of parameters, it still demands
considerable runtime memory. Specifically, as shown in Figure |3| the resulting value tensor of

Translution is V € RN*N*C', which is considerably larger than the Transformer’s value matrix
V € RC. To address this issue, we implement a-Translution as follows,

N N
fi= So aig x f)-W? + (Sain x fj (Ww W3..s,)) we
j=l

ai

This reformulation reduces the peak runtime memory usage from N x N x C’ to N x C’+ N x
N x C?, where C? < C’, thus significantly alleviating memory demands during computation.

15


Technical report

E COMPARISON WITH EXISTING POSITION MODELING METHODS

Existing methods typically encode positional information by introducing additional positional biases
(either scalars or vectors). In this paper, inspired by convolution, we propose an alternative approach
that employs offset-based matrices for relative encoding. In this section, we provide a detailed
comparison between these approaches. Suppose 2; € IR!*° represents the feature or representation
of the i-th patch, located at (;, y;) in an image composed of N = H x W patches.

1. Baseline (Self-attention w/o Positional Embedding)

We consider the self-attention without position embedding as the baseline, formulated as follows:
w/o input position embedding: f; = xj,
query encoding: q; = fi: Wa,
key encoding: k; = f;- We,

i ky ets

n=

attention: aj; =
value encoding: uv; = fj: Wz,
N

weighted sum: f/ = So aij X Uj.

j=l
2. Transformer (Self-attention with Positional Embedding)

Most Transformers, including the original Transformer 2017), employ position em-

bedding to incorporate positional information. Specifically, they integrate absolute positions into
element representations, formulated as follows:

w/ input position embedding: f; = x; + Embed(x;, y;),

query encoding: q; = fi: Wa,

key encoding: k; = f;- We,

en 7 a
value encoding: vu; = fj - Wb,
N

weighted sum: fj = So aij xy

r=1

attention:

3. Relative Key Vector

(2018) enhanced Transformer for language modeling by adding learnable relative po-
sitional vectors into the key computations. BoTNet 2021) and HaloNet

extended this approach to two dimensions for image processing by adding learnable
relative positional vectors into the key computation. This can be formulated as follows,

w/o input position embedding: f; = xi,

query encoding: q; = fi: Wa,

key encoding: kj; = f;-W + 15,.5,,
qi- kj ets

n=

attention:
value encoding: vu; = fj - Wb,

N
weighted sum: f; = S Cig XV;
j=l

v
where 75,5, € RIX©.

16


Technical report

4. Relative Value Vector

(2018) also extended the above relative vector method to the value computations, as
follows:
w/o input position embedding: f; = xi,
query encoding: q; = fi: Wa,
key encoding: k; = f;- Wk,
qi- ke eM
aj =

vor’ > ern ,

=

self—attention: ai; =
value encoding: v; = f;-W, + 75,.5,;

N
weighted sum: f/ = So ais K ty,
j=l
5. Relative Positional Scalar

Swin Transformer 2021) and CoAtNet 2021) incorporate a learnable relative

positional bias (a scalar) into the attention score. In these methods, the original self-attention can
be regarded as content attention, which measures relationships from the token-feature perspective,
while the additional relative positional bias can be regarded as position attention, which measures
relationships from the token-position perspective. Formally, this can be expressed as follows:

w/o input position embedding: f; = xi,

query encoding: qi = fi: Wa,

key encoding: k; = f;- We,

T a

. qi: kj eri
attention: aigj= Vor ae bs, Oy aj = N a,’
Di,

value encoding: v; = f;- Wz,
N
weighted sum: fj = y Qj X U5,

g=l
where bs, ,6, € R. ConViT (d’Ascoli et al.| |2021) introduces Gated Positional Self-Attention

(GPSA), a variant of self-attention that incorporates a positional inductive bias. Moreover, a learn-
able gating parameter in each attention head controls the balance between positional and content-
based attention, as follows,

w/o input position embedding: f; = xi,

query encoding: qi = fi: Wg,

key encoding: k; = f;- We,

T -
. qi . k- etd
patch attention: aj; = —_i “Gj = =>
: JC’ > eti.n
n=1
b
wpe . e’ts
position attention: 6; ; = w- Ti5|> as = Ww _
ps | €
é
gated attention: gg = (1 = o(A)) x oT a a(A) x Beas fey = =

value encoding: v; = f;-W.,
N

weighted sum: f/ = pee x U;,
j=l

where w is a trainable vector for embedding, 7/51) is the relative positional encoding, A is a learnable
gate and a is the Sigmoid function.

17


Technical report

6. Rotary Position Embedding

Unlike the above vector- and scalar-based methods, RoFormer proposes a rotation-
based positional encoding method that is applied directly to queries and keys. As a result, attention
scores depend solely on relative distances, eliminating the need to explicitly store a positional vector
or scalar, as follows,

w/o input position embedding: f; = xi,
query encoding: qi = fi: Wg,
key encoding: k,; = f;- We,

attention: g/, k’

; = rotary(qi, kj), a7 =

value encoding: vu; = f;- Wb»,

N
weighted sum: fj = y Qj X Vj,
j=l

where rotary(-) is a rotary position embedding function.
7. Relative Positional Matrix (Translution)

Inspired by convolution, we propose Translution that performs matrix multiplication to produce a
vector output that encodes displacement or offset information, defined as follows:

w/o input position embedding: f; = xi,

relative query encoding: q;,; = fi - Wi.,.6,°
relative key encoding: kj; = f;- Wes. s3

Gig Ky; a. = eri

relative value encoding: wij = fj-W5,5,,

relative attention: aj; =

N
weighted sum: f/ = y Qj X Ui,7-

al

Table [8]provides a summary of various positional encoding strategies.

Table 8: Summary of different position encoding strategies.

Method
w/o Pos Emb f, = 2; Baseline
w/ Pos Emb fi, = #; + Embed(a;, y;) | Transformer (Vaswani et al.|/2017) _
Relative Positional Vector Key ! an
Value
Relative Positional Scalar w/o gating
w/ gating c tal.|[2
Rotary Position Embedding RoFormer J
Relative Positional Matrix a-Trans ution
Translution

18


Technical report

F TRANSLUTION WITH INPUT POSITIONAL EMBEDDING

In this section, we examine whether incorporating the input positional embedding method from
Transformer can further improve Translution. To this end, we implement Translution as follows:

w/ input position embedding: f; = a; + Embed(x;, y;),

relative query encoding: qji,; = fi: W.

0% 04?
relative key encoding: kj = f;- Ws; =
T .
relative attention: a;; = G5 Fy 47 = a
: 7 4,9 —~ pa tJ SAN og?
C met 8 ;

relative value encoding: v;,; = fj-W5_5,,

N
weighted sum: f; = y Ay X Vj,5-

j=1

As shown in Table) incorporating the Transformer’s absolute positional embedding does not yield
a clear performance gain for Translution in the static-to-static setting, leads to a slight drop in the
dynamic-to-dynamic setting, and results in a substantial drop in the static-to-dynamic setting.

Table 9: Accuracy (%) of Translution w/o and w/ the absolute positional embedding method from
Transformer. Results are reported on Static and Dynamic MNIST with ViT-A/12.

Method Embed(a;, yi) | #Parameters | StaticStatic | Dynamic+Dynamic | Static+Dynamic
acTausivden x 4.6M 98.48 97.31 34.90
v 4.6M 98.72 96.81 17.20
Translution x 116.2M 98.60 97.35 36.24
v 116.2M 98.47 96.31 16.50

G_ IMPACT OF W!, W* AND W” ON a-TRANSLUTION

Recall that: To reduce the number of parameters, we propose a-Translution, which decreases
both the input dimension C* and the output dimension C? of each Wi, , WE 5,7 and Ws 5y*

However, setting C1 and C? too small can overly compress the query, key, and value representations,
thereby degrading performance. To address this issue, we integrate the W%, W*, and W” of
Transformer into a-Translution to better preserve essential information.

In this section, we analyze the impact of W7, W*, and W” by systematically removing them from
Eq. (2) as follows:
query encoding: qij = fi; Wi Ws 5, gi=dr- i
key encoding: k;;=f;-Wy- Ws 5 me ,
ae soe i Qig
self—attention: a;.; = ij * i ake aj = _ oO
oJ JC J

’ N ’
_
wen=1 Goer

value encoding: ;,5 = fj -(Wy’- W5) 5, - Wy +W"),

a—Translution
N

weighted sum: f/ = ) ig X Vij.
j=l

As shown in Table[10] incorporating W7, W*, and W” significantly enhances the performance of
a-Translution, particularly when C! and C? are small. As C1 and C? grow larger, the improvement
decreases because the information is no longer overly compressed. In this case, W%, W", and W”
become less critical.

19


Technical report

Table 10: Impact of W%, W* and W” on a-Transformer. Results are reported on ImageNet-1K
with ViT-A/56, trained from scratch (no external pretraining) using a batch size of 256.

Relative Encoding Dimension Ww?,w*,w? #Parameters Top-1 Top-5
C=C =D v 4.68 M 42.49 67.39
C=C =8 7 S33M_— | 4836 | 7531

H RELATIVE CLS TOKEN

For classification tasks, besides the image tokens, there is an additional CLS token (classification to-
ken) that serves as a global representation of the input image. Usually, the CLS token is a learnable
embedding appended at the beginning of the input token sequence fed into Transformer. To apply
the strategy of relative encoding to the CLS token, we introduce additional parameters: Wwé LS-in?
Weis: Wéers ou Wers ine Wes: WErs our ad Wer ins Weis» Wexrs outs Correspond-
ing to the query, key, and value, respectively.

Figure 7: Illustration of relative encoding for the CLS token. For CLS, there are three encoding
directions: in, in-place, and out. Correspondingly, three sets of weights, i.e., Woxs.in, Wors, and
Wes out, are introduced for relative encoding in each respective direction.

As shown in Figure [7] Wes in is utilized when gathering information from the image tokens to
update the CLS token; Wo_¢ is applied when updating the CLS token based on its own information;
and Wcezs_out is employed when image tokens gather information from the CLS token to update
themselves.

20
