1809.01448v1 [cs.CL] 5 Sep 2018

1V

arX

Appendix - Recommended Statistical Significance Tests for
NLP ‘Tasks

Rotem Dror Roi Reichart
Faculty of Industrial Engineering and Management, Technion, IIT
{rtmdrr@campus|roiri}.technion.ac.il

This document is an annex to the paper "The Hitchhiker’s Guide to Testing Statistical Significance
in Natural Language Processing".Rotem Dror, Gili Baumer, Segev Shlomov and Roi Reichart (ACL
2018). Please cite:
@inproceedings{dror2018hitchhiker,
title={The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing},
author={Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},

booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) },

volume={1},

pages= {1383-1392},

year={2018}

Abstract

Statistical significance testing plays an important role when drawing conclusions from
experimental results in NLP papers. Particularly, it is a valuable tool when one would like to
establish the superiority of one algorithm over another. This appendix complements the guide
for testing statistical significance in NLP presented in [5] by proposing valid statistical tests
for the common tasks and evaluation measures in the field.

Statistical Significance Test Table

For each evaluation measure, the following table presents valid statistical tests’ and explana-
tions about the assumptions made when using each test. Our recommendations are based on
the considerations discussed in [5]. We will be happy to get feedback about their validity in
case the reader does not found our considerations to hold.

Notice that for each measure we present both parametric and non-parametric tests that
can be used under certain assumptions. The parametric tests discussed here assume that the
data is normally distributed. This assumption is likely to hold when using evaluation measures
that calculate an average of counts of correct classifications. When the normality assumption
that accompanies these tests holds in practice, they have higher statistical power than their
non-parametric counterparts proposed in the table, hence it is recommended to use them.
Otherwise, one should use non-parametric tests that do not make any such assumptions (we
marked cases where it is unlikely to assume normality by — in the parametric test column).

For example, in the case of precision, recall and F-score, [14] described why the t-test can
only be used for the recall metric but not for the precision and F-score measures. For other
evaluation measures, one can test if the data is normally distributed by applying statistical
tests that check for normality (see [5] for more details). In this table, we only mention a
parametric test when we consider it likely to assume normality.

Additionally, when comparing the performance of two algorithms that are applied on the
same dataset, one should use the paired version of the statistical significance test (such as the
matched-pair t-test). An implementation of the paired versions of all statistical tests presented
here as well as of other tests can be found at https: //github.com/rtmdrr/testSignificanceNLP.

1A statistical test is called valid if it guarantees that the probability of making type one error is bounded by a
pre-defined constant. See [5] for a detailed explanation.


Evaluation Parametric Men- Exemplary Assump-
tions /

Description Parametric
Measure
Test Comments

Test Task
2 x 2 table“ which 1p Caen
Contingency presents the outcomes of Binary positive), fp
table/ an algorithm on a McNemar’s seat nk (false positive),
confusion sample of n data points: test lassificati fn (false
matrix classification negative), tn
(true negative)
Percentage of
Exact match predictions that match icteat bootstrap/ Question
any one of the ground permutation answering
truth answers exactly
Proportion of true
results (both true
Anonrsey positives and true t-test bootstrap/ Sequence
negatives) among the permutation labeling
total number of cases
examined?

bootstrap / Phrase-based

true positive 4 ootstrap .

Recall true positive+false negative t-test permutation (constituent)
parsing

1,2
1,2
1, 2; 6
7 hantstnap/ Phrase-based
Precision mic postive > P. (constituent) 6
positive+false positive permutation : ?
parsing
Fg =(14+ §*)- bootstra Semantic
F score p= (4 8) P/ . ; 6
(2 precision) --recall permutation parsing
a)
5 io
1, 2; 4
, 2,4

2
|
Measurement of how
well a probability Wilcoxon Language
Perplexity distribution or signed-rank : 5
probability model test Tap deling
predicts a sample®
Measure of rank
Spearman correlation between the 7-test bootstrap / Word 9
correlation ranking produced by permutation similarity
two algorithms ”
Measure of the linear
Pearson correlation between the 7-test bootstrap / Word 9
correlation results of two permutation similarity
algorithms®
UAS
Unlabeled attachment bootstrap / Dependency
(sentence- t-test . .
level} score [7| fie | permutation parsing Ea
2
2
2

LA
8 Labeled attachment bootstrap / Dependency
(sentence- t-test . . t
level) score [7| permutation parsing
bootstrap / Summariza-
permutation tion

bootstrap / Machine
permutation translation
bootst :

oors rap/ Paraphrasing 2,
permutation

bootstrap / Machine
permutation translation

*https://en.wikipedia. org/wiki/Confusion_matrix
Shttps://en.wikipedia.org/wiki/Accuracy_and_precision
4https://en.wikipedia.org/wiki/Precision_and_recall
Snttps://en.wikipedia.org/wiki/Precision_and_recall
Shttps://en.wikipedia.org/wiki/Perplexity

“https: //en.wikipedia. org/wiki/Spearman’,27s_rank_correlation_coefficient
Shttps://en.wikipedia. org/wiki/Pearson_correlation_coefficient


Image
description
generation

[13, 1, 10, 8] bootstrap/ Coreference
permutation resolution

bootstrap /
permutation

Krippendorf’s

alpha,
Cohen’s

kappa

Statistical measures of bootstrap/ Annotation

agreement [6, 4] permutation reliability

Mean reciprocal rank,
used for algorithms that
produce a list of
possible responses to a
sample of queries,
ordered by probability
of correctness?

Question
bootstrap / answering,
permutation information

retrieval

Assumptions and Comments

. t-test: assuming that an average count of correct classifications is normally distributed.

Bootstrap: assuming the dataset is representative of the whole population. I.e., the data
sample should be big enough to cover many use-cases and represent the domain in a
satisfactory manner.

Fisher transformation: F'(r) = 5 In

e For Spearman: 23F (r) approximately follows a standard normal distribution.

e For Pearson: the Fisher transformation approximately follows a normal distribution
with a mean of F'(p) and a standard deviation of wo where n is the sample size.
One can use this to apply z-test to test for significance.

UAS and LAS are actually accuracy measures.

5. Wilcoxon signed-rank test: since the number of possible predictions (vocabulary words)

a language model may make can be very large, the non-sampling non-parametric tests
are preferable.

See [14] for explanations.

7. All measures used for evaluating coreference resolution are functions of precision and

recall. Since parametric tests are not suitable in the case of precision, these tests are also
not valid for these measures.

References

[1]

[2]

[4]

Amit Bagga and Breck Baldwin. Algorithms for scoring coreference chains. In The first
international conference on language resources and evaluation workshop on linguistics
coreference, volume 1, pages 563-566. Granada, 1998.

Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with
improved correlation with human judgments. In Proceedings of the ACL workshop on in-
trinsic and extrinsic evaluation measures for machine translation and/or summarization,
pages 65-72, 2005.

David L Chen and William B Dolan. Collecting highly parallel data for paraphrase eval-
uation. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume 1, pages 190-200. Association for
Computational Linguistics, 2011.

Jacob Cohen. A coefficient of agreement for nominal scales. Educational and psychological
measurement, 20(1):37-46, 1960.

°nttps://en.wikipedia.org/wiki/Mean_reciprocal_rank


[5]

[9]

[10]

[11]

[12]

[13]

[14]

Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. The hitchhiker’s guide
to testing statistical significance in natural language processing. In Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1383-1392, 2018.

Klaus Krippendorff. Computing krippendorff’s alpha-reliability. 2011.

Sandra Kiibler, Ryan McDonald, and Joakim Nivre. Dependency parsing. Synthesis
Lectures on Human Language Technologies, 1(1):1-127, 2009.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. Joint
entity and event coreference resolution across documents. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pages 489-500. Association for Computational Linguistics,
2012.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summa-
rization Branches Out, 2004.

Xiaoqiang Luo. On coreference resolution performance metrics. In Proceedings of the
conference on human language technology and empirical methods in natural language pro-
cessing, pages 25-32. Association for Computational Linguistics, 2005.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for
automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on
association for computational linguistics, pages 311-318. Association for Computational
Linguistics, 2002.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based
image description evaluation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 4566-4575, 2015.

Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman.
A model-theoretic coreference scoring scheme. In Proceedings of the 6th conference on
Message understanding, pages 45-52. Association for Computational Linguistics, 1995.

Alexander Yeh. More accurate tests for the statistical significance of result differences. In
Proceedings of the 18th conference on Computational linguistics- Volume 2, pages 947-953.
Association for Computational Linguistics, 2000.
