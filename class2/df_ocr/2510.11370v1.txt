arXiv:2510.11370v1 [cs.CL] 13 Oct 2025

Xiaomi MiMo rat]

Stabilizing MoE Reinforcement Learning by Aligning
Training and Inference Routers

Wenhan Ma‘™* — Hailin Zhang* ~—_ Liang Zhao* ~—*Yifan Song**
Yudong Wang"* = Zhifang Sui’® —- Fuli Luo®

*State Key Laboratory of Multimedia Information Processing, School of
Computer Science, Peking University
*LLM-Core Xiaomi

Abstract

Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of
large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism
often introduces instability, even leading to catastrophic RL training collapse. We analyze the
training-inference consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions, the routing
framework can yield divergent expert selections across repeated forward passes. To address this
foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records
routing distributions from the inference engine and replays them during training. R3 significantly
reduces training-inference policy KL divergence and mitigates extreme discrepancies without
compromising training speed. Extensive experiments on various settings confirm that R3 succeeds
in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS.
We believe this work can offer a new solution for stabilizing RL in MoE models.

1 Introduction

Reinforcement learning (RL) has become a cornerstone in the post-training of large language

models (LLMs) (Guo et al.) |2025 2024} |Ouyang et al.|/2022). By leveraging large-scale

RL, LLMs acquire the advanced capabilities necessary to tackle complex problems, including

competition-level mathematics 2025) and practical code agent tasks (Luo et al.
2025a), through more profound and extended reasoning.

A critical challenge in LLM-based RL is balancing efficiency and stability, with the latter being
essential for reliable performance. Modern RL frameworks typically employ distinct engines for
inference and training phases (e.g., SGLang for rollout and Megatron
for training). This architectural separation can lead to divergent token probabilities,
potentially causing catastrophic RL collapse (2025). To mitigate this discrepancy, Yao)
incorporate importance sampling mechanism in policy updating, while ae

(2025) introduce specialized compute kernels to reduce nondeterminism during LLM Inference.
However, in practice, existing approaches do not fully resolve the intensified off-policy issue that
arises during RL training on Mixture-of-Experts (MoE) models.

In this work, we identify the routing distribution as a pivotal factor contributing to the instability

“Work done during internship at Xiaomi Corporation.
°Co-corresponding authors.


Qwen3-30B-A3B + R3

w/o R3
w/ R3

0.50 0.75, 1.00
e

Training Probability

0.25

0.00

0.00 0.25 0.50 0.75 1.00
Inference Probability

Avg Val Score

0.704
— w/o R3
>, 0.657 —— w/R3
A 0.60 4
Router

0.554

0.504

0.45 4

0.404
a JD \& input J) 0 50 100 150

Global Step

Figure 1 Left: Illustration of the Rollout Routing Replay (R3). Top right: Training and inference
discrepancies before and after applying R3. Bottom right: Reinforcement learning training
performance before and after applying R3.

of MoE RL. Within MoE models, the router dynamically selects and activates a subset of experts
for each input token. The varied routing decisions result in greater policy discrepancies between
training and inference in MoE models compared to their dense counterparts. Rather than resorting

to workarounds such as discarding data with large discrepancies (Zhao et al.||2025a), we propose

to tackle this instability by addressing its root cause: the routing distribution itself.

Specifically, we propose Rollout Routing Replay (R3), a simple yet effective method for stabilizing
RL training of MoE models. R3 works by capturing the routing distributions from the inference
engine during sequence generation and replaying them directly into the training engine. This
process significantly narrows the gap between training and inference, marked by a substantial
reduction in KL divergence of logits produced by the different engines. As a result, the num-
ber of tokens with significant probability discrepancies between the two phases is reduced by
approximately an order of magnitude.

On realistic RLVR tasks with MoE models, R3 demonstrates significant superiority in training
stability, and performance. Our method consistently shows marked improvements in both efficiency
and overall performance when compared against existing approaches designed to stabilize RL
training. Furthermore, its applicability to both on-policy and mini-batch style off-policy RL
scenarios underscores the robustness of our approach.

Our main contributions are as follows:

1. We systematically identify and analyze routing distribution discrepancies between training
and inference in MoE models, highlighting their role in training instability.

2. We proposed Rollout Routing Replay, which reuses inference-time routing distributions
inside the training engine to align routing behavior between training and inference.

3. We apply R3 in multiple RL settings (multi-/single-mini-step and Base/SFT models) for MoE
reinforcement learning and show that R3 outperforms GSPO and TIS in terms of stability
and overall performance.


2 Preliminaries

Notation We consider an autoregressive language model, parameterized by 0, represented as a
policy x that generates a response y from a query x € D. The likelihood of the sequence is given
by the factorization zg(y|x) = mn” , te(y:|xX, Yer), Where |y| is the sequence length. zinfer and train
denote the policy as it operates within the inference and training engines, respectively.

Proximal Policy Optimization (PPO) (Schulman et al.)|2017) is a cornerstone algorithm for

policy optimization in reinforcement learning. For a given query x, PPO updates the policy 29 by
maximizing:

ly|

1 . no, a
Tero (9) = Ex~D,y~minfer (ota) (12) i » min (w.(0)A, clip(w;(@), 1 — €,1 + e)A,) . (1)
t=1

The importance sampling ratio w;(0@) for token y; in sequence y is defined as:

_ Train(9) (Yelx, Yet)
w¢(0) = ——__————_.
Train (Bold) (Yelx, Yee)

The advantage A; for token y; is typically estimated by a separate value model, and ¢ is the clipping
range for the importance ratio. For brevity, we omit the KL regularization term.

A critical inconsistency arises from the common practice of using separate engines for rollout and
training, where data is sampled via an inference policy (7jnfe,) but the loss is computed using a
training policy (atrain), as shown in Equation|1| This policy mismatch drives training instability
in reinforcement learning, an issue that, within MoE models, we identify as stemming primarily
from router inconsistency. The proposed solution alleviates this fundamental issue, making it
broadly applicable, orthogonal to, and compatible with recent policy optimization frameworks

like GRPO (Shao et al.||2024), GSPO (Zheng et al.|/2025) and DAPO 2025).

3 Training-Inference Discrepancies

Training-inference discrepancies in RL frameworks frequently lead to unstable training and model
collapse. In this section, we demonstrate that this policy mismatch is significantly amplified in
MoE models, primarily stemming from inconsistent routing distributions Furthermore, we observe
that even multiple runs of the same training framework can produce divergent token probabilities,
further contributing to RL training instability.

3.1 Policy Discrepancies between Training and Inference in MoE Models
Here we employ an MoE model, Qwen3-30B-A3B (Yang et al. |2025), for our experiments to

analyze the policy discrepancy between the training and inference engines. First, we use the
SGLang inference engine to generate responses for 2,048 mathematics problems, saving the token
probabilities for each generated token. This process yields about 20 million response tokens. Then
these responses are passed through Megatron to obtain the corresponding probabilities assigned
by the training engine. We use several metrics to quantify the divergence between these two
probability distributions.

KL Divergence Estimation Let T be the set of all response tokens, we use the k3 method proposed
in (2020) to estimate the KL-divergence between the training and inference probability

3


Qwen3-30B-A3B Inference-Training Probability Qwen3-30B-A3B + R3 Inference-Training Probability Qwen3-8B Inference-Training Probability Difference Distribution Analysis

KL=0.001535 bid KL=0.000754

19 KL=0.000640 10? — Qwen3-8B
—— Qwen3-30B-A3B
—— Qwen3-30B-A3B + R3

Inference Probability Inference Probability

(a) MoE (b) MoE + R3 (c) Dense (d) Extreme Token Distri-
bution Analysis

Figure 2 (a): Illustration of the training-inference discrepancy in the MoE model. (b): Illustration
of the training-inference discrepancy in the MoE+R3 model. (c): Illustration of the training-
inference discrepancy in the Dense model. (d): Extreme Token Distribution Function, calculated
based on Equation |3}

distributions:

Train (9) (t) Train (9) (t)
Duc. (tisain(9)|linfer()) * Fr rie Oy 71 8s eyo! (2)

Our calculations show that the estimated KL divergence of Qwen3-30B-A3B (MoE) is 1.535 107°,
while that of Qwen3-8B (Dense baseline) is 6.4 x 10-4.

Visualization To visualize the training-inference discrepancies of the MoE model, we randomly
sample 10 million response tokens and plot a scatter diagram with SGLang probabilities on the
x-axis and Megatron probabilities on the y-axis. The degree of concentration around the y = x
line indicates the degree of consistency. As shown in Figure |2a| and compared to Qwen3-
8B, Qwen3-30B-A3B exhibits a much wider scatter band, revealing a larger training-inference
discrepancy.

Extreme Token Distribution Analysis To quantify the discrepancy between the model’s behavior

during training and inference, we introduce the Extreme Token Distribution Function, F(t), defined
as:
Ttrain(@)(t) infer(O)(t) |

F(t) = my tne ax( “22 , Sater) >t]. (3)

Tinfer(@) (t)’ Train (9) (t)

This function measures the proportion of extreme tokens—those for which the probability ratio
between the training distribution z;ain(@) and the inference distribution mjnfe;(@) surpasses a
threshold t. Figure[2d] plots this function F(t) against the threshold t. The plot reveals that for
Tt > 2, the Qwen3-30B-A3B model exhibits a fraction of extreme tokens an order of magnitude
larger than that of the Qwen3-8B model. This significant gap indicates a substantially higher
degree of training-inference variability in the MoE model.

3.2 Routing Discrepancies between Training and Inference in MoE Models

From the perspective of functional continuity, the key difference between MoE and Dense models
lies in the non-continuity introduced by routing. In MoE models, small perturbations in the router
input can lead to entirely different experts being selected, causing large changes in layer outputs.
Dense models, lacking an explicit expert selection process, do not exhibit this phenomenon.

4


Router Diff Frequency (Router Level) Router Diff Frequency (Token Level) Router Diff Frequency (Sequence Level)

1071
10-2 10-?
<< 1053
fo

FS
g 10+

Frequency

o
10-3 = 10>
10-6

10-7

1
0 1 2 3 4 5 6 7 8 to) 50 100 150 200 250 300 350 400 4 6 8 10 12
MoE router diff count(per token per layer) MoE router diff count(per token) MoE router diff count(avg per token in seq)

(a) Router-level Difference (b) Token-level Difference (c) Sequence-level Differ-
ence

Figure 3 Router discrepancy analysis

Based on this, we further analyze router distribution discrepancies between training and inference
in MoE models. We use SGLang (Zheng et al.|!2024) to generate responses for 2048 mathematical
problems with Suecn 308 Ao fee ast DOR For each response, we collect the routing
distribution of all tokens (including input tokens) from the inference engine. Then, we feed these
sequences into the Megatron engine for forward propagation. This process yields the routing

distribution observed from training engine. We compare these two sets of routing information at
different levels:

Router-level Comparison: For each token and each MoE layer, we count the number of differing
expert choices made by the MoE Router and calculate the frequency of these differences. Figure
[3a] illustrates the results. It is observed that approximately 10% of the routers select different
experts during training compared to inference.

Token-level Comparison: For each token, we count the number of differing expert choices made
by the MoE Router across all layers. We also determine the frequency of these occurrences. Figure
presents these findings. It shows that 94% of tokens select a different expert in at least one
layer during the forward pass.

Sequence-level Comparison: For a sequence, we compute the

router distribution difference of each token, then average over
: : v0 KL=0.000842 dt

tokens to obtain the mean difference per sequence and plot a 0s i

histogram [3c] Results show that the mean difference per token a

is approximately 6 routers.

Qwen3-30B-A3B Training1-Training2 Probability

°
a

in

These findings demonstrate that during training and inference,
MoE models exhibit router distribution discrepancies. In [4] we
will show empirically that routing discrepancies isthe main con- _.,
tributor to the additional training-inference discrepancies of MoE 04
compared to Dense models.

°°

Training2 Probability
BS

os 6s 6 6 so
Training Probability

3.3. Variation in Repeated Forward Passes of the Same Figure 4 Probabilities ob-
Sequence within the Same Framework tained by performing forward
propagation twice using the

We conduct two forward passes of the same sequence set under Megatron

the Megatron framework and obtain two probability distributions.
Following the procedure in Section [3.1] we compute the KL di-
vergence between these distributions and plot the results (scatter
plot shown in Figure|4| KL divergence = 8.4 x 10‘-#),

The results show that, for MoE models in the Megatron engine, even when the input sequence is
identical, the final output probabilities from two forward passes may differ. In a reinforcement


learning setting, this variation adds noise to the computation of the old policy train (@o1q). Such
noise makes the importance sampling ratio unreliable, which can destabilize and even break the
reinforcement learning process.

4 Rollout Routing Replay

This section provides a detailed description of the implementation of Rollout Routing Replay (R3),
its caching support in multi-turn dialogue, and an analysis of its effects on training-inference
discrepancies.

4.1 Implementation

We first describe the conventional forward pass of a MoE layer within a training framework.
Consider a sequence s at the t-th token and the MoE layer in the /-th Transformer block. Let the
input to this layer during training be Xtrain. The router logits are calculated by:

Strain = XtrainW,, (4)

where W,. denotes the router’s linear weight matrix. Let the number of experts be M and the
number of experts to select be K. During training, the router selects the top-K experts based on
the logits. This is represented by a binary mask:

Tain = TopKMask (Strain; kK), (5)

where Iain € {0,1}” and >); Itrainy = K. The gating weights are then produced by applying a
softmax over the logits of the selected experts:

I train,i exp (Straini)

Ee fori=d,...,M. (6)
dj=1 Train, j exp (Strain, j)

Strainji =

Finally, the MoE layer’s output is computed as a weighted sum of the expert outputs:

M
Ytrain = SY) Strain’ &i(Xtrain); (7)
iel

where &;(-) represents the i-th expert network.

Now we introduce the Rollout Routing Replay. Assume that during the inference stage, the input
to the MoE layer is Xjnfer. The router then computes the inference logits Sinfer = XinferW;, from
which the routing mask Ijnfe, = TopKMask(Sinfer, K) is obtained. The main idea of Rollout Routing
Replay is to reuse the inference routing mask Ijpfe, during the training forward pass while still
applying the softmax to the training logits to preserve gradient flow. Specifically, along this replay
path, the "replay" gating weights g;eplay are computed as:

Tinfer,i €XP (Strain,i)

—antens Sin — fori=l,...,M. (8)
dj Tinfer,j exp (Strain, j)

Sreplay,i =
These replay weights are then used to combine the training experts’ outputs, producing the replay
output Yreplay:

M
Yreplay = > Beep &j(Xtrain)- (9)
i=1


This design serves two main purposes: (a) Aligning training and inference: Using Ijn-, ensures
that the experts used during training replay match those selected during inference, eliminating
mismatch in expert selection. (b) Preserving the gradient data flow: By replaying only the
mask, the gradients can still flow back to the logits without interfering with the computation
graph, which helps to optimize the router effectively.

4.2 Router Mask Caching and Multi-Turn Dialogue Support
Many inference engines use a KVCache prefix caching strategy 2023; |Zheng et al.

to prevent redundant prefill computations on previously seen context, which significantly
reduces the total computation in multiple-turn interactions. We observe that the cached router
masks share a similar property: for the same prefix tokens, the MoE router should yield identical
results. Therefore, routing masks from inference engines Ijnfer can be cached along with the
prefix KVcache. Concretely, for each layer and token prefix, the corresponding routing masks
are stored with KVCache. When the same prefix occurs and hits the cache, the masks can be
reused, eliminating the need for recomputation. This allows Rollout Routing Replay to integrate
seamlessly with prefix caching mechanisms.

Caching routing masks is especially beneficial in agent scenarios. Many agent tasks, like software
engineering and web browsing (2025), involve multiple turns
of interactions between autoregressive generation and tool calling. To improve efficiency, these
processes directly reuse the KVCache from previous turns, so they do not have to regenerate data
that’s already been computed. Routing mask caching enables R3 to remain efficient in RL agent
tasks without re-prefilling to generate the routing masks, which is crucial for training large-scale,
advanced MoE models.

4.3 Empirical Analysis of R3 on Training-Inference Discrepancies

To evaluate the effectiveness of R3 in reducing training-inference discrepancies, we repeat the
procedure described in Section 3.1} with Qwen3-30B-A3B model. In this process, we cache the
routing distributions obtained during inference on SGLang and replay them within the Megatron
framework. After obtaining the inference engine probability and training engine probability, using
the equation [2| we estimate the KL divergence between training and inference. Results shows that
after applying R3, the KL divergence between training and inference decrease from 1.5 x 107° to
7.5 x 10~*, which is near the 6.4 x 10~* observed for the dense model. This intuitively indicates a
reduction in the training-inference discrepancy. We also drew the cumulative distribution plot of
the ratio of training-inference discrepancies in Fig. [2d] with R3. The plot indicates that, for the
MoE model, applying R3 reduces by an order of magnitude the frequency of tokens with large
training-inference discrepancies.

5 Experiments

In this section, we evaluate the performance improvement of our R3 method for reinforcement
learning and compare it with other baseline methods.

5.1 Setting

Task and Dataset We choose mathematical reasoning as the target task for training. For the
training dataset, we collect and filter problems from many open-source datasets, including

BigMath (Albalak et al.| |2025), ORZ 2025), and others, yielding approximately

7


Best Metric(Best Global Step)

Method AIME24(T) AIME25(7) =AMC23(7) MATHS500 Lv5(T) Avg(T) Crash Step
Qwen3-30B-A3B-SFT, mini_step=8, max_global_step=180

GRPO 32.81(65) — 20.73(90) = 74.8460) 71.83(100) 48.84(100) 120

GSPO 55.52(165)  38.23(160) 90.16(125) 86.38(125) 66.76(125) -

GRPO+R3 57.92(180) 38.02(155) 90.16(155) 88.62(170) 68.05(180)

GSPO+R3 58.44(160) 39.17(160) 92.50(165) 87.87(165) 69.00(165)
Qwen3-30B-A3B-SFT, mini_step=1, max_global_step=180

GRPO 49.06(45)  32.08(50)  86.41(55) 83.77(55) 62.23(55) 60

GRPO+TIS 54.90(85)  36.67(90) + ~— 88.59(90) 85.63(85) 66.24(90) 105

GRPO+R3 62.92(165) 41.98(170) 93.91(165) 89.93(155) 71.83(165)

GRPO+TIS+R3 58.75(180) 41.15(180) 91.87(175) 88.99(180) 70.14(175)
Qwen3-30B-A3B-Base, mini_step=1, max_global_step=180

GRPO 50.63(100) 32.60(100) 83.13(100) 80.78(90) 61.69(100) 105
GRPO+TIS 56.77(170) 40.10(170) 92.50(165) 89.37(180) 69.22(170) -
GRPO+R3 60.94(180) 41.35(180) 92.81(175) 89.74(175) 70.73(180)

GRPO+TIS+R3 56.67(170) 40.42(170) 93.12(175) 88.99(165) 69.17(175)

Table 1 Main evaluation results. This table presents the best scores (with corresponding global
step in parentheses) achieved by various methods on evaluation benchmarks under three training
configurations . The table also reports the average score (Avg) and the step at which training
collapse occurred (Crash Step).

100,000 verifiable math problems. For the evaluation dataset, we adopt AIME24, AIME25, AMC23,
and MATHS00 (level 5) as our benchmark datasets. We report Avg@32 for AIME24 and AIME25,
Avg@16 for AMC23, and Avg@4 for MATH500 (level 5). During training, some models may
experience performance degradation or even collapse in later stages. To ensure a fair evaluation,
we measure model performance every 5 global steps throughout a single training run. We then
report the highest observed performance along with the corresponding training step at which it
occurs.

Models We select two models for our experiments: (a) Qwen3-30B-A3B-Base (Yang et al.
2025) (b) Qwen3-30B-A3B-SFT, which is fine-tuned from Qwen3-30B-A3B-Base on our general
instruction-following dataset.

Baseline Methods We consider the following baseline optimization methods for comparison: (a)

GRPO (Shao et al.||2024), additionally applies the Clip Higher technique from DAPO

2025), with parameters €jow = 0.2 and énigh = 0.27; (b) TIS (Yao et al.||2025), uses an upper
clipping threshold C = 2; (C) GSPO (Zheng et al.}|2025), employs sequence-level importance

sampling, with parameters €ow = 3x 10™* and ehigh = 4X 10-*. Since our R3 method is orthogonal
to optimizers such as GSPO or TIS, we also evaluate various combinations of these techniques.

Multiple Mini Steps vs. Single Mini Step In PPO-like algorithms, a batch of samples collected
in one global step is typically divided into multiple mini steps so that the policy can be updated
several times. However, previous research has shown that applying a strictly on-policy strategy with
only a single mini step can yield better performance. We investigate both settings. For multiple
mini steps, we set the mini step count to 8, meaning each PPO mini step trains 2048/8 = 256
samples with 8 optimizer updates and a learning rate of 1 x 10~°. In the multiple mini-step
scenario, for R3 method we replayed the routing both when recomputing the old policy and
updating policy. For the single mini step scenario, we set the mini step count to 1, meaning all


Qwen3-30B-A3B-SFT Qwen3-30B-A3B-SFT Qwen3-30B-A3B-Base Qwen3-30B-A3B-Base

F(t=2) Training-Inference KL F(t=2) Training-Inference KL
107 10° 4 —_— isis + ERs 5 _— were Ne 107 +— Teas + wees
a — wiTis +wio -2 | — wis + wio — wiTis +wio
—— w/o TIS + w/ R3 10 7— w/o TIS + w/ R3 — w/o TIS + w/ R3
10-2 al — w/TIs + w/ R3 10-3 — w/TIS + w/ R3 — w/TIs + w/ R3
10-3 4 10-4 4
10-4 4 E 10-5
—— w/o TIS + w/o R3 4
= —— w/TIS + w/o R3 -6 |
10> 7 wots w/ R3 103 4 10 ee nant
— wiTIs + w/R3 ae i ee
1 7 T T T T T T 107 & T T T T T T T
0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150
Global Step Global Step Global Step Global Step

Figure 5 Analysis of training—inference collapse. The plot shows the estimated training—inference
KL divergence and the extreme token distribution function F(t = 2) (Eq.|3) at each training step.

Qwen3-30B-A3B-Base Qwen3-30B-A3B-Base Qwen3-30B-A3B-Base Qwen3-30B-A3B-Base
Response Length Grad Norm Entropy Avg Val Score
0.3 0.3
15000 + — wo Tis + w/o R3 — who TIS + wo R3 — who TIS + wo R3 0.7 +— wi TIS + w/o RB
—— w/TIS + w/o R3 —— wiTIS + w/o R3 —— w/TIS + w/o R3 —— w/TIS + w/o R3
— wio TIS + w/ R3 — w/o TIS + w/ R3 — w/o TIS + w/ R3 — w/o TIS + w/ R3
— w/TIS + w/ R3 — w/TIS + w/ R3 — w/TIs + w/ R3 — w/TIS + w/ R3
0.2 4
10000 4 0.6
0.1 4 0.5
5000 4
0.0 4 0.4
0 50 100 150 0 50 100 150 ie} 50 100 150 0 50 100 150
Global Step Global Step Global Step Global Step

Figure 6 Training dynamics of Qwen3-30B-A3B-Base, including response length, gradient norm,
entropy, and average validation score throughout the training process

2048 samples are updated once. Due to fewer updates, we increase the learning rate to 3 x 10~°.
We don’t recompute old policy in this scenario.

Other Setting We implement R3 using the VeRL (Sheng et al.| |2024) framework, use Mega-
tron for training, and selene eet oe for inference. We set
the batch size to 256 and n = 8, totaling 2048 samples per round. The maximum prompt length
is 2048, and the maximum generation length is 30720. We adopt the Dynamic Sampling strategy

from|Yu et al.| (2025), retaining only partially correct samples during generation until a sufficient
batch size is accumulated for training. No auxiliary loss for expert balancing is introduced.

5.2 Experimental Results and Analysis

The main evaluation results are shown in Table [1] More detailed evaluation results and training
logs, including per-evaluation scores as well as training metrics such as entropy, average reward,
gradient norm, and training-inference KL divergence, are provided in the appendix|A] and [|

Overall Performance. R3 achieves better results across different scenarios. In the multi mini-step
setting, GRPO+R3 outperforms GSPO by 1.29. Furthermore, combining R3 with GSPO further
improves performance by 0.95 points. In the single mini-step setting, R3 outperforms TIS by 5.58
on SFT model and 1.51 on base model. However, combining R3 with TIS does not yield clear
gains and may even degrade performance; for instance, in the single mini-step on SFT model,
TIS+R3 scores 1.69 points lower than R3 alone. Since R3 already significantly reduces the policy
discrepancy between training and inference, the additional correction from TIS offers negligible

benefit (Yao et al.|/2025).

Training Stability. In the single mini-step setting, three reinforcement learning processes without
R3 collapsed during training. To investigate the causes of these collapses, we plotted the estimated
training—inference KL divergence and the extreme token distribution function F(t = 2) (eq. |3) at


each training step(see Fig. [5). We observed that, in each training process, both the KL divergence
and F(t = 2) values increased over training process. Moreover, collapsed training runs were
almost always accompanied by abnormally high KL and F(t = 2) values. For instance, in the SFT
model trained using GRPO under the single mini-step setting, after 60 global steps, the value
of F(t = 2) exceeded 0.1. This indicates that for 10% of the tokens, the probability under the
training framework differed from that under the inference framework by more than a factor of 2,
demonstrating severe training—inference inconsistency. In contrast, for the majority of the time
during training processes using R3, the value of F(t = 2) remained below 10~*. By aligning the
training and inference distributions, R3 effectively stabilized reinforcement learning for MoE
models.

Optimization and Generation Behavior. During training, R3 also enhances optimization stability,
exploration behavior, and generation dynamics. We plotted the sequence length, gradient norm,
generation entropy, and evaluation score throughout training for the single mini-step + base
model group (see Fig. (6). It shows that R3 has smaller gradient norms, a smoother sequence
growth pattern, and more stable entropy. (a) Sequence Length: With R3, the generated sequence
length rises rapidly at the beginning of training, indicating that R3 can quickly capture the correct
optimization direction. By contrast, the other two training processes increase only slowly after
step 80 and show more pronounced fluctuations. (b) Gradient Norm: R3 maintains consistently
lower gradient norms, indicating a more stable optimization process. (c) Generation Entropy:
With R3, entropy begins to increase steadily after about 25 step, suggesting that the model starts
exploring better strategies earlier. Without R3, entropy increases much later and fluctuates heavily.

6 Related Works

6.1 Mixture of Experts

The Mixture-of-Experts (MoE) architecture, originating as an ensemble method to route inputs to
specialized subnetworks (1994), has become a cornerstone
for scaling modern language models. By employing a gating network to sparsely activate only a
subset of expert parameters per token, MoE decouples a model’s total parameter count from its
inference cost, enabling a substantial increase in model capacity. This computational efficiency

has driven its adoption in state-of-the-art Transformer models (Jiang et al.||2024} 2024}
2025 2025). However, MoE models are susceptible to training instability
stemming from the sensitivity of the gating network (Dai et al.||2022 2025), which

renders router robustness a central challenge for effective model convergence.

6.2 Instability in Reinforcement Learning for LLMs

Reinforcement Learning with verifiable rewards (RLVR) has become a standard method for refining
the complex reasoning (Xie et al.|{2025), mathematical 2024), and
code abilities (Luo et al.|!2025b) of large language models. Algorithms such as GRPO (Shao et al.
and DAPO (Yu et al.,/2025) are now widely adopted. However, this training paradigm
also faces challenges with training instability, which can hinder convergence and risk model

collapse (Chen et al.|/2025 2025 2025). Recent research has identified
several root causes and proposed corresponding solutions. For instance, (2025b) aim
rately

to stabilize training by optimizing the geometric mean of token-level rewards. Sepa
(2025) posit that GRPO’s instability stems from a misapplication of importance sampling
weights and introduce a new sequence-level importance ratio to correct it.

10


propose Truncated Importance Sampling (TIS) to mitigate the inconsistency
stemming from discrepancies between training frameworks (e.g., FSDP (Zhao et al.||2023) and
inference engines (e.g., vLLM (Kwon et al.|/2023)). Alternatively, eae Tab] DOSS) ore
instability to non-determinism in compute kernels, proposing batch-invariant operations, though
this incurs significant performance overhead and has not been explored for RL on MoE models.
These stability challenges are critically exacerbated in Mixture-of-Experts (MoE) architectures.
We find this heightened instability is primarily caused by routing inconsistencies: the router’s
sensitivity, combined with the framework gap, causes expert assignments for the same response to
differ between the rollout and training phases. To directly address this core issue, our proposed
method, Rollout Routing Replay, explicitly records and replays the inference-time routing decisions
during the training pass. This simple strategy enforces consistency, enabling MoE models to
achieve training stability comparable to their dense counterparts without incurring computational
overhead.

7 Conclusion

In this work, we identify training-inference routing discrepancies as the primary source of in-
stability in MoE reinforcement learning. To address this, we propose Rollout Routing Replay
(R3), which reuses inference-time routing distributions during training to align expert selection
while preserving gradient flow. Experiments across multiple RL settings demonstrate that R3 sub-
stantially reduces training-inference divergence, stabilizes training, and consistently outperforms
existing methods. Our results demonstrate the importance of aligning training and inference in
MoE models and show that R3 provides a practical solution for improving stability.

References

A. Albalak, D. Phung, N. Lile, R. Rafailov, K. Gandhi, L. Castricato, A. Singh, C. Blagden, V. Xiang,
D. Mahan, and N. Haber. Big-math: A large-scale, high-quality math dataset for reinforcement

learning in language models, 2025. URL https://arxiv.org/abs/2502. 17387

A. Chen, A. Li, B. Gong, B. Jiang, B. Fei, B. Yang, B. Shan, C. Yu, C. Wang, C. Zhu, et al.
Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint
arXiv:2506.13585, 2025.

D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei. Stablemoe: Stable routing strategy
for mixture of experts. arXiv preprint arXiv:2204.08396, 2022.

D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al.
Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning. arXiv
preprint arXiv:2501.12948, 2025.

H. He and T. M. Lab. Defeating nondeterminism in Ilm inference. Thinking
Machines Lab: Connectionism, 2025. doi: 10.64434/tm1.20250910.
https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/.

J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum. Open-reasoner-zero: An open
source approach to scaling up reinforcement learning on the base model, 2025. URL/ht tps:
//arxiv.org/abs/2503. 24290

R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.
Neural computation, 3(1):79-87, 1991.

11


A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1.
Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
2024.

C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can

language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs
(2310. 06770.

M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
computation, 6(2):181-214, 1994.

W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.
Efficient memory management for large language model serving with pagedattention. In
Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.

A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.

M. Luo, N. Jain, J. Singh, S. Tan, A. Patel, Q. Wu, A. Ariyak, C. Cai, T. Venkat, S. Zhu, B. Athi-
waratkun, M. Roongta, C. Zhang, L. E. Li, R. A. Popa, K. Sen, and I. Stoica. Deepswe: Training

a state-of-the-art coding agent from scratch by scaling rl. https: //pretty-radio-b75.not
ion. site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Codin

g-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33, 2025a. Notion Blog.

M. Luo, S. Tan, R. Huang, A. Patel, A. Ariyak, Q. Wu, X. Shi, R. Xin, C. Cai, M. Weber, C. Zhang,
L. E. Li, R. A. Popa, and I. Stoica. Deepcoder: A fully open-source 14b coder at 03-mini level.

https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-1

4B-Coder-at-03-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025b. Notion
Blog.

OpenAI. Learning to reason with Ilms, 2024. URL https: //openai.com/index/learnin

g-to-reason-with-11lms/

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. Advances
in neural information processing systems, 35:27730-27744, 2022.

J. Schulman. Approximating kl divergence, 2020. URL http: //joschu.net/blog/kl-appro

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv: 1707.06347, 2017.

Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseek-
math: Pushing the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300, 2024.

G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: A
flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024.

M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm:
Training multi-billion parameter language models using model parallelism. arXiv preprint
arXiv:1909.08053, 2019.

12


K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5:
Scaling reinforcement learning with Ilms. arXiv preprint arXiv:2501.12599, 2025.

B. Thérien, C.-E. Joseph, Z. Sarwar, A. Panda, A. Das, S.-X. Zhang, S. Rawls, S. Sahu, E. Belilovsky,
and I. Rish. Continual pre-training of moes: How robust is your router? arXiv preprint
arXiv:2503.05029, 2025.

J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and
A. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents, 2025. URL

https://arxiv.org/abs/2504. 12516

B. Xia, B. Shen, Cici, D. Zhu, D. Zhang, G. Wang, H. Zhang, H. Liu, J. Xiao, J. Dong, L. Zhao,
P. Li, P. Wang, S. Yu, S. Chen, W. Wang, W. Ma, X. Deng, Y. Huang, Y. Song, Z. Jiang, B. Ye,
C. Cai, C. He, D. Zhang, D. Zhang, G. Wang, H. Tian, H. Zhao, H. Qu, H. Xu, J. Shi, K. Bao,
K. Fang, K. Zhou, K. Zhou, L. Li, M. Zhu, N. Chen, Q. Wang, S. Liu, S. Li, S. Gu, S. Ren,
S. Liu, S. Deng, W. Zhuang, W. Lv, W. Yang, X. Zhang, X. Yong, X. Zhang, X. Song, X. Xu,
X. Wang, Y. Yan, Y. Tu, Y. Tian, Y. Wang, Y. Yu, Z. Lin, Z. Song, and Z. Yue. Mimo: Unlocking
the reasoning potential of language model — from pretraining to posttraining, 2025. URL

https://arxiv.org/abs/2505 .07608

T. Xie, Z. Gao, Q. Ren, H. Luo, Y. Hong, B. Dai, J. Zhou, K. Qiu, Z. Wu, and C. Luo. Logic-rl: Unleash-
ing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768,
2025.

A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu,
F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang,
J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang,
P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang,
X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou,

and Z. Qiu. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505 .09388
F. Yao, L. Liu, D. Zhang, C. Dong, J. Shang, and J. Gao. Your efficient rl framework secretly brings

you off-policy rl training, Aug. 2025. URL https://fengyao notion. site/off-polic

Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al.
Dapo: An open-source Ilm reinforcement learning system at scale, 2025. URL https://arxiv.
org/abs/2503.14476, 2025.

X. Zhao, Y. Liu, K. Xu, J. Guo, Z. Wang, Y. Sun, X. Kong, Q. Cao, L. Jiang, Z. Wen, Z. Zhang, and
J. Zhou. Small leak can sink a great ship—boost rl training on moe with icepop!, Sep 2025a.

URL https: //ringtech.notion.site/icepop

Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer,
A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Mathews, and
S. Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https:
//arxiv.org/abs/2304. 11277

Y. Zhao, Y. Liu, J. Liu, J. Chen, X. Wu, Y. Hao, T. Lv, S. Huang, L. Cui, Q. Ye, et al. Geometric-mean
policy optimization. arXiv preprint arXiv:2507.20673, 2025b.

C. Zheng, S. Liu, M. Li, X.-H. Chen, B. Yu, C. Gao, K. Dang, Y. Liu, R. Men, A. Yang, J. Zhou, and

J. Lin. Group sequence policy optimization, 2025. URL|https://arxiv.org/abs/2507.1
8071)

13


L. Zheng, L. Yin, Z. Xie, C. Sun, J. Huang, C. H. Yu, S. Cao, C. Kozyrakis, I. Stoica, J. E. Gonzalez,
C. Barrett, and Y. Sheng. Sglang: Efficient execution of structured language model programs,

2024. URL https ://arxiv.org/abs/2312.07104

14


Appendix

A. Detail Evaluation Results

AIME24 AIME25 AMC23 MATHSO0 Iv5 Avg
0.6 0.40 0.90 0.70
rc 0.90
= 0.35 0.85 0.65 -
00 0.5
ae 0.85 6 —— GRPO + w/o R3
<3 0.30 oo 0.80 } 6.60 —— GSPO + w/o R3
Bel o4 , 055 — GRPO + w/R3
neo 0.25 0.75 :
ge 0.75 — GSPO + w/R3
FS 0.50
0.20 0.70 :
So 63 0.70 7,
0.15 0.65 0.65 ote
0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150
0.95
0.6 0.40 0.90 0.70
im
ta | 0.90 os
Da 0.5 o38 : 0.85 : O68 — w/oTIS + w/o R3
ae 030 0.80 0.60 i — w/TIS + w/o R3
oO 0.80 — w/oTlS + w/R3
mE 04 0.75 0.55 — w/TIS + w/ R3
: = 0.25 ows : :
o 0.20 | 0.70 0.50
a3 1 0.70 j
- 0.15 0.65 | 0.65 0.45 -
0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150
0.6 0.40 190 07
a 9:20 0.85
3 0.35 “4
B05 0.85
m i 0.30 0.80 0.6 — w/oTIS + w/o R3
mt :
23 0.80 — w/TIS + w/o R3
my 0.4 0.25 0.75
mal : ons — w/oTiS + w/R3
4 as 0.20 0.70 05 — w/TIS + w/ R3
§ . 0.70
0.15 0.65
- o2 0.65 0.4
. 0.10 0.60 ,
0.60
0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150
step step step step step
B Detail Training Metri
Response Length Reward Grad Norm Entropy F(t=2) Training-Inference KL PG Clip Fraction
17500 OBS: 8
15000 060 ee a eet ox Ne ant
23 r2s00 — aro + wo R3
1) + — GSP0 + wio R3
85, 10000 0.55 04 a9 ee One _ in a
BE 7500 oso lo oa tM. | — SPO + w/ Ra
3 0.2
2500 Os 0.00 00 Toe 0.0
050 100 150 o 50 100 150 o 50 100 150 o 50 100 150 0 50 100-150 o 50 100 150 050 100 150
0.50 1 08
i 10°
_ 2m ae _— ] 10 0.04
4 0.6 2
af 15000 | 0.100 10 ie 0.02 — w/o TIS + wio R3
33 os | weal arms + wo 3
8%. 10000 i 075) Oe I " 00 — w/o Tis + w/ R3
SE o 0.050 | 10+ w 0.02 — w/Tis + w/ RB
$* 5000 02 .
cs 03 uae: a ee
a: 0.000 0.0
050 100 150 050 100 150 o 50 100 150 050 100 150 o 50 100 150 050 100 150 050 100 150
15000 0.30 0.20 tos
g 0.25 107 0.04
4 12500 ws ae
2 000) 0.20 10 = we 0.02 — wio TIS + wio R3
a ie. wits + wion3
$4, 7500 tal ie | oi 08 — wo Ts + w/ R3
2 oa | o.0s 4
& 2500 0.05 10 CE | -0e
0.00 0.00 rn
050 100 150 0 50 100 150 050 100 150 050 100 150 0 50 100 150 o 50 100 150 050 100 150
step step step step step step step

15
