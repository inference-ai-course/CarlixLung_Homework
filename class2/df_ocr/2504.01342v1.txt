arX1v:2504.01342vl1 [cs.CL] 2 Apr 2025

Foundations and Evaluations in NLP
New Methods for Annotating Linguistic Resources and
Evaluating System Performance

Jungyeul Park
Department of Linguistics
The University of British Columbia
2613 West Mall, Vancouver, BC V6T 1Z4, Canada

jungyeul@mail.ubc.ca

Mémoire d’habilitation a diriger des recherches, 2024-2025




Contents

i |

rf

2

Prologue

1.1 Morpheme-based Language Annotation. ................
1.2 Evaluation by Alignment. .................022000.
1.3 Summary ... 2... 0.2 ee

How to Create LRs for NLP

Word Segmentation Granularity in Korean

Qel Tii@OdGGO. 22. i ta wk awh aw

2.2 Previous Work .......0. 0.00002 ee

2.3 Definition of Segmentation Granularity .................
2.3.1 Level 1: eojeols .... 2.2... 2002.02.00 02000002008.
2.3.2 Level 2: separating words and symbols .............
2.3.3 Level 3: separating case markers... ...........04.
2.3.4 Level 4: separating verbal endings. ...............
2.3.5 Level 5: separating all morphemes................
2.3.6 Discussion... . 2... 0. ee

aa Diagetic Amalygis . 2 em ts ew mk ete ke ww e
2.4.1 Language processing tasks .................004.
2A Tesultg and dis@gsso 2.2 eit cee t teweE w HE E

2.5 Conclusion... 2...

From Morphology to Semantics

3.1 POS Tagging .... 2... 02.02.0200 00 2 ee eee
3.1.1 CoNLL-U Format for Korean ..................
3.1.2 A new annotation scheme .................04.

3.2 Named Entity Recognition ..................2.2.2040.
3.2.1 Representation of NEs for the Korean language ........

3

17
18
20
22

25

27
27
30
32
33
39
38
41
44
46
46
48
50
52


3.3

3.4

3.2.2 Experiments... ............2..008.
3.2.3 Results... 2... .. 02.02.2000 0 02008.
Dependency Parsing .................048.-
3.3.1 Representation of MORPHUD ...........
3.3.2 Experiments and results ..............
FrameNet Parsing... 2 eee tee eee ewe ke we
3.4.1 Korean FrameNet dataset ...........4..

3.4.2. Morphologically enhanced FrameNet dataset

3.4.3 Experiments and results ..............

II How to Evaluate Results of NLP

4 Evaluation by Alignment

4.1
4.2

5.1

5.2

5.3

THRO 2k we tH wwe
Jointly Preprocessed Evaluation Algorithm ........
4.2.1 Sentence alignment .................
4.2.2 Word alignment...................
4.2.3 Proof... .... ee
5 Applications of JP-ALGORITHM
JP-PREPROCESSING ...........00.0. 000004
Galll TOMO: sa et ek we
5.1.2. Mismatches in preprocessing ............
5.1.3 Alignment-based evaluation ............
5.1.4 Experiments and results ..............
5.1.5 Conclusion... .......0 0000000 2 eee
JP-EVALB ... 10... ee
5.2.1 Introduction... ............0 2008.
5.2.2 Known problems ...............0.4.
5.2.3 Implementing new PARSEVAL measures... . .
5.2.4 Case studies... 2... 0.0... ee ee
5.2.5 Discussion... .. 2... 0.00. eee ee
Ba Ceoeusetia: . 2s es iw eB kaw Rw we
JP-ERRANT .. 20... 0. ee
Dotsl TROUOGOE: «2s es ie kw ww
5.3.2 Previous GEC evaluation measures ........
5.3.3 Alignment-based errant ..............

5.3.4 Experiments and results ..............

CONTENTS


CONTENTS

5.3.5
5.3.6

6 Epilogue

Multilingual alignment-based errant

Conclusion


CONTENTS


List of Figures

Dial

2.2

2.3

2.4

26

2.6
ast

3.1
3.2

3.3

3.4

3.5

Morphological analysis and part of speech (POS) tagging example in
the Sejong corpus: NN* are nouns, JK* are case makers and postpo-

sitions, V* are verbs, and E* are verbal endings............. 28
Different syntactic analyses using different segmentation granularities.

POS labels. gee oMmiibidd, 2:2: se wet caw ol
Example of the Sejong corpus for the sentence in (1) ......... 35

Example of the Penn Korean treebank where the punctuation mark
is separated from the word (tokenized): N* are nouns PA* are case

markers and postpositions, V* are verbs, and E* are verbal endings. . 36
GB theory for Korean syntactic analyses, in which the entire sentence
depends on verbal endings ............0..0.0 00000004 43
Example of downstream application processes ............. 45
Five levels of segmentation granularity in Korean and their POS an-
notation... 2... AT
Examples of MWTsin UD ..........2..2..0.0..0.2.0048. 59
The proposed CoNLL-U style annotation with multi-word tokens (MWT)

for morphological analysis and POS tagging .............. 60

The current CoNLL-U style UD annotation for Korean. It is based on
other agglutinative languages such as Finnish and Hungarian in Uni-
versal Dependencies. It separates punctuation marks for tokenisation. 63
CoNLL-U style annotation with multiword tokens for morphological
analysis and POS tagging. It can include BIO-based NER annotation
where B-LOC is for a beginning word of LOCATION and J-PER for an
inside word of PERSON... 2... 2 ee 67

CRF feature template example for word and pos. ........... 72

7


3.6

3.0

3.8

3.9

3.10

= lal

3.12

3.13

3.14

5.1

LIST OF FIGURES

Example of morpheme-based universal dependencies for Korean: while
dependencies in top-side are the original dependencies between words,
dependencies in bottom-side are newly added dependencies for be-
tween morphemes. ..... 2.0.00. pe 79

Confusion matrix for the direction of arcs where the column represents
gold, and the row system: Left, Right, and O for TO ROOT. ..... 84

Confusion matrix for the depth of arcs where the column represents
gold, and the row system... ... 2... 0... .. 0.02.00 00000. 85

Example of the 2/1 error by MaltEval where the gold’s arc depth is
2 and the system’s depth is 1. Note that MORPHUD results are con-
verted back to WORDUD: geuligo wang-ui chinjog-i byeoseul-eul hal
su issdolog jongchin gwageo jedo-leul silsihayeossda ‘And they intro-
duced a clan system to make sure that the king’s relatives can obtain
the government position’... 2... 2... ee 85

Compound noun with a left-skewed tree for NP modifiers in the Ko-
Wan WeSDRMR oe wt te eek hh 86

Comparisons between annotations on the same instance in Korean
PropBank and the Korean FrameNet dataset. The meaning of the
above instance is ‘if North Korea were excluded from the Department
of State’s list of state sponsors of terrorism...’, which is part of a
sentence in the Korean PropBank..................04. 88

Comparisons between annotations on the same instance in the Japanese
FrameNet dataset and the Korean FrameNet dataset. The meaning
of the above instance is ‘elementary school students cross a crosswalk
on the green light’. . 2... 2... ee 89

Comparisons between the corresponding information in Sejong Dictio-
nary and the annotation in the Korean FrameNet dataset with regard
to asingle instance. The meaning of the above instance is “that person
interferes in our affairs constantly and meddles in everything”. .... 90

Example of the morphologically enhanced FrameNet data: 30yeonyeon-
gan oseuteulialeul jibaehan jwaigi... (‘The left wing that ruled Austria
for over 30 years...”). 0 0 92

BIO-style evaluation in previous work where partial matched sentences
(e.g Click here) could be considered as true positive......... 112


LIST OF FIGURES

5.2

5.3

5.4
5.9
5.6

5.7
5.8
Ow)

Example of intermediate results of the evaluation by alignment algo-
rithm where Click here and To view it. In the system result are the
realigned, produced by merging after the implementation of sentence
alignment: LI is a symbol for tokenization, 1 is a symbol for SBD, and
~~~ is a symbol for merged sentences by sentence alignment. ... .
Examples with the jp-algorithm in Algorithm 3 for true positive of
sentences. If not, merging sentence boundaries. ............
Examples for true positive of tokens. If not, merging tokens.
Difference between jp-evalb andevalb ..............0..
Examples of evaluation results on Section 23 of the English Penn tree-
bank 2...
Evaluation results of bug cases... 2... ee
Evaluation results of the end-to-end Korean constituency parsing
Procedure example of jp-errant: stanza m2 indicates that sentence
boundaries are detected by stanza from raw text. ...........

5.10 Differences between errant and jp-errant ..............
5.11 Difference example of m2 files for Chinese and Korean. ........

114


10

LIST OF FIGURES


List of Tables

Dial

2d

3.1
Be
3.3

3.4

3.9

3.6

3.7

3.8

The number of tokens, the ratio of the morphologically complex words
(MCW) and the number of immediate non-terminal (NT) in the corpus 48
Experiment results on POS tagging, syntactic parsing and machine
translation based on different segmentation granularity levels. For
comparison purposes, we convert POS tagging results into Level 1
and syntactic parsing results into Level 5. Translation direction is

Remsen, im Pmglieh, 22 ett ck ew 50
POS tags in the Sejong corpus and their 1-to-1 mapping to Universal

POT see eet tem mE tee ee wt ee D7
Suffix normalisation examples ..........2. 00004 e eee 60

CRF /Neural results using different models using NAVER’s data con-
verted into the proposed format: fastText (Joulin et al., 2016) for

LSTM+CRF word embeddings. ..........0...0.00 20004 75
CRF /Neural results using the various sets of features using NAVER’s
data converted into the proposed format. ............... 75

CRF/Neural result comparison between the proposed CoNLL-U for-

mat versus NAVER’s eojeol-based format using NAVER’s data where

POS features are not applied. ... 2... ....02.02.0..0.2.0048. 77
CRF/Neural result comparison between BIO versus BIOES annota-

tions using NAVER’s data converted into the proposed format where

POS features are not applied. ..........02.2..0...20048. 78
Result comparison between the proposed CoNLL-U format and the
syllable-based format using MODU (19 & 21), KLUE, and ETRI
datasets where POS features are not applied (Model:XLM-ROBERTA+CRF). 78
Dependency parsing results: for the comparison purpose all MOR-
PHUD results are converted back to WORDUD after training and pre-
dicting with the format of MORPHUD.................. 84

11


12

LIST OF TABLES

3.9 Distributions of the lexical units (LUs) of the targets in 3 Korean
FrameNet datasets. An LU is a word with its part-of-speech.

3.10 The number of frames per lexical unit for each of the Korean FrameNet
datasets 2...

3.11 Numbers of sentences and instances in the 3 Korean FrameNet datasets.

3.12 The cross validation mean plus-minus standard deviation of exact and
partial F scores on eojeol- and morpheme-based variants of PKFN,
JKFN and SKFN datasets... 2... 0

5.1 Numbers of TP (true positive), FP (false positive) and FN (false neg-
ative) using nltk and stanza for sentence boundaries and tokens.
The stanza* line provides result numbers by the CoNLL-U evalua-
tion script. 2...
5.2. Numbers of TP , FP and FN using stanza for sentence boundaries and
tokens for multilingual case studies using UD_*-GSD where * is Ger-

man, Spanish, French, Indonesian, Japanese, Korean, and Portuguese.

5.3 Comparison to previous parsing evaluation metrics ..........
5.4 The most frequent errors and their ratios in the W&I dataset... . .
5.5 SOTA GEC results achieved by GECTOR and T5 with the English-
specific error classification module with gold and system sentence
boundaries... 6 2.
5.6 Multilingual GEC results by previous work and jp-errant with gold
and system sentence boundaries ............0.00 0004 %

125


Abstract

This memoir explores two fundamental aspects of Natural Language Processing
(NLP): the creation of linguistic resources and the evaluation of NLP system per-
formance. Over the past decade, my work has focused on developing a morpheme-
based annotation scheme for the Korean language that captures linguistic properties
from morphology to semantics. This approach has achieved state-of-the-art results
in various NLP tasks, including part-of-speech tagging, dependency parsing, and
named entity recognition. Additionally, this work provides a comprehensive analy-
sis of segmentation granularity and its critical impact on NLP system performance.
In parallel with linguistic resource development, I have proposed a novel evalua-
tion framework, the jp-algorithm, which introduces an alignment-based method to
address challenges in preprocessing tasks like tokenization and sentence boundary
detection (SBD). Traditional evaluation methods assume identical tokenization and
sentence lengths between gold standards and system outputs, limiting their appli-
cability to real-world data. The jp-algorithm overcomes these limitations, enabling
robust end-to-end evaluations across a variety of NLP tasks. It enhances accuracy
and flexibility by incorporating linear-time alignment while preserving the complexity

of traditional evaluation metrics. This memoir provides key insights into the process-

13


14 LIST OF TABLES

ing of morphologically rich languages, such as Korean, while offering a generalizable
framework for evaluating diverse end-to-end NLP systems. My contributions lay
the foundation for future developments, with broader implications for multilingual

resource development and system evaluation.


Résumé

Ce mémoire explore deux aspects fondamentaux du traitement automatique des
langues (TAL) : la création de ressources linguistiques et l’évaluation des perfor-
mances des systémes de TAL. Au cours de la derniére décennie, mes travaux se sont
concentrés sur le développement d’un schéma d’annotation basé sur les morphémes
pour la langue coréenne, qui capture les propriétés linguistiques allant de la mor-
phologie a la sémantique. Cette approche a permis d’obtenir des résultats a la pointe
de l’état de l’art dans diverses taches de TAL, y compris l’étiquetage morphosyntax-
ique, l’analyse en dépendances et la reconnaissance d’entités nommées. De plus, ce
travail propose une analyse approfondie de la granularité de segmentation et de son
impact crucial sur les performances des systémes de TAL. En paralléle du développe-
ment de ressources linguistiques, j’ai proposé un nouveau cadre d’évaluation, le jp-
algorithme, qui introduit une méthode basée sur lalignement pour traiter les défis
liés aux taches de prétraitement telles que la tokenisation et la détection des fron-
tiéres de phrases (SBD). Les méthodes d’évaluation traditionnelles supposent une
tokenisation et une longueur de phrases identiques entre les standards de référence
et les sorties du systéme, limitant ainsi leur applicabilité aux données réelles. Le jp-

algorithme surmonte ces limitations en permettant des évaluations robustes de bout

15


16 LIST OF TABLES

en bout 4 travers une variété de taches de TAL. I] améliore la précision et la flexibilité
en intégrant un alignement en temps linéaire tout en préservant la complexité des
métriques d’évaluation traditionnelles. Ce mémoire apporte des perspectives clés sur
le traitement des langues morphologiquement riches, telles que le coréen, tout en of-
frant un cadre généralisable pour l’évaluation de divers systémes de TAL de bout en
bout. Mes contributions posent les bases de futurs développements, avec des impli-
cations plus larges pour le développement de ressources multilingues et l’évaluation

des systémes.


Chapter 1

Prologue

In this memoir, I aim to address two critical aspects of Natural Language Process-
ing (NLP): the creation of linguistic resources for building more robust models and
the evaluation of these models’ performance. Over the past decade, I have devel-
oped a morpheme-based annotation scheme that effectively captures the linguistic
properties of the Korean language across various levels of language processing, from
morphology to semantics. This scheme has yielded state-of-the-art results in several
tasks. However, many existing evaluation metrics face significant limitations, such
as the requirement for identical tokenization, which assumes equal-length sentences
and words between gold-standard and system output files. My proposed evaluation
method introduces an alignment-based preprocessing step for sentences and words
without altering the original evaluation metrics. This approach preserves the com-
plexity of traditional evaluation methods while incorporating a linear-time alignment
process, enabling end-to-end evaluation of numerous NLP tasks that were previously

infeasible.

17


18 CHAPTER 1. PROLOGUE
1.1 Morpheme-based Language Annotation

My research over the past decade has primarily focused on the development of lin-
guistic resources (LRs), particularly for Korean. From 2011 to 2013, I developed
syntactic analysis systems for Korean, utilizing both phrase structure and depen-
dency parsing, in collaboration with KAIST, South Korea, and Kyoto University,
Japan. This work extends directly from my PhD dissertation, which was grounded
in tree-adjoining grammars (Joshi et al., 1975; Joshi and Schabes, 1991), a formalism
that models sentence structure through derived trees (constituency structure) and
derivation trees (dependency structure). The syntactic analysis systems I developed
during that period achieved state-of-the-art (SOTA) performance in Korean syntac-
tic parsing across both frameworks (Choi et al., 2012; Park et al., 2013). I have
since continued refining these systems, and they have consistently maintained SOTA
performance in both phrase structure and dependency parsing for Korean (Kim and

Park, 2022; Chen et al., 2022).

Working with two distinct approaches to syntactic parsing for Korean has led me
to recognize a crucial aspect of language processing: the granularity of word seg-
mentation. In constituency parsing, I proposed a morpheme-based structure where
terminal nodes represent the morphemes within a Korean word. For dependency
parsing, however, I adopted the eojeol (a space-separated unit in Korean) as the basic
unit of analysis. Most Korean language processing systems and corpora, including
the widely-used Sejong Corpus, utilize the eojeol as the primary unit of analysis.

Consequently, dependency parsing systems for Korean have largely adopted eojeol-


1.1. MORPHEME-BASED LANGUAGE ANNOTATION 19

based approaches, which I have also incorporated into my work. However, as Korean
is an agglutinative language, the productive combination of content and functional
morphemes within words calls for a more fine-grained analysis. Each morpheme is
assigned a distinct part-of-speech (POS) tag, functioning as a terminal node in a

constituency parse tree, similar to constituency treebanks in other languages.

Research in Korean language processing has consistently emphasized the repre-
sentation of words through various component units, ranging from eojeols, a basic
word-like unit, to more detailed morphological decompositions. These segmentation
strategies have typically been justified on linguistic or technical grounds, with min-
imal consideration of alternative approaches. However, the choice of segmentation
granularity critically affects the performance of algorithms in a wide range of tasks,
such as part-of-speech (POS) tagging, syntactic parsing, and machine translation. In
response to these challenges, I have conducted an extensive analysis of the granularity
levels previously proposed and employed in Korean language processing. Beginning
with the application of these various granularity levels in dependency parsing (Park,
2017), I introduced a comprehensive framework for segmentation granularity in Ko-
rean, organizing it into five distinct levels from both linguistic and computational
perspectives. Furthermore, I have presented the outcomes of these five segmentation
levels across various tasks, evaluating their relative effectiveness in Korean language

processing (Park and Kim, 2024).

Through an in-depth analysis of how the Korean language has been represented
across various language processing systems, I developed a new annotation method for

morphologically segmented words, analogous to the annotation of multiword tokens


20 CHAPTER 1. PROLOGUE

(MW's) in the CoNLL-U format. Using this novel annotation scheme, I collab-
orated with colleagues and supervised students in investigating various tasks for
Korean, including part-of-speech (POS) tagging (Park and Tyers, 2019), named en-
tity recognition (NER) (Chen et al., 2024b), dependency parsing (Chen et al., 2022)
and FrameNet parsing (Chen et al., 2024a). For each of these tasks, I re-annotated
linguistic resources using the proposed method, achieving state-of-the-art (SOTA)
results, which continue to represent the leading benchmarks in Korean language pro-

cessing.

1.2 Evaluation by Alignment

Accurate evaluation is fundamental in NLP to assess the effectiveness of systems.
Traditional methods, which compare system outputs with gold-standard references,
have been useful in component-based systems but fail to address the complexities
posed by modern end-to-end systems. These systems, which process entire tasks
without relying on modular components, highlight the limitations of conventional
evaluation, particularly in handling real-world data where sentence boundaries and

tokenization may not align between system outputs and gold standards.

In response to these challenges, I proposed a novel alignment-based evaluation
algorithm, termed the jp-algorithm (jointly preprocessed alignment-based evaluation
algorithm), specifically designed to address mismatches that arise during preprocess-
ing tasks such as tokenization and sentence boundary detection (SBD). I began by

integrating this evaluation method for preprocessing tasks, including tokenization


1.2. EVALUATION BY ALIGNMENT 21

and SBD, into my undergraduate course, with the support of the Centre for Teach-
ing, Learning and Technology (CTLT) at the University of British Columbia (UBC),

1 Building on

to redesign LING 242, Computational Tools for Linguistic Analysis.
alignment techniques from machine translation (MT), this approach improves eval-
uation robustness and accuracy by accounting for inconsistencies between system
outputs and gold standards. The proposed algorithm improves precision and recall,
particularly in tasks such as constituent parsing, grammatical error correction (GEC)

as well as preprocessing tasks, where traditional evaluation metrics rely on consistent

tokenization and sentence segmentation.

I made three key contributions based on the concept of evaluation by alignment:
(1) introducing an alignment-based method that enhances the evaluation of end-to-
end NLP systems, (2) extending the method’s applicability across a range of NLP
tasks, including preprocessing, parsing, and grammatical error correction (GEC),
and (3) addressing the complexities of real-world writing structures, ensuring reli-
able evaluations despite variations in sentence boundaries and tokenization. The
jp-algorithm provides a more accurate, flexible, and robust evaluation framework,
offering significant improvements over traditional methods in assessing NLP systems

across diverse text inputs.

'This course was initially designed for graduate students at the University of Arizona during
2016-2017, and I taught the same material at The State University of New York at Buffalo during
2018-2019. At UBC, I undertook the redesign of the course to adapt the material specifically
for undergraduate students through the Students as Partners program: https://sap.ubc.ca/
funded-projects/redesigning-ling242-computational-tools-for-linguistic-analysis/


22 CHAPTER 1. PROLOGUE

1.3. Summary

Following this Prologue chapter, the remainder of this memoir is divided into two
parts: Part I, How to Create LRs for NLP, and Part II, How to Evaluate Results
of NLP. In Part I, Chapter 2 addresses word segmentation granularity in Korean,
based on my publication in Korean Linguistics (Park and Kim, 2024). This research
began during my time at the University of Arizona in 2016-2017 and was published
in 2024. It provides the theoretical foundation for morpheme-based language annota-
tion for Korean. Chapter 3 expands on my collaborative work with Yige Chen (The
Chinese University of Hong Kong, Hong Kong). This chapter originates from my
initial work on POS tagging (Park and Tyers, 2019), where I proposed a morpheme-
based annotation scheme. Yige Chen extended this annotation approach to named
entity recognition (Chen et al., 2024b), dependency parsing (Chen et al., 2022) and
FrameNet parsing (Chen et al., 2024a). In Part H, Chapter 4 introduces the jp-
algorithm, detailing the alignment-based evaluation approach. This research was
supported by the Centre for Teaching, Learning and Technology (CTLT) through
collaboration with students, including Eunkyul Leah Jo (Department of Computer
Science) and Angela Yoonseo Park (Department of Linguistics) at The University
of British Columbia (UBC), and by the Academy of Korean Studies (AKS), which
provided funding to hire Junrui Wang as my research assistant during his master’s
studies at UBC. In Chapter 5, I developed applications of the jp-algorithm with the
assistance of Eunkyul Leah Jo and Angela Yoonseo Park for jp-preprocessing (Jo

et al., 2024b; Park et al., 2024) and jp-evalb (Jo et al., 2024a), and with Junrui


1.3. SUMMARY 23

Wang for jp-errant, which is currently under review. Finally, I will present my future

research plans in the concluding Epilogue chapter.


24

CHAPTER 1.

PROLOGUE


Part I

How to Create LRs for NLP




Chapter 2

Word Segmentation Granularity in
Korean

2.1 Introduction

Morphological analysis for Korean has been based on an eojeol, which has been con-
sidered as a basic segmentation unit in Korean delimited by white blank spaces in
a sentence. Almost all of the language processing systems and language data sets
previously developed for Korean have utilized this eojeol as a fundamental unit of
analysis. Given that Korean is an agglutinative language, joining content and func-
tional morphemes of words is very productive and the number of their combinations
is exponential. We can treat a given noun or verb as a stem (also content) followed by
several functional morphemes in Korean. Some of these morphemes can, sometimes,
be assigned its syntactic category. Let us consider the sentence in (1).

The corresponding morphological analysis is also provided in Figure 2.1. Unggaro
(‘Ungaro’) is a content morpheme (a proper noun) and a postposition -ga (nom-

inative) is a functional morpheme. They form together a single eojeol (or word)

27


28 CHAPTER 2. SEGMENTATION GRANULARITY

unggaro-ga (‘Ungaro+NOM’). For the sake of convenience, we add a figure dash (-)
at the beginning of functional morphemes, such as -ga (NOM) to distinguish between
content and functional morphemes. The nominative case markers -ga or -2 may
vary depending on the previous letter — vowel or consonant. A predicate naseo-
eoss-da also consists of the content morpheme naseo (‘become’) and its functional
morphemes, -eoss (‘PAST’) and -da (‘DECL’), respectively.

(Q) Bea] MA = aol daha Stet
peurangseu-ut segye-jeok-i-n utsang dijaineo emmanuel unggaro-ga
France-GEN world class-REL fashion designer Emanuel Ungaro-NOM
AW AIS AB aoe Uae.

silnae jangsik-yong jikmul dijaineo-ro naseo-eoss-da.
interior decoration textile designer-AJT become-PAST-DECL

’The world-class French fashion designer Emanuel Ungaro became an interior

textile designer.’

UgtA2|  peurangseu-ui peurangseu/NNP+ui/JKG France-GEN
AAA el — segye-jeok-i-n segye/NNG+jeok/XSN+i/VCP+n/ETM — world class-REL
O\AF  uisang uisang/NNG fashion
T]Abo] Le] dijaineo dijaineo/NNG designer
Anoh+-4 emmanuel emmanuel /NNP Emanuel
S7}E27} unggaro-ga unggaro /NNP+ga/JKS Ungaro-NOM
Al] silnae silnae/NNG interior
ALA] jangsikyong jangsikyong/NNG decoration
AS jikmul gikmul /NNG textile
Uz] dijaineo-ro dijaineo/NNG+ro/JKB designer-AJT
LAIT. ~— naseo-eoss-da. naseo/VV-+eoss /EP+da/EF+./SF become-PAST-DECL

Figure 2.1: Morphological analysis and part of speech (POS) tagging example in the
Sejong corpus: NN* are nouns, JK* are case makers and postpositions, V* are verbs,
and E* are verbal endings.

Every approach for Korean language processing has decided how to separate se-

quences of morphemes into component parts, ranging from eojeols, a basic word-like


2.1. INTRODUCTION 29

unit, all the way down to a complete morphological parse. These decisions have been,
for the most part, argued as either linguistically or technically motivated, with little
or no interest in exploring an alternative. The choice does have some impact on the
performance of algorithms in various tasks, such as part of speech (POS) tagging,
syntactic parsing and machine translation. In the study, we analyze different gran-
ularity levels previously proposed and utilized for Korean language processing. In
accordance with these analyzing works, we present the results of language processing
applications using different segmentation granularity levels for future reference. To
the best of the authors’ knowledge, this is the first time that different granularity
levels in Korean have been compared and evaluated against each other. This would
contribute to fully understanding the current status of various granularity levels that
have been developed for Korean language processing. Specifically the main goal of
this section is to diagnose the current state of natural language processing in Korean
by tracing its development procedures and classifying them into five steps. Addi-
tionally, this section aims to clearly explicate and evaluate the challenges unique
to Korean language processing, with the objective of contributing to the improve-
ment of various methodologies in this field. To this end, after presenting previous
work in Section 2.2, the study introduces the segmentation granularity in Korean
by classifying them into five different levels with a linguistic perspective as well as
a natural language processing perspective in Section 2.3, and presents several appli-
cation for Korean language processing using the five segmentation granularity levels
by comparing them each other in Section 2.4. Finally, Section 2.5 concludes the

discussion.


30 CHAPTER 2. SEGMENTATION GRANULARITY

2.2. Previous Work

Different granularity levels have been proposed mainly due to varying different syn-
tactic analyses in several previously proposed Korean treebank datasets: KAIST
(Choi et al., 1994), Penn (Han et al., 2002), and Sejong. While segmentation granu-
larity, which we deal with, is based on morphological analysis, the syntactic theories
are implicitly presented in the corpus for Korean words. Figure 2.2 summarizes the
syntactic trees which can be represented in Korean treebanks for different segmen-
tation granularity levels. Korean TAG grammars (Park, 2006) and CCG grammars
(Kang, 2011) described Korean by separating case markers. Most work on language
processing applications such as phrase structure parsing and machine translation for
Korean which uses the sentence by separating all morphemes (Choi et al., 2012; Park
et al., 2016; Kim and Park, 2022). The Penn Korean treebank introduced a tokeniza-
tion scheme for Korean, while the KAIST treebank separates functional morphemes
such as postpositions and verbal endings. Note that there are no functional tags

(i.e., -sbj or -ajt) in the KAIST treebank.

Syllable-based granularity (e.g., se LI gye U jeok U i, ‘world-class’) (Yu et al., 2017;

Choi et al., 2017) and even character-based granularity using the Korean alphabet (s

UeUgU yeUjuU eu kU 2) (Stratos, 2017; Song and Park, 2020) have also been
proposed where LI indicates a blank space. They incorporate sub-word information
to alleviate data sparsity especially in neural models. Dealing with sub-word level
granularity using syllables and characters does not consider our linguistic intuition.

We describe granularity based on a linguistically motivated approach in this section,


2.2. PREVIOUS WORK 31

ee ee
NP-SBJ VP
NP NP-SBJ = NP-COMP. VP
$ NP unggaro-ga ee naseo-eoss-da
NP-SBJ VP
WHNP-1 SS)
NP NP-SBJ = NP-AJT VP KOS NP-SBJ Vp
ee NP unggaro-ga nee naseo-eoss-da. «Tel I
segye-jeok-i-n +++ segye-jeok-i-n
(a) Sejong-style treebank (b) Penn-style Korean treebank
ae
eet |
oN -ga NP-AJT VP NP -ga vP AUXP
NP NP-SBJ tee naseo-eoss-da _— NP VP -€088
aan \ unggaro | -n NP unggaro ‘++ naseo
segye-jeok-i-n +++ segye-jeok -i
(c) Korean tree-adjoining grammar (d) KAIST-style treebank
ee
NP-SBJ VP
NP NP-SBJ = NP-AJT TT
VNP-MOD NP unggaro -ga tee naseo -eoss -da :
segye -jeok  § -i -n

(ec) Phrase structure parsing structure

Figure 2.2: Different syntactic analyses using different segmentation granularities.
POS labels are omitted.

in which each segmentation is a meaningful morphological unit.


32 CHAPTER 2. SEGMENTATION GRANULARITY

2.3 Definition of Segmentation Granularity

The annotation guidelines for Universal Dependencies stipulate each syntactic word,
which is an atom of syntactic analysis, as a basic unit of dependency annotation
(Nivre et al., 2020). This stipulation presupposes that there must be a separate
autonomous level of syntax and morphology. One of the features of agglutinative
languages is that there is a one-to-one mapping between suffixes and syntactic cat-
egories, indicating that each suffix must have an appropriate syntactic category to
which it belongs. More specifically, nouns can have individual markers indicating
case, number, possessive, etc., whose orders are fixed. Thus, we can regard any
given noun or verb as a stem followed by several inflectional or derivational mor-
phemes. The number of slots for a given part of a category may be pretty high. In
addition, an agglutinating language adds information such as negation, passive voice,
past tense, honorific degree to the verb form. That is, in an agglutinating language,
verbal morphemes are added to the root of the verb to convey various grammatical
features, such as negation, passive voice, past tense, and honorific degree. One of the
characteristics in Korean is to use a system of honorifics to express the hierarchical
status and familiarity of participants in conversation with respect to the subject,
object or the interlocutor. This system plays a great role in Korean grammar. When
a speaker uses honorific expression, we can figure out the social relationship between
the speaker, the interlocutor, and the object in the subject position at the same time.
This honorific system is reflected in the honorific markers attached to the nouns, and

verbal endings to the verb.


2.3. DEFINITION OF SEGMENTATION GRANULARITY 33

Such a complex and rich morphological system in agglutinative languages poses
numerous challenges for natural language processing. The key obstacle lies in a vo-
luminous number of word forms that can be derived from a single stem. Word form
analysis involves breaking a given surface word form into a sequence of morphemes in
an order that is admissible in Korean. However, several difficulties may arise in divid-
ing these sequences of morphemes into appropriate units. This section describes the
segmentation granularity procedures that could influence the performance of algo-
rithms in various tasks and the various analyses that have been adopted in Korean.
First of all, we define five different levels of segmentation granularity for Korean,
which have been independently proposed in previous work as different segmentation
units. While Levels 1 (eojeols as they are), 2 (tokenization — separating words and
symbols process), and 5 (separating all morphemes process) are due to technical
reasons, Levels 3 (separating case markers process) and 4 (separating verbal endings

process) are based on linguistic intuition.

2.3.1 Level 1: eojeols

As described previously, most Korean language processing systems and corpora have
used the eojeol as a fundamental unit of analysis. For example, the Sejong corpus, the
most widely-used corpus for Korean, uses the eojeol as the basic unit of analysis.'
The Sejong corpus was first released in 2003 and was continually updated until

2011. The project produced the largest corpus for the Korean language. It includes

'The Ministry of Culture and Tourism in Korea launched the 21st Century Sejong Project in
1998 to promote Korean language information processing. The project has its name from Sejong the
Great who conceived and led the invention of hangul, the writing system of the Korean language.


34 CHAPTER 2. SEGMENTATION GRANULARITY

several types of corpora: historical, contemporary, and parallel text. Contents of the
Sejong corpus represent a variety of sources: newswire data and magazine articles
on various subjects and topics, several book excerpts, and scraped texts from the
Internet. The Sejong corpus consists of the morphologically (part of speech tagged),
the syntactically (treebank), and the lexical-semantically annotated text as well as
a list of Korean words as dictionaries based on part of speech categories. Figure 2.3

shows an example of the Sejong corpus for the sentence in (1).

We define eojeols, as in the Sejong corpus, as granularity Level 1. Rationale of
this segmentation granularity in Korean language processing is simply to use the
word as it is in the surface form, in which the word is separated by a blank space in
the sentence (that is, in a manner of what you see is what you get). Most morpholog-
ical analysis systems have been developed based on eojeols (Level 1) as input and can
yield morphologically analyzed results, in which a single eojeol can contain several
morphemes. The dependency parsing systems described in Oh and Cha (2013) and
Park et al. (2013) used eojeols as an input token to represent dependency relation-
ships between eojeols. Oh et al. (2011) presented a system which predict phrase-level
syntactic label for eojeols based on the sequence of morphemes in the eojeol. What
is the most interesting is that Petrov et al. (2012) proposed Universal POS tags for
Korean based on the eojeol and Stratos et al. (2016) worked on POS tagging ac-
cordingly. Taking these basic trends into consideration, the study defines eojoeols
as Level 1. Recently released KLUE (Korean Language Understanding Evaluation)

also used the eojeol as a fundamental unit of analysis (Park et al., 2021).?

*https: //klue-benchmark.com


2.3. DEFINITION OF SEGMENTATION GRANULARITY 35

BTAA0001-00000012  2#}A9] metA/NNP+°2]/JKG
BTAA0001-00000013  4]7]4491 AA] /NNG+4 /XSN-+0]/VCP+  /ETM
BTAA0001-00000014 2}4t 2]At/NNG
BTAA0001-00000015. FJ zfo] 4] c]Zz}]4] /NNG
BTAA0001-00000016 @n}3-e] ajo} /NNP
BTAA0001-00000017. 3-7} 27} 27}= /NNP+7}/JKS
BTAA0001-00000018 A2Y Aly] /NNG
BTAA0001-00000019 f4}-2 Ayal /NNG+2/XSN
BTAA0001-00000020  4-& AlS/NNG
BTAA0001-00000021 UZz}o] 4H = z}o] |] /NNG+#/JKB
BTAA0001-00000022 1} 44}. UAL/VV+9/EP+t}/EF+./SF

(a) Morphologically (part of speech tagged) analyzed corpus where the word is analyzed as
in the surface form. Therefore, even the punctuation mark is a part of the word.

(S (NP-SBJ (NP (NP-MOD =#4/NNP+°2]/JKG)

(NP (VNP-MOD 4]A]/NNG+4]/XSN+0]/VCP+t /ETM)
(NP (NP 2]4t/NNG)

(NP ]Zp°]4]/NNG))))
(NP-SBJ (NP @u}-o/NNP)

NP-SBJ 37} /NNP-+7}/JKS)))

(VP (NP-AJT (NP (NP (NP 4JU|/NNG)

(NP 44] /NNG+-8/XSN))
(NP 4|2/NNG))

(NP-AJT c]Z}o] /NNG+#/JKB))

(VP UAV/VV+9/EP+t}/EF+./SF)))

=

(b) Syntactically analyzed corpus (treebank) where it inherits the annotation from the
morphologically (part of speech tagged) analyzed corpus and added bracketing syntactic
tree structure.

BSAA0001-00000012.  =Ze}Ag]
BSAA0001-00000013 4] 7|2¢1 AJAl__02/NNG+21/XSN-+0]/VCP+/ETM
BSAA0001-00000014 2] + oF 01/NNG

BSAA0001-00000015 4] Z}o] r]z}0]4] /NNG
BSAA0001-00000016 uh - al au}=e /NNP
BSAA0001-00000017  -3-7} 27} 27} /NNP+7}/JKS

me}A /NNP+2]/JKG

BSAA0001-00000018 44 Al] /NNG
BSAA0001-00000019 #A}-2 A¥A]__05/NNG+8/XSN
BSAA0001-00000020 2] Ale /NNG
BSAA0001-00000021 zo] zfo]U] /NNG+#/JKB
BSAA0001-00000022 ACH. UA] /VV+9/EP+t}/EF+./SF

(c) Semantically analyzed corpus where it includes the lexical semantic annotation to dis-
ambiguate the sense of the word by the Sejong dictionary.

Figure 2.3: Example of the Sejong corpus for the sentence in (1)
2.3.2 Level 2: separating words and symbols

The process of tokenization in the Korean language has often been overlooked, pri-

marily because eojeols has traditionally been used as the basic unit of analysis.


36 CHAPTER 2. SEGMENTATION GRANULARITY

However, it has come to our attention that certain corpora have started adopting
an English-like tokenization approach, which results in preprocessed words within
these corpora. For example, the Penn Korean treebank (Han et al., 2002), which
punctuation marks are separated from words.? This segmentation granularity es-
pecially in the Penn-treebank style corpus focuses on multilingual processing where
Penn treebanks include English (Marcus et al., 1993; Taylor et al., 2003), Chinese
(Xue et al., 2005), Arabic (Maamouri and Bies, 2004) and Korean (Han et al., 2002).
The Penn Korean treebank follows the tokenization scheme that has been used in the
other language of the Penn treebanks, as shown in Figure 2.4. The most distinctive
feature in Level 2 lies in that the punctuation mark is all separated from the original
word (tokenized).

(S (NP-SBJ =1/NPN+-2/PAU)

(VP (S-COMP (NP-SBJ  =t-/NPR+°]/PCA)
(VP (VP (NP-ADV 3/NNU
4 /NNX+2/NNX+7}2]/PAU)
(VP (NP-OBJ 1--/NNC+2]2]/NNC
Al8t/NNC+2/PCA)
Zt/VV+31/ECS))
Q)/VX+t}/EFN+31/PAD))

20] /VV+9/EPF+t}/EFN)
_/SEN)

Figure 2.4: Example of the Penn Korean treebank where the punctuation mark
is separated from the word (tokenized): N* are nouns PA* are case markers and
postpositions, V* are verbs, and E* are verbal endings.

We define the tokenization by separating words and symbols as a granularity

3While the Penn Korean treebank separates all punctuation marks, quotation marks are the only
symbols that are separated from words in the Sejong treebank to distinguish between the quoted
clause and the main sentence in the tree structure. We also note that among the existing corpora for
Korean, only the Sejong treebank separates quotation marks from the word. Other Sejong corpora
including the morphologically analyzed corpus do not separate the quotation marks, and still use
the eojeol as a basic analysis unit.


2.3. DEFINITION OF SEGMENTATION GRANULARITY 37

Level 2. Chung and Gildea (2009) used a granularity Level 2 for a baseline tok-
enization system for a machine translation system from Korean into English where
they proposed an unsupervised tokenization method to improve the machine trans-
lation result. Figure 2.4 illustrates that the punctuation marker has been separated
from the verb deosbut-i-eoss-da (‘added’) and assigned its own category with the
marker being designated as ‘textttsfn’ in Penn Treebank. In addition, the tokeniza-
tion schema of the sentence follows the method similar to the English language. That
is, syntactic unit 3-wol-mal-kka-ji (‘until the end of March’) is traditionally treated
as one eojeol, but in Level 2, this unit is tokenized as three different units such as
3-wol (‘March’), mal (end) and kka-ji (‘until’), which is tokenized identically to that
of English such as until the end of March. As mentioned in Level 1, most Korean lan-
guage processing systems have used an eojeol as their basic unit of analysis, resulting
in a single eojeol involved with several different morphemes, which is a prominent
feature in Level 1. According to this principle, we can easily identify that the noun
phrase in a subject position geu-eun forms one eojeol consisting of a stem geu and
a topic marker eun. In the same way, the verb phrase deosbut-i-eoss-da (‘added’)
creates one eojeol with a root deosbut, a passive suffix -7, a past tense marker -eoss

and verb ending marker -da.

Park et al. (2014) also used this granularity to develop Korean FrameNet lexicon
units by using the cross-lingual projection method from the Korean translation of
the English Propbank (Palmer et al., 2005). Universal Dependencies (Nivre et al.,
2016, 2020) contains two Korean dependency treebanks, namely the GSD treebank
(McDonald et al., 2013) and the KAIST treebank (Choi et al., 1994; Chun et al.,


38 CHAPTER 2. SEGMENTATION GRANULARITY

2018), which also use the tokenization scheme by separating words and punctuation
marks.

Recently, Park and Kim (2023) insisted that the functional morphemes in Ko-
rean should be treated as part of a word in Korean categorial grammars, with the
result that their categories for detailed morphemes do not require to be assigned
individually in a syntactic level, and also that it would be more efficient to assign
the syntactic categories on the fully inflected lexical word derived by the lexical rule

of the morphological processes in the lexicon.

2.3.3 Level 3: separating case markers

From a purely linguistic perspective, postpositions as functional morphemes in Ko-
rean convey grammatical cases (e.g., nominative or accusative), adverbial relations
(spatial, temporal or directional), semantic roles and conjunctives by combining with
the lexical words. We may separately indicate them as case marker, adverbial post-
position, auxiliary postposition, and conjunctive postposition, respectively, though
we generally term them as postpositions or case markers, depending on the authors.
In linguistics, a marker also refers to a free or bound morpheme indicating the gram-
matical function of the word, phrase or sentence. For the sake of convenience, this
section uses case markers as a term for covering them. Case markers are immediately
attached following a noun or pronoun. They are used to indicate the grammatical
roles of a noun in a sentence such as subject, object, complement or topic.

First of all, -c and -ga are nominative case markers whose form depends on

whether the stem ends with a vowel or consonant. When the honorific subject is


2.3. DEFINITION OF SEGMENTATION GRANULARITY 39

used, this nominative case marker will be replaced by the honorific marker -kkeseo,
instead of -i or -ga. An honorific is marked to encode the relative social status of the
interlocutors. A major feature of this honorific system is typically to convey the same
message in both honorific and familiar forms. Korean honorifics are added to nouns,
verbs, and adjectives. Similarly to this nominative case marker, the honorific dative
case marker -kke will be used instead of the familiar dative case marker -ege. The
rest of the markers are used to express the adverbial relations such as directional,
temporal, spatial including source and destination, and accompaniment. All of these
markers attached to the noun stem cannot be duplicated, showing complementary
distribution. As shown in the example (2), the nominative case marker -ga cannot
be together with the instrumental case marker -ro in (2b), and cannot collocate also

with the dative case marker -ege in (2c).

(2) a. holangi-ga sanab-da.
tiger-NOM fierce-DECL

’A tiger is fierce.’

b. *holangi-ga-ro sanab-da.
tiger-NOM-‘TO’ fierce-DECL

’A tiger is fierce.’

c. *holangi-ga-ege sanab-da.
tiger-NOM-DAT fierce-DECL

’A tiger is fierce.’

Under a perspective of natural language processing, the Sejong corpus has been
criticized for the scope of the case marker, in which only a final noun (usually the

lexical anchor) in the noun phrase is a modifier of the case marker. For example,


40 CHAPTER 2. SEGMENTATION GRANULARITY

Emmanuel Ungaro-ga in the Sejong corpus is annotated as (NP (NP Emmanuel)
(NP Ungaro-ga)), in which only Ungaro is a modifier of -ga (‘NOM’). For example
as described in Ko (2010), while there are several debates on whether a noun or a
case marker is a modifier in Korean, this is beyond the scope of the section. The
Penn Korean treebank does not explicitly represent this phenomenon. It just groups
a noun phrase together: e.g., (NP Emmanuel Ungaro-ga), which seems to be treated
superficially as a simple compound noun phrase. Collins’ preprocessing for parsing
the Penn treebank adds intermediate NP terminals for the noun phrase (Collins,
1997; Bikel, 2004), and so NPs in the Penn Korean treebank will have a similar NP
structure to the Sejong corpus (Chung et al., 2010). To fix the problem in the previous
treebank annotation scheme, there are other annotation schemes in the corpus and
lexicalized grammars. They are introduced to correctly represent the scope of the
case marker. Park (2006) considered case markers as independent elements within
the formalism of Tree adjoining grammars (Joshi et al., 1975). Therefore, he defined
case markers as an auxiliary tree to be adjoined to a noun phrase. In contrast to
case markers, verbal endings in the inflected forms of predicates are still in the part
of the eojoel and they are represented as initial trees for Korean TAG grammars.
The lemma of the predicate and its verbal endings are dealt with as inflected forms
instead of separating functional morphemes (Park, 2006). This idea is going back
to Maurice Gross’s lexicon grammars in 1970s (Gross, 1975) and his students who
worked on a descriptive analysis of Korean in which the number of predicates in
Korean could be fixed by generating all possible inflection forms: e.g., Pak (1987);
Nho (1992); Nam (1994); Shin (1994); Park (1996); Chung (1998); Han (2000).


2.3. DEFINITION OF SEGMENTATION GRANULARITY Al

2.3.4 Level 4: separating verbal endings

With a purely linguistic perspective, Korean verbs are formed in terms of the agglu-
tinating process by adding various endings to the stem. Korean is widely known to
have a great many verbal endings between this stem and final verbal endings. More
specifically, the verbal endings in Korean are well known to be complex in their syn-
tactic structures in the sense that the verbal endings carry much of functional load
in the grammatical aspects such as sentence mood, tense, voice, aspect, honorific,
conjunction, etc.: for example, inter alia, tense (Hwang, 2003), grammatical voice
(Park, 2007), interaction of tense-aspect—mood marking with modality (Song, 1998),
evidentiality (Lim, 2008), and interrogativity (Lim, 2011). More additional endings
can be used to denote various semantic connotations. That is, a huge number of
grammatical functions are achieved by adding various verbal endings to verbs. The
number can also vary depending on the theoretical analyses, naturally differing in
their functions and meanings. These endings, of course, do not change the argument
structures of a predicate. A finite verb in Korean can have up to seven suffixes as
its endings, whose order is fixed. As mentioned in the previous section, the Korean
honorific system can also be reflected in verbs with honorific forms. When a speaker
expresses his respect toward the entities in a subject or indirect object position, the
honorific marker -(eu)si is attached to the stem verb, thereby resulting in the verb
form sanchaegha (‘take a walk’). The suffixes denoting tense, aspect, modal, formal,

mood are followed by the honorific.

Unlike the markers attached to nouns, Korean verbal endings are added to the


42

verb stem in a specific order, depending on the tense, mood, and politeness level
of the sentence, as illustrated in (3).
can be followed by the honorific -si in (3a). The two suffixes indicating an honorific
and past tense can be attached to the verb stem in (3b).
suffix of retrospective aspect is added in the example in (3c). If the order of a past

suffix and honorific suffix is changed in the verbal endings, the sentence would be

CHAPTER 2. SEGMENTATION GRANULARITY

ungrammatical, as in (3d).

(3)

Government and Binding (GB) theory (Chomsky, 1981, 1982) for Korean syn-
tactic analyses, in which the entire sentence depends on verbal endings as described
in Figure 2.5 for naseo-eoss-da (‘became’). This means that the functional mor-

pheme -eoss is assigned its own syntactic category T(ense) and the verbal ending

. halabeoji-kkeseo jams sanchaegha-si-n-da.

Grandfather-NOM-HON for a while take a walk-HON-PRES-DECL

’Grandfather takes a walk for a moment.’

. halabeoji-kkeseo jams sanchaegha-sy-eoss-da.

Grandfather-HON for a while take a walk-HON-PAST-DECL

’Grandfather took a walk for a moment.’

. halabeoji-kkeseo jamsi sanchaegha-sy-eoss-deon jeog-t

Grandfather-HON once go for a walk-HON-PAST-ASP experience-COP
iss-da.
be-DECL

’Grandfather once went for a walk for a moment.’

. *halabeoji-kkeseo jamsi sanchaegha-eoss-sy-deon jeog-t

Grandfather-HON once go for a walk-PAST-HON-ASP experience-COP
iss-da.
be-DECL

’Grandfather once went for a walk for a moment.’

The verb stem sanchaegha (‘to take a walk’)

One more additional


2.3. DEFINITION OF SEGMENTATION GRANULARITY 43

-da C(omplimentizer) attached in the final position determines the whole syntactic

category CP in Korean.

CP
|
C >
a
IP Cc
eee eg |
NP ~? -da
| 2—™
. VP I
aN |
eV T
| |
naseo  _-€0ss

Figure 2.5: GB theory for Korean syntactic analyses, in which the entire sentence
depends on verbal endings

From the Natural Language Processing perspective, the KAIST treebank (Choi
et al., 1994), an earliest Korean treebank, introduced this type of analysis, which
is Level 4. It is the granularity Level 4 that we adapt the KAIST treebank rep-
resentation. While the KAIST treebank separates case markers and verbal endings
with their lexical morphemes, punctuation marks are not separated and they are still
a part of preceding morphemes as represented in the Sejong treebank. Therefore,
strictly speaking, one could judge that the KAIST treebank is not granularity Level
4 by our definition because we separate punctuation marks. In addition, while it
also represents derivational morphology in the treebank annotation (7.e., for a cop-

ula segye-jeok U -i LI -n (‘world-class’) in the KAIST treebank), we separate only


44 CHAPTER 2. SEGMENTATION GRANULARITY

verbal endings (i.e., segye-jeok-i LI -n).

2.3.5 Level 5: separating all morphemes

Many downstream applications for Korean language processing are based on the gran-
ularity Level 5, in which all morphemes are separated: POS tagging (Jung et al.,
2018; Park and Tyers, 2019), phrase-structure parsing (Choi et al., 2012; Park et al.,
2016; Kim and Park, 2022) and statistical machine translation (SMT) (Park et al.,
2016, 2017), etc. where the applications take all the morphemes separated sequence
instead of the surface sentence segmented by a blank, as input for language pro-
cessing. A morpheme-based annotation scheme proposed in Park and Tyers (2019)
for POS tagging has been extended to dependency parsing (Chen et al., 2022) and
named-entity recognition (Chen et al., 2024b) and it attained the most advanced
evaluation outcomes. Figure 2.6 shows examples of the downstream application pro-
cess: constituent parsing using the Sejong treebank and machine translation from
Korean into English. The sentence often in these applications is converted into the
sequence of morphemes to be parsed or translated. They mostly implement gran-
ularity level 5 to avoid the problems of data sparsity and unknown words because
the number of possible types combined in longer segmentation granularities, such
as eojeol, can increase exponentially. Such morpheme-based analysis for the word
can be generated by a morphological analysis system. Therefore, most POS tagging
systems can produce segmentation granularity Level 5. Separating these morphemes
is straightforward from such morphological analysis results. For instance, as shown

in Figure 2.6, in Level 5, The phrase segyejeokin (‘world-class’), which also includes


2.3. DEFINITION OF SEGMENTATION GRANULARITY 45

derivational morphemes, is treated as a separated four morphemes sequence segye-
jeok-i-n instead of one surface segment as input for language processing. Specifically,
this phrase is assigned four different categories: a NNG (common noun for segye),
XSN (nominal derivational affix for jeok), VCP (copular for 7) and ETM (adnominal
affix for n), respectively. These categories consist of the word stem, two deriva-
tional morphemes, and an inflectional morpheme, resulting in a new category verb

functioning as a modifier in this sentence.

NP-SBJ VP
i ae
NP NP-SBJ NP-AJT VP
NP-MOD NP NP NP-SBJ NP NP-AJT VV EP EF SF
aN ao L “SN aN, YN I LL
NNP JKG VNP-MOD NP NNP NNP JKS NP NP NNG JKB UA4| si ut «
Lt LN OL
zyA ©} NNG XSN VCP ETM NP NP Ant-a 87}2 7+ NP NP NNG OApo]y
Pt J ft | /\ J
AIA A °] u NNG NNG NNG NNG XSN 4&
| | | |
spy aatoyy AW ALB

(a) Morpheme-based treebank for constituent parsing (Choi et al., 2012)
HeA9| AAzel OF c|apo|y uppel S77} Ay] BLS 2HE capo] uyztch.
) (separating all morphemes)
BBA 2 AMA SO] LU St pole] Baha S7t= 7} Bu 4] B Je pole] BUA Ae.
) (machine translation)

The world-class French fashion designer Emanuel Ungaro became an interior textile designer.

(b) Example of a machine translation process (Park et al., 2017)

Figure 2.6: Example of downstream application processes


46 CHAPTER 2. SEGMENTATION GRANULARITY

2.3.6 Discussion

Figure 2.7 summarizes an example of each segmentation granularity level. For our
explanatory purpose, we use the following sentence in (1): segye-jeok-i-n .... unggaro-
ga... naseo-eoss-da. (‘The world-class ... Ungaro became ...’). The advantage of
Level 1 is that it has many linguistics resources to work with. The main weakness of
Level 1 is that it requires segmentation including the tokenization process which has
been a main problem in language processing in Korean. While Level 2 has appeared
more frequently especially in recent Universal Dependencies (UD)-related resources,
and Levels 3 and 4 propose an analysis more linguistically pertinent, they do not
mitigate the segmentation problem. Level 5 has the practical merits of a processing
aspect. However, the eventual problem for the reunion of segmentation morphemes,
for example the generation task in machine translation, still remains, and it has not

been discussed much yet.

2.4 Diagnostic Analysis

In this section, we present several applications for Korean language processing using
proposed segmentation granularity levels to compare them to each other. We use
the default options that the system provides for experiments. For experiments, we
convert all data sets into each segmentation granularity. We utilize a 90-10 split for
the Sejong treebank for the training and evaluation for POS tagging and syntactic
parsing. We utilize training and evaluation data sets for Korean-English machine

translation provided by Park et al. (2016).


2.4. DIAGNOSTIC ANALYSIS AT

1 2 3

segye-jeok-i-n .. UNnggaro-ga  naseo-e0ss-da
NNG+XSN+VCP+ETM NNP+JKS VV+EP+EF+SF

‘world-class’ ‘Ungaro’+NOM ‘become’+PAST+DECL+PUNCT

| separating words and symbols

1 5 3 4
segye-jeok-i-n ..  unggaro-ga_... naseo-eoss-da
NNG+XSN+VCP+ETM NNP+JKS VV+EP+EF SF

| separating case markers

1 2 3 4 5
segye-jeok-i-n ..  unggaro -ga ... ~—- naseo-eoss-da
NNG+XSN+VCP+ETM NNP JKS VV+EP+EF SF

| separating verbal endings

1 2 3 4 5 6 7 8
segye-jeok-t -n ..  unggaro -ga ... ~naseo -eoss -da .
NNG+XSN+VCP ETM NNP JKS VV EP EF SF

| separating all morphemes

1 2 3 4 5 6 7 8 9 10
segye -jeok -t -n ..  unggaro -ga ..  maseo -eoss -da .
NNG XSN VCP ETM NNP JKS VV EP EF SF

Figure 2.7: Five levels of segmentation granularity in Korean and their POS anno-
tation.

Firstly, Table 2.1 shows the number of tokens, the ratio of morphologically com-
plex words (MCW) which are made up of two or more morphemes, and the number
of immediate non-terminal (NT) nodes (the number of monomorphemic and complex

word patterns) in the entire Sejong treebank. Therefore, the immediate NT nodes


48 CHAPTER 2. SEGMENTATION GRANULARITY

signify the POS labels, and can be eojeols, morphemes and symbols according to

different segmentation granularity.

Level 1 Level2 Level3 Level 4 Level 5

Token 370,729 436,448 577,153 752,654 829,506
MCW _ 0.7881 0.6451 0.2939 0.0934 0
Immediate NT 4,318 2,378 1,228 526 45

Table 2.1: The number of tokens, the ratio of the morphologically complex words
(MCW) and the number of immediate non-terminal (NT) in the corpus

2.4.1 Language processing tasks

Word segmentation, morphological analysis and POS tagging Word seg-
mentation, morphological analysis and POS tagging for Korean requires detection
of morpheme boundaries. We use UDPipe, a trainable pipeline (Straka et al., 2016)
to perform tokenizing and POS tagging tasks. The current experimental setting
achieved the state of the art word segmentation and POS tagging result for Korean
(Park and Tyers, 2019). Each trained POS tagging model assigns POS labels for
its tokens of granularity. For example, a model should generate segye+jeok+i+n
for morpheme boundary and nng+xsn+vcptetm as a single POS label in Level 1 for

segyejeokin (‘world-class’), or nng in Level 5 for segye (‘world’). We present the fl

precision-recall

BCOre (2 , precision+recall

) for word segmentation evaluation using precision and recall

described in (2.1), and the accuracy score for POS tagging evaluation as in (2.2).


2.4. DIAGNOSTIC ANALYSIS 49

# of relevant word segments M # of retrieved word segments

ee # of retrieved word segments (2.1)
1 # of relevant word segments M # of retrieved word segments ,
recall =
# of relevant word segments
correct # of POS tagging labels
accuracy = (2.2)

total # of POS tagging labels

Syntactic parsing Using the granularity Level 5 has been the de facto standard
for Korean phrase structure parsing (Choi et al., 2012; Park et al., 2016; Kim and
Park, 2022). We train and evaluate the Berkeley parser (Petrov et al., 2006; Petrov
and Klein, 2007) with the different granularity levels. The Berkeley parser uses
the probabilistic CFG with latent annotations previously proposed in Matsuzaki
et al. (2005), and performs a series of split and merge cycles of non-terminal nodes
to maximize the likelihood of a treebank. It still shows relatively good parsing
results. We keep the structure of the Sejong treebank, and terminal nodes and
their immediate NTs are varied depending on the granularity level. We provide gold
POS labels as input instead of predicting them during parsing to the original word
boundary in the word. This allows us to evaluate parsing results with the same
number of terminals for all granularity levels. We present the fl score by precision
and recall of bracketing using EVALB (Black et al., 1991) for parsing evaluation

which uses the fl score based on precision and recall presented in (2.3).


50 CHAPTER 2. SEGMENTATION GRANULARITY

# of relevant constituents N # of retrieved constituents

precision = ; 7
# of retrieved constituents

# of relevant constituents M # of retrieved constituents

(2.3)

recall =
7 of relevant constituents

Machine translation Using the granularity Level 5 has been the de facto stan-
dard for machine translation for Korean (Park et al., 2016, 2017). We use the Moses
statistical machine translation system (Koehn et al., 2007) with the different gran-
ularity levels for Korean to train the phrase-based translation model and minimum
error rate training (Och, 2003) during validation. We present the BLEU (BiLingual

Evaluation Understudy) score (Papineni et al., 2002) for evaluation.

2.4.2 Results and discussion

Level 1 Level 2 Level 3 Level 4 Level 5
Segmentation 100.00 95.43 94.31 93.05 90.15 (Fi)
POS tagging 83.18 86.28 89.21 92.82 96.01 (Acc)
(
(

Syntactic parsing 76.69 77.50 81.54 84.64 82.23 Fy)
Machine translation 5.86 6.87 7.64 7.85 7.98 BLEU)

Table 2.2: Experiment results on POS tagging, syntactic parsing and machine trans-
lation based on different segmentation granularity levels. For comparison purposes,
we convert POS tagging results into Level 1 and syntactic parsing results into Level
5. Translation direction is Korean into English.

The direct interpretation of task results between the different granularity levels
would be difficult because the levels of representation are different (e.g., the number

of lexical tokens is different in Table 2.1). For comparison purposes of experiment

results, (1) we report segmentation results where Level 1 does not require any seg-


2.4. DIAGNOSTIC ANALYSIS 51

mentation. (2) We convert all POS tagging results into Level 1 based on eojeol after
training and predicting results for each segmentation granularity level. Therefore,
the presented POS tagging accuracy is based on Level 1 eojeols as in previous work
on POS tagging (Cha et al., 1998; Hong, 2009; Na, 2015). (3) We convert syn-
tactic parsing results into morpheme-based Level 5 as in previous work on phrase
structure parsing (Choi et al., 2012; Park et al., 2016; Kim and Park, 2022). Al
though the Berkeley parser can predict the POS label during parsing, we provide
gold POS labels, which is correct POS labels from the test dataset as input for the
parsing system to keep original morpheme boundaries. After parsing sentences for
each segmentation granularity level, we convert parsing results into Level 5. (4)
For machine translation, we translate Korean sentences in different segmentation
granularity into English where there is no different segmentation granularity. We use
multi-bleu.perl provided by Moses (Koehn et al., 2007) to evaluate the translation

result.4

All results based on different segmentation granularity levels are reported in Ta-
ble 2.2. The interpretation of results of segmentation is straightforward where no
tokenization is required in Level 1 and more tokenization is required in Level 5.
From POS tagging to MT, we provide a gold-segmented sequence to evaluate each
task. Results of POS tagging indicate that such morpheme-based analysis outperform
other granularity, which conforms to the previous results on morphological analysis
and POS tagging for Korean (Park and Tyers, 2019). As we described, fine-grained

granularity by separating all morphemes (Level 5) has been utilized for downstream

“https: //github.com/moses-smt/mosesdecoder


52 CHAPTER 2. SEGMENTATION GRANULARITY

applications such as machine translation for Korean and it shows the best perfor-
mance in the BLEU score (Papineni et al., 2002). Whereas phrase structure parsing
also uses by separating all morphemes (Level 5) as input for the previous parsing
system (Choi et al., 2012; Park et al., 2016; Kim and Park, 2022), granularity by
separating only functional morphemes including case markers and verbal endings
and keeping other affixes for morphological derivation (Level 4) outperform Level 5.
The modern statistical parsers have used markovization annotation for non-terminal
nodes to elaborate context-free grammar rules for parsing either using the manual
heuristics (Johnson, 1998; Klein and Manning, 2003) or machine learning techniques
(Petrov et al., 2006; Petrov and Klein, 2007). Parsing performance in the statistical
parsers is directly related with the size and the quality of CFG rules generated by
these annotation schemes of non-terminal nodes. The other explanation for Level
4’s parsing performance involves its linguistically soundness of its segmentation of
the word, in which its immediate non-terminal nodes represent actual part-of-speech
information of the word with its adjoined functional morphemes. Linguistic informa-
tion of this kind might help to improve the representation of the treebank grammar

that is implied by the parsing system.

2.5 Conclusion

The study addresses word segmentation granularity for the segmentation in Korean
language processing. There have been multiple possible word segmentation granular-

ity levels from a word to morphemes in Korean, and for specific language processing


2.5. CONCLUSION 53

and annotation tasks, several different granularity levels have been proposed and
developed. It is that the agglutinative languages including Korean can have a one-
to-one mapping between functional morpheme and syntactic category, even though
the annotation guidelines for Universal Dependencies typically regard a basic unit of
dependency annotation as a syntactic word. We have presented five different levels
of segmentation granularity in Korean. We have analyzed and compared these levels
of granularity by using Korean language applications as well. Previous work for Ko-
rean language processing has not explicitly mentioned which level of segmentation
granularity is used, and this makes it difficult to properly compare results between
systems. As described, these different levels of segmentation granularity could exist
mainly because various Korean treebanks represent their syntactic structure differ-
ently. These treebanks also use the different segmentation of words depending on
their linguistic and computational requirements. While a certain segmentation gran-
ularity may be well suited for some linguistic phenomena or applications, we need
to find a correct segmentation granularity level to adapt to our requirements and

expectations for Korean language processing.”

°This chapter is based on "Word segmentation granularity in Korean" by Jungyeul Park and
Mija Kim, published in Korean Linguistics, John Benjamins Publishing Company. 20(1):82-112
(Park and Kim, 2024).


54

CHAPTER 2. SEGMENTATION GRANULARITY


Chapter 3

From Morphology to Semantics

3.1 POS Tagging

This section examines the Sejong POS-tagged corpus to propose a new annotation
method for end-to-end morphological analysis and POS tagging. Many upstream
applications in Korean language processing are based on a segmentation scheme
where all morphemes are separated. For instance, previous work on phrase-structure
parsing (Choi et al., 2012; Park et al., 2016) and statistical machine translation
(SMT) (Park et al., 2016, 2017) follows this approach to mitigate data sparsity, as
longer segmentation granularity can combine words exponentially. To address this,
we propose a novel annotation method using morphologically separated words, based

on the approach for annotating multiword tokens (MWT) in the CoNLL-U format.'

lhttp: //universaldependencies.org/format .html

59


56 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

3.1.1 CoNLL-U Format for Korean

We use CoNLL-U style Universal Dependency (UD) annotation for Korean mor-
phology. We first review the current approaches to annotating Korean in UD and
their potential limitations. The CoNLL-U format is a revised version of the previous
CoNLL-X format, which contains ten fields from word index to dependency relation
to the head. This paper concerns only the morphological annotation: word form,
lemma, universal POS tag and language-specific POS tag (Sejong POS tag). The
other fields will be annotated either by an underscore which represents not being
available or dummy information so that it is well-formed for input into applications

that process the CoNLL-U format such as UDPipe (Straka and Strakova, 2017).

3.1.1.1 Universal POS tags and their mapping

To facilitate future research and to standardize best practices, Petrov et al. (2012)
proposed a tagset of Universal POS categories. The current Universal POS tag map-
ping for Sejong POS tags is based on a handful of POS patterns of eojeols. However,
combinations of words in Korean are very productive and exponential. Therefore,
the number of POS patterns of the word does not converge even though the number
of words increases. For example, the Sejong treebank contains about 450K words
and almost 5K POS patterns. We also test with the Sejong morphologically analysed
corpus which contains 9.2M eojeols. The number of POS patterns does not converge
and it increases up to over 50K. The wide range of POS patterns is mainly due to the
fine-grained morphological analysis, which shows all possible segmentations divided

into lexical and functional morphemes. These various POS patterns might indicate


3.1. POS TAGGING 57

useful morpho-syntactic information for Korean. To benefit from the detailed an-
notation scheme in the Sejong treebank, Oh et al. (2011) predicted function labels
(phrase-level tags) using POS patterns that improve dependency parsing results. Ta-
ble 3.1 shows the summary of the Sejong POS tagset and its detailed mapping to
the Universal POS tags. Note that we convert the XR (non-autonomous lexical root)
into the NOUN because they are mostly considered nouns or a part of a noun:e.g.,

minju/XR (‘democracy’).

Sejong POS (S) description Universal POS (U)
NNG, NNP, NNB, NR, XR noun related NOUN
NNP proper noun PROPN
NP pronoun PRON
MAG adverb ADV
MAJ conjunctive adverb CONJ
MM determiner DET
VV, VX, VCN, VCP verb related VERB
VA adjective ADJ
EP, EF, EC, ETN, ETM verbal endings PART
JKS, JKC, JKG, JKO, JKB, JKV, JKQ, JX, JC postpositions (case markers) ADP
XPN, XSN, XSA, XSV suffixes PART
IC interjection INTJ
SF, SP, SE, SO, SS punctuation marks PUNCT
SW special characters x
SH, SL foreign characters Xx
SN number NUM
NA, NF, NV unknown words x

Table 3.1: POS tags in the Sejong corpus and their 1-to-1 mapping to Universal POS
tags

3.1.1.2 MWTs in UD

Multiword token (MWT) annotation has been accommodated in the CoNLL-U for-
mat, in which MWTs are indexed with ranges from the first token in the word to the

last token in the word, e.g. 1-2. These have a value in the word form field, but have


58 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

an underscore in all the remaining fields. This multiword token is then followed by
a sequence of words (or morphemes). For example, a Spanish MWT vdémonos (‘let’s
go’) from the sentence vémonos al mar (‘let’s go to the sea’) is represented in the
CoNLL-U format as in Figure 3.la.?_ Vémonos which is the first-person plural present
imperative of ir (‘go’) consists of vamos and nos in MW'T-style annotation. In this
way, we annotate the Korean eojoel as MWTs. Figure 3.1b shows that naseossda
(‘became’) in Korean can also be represented as MWTs, and all morphemes including
a verb stem and inflectional-modal suffixes are separated. Sag et al. (2002) defined
the various kinds of MWTs, and Salehi et al. (2016) presented an approach to de-
termine MWT types even with no explicit prior knowledge of MWT patterns in a
given language. Coltekin (2016) describes a set of heuristics for determining when to
annotate individual morphemes as features or separate syntactic words in Turkish.
The two main criteria are (1) does the word enter into a labelled syntactic relation
with another word in the sentence (e.g. obviating the need for a special relation for
derivation); and (2) does the addition of the morpheme entail possible feature class

(e.g. two different values for the Number feature in the same syntactic word).

3.1.2. A new annotation scheme

This section describes a new annotation scheme for Korean. We propose a conversion
method for the existing UD-style annotation of the Sejong POS tagged corpus to the

new scheme.

?The example copied from http: //universaldependencies.org/format .html


3.1. POS TAGGING 59

1-2. vamonos) _

1 vamos ir (‘go’)
2 nos nosotros (‘us’)

(a) vdmonos (‘let’s go’)

18-20 naseossda

18 naseo naseo (‘become’)
19 e0ss eoss (‘PAST’)
20 da da (‘IND’)

(b) naseossda (‘became’)

Figure 3.1: Examples of MWTs in UD

3.1.2.1 Conversion scheme

The conversion is straightforward. For one-morpheme words, we convert them into
word index, word form, lemma, universal POS tag and Sejong POS tag. For multiple-
morpheme words, we convert them as described in §3.1.1.2: word index ranges and
word form followed by lines of morpheme form, lemma, universal POS tag and Sejong
POS tag. For the lemma of suffixes, we use the Penn Korean treebank-style (Han
et al., 2002) suffix normalisation as described in Table 3.2. Figure 3.2 shows an
example of the proposed CoNLL-U format for the Sejong POS tagged corpus. As
previously proposed for Korean Universal Dependencies, we separate punctuation
marks from the word in order to tokenize them, which is the only difference from the
original Sejong corpus which is exclusively based on the eojeol (that is, punctuation
is attached to the word that precedes it). One of the main problems in the Sejong
POS tagged corpus is ambiguous annotation of symbols usually tagged with SF, SP,
SE, SO, SS, SW. For example, the full stop in naseo/VV + eoss/EP + da/EF +
./SF (‘became’) and the decimal point in 3/SN + ./SF + 14/SN (‘3.14’) are not


60

CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

word form lemma
verbal ending i. 2
=] 22]
case marker 7i Oo] (‘NOM’)
= = (‘acc’)
= 2 (‘AUX’)

Table 3.2: Suffix normalisation examples

distinguished from each other. We identify symbols whether they are punctuation

marks using heuristic rules, and tokenize them.

# sent _id = BTAA0001-00000012

SpaceA fter=No

# text = EAS] AJz{el pt capo] Aube S7-e7} Ay AAs 4]
1-2 BHeAS|
1 mepA mepA PROPN NNP
2 (S| Q ADP JKG
3-6 AAA 21 _ _ _
3 AA AAI NOUN NNG
4 Al Al PART XSN
5 °} °] VERB VCP
6 ie 2 PART ETM
7 oA} ojAt NOUN NNG
8 czpo]y czpo]-y]) NOUN NNG
9 au} aju}t-a PROPN NNP
10-11 7} 7} _ _ _
10 Q7e 27H PROPN NNP
1 7h 7k ADP JKS
12 Ald Aly NOUN NNG
13-14 44-8 _ _ _
13 Ayal APA] NOUN NNG
14 2 2 PART XSN
15 AS AS NOUN NNG
16-17 zooqe _ _ _
16 cz-o]Yy Uzbo]-y] ~=NOUN NNG
ig E E ADP JKB
18-20 44lc} _ _ _
18 uA uy VERB VV
19 g ie PART EP
20 uf ut PART EF
21 © PUNCT SF

= 7p] Ur.
peurangseu-ui (‘France-GEN’)
peurangseu (‘France’)

-ut (‘-GEN’)

segye-jeok-i-n (‘world class-REL’)
segye (‘world’)

-jeok (‘-SUF’)

-i (“copP’)

-n (‘-REL’)

uisang (‘fashion’)

dijaineo (‘designer’)
emmanuel (‘Emanuel’)
unggaro-ga (‘Ungaro-NoM’)
unggaro (‘Ungaro’)

-ga (‘-NOM’)

silnae (‘interior’)
jangsikyong (‘decoration’)
jangsik (‘decoration’)

-yong (‘usage’)

jikmul (‘textile’)

dijaineo-ro (‘designer-AJT’)
dijaineo (‘designer’)

-ro (‘-AJT’)

naseo-eoss-da (‘become-PAST-IND)
naseo (‘become’)

-e08s (‘PAST’)

-da (‘-IND)

Figure 3.2: The proposed CoNLL-U style annotation with multi-word tokens (MWT)
for morphological analysis and POS tagging


3.1. POS TAGGING 61

3.1.2.2 Experiments and results

For our experiments, we automatically convert the Sejong POS-tagged corpus into
CoNLL-U style annotation with MWE annotation for eojeols. We evaluate tokeni-
sation, morphological analysis, and POS tagging results using UDPipe (Straka and
Strakova, 2017). We obtain 99.88% f, score for segmentation and 94.75% accuracy
for POS tagging for language specific POS tags (Sejong tag sets). Previously, Na
(2015) obtained 97.90% and 94.57% for segmentation and POS tagging respectively
using the same Sejong corpus. While we outperform the previous results including
Na (2015), it would not be the fair to make a direct comparison because the pre-
vious results used a different size of the Sejong corpus and a different division of
the corpus.*? Jung et al. (2018) showed 97.08% f, score for their results (instead of
accuracy). They are measured by the entire sequence of morphemes because of their

seq2seq model. Our accuracy is based on a word level measurement.

3.1.2.3. Comparison with the current UD annotation

There are currently two Korean treebanks available in UD v2.2: the Google Korean
Universal Dependency Treebank (McDonald et al., 2013) and the KAIST Korean
Universal Dependency Treebank (Chun et al., 2018). For the lemma and language-
specific POS tag fields, they use annotation concatenation using the plus sign as
shown in Figure 3.3. We note that Sejong and KAIST tag sets are used as language-

specific POS tags, respectively. However, while the current CoNLL-U style UD anno-

3Previous work often used cross validation or a corpus split without specific corpus-splitting
guidelines. This makes it difficult to correctly compare the POS tagging results.


62 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

tation for Korean can simulate and yield POS tagging annotation of the Sejong cor-
pus, they cannot deal with NER or SRL tasks. For example, a word like peurangsewut
(‘of France’) is segmented and analysed into peurangseu/PROPER NOUN and wi/GEN.
The current UD annotation for Korean makes the lemma peurangseu+ui and makes
NNP+JKG language-specific POS tag, from which we can produce Sejong style POS
tagging annotation: peurangseu/NNP+ui/JKG. While a named entity peurangseu
(‘France’) should be recognised independently, UD annotation for Korean does not
have any way to identify entities by themselves without case markers. In addition,
as we described in §3.1.1.1 the number of POS patterns of the word which is used
in the language-specific POS tag field does not converge. Recall that the language-

specific POS tag is the sequence of concatenated POS tags such as NNP+JKG or

NNG+XSN+VCP+ETM. The number of these POS patterns is exponential because
of the agglutinative nature of words in Korean. However, this poses a significant chal-
lenge for system implementation if we aim to process the entire Sejong corpus, which
contains over 50,000 tags and tag combinations. This increases the search space and

may result in memory overload issues.*

3.2 Named Entity Recognition

Due to the linguistic features of the name entities (NE) in Korean, conventional eo-
jeol-based segmentation, which makes use of whitespaces to separate phrases, does

not produce ideal results in named entity recognition (NER) tasks. Most language

4This section is based on "A New Annotation Scheme for the Sejong Part-of-speech Tagged
Corpus" by Jungyeul Park and Francis Tyers, published in Proceedings of the 138th Linguistic An-
notation Workshop, pages 195-202 (Park and Tyers, 2019).


3.2. NAMED ENTITY RECOGNITION 63

# sent _id = BTAA0001-00000012
# text = EYAS} AAjAel py cjapoly auppel 7-7} AY AAR BS cjapoys yale.
1 ZeAQ eA] PROPN NNP+JKG _
2 Allie! AJAI+4+0]+L NOUN  NNG+XSN+VCP+ETM _
3 oly o|4t NOUN NNG _
4 CAFO] LT] UT Zzf°]U] NOUN NNG _
5 nfo aap] PROPN NNP _
6 Q7;e7} Q7}E+7} PROPN NNP+JKS _
7 Al] aly NOUN NNG _
8 ARAL ARAL 4-2 NOUN NNG+XSN _
9 ae ae NOUN NNG _
10 GAYS Aoy+s NOUN NNG+JKB _
ll Ua UA+ 9404 VERB VV+EP+EF SpaceAfter=No
12 ‘ PUNCT SF

Figure 3.3: The current CoNLL-U style UD annotation for Korean. It is based on
other agglutinative languages such as Finnish and Hungarian in Universal Depen-
dencies. It separates punctuation marks for tokenisation.

processing systems and corpora developed for Korean use eojeol delimited by whites-
paces in a sentence as the fundamental unit of text analysis in Korean. This is
partially because the Sejong corpus, the most widely-used corpus for Korean, em-
ploys eojeol as the basic unit. The rationale of eojeol-based processing is simply
treating the words as they are in the surface form. It is necessary for a better format
and a better annotation scheme for the Korean language to be adapted. In partic-
ular, a word should be split into its morphemes. To capture the language-specific
features in Korean and utilize them to boost the performances of NER models, we
propose a new morpheme-based scheme for Korean NER corpora that handles NE
tags on the morpheme level based on the CoNLL-U format designed for Korean, as in
Park and Tyers (2019). We also present an algorithm that converts the conventional
Korean NER corpora into the morpheme-based CoNLL-U format, which includes
not only NEs but also the morpheme-level information based on the morphologi-
cal segmentation. The contributions of this study for Korean NER are as follows:

(1) An algorithm is implemented in this study to convert the Korean NER corpora


64 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

to the proposed morpheme-based CoNLL-U format. We have investigated the best
method to represent NE tags in the sentence along with their linguistic properties
and therefore developed the conversion algorithm with sufficient rationales. (2) The
proposed Korean NER models in the paper are distinct from other systems since our
neural system is simultaneously trained for part-of-speech (POS) tagging and NER
with a unified, continuous representation. This approach is beneficial as it captures
complex syntactic information between NE and POS. This is only possible with a
scheme that contains additional linguistic information such as POS. (3) The pro-
posed morpheme-based scheme for NE tags provides a satisfying performance based
on the automatically predicted linguistic features. Furthermore, we thoroughly in-
vestigate various POS types, including the language-specific XPOS and the universal
UPOS (Petrov et al., 2012), and determine the type that has the most effect. We
demonstrate and account for the fact that the proposed BERT-based system with
linguistic features yields better results over those in which such linguistic features

are not used.

3.2.1 Representation of NEs for the Korean language

The current Universal Dependencies (Nivre et al., 2016, 2020) for Korean uses the
tokenized word-based CoNLL-U format. Addressing the NER problem using the
annotation scheme of the Sejong corpus or other Korean language corpora is diffi-

cult because of the agglutinative characteristics of words in Korean.® They adopt

> Although the tokenized words in Korean as in the Penn Korean treebank (Han et al., 2002) and
the eojeols as the word units in the Sejong corpus are different, they basically use the punctuation
mark tokenized eojeol and the surface form eojeol, respectively. Therefore, we distinguish them as
word (or eojeol)-based corpora from the proposed morpheme-based annotation.


3.2. NAMED ENTITY RECOGNITION 65

the eojeol-based annotation scheme which cannot handle sequence-level morpheme
boundaries of NEs because of the characteristics of agglutinative languages. For
example, an NE emmanuel unggaro (PERSON) without a nominative case marker in-
stead of emmanuel unggaro-ga (‘Emanuel Ungaro-NOM’) should be extracted for the
purpose of NER. However, this is not the case in previous work on NER for Korean.

We propose a novel approach for NEs in Korean by using morphologically sep-
arated words based on the morpheme-based CoNLL-U format of the Sejong POS
tagged corpus proposed in Park and Tyers (2019), which has successfully obtained
better results in POS tagging compared to using the word-based Sejong corpus.
While Park and Tyers (2019) have proposed the morpheme-based annotation scheme
for POS tags and conceived the idea of using their scheme on NER, they have not
proposed any practical method to adopt the annotation scheme to NER tasks. As
a result, existing works have not explored these aspects such as how to fit the NE
tags to the morpheme-based CoNLL-U scheme and how the NEs are represented in
this format. Our proposed format for NER corpora considers the linguistic char-
acteristics of Korean NE, and it also allows the automatic conversion between the
word-based format and the morpheme-based format.® Using the proposed anno-
tation scheme in our work as demonstrated in Figure 3.4, we can directly handle
the word boundary problem between content and functional morphemes while us-
ing any sequence labeling algorithms. For example, only peurangseu (‘France’) is
annotated as a named entity (B-LOC) instead of peurangseu-ui (‘France-GEN’), and

,

emmanuel unggaro (‘Emanuel Ungaro-) instead of emmanuel unggaro-ga (‘Emanuel

°Details of the automatic conversion algorithm between the word-based format and the
morpheme-based format are addressed in §3.2.2.


66 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

Ungaro-NOM’) as B-PER and I-PER in Figure 3.4.

In the proposed annotation scheme, NEs are, therefore, no longer marked on the
eojeol-based natural segmentation. Instead, each NE annotation corresponds to a
specific set of morphemes that belong to the NE. Morphemes that do not belong to
the NE are excluded from the set of the NE annotations and thus are not marked
as part of the NE. As mentioned, this is achieved by adapting the CoNLL-U format
as it provides morpheme-level segmentation of the Korean language. While those
morphemes that do not belong to the NE are usually postpositions, determiners, and
particles, this does not mean that all the postpositions, determiners, and particles
are not able to be parts of the NE. An organization’s name or the name of an artifact
may include postpositions or the name of an artifact may include postpositions or
particles in the middle, in which case they will not be excluded. The following NE

illustrates the aforementioned case:

1

2 ‘ _ SpaceA fter=No

34 Beyygs _ _ peuropesyeoneol-ut

3 Beye B-AFW peuropesyeoneol (‘professional’)
4 2| LAFW -ut (‘-GEN’)

5 Az) ILAFW  SpaceAfter=No  wonchik (‘principle’)

6 , _ SpaceA fter=No

7 2 _ _ -eun (‘-TOP’)

8

The NE 2ByA 22] 44! peuropesyeoneol-ui wonchik (‘the principle of profes-
sional’) is the title of a book belonging to AFW (ARTIFACTS/WORKS). Inside this

NE, the genitive case marker -u7, which is a particle, remains a part of the NE.


3.2. NAMED ENTITY RECOGNITION 67

Because these exceptions can also be captured by sequence labeling, such an anno-
tation scheme of Korean NER can provide a more detailed approach to NER tasks
in which the NE annotation on eojeol-based segments can now be decomposed to
morpheme-based segments, and purposeless information can be excluded from NEs

to improve the performance of the machine learning process.

# sent_id = BTAA0001-00000012
# text = EGA AMAIA2) AE A Zzpo]y] Mapa S7tH7t A] B48 AS Zzpo|ye
UAltt.
1-2 wegAY] _ _ _ _
1 maa maa PROPN NNP B-LOC _ peurangseu (‘France’)
2 2] 2] ADP JKG _ _ -ui (‘GEN’)
3-6 AAA _ _ _ _ _
3 AMA] AIA NOUN NNG _ _ segye (‘world’)
4 Al Al PART XSN _ _ -jeok (‘-SUF’)
5 °] 2] VERB VCP _ -t (“COP’)
6 Le 2 PART ETM _ _ -n (‘-REL’)
7 oJAP o}Ay NOUN NNG _ _ uisang (‘fashion’)
8 UT] Zzf°] 4] tZzpo]ey] NOUN NNG _ _ dijaineo (‘designer’)
9 Aop+-a) ao}+-e] PROPN NNP B-PER _ emmanuel (‘Emanuel’)
0-11 7}=7} _ _ _ _ _
10 SIE S7}e PROPN NNP I-PER _ unggaro (‘Ungaro’)
11 7 7k ADP JKS _ _ -ga (‘-NOM’)
2 aly Aly NOUN NNG _ _ silnae (‘interior’)
13-14 4A4)-B _ _ _ _ _
13 Ayal AYA] NOUN NNG _ _ jangsik. (‘decoration’)
4 2 2 PART XSN _ -yong (‘usage’)
1 4s cabs NOUN NNG _ Z jikmul (‘textile’)
16-17 Uzoqe _ _ _ _ _
6 Apo] yy tzpo]uy NOUN NNG _ _ dijaineo (‘designer’)
7 = E ADP JKBO _ -ro (“AJT’)
18-20 441t} _ _ _ _ SpaceAfter=No
8 UA] UA) VERB VV _ _ naseo (‘become’)
19 3 gq PART EP _ _ -eoss (‘PAST’)
20 u} u} PART EF _ _ -da (‘-DECL’)
1 PUNCT SF 7

Figure 3.4: CoNLL-U style annotation with multiword tokens for morphological
analysis and POS tagging. It can include BIO-based NER annotation where B-LOC
is for a beginning word of LOCATION and I-PER for an inside word of PERSON.


68 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

3.2.2 Experiments

3.2.2.1 Data

The Korean NER data we introduce in this study is from NAVER, which was orig-
inally prepared for a Korean NER competition in 2018. NAVER’s data includes
90,000 sentences, and each sentence consists of indices, words/phrases, and NER
annotation in the BIO-like format from the first column to the following columns,
respectively. However, sentences in NAVER’s data were segmented based on eojeol,
which, as mentioned, is not the best way to represent Korean NEs.

We resegment all the sentences into the morpheme-based representation as de-
scribed in Park and Tyers (2019), such that the newly segmented sentences follow the
CoNLL-U format. An eoj2morph script is implemented to map the NER annotation
from NAVER’s eojeol-based data to the morpheme-based data in the CoNLL-U for-
mat by pairing each character in NAVER’s data with the corresponding character in
the CoNLL-U data, and removing particles and postpositions from the NEs mapped
to the CoNLL-U data when necessary. The following presents an example of such
conversions:

NAVER (proposed) CONLLU
1-2 BHA peurangseu-ui
1 waeAel B-LOC = 1 Hawa B-LOC  peurangseu (‘France’)
2 S| _ -ut (‘-GEN’)
In particular, the script includes several heuristics that determine whether the
morphemes in an eojeol belong to the named entity the eojeol refers to. When

both NAVER’s data that have eojeol-based segmentation and the corresponding NE


3.2. NAMED ENTITY RECOGNITION 69

annotation, and the data in the CoNLL-U format that only contain morphemes,
UPOS (universal POS labels, Petrov et al. (2012)) and XPOS (Sejong POS labels as
language-specific POS labels) features, are provided, the script first aligns each eojeol
from NAVER’s data to its morphemes in the CoNLL-U data, and then determines
the morphemes in this eojeol that should carry the NE tag(s) of this eojeol, if any.
The criteria for deciding whether the morphemes are supposed to carry the NE tags
is that these morphemes should not be adpositions (prepositions and postpositions),
punctuation marks, particles, determiners, or verbs. However, cases exist in which
some eojeols that carry NEs only contain morphemes of the aforementioned types.
In these cases, when the script does not find any morpheme that can carry the NE
tag, the size of the excluding POS set above will be reduced for the given eoyeol.
The script will first attempt to find a verb in the eojeol to bear the NE annotation,
and subsequently, will attempt to find a particle or a determiner. The excluding set
keeps shrinking until a morpheme is found to carry the NE annotation of the eojeol.
Finally, the script marks the morphemes that are in between two NE tags represent-
ing the same named entity as part of that named entity (e.g., a morpheme that is
between B-LOC and I-LOC is marked as I-LOC), and assigns all other morphemes an
“O” notation, where B, I, and O denote beginning, inside and outside, respectively.
Because the official evaluation set is not publicly available, the converted data in the
CoNLL-U format from NAVER’s training set are then divided into three subsets,
namely the training, holdout, and evaluation sets, with portions of 80%, 10%, and
10%, respectively The corpus is randomly split with seed number 42 for the baseline.

In addition, during evaluation of neural models, we use the seed values 41-45, and


70 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

report the average and their standard deviation.

We implement a syl2morph script that maps the NER annotation from syllable-
based data (e.g., KLUE and MODU) to the data in the morpheme-based CoNLL-U
format to further test our proposed scheme. While the eoj2morph script described
above utilizes UPOS and XPOS tags to decide which morphemes in the eojeol should
carry the NE tags, they are not used in sy12morph anymore, as the NE tags annotated
on the syllable level already excludes the syllables that belong to the functional
morphemes. Additionally, NEs are tokenized as separate eojeols at the first stage
before being fed to the POS tagging model proposed by Park and Tyers (2019). This
is because the canonical forms of Korean morphemes do not always have the same
surface representations as the syllables do. Because the syl2morph script basically
follows the similar principles of eoj2morph, except for the two properties (or the lack

thereof) mentioned above, we simply present an example of the conversion described

above:
KLUE (proposed) CONLLU
1 = B-LOC 1-2 BHA peurangseu-ui
2 42 JLLOC = 1 ada B-LOC _ peurangseu (‘France’)
3 2 LLOC 2 2| _ -ut (‘-GEN’)
19

We further provide morph2e0j and morph2syl1 methods which allow back-conversion
from the proposed morpheme-based format to either NAVER’s eojeol-based format or
the syllable-based format, respectively. The alignment algorithm for back-conversion
is simpler and more straightforward given that our proposed format preserves the

original eojeol segment at the top of the morphemes for each decomposed eojeol. As a


3.2. NAMED ENTITY RECOGNITION val

result, it is not necessary to align morphemes with eojeols or syllables. Instead, only
eojeol-to-eojeol or eojeol-to-syllable matching is required. The morph2e0j method
assigns NE tags to the whole eojeol that contains the morphemes these NE tags be-
long to, given that in the original eojeol-based format, eojeols are the minimal units
to bear NE tags. The morph2syl method first locates the eojeol that carries the NE
tags in the same way as described above for morph2e0j. Based on the fact that NEs
are tokenized as separate eojeols by syl2morph, the script assigns the NE tags to

each of the syllables in the eojeol.

Back-conversions from the converted datasets in the proposed format to their
original formats are performed. Both eojeol-based and syllable-based datasets turned
out to be identical to the original ones after going through conversions and _ back-
conversions using the aforementioned script, which shows the effectiveness of the
proposed algorithms. Manual inspection is conducted on parts of the converted data,
and no error is found. While the converted dataset may contain some errors that
manual inspection fails to discover, back-conversion as a stable inspection method
recovers the datasets with no discrepancy. Therefore, we consider our conversion and
back-conversion algorithms to be reliable. On the other hand, no further evaluation
of the conversion script is conducted, mainly because the algorithms are tailored in
a way that unlike machine learning approaches, linguistic features and rules of the
Korean language, which are regular and stable, are employed during the conversion

process.


72 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

3.2.2.2 Experimental setup

Our feature set for baseline CRFs is described in Figure 3.5. We use crft++’ as
the implementation of CRFs, where crf++ automatically generates a set of feature

functions using the template.

# Unigram

W_2 P-2
W_1 P-1

wo (current word) po (current pos)
WI PL

W2 P2

# Bigram

w2/w_1 p—2/p-1
w_1/wo p-1/Po
wo/W1 Po/P1
w/w P1/p2

Figure 3.5: CRF feature template example for word and pos

We apply the same hyperparameter settings as in Lim et al. (2018) for BiLSTM
dimensions, MLP, optimizer including ', and learning rate to compare our results
with those in a similar environment. We set 300 dimensions for the parameters
including LSTM, Q, and MLP. In the training phase, we train the models over
the entire training dataset as an epoch with a batch size of 16. For each epoch,
we evaluate the performance on the development set and save the best-performing
model within 100 epochs, with early stopping applied.

The standard F, metric (= 2- 75) is used to evaluate NER systems, where

"https: //taku910.github.io/crfpp


3.2. NAMED ENTITY RECOGNITION 73

precision (P) and recall (R) are as follows:

retrieved named entities M relevant named entities

P=
retrieved named entities

retrieved named entities M relevant named entities

R= =
relevant named entities

We evaluate our NER outputs on the official evaluation metric script provided by

the organizer.®

3.2.3 Results

We focus on the following aspects of our results: (1) whether the conversion of Korean
NE corpora into the proposed morpheme-based CoNLL-U format is more beneficial
compared to the previously proposed eojeol-based and syllable-based styles, (2) the
effect of multilingual transformer-based models, and (3) the impact of the additional
POS features on Korean NER. The outputs of the models trained using the proposed
morpheme-based data are converted back to their original format, either eojeol-based
or syllable-based, before evaluation. Subsequently, all reported results are calculated
in their original format for fair comparisons, given the fact that the numbers of tokens
in different formats vary for the same sentence. Nevertheless, it is worth noting that
all experiments using the morpheme-based CoNLL-U data are both trained and

predicted in this proposed format before conducting back-conversion.

Shttps: //github.com/naver/nlp- challenge


74 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

3.2.3.1 Intrinsic results on various types of models

We compare the intrinsic results generated by different types of models. By saying
“intrinsic”, it implies that the results in this subsection differ owing to the ways and
approaches of learning from the training data. We compare the performances of the
baseline CRF and our proposed neural models, and we also investigate the variations
when our models use additional features in the data.

Table 3.3 summarizes the evaluation results on the test data based on the pro-
posed machine learning models. Comparing the transformer-based models with
LSTM+crF, we found that both mutlingual BERT-based models (BERT-MULTI,
XLM-ROBERTA) outperformed LSTM+crr. The comparison reveals a clear trend:
the word representation method is the most effective for the NER tasks adopting our
proposed scheme. For the LSTM+CRF model, we initialized its word representation,
vw, with a fastText word embedding (Joulin et al., 2016). However, once we initial-
ized word representation using BERT, we observed performance improvements up to
3.4 points with the identical neural network structure as shown in Table 3.3. Mean-
while, there are two notable observations in our experiment. The first observation is
that the CRF classifier exhibits slightly better performance than the MLP classifier,
with an improvement of 0.23, for XLM-ROBERTA. However, the improvement through
the use of the CRF classifier is relatively marginal compared with the reported re-
sults of English NER (Ghaddar and Langlais, 2018). Moreover, when comparing the
multilingual models (BERT-MULTILINGUAL and XLM-ROBERTA) to the monolingual
model (KLUE-ROBERTA), we found that KLUE-ROBERTA outperforms both BERT-

MULTILINGUAL and XLM-ROBERTA. This is because KLUE-ROBERTA is trained solely


3.2. NAMED ENTITY RECOGNITION 75

on Korean texts and utilizes better tokenizers for the Korean language (Park et al.,

2021).
baseline LSTM BERT-MULTI XLM-ROBERTA KLUE-ROBERTA
CRF +CRF +MLP +CRF +MLP +CRF +MLP +CRF

Table 3.3: CRF /Neural results using different models using NAVER’s data converted
into the proposed format: fastText (Joulin et al., 2016) for LSTM+CRF word embed-
dings.

Table 3.4 details results using various sets of features. We use incremental words,
UPOS, and XPOS, for the input sequence x and their unigram and bigram features
for CRFs. Both LSTM and XLM-RoBERTa achieve their best F; score when only

the +UPOS feature is attached.

baseline CRF LSTM+CRF XLM-ROBERTA+CRF
WORD WORD +UPOS WORD +UPOS +XPOS
71.50 84.7640.29 84.9440.34 | 88.1640.33 88.4l40.27 88.37+40.22

Table 3.4: CRF/Neural results using the various sets of features using NAVER’s
data converted into the proposed format.

3.2.3.2 Extrinsic results on different types of data

This subsection examines the extrinsic results given different types of data, whereas
the previous subsection focuses on the differences in various models given only the
morpheme-based CoNLL-U data. In this subsection, the performances of our models
trained on the datasets either in the proposed format or in their original formats,
and in either the BIO tagging format or the BIOES tagging format, are investigated.

As described in Table 3.5, both the baseline CRF-based model and the BERT-

based model achieve higher F; scores when the proposed CoNLL-U data are used, in


76 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

contrast with NAVER’s eojeol-based data. The testing data are organized in a way
that the total number of tokens for evaluations remains the same, implying that the
F, scores generated by conlleval are fair for both groups. This is realized by con-
verting the model output of the morpheme-based CoNLL-U data back into NAVER’s
original eojeol-based format such that they have the same number of tokens. The
morpheme-based CoNLL-U format outperforms the eojeol-based format under the
CRFs, whereas the CoNLL-U format still outperformed the eojeol-based format by

over 1% using the BERT-based model.

Table 3.6 presents the comparative results on two types of tagging formats where
the models do not use additional POS features. Previous studies show that the
BIOES annotation scheme yields superior results for several datasets if the size of
datasets is enough to disambiguate more number of labels (Ratinov and Roth, 2009).
Both the baseline CRF and neural models achieve higher F, scores than that of the
BIOES tagging format when the BIO tagging format is used, which has two more
types of labels — E as endings, and S as single entity elements. Our result reveals
that adding more target labels to the training data degrades model prediction (14
labels x 2 for E and S). Accordingly, we observe that in this specific case, adding

the two additional labels mentioned increases the difficulty of predictions.

Table 3.7 compares the results of the proposed morpheme-based CoNLL-U data
and the syllable-based data. We use the XLM-ROBERTA+CRF model and only use
word features for the experiments. Similar to the previous experiments, we back-
convert the model output of the morpheme-based data back to the syllable-based

format for fair comparisons. The results are consistent that for all four datasets,


3.2. NAMED ENTITY RECOGNITION TF

we observe performance improvement ranging from 3.12 to 4.31 points when the
CoNLL-U format is adopted.

The performance difference between syllable-based NER results and morpheme-
based NER results is mainly due to the fact that the XLM-ROBERTA+CRF model we
used employs a subword-based tokenizer with larger units, rather than a syllable-
based tokenizer. Therefore, one needs to use a BERT model with a syllable-based
tokenizer for fair comparisons, if the goal is to only compare the performance between
the morpheme-based format and the syllable-based format. However, this makes it
difficult to conduct a fair performance evaluation in our study, because the evalua-
tion we intended is based on BERT models trained in different environments when
morpheme-based, eojeol-based, or syllable-based data is given. Since subword-based
tokenizers are widely employed in a lot of pre-trained models, our proposed format
would benefit the syllable-based Korean NER corpora in a way that not only lan-
guage models using syllable-based tokenizers can take them as the input, but those
using BPE tokenizers or other types of subword-based tokenizers can also be trained

on these syllable-based corpora once converted.?

baseline CRF XLM-ROBERTA+CRF
CONLLU- NAVER CONLLU NAVER
71.50 49.15 | 88.1649.33  86.72+40.49

Table 3.5: CRF/Neural result comparison between the proposed CoNLL-U format
versus NAVER’s eojeol-based format using NAVER’s data where POS features are
not applied.

°This section is based on "Korean named entity recognition based on language-specific features"
by Yige Chen, KyungTae Lim and Jungyeul Park, published in Natural Language Engineering,
Cambridge University Press, 30(3), 625-649 (Chen et al., 2024b).


78 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

baseline CRF XLM-ROBERTA+CRF
BIO BIOES BIO BIOES
71.50 70.67 | 88.16+40.33 85.70+0.35

Table 3.6: CRF/Neural result comparison between BIO versus BIOES annotations
using NAVER’s data converted into the proposed format where POS features are not
applied.

MODU 19 MODU 21 KLUE ETRI
CONLLU SYLLABLE | CONLLU SYLLABLE | CONLLU SYLLABLE | CONLLU SYLLABLE
88.03+40.20 84.9l+0.35 | 81.7240.31 78.10+0.45 | 91.72+40.29 88.1540.42 | 97.5940.12 93.28+0.37

Table 3.7: Result comparison between the proposed CoNLL-U format and the
syllable-based format using MODU (19 & 21), KLUE, and ETRI datasets where
POS features are not applied (Model:XLM-ROBERTA+CRF).

3.3 Dependency Parsing

In this study, we propose a morpheme-based scheme for Korean dependency parsing
that is developed based on Park and Tyers (2019), and adopt the proposed scheme to
Universal Dependencies (Nivre et al., 2016, 2020), which contains two Korean depen-
dency parsing treebanks, namely the GSD treebank (McDonald et al., 2013) and the
Kaist treebank (Choi et al., 1994; Chun et al., 2018). While the two Korean treebanks
meet the standards of Universal Dependencies and have been studied for dependency
parsing tasks extensively, the treebanks are formatted in a way that the natural
segmentations of Korean texts are preserved, and even with some morpheme-level
information, only the language-specific part-of-speech tags on the morpheme level
are included in the treebanks, and both treebanks do not have any morpheme-level
parsing tags. Different from the traditional scheme based on natural segmentation,

this scheme utilizes the inherent morphological and typological features of the Korean


3.3. DEPENDENCY PARSING 79

language, and the morpheme-level parsing tags can therefore be derived using a set of
linguistically motivated rules, which are further used to produce the morpheme-level
dependency parsing results and automatic conversions between the morpheme-based
format and the traditional format. The proposed morpheme-based representation
is examined using several dependency parsing models, including UDPipe (Straka
et al., 2016; Straka and Strakova, 2017) and Stanza (Qi et al., 2020). Compared
to the baseline models trained using the two treebanks without modification, our
proposed format makes statistically significant improvements in the performances of

the parsing models for the Korean language as reported in the error analysis.

3.3.1 Representation of MORPHUD

(mmod)
{acl}
pound compound) {advel} {punct}
| \

=gA oO Al Ao] LO Gao auhpal ge 7} AW AA 8 Ae ody eB uy go
peurangseu -ui segye -jeok -i -n wisang dijaineo emmanuel unggaro -ga  silnae  jangsik -yong jikmul dijaineo -ro naseo -eoss -da .
France -GEN world -SUF -COP -REL fashion designer Emanuel Ungaro -NOM interior decoration usage textile designer -AJT become -PAST -IND .
1 2 é 6 7 8 9. 10 i 12 13 4 15 16 17 18 19 20 21

Figure 3.6: Example of morpheme-based universal dependencies for Korean: while
dependencies in top-side are the original dependencies between words, dependencies
in bottom-side are newly added dependencies for between morphemes.

In this study, we adopt a morpheme-based format that captures the linguistic
properties of the Korean language proposed by Park and Tyers (2019). The natural
segmentation of Korean is based on eojeol, which does not necessarily reflect the
actual word or morpheme boundaries of the language. For example, an eojeol of

Korean may contain both a noun and its postposition, or both a verb and its par-


80 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

ticles marking tense, aspect, honorifics, etc. While this is typical for Korean as an
agglutinative language, it creates difficulties and challenges for NLP tasks regarding
the Korean language, including dependency parsing. It is not ideal that the tokens
dependency relations are annotated on are sometimes words, and sometimes phrases
as an eojeol may consist of more than a word. Furthermore, Korean as an agglutina-
tive language has very regular conjugations, which makes it easy and natural to split
those words and phrases into morphemes when analyzing the language since nearly

every piece of an eojeol can be identified to be of a certain meaning or function.

The morpheme-based format aims at decomposing the Korean sentences further
into morphemes, which means that dependency relations are no longer marked on
the eojeol level. Instead, they are marked on morphemes such that within each
eojeol that is not monomorphemic, a head of that eojeol will be found and all other
morphemes will be attached directly to the head. As a result, the head of a non-
monomorphemic eojeol carries the dependency relation this eojeol originally has, and
all other morphemes will be attached to it. In order to find the head, we develop a
script and apply some heuristics which include that the head of an eojeol is usually
a noun, a proper noun, or a verb, and while there is no noun or verb in an eojeol,
the script we implemented continues to find other morphemes such as pronouns,
adjectives, adverbs, numerals, etc. The script also excludes the use of adpositions,
conjunctions, and particles as heads in most cases, unless these are the only part-of-
speeches in an eojeol except for punctuations. While there are multiple morphemes
that can be heads in an eojeol, the script will decide which one to take based on

the part-of-speeches of the morphemes. For instance, when there are multiple nouns,


3.3. DEPENDENCY PARSING 81

the last one will carry the dependency relation of the eojeol, whereas when there are
multiple verbs, the first verb will carry the dependency relation as Korean is a head-
final language. Once the head of a non-monomorphemic eojeol is found, the other
morphemes will be dependent on the head and be assigned with other dependency

relations such as compound, case, auxiliary depending on their UPOS and XPOS.

3.3.2 Experiments and results

3.3.2.1 Data and systems

In this study, we deploy two parsers to evaluate our proposed format, namely UDPipe
(Straka et al., 2016) as a baseline system and Stanza (Qi et al., 2020) as one of the
state-of-the-art dependency parsers. UDPipe is a pipeline designed for processing
CoNLL-U formatted files, which performs tokenization using Bi-LSTM, morpholog-
ical analysis, part-of-speech tagging, lemmatization using MorphoDiTa (Strakova
et al., 2014), and dependency parsing using slightly modified Parsito (Straka et al.,
2015). Since the whole pipeline needs no language-specific knowledge, which means
that it can be trained using corpora in a different scheme, we choose UDPipe as
our baseline. Stanza is another natural language processing toolkit that includes
Dozat’s biaffine attention dependency parser (Dozat and Manning, 2017). Dozat’s
dependency parser uses the minimum spanning tree algorithm that can deal with
non-projectivity dependency relations, and more importantly it excelled all of de-
pendency parsers during CoNLL 2017 and 2018 Shared Task (Zeman et al., 2017,

2018) In this study, the two dependency parsing pipelines take both the original


82 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

word-based form’? (the current scheme adopted by Universal Dependencies), which
we denote as WORDUD, and the morpheme-based form, which we denote as MOR-

PHUD, of the GSD and KAIST treebanks as the input.

We develop the script to convert between the WORDUD format and our proposed
MORPHUD format. The script consists of two major components, which are WOR-
DUD to MORPHUD (Word2Morph) and MORPHUD to WORDUD (Morph2Word). The
Word2Morph component splits the word tokens in the CoNLL-U treebank of Korean
into morphemes using the lemmas already provided, and assigns dependency relations
on the resegmented tokens based on the original dependency relations annotated on
the word tokens. The Morph2Word component, on the other hand, firstly pairs the
tokens in the WORDUD dataset and the MORPHUD dataset, and then assigns the
dependency relations from morpheme tokens in MORPHUD to word tokens in WOR-
DUD. Within both components, a root detector for the word is implemented in order
to find the root (or stem) of a word when the word is multimorphemic (i.e., needs
to be split into morphemes and attach dependency relations on it correspondingly).
Evaluations of the conversion scripts are not conducted in this study, since the mor-
phemes are inherited from the lemmas in the treebanks, and the part-of-speech tags,
roots, and dependency relations are predicted and assigned to the morphemes based
on the linguistic features and the grammar of Korean that are regular, as presented

in Section 3.3.1.

l0While words and eojeols are not the same in Korean based on their definitions, in this study,
the terms “word-based” and “eojeol-based” are interchangeable.


3.3. DEPENDENCY PARSING 83

3.3.2.2 Results

We report the labeled attachment score (LAS), which is a standard evaluation met-
ric in dependency parsing, using the evaluation script (2018 version) provided by
CoNLL 2018 Shared Task.'! Table 3.8 shows results of udpipe as a baseline sys-
tem and stanza as one of the state-of-the-art systems. All results are reported in the
WORDUD format. That is, all experiments are trained and predicted in the proposed
MORPHUD format, and then the result is converted back to the WORDUD format
for comparison purposes. We train udpipe once because it can produce the same
parsing model if we train it on the same machine. For stanza, we provide average
LAS and its standard deviation after five training and evaluation. Both systems
use the finely crafted 300d embedding file by fastText (Bojanowski et al., 2017):
WORDUD and MORPHUD use words and morphemes as their embedding entries, re-
spectively to make sure that their input representation would be correctly matched.
For embeddings, there are 9.6M sentences and 157M words (tokenized) based on
WORDUD. The set of documents for embeddings includes all articles published in
The Hankyoreh during 2016 (1.2M sentences), Sejong morphologically analyzed cor-
pus (3M), and Korean Wikipedia articles (20201101) (5.3M). As expected, all results

of MORPHUD outperform WORDUD in Table 3.8.

3.3.2.3 Error analysis and discussion

Figure 3.7 shows the confusion matrix between WORDUD and MORPHUD, in which

the column and the row represent the arc direction of gold and system, respectively.

Unttps://universaldependencies.org/conl118/conl1118_ud_eval.py


84 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

ko_gsd ko_kaist
WORDUD +MORPHUD WORDUD +MORPHUD
udpipe 70.90 77.01 77.01 81.80
stanza | 84.63 (£0.18) 84.98 (40.20) | 86.67 (+0.17) 88.46 (+0.14)

Table 3.8: Dependency parsing results: for the comparison purpose all MORPHUD
results are converted back to WORDUD after training and predicting with the format
of MORPHUD

MORPHUD outperforms WORDUD in predicting all directions except for right (gold)
/ left (system). The system predicts the left arc instead of the correct right arc
(212 are direction errors in WORDUD vs. 240 in MORPHUD). This is because we
spuriously added left arcs for functional morphemes in MORPHUD where the system

learned more left arc instances during training.

L R O L R O
L L
R R
O O

(a) WORDUD (b) MORPHUD

Figure 3.7: Confusion matrix for the direction of arcs where the column represents
gold, and the row system: Left, Right, and O for TO ROOT.

Figure 3.8 presents the confusion matrix for the arc depth. The most frequent arc
depth error is 2 (gold) / 1 (system) (901 arc depth errors in WORDUD vs. 855 in
MORPHUD). Figure 3.9 shows an example of parsing errors generated by MaltEval
(Nilsson and Nivre, 2008). The parsing error shows that whereas the gold’s arc
requires the depth 2, the system predicts the depth 1. This is mainly because the
analysis of compound nouns for the NP modifier in the Korean treebank prefers

a left skewed tree as shown in Figure 3.10 where some nouns are a verbal noun,


3.3. DEPENDENCY PARSING

0123 45 6 7 8 9

OONDOTBEWNrF CO

Ke
oO

(a) WORDUD

10

012 3 4 5 6 7 8 9 10
0

1

2

3 a
4

5

6

7

8

9

10

(b) MORPHUD

89

Figure 3.8: Confusion matrix for the depth of arcs where the column represents gold,

and the row system.

eee
File Settings Navigate Help

MaltEval Tree Viewer 1.0.1

Searchin: ~:Choose data:- J Search by: -:Choose search strategy:- © Search for: Negate Search |. Result: ® search direction: << | >>
|Gold-standard: stanza_models/conllu_dir_wd/UD_Korean-Kaist/ko_kaist-ud-test.conl
root
ccomp
nsubj compound
nmod obj aux fixed compound | obj punct
aie iis - -
<ROOT> 22/7 22] 240] HSS 2 + UES SH 2A ALS AAs
0 1 2 3 4 5 7 8 9 10 1 12
Parsed 1: stanza_models/conilu_dir_mor_plus/UD_Korean-Kaist/ko_kaist-ud-test-sys-wordUD.conll
root
nsubj ccomp
nmod | obj \ aux fixed compound compound obj punct
y v V VoWwoilv) fy
© = ai —
<ROOT> Jzj]u g2| #40] Hae HES SH a1 AES AAstct
o 1 2 3 4 5 7 8 9 10 1 12
id Sentence rs
| 11 2am a9) A140] wee tt + SER Bel 27] ASR AleIstCt ev sent.
| 12 Salo] siete WMEgeI ofa! fet ml MEE! AHS CHa Bn Ba Chal wleveistct« Next sent.
13 Be 9s iz Booty Maw Ul el okt XO ASE BRON , 7b AP QUI Me Bele AIusietC.
| 14 SEMB YAGI! , 1 Set SES Bal elel aelobs Betuls ul sisict. PPE err
15 GZe Ar} Mate SACO lel oleteta Hele ele AelsizAct .
| 16 Mp SE o| mol -#5I8i ON Eb CHEZOl 2he}atatCt NeXterer
|

Figure 3.9: Example of the 2/1 error by MaltEval where the gold’s arc depth is

2 and the system’s depth is 1. Note that MORPHUD results are converted back
to WORDUD: geuligo wang-ut chinjog-1 byeoseul-eul hal su issdolog jongchin gwageo
jedo-leul silsihayeossda ‘And they introduced a clan system to make sure that the
king’s relatives can obtain the government position’


86 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

NP-AJT
a
NP NP
eo Se
NP NP
pe

NP NP

a or qe  dzplye
stlnae jangsik-yong gikmul dijaineo-ro

(‘interior’) (‘ornamental’) (‘textile’)  (‘designer-AJT’)

Figure 3.10: Compound noun with a left-skewed tree for NP modifiers in the Korean
treebank

and it plays a role as a predicate of the precedent NP modifier. This is a quite
different from the English treebank where the right skewed tree dominates: [xp [paps
its] [y [yw Micronite] [x [nw cigarette] [xs filters]]]||. This is a well-known problem
when parsing the Korean treebank because it requires the semantics of the noun to
distinguish between the right and the left skewed trees. One possible remedy for this

problem was to build a fully lexicalized parsing system (Park et al., 2013).1”

!2This section is based on "Yet Another Format of Universal Dependencies for Korean" by Yige
Chen, Eunkyul Leah Jo, Yundong Yao, KyungTae Lim, Miikka Silfverberg, Francis M. Tyers and
Jungyeul Park, published in Proceedings of the 29th International Conference on Computational
Linguistics, pages 5432-5437 (Chen et al., 2022).


3.4. FRAMENET PARSING 87

3.4 FrameNet Parsing

3.4.1 Korean FrameNet dataset

The dataset we use was originally developed and published by KAIST (Park et al.,
2014; Kim et al., 2016; Hahm et al., 2018), and it includes multiple sources from
which the data are collected. We choose parts of the whole dataset originating from
three sources for the purpose of this study, which are the Korean FrameNet data
from Korean PropBank (PKFN), the Japanese FrameNet (JKFN), and the Sejong
Dictionary (SKFN). While the Korean FrameNet data from the English PropBank
(EKFN) is also available, we noticed that the tokenization scheme does not agree with
other datasets, and decided not to adopt it to the current study. Table 3.9 introduces
statistics describing the distribution of the lexical units (LUs). Table 3.10 presents
the number of frames per LU, which measures the degree of ambiguity in the lexical
units within the three subsets. Table 3.11 shows the total number of sentences and
instances in each subset, in which identical sentences with different frames count as

a single sentence but as separate instances.

# of LUs PKFN JKFN  SKFN
Noun 0 755 0

Verb 644 500 =2,252
Adjective 6 155 0
Others 0 14 0

Total 650 1,424 2,252

Table 3.9: Distributions of the lexical units (LUs) of the targets in 3 Korean
FrameNet datasets. An LU is a word with its part-of-speech.


88 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

7# of frames per LU) PKFN JKFN  SKFN
Noun 0 1.109 0

Verb 1.183 1.276 = 1.274
Adjective 1.167 1.290 0
Others 0 1.286 0

Overall 1.183 1.189 1.274

Table 3.10: The number of frames per lexical unit for each of the Korean FrameNet
datasets

PKFN JKFN SKFN

# of sentences 1,767 1,357 5,703

7# of instances 2,350 2,919 5,703

7 of frames per sentence 1.330 2.151 1.000

Table 3.11: Numbers of sentences and instances in the 3 Korean FrameNet datasets.

PKFN The PKFN data in the Korean FrameNet dataset was sourced from the Ko-
rean PropBank.’’ The dataset contains mainly verbal targets, along with a few
adjectival targets. Figure 3.11 illustrates how a single sentence is labeled in the Ko-
rean PropBank and the Korean FrameNet dataset respectively, where the FrameNet
annotation inherits the predicate-argument relation from PropBank and re-analyzes

the sentence using frame semantics.

=o] ete] Belz] ap |B tol) EZ] AA] aa...
bughan-i gugmubuut teleojiwongug myeongdan-e ppajige doemyeon ...
‘North Korea-NOM’ ‘Department of State’s list of state sponsors of terrorism-OBL‘ ‘exclude’ ‘if/when’ ...
PropBank  (thing-excluded) arc1 (excluded-from) 4 xc2 (exclude) prep
|
FrameNet Theme Source Removingtarcsr

Figure 3.11: Comparisons between annotations on the same instance in Korean Prop-
Bank and the Korean FrameNet dataset. The meaning of the above instance is ‘if
North Korea were excluded from the Department of State’s list of state sponsors of
terrorism...’, which is part of a sentence in the Korean PropBank.

Shttps://catalog.1ldc.upenn.edu/LDC2006T03


3.4. FRAMENET PARSING 89

JKFN The JKFN data, as presented in Kim et al. (2016), was projected from the
Japanese FrameNet (Ohara et al., 2003). Given the syntactic similarities between
Korean and Japanese, the JKFN data are direct and literal translations from the
original word chunks separated by frame data in the Japanese FrameNet, in which
way the projected JKFN data preserves the boundaries of the frames (Kim et al.,
2016) as shown in Figure 3.12. The dataset contains a large number of nominal
targets and a considerable number of verbal targets, whereas adjectival targets are

also present in the dataset.

IRAE DY fas C aT a 1 WS.

shogakusei-ga aoshingo-de odan hodo-o wataru
Japanese FrameNet Theme Path Path_shapetarcer

L

Korean FrameNet Theme Path Path_shaperarcer

BEANO] thet SO] QyerS Adc.

chodeunghagsaeng-i palan bul-e hoengdanbodoleul geonneonda
‘elementary school students-NOM’ ‘green light-OBL’ —‘crosswalk-ACC’ ‘cross’

Figure 3.12: Comparisons between annotations on the same instance in the Japanese
FrameNet dataset and the Korean FrameNet dataset. The meaning of the above
instance is ‘elementary school students cross a crosswalk on the green light’.

SKFN The SKFN data is based on the example sentences in the Sejong dictionary.
The major characteristic that differentiates SKFN from the above two subsets is that
the example sentences in the dictionary are usually short, and as a result, a sentence
in the SKFN data carries a single frame only. All frame targets in SKFN are verbs
with no exception. Figure 3.13 presents an example of the frame-based information
in the Sejong dictionary and how its example sentence is annotated in the FrameNet
data. Note that ©] (-i) denotes any nominative particle in Sejong Dictionary. As

a result, X corresponds to the nominative noun phrase jeo salam-eun (that person),


90 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

and Y corresponds to the event uli il-e (out affairs), in the example. The boundaries
of frame arguments cannot be inherited from the original source because the Sejong
dictionary did not explicitly specify such boundaries. Instead, automatic detection
and mapping between frame elements and arguments for the frame of the given

predicate are conducted.

Sejong 7}9}s}t} (gaeibhada, to intervene)
Frame: X=NO0-°] Y=N1-o]] V
X: AGT (individual|group); Y: LOC (abstract object|event|action)

Al AS AAA +=] Yo 7H SL SEC.
jeo salam-eun sasageongeon ult il-e gaetbhanda
‘that person-TOP’ ‘everything’ ‘our affairs-OBL’ ‘interfere’
FrameNet Participant Manner Event Participation T,rcer

Figure 3.13: Comparisons between the corresponding information in Sejong Dictio-
nary and the annotation in the Korean FrameNet dataset with regard to a single
instance. The meaning of the above instance is “that person interferes in our affairs
constantly and meddles in everything”.

3.4.2 Morphologically enhanced FrameNet dataset

We propose a morpheme-based scheme for Korean FrameNet data that leverages the
linguistic properties of the Korean language. As an agglutinative language, Korean
possesses the feature that the natural segmentation, namely an eojeol, can consist of
both the lexical morpheme and its postposition, such as a particle that marks tense
or case. This poses challenges in Korean FrameNet parsing, as the parser is not able
to distinguish the arguments from their functional morphemes given the eojeol-based
segmentation. In other words, the smallest unit (i.e., eojeol) as a single token is a

mixture of the lexical part and the functional part, and a sequence labeling model is


3.4. FRAMENET PARSING O1

not able to learn from the eojeol-based data and tell what the lexical morphemes are
in an eojeol. Since the lexical morphemes contribute to the semantic meaning of the
eojeol on a large scale and determine the lexical units the targets instantiate and the
semantic frame they evoke, it is essential to separate them from their postpositions

during processing.

As illustrated in Figure 3.14, the sentence is decomposed into morphemes as the
basic unit of tokens. On the other hand, the information on its natural segmen-
tation is preserved by keeping the eojeols at the top of the morphemes that are
split from the corresponding eojeol following the CoNLL-U format. The frames are
therefore annotated on morphemes instead of eojeols, and lexical morphemes and
functional morphemes are split into separate tokens. Although whether a token is
lexical or functional is not explicitly annotated, the morphologically enhanced anno-
tation scheme allows the parser to subconsciously distinguish functional components
from the lexical morphemes that trigger semantic frames. This is in line with the

aforementioned agglutinative feature of the Korean language.

We neither exclude the functional morphemes from the annotated targets or ar-
guments, nor do we introduce additional labels to annotate them. This is because
(1) functional morphemes are parts of the targets/arguments (Park and Kim, 2023)
that a parser should identify (therefore must not be labeled as 0’s), (2) introducing
additional labels would potentially confuse the parser, worsening the model perfor-
mance, and (3) separation between lexical morphemes and functional morphemes
can be performed in postprocessing steps if necessary. Based on the above, we im-

plement a script that automatically converts existing Korean FrameNet datasets into


92 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

the morpheme-based format, and back-converts our morpheme-based format into the
original format. Conversions in both directions rely on alignments between eojeols
and morphemes and assignments of tags on the aligned tokens. The morphologically
enhanced FrameNet datasets are therefore prepared using the aforementioned script

for further experiments.

index word lexeme target frame annotation

16 30 30 _ _ B-Time

17-19  o}\azt _ 7 7 _

17 °F °F _ _ I-Time

18 ral ral _ _ I-Time

19 A Zt _ _ I-Time

20-21 SAE | Z Z _

20 QPAEZOF QAEDOP | _ B-Dependent_ entity
21 =] = _ _ I-Dependent_ entity
22-24 =2|e 7 a ~ 7

22 2) 2] SaAlott}.v Being in control B-FrameTarget

23 of of _ _ I-FrameTarget

24 L & _ _ I-FrameTarget

25-26 2 2}o] _ _ _ _

25 A}9} Zfe] _ _ B-Controlling entity
26 °] °] _ _ I-Controlling entity

Figure 3.14: Example of the morphologically enhanced FrameNet data: 30yeonyeon-
gan oseuteulialeul jibaehan jwaigi... (‘The left wing that ruled Austria for over 30
years...’)

KoELECTRA-Base KR-BERT-char16424
PKFN JKFEN SKFN PKFN JKEN SKFN
exact eojeol | 0.252340.0215 0.3968+ 0.0445 0.8091 + 0.0003 | 0.2964+ 0.0229 0.34934 0.0281 0.8041 + 0.0009
morph | 0.3319+ 0.0807 0.6528+ 0.0135 0.6054+ 0.0056 | 0.3070+ 0.0868 0.6256+ 0.0127 0.5343 + 0.0042
partial eojeol | 0.305140.0224 0.4438+ 0.0444 0.8279+ 0.0003 | 0.3475 + 0.0226 0.4010+ 0.0267 0.8241 + 0.0008
morph | 0.4091 + 0.0694 0.7152+ 0.0096 0.7373 + 0.0047 | 0.4094+ 0.0677 0.6929+ 0.0083 0.6627 + 0.0036

Table 3.12: The cross validation mean plus-minus standard deviation of exact and
partial F scores on eojeol- and morpheme-based variants of PKFN, JKFN and SKFN
datasets.


3.4. FRAMENET PARSING 93

3.4.3 Experiments and results

We perform semantic frame parsing on the proposed datasets and the original datasets
respectively. Specifically, we focus only on the argument extraction task with the
assumption that the frame target and the frame itself have already been given to the
parsers as inputs. This allows us to approach the problem as a sequence labeling task,
where the tokens are the lexical units and the classes are frame elements. We remap
the frame-specific elements into general arguments given that the Korean FrameNet
datasets contain more than 2,000 unique frame elements which are hard to be clas-
sified with the limited instances. Hence, our classification is over five classes: 0,
B-FrameTarget, I-FrameTarget, B-Argument, and I-Argument, following the BIO
tagging scheme. Our parsers are based on the pre-trained KoELECTRA-Base-v3 dis-
criminator model'* and the KR-BERT-char16424 model!’, and are fine-tuned for the
argument detection task using our proposed datasets. The models have their own
tokenizers whereas they process the already segmented eojeols and morphemes from

our proposed datasets. The hyperparameter settings are as follows:

Epochs 3
Learning Rate 5e-5
Batch Size (train) 128
Batch Size (eval) 256

Evaluation Strategy epoch

For evaluation of the parsers’ performance, we use measurements as suggested in

SemEval’13 (Jurgens and Klapaftis, 2013). Specifically, we use the exact F score to

Mnttps://github.com/monologg/KoELECTRA
https: //github.com/snunlp/KR-BERT


94 CHAPTER 3. FROM MORPHOLOGY TO SEMANTICS

choose our best epoch out of three training epochs. The morpheme-based outputs
are converted back into the eojeol-based format for fair comparisons of the results.
The exact and partial F) scores of parsers trained on eojeol- and morpheme-based
data using 2-fold cross-validation is summarized in Table 3.12. It is observed that
the parsers trained on the morpheme-based datasets substantially outperform those
trained on the eojeol-based alternatives with regard to the PKFN and JKFN data. The
disagreement from SKFN may be owning to the fact that the argument boundaries
are not direct inheritances from its source data, as discussed in Section 3.4.1. This
potentially causes some discrepancies within the SKFN dataset, and the discrepancies
further hinder the morpheme-based parsers from obtaining satisfactory performance
since morphemes as smaller units than eojeols are more sensitive to the boundaries.
Overall, we find our proposed scheme an effective approach to representing Korean
FrameNet data as previous work suggested in other Korean language processing tasks.
As future work, resolving the discrepancies within SKFN will necessitate a compre-
hensive strategy. Primarily, it is essential to conduct a more thorough investigation
into the underlying causes of these inconsistencies, as detailed in Section 3.4.1, with
the goal of fortifying the dataset’s reliability. This may involve the refinement of
argument boundary derivation processes or the exploration of alternative methods

to ensure greater precision and consistency in annotations.’®

16This section is based on "Towards Standardized Annotation and Parsing for Korean FrameNet"
by Yige Chen, Jae Ihn, KyungTae Lim, and Jungyeul Park, published in Proceedings of the 2024
Joint International Conference on Computational Linguistics, Language Resources and Evaluation

(LREC-COLING 2024), pages 16653-16658 (Chen et al., 2024a).


Part II

How to Evaluate Results of NLP




Chapter 4

Evaluation by Alignment

4.1 Introduction

Accurate evaluation is a cornerstone in the field of Natural Language Processing
(NLP), playing an important role in determining how effectively a system meets its
intended goals. Traditionally, evaluation in NLP involves comparing system outputs
against gold-standard answers to measure performance. This method has been par-
ticularly useful in assessing component-based systems, where individual systems are
evaluated separately to prevent errors from propagating through subsequent prepro-
cessing steps. However, this traditional approach may not fully capture the complex-
ities of real-world scenarios. The rise of end-to-end systems in NLP, which manage
entire tasks from start to finish without relying on separate modular components, has
highlighted the need for more flexible and robust evaluation methodologies. While
these systems offer streamlined solutions, they also introduce new challenges for
evaluation, particularly when working with real-world data and its outcomes. One

major challenge is that traditional evaluation metrics often assume predefined, con-

97


98 CHAPTER 4. EVALUATION BY ALIGNMENT

sistent sentence boundaries between the gold standard and the system outputs. In
practice, this assumption often does not hold, leading to significant evaluation inac-
curacies and potential failures. Differences in sentence boundaries, especially when
processing raw text, can result in mismatches that traditional evaluation methods

are ill-equipped to handle.

In response to these issues, we introduce a new evaluation algorithm, which we
refer to as the jp-algorithm (jointly preprocessed evaluation algorithm). This al-
gorithm employs an alignment-based approach to improve evaluation accuracy by
directly addressing the mismatches that occur during preprocessing, particularly in
tasks like tokenization and sentence boundary detection (SBD). Drawing inspira-
tion from alignment techniques commonly used in machine translation (MT), our
algorithm simplifies the evaluation process for mismatches that occur during prepro-
cessing tasks, making it both more efficient and accurate for subsequent evaluation of
language processing tasks. This allows us to reevaluate key edge cases, refine preci-
sion, and improve recall for F measures in the final evaluation results. To emphasize
the significance of reevaluating language processing tasks, we begin by addressing
certain overlooked aspects of language processing evaluation. We propose that the
mismatches between sentence and word segmentation be reconsidered to improve
the accuracy of evaluation for NLP systems, particularly those that rely on precise
sentence and word boundaries for downstream tasks like constituency parsing and

grammatical error correction.

In constituent parsing, for example, the EVALB implementation has long been the

standard tool for evaluating parser performance, using PARSEVAL measures (Black


4.1. INTRODUCTION 99

et al., 1991) to compare predicted parse trees against human-labeled reference trees.
A constituent in a hypothesis (system) parse is considered correct if it matches a
constituent in the reference (gold) parse with the same non-terminal symbol and
span. Despite EVALB’s success in evaluating constituency parsing results, it faces
unresolved issues, such as the requirement for consistent tokenization results and
equal-length gold and system parse trees. Similarly, in GEC, evaluation metrics such
as errant‘s F.5 scoring (Bryant et al., 2017; Bryant, 2019) often share the same
limitation: they require consistent sentence boundaries between the gold standard
and system outputs. This requirement is problematic when dealing with raw text
input, where the sentence boundaries in learners’ writing may not align with the

predefined corrections.

The contributions of our work are threefold. First, we introduce an alignment-
based method that enhances the robustness and accuracy of end-to-end evaluation.
Second, we extend the applicability of this method to several NLP tasks, including
preprocessing, constituent parsing, and GEC. Third, we emphasize the real-world
relevance of our approach by addressing the complexities of writing structures, re-
gardless of their preprocessing—such as SBD and tokenization—ensuring reliable
evaluations across diverse text inputs. In conclusion, the jointly preprocessed algo-
rithm represents a significant advancement in NLP evaluation methodologies. By
addressing the limitations of traditional methods, particularly in the context of end-
to-end systems, our approach offers a more accurate and reliable way to assess the
performance of NLP systems. Whether applied to constituency parsing, GEC, or

other NLP tasks where predefined sentence boundaries and consistent tokenization


100 CHAPTER 4. EVALUATION BY ALIGNMENT

results are required, the algorithm ensures that evaluations reflect the true capabili-

ties of the system, taking into account the complexities of real-world data.

4.2 Jointly Preprocessed Evaluation Algorithm

The Jointly Preprocessed (JP) alignment-based evaluation algorithm introduces a
pattern-based approach for sentence and word alignment in a monolingual context.
Algorithm 1 outlines the general procedure of the jp-algorithm, facilitating both
sentence and word alignment processes. The input and output for the jp-alignment

algorithm are defined as follows:

Input £ and FR are unaligned texts.

Output £’ and R’ are aligned texts, with equal lengths of either sentences or words.

£ and FR represent either a list of sentences or words, depending on whether
sentence alignment or word alignment is being performed. L’ and R’ denote the
aligned lists of sentences or words, where LEN(L’) and LEN(7’) are equal. MATCHED
CASES (;,;, may also vary depending on whether sentence alignment or word alignment

is being applied.

4.2.1 Sentence alignment

For the SENTENCEALIGNMENT algorithm, we define the input and output as follows:

Input: CL and FR, representing the list of sentences.


4.2. JOINTLY PREPROCESSED EVALUATION ALGORITHM 101

Algorithm 1 Pseudo-code for JP-alignment

1: function ALIGNMENT (ZL, 7):

2: while £ and R do

3 if MATCHED CASES(,,;) then

4 LPR’ L'+L,,R'+ R;

i else

6 while —=(MATCHED CASES(i+1,;+1) do
7 if LEN(L;) > LEN(R;) then

8
9

Uel'+L,;

; ait
10: else
11: RR+R,;
12: jejrl
13: end if
14: end while
15: LRH L'+ LT R'+R
16: end if

17: end while
18: return CL’, R’

Output: L’ and R’, representing the aligned list of sentences, where both lists have

equal lengths.

We designate two sets of inputs, £ (representing the gold data) and R (repre-

senting the system outputs), where £ is expressed as [L1, Lo, L3,..., Lm] and R as
[R, Ro, R3,...,R,]. After aligning the sentences, we generate aligned document out-
puts such as £L’ = [[Ly], [Lo, Ls],...,[Lm]] and R’ = [[Ri, Ro], [Rs],...,[Rn]], where

LEN(L’) and LEN(R’) are equal. We define the following two cases for MATCHED


102 CHAPTER 4. EVALUATION BY ALIGNMENT

CASES,;,;) in sentence alignment:

Li) = Ry (4.1)

(Liw ~ Ryw) A (Linn = Biri V Lit & Ris) (4.2)

We define that L; and R; can be aligned if their respective sequences, denoted as
Liy and Ry, match when all spaces are removed, as shown in (4.1). This compari-
son accounts for tokenization results in the monolingual context, ensuring sentence
alignment considers both tokenized systems and gold sentences. However, simply
concatenating words by removing spaces is insufficient for comparing tokenized sys-
tems and gold sentences, as tokenization may introduce grammatical morphemes
that are absent in the gold sentence, and vice versa. Therefore, we also allow L; and
R, to be aligned when their respective sequences, Ly and Rjy, exhibit high character
similarity (> a), provided that L;,; and Rj; also match or are sufficiently similar.

We define the similarity measure a as follows:

|LEN(Li4) — LEN(Riy)|
* LEN(Liy) + LEN(Riy)

where LEN denotes the function returning the number of characters.

These two conditions form the sentence matching criteria. If L; and R; cannot
be aligned, we concatenate either L;, Li4; or R;, Rj+1 based on their respective
lengths. If the length of L;.,, exceeds that of R;.,, we concatenate L; and L,,,, and
vice versa if Rj., is longer. This process continues until Lj; and Rj; meet the

sentence matching criteria.


4.2. JOINTLY PREPROCESSED EVALUATION ALGORITHM 103

4.2.2 Word alignment

The WORDALIGNMENT algorithm follows a logic similar to sentence alignment and

is defined as follows for input and output:
Input / and r represent lists of words.
Output /’ and r’ are the aligned lists of words with equal lengths.

Upon aligning the words, we generate aligned word outputs, such as I’ = [[l,], [l2, Is], ..- , [Um]
and r’ = [[ri, 79], [rs],---, [Pn]], where LEN(/') == LEN(r’). This involves accumulat-
ing words in /' and r’ when pairs of |; and r; do not match, often due to tokenization
mismatches. We assume interchangeability between the notations used for sentence
alignment (£) and word alignment (1;). We define the following two cases for matched

CASES(;,;) in word alignment:

(i Ary) A (litt = rj41) (4.5)

When deciding whether to accumulate the token from J;,; or rj; in the case
of a word mismatch, we base our decision on the following condition, rather than
a straightforward comparison between the lengths of the current tokens J; and rj:
(LEN(1) — LEN(lo.;)) > (LEN(r) — LEN(ro..;)). This condition compares the remaining
lengths of the two lists starting from their respective current positions. Specifically,

we calculate the difference between the total length of the list / and the cumulative


104 CHAPTER 4. EVALUATION BY ALIGNMENT

length of tokens up to /; for the left side, and similarly for the right side with r;. If the
remaining length of / is greater than that of r, the algorithm decides to accumulate
the next token from 1,41; otherwise, it accumulates from r;+1.

For word-aligned sentence pairs, we re-index the words after alignment. For exam-

ple, given two sentences [5;,.5;] where S; = [w1, w2,...,Wm] and S; = [wy, w2,...,Wnl,

when they are concatenated, the word indices become [w , W1,..., Wm; Wm+1; Wm42; +++

This re-indexing ensures that each word in the aligned sentences has a distinct index.

For cases where words are concatenated within a sentence, we also re-index. For ex-

ample, if two words [w;, w;] are combined in a sequence like [. . . , [wa], [wi, wy], [wy4i],---

their indices are re-assigned as [... , [w@—1)], [wa,1), Wa,2)|, [(WG)],---], where woz) and
W(,2) represent the re-indexed words in a tuple. In terms of results, we ensure that
both the aligned sentences L’ and R’ have an equal number of sentence lists, and sim-

ilarly, the aligned words / and r have an equal number of word lists after re-indexing.

4.2.3. Proof

We define a perfect sentence alignment pair as Ly = Ry. Additionally, we define
another perfect matched pair as L' 41 = R’ 1, obtained by accumulating unmatched
lines from L and R when Ly 4 Ry. Our goal is to demonstrate that such an alignment

process can lead to L’ and R’, where Ly’ = Ry’.

Soundness The algorithm is sound if, given true premises, it produces a true result,
i.e., it leads to a tautology. According to the Soundness Theorem, Algorithm A is

sound under the following three cases: Let £,R, L’, and R’ be elements of A.

; Wenrnl 7


4.2. JOINTLY PREPROCESSED EVALUATION ALGORITHM 105

Case 1. Algorithm 1 is sound if Ly L implies Ly tau L and Ry - R implies

Ry taut es

Explanation: This case demonstrates that if Lj and Ry, which are the non-
tokenized representations of Z and R, respectively, are consistent with their tokenized

forms, the algorithm holds soundness.

Case 2. Algorithm 1 is sound if (Ly A Ly) © L implies (Ly A Lu) Frau L and

(Ry A Ry) R implies (Ry A Ru) Faw B.-

Explanation: This case shows that combining tokenized (LZ, and R,) and non-
tokenized (Ly and Ry) representations for both L and R still preserves soundness
in the algorithm. If both the tokenized and non-tokenized versions align with the

original, the alignment process remains valid.

Case 3. Algorithm 1 is sound if Ly t+ L' implies Ly Ftau L’ and Ry + R' implies
Ry Sanne R’.

Explanation: This case illustrates that even after accumulating and aligning lines,
the resulting L’ and R’ remain consistent with their original non-tokenized represen-

tations, Ly and Ry. Therefore, the alignment is sound.

Correctness ‘To prove the correctness of the statement L’' = R’, we assume that
the algorithm aligns two sequences L and R such that the final outputs L’ and R’
have equal values.

Let L! = Lj, Lisa,:++ , Ly and R’ = Rj, Rjyi,-++ , Rm for some i, j,n,m € Z*°.


106 CHAPTER 4. EVALUATION BY ALIGNMENT

Case 4. Liy; = Rj. For anyi and j, ifi+1 and j+1 are such thati, j € Z=°, then
Lizi1 and Rj+41 have the same value. This implies that for each step of the algorithm,
if Liz, and Rj41 are equal, they are aligned together. Since both L and R remain in
the domain of the input space, Liz; and Rj, will be accumulated together into L’

and R’.

Case 5. Lnit = Rmii. For anyn andm, ifn+1 andm-+1 are such thatn,m € Z*°,
then Lny, and Ryy1 are aligned. This implies that as the algorithm processes L
and R, the elements Ly+1 and Ry+1 will be accumulated together, ensuring that the

alignment 1s maintained through the process.

Therefore, for all indices i, j7,n,m € Z=°, the output sequences L’ and R’ are correctly

aligned, and L’ = R’ holds.


Chapter 5

Applications of JP-ALGORITHM

5.1 JP-PREPROCESSING

5.1.1 Introduction

Recognizing and adjusting for errors is crucial to achieve dependable outcomes in
most computer-based language tasks. However, this step often does not get the
attention it deserves because there is a common practice of accepting some level of
errors. This lack of focus becomes even more problematic during the initial stages of
text processing, particularly when dealing with tasks like tokenization and sentence
boundary detection (SBD). Given that most commonly used preprocessing methods
are generally recognized as well-established and can be easily reused, people tend to
think that challenges like tokenization and SBD have already been completely solved.
However, what often goes unnoticed is the fact that there has been a longstanding
lack of attention to the evaluation process, which has been underestimated despite

the apparent success of these tools.

107


108 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

In response to these challenges, the literature has seen various efforts aimed at
addressing and improving the limitations of tokenization and SBD. These efforts
include extensive comparisons and detailed reevaluations for tokenization (Dridan
and Oepen, 2012) and for SBD (Read et al., 2012). Although these reevaluations
provide comprehensive and convincing results, they often fall short in offering clear
and adaptable configurations that can be easily re-implemented by others.

When you examine these efforts side by side, it becomes evident that the tasks
of sentence and word segmentation are closely related and complement each other
more than they are typically considered as separate steps. In response to this, some
innovative sequence labeling methods (Evang et al., 2013), have been proposed, show-
ing promising results in terms of reproducibility across various domains. However,
there are still noticeable challenges related to sentence boundaries that may remain

unresolved in the evaluation process.

5.1.2. Mismatches in preprocessing

5.1.2.1 Tokenization

The primary challenge in tokenization, as demonstrated by several methods, revolves
around handling the ambiguity related to sentence periods and contractions (Dridan
and Oepen, 2012). For instance, consider the sentence. When No. 1 Isn’t the Best'
Different tokenization schemes, such as tok.sed?, Moses (Koehn et al., 2007), and

CoreNLP (Manning et al., 2014), tokenize the contraction and abbreviation of com-

‘https: //www.washingtonpost .com/archive/sports/2004/05/26/
when-no-1-isnt-the-best/fa34156f -881a-4181-b0fe-4447e2f36f£0e/
2ftp://ftp.cis.upenn. edu/pub/treebank/public_html/tokenization. html


5.1. JP-PREPROCESSING 109

mon words in this sentence differently.

tok.sed When Now. 1 isLin’t the Best

Moses When No. 1 isn’t the Best

CoreNLP When No. 1 isUn’t the Best

where L! is a symbol for a token delimiter. Typically, periods are treated as individual
tokens in text processing. However, due to the common use of word-final periods
in conjunction with abbreviations, acronyms, and named entities, there is a lack of
consistency in how these cases are handled. This inconsistency results in variations
in tokenization approaches for these words. While there are various tokenization
methods available, CoreNLP results suggest that, in practice, the terminal leaf nodes
in the Penn treebank (Marcus et al., 1993; Taylor et al., 2003) have become a widely

accepted standard tokenization scheme for English.

5.1.2.2 Sentence boundary detection

The task of sentence boundary detection involves identifying the points where sen-
tences start and end. In many prior studies (Palmer and Hearst, 1997; Reynar and
Ratnaparkhi, 1997; Kiss and Strunk, 2006; Gillick, 2009; Lu and Ng, 2010), sen-
tence boundary disambiguation has been treated as a classification problem. It is
important to note that sentence boundary disambiguation is distinct from sentence
boundary detection. The former requires the use of punctuation marks for clas-
sification, while the latter does not necessarily rely on punctuation to determine
sentence boundaries. A significant challenge in sentence boundary disambiguation

arises when dealing with text that lacks consistent punctuation marks. To tackle


110 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

this issue, Evang et al. (2013) proposed a method involving supervised character-
level sequence labeling for both sentence and word segmentation. Additionally, Lim
and Park (2024) annotated a dataset with rich linguistic information to improve
sentence boundary detection. However, many common sentence boundary detec-
tion systems, such as splitta (Gillick, 2009), CoreNLP (Manning et al., 2014), and
Elephant (Evang et al., 2013), often fail to correctly identify sentence boundaries in
cases where there are no punctuation marks between two sentences, even when their
methods are not designed to rely on punctuation marks. For instance, this issue can
be observed between ... session and J... in the following text extracted from the

Europarl parallel corpus (Koehn, 2005):

Opening of the session I declare resumed the 2000-2001 session

of the European Parliament.

Additionally, textual content often exhibits improper formatting, including issues
such as missing or incorrect punctuation, inconsistent casing, and the presence of
unnecessary symbols. For instance, in social media text, individuals may write in all
uppercase or with incorrect punctuation and casing, or entirely omit punctuation. In
the domain of automatic speech recognition (ASR) where there are no punctuation
marks, several SBD-related works have been proposed (Treviso et al., 2017; Gonzalez-
Gallardo and Torres-Moreno, 2018). Punctuation restoration in ASR has also been

explored (Fu et al., 2021; Alam et al., 2020; Poldéek et al., 2023).

5.1.2.3. Evaluation

Methods for evaluating sentence boundary disambiguation and tokenization can be

broadly categorized into three main approaches:


5.1. JP-PREPROCESSING 111

Accuracy and error rate This method assesses how accurately the boundaries are

detected or tokens are separated, and quantifies the rate of errors made.

Precision and recall for F1 Measure This approach focuses on measuring the
precision (how many of the detected boundaries or tokens are correct) and
recall (how many of the actual boundaries or tokens are detected). The F1

measure is then calculated to balance these two factors.

Levenshtein distance This method involves calculating the Levenshtein distance,
which represents the number of edits (insertions, deletions, or substitutions)

needed to transform the detected boundaries or tokens into the correct ones.

These three evaluation approaches help assess the performance and accuracy of sen-
tence boundary detection and tokenization methods.

In previous SBD tasks, such as the one mentioned in Dridan and Oepen (2012),
each character position is considered a potential boundary point. If a gold-standard
boundary is missed, it is counted as a false negative. This imbalance between negative
and positive cases makes precision and recall for F1 evaluation metrics more infor-
mative than accuracy or error rate metrics. To address these issues, character-level
methods aim to facilitate the integration of evaluations between the two tasks, as
proposed by Evang et al. (2013). However, even at the character level, the imbalance
problem persists, especially in cases where there are partial matches, which can lead
to miscounted true positive cases. For example, in Figure 5.1, within the context
of tokenization, a perfect alignment between the system output and the reference
gold standard is observed, yielding an F1 measure of 1.0 (100%). However, when it

comes to sentence boundary detection (SBD), there’s a difference. Using previous


112 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

methods, Click here and To view it. are identified as two separate sentences in SBD,
with two "S" labels marking sentence boundaries under Click and To, as shown in
Figure 5.1. However, the first sentence boundary marked by Click is essentially a
partial match, while the second part is separated into another sentence by the second
sentence boundary marked by To.
Click here To view it.
sys SIIIIOTIIIOSIOTIIIOTIT

GOLD SIIIIOTIIIOTIOTIIIOTIT

Figure 5.1: BIO-style evaluation in previous work where partial matched sentences
(e.g Click here) could be considered as true positive.

The proposed evaluation using the alignment algorithm aims to prevent such
partially matched sentences from being incorrectly counted during evaluation. This
is achieved by incorporating sentence alignment methods borrowed from machine

translation (MT), addressing various preprocessing issues along the way.

5.1.3 Alignment-based evaluation

We begin by employing a fundamental algorithm that leverages sentence and word
alignment techniques typically used in MT. In MT and other cross-language appli-
cations, sentence and word alignment have been crucial sub-tasks. They are often
employed when working with parallel corpora, where sentences in one language need
to be aligned with their translations in another language. This necessity arises from
the fact that when two sentences are direct translations of each other, they can differ
significantly in terms of word order, length, and structure. Therefore, to enable down-

stream cross-language tasks, it’s often essential to perform comprehensive structural


5.1. JP-PREPROCESSING 113

adjustments by establishing sentence and word alignments between the source and
target languages. These alignment methods, used to address inter-lingual differences,
shed light on the challenges we discuss in Section 5.1.2 regarding evaluation.

However, when dealing with monolingual inputs from both the system and gold
results, there are no inter-lingual differences between sequences on each side (L and
R). Instead, these sequences are essentially identical sentences, differing only in
terms of token and sentence boundaries. Nevertheless, when evaluating tokenization
and SBD tasks, they share a commonality with cross-language tasks: for a corre-
sponding sentence pair from the system and gold results, even if they are identical
at the character level, they can still vary in length across lines due to differences
in tokenization and SBD results, as demonstrated in the system result and gold
file in Figure 5.2. This is where the alignment approach used in MT becomes useful
in the evaluation algorithm we propose. Figure 5.2 provides a simplified example of
how our proposed evaluation, facilitated by the alignment process, would resolve the
partial match issue discussed in the previous section.

While Figure 5.2 presents sentence alignment for the evaluation of sentence
boundary detection, a same approach will be employed for tokenization evaluation,

utilizing word alignment.

5.1.3.1 Algorithm description

Notations ‘To describe our proposed algorithms more concisely and clearly, we’ll
use the following notations: £ and R represent the system output and the gold files

for preprocessing, respectively. We assume LEN(£Ly) and LEN(Ry) are equal, where


114 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

INPUT FILE:

Click here To view it. He makes some good]

observations on a few of the picture’s. I]

SYSTEM RESULT:

Click here I

To view itu.

He makes some good observations on a few of the pictureLll’s
GOLD FILE:

Click here To view itlUl. [1
He makes some good observations on a few of the pictureLl’sL.
SENTENCE-ALIGNED RESULT FOR PREPROCESSING EVALUATION:

Click here ~~~ To view itll. [1 Click here To view itu. [1
He makes some good observations on a He makes some good observations on a
few of the pictureLl’sU. fT few of the pictureLl’sUl. [1

Figure 5.2: Example of intermediate results of the evaluation by alignment algorithm
where Click here and To view it. In the system result are the realigned, produced by
merging after the implementation of sentence alignment: LU is a symbol for tokeniza-
tion, [is a symbol for SBD, and ~~~ is a symbol for merged sentences by sentence
alignment.

Ly and Ry are sequences of characters with all spaces removed from £ and R. L
and R denote the current sentence pair from the system output and the gold files.

The following notations are also used interchangeably for both L and R:

C.p(L) and C:z(£): These represent the total number of sentence boundaries (sb)

and the total number of tokens (tk) in L.

TP,» and TPi,: These denote the number of true positives (tp) for sentence bound-

aries and tokens, respectively.

Ly: This represents LZ where spaces between tokens have not been removed.

Ly: This represents LZ where spaces between tokens have been removed.

L,;: It stands for the zth token in L,,.


5.1. JP-PREPROCESSING 115

Evaluation measures ‘To calculate an F1 score for SBD, we need to have the
total counts of sentence boundaries in both the system output and the gold stan-
dard (Cy(L) and C,(R), respectively), as well as the number of true positives for
sentence boundaries (TP,,). Precision and recall, for example, are defined as follows,

considering sentence boundary detection:

relevant # of sb retrieved # of sb

ae retrieved # of sb
_ Cop(L) NCg(R) _ TP sp
C.(L) C.y(L)
rT relevant # of sb retrieved # of sb
recall =

relevant # of sb
— C(L) A Cso(R) TP et

To apply in the algorithm, ‘relevant # of sb M retrieved # of sb’ represents TP,y,
‘retrieved + of sb’ is C,,(£) by a system result, and ‘relevant # of sb’ is C,,(7) by a
gold file. The same precision and recall can be defined for tokenization using Cy,(L),

Ci. (Fe) and TPyy.

Sentence alignment for SBD evaluation The proposed alignment algorithm
processes £ and R and increments the TP,, count if Ly is equal to Ry. However,
when they do not match, the algorithm merges sentences in L’ and R’ for mismatched
sentence pairs until L’ and R’ are identical (practically, this is done until the following
Ly and the following Ry are identical instead of verifying L’ and R’ being identical).

After this merging process, the algorithm continues to increment TP., if Ly is equal


116 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

to Ry.

Word alignment for tokenization evaluation In the context of our evaluation
algorithm, we increment the TP, count in the word-aligned L,, and R,, pair if LD;
is equal to R;, where L; and R; represent the i-th and j-th tokens in Li, and Ry,
respectively. While IBM word alignment can be adapted for token alignment within
our evaluation approach, our focus lies in aligning tokens within the specific sentence
pair expected to be identical, rather than traversing the entire document as IBM
models are designed to find the proper lexical translation pair between sentence
pairs. For our evaluation algorithm, token alignment only needs to traverse each
aligned sentence pair within L,, and R,, directly assessing token equivalence within

the corresponding sentence pair.

Summary Drawing an analogy from sentence and word alignment, we introduced
an alignment-based algorithm that significantly enhances the efficiency of evaluating
sentence boundary detection (SBD) and tokenization results. This algorithm allows
us to evaluate both tasks in a single pass through the input. Through the alignment
of sentences and tokens, we can streamline the evaluation process, resulting in more

precise counts of true positive cases for both tokenization and SBD results.

5.1.3.2 Pseudo-codes

Alignment algorithm The alignment algorithm is employed in two key scenarios:
(a) to align sentences between the system and the gold results, and (b) to align tokens

within the current sentence pair. Once the alignment is established, we can conduct


5.1. JP-PREPROCESSING 117

a side-by-side comparison between the system and gold results. This comparison
allows us to discern and integrate all the intertwined matched and mismatched cases
between sentence boundary detection (SBD) and tokenization. Algorithm 1 provided
a generic representation of the alignment algorithm, which we can apply to both
sentence boundary and tokenization evaluation. The inner while statement, where
mismatches are addressed, is part of an iterative process controlled by the outer while
loop, which iterates through each element. The time complexity of this algorithm is
O(N +N’), where N and N’ represent the lengths of the left and right sentences (L

and R), respectively. Additionally, this algorithm is applicable to tokens.

Implementations We provide two comprehensive implementations of the algo-
rithm: one as a basic algorithm (Algorithm 2) and another as a joint algorithm (Al-
gorithm 3). In Algorithm 2, the evaluation procedure distinguishes between sentence
boundary detection (SBD) and tokenization. Its time complexity is O(2N +4NM),
where N represents the number of sentences and M denotes the number of tokens in
each sentence. We assume that N ~ N’ and M ~ M’ for the left (Z) and right (R)
sentences, respectively. This version simplifies notations by removing indexes 7 and
j. During the alignment of sentences, we obtain TPsb. Subsequently, in the second
outer while loop, we obtain TPtk during word alignment, where Lj ,, represents the
i-th token in the i-th sentence of L’. Both iteration processes resemble those in
Algorithm 1. In contrast, Algorithm 3 processes SBD and tokenization jointly, re-
ducing the time complexity to O(4NM). We maintain the assumption that N ~ N’
and M ~ M’ for L and R.


118 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

Algorithm 2 BAsIc ALGORITHM

1: function BASIC ALGORITHM (ZL, R):

2: Input: £L, R

3: Output: C.(L), C(L), Coo(R), Crn(R), TPsb, TPee
4: Obtain C.,(L), Cin(L), Cop(7e), and Cy,(R) from £L and R
5: while £ and R do

6:  /* Obtain TPs, */

8: TPsp < TPsp + 1

9: LPR L+L,R' +R;

10: else

12: el +t;

13: ROR+R;

14: end while

15: LRH L+ LD R+R

16: end if

17: end while

18: while £’ and R’ do
19: /* Obtain TP, */
20: if Ly, == Rj, then

21: TPix < TPix + LEN(L;U)

22: else

23: while Li, and Rj, do

24: TPix < TPex + 1 if Liv == Rj,
25: end while

26: end if

27: end while
28: return C,,(L), Ciz(L), Cso(R), Cin(F2), TPsv, TPex

Examples Figure 5.3 provides examples to illustrate the jp-algorithm in different
scenarios. In the first case, where Ly matches Ry, we obtain true positive instances
for sentence boundaries, denoted as TP,,. The second case depicts conditions where
true positive sentence boundaries are not present. In such instances, the algorithm

proceeds to build L’ and R’ if a sentence is segmented and requires merging (which we


5.1. JP-PREPROCESSING 119

Algorithm 3 JOINT

ALGORITHM

1: function JOINT ALGORITHM (ZL, R):

2: Input: £L,R

3: Output: Cyo(L), Cin(L), Cso(R), Cir(R), TPsv, TPex
4: Obtain C,,(L), Ciy,(L) from £ and C,,(R), Cu(R) from R
5: while £ and R do

6: /* Obtain TPs, */

8: TPsp < TPsp + 1

9: /* Obtain TP, */
10: if Ly, == Rj, then
11: TPex < TPix + LEN(L)

12: else

13: while Li, and Rj, do

14: TPex © TPex + lif Liv == Rj,

15: end while

16: end if

17: else

18: while Lisi fh Rj+iy do

19: el +t;
20: RCR+R,;
21: end while

22: while Li, and R/, do

23: TPiz < TPy
24: end while
25: end if

26: end while

consider sentence alignment). After merging, these sentences should align correctly,

and we identify the true positives among these pairs. To count the number of true

positives for tokens between L and R, we obtain TP, if L; == R,;. Figure 5.4 provides

a detailed breakdown

of how to evaluate and count the number of true positives (TP)

for tokens. This is done using the same alignment method as previously explained


120 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

for sentence alignment.

L£ (SYSTEM) R (GOLD)
Ly == Ry When No. 1 Isn ’t the Best When No. 1 Is n’t the Best lM TP
Ly # Ry Mike McConnell 07/06/2000 14:57 Mike McConnell
John ,
(before alignment) | Hello from South America . 07/06/2000 14:57 M1
John , Hello from South America .
im
| (after alignment) | Mike McConnell 07/06/2000 14:57 — Mike McConnell ~\~ 07/06/2000, yo]
John , 14:57
Hello from South America. [1 ~ww John , Hello from South
America. [1

Figure 5.3: Examples with the jp-algorithm in Algorithm 3 for true positive of
sentences. If not, merging sentence boundaries.

£ (SYSTEM) | When No. 1 Isn~n~’t the Best
R (GOLD) | When No. 1 Is~n~n’t the Best

Figure 5.4: Examples for true positive of tokens. If not, merging tokens.

Discussion The length-based sentence alignment algorithms, like the one described
by Gale and Church (1993, p.83), typically consider matches in the ratios of 1:0, 0:1,
1:1, 2:1, 1:2, and 2:2. However, we need to account for cases where the system
segments a sentence into more than two sentences or where gold sentence boundaries
are segmented into more than two sentences. In other words, even after using sentence
alignment, the segmented results of Ly and Ry may still differ from each other if
we apply pre-existing sentence alignment algorithms from MT. To address this, we
accumulate and merge L and R together until their characters match, resulting in
L’ == R’ instead of using MT’s alignment algorithm. As described in Algorithm 3,
the ’else’ condition (when Ly # Ry) entails aggregating L’ and R’ to form a matched

sentence pair between L and R. This process allows for the accumulation of pairs


5.1. JP-PREPROCESSING 121

such as m:1, 1:n, or m:n sentence segments.

5.1.4 Experiments and results

5.1.4.1 Case study on English corpora

For our case studies, we conduct preprocessing on five raw input files sourced from
English Universal Dependencies (Nivre et al., 2016, 2020). These files include: (1)
Universal Dependencies syntax annotations from the GUM corpus. (2) A multilingual
parallel treebank known as ParTUT, developed at the University of Turin. (3) A gold
standard Universal Dependencies corpus for English, constructed using the source
material of the English Web Treebank (EWT). (4) The English portion of the parallel
Universal Dependencies (PUD) treebanks. (5) The English half of the LinES Parallel
Treebank. (6) A dataset specifically created for pronoun identification (Pronouns).

We employ the n1tk library (Loper and Bird, 2002; Bird et al., 2009) along with its
word_tokenizer and sent_tokenizer for both sentence boundary detection (SBD)
and tokenization tasks. We also utilize the stanza toolkit (Qi et al., 2020), which is
a natural language processing toolkit based on Dozat’s biaffine attention dependency
parser (Dozat and Manning, 2017). This toolkit includes a standard tokenizer with
built-in sentence boundary detection, enabling us to generate text and CoNLL-U
format? outputs. We use these outputs to evaluate preprocessing results using our
alignment-based evaluation algorithm. Furthermore, we juxtapose our evaluation
findings with those of prior methods, such as the evaluation script utilized in the

CoNLL 2018 Shared Task (Zeman et al., 2018), employing the CoNLL-U format

3https: //universaldependencies.org/format .html


122 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

outputs generated by stanza.

Both nltk and stanza handles tokenization and sentence boundary detection
similarly in some aspects. For instance, they both split words like gift’s into two
tokens, gift and ’s, and treat periods as separate tokens. However, they differ in
their treatment of certain contentious areas, such as words containing dashes. For
example, in the case of search-engine found in EWT, n1tk considers it as one token,
while stanza separates it into three tokens: searchLi-Liengine. Another difference
arises in how they identify sentence boundaries, especially in cases where the text
lacks capitalized beginnings or period endings. This ambiguity is more pronounced
in sentences without clear markers. Table 5.1 presents the results of case studies,
including the numbers of true positives (TP), false positives (FP), and false negatives
(FN) for both sentence boundaries and tokens. These results were obtained using the
proposed alignment-based evaluation algorithm with both nltk and stanza. Addi-
tionally, we provide the results of the CoNLL evaluation script, which was applied to

the SBD and tokenization results produced by stanza and formatted in CoNLL-U

format.
GUM ParTUT EWT PUD LinES Pronouns

TP FP FN] TP FP FN] TP FP FN TP FP FN TP FP FN] TP FP FN

sbd nltk & jp 845 137 251 | 149 2 4 1084 349 993 976 26 24 865 141 170 | 285 0 0

stanza & jp 1038 38 8 144 6 9 1805 160 = 272 998 4 2 912 128 123 | 285 0 0
TS stanza* & conliu | 1038 38 58 | 144 6 9 | 1805 160 272] 998 4 2 | 912 128 123 | 285 0 0

tk nltk & jp 19443 351 4462 | 3345 27 63 | 24176 1109 918 | 20733 251 443) 17571 56 104] 1673 16 32

stanza & jp 19791 118 114 | 3381 20 27 | 24724 286 370) 21162 18 14 | 17517 +380 158 | 1649 28 56
Os stanza* & conllu | 19791 118 114 | 3381 20 27 | 24724 286 370 | 21162, 18 «14 | 17517 380 158 | 1649 28 56

Table 5.1: Numbers of TP (true positive), FP (false positive) and FN (false nega-
tive) using nltk and stanza for sentence boundaries and tokens. The stanza* line
provides result numbers by the CoNLL-U evaluation script.

The previous evaluation method also utilized precision and recall for F1 measures


5.1. JP-PREPROCESSING 123

as evaluation metrics, relying on true positives (TP), false positives (FP), and false
negatives (FN). In the CoNLL 2018 Shared Task evaluation script,* both tokens
and sentences are treated as spans. In the case of a character-level mismatch in the
positions of spans between the system output and the gold file, the script adjusts
by skipping to the next token in the file with the smaller start value until the posi-
tions align. This process is also applied to sentence boundaries. The start and end
values of sentence spans are compared between the system and the gold file, with
matching values incrementing the count of correctly matched sentences (true posi-
tive sentence boundaries). However, in our alignment-based evaluation method, this
process is limited to the aligned sentence pair, ensuring accuracy in the counts. Any
miscounted true positives can significantly impact the evaluation results negatively.
Therefore, we address these issues in the existing evaluation scripts, highlighting
them as sources of mismatches, and suggests adjusted alternatives to enhance the

reliability of sentence preprocessing evaluation.

Due to the differing characters in these representations, our character-level eval-
uation of both nltk and stanza preprocessing results may not capture such cases
accurately. The lack of consensus on tokenization across different corpora, including
Universal Dependencies, contributes to the mismatch issue. Notably, EWT tokenizes
can’t as ca and n’t, while ParTUT tokenizes it as can and not. We identified vari-
ations in the representation of contractions like can’t and ain’t. These contractions
can be expressed in multiple ways, where EWT tokenizes can’t as ca and n't, while

ParTUT tokenizes it as can and not. The same issue can arise when converting

“https: //universaldependencies.org/conl118/con1118_ud_eval.py


124 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

"starting quotes" (‘‘) and "ending quotes" (’’) in the corpus into straight quotes
(") in n1tk, resulting in discrepancies. Since the number of contractions and symbols
to convert, such as quotes, in a language is limited, we have created an exception
list for our system to capture such cases in English. During the final stages of our
implementation, we cross-check against the exception list to ensure that every case
can be correctly handled by the proposed algorithm. As a result, the algorithm ef-
fectively addresses the preprocessing mismatches discussed in Section 5.1.2, which

could otherwise disrupt our evaluation procedures.

5.1.4.2 Case study on Multilingual corpora

While we have addressed tokenization mismatches, such as the representation of
contractions, other tokenization issues may arise from morphological segmentation
or analysis, where additional morphemes can be introduced during the analysis rather
than through mechanical token segmentation. Table 5.2 presents the results of case
studies using seven GSD corpora (McDonald et al., 2013) in UD, which have been
provided by Google. Our results closely align with those suggested by the CoNLL
UD evaluation script, except for French. In French GSD, tokenization often combines
several units into a single token. For example, the expression de 1 000 métres (‘of
1,000 meters’) is treated as three tokens instead of four, as spaces are used to separate
the words. Notably, this discrepancy is not a limitation of the proposed algorithm
but rather stems from differences in tokenization conventions between plain text and
the rich CoNLL-U format. Even when the input text is de 1 000 métres, with 1

and 000 separated, it is tokenized as separated in plain text. Such examples occur


5.1. JP-PREPROCESSING 125

across various treebanks. For example, in UD_Kurmanji-MG, phrases like dagir kiriye

(‘occupied’) are tokenized as a single unit.

DE ES FR ID JA KO PT
TP FP FN| TP FP FN/| TP FP FN[| TP FP FN| TP FP FN{| TP FP FN| TP FP FN
sbd stanza & jp 787 «110 :«:190 | 373 3553 | 393 2B BT 515) 85 42 | B41 52] 629) 1438-360 |] 1161 71 39
_ 4 stanza* & conllu | 787 110190 | 373 35 53 | 393-23 23 | 5Ib B54 | dd 52 F629 143360 | 116i 71389 |
tk stanza & jp 16172, 61 +52 | 11705 33-28 | 9714 17-22 | 115239 18 | 12750 361 284 | 11332 184 345 | 29311 5250
—s stanza* & conllu | 16172 6152 | 11705 33-28 | 9710 19 23 | 11523 9 18 | 12750 B61 284 | 11332 184 345 | 29311 5250 |

Table 5.2: Numbers of TP , FP and FN using stanza for sentence boundaries and
tokens for multilingual case studies using UD_*-GSD where * is German, Spanish,
French, Indonesian, Japanese, Korean, and Portuguese.

5.1.4.3. Discussion and limitation

The effectiveness of the proposed word alignment approach would remain unaffected
even in the presence of significant morphological mismatches. For example, we trace
back to the sentence in Hebrew (Tsarfaty et al., 2012) as a word mismatch example

caused by morphological analyses:

gold 9B Han. MOL ?FL 3HM 49H Aww 41NEIM

in’ the’ ’shadow’ of’ = *them’ the’ ’pleasant’
sys °B LOL, 2FL 3%HM 4 HNEIM
in’ ‘shadow’ ‘of? = ’them’ *made-pleasant’

Pairs of {*°H '* CL, 'CL} (the shadow’) and {*°H *! NEIM, “HNEIM } (’the pleas-
ant’) are word-aligned using the proposed algorithm, and we can obtain 4/5 and 4/7
for precision and recall using the proposed method. The CoNLL evaluation script
is unable to assess such mismatches because the concatenation of tokens in gold file
and in system file differ.” Unfortunately, we were unable to find real-world examples

of morphological mismatches from GSD treebanks. Since stanza has been trained

°This is an actual UDError message.


126 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

on UDs, it would produce UD-friendly results without discrepancies. Therefore, we
view this area as a potential subject for future investigation.

Given the absence of consensus evaluation methods for tokenization and sentence
boundary detection, as well as the lack of direct approaches to evaluate preprocessing
outcomes, a comprehensive comparison with previous work is unfeasible. Instead of
utilizing the plain text format, where conventional preprocessing tools are typically
applied, we opt to perform comparisons using the CoNLL-U evaluation script in the
CoNLL-U format, which offers representational advantages over plain text, such as
representing tokenization results like 1 000 as a single token.

We have expanded our alignment-based evaluation approach to include other
tasks, such as evaluating constituency parsing results. The widely used evalb script
has traditionally been employed for evaluating the accuracy of constituency parsing
results, albeit with the requirement for consistent tokenization and sentence bound-
aries. We align sentences and words when discrepancies arise to to overcome several
known issues associated with evalb by utilizing the ‘jointly preprocessed alignment-
based method (Jo et al., 2024a; Park et al., 2024). The proposed approach will also
be applicable to various sentence-based evaluation metrics, including POS tagging,

machine translation, and grammatical error correction.

5.1.5 Conclusion

While most existing tokenization and sentence boundary detection (SBD) implemen-
tations are generally considered suitable for direct re-implementation, it is important

to note that when they are applied to new use cases, many miscounted true positives


5.2. JP-EVALB 127

are likely to be overlooked. As a result, these inaccuracies remain hidden and not
immediately apparent in the intermediate preprocessing results. However, these text
segmentation tasks play a fundamental role in sentence processing. Any unnoticed
inaccuracies at these early stages can potentially be magnified in downstream NLP
tasks, significantly affecting the entire NLP pipeline. This issue of miscounted true
positives is a largely unacknowledged aspect of sentence preprocessing. By introduc-
ing sentence and word alignments into the proposed pipeline, we can better identify
and reassess such hidden but prevalent inaccuracies in the foundational preprocess-
ing steps. Through the jp-algorithm, we can focus on addressing mismatches that
occur during crucial preprocessing procedures and accurately count the true positives

during evaluation.°®

5.2 JP-EVALB

5.2.1 Introduction

Evaluation is a systematic method for assessing a design or implementation to mea-
sure how well it achieves its goals. In natural language processing (NLP) systems,
quality is assessed using evaluation criteria and measures by comparing them to gold

standard answer keys. In the context of constituent parsers, we evaluate the fitness

°This chapter is based on "An Untold Story of Preprocessing Task Evaluation: An Alignment-
based Joint Evaluation Approach" by Eunkyul Leah Jo, Angela Yoonseo Park, Grace Tianjiao
Zhang, Izia Xiaoxiao Wang, Junrui Wang, MingJia Mao, and Jungyeul Park, published in Proceed-
ings of the 2024 Joint International Conference on Computational Linguistics, Language Resources
and Evaluation (LREC-COLING 2024), pages 1327-1338, Torino, Italia. ELRA and ICCL (Jo
et al., 2024b).


128 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

of our predicted parse tree against the human-labeled reference parse tree in the
test set. For constituent parsing, whether statistical or neural, we rely on the EVALB
implementation’. It uses the PARSEVAL measures (Black et al., 1991) as the stan-
dard method for evaluating parser performance. A constituent in a hypothesis parse
of a sentence is labeled as correct if it matches a constituent in the reference parse
with the same non-terminal symbol and span (starting and end indexes). Despite its
success in evaluating language technology, EVALB faces an unresolved critical issue
in our discipline. EVALB has constraints, such as requiring the same tokenization
results. Its implementation assumes equal-length gold and system files, with one
tree per line. Nevertheless, we evaluate parser accuracy using EVALB’s standard F1
metric for constituent parsing.

Furthermore, in today’s component-based NLP systems, it is common practice
to evaluate parsers individually. This approach helps improve accuracy by prevent-
ing errors from propagating through dependent preprocessing steps. We propose a
new way of constituent parsing evaluation algorithm, which better simulates real-
world scenarios and extends beyond controlled and task-specific settings. Hence, we
propose a new way of calculating PARSEVAL measures, which aim to solve some
limitations of EVALB for more error-free and accurate evaluation metrics. By rectify-
ing its restrictions, we would be able to present refined precision and recall for the
F1 measures in constituent parsing evaluation.

To emphasize the importance of our new methodology, we will first address the

task-specific inherent problems in tokenization and sentence boundary detection be-

"http://nlp.cs.nyu.edu/evalb. There is also an EVALB_SPMRL implementation, specifically
designed for the SPMRL shared task (Seddah et al., 2013, 2014).


5.2. JP-EVALB 129

fore constituent parsing. We will then demonstrate the new implementation of PAR-
SEVAL measures by presenting solutions to each identified mismatch case and their
corresponding algorithms. To ensure the reliability and applicability of these algo-

rithms, we will also conduct additional discussion towards the end of the chapter.

5.2.2. Known problems

To illustrate how we present this new approach, consider some known problems of
EVALB that dictate why this new solution is needed. Firstly, evaluation cannot be
complete if the terminal nodes of the gold and system trees are different, causing a
word mismatch error. An example of this can be found when the gold and system
spans differ on the character level with tokens like This versus this. These tokens
are considered identical if we disregard the distinction made by letter case. Hence,
we can resolve this character discrepancy by converting all letters to lowercase. This
adjustment allows our evaluation system to treat This and this as a matching word
pair.

Secondly, tokens represented as terminal nodes in gold parse trees can differ from
those in parser outputs due to the token and sentence segmentation of the system.
During preprocessing, even with the same sentence boundary, tokenization discrep-
ancies may arise when compared to the gold standard tree from the Penn Treebank.
This mainly occurs when periods and contractions create ambiguity among words
that are abbreviations or acronyms. Such discrepancies can lead to the preprocessing
results diverging into several different tokenization schemes. Importantly, EVALB is

unable to evaluate constituent parsing when the system’s tokenization result differs


130 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

from the gold standard.

Example: gold This can’t be rightU.

system this canLinot be right. where U is a token delimiter.

The discrepancy is evident in such a comparison of can’t (gold) versus cannot
(system) for cannot. In this context, it is readily apparent to human eyes that the
gold and system tokens are actually the same. To handle such an instance that
EVALB cannot manage, we observe that caLin’t and cannot are indifferent to each
other between all tokens when we create the set of constituents. This observation
plays a pivotal role in shaping our approach to address tokenization challenges, and
it is equally significant in resolving issues related to sentence boundaries prior to
constituent parsing.

The mission of a sentence boundary detection system is to recognize where each
sentence starts and ends. A major hurdle in this task is to detect sentence beginnings
and endings given some text that lacks punctuation marks. In the following exam-
ple, although there is no tokenization discrepancy, a sentence boundary discrepancy
exists. In the system, Click here To view it. is perceived as two separate sentences:
Click here and To view it. The previous method proposed by EVALB could not as-
sign a score to the unmatched sentences. However, it is worth noting that there are
partial matches between the gold and system trees, even though the current EVALB

does not consider them.

Example: gold Click here To view itu.

system Click here To view itll. where M is a sentence delimiter.


5.2. JP-EVALB 131

Consequently, tokens undergoing tokenization and sentences handled through sen-
tence boundary detection share a common quality during evaluation. The gold and
system results turn out to be two identical sequences of characters. However, they
may still differ in length across tokens and lines due to the various tokenization and
sentence boundary detection results. Therefore, we suggest the next step beyond
EVALB, re-indexing system lines through sentence and word alignment. As part of
our solution, we propose an evaluation-by-alignment algorithm to avoid mismatches
in sentences and words when deriving constituents for eventual evaluation. The al-
gorithms of the new PARSEVAL measures allow us to reassess such edge cases of

mismatch.

Finally, the question of how to evaluate constituent parsing results from these end-
to-end systems has been a longstanding challenge. Conventionally, EVALB has proven
useful in a component-based preprocessing pipeline, with each component evaluated
individually under ideal circumstances. However, conducting end-to-end evaluations
with all preprocessing in a single pipeline can offer an alternative perspective in
constituent parsing evaluation, and this is the approach adopted for the proposed
new PARSEVAL measures. By addressing the constraints discovered in EVALB that
lead to issues in preprocessing, we create an opportunity to compare end-to-end
parser results. Even when different preprocessing results are produced due to the
use of various models in sentence boundary detection and tokenization, the extension
of the evaluation technique with the new way of calculating PARSEVAL measures

makes this comparison possible.


132 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

5.2.3 Implementing new PARSEVAL measures

Algorithm To describe the proposed algorithms, we use the following notations
for conciseness and simplicity. Tz and Tr introduce the entire parse trees of gold and
system files, respectively. Jc is a simplified notation representing 7c), where / is the
list of tokens in £. This notation applies in the same manner to R. Sz represents
a set of constituents of a tree 7, and C(7) is the total number constituents of T.
C(tp) is the number of true positive constituents where S;, 1 S7,, and we count it
per aligned sentence. The presented Algorithm 4 demonstrates the pseudo-code for
the new PARSEVAL measures.

Algorithm 4 Pseudo-code for new PARSEVAL measures

1: function PARSEVALMEASURES (Jz and Tr):

2: Extract the list of tokens £ and R from Jz and Tr

3: L', R’ <— SENTENCEALIGNMENT(L, R) where LEN(L’) = LEN(R’)
4: Align trees based on L’ and FR’ to obtain Tz and Tp:

5: while Jz and Tp do
6
7
8
9

Extract the list of tokens / and r from Tz, and Tp,
I’, r’ — WORDALIGNMENT(I, 1)
Sy, <- GETCONSTITUENT(T (7), 0) where 0 < i < LEN(L’)
Sjz <- GETCONSTITUENT (Tpi(r1), 0) where 0 < 7 < LEN(R’)
10: C(Tz) — C(Tz)+ LEN(S7,)
11: C(Tr) + C(TR)+ LEN(S7, )

12: while S7, and S7, do

13: if (LABEL, START, ENDz,l;) = (LABEL, STARTR, ENDR,;) then
14: C(tp) — C(tp) +1

1s end if

16: end while

17: end while
1s: return C(7z), C(7r), and C(tp)

In the first stage, we extract leaves £ and R from the parse trees and align


5.2. JP-EVALB 133

Algorithm 5 Pseudo-code for sentence alignment

1: function SENTENCEALIGNMENT (ZL, ?):
2: while £ and R do

3: if (Lia) = Ry) > CASE 1 (1,3)
V (Lia = Raw A (Liv = Ror V Livy = Ry+1y))) > CASE 2 3)

then

4: L',R'+ L'4+L;,R' +R; where 0 <i < LEN(L), 0 <j < LEN(R)

Bs else

ic if LEN(L;) < LEN(R,;) then

9: ie-itl

10: else

11: ROR+R;

12: 7 4-9

is: end if

14: end while

15: LRH L'+D R'+R

16: end if

17: end while
18: return L’, R’



134

CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

Algorithm 6 Pseudo-code for word alignment

1: function WORDALIGNMENT (I, r):

2:
3:

while / and r do

V (i A173) A (liga = 7541)
then

U,r’ &1,,r; where 0 <i < LEN(I), 0 <j < LEN(r)
else
while =(CASE 1 (41,541) V CASE 2 (i41,341)) do
if (LEN(I) — LEN(Io,/;)) > (LEN(r) — LEN(ro,1;)) then
We uti;
~-—titl
else
Tr HTP +7;
jegt+l1
end if
end while
or &llyrr
end if
end while
return [', 7’



5.2. JP-EVALB 135

sentences to obtain L’ and R’ using Algorithm 5. While the necessity of sentence
alignment is rooted in a common phenomenon in cross-language tasks such as ma-
chine translation, the intralingual alignment between gold and system sentences does
not share the same necessity because £ and F are identical sentences that only differ
in sentence boundaries and tokenization results. A notation J is introduced to rep-
resent spaces that are removed during sentence alignment when comparing £2 and
R,, irrespective of their tokenization results. If there is a mismatch due to differences
in sentence boundaries, the algorithm accumulates the sentences until the next pair
of sentences represented as CASE n (i+ 1,j+ 1), is matched. In the next stage of
Algorithm 4, we align trees based on L’ and FR’ to obtain Jz and Tr. By iterat-
ing through Jz and TR, we conduct word alignment and compare pairs of sets of
constituents for each corresponding pair of Tc, and Tri. The word alignment in Algo-
rithm 6 follows a logic similar to sentence alignment, wherein words are accumulated
in lJ and rr if the pairs of J; and r; do not match due to tokenization mismatches.
Finally, we extract a set of constituents using Algorithm 7, a straightforward pro-
cedure for obtaining constituents from a given tree, which includes the label name,
start index, end index, and a list of tokens. The current proposed method utilizes
simple pattern matching for sentence and word alignment, operating under the as-
sumption that the gold and system sentences are the same, with minimal potential
for morphological mismatches. This differs from sentence and word alignment in
machine translation. MT usually relies on recursive editing and EM algorithms due

to the inherent difference between source and target languages.


136 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

Algorithm 7 Pseudo-code for getting constituents

1: function GETCONSTITUENT (7, start):

2: if HEIGHT(7)>2 then

3 END < START + LEN(LEAVES(7 ))

4: Sr + Sy; + (LABEL(7), START, END, LEAVES(7 ))

Ss while T do

6 GETCONSTITUENT(7;, start) where 7; is a child of T
7 START < LEN(LEAVES(7;))

8 end while

9: end if

10: return S7

Word mismatch We have observed that the expression of contractions varies
significantly, resulting in inherent challenges related to word mismatches. As the
number of contractions and symbols to be converted in a language is finite, we
composed an exception list for our system to capture such cases for each language
to facilitate the word alignment process between gold and system sentences. In the
following example, we achieve perfect precision and recall of 5/5 for both because

their constituent trees are exactly matched, regardless of any mismatched words.

(gold) (system)
S(o,4) NPoa, DT °This | °this DT  NPoay 80,4)
VP (1,4) MD !%ca | !°can MD VP (4)
RB }!nt | 1tnot RB
VPow VB 2be | 2be VB VP 2.4)
AdjP(34) JJ 3 right Sright JJ AdjP (3,4)

If the word mismatch example is not in the exception list, we perform the word
alignment. We can still achieve perfect precision and recall (5/5 for both) without
the word mismatch exception list because their constituent trees can be exactly

matched based on the word-alignment of {'°ca 'n’t} and {*°can 1!" not}.


5.2. JP-EVALB 137

gold °This *'°ca '!n’'t *be 3right
system this '°can ‘!not be right
The effectiveness of the word alignment approach remains intact even for mor-
phological mismatches where "morphological segmentation is not the inverse of con-
catenation" (Tsarfaty et al., 2012), such as in morphologically rich languages. For
example, we trace back to the sentence in Hebrew described in Tsarfaty et al. (2012)
as a word mismatch example caused by morphological analyses:
gold °B °H tI'CL ?FL 3HM *°H 41NEIM
‘in’ ’the’ ‘shadow’ of’ ‘them’ ’the’ ‘pleasant’
system °B LOL 2FL °HM 4HNEIM
‘in’ shadow’ ‘of’ ’them’ ’made-pleasant’
Pairs of {*°H !!CL, 'CL} (the shadow’) and {*°H *' NEIM, “HNEIM } (’the pleas-

ant’) are word-aligned using the proposed algorithm, resulting in a precision of 4/4

and recall of 4/6.

(gold) (system)
REGS ‘in’ oR °B "in! Peay
NPas) NPaa) "the? 10
’shadow’ LEO 1CL *shadow’ NPiaiay NPs)
PP (2,2) of? 2 PL 2FL of PP eo)
them’ 3HM SHM them’
AdjPus) the’ 407
*pleasant’ +1NEIM | 4HNEIM  ’made-pleasant’

Sentence mismatch When there are sentence mismatches, they would be aligned
and merged as a single tree using a dummy root node: for example, @S which can
be ignored during evaluation. In the following example, we obtain precision of 5/8

and recall of 5/7.


138 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

(gold) (system, merged after alignment)
Stee) Stas) VB Click | °Click VB VPio2) Se.) @S,o,6)
AdvPq2,) RB there | there RB AdvP(.2)
Sins) VPr2,5) TO ?% | 2% TO VPs) | Sis) Meee
VP(3,5) VB 3view | 3view VB VP (3,5)
NPus) PRP 4it ‘if PRP NPis)

Assumptions ‘To address morphological analysis discrepancies in the parse tree
during evaluation, we establish the following two assumptions: (i) The entire tree
constituent can be considered a true positive, even if the morphological segmentation
or analysis differs from the gold analysis, as long as the two sentences (gold and
system) are aligned and their root labels are the same. (ii) The subtree constituent
can be considered a true positive if lexical items align in word alignment, and their

phrase labels are the same.

5.2.4 Case studies

It’s important to note that the original evalb excludes problematic symbols and
punctuation marks in the tree structure. Our results include all tokens in the given
sentence, and bracket numbers reflect the actual constituents in the system and gold
parse trees. Accuracy in the last column of the result is determined by comparing
the correct number of POS-tagged words to the gold sentence including punctuation
marks, differing from the original evalb which doesn’t consider word counts or cor-
rect POS tags. Figure 5.5 visually depicts the difference in constituent lists between
jp-evalb and evalb. The original evalb excludes punctuation marks from its con-

sideration of constituents, resulting in our representation of word index numbers in


5.2. JP-EVALB 139

red for evalb. Consequently, evalb displays constituents without punctuation marks
and calculates POS tagging accuracy based on six word tokens. On the other hand,
jp-evalb includes punctuation marks in constituents and evaluates POS tagging
accuracy using eight tokens, which includes two punctuation marks in the sentence.
We note that the inclusion of punctuation marks in the constituents does not affect

the total count, as punctuation marks do not constitute a constituent by themselves.

Section 23 of the English Penn treebank Under identical conditions where
sentences and words match, the proposed method requires around 4.5 seconds for
evaluating the section 23 of the Penn Treebank. On the same machine, evalb com-
pletes the task less than 0.1 seconds. We do not claim that our proposed imple-
mentation is fast or faster than evalb, recognizing the well-established differences in
performance between compiled languages like C, which evalb used, and interpreted
languages such as Python, which our current implementation uses. Our proposed
method also introduces additional runtime for sentence and word alignment, a pro-
cess not performed by evalb. We present excerpts from three result files generated
by evalb and our proposed method in Figure 5.6. The parsed results were obtained
using the PCFG-LA Berkeley parser (Petrov and Klein, 2007). It’s worth noting
that there may be slight variations between the two sets of results because evalb ex-
cludes constituents with specific symbols and punctuation marks during evaluation.
However, as we mentioned earlier, jp-evalb can reproduce the exact same results

as evalb for a legacy reason.


140 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

TOP
|
S
eee NO
INTI NP VP
| | | oF |
RB , PRP VBD RB NP
|
0,0No Bhi Biden diag NNP NNP

>4Black °°Monday

(a) Example of the parse tree

(?S’, 0, 8, "No , it was n’t Black Monday .")
(?INTJ’?, 0, 1, ’No’)

(?NP’?, 2, 3, °it’)

(?VP’?, 3, 7, "was n’t Black Monday")

(?NP’?, 5, 7, ?Black Monday’)

(b) List of constituents by jp-evalb

(?S’?, 0, 6, "No it was n’t Black Monday")
(?INTJ’?, 0, 1, "No")

(?NP?, 1, 2, “it")

(?VP?, 2, 6, "was n’t Black Monday")
(?NP’?, 4, 6, "Black Monday")

(c) List of constituents by evalb

Figure 5.5: Difference between jp-evalb and evalb


5.2. JP-EVALB 141

Sent Mt Br Cr Co Tag
ID L St Re Pr Br gd te Br Wd Tg Acc
“1 8 0 100.00 100.00 5 5 5 0 8 7. 87.50—
2 40 O 70.97 73.33 22 31 30 7 40 40 100.00
3 31 O 95.24 95.24 20 21 21 O 31 31 100.00
4 35 0 90.48 86.36 19 21 22 2 35 35 100.00
5 26 0 86.96 86.96 20 23 23 2 26 25 96.15

(a) Example of jp-evalb results considering punctuation marks during evaluation

Sent Mt Br Cr Co Tag
ID L St Re Pr Br gd te Br Wd Tg Acc
“1 8 0 100.00 100.00 5 5 5 0 6 5. 83.33
2 40 O 70.97 73.33 22 31 30 7 37 37 = «100.00
3 31 O 95.24 95.24 20 21 21 OO 26 26 100.00
4 35 0 90.48 86.36 19 21 22 2 32 32 100.00
5 26 0 86.96 86.96 20 23 23 2 24 23 95.83

(b) Example of jp-evalb results with the legacy option, which produces the exact same
results as evalb

Sent Mt Br Cr Co Tag
ID L St Re Pr Br gd te Br Wd Tg Acc
1 8 0 100.00 100.00 5 5 5 0 6 5. 83.33—
2 40 0 70.97 73.33 22 31 30 7 37 37 100.00
3 31 0 95.24 95.24 20 21 21 0 26 26 100.00
4 35 0 90.48 86.36 19 21 22 2 32 32 100.00
5 26 0 86.96 86.96 20 23 23 2 24 23 95.83

(c) Example of the original evalb results

Figure 5.6: Examples of evaluation results on Section 23 of the English Penn treebank

Bug cases identified by evalb We evaluate bug cases identified by evalb. Fig-
ure 5.7 displays all five identified bug cases, showcasing successful evaluation without
any failures. In three instances (sentences 1, 2, and 5), a few symbols are treated as
words during POS tagging. This leads to discrepancies in sentence length because
evalb discards symbols in the gold parse tree during evaluation. Our proposed solu-

tion involves not disregarding any problematic labels and including symbols as words


142 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

during evaluation. This approach implies that POS tagging results are based on the
entire token numbers. It is noteworthy that evalb’s POS tagging results are rooted
in the number of words, excluding symbols. The two remaining cases (sentences 3
and 4) involve actual word mismatches where trace symbols (*-num) are inserted
into the sentences. Naturally, evalb cannot handle these cases due to word mis-
matches. However, as we explained, our proposed algorithm addresses this issue by

performing word alignment after sentence alignment.

Sent Mt Br Cr Co Tag
ID L St Re Pr Br gd te Br Wd Tg Acc
“4 37 +0 77.27 62.96 17 22 27 5 837 30 81.08
2 21 O 69.23 60.00 9 13 15 2 21 17 80.95
3 47 O 77.78 80.00 28 36 35 4 48 43 89.58
4 26 0 33.33 35.29 6 18 17 8 27 19 70.37
5 44 0 42.31 32.35 11 26 34 17 44 33 75.00

Figure 5.7: Evaluation results of bug cases

Korean end-to-end parsing evaluation We conduct a comprehensive parsing
evaluation for Korean, using system-segmented sequences as input for constituency
parsing. These sequences may deviate from the corresponding gold standard sen-
tences and tokens. We utilized the following resources for our parsing evaluation
to simulate the end-to-end process: (i) A set of 148 test sentences with 4538 to-
kens (morphemes) from BGAA0001 of the Korean Sejong treebank, as detailed in
Kim and Park (2022). In the present experiment, all sentences have been merged
into a single text block. (ii) POS tagging performed by sjmorph.model (Park and

Tyers, 2019) for morpheme segmentation.? The model’s pipeline includes sentence

Shttps: //github.com/jungyeul/sjmorph


5.2. JP-EVALB 143

boundary detection and tokenization through morphological analysis, generating an
input format for the parser. (iii) A Berkeley parser model for Korean trained on
the Korean Sejong treebank (Park et al., 2016).°. Figure 5.8 presents the showcase
results of end-to-end Korean constituency parsing. Given our sentence boundary
detection and tokenization processes, there is a possibility of encountering sentence
and word mismatches during constituency parsing evaluation. The system results
show 123 sentences and 4367 morphemes because differences in sentence boundaries
and tokenization results. During the evaluation, jp-evalb successfully aligns even
in the presence of sentence and word mismatches, and subsequently, the results of

constituency parsing are assessed.

Sent Mt Br Cr Co Tag
ID L St Re Pr Br gd te Br Wd Tg Acc
““4 "28" 0 85.71 85.71 18 21 21 3 29 26 89.66.
2 27 O 91.30 84.00 21 23 25 2 28 25 89.29
3 33 O 88.00 88.00 22 25 25 3 35 31 88.57
4 43 0 72.73 72.73 24 33 33 7 43 40 93.02
5 18 0 69.57 84.21 16 23 19 2 19 12 63.16

Figure 5.8: Evaluation results of the end-to-end Korean constituency parsing

5.2.5 Discussion

Complexity The proposed algorithm has a linear time complexity. Sentence and
word alignments require O(J + J), where J and J represent the lengths of the gold
and system sentences or words. The process for constituent tree matches uses tree

traversal algorithm which requires O(N + E) where N is a number of nodes and E

*nttps : //zenodo. org/records/3995084


144 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

is for branches. We retain the same time complexity of the original EVALB by adding

alignment-based preprocessing for mismatches of sentences and words.

Comparison ‘Table 5.3 compares previous parsing evaluation metrics with the pro-
posed algorithm. tedeval (Tsarfaty et al., 2012) is based on the tree edit distance of
Bille (2005), and numbers of nonterminal nodes in system and gold trees. A similar
idea on the tree edit distance has proposed for classifying constituent parsing errors
based on subtree movement, node creation, and node deletion (Kummerfeld et al.,
2012). conllu_eval for dependency parsing evaluation within Universal Dependen-
cies (Nivre et al., 2020) views tokens and sentences as spans. If there is a mismatch of
positions of spans between the system and the gold file on a character level, whichever
file has a smaller start value will skip to the next token until there is no start value
mismatch. Evaluating sentence boundaries also follows similar processes as tokens.
The start and end values of the sentence span are compared between the system
and the gold file. When they match, it increases the count of correctly matched
sentences. sparseval (Roark et al., 2006) uses a head percolation table (Collins,
1999) to identify head-child relations between two terminal nodes from constituent
parsing trees, and calculate the dependency score. We also add an aligning trees
method (Calder, 1997) in our comparison, which performs an alignment of the tree
structures from two different treebanks for the same sentence, both of which employ

distinct POS labels.

A note on constituent parsing Syntactic analysis in the current field of lan-

guage technology has been predominantly reliant on dependencies. Semantic pars-


5.2. JP-EVALB 145

evaluation approach addressing mismatches
tedeval  tree-edit distance based on constituent trees words
conllu_eval dependency scoring words and sentences
sparseval dependency scoring words and sentences
aligning trees constituent tree matches words
: EVALB constituent tree matches not applicable
proposed method constituent tree matches words and sentences

Table 5.3: Comparison to previous parsing evaluation metrics

ing in its higher-level analyses often relies heavily on dependency structures as well.
Dependency parsing and its evaluation method have their own advantages, such as
a more direct representation of grammatical relations and often simpler parsing al-
gorithms. However, constituent parsing maintains the hierarchical structure of a
sentence, which can still be valuable for understanding the syntactic relationships
between words and phrases. Numerous studies in formal syntax have focused on
constituent structures, including combinatory categorial grammar (CCG) parsing
(Lewis et al., 2016; Lee et al., 2016; Stanojevié and Steedman, 2020; Yamaki et al.,
2023) or tree-adjoining grammar (TAG) parsing (Kasai et al., 2017, 2018). Notably,
CCG and TAG inherently incorporate dependency structures. In addition to these
approaches, new methods for constituent parsing, such as the linearization parsing
method (Vinyals et al., 2015; Fernandez-Gonzalez and Gémez-Rodriguez, 2020; Wei
et al., 2020), have been actively explored. If a method designed to achieve the goal
of creating an end-to-end system utilizes constituent structures, it necessitates more

robust evaluation methods for assessing its constituent structure.


146 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

5.2.6 Conclusion

Despite the widespread use and acceptance of the previous implementation of PAR-
SEVAL measures as the standard tool for constituent parsing evaluation, it has a
significant limitation in that it requires specific task-oriented environments. Con-
sequently, there is still room for a more robust and reliable evaluation approach.
Various metrics have attempted to address issues related to word and sentence mis-
matches by employing complex tree operations or adopting dependency scoring meth-
ods. In contrast, our proposed method aligns sentences and words as a preprocessing
step without altering the original PARSEVAL measures. This approach allows us to
preserve the complexity of the previous implementation of PARSEVAL while intro-
ducing a linear time alignment process. Given the high compatibility of our method
with existing PARSEVAL measures, it also ensures the consistency and seamless inte-
gration of previous work evaluated using PARSEVAL into our approach. Ultimately,
this new measurement approach offers the opportunity to evaluate constituent pars-
ing within an end-to-end pipeline. It addresses discrepancies that may arise during
earlier steps, such as sentence boundary detection and tokenization, thus enabling a

more comprehensive evaluation of constituent parsing.!°

10This chapter is based on "A Novel Alignment-based Approach for PARSEVAL Measures" by
Eunkyul Leah Jo, Angela Yoonseo Park, and Jungyeul Park. Computational Linguistics 2024; doi:
https: //doi.org/10.1162/coli_a_00512 (Jo et al., 2024a), and "jp-evalb: Robust Alignment-
based PARSEVAL Measures" by Jungyeul Park, Junrui Wang, Eunkyul Jo, Angela Park. In
Proceedings of the 2024 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies (Volume 8: System Demonstrations), pages
70-77, Mexico City, Mexico. Association for Computational Linguistics (Park et al., 2024).


5.3. JP-ERRANT 147

5.3. JP-ERRANT

5.3.1 Introduction

In modern natural language processing (NLP), end-to-end systems have become in-
creasingly popular due to their ability to manage entire tasks from start to finish,
offering streamlined and efficient solutions. In this context, evaluation is important
as it allows for consistent and objective assessment of these systems, ensuring they
meet the intended goals without the need for manual intervention. A good evalu-
ation system must be flexible, able to adapt to various tasks and data types, and
robust, providing reliable results even in the face of diverse or unexpected inputs. It
should also align with high-quality standards to accurately measure the effectiveness
of the design or implementation being evaluated. For instance, the CoNLL 2017-
2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies
(Zeman et al., 2017, 2018) demonstrated that a system could take raw text and
parse it into a structured format that shows how words relate to each other in many
languages. This approach is comprehensive, covering everything from identifying
sentence boundaries and breaking the text into words, to labeling parts of speech
and analyzing dependency relationships. Most importantly, the evaluation method
of Universal Dependencies (UD) is designed to accurately measure the performance
of the entire process, even if there are mismatches in sentence boundaries between
the system’s output and the predefined standard. This makes the metric flexible, ro-

bust, and applicable in various settings, accommodating differences that might arise


148 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM
in the preprocessing stages.

Grammatical error correction (GEC) plays an essential role in facilitating effective
communication, supporting language learning, and ensuring the accuracy of written
texts. GEC systems provide automated assistance for both instructors and learn-
ers, making them invaluable tools in educational and professional settings. Over the
years, various NLP systems and methodologies have been developed to enhance the
effectiveness of automated GEC. Alongside these advancements, several evaluation
metrics, including M? (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015), errant
(Bryant et al., 2017; Bryant, 2019), and PT M* (Gong et al., 2022), have been in-
troduced to measure the performance and reliability of these systems, ensuring they
meet the high standards required for accurate grammatical correction. However,
these metrics often share a common limitation: they require predefined, consistent
sentence boundaries between the gold standard—an ideal set of corrections—and the

outputs generated by the system.

When applied to raw text input—reflecting real-world language learners’ writ-
ing scenarios—the current GEC evaluation method suffers due to differing sentence
boundaries detected during preprocessing, where the sentences in learners’ writing
and the predefined corrections might not align. This challenge is similar to issues
faced in other NLP tasks, such as Machine Translation (MT), where sentence align-
ment between source and target sentences is crucial for creating a parallel corpus.
In MT, sentence alignment involves matching sentences in two or more languages,
connecting each sentence in one language to its corresponding sentences in another.

Sentence alignment has evolved over several decades, leading to the development of


5.3. JP-ERRANT 149

various algorithms. Initially, alignment studies relied on a length-based statistical
method (Gale and Church, 1993), which used bilingual corpora to model differ-
ences in sentence length across languages as a basis for alignment. Later advance-
ments included more sophisticated techniques like Bleualign, which uses an iterative
bootstrapping method to refine length-based alignment. Other early approaches im-
proved alignment accuracy by incorporating lexical correspondences, exemplified by
hunalign (Varga et al., 2005) and the IBM model’s lexicon translation approach
(Moore, 2002). More recent efforts, like vecalign (Thompson and Koehn, 2019),
integrate linguistic knowledge, heuristics, and various scoring methods to enhance

alignment efficiency.

Built upon advancements in MT alignment, we propose a refined approach to ad-
dress GEC-specific challenges, particularly in end-to-end evaluation scenarios. The
key contributions of our work are as follows: We introduce an alignment-based
method that significantly improves end-to-end GEC evaluation by addressing sen-
tence boundary discrepancies that often arise during preprocessing, especially when
systems process raw, unsegmented text. Our approach employs an advanced jointly
preprocessed algorithm, overcoming limitations of traditional methods that rely on
predefined sentence boundaries. Moreover, we provide additional enhancements to
GEC evaluation by reimplementing errant: (i) We improve error annotation accu-
racy by replacing spaCy with stanza for language processing, leading to more precise
part-of-speech tagging and dependency parsing (§5.3.4). (ii) We extend our approach
to multilingual contexts, demonstrating its potential for consistent grammatical error

annotation and evaluation across multiple languages (85.3.5).


150 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

Our work aims to enhance the robustness, relevance, and real-world applicabil-
ity of GEC evaluation methodologies. Our approach addresses the complexities of
language learners’ writing in real-world contexts, ensuring reliable evaluations across
diverse text inputs and more precisely reflecting the demands of actual language

usage.

5.3.2 Previous GEC evaluation measures

The MaxMatch (M?) metric identifies the sequence of edits from the input to the
system correction that achieves the maximum overlap with the gold standard edits,
based on Levenshtein distance (Dahlmeier and Ng, 2012). The GLEU metric extends
the BLEU metric used in machine translation (Papineni et al., 2002), modifying the
precision calculation by giving extra weight to n-grams in the candidate text that
align with the reference but not with the source (i.e., the set of n-grams R\S). It also
introduces a penalty for n-grams present in both the candidate and the source but
absent in the reference, referred to as false negatives (S\R) (Napoles et al., 2015).
A novel pretraining-based approach to M? uses BERTScore and BARTScore to
calculate edit scores, allowing assessments based on insights from pretrained metrics.
However, directly applying PT-based metrics often yields unsatisfactory correlations
with human judgments due to an excessive focus on unchanged sentence parts. To
address this, PT-M? has been introduced, leveraging PT-based metrics only for scor-
ing corrected parts, significantly improving correlation with human evaluations and
achieving a state-of-the-art Pearson correlation of 0.95 on the CoNLL14 evaluation

task (Gong et al., 2022).


5.3. JP-ERRANT 151

While these different metrics have their strengths and limitations, currently errant
(ERRor ANnotation Toolkit) is the de facto standard for evaluating GEC. errant
compares error annotations between the gold standard and system m2 files’, calcu-
lating precision, recall, and reporting the Fo,5 score. This score emphasizes precision
over recall, reflecting the importance of providing accurate feedback to language
learners in GEC systems (Bryant et al., 2017; Bryant, 2019). errant addresses an
important limitation of the original M? metric-the tendency to inflate performance
by heavily weighting true positives. Another advantage of errant over other met-
rics is that in addition to providing a score, it also offers detailed error annotation,
which facilitates a deeper analysis of system performance and specific error patterns.
errant has been adapted for multiple languages, including German, Chinese, and
Korean, among others (Boyd, 2018; Hinson et al., 2020; Zhang et al., 2022; Sonawane
et al., 2020; Belkebir and Habash, 2021; Naplava et al., 2022; Katinskaia et al., 2022;
Yoon et al., 2023).

Given the advantages of errant and its widely accepted status as the de facto
standard for GEC evaluation, our work adapted errant by incorporating an alignment-
based preprocessing approach. This adaptation addresses challenges in end-to-end
GEC scenarios, particularly discrepancies in sentence boundaries between the gold
standard and system predictions during preprocessing. Our method ensures accurate
evaluations even with differing sentence boundaries, maintaining errant’s reliability

in real-world GEC applications.

11m2 is acommon format for representing grammatical errors and corrections. For each sentence,
it includes the original tokenized text (the S line), and one or more error annotation lines (A lines).
These A lines contain the position of each error, the error type (or no error), the correction, and
other information. See Figure 5.9 for an example in English.


152 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

5.3.3 Alignment-based errant

We utilize an alignment-based evaluation algorithm to enhance end-to-end GEC eval-
uation measures. Recognizing that sentence boundaries between the gold standard
and system outputs may vary during preprocessing, this algorithm employs sentence
alignment to accurately match sentences from the gold and system GEC results,
ensuring correct evaluations. Consequently, while the fundamental GEC evaluation
measures remain unchanged, they are now applied to sentence-aligned results, im-

proving the accuracy and reliability of the metrics.

We adapt a jointly preprocessed algorithm, where it preprocesses sentence bound-
ary and tokenization between source and target through alignment, as described in
Algorithm 8. This algorithm is specifically designed for environments where gold and
system sentences are nearly identical in a monolingual context. A similar alignment-
based joint preprocessing approach has been shown to be effective in improving
evaluation of constituent parsing (Jo et al., 2024a; Park et al., 2024) and preprocess-
ing tasks (Jo et al., 2024b) where they have shown the effectiveness of the algorithm
for several languages including several European languages as well as Chinese and
Korean. This contrasts with traditional sentence alignment methods in MT that of-
ten require recursive editing to accommodate significant differences between source
and target languages. In cases of mismatches due to varying sentence boundaries,
our pattern matching-based algorithm accumulates sentences until it finds a match-
ing pair. The computational efficiency of our approach is notable, requiring linear

time, O(m +n), where m and n are the lengths of the gold and system sentences,


5.3. JP-ERRANT 153

respectively. This is a significant improvement over the traditional cubic complexity,
n?, of standard length-based sentence alignment algorithms in MT. The proposed

jp-algorithm introduces sentence alignment to ensure correct GEC evaluations.

Algorithm 8 Pseudo-code for sentence alignment

1: function PATTERNMATCHINGSA (ZL, FR):
2: while £ and R do

3 if Lic) = Ry) then

4 Li,R' < L'4+L;,R' +R; where 0 <i < LEN(L), 0 < 7 < LEN(R)
5: else

7 if LEN(L;) < LEN(R,;) then

8

: Lie L' + L;
9: ieitl
10: else
11: RoR+R;
12: jegtl
13: end if
14: end while
15: LRH L+D' R+R

16: end if
17: end while
18: return L’, R’

In Equation (5.1), we define that sequences £; and R; can be aligned if they
match when all spaces are removed, denoted as £; 4 and R; MW. This method aims

to minimize differences due to tokenization.

Liw == Ry) (5.1)

However, simply removing spaces and concatenating words may not sufficiently iden-
tify identical sentence pairs. Variations in tokenization can introduce grammatical
morphemes absent in the gold-standard sentences, or vice versa. For example, the

contraction can’t presents tokenization challenges, as it can be tokenized as can not


154 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

or ca nt, with each version introducing different characters. Such variations mean
that our character-level evaluation may fail to accurately capture these discrepan-
cies. The inconsistency in tokenization standards across different corpora further
complicates this issue. For instance, the English UD corpus from EWT tokenizes
can’t as ca and n’t, whereas ParTUT tokenizes it as can and not. We therefore
propose to align sequences £; and R,, denoted as Lay and R,y respectively, when
they demonstrate close character similarities that exceed a predefined threshold, a.
Moreover, the subsequent sequences, £;+; and Rj;+1, must either directly match or

exhibit sufficient similarity, as outlined in Equation (5.2).

(Lie =~ Ri) A

(Livi) == Rory) V Lis = Ry) (5.2)

We modify the Jaro-Winkler distance, traditionally used to measure the similarity
between two strings, by incorporating a suffix scale in addition to the existing prefix
scale. The original Jaro similarity, denoted by sim;, calculates matches based on
the number of forward-matching characters between two strings, s; and sg. The
Jaro-Winkler distance enhances this similarity by introducing a prefix scale p for a
specified prefix length /. Our modification extends this method by adding a similar
scale for a defined suffix length, thereby improving the algorithm’s ability to recognize
suffix similarities as well.

(lp + U'p)(1 — sim;)

a= sim; —
2

(5.3)


5.3. JP-ERRANT 155

where sim, is the Jaro similarity between two strings s; and s9, / and I’ are the lengths
of the common prefix and suffix of the strings, respectively, and p is a constant scaling
factor (set to 0.1)."" If 2; and R; cannot be aligned, we proceed by concatenating
sequences based on their lengths. Specifically, if the length of L;.,, exceeds that of
Ryn, then £; is concatenated with £41. Conversely, if Rj, is longer, then R; is
concatenated with 2,4. This concatenation process is repeated until the pairs £j+1
and Rj+; meet the established sentence matching criteria.

We have re-implemented errant incorporating a joint preprocessing step, now re-
ferred to as jp-errant. Unlike the original errant, which used spaCy for language
processing, jp-errant employs the part-of-speech tagging capabilities of stanza (Qi
et al., 2020), chosen for its demonstrably clear performance.!? We maintain the orig-
inal error annotation scheme of errant, but we adjust word positions by re-indexing
them during the sentence alignment process when sentences are concatenated. This
adjustment is crucial to handle discrepancies in sentence boundaries between the
gold standard and those processed by stanza.

In the alignment algorithm, concatenating sentences necessitates updates to the
positions of corresponding edits. After sentence alignment, the re-indexing process
is carried out in two primary steps: First, we update all non-empty edits. For
each concatenated sentence, we accumulate its token count to serve as the offset for
subsequent edits. Consider the following example: when concatenating sentences

5S; = (wi, We, .-.;Wm] and S; = [wi, wo, ..., Wn], the edits E; = [ea] and E; = [e(cay|

!2The value of a represents a trade-off between the correctness and precision of the alignments.
If a is too small, we risk boundary errors (false positives), while if a is too large, we may miss some
boundaries (false negatives).

https: //stanfordnlp.github.io/stanza/performance. html


156 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

are adjusted to become [€(a,4), €(c+m,d+m)| in the concatenated sequence. Second, after
re-indexing all edits, we proceed to remove any unnecessary empty edits. This step
is crucial to ensure that the m2 results are fully aligned, with no unnecessary empty
edits.

Details of the re-indexing process by jp-errant are shown in Figure 5.9. For
example, the m2 file’s state is shown both before and after sentence alignment. The
gold m2 file features an empty edit (-1 -1]||noop) and a capitalization edit (0
1|||R:ADV), where the adverb how is replaced by How. Conversely, the stanza m2
file contains two empty edits, which indicate places where no grammatical errors

were corrected by the system.

1. Preparation

gold m2 | S Kate Ashby ,
A -1 -11||noop|!||-NONE-| | |REQUIRED| | |-NONE-| | |0
S how are you ? I hope you are well .
AO 1/|/R:ADV| | [How] | |REQUIRED| | |-NONE-| | |0
stanza m2 | S Kate Ashby ; how are “you an a 7
A -1 -11||noop!||-NONE-| | |REQUIRED| | |-NONE-| | |0
S I hope you are well .
A -1 -1]|||noop|||-NONE-| | |REQUIRED| | |-NONE-| | |0
2. Sentence alignment

gold m2 | S Kate Ashby , how

are you ? I hope you are well .

A -1 -1]|||noop|||-NONE-| | |REQUIRED| | |-NONE-| | |0

AO 11] ]R:ADV| | |How] | |REQUIRED| | |-NONE-|||0
intents ~ ae
0

“stanza m2 | S Kate Ashby ; how are you ? I hope you are well ._
A -1 -11||noop|!||-NONE-| | |REQUIRED| | |-NONE-| | |0
A -1 -11||noop|||-NONE-| | |REQUIRED| | |-NONE-| | |0

3. Re-indexing

gold m2 | S Kate Ashby , how are you ? I hope you are well .
A 3 4|||R:ADV| | |How| | |REQUIRED| | | -NONE-| | |0
“stanza m2 | S Kate Ashby ; how are “you ? I hope you are well .
A -1 -1]|||noop|||-NONE-| | |REQUIRED| | |-NONE-| | |0

Figure 5.9: Procedure example of jp-errant: stanza m2 indicates that sentence
boundaries are detected by stanza from raw text.


5.3. JP-ERRANT 157

5.3.4 Experiments and results

GEC dataset For our experiments, we utilized the development dataset from the
Cambridge English Write & Improve (W&J) corpus, which was introduced during the
Building Educational Applications 2019 Shared Task: Grammatical Error Correction
(BEA2019) (Bryant et al., 2019). This dataset is manually annotated with Common
European Framework of Reference (CEFR) proficiency levels—beginner (A), inter-
mediate (B), and advanced (C) (Yannakoudakis et al., 2018). The texts, written by
L2 English learners, show a trend where sentences from higher proficiency levels tend
to be longer than those from lower levels. Specifically, the average token counts per
sentence for levels A, B, and C are 17.538, 18.304, and 19.212, respectively. We ana-
lyzed the distribution of errors across different language proficiency levels. The error
types in levels B and C are similar, including missing punctuation marks (M:PUNCT),
incorrect prepositions (R: PREP), and missing determiners (M:DET). Additionally, level
A frequently exhibits orthographic errors (R:ORTH), such as case or whitespace is-
sues. Table 5.4 presents the ratios of the most frequent error types within the W&I

training data, where the ratios represent the distribution of grammatical errors (Zeng

et al., 2024).
Proficiency A Proficiency B Proficiency C
M:PUNCT 0.0933 M:PUNCT 0.1134 M:PUNCT 0.1183
R:ORTH 0.0602 R:PREP 0.0589 R:PREP 0.0517
R:PREP 0.0506 M:DET 0.0442 M:DET 0.0345
R:VERB:TENSE — 0.0455 R: VERB 0.0414 R: VERB 0.0323
R: VERB 0.0419 | R:VERB:TENSE 0.0393 | R:VERB:TENSE 0.0273

Table 5.4: The most frequent errors and their ratios in the W&I dataset


158 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

GEC evaluation results We utilized two off-the-shelf state-of-the-art GEC sys-
tems: GECTOR (Omelianchuk et al., 2020) and T5 (Rothe et al., 2021). Briefly,
GECTOR employs a sequence tagging approach instead of sequence generation. This
system uses a Transformer encoder to predict token-level edit operations, making it
significantly faster and more efficient than traditional seq2seq models. In our experi-
ments, we used the RoBERTa pre-trained model as the encoder, which showed the best
performance among various transformer models tested (Omelianchuk et al., 2020).
T5, or Text-To-Text Transfer Transformer, is a unified framework for NLP tasks that
converts all tasks into a text-to-text format. We used the T5-small model, a smaller
variant with approximately 60 million parameters. This model was fine-tuned on the
cleaned English LANG-8 corpus and achieved reported Fo.5 scores of 60.54 and 65.01
on the CoNLL2014 and BEA2019 test sets, respectively (Rothe et al., 2021).1°

The GEC results using the original errant and jp-errant with the gold-standard
sentence boundaries, as well as jp-errant with the system-generated sentence bound-
aries, are presented in Table 5.5. It’s important to note that the original errant
does not allow the use of different sentence boundaries, which precludes a “Sys +
errant” setup. The results of GEC may also differ between texts with gold-standard
sentence boundaries and those with system-generated boundaries due to the nature
of sequence-to-sequence GEC. The former shows the reproducibility of jp-errant,
while the latter presents how the proposed method can handle real-world scenar-
ios. If there are mismatches in sentence boundaries between the gold-standard and

system-generated results, we initiate a sentence alignment process.

Mnttps://github.com/grammarly/gector
https: //huggingface.co/Unbabel/gec-t5_small


5.3. JP-ERRANT 159

GECTOR A B
TP FP FN Prec Rec F0.5 TP FP FN Prec Rec F0.5
GOLD + errant | 1299 798 1680 0.6195 0.4361 0.5714 | 1049 621 1470 0.6281 0.4164 0.5702

SYS + jp-errant | 1220 842 1753 0.5917 0.4104 0.5436 | 1039 626 1464 0.624 0.4151 0.5670

GOLD + errant | 415 350 706 0.5425 0.3702 0.4963 | 2763 1769 3856 0.6097 0.4174 0.5582

SyS + jp-errant | 413 347 704 0.5434 0.3697 0.4968 | 2672 1815 3921 0.5955 0.4053 0.5444
T5 A B

GOLD + errant | 1271 696 1708 0.6462 0.4267 0.5859 | 960 593 1559 0.6182 0.3811 0.5498

SyS + jp-errant | 1173 771 1800 0.6034 0.3946 0.5456 | 928 614 1575 0.6018 0.3708 0.5351

TP FP FN Prec Rec FO0.5 TP FP FN Prec Rec F0.5

SYS + jp-errant | 351 332 766 0.5139 0.3142 0.456 | 2452 1717 4141 0.5882 0.3719 0.5269

Table 5.5: SOTA GEC results achieved by GECTOR and T5 with the English-specific
error classification module with gold and system sentence boundaries

We have integrated the English-specific classification module from the original
errant, which identifies types of grammatical errors for English. This module cat-
egorizes detailed grammatical error types, such as NOUN:POSS for errors related to
possessive noun suffixes. It utilizes universal part-of-speech tags (Petrov et al., 2012)
and dependency relation tags to determine error types. For instance, if the first token
in an edit is tagged as PART—typically indicating particles or function words—and
its dependency relation is case: poss (indicative of a possessive case), the classifier
labels it as a NOUN: POSS error based on this information. Without the classification
module, jp-errant can still produce generic error annotations with corresponding
POS labels and evaluate the results based on error edits regardless of languages.
However, with this language-specific classification module, it can generate language-

specific error annotations for other languages.


160 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

While we successfully reproduced errant, discrepancies persist, as shown in Fig-
ure 5.10. One notable distinction lies in preposition naming: all POS labels adhere
to the Universal POS label names, yet the original errant continues to use PREP in-
stead of ADP for prepositions. Another difference arises from an error in POS tagging
by spaCy, which was employed by the original errant. In the second sentence, your
at positions 3 and 4 is identified as a pronoun (PRON) by stanza, whereas spaCy

labels it as a determiner (DET).

errant
S It ’s difficult answer at the question
A 3 3]||M:VERB:FORM| || tol | |REQUIRED| | |-NONE-| | |0
A 4 5|||U: ADP] 1111 |/REQUIRED| | |-NONE-|| 10

S$ Thank you for “your e - mail , it was wonderful to hear from you _
A 3 4||1R:DET|||your| | |REQUIRED| | | -NONE-|||0
A 7 QIIIR:PUNCTI||. It] | |REQUIRED| | |-NONE-|||0
jp-errant
S It ’s difficult answer at the question
A 3 3]||M:VERB:FORM| || tol | |REQUIRED| | |-NONE-| | |0
A 4 5|||U:PREP|111| |REQUIRED| | |-NONE-| | 10

S$ Thank you for “your e - mail , it was wonderful to hear from you _
A 3 4|||R:PRON|||your|||REQUIRED| | |-NONE-| | |0
A 7 QIIIR:PUNCTI||. It] | |REQUIRED| | |-NONE-|||0

Figure 5.10: Differences between errant and jp-errant

5.3.5 Multilingual alignment-based errant

We present multilingual errant results, focusing on Chinese and Korean L2 GEC. By
aligning and evaluating these languages, we demonstrate the challenges and potential
solutions in applying the proposed GEC evaluation methodology across different

languages.


5.3. JP-ERRANT 161

Chinese L2 GEC dataset Multi-Reference Multi-Source Evaluation Dataset for
Chinese Grammatical Error Correction (CGEC) is a multi-reference multi-source
dataset comprising sentences from the NLPCC18 test set (Zhao et al., 2018), CGED-
2018 and CGED-2020 test datasets (Rao et al., 2018, 2020), and randomly selected
Lang-8 dataset (Zhang et al., 2022).‘° The MuCGEC dataset exhibits an average
number of target references per sentence exceeding 2. They discovered that aug-
menting the average number of references per sentence enhances the reliability of

evaluations, attributed to its multi-reference characteristics.

Korean L2 GEC dataset The GEC dataset for Korean consists of two distinct
types: L1 writing from the Center for Teaching and Learning for Korean, and L2
writing from the National Institute of Korean Language (NIKL) corpus (Yoon et al.,
2023).!” This dataset includes the original text, the corrected text, and its corre-
sponding error-annotated errant-style m2 file, automatically generated for Korean.
Utilizing the proposed split (70/15/15) of the GEC dataset for Korean. The original
L2 dataset is collected by NIKL with 3613 files. It provides the original L2 sentences,
and their morphological segmentation with part-of-speech tags. The correction is an-
notated at the morpheme level by adding, deleting and replacing the grammatical

error morpheme.

Error annotation for Chinese and Korean Figure 5.11 shows examples of
differences in error annotation (i.e., m2 files) for Chinese and Korean. ChERRANT

(Chinese errant), an adaptation of the original English errant, is the most re-

Shttps://github.com/HillZhang1999/MuCGEC
‘https: //github.com/soyoung97/Standard_Korean_GEC


162 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

cent development in annotating Chinese grammatical errors and evaluating Chinese
GEC systems (Zhang et al., 2022). ChERRANT operates at two levels of granularity:
character-based and word-based, and primarily categorizes errors into three opera-
tional types: redundant (R, equivalent to U for unnecessary in the original errant),
missing (M), and substitution (S, equivalent to R for replacement in the original
errant) errors. For word-based error annotation, ChERRANT utilizes LTP-based word
segmentation (Che et al., 2010)!® and converts its part-of-speech (POS) tags to Uni-
versal POS labels (Petrov et al., 2012). While word-based annotation allows for
more detailed error categorization (e.g., S:'VERB would indicate a verb substitution
error), due to potential word segmentation inaccuracies, ChERRANT applies character-
level annotation by default and has not fully implemented all word-level annotations.
jp-errant, as proposed in the current work, attempts to address the limitations of
ChERRANT by: 1) using stanza for improved Chinese word segmentation and POS
tagging, and 2) adhering to the original errant conventions (Missing, Unecessary, and
Replacement) for consistent multilingual grammatical error annotation.

While the original Koraen L2 dataset collected by the National Institute of Korean
Language (NIKL), provided error annotations with three different levels based on
the POS of the morpheme (e.g. noun, verb, case marker, and other grammatical
categories), transformation of the morpheme (omission, addition, replacement, and
misformation!’), and its linguistic dimensions (pronunciation, syntax, and discourse),
the previous work proposed fourteen error types such as INSERTION, DELETION, and

WS (word space) at the word level by converting sequences of morphemes into words

Shttps://github.com/HIT-SCIR/1tp
The term misformation by NIKL is not commonly used to refer to a spelling error.


5.3. JP-ERRANT 163

(Yoon et al., 2023). Given that grammatical errors manifest at the morpheme level,
while the current error annotation operates at the word level, the previous work
established two priority rules for categorizing error types to assign a single error
type for each word, as follows: (i) INSERTION > DELETION > others, and (ii) WSs
> WO > SPELL > SHORTEN > PUNCTUATION > others, where WO stands for ’word
order’. However, the PUNCTUATION and WO error types do not appear in the m2
files, indicating that this type of error might not have been explicitly annotated or
utilized in the previous work.
m2 file generated by ChERRANT:
St + ES RU AA TR ce Ree A Ile | OM AR NEE
TO-aAo *{ —“*S Haw UL SAR 2 ARK ES A label. OM KR A -
A 2 3|||S:SPELL| | | £48] ||REQUIRED| | |-NONE-| | |0

A 7 7/1 1M:VERB||14@|||REQUIRED| | |-NONE-|||0
A 8 8/11 |M:ADJ|11X1||REQUIRED| | | -NONE-|||0

li

m2 file generated by jp-errant:

SW —+ £% RU SA HH 2 RCS NW Ae, WBA HE .
A 2 3/1 |R:NOUN| || 4451 | |REQUIRED| | |-NONE-|||0

A 7 7||1M:VERB||14 || |REQUIRED| | |-NONE-|||0

A 8 8/|IM:ADJI||A1||REQUIRED| | |-NONE-| 110

(a) Example of word based m2 files for Chinese (‘Air pollution is a very harmful problem
to a life and is bad for the body.’)

m2 file generated by KAGAS:

S se ojseetay YS 2a AAact

AO 1/1 IWS 1 1et=aO} <Qet CY] | |REQUIRED| | | -NONE-| | 10
A 4 4||| INSERTION] ||. || |REQUIRED| | | -NONE-| | 10

m2 file generated by jp-errant:

S ste ojegetay YS zt AAact

A O 1/|1R:NOUN VERB NOUN|||¢t=0] 4=@/et (11 |REQUIRED| | | -NONE-| | |0
A 4 4|||M:PUNCT||1|.1|1||REQUIRED| | |-NONE-|||0

(b) Example of m2 files for Korean (‘I really wanted to sleep during Korean class.’)

Figure 5.11: Difference example of m2 files for Chinese and Korean


164 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

Multilingual experiments and results For Chinese GEC, we employed a recent
system based on the Chinese BART-large model (Zhang et al., 2023). The model was
trained using a combination of the HSK and Lang§8 datasets, totaling approximately
1.3 million sentence pairs. This model is designed to effectively handle grammatical
errors in texts written by Chinese L2 learners.?? Similarly, for Korean GEC, we uti-
lized a system based on the Korean BART model (Yoon et al., 2023). The model was
fine-tuned using both Korean L1 and L2 datasets. Compared to Hanspell, a widely
regarded top rule-based Korean GEC system, this model demonstrates both effi-
ciency and superior performance in correcting grammatical errors in Korean texts.7!
Table 5.6 presents the multilingual GEC results using jp-errant, where all input
text is concatenated into a single text block, and sentence boundaries are detected us-
ing stanza. In previous work, language-specific tools were used for error annotation
and evaluation: ChERRANT for Chinese (Zhang et al., 2022) and KAGAS for Korean
(Korean Automatic Grammatical error Annotation System; Yoon et al., 2023). In
the present work, we apply the generic jp-errant to generate m2 files for Chinese
and Korean without any language-specific error classification. The m2 files gener-
ated by jp-errant adhere to the original m2 file conventions (Missing, Unecessary,
and Replacement) to support future multilingual GEC evaluations. Discrepancies
between previous work and jp-errant would be reduced if language-specific error
classifications were provided. A possible reason that ChERRANT and jp-errant have
similar results on Chinese L2 data is there are few language-specific grammatical

error annotations. According to results from ChERRANT, about 4.56% of the errors in

20nttps: //huggingface.co/HillZhang/real_learner_bart_CGEC
2Inttps: //huggingface.co/Soyoung97/gec_kr


5.3. JP-ERRANT 165

the validation set belong to language-specific error types that were not part of the
the original errant: spelling errors (SPELL), missing components (MC), and quantifier
errors (QUANT). Spelling errors in Chinese are defined based on the strings similarity
in pinyin pronunciation and character shape. Missing components are defined as
placeholders for important words that were somehow omitted in the source sentence.
Quantifiers are were not part of the original errant labels. Among these, spelling
errors are the most common, accounting for 3.96% of all errors. In contrast, Table 5.6
shows large discrepancies in the Korean results. It has been recognized that the pre-
vious KAGAS system has limitations in error detection and annotation. Our current
implementation using stanza demonstrates improved language processing capabil-
ities for Korean. A refined Korean-specific error annotation system is currently in
development, aiming to address existing issues and enhance the accuracy of Korean

GEC evaluation.

Chinese Korean
TP FP FN Prec Rec F0.5 Le FP FN Prec Rec F0.5
GOLD + previous work | 699 1854 3534 0.2738 0.1651 0.242 | 3037 7323 5822 0.2931 0.3428 0.3019
GOLD + jp-errant | 788 2215 3765 0.2624 0.1731 0.2379 | 3187 2212 5090 0.5903 0.385 0.5334
SYS + jp-errant | 499 1781 4054 0.2189 0.1096 0.1825 | 3418 2307 4859 0.597 0.413 0.5482

Table 5.6: Multilingual GEC results by previous work and jp-errant with gold and
system sentence boundaries

5.3.6 Conclusion

In this study, we addressed various challenges in grammatical error correction by im-
plementing and refining a methodology for end-to-end processing. We demonstrated
the effectiveness of our methods by reproducing previous methodologies and high-

lighting our contributions. Our primary contribution is the refined alignment process,


166 CHAPTER 5. APPLICATIONS OF JP-ALGORITHM

which addresses discrepancies in sentence boundaries between gold-standard and
system-generated results. Despite successfully reproducing errant using stanza, we
identified persistent discrepancies, such as differences in POS label naming conven-
tions and tagging errors, which was used in the original errant. We also took an
additional step to facilitate multilingual evaluation by generalizing errant, and we
presented case studies for Chinese and Korean, which enabled potential multilingual
evaluation for GEC. Such universality will provide a framework for the consistent
annotation of grammatical errors across different languages. In conclusion, we lay a
robust foundation for advancing grammatical error correction, particularly its eval-

uation, and enhancing its applicability in real-world contexts.”?

?2This chapter is based on "Refined Evaluation for End-to-End Grammatical Error Correction
Using an Alignment-Based Approach" by Junrui Wang, Mengyang Qiu, Yang Gu, Zihao Huang, and
Jungyeul Park. In Proceedings of the 31st International Conference on Computational Linguistics,
pages 774-785, Abu Dhabi, UAE. Association for Computational Linguistics (Wang et al., 2025).


Chapter 6

Epilogue

In this memoir, I addressed two key aspects of Natural Language Processing (NLP)
that I have worked on over the past decade: the creation of linguistic resources and
the evaluation methodologies for system performance. My current work focuses on
syntactic structures, addressing long-standing issues in constituency parsing, such as
treebank binarization. Traditional methods for tree binarization, such as those uti-
lizing Michael Collins’ head percolation table, are widely accepted. However, despite
their use, the limitations of these methods have not been thoroughly addressed, as
they achieve only around 60% accuracy in identifying the head child in constituency
structures. This inaccuracy directly affects parsing algorithms that rely on Chomsky
Normal Form (CNF), particularly in both CKY chart-based and transition-based
bottom-up constituency parsing, where the original treebank should be transformed
into a binary structure. Incorrect head-child identification in the treebank’s binary
structure undermines overall parsing performance. My work focuses on refining this
process, with the potential to redefine how head-finding algorithms are formulated

and applied in constituency parsing. Building on my work in treebank binarization,

167


168 CHAPTER 6. EPILOGUE

I am also developing a trainable conversion scheme for dependency-to-constituency
structures through a transition-based joint parsing algorithm that integrates both
constituency and dependency parsing. This approach integrates a transition-based
bottom-up constituency parsing algorithm (Sagae and Lavie, 2005) with a transition-
based arc-standard dependency parsing algorithm (Yamada and Matsumoto, 2003).
It combines Penn Treebank constituency structures with their corresponding depen-
dency structures across three languages—English, Chinese, and Korean—to develop
a trainable model for dependency-to-constituency conversion. My ultimate goal is
to extend this methodology to create Universal Constituencies, providing a frame-
work for consistent annotation of grammar including parts of speech and syntactic
constituencies across different human languages.! The development of such a frame-
work is important as it offers a unified approach to parsing in a multilingual context,
enabling cross-linguistic comparisons and the development of more robust models
capable of handling diverse syntactic structures. This research on Universal Con-
stituencies will also be extended to include further work on CCGbank and TAGbank,
two key linguistic resources that require both constituency and dependency struc-
tures. My work on combinatory categorial grammars (CCG) has already explored
how morphology can be integrated within this syntactic framework (Park and Kim,
2023). Similarly, TAGbank is envisioned as a novel resource that translates exist-
ing treebanks into tree-adjoining grammar (TAG) derivations. TAGbank will also
include both phrase (derived) structures and dependency (derivation) structures, of-

fering a more detailed and versatile linguistic resource for multiple languages. My

‘As Universal Dependencies (UD) is a framework for consistent annotation of grammar across
different human languages.


169

future work will focus on expanding CCGbank and TAGbank to cover additional lan-
guages, based on Universal Constituencies and Universal Dependencies. This expan-
sion is driven by the need for multilingual treebanks that not only provide syntactic
parsing frameworks but also integrate both constituency and dependency structures.
The combination of these linguistic resources will provide a foundation for exploring
linguistic phenomena at the syntactic, morphological, and semantic levels. These
resources will be essential for future work in NLP, contributing to the development
of more comprehensive linguistic tools. The broader goal is to create a resource that
facilitates various NLP applications and advances the field. By focusing on consis-
tent annotation frameworks and refining analysis methodologies, my research aims
to set new standards for how syntactic and grammatical structures are represented
and processed in NLP systems. My research will continue to advance the field of
syntactic structure analysis by focusing on refining existing methodologies, develop-
ing robust multilingual linguistic resources, and exploring their representation within
various architectures and models. This work will contribute to advancing the state

of the art in both theoretical and applied aspects of natural language processing.


170 CHAPTER 6. EPILOGUE


Bibliography

Tanvirul Alam, Akib Khan, and Firoj Alam. Punctuation Restoration using Trans-
former Models for High-and Low-Resource Languages. In Wei Xu, Alan Ritter,
Tim Baldwin, and Afshin Rahimi, editors, Proceedings of the Sixth Workshop on
Noisy User-generated Text (W-NUT 2020), pages 132-142, Online, 11 2020. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2020.wnut-1.18. URL
https: //aclanthology.org/2020.wnut-1.18.

Riadh Belkebir and Nizar Habash. Automatic Error Type Annotation for Arabic.
In Arianna Bisazza and Omri Abend, editors, Proceedings of the 25th Conference
on Computational Natural Language Learning, pages 596-606, Online, 11 2021.
Association for Computational Linguistics. doi: 10.18653/v1/2021.conll-1.47. URL
https: //aclanthology.org/2021.conl1-1.47.

Daniel M. Bikel. Intricacies of Collins’ Parsing Model. Computational Linguistics,
30(4):479-511, 2004. doi: 10.1162/0891201042544929. URL https://doi.org/
10.1162/0891201042544929

Philip Bille. A survey on tree edit distance and related problems. Theoret-
ical Computer Science, 337(1):217-239, 2005. ISSN 0304-3975. doi: https:
//doi.org/10.1016/j.tcs.2004.12.030. URL https://www.sciencedirect.com/
science/article/pii/S0304397505000174.

Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with
Python. O’Reilly Media, Newton, Massachusetts, United States, 2009. URL http:
//shop.oreilly.com/product/9780596516499. do.

Ezra Black, Steve Abney, Dan Flickinger, Claudia Gdaniec, Ralph Grishman, Phil
Harrison, Donald Hindle, Robert Ingria, Frederick Jelinek, Judith L. Klavans,
Mark Liberman, Mitch Marcus, Salim Roukos, Beatrice Santorini, and Tomek
Strzalkowski. A Procedure for Quantitatively Comparing the Syntactic Coverage

171


172 BIBLIOGRAPHY

of English Grammars. In Speech and Natural Language: Proceedings of a Work-
shop Held at Pacific Grove, California, February 19-22, 1991, pages 306-311, Pa-
cific Grove, California, 1991. DARPA/ISTO. URL https: //aclanthology.org/
H91-1060.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching
Word Vectors with Subword Information. Transactions of the Association for
Computational Linguistics, 5:135-146, 2017. ISSN 2307-387X. URL https://
transacl.org/ojs/index.php/tacl/article/view/999.

Adriane Boyd. Using Wikipedia Edits in Low Resource Grammatical Error Correc-
tion. In Wei Xu, Alan Ritter, Tim Baldwin, and Afshin Rahimi, editors, Proceed-
ings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-
generated Text, pages 79-84, Brussels, Belgium, 11 2018. Association for Compu-
tational Linguistics. doi: 10.18653/v1/W18-6111. URL https://aclanthology.
org/W18-6111.

Christopher Bryant, Mariano Felice, and Ted Briscoe. Automatic Annotation and
Evaluation of Error Types for Grammatical Error Correction. In Regina Barzi-
lay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 793-
805, Vancouver, Canada, 7 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1074. URL https://aclanthology.org/P17- 1074.

Christopher Bryant, Mariano Felice, Wistein E. Andersen, and Ted Briscoe. The
BEA-2019 Shared Task on Grammatical Error Correction. In Proceedings of the
Fourteenth Workshop on Innovative Use of NLP for Building Educational Applica-
tions, pages 52-75, Florence, Italy, 8 2019. Association for Computational Linguis-
tics. doi: 10.18653/v1/W19-4406. URL https: //aclanthology.org/W19-4406.

Christopher Jack Bryant. Automatic annotation of error types for grammatical error
correction. PhD thesis, University of Cambridge, Churchill College, Cambridge,
UK, 2019. URL https://doi. org/10.17863/CAM. 40832.

Jo Calder. On aligning trees. In Second Conference on Empirical Methods in Nat-
ural Language Processing, pages 75-80, 1997. URL https://aclanthology.org/
W97-0308.

Jeong-Won Cha, Geunbae Lee, and Jong-Hyeok Lee. Generalized Unknown Mor-
pheme Guessing for Hybrid POS Tagging of Korean. In Eugene Charniak, ed-
itor, Proceedings of the Siath Workshop on Very Large Corpora, pages 85-93,


BIBLIOGRAPHY 173

Montreal, Quebec, Canada, 1998. Morgan Kaufrnann Publisher. URL https:
//www.aclweb.org/anthology/W98-1110/.

Wanxiang Che, Zhenghua Li, and Ting Liu. LTP: A Chinese Language Technology
Platform. In Yang Liu and Ting Liu, editors, Coling 2010: Demonstrations, pages
13-16, Beijing, China, 8 2010. Coling 2010 Organizing Committee. URL https:
//aclanthology .org/C10-3004.

Yige Chen, Eunkyul Leah Jo, Yundong Yao, KyungTae Lim, Miikka Silfverberg,
Francis M. Tyers, and Jungyeul Park. Yet Another Format of Universal Depen-
dencies for Korean. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James
Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia
Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan
Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis
Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Con-
ference on Computational Linguistics, pages 5432-5437, Gyeongju, Republic of
Korea, 10 2022. International Committee on Computational Linguistics. URL
https: //aclanthology.org/2022.coling-1.482.

Yige Chen, Jae Ihn, KyungTae Lim, and Jungyeul Park. Towards Standardized
Annotation and Parsing for Korean FrameNet. In Nicoletta Calzolari, Min-
Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue,
editors, Proceedings of the 2024 Joint International Conference on Computa-
tional Linguistics, Language Resources and Evaluation (LREC-COLING 2024),
pages 16653-16658, Torino, Italy, 5 2024a. ELRA and ICCL. URL https:
//aclanthology.org/2024.1lrec-main. 1447.

Yige Chen, KyungTae Lim, and Jungyeul Park. Korean named entity recognition
based on language-specific features. Natural Language Engineering, 30(3):625-649,
2024b. doi: 10.1017/S1351324923000311.

DongHyun Choi, Jungyeul Park, and Key-Sun Choi. Korean Treebank Transfor-
mation for Parser Training. In Proceedings of the ACL 2012 Joint Workshop on
Statistical Parsing and Semantic Processing of Morphologically Rich Languages,
pages 78-88, Jeju, Republic of Korea, 2012. Association for Computational Lin-
guistics. URL http://www.aclweb. org/anthology/W12-3411.

Key-Sun Choi, Young S. Han, Young G. Han, and Oh W. Kwon. KAIST Tree
Bank Project for Korean: Present and Future Development. In Proceedings of
the International Workshop on Sharable Natural Language Resources, pages 7-14,


174 BIBLIOGRAPHY

Nara Institute of Science and Technology, 1994. Nara Institute of Science and
Technology.

Sanghyuk Choi, Taeuk Kim, Jinseok Seol, and Sang-goo Lee. A Syllable-based
Technique for Word Embeddings of Korean Words. In Proceedings of the First
Workshop on Subword and Character Level Models in NLP, pages 36-40, Copen-
hagen, Denmark, 9 2017. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/W17-4105.

Noam Chomsky. Lectures on Government and Binding. Studies in Generative Gram-
mar. Foris Publications, Dordrecht, The Netherlands, 1981.

Noam Chomsky. Some Concepts and Consequences of the Theory of Government
and Binding. Linguistic Inquiry Monograph 6. The MIT Press, Cambridge, MA,
1982. ISBN 9780262030908.

Jayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D. Choi. Building Univer-
sal Dependency Treebanks in Korean. In Proceedings of the Eleventh Interna-
tional Conference on Language Resources and Evaluation (LREC 2018), Miyazaki,
Japan, 2018. European Language Resources Association (ELRA). ISBN 979-10-
95546-00-9.

Min-Chung Chung. Les nominalisations d’adjectifs en coréen : constructions nomi-
nales a support issda (il y avoir). PhD thesis, Université Paris 7 - Denis Diderot,
Paris, France, 1998. URL https://www.theses.fr/1998PA070002.

Tagyoung Chung and Daniel Gildea. Unsupervised Tokenization for Machine Trans-
lation. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 718-726, Singapore, 2009. Association for Computa-
tional Linguistics. URL http: //www.aclweb. org/anthology/D/D09/D09- 1075.

Tagyoung Chung, Matt Post, and Daniel Gildea. Factors Affecting the Accuracy
of Korean Parsing. In Proceedings of the NAACL HLT 2010 First Workshop on
Statistical Parsing of Morphologically-Rich Languages, pages 49-57, Los Angeles,
CA, USA, 2010. Association for Computational Linguistics. URL http://www.
aclweb.org/anthology/W10- 1406.

Michael Collins. Three Generative, Lexicalised Models for Statistical Parsing. In
Proceedings of the 35th Annual Meeting of the Association for Computational Lin-
guistics, pages 16-23, Madrid, Spain, 1997. Association for Computational Lin-
guistics. doi: 10.3115/976909.979620. URL http: //www.aclweb. org/anthology/
P97-1003.


BIBLIOGRAPHY 175

Michael Collins. Head-Driven Statistical Models for Natural Language Parsing. PhD
thesis, University of Pennsylvania, 1999. URL http://www.cs.columbia.edu/
~mcollins/papers/thesis.ps.

Cagr1 Coltekin. (When) do we need inflectional groups? In Proceedings of The First
International Conference on Turkic Computational Linguistics (TurCLing 2016),
pages 38-43, Konya, Turkey, 2016.

Daniel Dahlmeier and Hwee Tou Ng. Better Evaluation for Grammatical Error
Correction. In Eric Fosler-Lussier, Ellen Riloff, and Srinivas Bangalore, editors,
Proceedings of the 2012 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language Technologies, pages
568-572, Montréal, Canada, 6 2012. Association for Computational Linguistics.
URL https://aclanthology.org/N12- 1067.

Timothy Dozat and Christopher D. Manning. Deep Biaffine Attention for Neural De-
pendency Parsing. In Proceedings of the 5th International Conference on Learning
Representations (ICLR 2017), Toulon, France, 11 2017. The International Confer-
ence on Learning Representations (ICLR). URL http://arxiv.org/abs/1611.
01734.

Rebecca Dridan and Stephan Oepen. Tokenization: Returning to a Long Solved
Problem — A Survey, Contrastive Experiment, Recommendations, and Toolkit
—. In Proceedings of the 50th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages 378-382, Jeju Island, Korea,
2012. Association for Computational Linguistics. URL http://www.aclweb.org/
anthology/P12-2074.

Kilian Evang, Valerio Basile, Grzegorz Chrupata, and Johan Bos. Elephant: Se-
quence Labeling for Word and Sentence Segmentation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 1422—
1426, Seattle, Washington, USA, 10 2013. Association for Computational Linguis-
tics. URL http://www.aclweb.org/anthology/D13- 1146.

Daniel Fernandez-Gonzaélez and Carlos Gomez-Rodriguez. Enriched In-Order Lin-
earization for Faster Sequence-to-Sequence Constituent Parsing. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics,
pages 4092-4099, Online, 7 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.376. URL https://aclanthology.org/2020.
acl-main.376.


176 BIBLIOGRAPHY

Xue-Yong Fu, Cheng Chen, Md Tahmid Rahman Laskar, Shashi Bhushan, and Si-
mon Corston-Oliver. Improving Punctuation Restoration for Speech Transcripts
via External Data. In Wei Xu, Alan Ritter, Tim Baldwin, and Afshin Rahimi, edi-
tors, Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT
2021), pages 168-174, Online, 11 2021. Association for Computational Linguis-
tics. doi: 10.18653/v1/2021.wnut-1.19. URL https://aclanthology.org/2021.
wnut-1.19.

William A. Gale and Kenneth W. Church. A Program for Aligning Sentences in
Bilingual Corpora. Computational Linguistics, 19(1):75-102, 1993. URL https:
//aclanthology.org/J93- 1004.

Abbas Ghaddar and Phillippe Langlais. Robust Lexical Features for Improved
Neural Network Named-Entity Recognition. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics, pages 1896-1907, Santa Fe,
New Mexico, USA, 8 2018. Association for Computational Linguistics. URL
https: //www.aclweb.org/anthology/C18-1161.

Dan Gillick. Sentence Boundary Detection and the Problem with the U.S. In Pro-
ceedings of Human Language Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, Com-
panion Volume: Short Papers, pages 241-244, Boulder, Colorado, 2009. Associa-
tion for Computational Linguistics. URL http://www.aclweb.org/anthology/
N/NO9/NO9- 2061.

Peiyuan Gong, Xuebo Liu, Heyan Huang, and Min Zhang. Revisiting Grammatical
Error Correction Evaluation and Beyond. In Yoav Goldberg, Zornitsa Kozareva,
and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pages 6891-6902, Abu Dhabi, United Arab Emi-
rates, 12 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.
emnlp-main.463. URL https: //aclanthology.org/2022.emnlp-main. 463.

Carlos-Emiliano Gonzalez-Gallardo and Juan-Manuel Torres-Moreno. WiSeBE:
Window-based Sentence Boundary Evaluation. In Advances in Computational
Intelligence: Proceedings of the17th Mexican International Conference on Artifi-
cial Intelligence (Part II), MICAI 2018, pages 119-131, Guadalajara, Mexico, 10
2018. Springer International Publishing. doi: 10.1007/978-3-030-04497-8. URL
https: //www. springer .com/gp/book/9783030044961.

Maurice Gross. Méthodes en syntaxe. Hermann, 1975.


BIBLIOGRAPHY 177

Younggyun Hahm, Jiseong Kim, Sunggoo Kwon, and Key-Sun Choi. Semi-automatic
Korean FrameNet Annotation over KAIST Treebank. In Nicoletta Calzolari,
Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hi-
toshi Isahara, Bente Maegaard, Joseph Mariani, Héléne Mazo, Asuncion Moreno,
Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of
the Eleventh International Conference on Language Resources and Evaluation
(LREC 2018), Miyazaki, Japan, 5 2018. European Language Resources Associ-
ation (ELRA). ISBN 979-10-95546-00-9.

Chung-Hye Han, Na-Rae Han, Eon-Suk Ko, Martha Palmer, and Heejong Yi. Penn
Korean Treebank: Development and Evaluation. In Proceedings of the 16th Pacific
Asia Conference on Language, Information and Computation, pages 69-78, Jeju,
Korea, 2002. Pacific Asia Conference on Language, Information and Computation.

Sunhae Han. Les predicats nominaux en coreen : Constructions a verbe support
hata. PhD thesis, Université Paris 7 - Denis Diderot, Paris, France, 2000. URL
https: //www.theses.fr/2000PA070002.

Charles Hinson, Hen-Hsen Huang, and Hsin-Hsi Chen. Heterogeneous Recycle Gen-
eration for Chinese Grammatical Error Correction. In Donia Scott, Nuria Bel,
and Chengqing Zong, editors, Proceedings of the 28th International Conference on
Computational Linguistics, pages 2191-2201, Barcelona, Spain (Online), 12 2020.
International Committee on Computational Linguistics. doi: 10.18653/v1/2020.
coling-main.199. URL https: //aclanthology.org/2020.coling-main.199.

Jeen-Pyo Hong. Korean Part-Of-Speech Tagger using Eojeol Patterns (M.S. Thesis).
Technical report, Changwon National University, Changwon, 2009.

Byung-sun Hwang. A Study on Interpretation of the Korean Tense. The Korean
Language and Literature, 79(1):309-346, 2003.

Eunkyul Leah Jo, Angela Yoonseo Park, and Jungyeul Park. A Novel Alignment-
based Approach for PARSEVAL Measuress. Computational Linguistics, 50
(3):1181-1190, 9 2024a. doi: 10.1162/coli{\_}a{\_}00512. URL https://
aclanthology.org/2024.c1-3.10.

Eunkyul Leah Jo, Angela Yoonseo Park, Grace Tianjiao Zhang, Izia Xiaoxiao Wang,
Junrui Wang, MingJia Mao, and Jungyeul Park. An Untold Story of Preprocessing
Task Evaluation: An Alignment-based Joint Evaluation Approach. In Nicoletta
Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and
Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on


178 BIBLIOGRAPHY

Computational Linguistics, Language Resources and Evaluation (LREC-COLING
2024), pages 1327-1338, Torino, Italy, 5 2024b. ELRA and ICCL. URL https:
//aclanthology.org/2024.1lrec-main. 119.

Mark Johnson. PCFG Models of Linguistic Tree Representations . Computational
Linguistics, 24(4):613-632, 1998. URL http://aclweb. org/anthology-new/J/
J98/ 598-4004. pdf.

Aravind K. Joshi and Yves Schabes. Tree-Adjoining Grammars and Lexicalized
Grammars. Technical report, Department of Computer and Information Science,
University of Pennsylvania, Philadelphia, Pennsylvania, USA, 1991. URL https:
//repository.upenn.edu/handle/20.500.14332/7375.

Aravind K. Joshi, Leon S$. Levy, and Masako Takahashi. Tree Adjunct Grammars.
Journal of Computer and System Sciences, 10(1):136-163, 1975.

Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jé-
gou, and Tomas Mikolov. Fast'Text.zip: Compressing text classification models.
http://araziv.org/abs/1612.03651, 12 2016. URL http://arxiv.org/abs/1612.
03651.

Sangkeun Jung, Changki Lee, and Hyunsun Hwang. End-to-End Korean Part-of-
Speech Tagging Using Copying Mechanism. AC'M Transactions on Asian and Low-
Resource Language Information Processing (TALLIP), 17(3):19:1-19:8, 2018. ISSN
2375-4699. doi: 10.1145/3178458. URL http: //doi.acm. org/10.1145/3178458.

David Jurgens and Ioannis Klapaftis. SemEval-2013 Task 13: Word Sense Induction
for Graded and Non-Graded Senses. In Suresh Manandhar and Deniz Yuret, edi-
tors, Second Joint Conference on Lexical and Computational Semantics (*{SEM}),
Volume 2: Proceedings of the Seventh International Workshop on Semantic Evalu-
ation (SemEval 2013), pages 290-299, Atlanta, Georgia, USA, 6 2013. Association
for Computational Linguistics. URL https: //aclanthology.org/S13-2049.

Juyeon Kang. Problémes morpho-syntaxiques analysés dans un modeéle catégoriel
étendu : application au coréen et au frangais avec une réalisation informatique.
PhD thesis, Université Paris IV - Paris-Sorbonne, Paris, France, 2011. URL http:
//www.sudoc.fr/161287484.

Jungo Kasai, Bob Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. TAG Pars-
ing with Neural Networks and Vector Representations of Supertags. In Proceed-
ings of the 2017 Conference on Empirical Methods in Natural Language Process-


BIBLIOGRAPHY 179

ing, pages 1712-1722, Copenhagen, Denmark, 9 2017. Association for Computa-
tional Linguistics. doi: 10.18653/v1/D17-1180. URL https://aclanthology.
org/D17-1180.

Jungo Kasai, Robert Frank, Pauli Xu, William Merrill, and Owen Rambow. End-to-
End Graph-Based TAG Parsing with Neural Networks. In Proceedings of the 2018
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1181—
1194, New Orleans, Louisiana, 6 2018. Association for Computational Linguistics.
doi: 10.18653/v1/N18-1107. URL https: //aclanthology.org/N18-1107.

Anisia Katinskaia, Maria Lebedeva, Jue Hou, and Roman Yangarber. Semi-
automatically Annotated Learner Corpus for Russian. In Nicoletta Calzolari,
Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry
Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Héléne
Mazo, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Thirteenth Lan-
guage Resources and Evaluation Conference, pages 832-839, Marseille, France, 6
2022. European Language Resources Association. URL https://aclanthology.
org/2022.1lrec-1.88.

Jeong-uk Kim, Younggyun Hahm, and Key-Sun Choi. Korean FrameNet Expansion
Based on Projection of Japanese FrameNet. In Proceedings of COLING 2016,
the 26th International Conference on Computational Linguistics: System Demon-
strations, pages 175-179, Osaka, Japan, 12 2016. The COLING 2016 Organizing
Committee. URL https: //aclanthology.org/C16-2037.

Mija Kim and Jungyeul Park. A note on constituent parsing for Korean. Natural
Language Engineering, 28(2):199-222, 2022. doi: 10.1017 /S1351324920000479.

Tibor Kiss and Jan Strunk. Unsupervised Multilingual Sentence Boundary Detection.
Computational Linguistics, 32(4):485-525, 2006. ISSN 0891-2017. URL http:
//aclweb.org/anthology/J/J06/JO6-4003. pdf.

Dan Klein and Christopher D. Manning. Accurate Unlexicalized Parsing. In Proceed-
ings of the 41st Annual Meeting of the Association for Computational Linguistics,
pages 423-430, Sapporo, Japan, 2003. Association for Computational Linguis-
tics. doi: 10.3115/1075096.1075150. URL http: //www.aclweb. org/anthology/
PO3- 1054.

Kil Soo Ko. La syntare du syntagme nominal et Vertraction du complément du
nom en coréen : description, analyse et comparaison avec le francais. PhD thesis,


180 BIBLIOGRAPHY

Université Paris 7 - Denis Diderot, Paris, France, 2010. URL https: //www.sudoc.
fr/150031696.

Philipp Koehn. Europarl: A Parallel Corpus for Statistical Machine Translation. In
Proceedings of The Tenth Machine Translation Summit X, pages 79-86, Phuket,
Thailand, 2005. International Association for Machine Translation.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Fed-
erico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses:
Open Source Toolkit for Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Sessions, pages 177-180,
Prague, Czech Republic, 2007. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/P07-2045.

Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. Parser
Showdown at the Wall Street Corral: An Empirical Investigation of Error Types
in Parser Output. In Proceedings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computational Natural Language Learn-
ing, pages 1048-1059, Jeju Island, Korea, 7 2012. Association for Computational
Linguistics. URL http: //www.aclweb. org/anthology/D12-1096.

Kenton Lee, Mike Lewis, and Luke Zettlemoyer. Global Neural CCG Parsing with
Optimality Guarantees. In Proceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2366-2376, Austin, Texas, 11 2016. As-
sociation for Computational Linguistics. URL https://aclweb.org/anthology/
D1i6-1262.

Mike Lewis, Kenton Lee, and Luke Zettlemoyer. LSTM CCG Parsing. In Proceedings
of the 2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 221-231, San
Diego, California, 6 2016. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/N16- 1026.

Donghoon Lim. The Mood and Modal systems in Korean. Korean Semantics, 26(2):
211-248, 2008.

Donghoon Lim. Sentence types in Korean. Journal of Korean Linguistics, 60(1):
323-359, 2011.


BIBLIOGRAPHY 181

Kyungtae Lim and Jungyeul Park. Real-world Sentence Boundary Detection using
Multi-Task Learning: A Case Study on French. Natural Language Engineering, 30
(1):150-170, 2024. doi: https://doi.org/10.1017/51351324922000134.

KyungTae Lim, Cheoneum Park, Changki Lee, and Thierry Poibeau. SEx BiST:
A Multi-Source Trainable Parser with Deep Contextualized Lexical Representa-
tions. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
Raw Text to Universal Dependencies, pages 143-152, Brussels, Belgium, 10 2018.
Association for Computational Linguistics. doi: 10.18653/v1/K18-2014. URL
https: //aclanthology.org/K18-2014.

Edward Loper and Steven Bird. NLTK: The Natural Language Toolkit. In Proceed-
ings of the ACL-02 Workshop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational Linguistics, pages 63-70,
Philadelphia, Pennsylvania, USA, 7 2002. Association for Computational Linguis-
tics. doi: 10.3115/1118108.1118117. URL http: //www.aclweb. org/anthology/
WO2-0109.

Wei Lu and Hwee Tou Ng. Better Punctuation Prediction with Dynamic Conditional
Random Fields. In Proceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 177-186, Cambridge, MA, 10 2010. Associa-
tion for Computational Linguistics. URL https: //aclanthology.org/D10-1018.

Mohamed Maamouri and Ann Bies. Developing an Arabic Treebank: Methods,
Guidelines, Procedures, and Tools. In Proceedings of the Workshop on Computa-
tional Approaches to Arabic Script-based Languages, pages 2-9, Geneva, Switzer-
land, 8 2004. COLING. URL https://aclanthology.org/W04- 1602.

Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard,
and David McClosky. The Stanford CoreNLP Natural Language Processing
Toolkit. In Kalina Bontcheva and Jingbo Zhu, editors, Proceedings of 52nd Annual
Meeting of the Association for Computational Linguistics: System Demonstrations,
pages 55-60, Baltimore, Maryland, 6 2014. Association for Computational Linguis-
tics. doi: 10.3115/v1/P14-5010. URL https: //aclanthology.org/P14-5010.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a
Large Annotated Corpus of English: The Penn Treebank. Computational linguis-
tics, 19(2):313-330, 1993. URL https://aclanthology.org/J93-2004.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. Probabilistic CFG with La-
tent Annotations. In Proceedings of the 43rd Annual Meeting of the Association


182 BIBLIOGRAPHY

for Computational Linguistics (ACL’05), pages 75-82, Ann Arbor, Michigan, 6
2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219850.
URL https://www.aclweb.org/anthology/P05-1010.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Di-
panjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Tack-
strom, Claudia Bedini, Nuria Bertomeu Castello, and Jungmee Lee. Universal
Dependency Annotation for Multilingual Parsing. In Hinrich Schuetze, Pascale
Fung, and Massimo Poesio, editors, Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), pages
92-97, Sofia, Bulgaria, 8 2013. Association for Computational Linguistics. URL
https: //aclanthology.org/P13-2017.

Robert C. Moore. Fast and Accurate Sentence Alignment of Bilingual Corpora. In
Stephen D. Richardson, editor, Proceedings of the 5th Conference of the Associ-
ation for Machine Translation in the Americas on Machine Translation: From
Research to Real Users, pages 135-244, Tiburon, CA, USA, 2002. Springer-Verlag.

Seung-Hoon Na. Conditional Random Fields for Korean Morpheme Segmentation
and POS Tagging. ACM Transactions on Asian and Low-Resource Language In-
formation Processing, 14(3):1-10, 2015. ISSN 2375-4699. doi: 10.1145/2700051.
URL http: //doi.acm. org/10.1145/2700051.

Jee-Sun Nam. Classification syntaxique des constructions adjectivales en coréen.
PhD thesis, Université Paris 7 - Denis Diderot, Paris, France, 1994. URL https:
//waw.theses.fr/1994PA070040.

Jakub Naplava, Milan Straka, Jana Strakové, and Alexandr Rosen. Czech Grammar
Error Correction with a Large and Diverse Corpus. Transactions of the Associa-
tion for Computational Linguistics, 10:452-467, 2022. doi: 10.1162/tacl{\_ }a{\_
}00470. URL https: //aclanthology.org/2022.tacl-1.26.

Courtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. Ground Truth
for Grammatical Error Correction Metrics. In Chengqing Zong and Michael Strube,
editors, Proceedings of the 58rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), pages 588-593, Beijing, China, 7
2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-2097. URL
https: //aclanthology.org/P15-2097.


BIBLIOGRAPHY 183

Yun-Chae Nho. Les constructions converses du coréen : études des prédicats nom-
inauz. PhD thesis, Université Paris 7 - Denis Diderot, Paris, France, 1992. URL
https: //www.theses.fr/1992PA070049.

Jens Nilsson and Joakim Nivre. MaltEval: an Evaluation and Visualization Tool
for Dependency Parsing. In Proceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC’08), Marrakech, Morocco, 5 2008. Eu-
ropean Language Resources Association (ELRA). URL http://www.1lrec-conf.
org/proceedings/1lrec2008/pdf/52_paper. pdf.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic,
Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia
Silveira, Reut Tsarfaty, and Daniel Zeman. Universal Dependencies v1: A Mul-
tilingual Treebank Collection. In Luis von Ahn, editor, Proceedings of the Tenth
International Conference on Language Resources and Evaluation (LREC 2016),
page 1659-1666, Portoroz, Slovenia, 2016. European Language Resources Associ-
ation (ELRA). URL https: //www.aclweb.org/anthology/L16-1262/0A.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Haji¢, Christopher D.
Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman.
Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection. In
Proceedings of the 12th Language Resources and Evaluation Conference, pages
4034-4043, Marseille, France, 5 2020. European Language Resources Associa-
tion. ISBN 979-10-95546-34-4. URL https: //www.aclweb.org/anthology/2020.
lrec-1.497.

Franz Josef Och. Minimum Error Rate Training in Statistical Machine Transla-
tion. In Proceedings of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160-167, Sapporo, Japan, 2003. Association for Compu-
tational Linguistics. doi: 10.3115/1075096.1075117. URL http://www.aclweb.
org/anthology/P03- 1021.

Jin-Young Oh and Jeong-Won Cha. Korean Dependency Parsing using Key Eojoel.
Journal of KIISE:Software and Applications, 40(10):600-608, 2013.

Jin-Young Oh, Yo-Sub Han, Jungyeul Park, and Jeong-Won Cha. Predicting Phrase-
Level Tags Using Entropy Inspired Discriminative Models. In International Con-
ference on Information Science and Applications (ICISA) 2011, pages 1-5, Jeju,
Korea, 2011. Information Science and Applications (ICISA).


184 BIBLIOGRAPHY

Kyoko Hirose Ohara, Seiko Fujii, Hiroaki Saito, Shun Ishizaki, Toshio Ohori, and
Ryoko Suzuki. The Japanese FrameNet Project: A Preliminary Report. In Pro-
ceedings of the conference Pacific Association for Computational Linguistics (PA-
CLING ’03), pages 249-254, Halifax, Canada, 2003. Pacific Association for Com-
putational Linguistics, Pacific Association for Computational Linguistics.

Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub, and Oleksandr
Skurzhanskyi. GECToR — Grammatical Error Correction: Tag, Not Rewrite.
In Proceedings of the Fifteenth Workshop on Innovative Use of NIP for Building
Educational Applications, pages 163-170, Seattle, WA, USA — Online, 7 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.bea-1.16. URL
https: //aclanthology.org/2020.bea-1.16.

Hyong-Ik Pak. Lexique-grammaire du coréen : construction a verbes datifs. PhD
thesis, Université Paris 7 - Denis Diderot, Paris, France, 1987. URL https://
www.theses.fr/1987PA070140.

David D. Palmer and Marti A. Hearst. Adaptive Multilingual Sentence Boundary
Disambiguation. Computational Linguistics, 23(2):241—267, 1997. ISSN 0891-2017.
URL http: //aclweb.org/anthology/J/J06/J06-4003. pdf.

Martha Palmer, Daniel Gildea, and Paul Kingsbury. The Proposition Bank: An
Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71-
106, 2005. doi: 10.1162/0891201053630264. URL https://doi.org/10.1162/
0891201053630264.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a Method
for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics, pages 311-318, Philadel-
phia, Pennsylvania, USA, 7 2002. Association for Computational Linguistics. doi:
10.3115/1073083.1073135. URL http: //www.aclweb. org/anthology/P02-1040.

Chulwoo Park. The Grammatical Voice in Korean: an Interface Phenomenon be-
tween Syntax and Semantics. Korean Linguistics, 37(1):207-228, 2007.

Jungyeul Park. Extraction automatique d’une grammaire d’arbres adjoints a partir
d’un corpus arboré pour le coréen. PhD thesis, Université Paris 7 - Denis Diderot,
Paris, France, 2006. URL http://www.sudoc.fr/107995174.


BIBLIOGRAPHY 185

Jungyeul Park. Segmentation Granularity in Dependency Representations for Ko-
rean. In Proceedings of the Fourth International Conference on Dependency Lin-
guistics (Depling 2017), pages 187-196, Pisa, Italy, 2017. Association for Computa-
tional Linguistics. URL http: //aclweb. org/anthology/W/W17/W17-6522. pdf.

Jungyeul Park and Mija Kim. A role of functional morphemes in Korean categorial
grammars. Korean Linguistics, 19(1):1-80, 2023. doi: 10.1075/k1.22003.par. URL
https: //doi.org/10.1075/k1.22003. par.

Jungyeul Park and Mija Kim. Word segmentation granularity in Korean. Korean Lin-
guistics, 20(1):83-113, 2024. URL https: //benjamins.com/catalog/k1.00008.
par.

Jungyeul Park and Francis Tyers. A New Annotation Scheme for the Sejong Part-of-
speech Tagged Corpus. In Proceedings of the 18th Linguistic Annotation Workshop,
pages 195-202, Florence, Italy, 8 2019. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/W19- 4022.

Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi, and Key-Sun Choi.  To-
wards Fully Lexicalized Dependency Parsing for Korean. In Proceedings of the
13th International Conference on Parsing Technologies (IWPT 2018), pages 120-
126, Nara, Japan, 11 2013. Assocation for Computational Linguistics. URL
https: //www.aclweb.org/anthology/W13-5714.

Jungyeul Park, Sejin Nam, Youngsik Kim, Younggyun Hahm, Dosam Hwang, and
Key-Sun Choi. Frame-Semantic Web : a Case Study for Korean. In [SWC-PD’1/:
Proceedings of the 2014 International Conference on Posters & Demonstrations
Track - Volume 1272, pages 257-260, Riva del Garda, Italy, 10 2014. International
Semantic Web Conference. URL https://dl.acm.org/doi/10.5555/2878453.
2878518.

Jungyeul Park, Jeen-Pyo Hong, and Jeong-Won Cha. Korean Language Resources
for Everyone. In Proceedings of the 30th Pacific Asia Conference on Language,
Information and Computation: Oral Papers (PACLIC 30), pages 49-58, Seoul,
Korea, 2016. Pacific Asia Conference on Language, Information and Computation.
URL http: //aclweb.org/anthology/Y/Y16/Y16-2002. pdf.

Jungyeul Park, Loic Dugast, Jeen-Pyo Hong, Chang-Uk Shin, and Jeong-Won
Cha. Building a Better Bitext for Structurally Different Languages through
Self-training. In Proceedings of the First Workshop on Curation and Applica-
tions of Parallel and Comparable Corpora, pages 1-10, Taipei, Taiwan, 11 2017.


186 BIBLIOGRAPHY

Asian Federation of Natural Language Processing. URL http://www.aclweb.
org/anthology/W17-5601.

Jungyeul Park, Junrui Wang, Eunkyul Jo, and Angela Park. jp-evalb: Robust
Alignment-based PARSEVAL Measures. In Kai-Wei Chang, Annie Lee, and
Nazneen Rajani, editors, Proceedings of the 2024 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies (Volume 3: System Demonstrations), pages 70-77, Mexico
City, Mexico, 6 2024. Association for Computational Linguistics. URL https:
//aclanthology.org/2024.naacl-demo.7.

Sounnam Park. La construction des verbes neutres en coreen. PhD thesis, Université
Paris 7 - Denis Diderot, Paris, France, 1996. URL https://www.theses.fr/
1996PA070023.

Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Ji Yoon Han, Jang-
won Park, Chisung Song, Junseong Kim, Youngsook Song, Taehwan Oh, Joohong
Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo,
Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do,
Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin,
Seonghyun Kim, Lucy Park, Alice Oh, Jung-Woo Ha, and Kyunghyun Cho.
KLUE: Korean Language Understanding Evaluation. In Joaquin Vanschoren and
Serena Yeung, editors, Proceedings of the Neural Information Processing Sys-
tems Track on Datasets and Benchmarks, volume 1, pages 1-25. Curran, 2021.
URL https: //datasets-benchmarks- proceedings. neurips.cc/paper_files/
paper/2021/file/98dce83da57b0395e163467 c9dae521b-Paper-round2. pdf.

Slav Petrov and Dan Klein. Improved Inference for Unlexicalized Parsing. In Human
Language Technologies 2007: The Conference of the North American Chapter of
the Association for Computational Linguistics; Proceedings of the Main Confer-
ence, pages 404-411, Rochester, New York, 2007. Association for Computational
Linguistics. URL http: //www.aclweb.org/anthology/N/NO7/NO7-1051.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning Accurate,
Compact, and Interpretable Tree Annotation. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages 433-440, Sydney, Australia,
2006. Association for Computational Linguistics. doi: 10.3115/1220175.1220230.
URL http: //www.aclweb.org/anthology/P06- 1055.


BIBLIOGRAPHY 187

Slav Petrov, Dipanjan Das, and Ryan McDonald. A Universal Part-of-Speech Tagset.
In Proceedings of the Eighth International Conference on Language Resources and
Evaluation (LREC-2012), pages 2089-2096, Istanbul, Turkey, 2012. European Lan-
guage Resources Association (ELRA). ISBN 978-2-9517408-7-7.

Martin Polaéek, Petr Cerva, Jindyich Zdansky, and Lenka Weingartova. Online
Punctuation Restoration using ELECTRA Model for streaming ASR Systems. In
Proceedings of INTERSPEECH 2028, pages 446-450, Dublin, Ireland, 2023. Inter-
national Speech Communication Association. doi: 10.21437 /Interspeech.2023-664.

Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D Man-
ning. Stanza: A Python Natural Language Processing Toolkit for Many Hu-
man Languages. In Asli Celikyilmaz and Tsung-Hsien Wen, editors, Proceed-
ings of the 58th Annual Meeting of the Association for Computational Linguis-
tics: System Demonstrations, pages 101-108, Online, 7 2020. Association for
Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.14. URL https:
//aclanthology.org/2020.acl-demos. 14.

Gaoqi Rao, Qi Gong, Baolin Zhang, and Endong Xun. Overview of NLPTEA-2018
Share Task Chinese Grammatical Error Diagnosis. In Yuen-Hsien Tseng, Hsin-Hsi
Chen, Vincent Ng, and Mamoru Komachi, editors, Proceedings of the 5th Workshop
on Natural Language Processing Techniques for Educational Applications, pages
42-51, Melbourne, Australia, 7 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-3706. URL https://aclanthology.org/W18-3706.

Gaoqi Rao, Erhong Yang, and Baolin Zhang. Overview of NLPTEA-2020 Shared
Task for Chinese Grammatical Error Diagnosis. In Erhong YANG, Endong XUN,
Baolin ZHANG, and Gaoqi RAO, editors, Proceedings of the 6th Workshop on
Natural Language Processing Techniques for Educational Applications, pages 25—
35, Suzhou, China, 12 2020. Association for Computational Linguistics. URL
https: //aclanthology.org/2020.nlptea-1.4.

Lev Ratinov and Dan Roth. Design Challenges and Misconceptions in Named En-
tity Recognition. In Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL-2009), pages 147-155, Boulder, Colorado,
2009. Association for Computational Linguistics. URL http: //www.aclweb.org/
anthology/W09-1119.

Jonathon Read, Rebecca Dridan, Stephan Oepen, and Lars Jergen Solberg. Sen-
tence Boundary Detection: A Long Solved Problem? In Proceedings of COLING


188 BIBLIOGRAPHY

2012: Posters, pages 985-994, Mumbai, India, 2012. The COLING 2012 Organiz-
ing Committee. URL http: //www.aclweb. org/anthology/C12-2096.

Jeffrey C. Reynar and Adwait Ratnaparkhi. A Maximum Entropy Approach to
Identifying Sentence Boundaries. In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing, pages 16-19, Washington, DC, USA, 3 1997.
Association for Computational Linguistics. doi: 10.3115/974557.974561. URL
http://www.aclweb. org/anthology/A97- 1004.

Brian Roark, Mary Harper, Eugene Charniak, Bonnie Dorr, Mark Johnson, Jeremy
Kahn, Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart, and Lisa Yung. SParseval:
Evaluation Metrics for Parsing Speech. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation (LREC’06), pages 333-338,
Genoa, Italy, 5 2006. European Language Resources Association (ELRA). URL
http://www.lrec-conf .org/proceedings/lrec2006/pdf/116_pdf . pdf.

Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei
Severyn. A Simple Recipe for Multilingual Grammatical Error Correction. In
Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 702-707, Online, 8 2021. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.89. URL
https: //aclanthology.org/2021.acl-short.89.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger.
Multiword Expressions: A Pain in the Neck for NLP. In Proceedings of the Third
International Conference on Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ’02, pages 1-15, London, UK, UK, 2002. Springer-Verlag. ISBN
3-540-43219-1. URL http://dl.acm. org/citation. cfm?id=647344.724004.

Kenji Sagae and Alon Lavie. A Classifier-Based Parser with Linear Run-Time Com-
plexity. In Proceedings of the Ninth International Workshop on Parsing Technol-
ogy (IWPT2005), pages 125-132, Vancouver, British Columbia, 2005. Associa-
tion for Computational Linguistics. URL http://www.aclweb.org/anthology/
W/WO5/W05-1513.

Bahar Salehi, Paul Cook, and Timothy Baldwin. Determining the Multiword Ex-
pression Inventory of a Surprise Language. In Proceedings of COLING 2016, the
26th International Conference on Computational Linguistics: Technical Papers,


BIBLIOGRAPHY 189

pages 471-481, Osaka, Japan, 2016. The COLING 2016 Organizing Committee.
URL http: //aclweb. org/anthology/C16- 1046.

Djamé Seddah, Reut Tsarfaty, Sandra Kiibler, Marie Candito, Jinho D. Choi,
Richard Farkas, Jennifer Foster, lakes Goenaga, Koldo Gojenola Galletebeitia,
Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier,
Joakim Nivre, Adam Przepiérkowski, Ryan Roth, Wolfgang Seeker, Yannick Ver-
sley, Veronika Vincze, Marcin Woliriski, Alina Wréblewska, and Eric Villemonte
de la Clergerie. Overview of the SPMRL 2013 Shared Task: A Cross-Framework
Evaluation of Parsing Morphologically Rich Languages. In Proceedings of the
Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages
146-182, Seattle, Washington, USA, 10 2013. Association for Computational Lin-
guistics. URL http://www.aclweb. org/anthology/W13-4917.

Djamé Seddah, Sandra Kiibler, and Reut Tsarfaty. Introducing the SPMRL 2014
Shared Task on Parsing Morphologically-rich Languages. In Proceedings of the
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 103-109, Dublin, Ire-
land, 8 2014. Dublin City University. URL https: //www.aclweb.org/anthology/
W1i4-6111.

Kwang-Soon Shin. Le verbe support hata en coréen contemporain : morpho-syntaxe
et comparaison. PhD thesis, Université Paris 7 - Denis Diderot, Paris, France,
1994. URL https: //www.theses.fr/1994PA070027.

Ankur Sonawane, Sujeet Kumar Vishwakarma, Bhavana Srivastava, and Anil Ku-
mar Singh. Generating Inflectional Errors for Grammatical Error Correction in
Hindi. In Boaz Shmueli and Yin Jou Huang, editors, Proceedings of the 1st Confer-
ence of the Asia-Pacific Chapter of the Association for Computational Linguistics
and the 10th International Joint Conference on Natural Language Processing: Stu-
dent Research Workshop, pages 165-171, Suzhou, China, 12 2020. Association for
Computational Linguistics. URL https://aclanthology.org/2020.aacl-srw.
24.

Hyun-Je Song and Seong-Bae Park. Korean Part-of-Speech Tagging Based on Mor-
pheme Generation. ACM Transactions on Asian and Low-Resource Language
Information Processing (TALLIP), 19(3):1-41, 1 2020. ISSN 2375-4699. doi:
10.1145/3373608. URL https://doi.org/10.1145/3373608.

Jae Mog Song. Semantic functions of the non - terminal suffix - te - in Korean : from
a typological perspective. Journal of Korean Linguistics, 32(1):135-169, 1998.


190 BIBLIOGRAPHY

Milos Stanojevié and Mark Steedman. Max-Margin Incremental CCG Parsing. In
Proceedings of the 58th Annual Meeting of the Association for Computational Lin-
guistics, pages 4111-4122, Online, 7 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.acl-main.378. URL https://www.aclweb.org/
anthology/2020.acl-main.378.

Milan Straka and Jana Strakova. ‘Tokenizing, POS Tagging, Lemmatizing and
Parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task:
Multilingual Parsing from Raw Text to Universal Dependencies, pages 88-99,
Vancouver, Canada, 8 2017. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/K17-3009.

Milan Straka, Jan Hajic, Jana Strakova, and Jan Haji¢ Jr. Parsing Universal Depen-
dency 'Treebanks using Neural Networks and Search-Based Oracle. In Proceedings
of the 14th International Workshop on Treebanks and Linguistic Theories (TLT
2015), pages 1-13, Warszawa; Poland, 2015.

Milan Straka, Jan Hajic, and Jana Strakové. UDPipe: Trainable Pipeline for Pro-
cessing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS
Tagging and Parsing. In Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016), pages 4290-4297, Paris, France,
5 2016. European Language Resources Association (ELRA). ISBN 978-2-9517408-
9-1.

Jana Strakova, Milan Straka, and Jan Haji¢. Open-Source Tools for Morphol-
ogy, Lemmatization, POS Tagging and Named Entity Recognition. In Kalina
Bontcheva and Jingbo Zhu, editors, Proceedings of 52nd Annual Meeting of the
Association for Computational Linguistics: System Demonstrations, pages 13-18,
Baltimore, Maryland, 6 2014. Association for Computational Linguistics. doi:
10.3115/v1/P14-5003. URL https: //aclanthology.org/P14-5003.

Karl Stratos. A Sub-Character Architecture for Korean Language Processing. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, pages 732-737, Copenhagen, Denmark, 9 2017. Association for Com-
putational Linguistics. URL https://www.aclweb. org/anthology/D17-1076.

Karl Stratos, Michael Collins, and Daniel Hsu. Unsupervised Part-Of-Speech Tag-
ging with Anchor Hidden Markov Models. Transactions of the Association for
Computational Linguistics, 4:245-257, 2016. ISSN 2307-387X. URL https:
//tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/837.


BIBLIOGRAPHY 191

Ann Taylor, Mitchell Marcus, and Beatrice Santorini. The Penn Treebank: An
Overview. In Anne Abeillé, editor, Treebanks: Building and Using Parsed Cor-
pora, pages 5-22. Springer Netherlands, Dordrecht, 2003. ISBN 978-94-010-0201-
1. doi: 10.1007/978-94-010-0201-1{\_}1. URL https://doi.org/10.1007/
978-94-010-0201-1_1.

Brian Thompson and Philipp Koehn. Vecalign: Improved Sentence Alignment in
Linear Time and Space. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Con-
ference on Natural Language Processing (EMNLP-IJCNLP), pages 1342-1348,
Hong Kong, China, 11 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1136. URL https: //aclanthology.org/D19- 1136.

Marcos Treviso, Christopher Shulby, and Sandra Aluisio. Evaluating Word Embed-
dings for Sentence Boundary Detection in Speech Transcripts. In Proceedings of
the 11th Brazilian Symposium in Information and Human Language Technology,
pages 151-160, Uberlandia, Brazil, 10 2017. Sociedade Brasileira de Computagao.
URL https://www.aclweb.org/anthology/W17-6618.

Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. Joint Evaluation of Morpholog-
ical Segmentation and Syntactic Parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics (Volume 2: Short Papers),
pages 6-10, Jeju Island, Korea, 7 2012. Association for Computational Linguistics.
URL http: //www.aclweb.org/anthology/P12-2002.

Daniel Varga, Lézl6 Németh, Péter Halacsy, Andrés Kornai, Viktor Tron, and Viktor
Nagy. Parallel corpora for medium density languages. In Proceedings of the RANLP
(Recent Advances in Natural Language Processing), pages 590-596, Borovets, Bul-
garia, 2005.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geof-
frey E. Hinton. Grammar as a Foreign Language. In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 28, pages 2773-2781. Curran Associates, Inc., 2015. URL
http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.

Junrui Wang, Mengyang Qiu, Yang Gu, Zihao Huang, and Jungyeul Park. Refined
Evaluation for End-to-End Grammatical Error Correction Using an Alignment-
Based Approach. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend
Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of


192 BIBLIOGRAPHY

the 31st International Conference on Computational Linguistics, pages '774—785,
Abu Dhabi, UAE, 1 2025. Association for Computational Linguistics. URL https:
//aclanthology.org/2025.coling-main.52/.

Yang Wei, Yuanbin Wu, and Man Lan. A Span-based Linearization for Con-
stituent Trees. In Proceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 3267-3277, Online, 7 2020. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.299. URL
https: //aclanthology.org/2020.acl-main. 299.

Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. The Penn Chinese
TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207-238, 6 2005. ISSN 1351-3249. doi: 10.1017/
9135132490400364X. URL https: //doi.org/10.1017/S135132490400364x.

Hiroyasu Yamada and Yuji Matsumoto. Statistical Dependency Analysis with Sup-
port Vector Machines. In Proceedings of the 8th International Workshop on Parsing
Technologies (IWPT 2003), pages 195-206, Nancy, France, 2003.

Ryosuke Yamaki, Tadahiro Taniguchi, and Daichi Mochihashi. Holographic CCG
Parsing. In Proceedings of the 61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages 262-276, Toronto, Canada, 7
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.
15. URL https://aclanthology.org/2023.acl-long. 15.

Helen Yannakoudakis, Wistein E. Andersen, Ardeshir Geranpayeh, Ted Briscoe, and
Diane Nicholls. Developing an automated writing placement system for ESL
learners. Applied Measurement in Education, 31(3):251-267, 7 2018. ISSN 0895-
7347. doi: 10.1080/08957347.2018.1464447. URL https://doi.org/10.1080/
08957347 .2018. 1464447.

Soyoung Yoon, Sungjoon Park, Gyuwan Kim, Junhee Cho, Kihyo Park, Gyu Tae
Kim, Minjoon Seo, and Alice Oh. Towards standardizing Korean Grammatical
Error Correction: Datasets and Annotation. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-
pers), pages 6713-6742, Toronto, Canada, 7 2023. Association for Computational
Linguistics. URL https://aclanthology.org/2023.acl-long. 371.

Seunghak Yu, Nilesh Kulkarni, Haejun Lee, and Jihie Kim. Syllable-level Neu-
ral Language Model for Agglutinative Language. In Proceedings of the First


BIBLIOGRAPHY 193

Workshop on Subword and Character Level Models in NLP, pages 92-96, Copen-
hagen, Denmark, 9 2017. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/W17-4113.

Daniel Zeman, Martin Popel, Milan Straka, Jan Haji¢, Joakim Nivre, Filip Gin-
ter, Juhani Luotolahti, Sampo Pyysalo, Slav Petrov, Martin Potthast, Francis
Tyers, Elena Badmaeva, Memduh Gokirmak, Anna Nedoluzhko, Silvie Cinkova,
Jan Haji¢ Jr., Jaroslava Hlavatova, Vaclava Kettnerova, Zdenka UreSova, Jenna
Kanerva, Stina Ojala, Anna Missila, Christopher D. Manning, Sebastian Schus-
ter, Siva Reddy, Dima Taji, Nizar Habash, Herman Leung, Marie-Catherine de
Marneffe, Manuela Sanguinetti, Maria Simi, Hiroshi Kanayama, Valeria de Paiva,
Kira Droganova, Héctor Martinez Alonso, Cagri Céltekin, Umut Sulubacak, Hans
Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marhei-
necke, Georg Rehm, Tolga Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran
Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fer-
nandez Alcalde, Jana Strnadova, Esha Banerjee, Ruli Manurung, Antonio Stella,
Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonga, Tatiana Lando, Rattima
Nitisaroj, and Josie Li. CoNLL 2017 Shared Task: Multilingual Parsing from
Raw Text to Universal Dependencies. In Proceedings of the CoNLL 2017 Shared
Task: Multihngual Parsing from Raw Text to Universal Dependencies, pages 1—
19, Vancouver, Canada, 8 2017. Association for Computational Linguistics. doi:
10.18653/v1/K17-3001. URL https: //aclanthology.org/K17-3001.

Daniel Zeman, Jan Hajic, Martin Popel, Martin Potthast, Milan Straka, Filip Gin-
ter, Joakim Nivre, and Slav Petrov. CoNLL 2018 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies. In Daniel Zeman and Jan
Hajic, editors, Proceedings of the CoNLL 2018 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages 1-21, Brussels, Belgium,
10 2018. Association for Computational Linguistics. doi: 10.18653/v1/K18-2001.
URL https://aclanthology.org/K18-2001.

Min Zeng, Jiexin Kuang, Mengyang Qiu, Jayoung Song, and Jungyeul Park. Evalu-
ating Prompting Strategies for Grammatical Error Correction Based on Language
Proficiency. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro
Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint
International Conference on Computational Linguistics, Language Resources and
Evaluation (LREC-COLING 2024), pages 6426-6430, Torino, Italy, 5 2024. ELRA
and ICCL. URL https://aclanthology.org/2024.1lrec-main. 569.

Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li, Bo Zhang, Chen Li, Fei Huang,


194 BIBLIOGRAPHY

and Min Zhang. MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset
for Chinese Grammatical Error Correction. In Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 3118-3130, Seattle, United States, 7 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.
227. URL https: //aclanthology.org/2022.naacl-main. 227.

Yue Zhang, Bo Zhang, Haochen Jiang, Zhenghua Li, Chen Li, Fei Huang, and
Min Zhang. NaSGEC: a Multi-Domain Chinese Grammatical Error Correction
Dataset from Native Speaker Texts. In Anna Rogers, Jordan Boyd-Graber, and
Naoaki Okazaki, editors, Findings of the Association for Computational Lin-
guistics: ACL 20238, pages 9935-9951, Toronto, Canada, 7 2023. Association
for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.630. URL
https: //aclanthology.org/2023.findings-acl.630/.

Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and Xiaojun Wan. Overview of the NLPCC
2018 Shared Task: Grammatical Error Correction. In Min Zhang, Vincent Ng,
Dongyan Zhao, Sujian Li, and Hongying Zan, editors, Natural Language Process-
ing and Chinese Computing, pages 439-445, Cham, 2018. Springer International
Publishing. ISBN 978-3-319-99501-4.
