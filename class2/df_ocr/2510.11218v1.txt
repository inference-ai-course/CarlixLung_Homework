arXiv:2510.11218vl1 [cs.CL] 13 Oct 2025

The Curious Case of Factual (Mis)Alignment
between LLMs’ Short- and Long-Form Answers

Saad Obaid ul Islam!

Anne Lauscher?

Goran Glavas!

'WiNLP, CAIDAS, University of Wiirzburg
{saad.obaid-ul-islam, goran.glavas}@uni-wuerzburg.de
?Data Science Group, University of Hamburg
anne. lauscher@uni-hamburg.de

Abstract

Large language models (LLMs) can correctly
answer "When was Einstein born?" yet fail to
provide the same date when writing about Ein-
stein’s life—trevealing a fundamental inconsis-
tency in how models access factual knowledge
across task complexities. While models dis-
play impressive accuracy on factual question-
answering benchmarks, the reliability gap be-
tween simple and complex queries remains
poorly understood, eroding their trustworthi-
ness. In this work, we introduce Short-Long
Form Alignment for Factual Question Answer-
ing (SLAQ), a controlled evaluation framework
that compares LLMs’ answers to the same fac-
tual questions asked (a) in isolation (short)
vs. (b) integrated into complex queries (Jong).
Looking at 16 LLMs across 600 queries, we
find a systematic misalignment of answers to
the corresponding short and long queries. We
further uncover position-dependent accuracy
loss and momentum effects where consecu-
tive correct or incorrect answers create self-
reinforcing patterns. Through mechanistic anal-
ysis, we find that aligned facts activate overlap-
ping model internals, and that metrics based
on mechanistic similarity can predict short-
long answer alignment with up to 78% accu-
racy. Our work establishes factual consistency
over query complexity as an important aspect
of LLMs’ trustworthiness and challenges cur-
rent evaluation practices, which implicitly as-
sume that good performance for simple fac-
tual queries implies reliability in more complex
knowledge-seeking tasks too.

1 Introduction

Large Language Models (LLMs) (Team et al.,
2023; Achiam et al., 2023; Grattafiori et al., 2024;
Yang et al., 2025a) are rapidly being adopted across
diverse applications, including education (Kasneci
et al., 2023), healthcare (Qiu et al., 2023), software
engineering (Fan et al., 2023), and general knowl-
edge search (Xu et al., 2023). Their utility and

trustworthiness are, however, compromised by their
tendency to hallucinate (Wang et al., 2023a) and
generate fictitious responses (Huang et al., 2025).

While earlier research on LLM evaluation exten-
sively examined factual accuracy in closed-domain
question answering (QA) for both short-form (Ra-
jpurkar et al., 2016; Joshi et al., 2017; Yang et al.,
2018) and long-form responses (Fan et al., 2019;
Dasigi et al., 2021), these evaluation benchmarks
have become somewhat outdated, since LLMs
are now primarily deployed as chat-based infor-
mation gathering assistants across a wide variety
real-world applications (Xu et al., 2023), i-e., they
are primarily used as (chat-based) open-domain
question-answering tools. Accordingly, factuality-
oriented evaluations have shifted toward open-
domain QA, considering both short-form (Lin et al.,
2022; Wei et al., 2024b) and long-form responses
(Min et al., 2023; Wei et al., 2024b; ul Islam et al.,
2025). Existing benchmarks, however, evaluate
short-form and long-form factuality in isolation,
and thus fail to assess factual consistency of mod-
els’ responses over query complexity: Will an LLM
yield the same answer to the same factual question
for queries of varying complexity?

In this work, we address this gap by intro-
ducing Short-Long Form Alignment for Factual
Question Answering (SLAQ), a novel evaluation
framework that tests whether models maintain an-
swer consistency—with respect to fact-seeking
questions—across queries of different complexity.
SLAQ presents an LLM with the same fact-seeking
questions, formulated (independently) in two dis-
tinct query formats: (1) long queries combine
five topically related factual questions, whereas
(2) short queries formulate those same questions
independently and ask them in isolation. With
this controlled design, we isolate the impact of
query/response complexity on factual answer ac-
curacy. By comparing the factual correctness of
models’ answers to long vs. short queries, we can


Dataset

Topic: Punic Wars

qi: When did the First Punic War occur?

q2: How long did the Punic Wars last?

a3: Which two powers fought in the Punic Wars?

q4: Who was the renowned Carthaginian general in

the Second Punic War?

q5: What did Hannibal famously cross Italy with?
LQ: Discuss the Punic Wars by covering (1) when

the First Punie War occurred, (2) how long the Punic
Wars lasted, (3) which two powers fought,(4) who
the renowned Carthaginian general in the Second

Fact Verification with
LLM-as-a-judge

Response Collection

} Short
Llama-3.x Answers
{a*q1...
a*qs}
Quwen-2.5/3

Evaluation

[ Compute overall Short
| form factuality F_S. '

Compute overall Long |
form factuality F_L.

Compute raw Alignment. |:

Compute direction of

Punic War was,and (5) what he famously crossed
bahewiis

% Gemma-2/3 Answer
; a A*LQ

Alignment.

Figure 1: Illustration of our Short-Long Form Alignment for Factual Question Answering (SLAQ) framework.
An instance in our SLAQ benchmark is a complex knowledge-seeking query, i.e., a long query, which consists of
five simple factual sub-queries, i.e., short queries, each with an unambiguous correct answer. LLMs independently
generate the answers to (1) the long query (i.e., all five short queries combined) and (2) each of the five short queries
in isolation. We use a state-of-the-art commercial LLM to judge the correctness of the generated answers to both
the Jong query and short queries against the set of reference answers; we use these judgments to compute models’
short- and long-form accuracy (F's, F’,) as well as the short-long alignment scores.

disentangle knowledge gaps (incorrect answers for
both query formats) from answer retrieval failures
(e.g., correct answer for the short query but incor-
rect for the long). Figure 1 illustrates SLAQ.
Studying 16 LLMs through the lens of SLAQ, we
find that, while models exhibit substantial short-
long alignment w.r.t. factual answer correctness,
most of this alignment stems from incorrect an-
swers, i.e., LLMs produce incorrect answers for
the same factual question in both long and short-
form queries (but it is not necessarily the same an-
swer). We observe that models consistently demon-
strate higher factual accuracy in responses to short
queries than in long-query responses: the majority
of misalignment cases thus stem from a (1) cor-
rect answer to the short query and an (2) incorrect
answer to the corresponding question included in
the Jong query. Beyond evaluating factual ques-
tion answering accuracy for both query formats,
we identify two critical patterns in model behavior
for long-form responses: (1) position-dependent
degradation, where factual accuracy declines sub-
stantially from 51% for facts appearing early in
responses to 30% for facts appearing later, and
(2) momentum effects, where consecutive correct
answers increase the likelihood of subsequent accu-
racy, while errors tend to cascade and compound.
To understand the mechanistic basis of the ob-
served factual misalignment, we next analyze the
model internals— attention and MLP activation
patterns—and identify minimal sets of model com-
ponents responsible for answer generation for short
and long-form queries, respectively. Using zero-
ablation (Olsson et al., 2022) activation patching
(Meng et al., 2022), we find that aligned answers

activate significantly more similar computational
pathways and exhibit stronger correlations in com-
ponent importance rankings. Moreover, we show
that these circuit-level differences have predictive
power: employing six pathway similarity metrics,
we can predict with 78% accuracy (ROC-AUC:
0.85) whether the answers to the same factual ques-
tion will align between the two query formats, short
and long; here we identify attention head rank cor-
relation as the most predictive feature.

Contributions. In sum, the contributions of this
work are threefold: (1) We establish factual consis-
tency over query complexity as an important aspect
of LLM reliability and introduce SLAQ, a novel
dataset for benchmarking such consistency; (2) We
document systematic factual misalignment (i.e., in-
consistency) patterns in LLMs, and relate factual
correctness of the responses to position effects and
momentum dynamics; (3) We provide mechanistic
evidence that this factual misalignment stems from
divergent internal processing, demonstrating that
circuit overlap metrics can predict alignment out-
comes. This work represents the first systematic
investigation of factual consistency over query com-
plexity in open-domain QA. Our findings challenge
a fundamental (implicit) assumption of modern
LLM evaluation: that factual knowledge that LLMs
exhibit for simple queries with straightforward fac-
tual questions propagates reliably to complex sce-
narios, where the same factual questions are part

of more complex knowledge-seeking queries.

'Github: https://github.com/WorldHellow/SLAQ


2 Background and Related Work

We provide a brief overview of background and
related work on (1) hallucinations and factuality in
open-domain QA, and (2) mechanistic interpretabil-
ity and its application to understanding factuality.

Hallucinations and Factuality. Evaluating fac-
tual accuracy in LLMs has evolved from simple
to complex formats. Early benchmarks like Triv-
iaQA (Joshi et al., 2017) and Natural Questions
(Kwiatkowski et al., 2019) evaluated LLMs on
closed-domain QA. But these benchmarks are now
saturated, and evaluation of LLMs has moved from
closed-domain to open-domain QA, with Truth-
fulQA (Lin et al., 2022) and SimpleQA (Wei et al.,
2024a) as two popular benchmarks that evaluate
LLMs for single factoid answers.

With respect to factual accuracy in long-form
LLM responses, FactScore (Min et al., 2023) eval-
uates LLMs on Wikipedia biographies. More re-
cently, LongFact (Wei et al., 2024b) and UNCLE
(Yang et al., 2025b) were proposed to evaluate long-
form factual accuracy across diverse domains. UN-
CLE is a concurrent effort to ours and, similarly
to our work, pairs short and long queries/prompts:
however, it analyses the short- and long-form in
isolation and studies uncertainty expression rather
than factual consistency of LLMs’ responses be-
tween the two query formats.

Several systematic phenomena have been ob-
served regarding hallucinations in LLMs. The
“snowballing” effect (Zhang et al., 2024) describes
how models justify a wrong claim by generating
additional false assertions. The “lost in the middl”
phenomena (Liu et al., 2024) shows input-position
sensitivity in closed-domain QA: accuracy peaks
when evidence appears at the beginning or end of
a long context and degrades when relevant infor-
mation lies in the middle. Complementing input-
position effects, Yang et al. (2025b) find that hal-
lucinations in long-document summarization oc-
cur more often near the end of generated outputs
(“hallucinate at the end”’), indicating degradation
dependent on the output position.

Mechanistic Interpretability (MI) aims to reverse-
engineer how neural networks result in specific be-
haviors by identifying causal computational struc-
tures (Elhage et al., 2021; Zhang et al., 2024).
The foundation of MI is localization: determining
which model components (attention heads, MLP
layers, neurons) are responsible for particular out-
puts. The primary technique for measuring compo-

nent importance is activation patching (Meng et al.,
2022), which quantifies causal influence through
intervention. The process involves: (1) comput-
ing baseline output logits @hase for the correct to-
ken, (2) ablating each component individually to
obtain fablateq, (3) measuring the importance as nor-
malized logit difference: |Cbase — abtatea| /|4basel.
where larger values indicate greater causal impor-
tance. Components are then assembled into mini-
mal circuits through greedy search (Conmy et al.,
2023; Hanna et al., 2024)—iteratively adding com-
ponents in importance order until the subset repro-
duces the original behavior within a faithfulness
threshold (Wang et al., 2023b). Two ablation strate-
gies exist: zero-ablation (Olsson et al., 2022) sets
component outputs to zero, while counterfactual
patching replaces them with activations from dif-
ferent inputs (Meng et al., 2022). In this work, for
computational efficiency, we resort to zero-ablation
when identifying component sets.

A significant amount of work focused on iden-
tifying parameters in which models store factual
information. ROME (Meng et al., 2022) localizes
factual associations to mid-layer MLPs, for which
Geva et al. (2023) further show that they function as
key-value memories. Yao et al. (2024) trace factual
retrieval circuits, revealing collaborative knowl-
edge encoding across attention heads and MLPs.
Limited work exists on comparing circuits between
tasks: Mondorf et al. (2024) find high node overlap
for compositionally similar tasks, while Hanna et al.
(2025) report minimal overlap between formal and
functional linguistic circuits. These studies share
a critical limitation: they analyze single-token out-
puts, ignoring the complexity of realistic free-form
multi-token answers.

3 Factual Consistency over Query
Complexity

Our goal is to capture the extent to which LLMs
provide consistent (i.e., factually equivalent) an-
swers to the very same fact-seeking questions, inte-
grated into queries of different complexity. To this
end, we introduce a novel task of factual answer
consistency over query complexity, for which we
create an evaluation benchmark.

3.1 Task Definition and Metrics

SLAQ tests whether LLMs provide consistent
answers to factual questions across different
query/response complexities. Because of this,


we organize the benchmark around fopics: a
topic t is a set of N topically related facts
{fi, fo,..-, fw}, for each of which SLAQ contains
a short-form question (SQ) that elicits the respec-
tive fact, {q1,q@2,---,qgn}. Each topic, as a set
of N facts, is additionally converted into a long
information-seeking (LQ) query: an example of a
topic t with N = 5 factual questions is given in Fig-
ure 1. The LLMs then independently respond to the
LQ, as well as to each of the N SQs. For each fac-
tual question g, (k € {1,2,...,N}), Sp € {0,1}
denotes the factual correctness of an LLM’s answer
to the SQ of that fact (1 = correct, 0 = incorrect)
whereas the L;, € {0, 1} indicates the correctness
for the same fact in the LLM’s answer to the LQ.

Alignment Definition. We declare that an LLM
produces a factually consistent response for a fact
fx if the SQ and LQ responses for that fact have the
same factual correctness label:

aligned(k) = 1{S, = Lx} (1)

where I{-} is the indicator function. Crucially,
alignment measures the consistency in factual
correctness of the answers, and not whether the
answers themselves are semantically equivalent.
When both responses are incorrect (S; = Ly = 0),
they are aligned w.r.t. factual correctness because
they are both incorrect. E.g., for question q; from
Figure 1, answers “264 to 243 BCE” (short) and
“264 to 241 AD” (long) are factuality-wise aligned
(both incorrect) because the correct answer is “264
to 241 BCE” We choose this alignment definition
as we aim to discern between (1) a knowledge gap
(i.e., both answers incorrect; irrelevant if they are
“the same incorrect’) and (2) failure to consistently
retrieve the knowledge that is stored in the model
(i.e., one answer correct, the other incorrect).

Evaluation Metrics. Following prior work (Min
et al., 2023; Wei et al., 2024a,b), we an LLM to
judge factual correctness by comparing responses
against gold answers. We characterize model be-
havior with following metrics: Short and Long
form factual Accuracy (F's and F’,), Alignment
score, and Signed Alignment score (Align+).

1 1
Fg = — S. Fp = — L 2
s N 22 Se L NW 2a be (2)

, i
Align = i S I{S, = Le} (3)

ead
lif Sp = Ly = 1,

N
Align, = = Arp, Arp= {ust =L,=0, (4)
> Oif Sp A Le.

F's and Fy, establish baseline performance and
quantify the accuracy gap between formats. Align
measures factual consistency regardless of correct-
ness. Raw alignment score conflate reliable knowl-
edge (both correct) and systematic failure (both
incorrect), which is why we introduce Align,
which additionally distinguishes the two correct-
ness cases: +1 (aligned, correct) vs. —1 (aligned,
incorrect), with 0 denoting factual misalignment.

3.2 Dataset

We construct SLAQ datasets from Wikipedia, lever-
aging its factual reliability and broad coverage. We
sample from 15 diverse English Wikipedia cate-
gories, selecting articles exceeding 1,000 words to
ensure sufficient factual density. To test models
across the knowledge popularity spectrum, we bal-
ance between popular and obscure topics, selecting
300 most-viewed and 300 least-viewed pages in
the past five years. This way, we take into account
evidence (Zhang et al., 2025) that fact frequency
drives LLMs’ hallucination.

Following the success of LLM as synthetic data
generators (Long et al., 2024) in open-domain QA
(Wei et al., 2024b; ul Islam et al., 2025; Yang et al.,
2025c) we employ a state-of-the-art commercial
LLM, OpenAI 03-mini-high, to generate factual
questions to which an answer exists in the article
content. The model receives the full Wikipedia text
and produces NV = 5 SQs targeting distinct facts,
plus one LQ that naturally elicits all five facts”.

We then manually verified all generated SQs and
LQs, ensuring their open-domain formulation (.e.,
that they are answerable without the source article)
as well as factual grounding (the correct answer
indeed exists in Wikipedia). We manually adjusted
the queries that did not meet both criteria.*. Overall,
we found 03-mini-high to be a reliable synthetic
data generator for this purpose: it introduced errors
in only 77 out of 3,600 (2.14%) query-answer pairs.

The SLAQ datasets covers 600 unique topics
(with 600 corresponding LQs) with 3000 SQs (N =
5), across 15 wikipedia categories (on average,
40 topics per category). We further profile the
SQs for fact-type, finding 1,071 entity-based facts
and 1,929 non-entity facts (definitions, properties,
equations, concepts)* The final SLAQ evaluation

"We provide the prompts for generating SQs, LQs, and
dataset samples in the §A.1

3 E.g., we identified 53 SQs and 24 LQs that violated open-
domain criteria and were judged as unanswerable without the

respective Wikipedia article
“For this labeling, we resort to the OntoNotes taxonomy.


60
50

40 y/o

30
20

39 39
i 26

10

47 46

(a) Factual Correctness

Factual Correctness (%)

G3-1B
G2-2B
G3-4B
G2-9B
G3-12B

(b) Alignment

77-9676 169g B77

74 75 75 75 75 75 73 75 75

L1B
L3B
L8B

ono
FOS
B83
Oo

Q3-4B
Q2.5-7B
Q3-8B
Q3-8B-R

mo
Se
mo N
oo

Q2.5-1.5
Q3-1.7B
Q2.5-3B
Q3-1.7B-

a

Align,

Q2.5-1.5
Q3-1.7B
Q2.5-3B
Q3-1.7B-
Q3-4B
Q2.5-7B
Q3-8B
Q3-8B-R

(c) Signed Alignment

Figure 2: Short—long factual alignment results across model families. (a) Factual Correctness: per-model short-
form accuracy F's (green) and long-form accuracy F’, (purple). (b) Alignment: Align = percentage of facts with
the same correctness label in short vs. long responses. (c) Signed Alignment: average over topics; for a single topic,

the score is the average of Align,
= Qwen-3, 8B parameters, R - reasoning).

dataset is thus both category-diverse and popularity-
balanced, and has a fair balance between factual
questions with entity vs. non-entity target answers.

4 Benchmarking and Evaluations

4.1 Experimental Setup

We evaluate models from five families, spanning
1B to 12B parameters: Qwen-2.5 (Yang et al.,
2024), Qwen-3 and Qwen-3-Reasoning (Yang et al.,
2025a), Llama-3 (Grattafiori et al., 2024), Gemma-
2 (Team et al., 2024), and Gemma-3 (Team et al.,
2025). All models use greedy decoding via the
Hugging Face API. We constrain short-form re-
sponses to single sentences and instruct models to
provide only requested information for long-form
queries (see Table 4 in the Appendix for prompts).

We employ Gemini-2.5-Flash (Team et al., 2023)
as our LLM judge, instructing it to judge answer
correctness based on semantic equivalence with the
gold answer rather than string matching. The LLM
judge agrees with the human annotator for 92.0%
and 94.8% of SQ and LQ responses, respectively
(see Appendix Tables 2 and 3 for prompts). We
then compute our evaluation metrics (Eq. 2—4) from
the judge’s binary correctness labels.

of its five facts. Models key: G = Gemma, L = Llama, Q = Qwen (e.g., Q3-8B-R

4.2 Results and Analysis

Figure 2 reveals three key patterns of factual
(in)consistency across the language models. Panel
(a) shows that most models achieve modest factual
accuracy of 30-50% on both SQ (F's) and LQ (F’,)
responses, with almost all models displaying higher
accuracy for short-form queries. Larger models dis-
play only modestly better performance, suggesting
that scale alone cannot dramatically improve fac-
tual recall.

Panel (b) shows remarkable consistency in raw
alignment across all models, with virtually ev-
ery model achieving 73-78% alignment regard-
less of size and architecture. Such uniformity in-
dicates that this level of factual consistency over
query complexity is an intrinsic property of mod-
ern LLMs, rather than something that can improve
with scale.

Panel (c) reveals the most critical finding: all
models show negative signed alignment (-0.01 to
-0.51), meaning they consistently provide wrong
answers in responses to both query formats more
often than correct answers for both cases. This
indicates that the high raw alignment primarily
reflects systematic failures rather than systematic
successes: models have developed stable internal
strategies for factual processing, but these strate-


(a) Slot Accuracy

60 5 60 5

50 4 50

40 4

30 4

Percent Correct (%)

20

(b) Trailing 1-streak

BO x (c) Trailing 0-streak
A Current slot

e —@ slot2 —t:- slot 4
50 4 —™- slot3 -@- slot 5

Slot (Sub-question Index)

Trailing 1-streak length

51.3
3 =
e fl
37.5 @ 40 i p
33.1 34.5 8 8
& L
30.1 fay 30 fal
|| i
20-4 T T T T 20 “4 T T T
1 2 3 4 5 ) 1 2 3 4 ) 1 2 3

Trailing 0-streak length

Figure 3: Long-form QA dynamics by sub-fact position. (a) Slot accuracy: percent correct for each fact position
(slots 1-5) in the LQ answer. (b) Trailing 1-streak: P(correct) for the current slot (2-5), conditioned on the length
of the immediately preceding run of correct slots. (c) Trailing 0-streak: P(correct) for the current slot (2-5),
conditioned on the length of the immediately preceding run of incorrect slots.

gies systematically fail to retrieve correct infor-
mation. The combination of high raw alignment
with negative signed alignment reveals that while
models are internally consistent in factual behavior,
most of it stems from generating incorrect answers
across query complexities.

Panel (a) shows that Fs is consistently larger
than F’ ae To better understand why F'y, is lower,
we next analyze LQ responses in more detail.

Position-dependent degradation. In Figure 3a,
we observe a monotonic decline in factual accuracy
based on the order in which facts are requested in
LQ prompts. Accuracy drops from 51.3% for the
first requested fact to 30.1% for the fifth, a 21.2
percentage point degradation. Unlike the U-shaped
patterns found in input processing (Liu et al., 2024),
this query-position effect is strictly linear, with ap-
proximately 5% accuracy loss per position. While
Yang et al. (2025b) documented hallucinations con-
centrating at response ends in the summarization
task, we find that in open-domain long-form QA,
degradation correlates with request order in the
prompt itself, suggesting that managing multiple
factual requirements imposes a cumulative load
that progressively impairs retrieval accuracy.

Momentum within a response. According to Fig-
ure 3b and 3c, following consecutive correct an-
swers (positive momentum), accuracy increases
from a 30% baseline to 57% after four correct
facts: each additional success adds roughly 7% to
subsequent accuracy. Conversely, consecutive er-
rors (negative momentum) reduce accuracy from

The only exceptions are the two smallest models in our
evaluation, Gemma-3 1B and Llama-3 1B, which yield very
low accuracy for both SQs and LQs.

45% to 24% after three mistakes. This not only con-
firms Zhang et al. (2024)’s finding of “snowballing”
for long-form QA but extends it by quantifying
the error propagation and success reinforcement ef-
fects. These momentum dynamics, combined with
position effects, explain why LQ responses system-
atically underperform short-form ones even when
eliciting the very same factual knowledge.

5 Mechanistic Analysis

Our behavioral analysis revealed that language
models exhibit systematic inconsistencies when
answering the same facts across short and long re-
sponse formats. This raises a fundamental question:
do these behavioral differences reflect distinct in-
ternal computational mechanisms? Understanding
the mechanistic basis of factual alignment could
inform interventions to improve consistency across
response formats. We hypothesize that factual
alignment corresponds to mechanistic similarity.
Formally, let sim(k) denote the mechanistic simi-
larity between short and long responses for fact k.
Our hypothesis predicts:

E[sim(k) | aligned] > E[sim(k) | misaligned] (5)

with alignment defined as in §3. We focus exclu-
sively on facts that are answered correctly in both
formats (aligned) versus facts where only one for-
mat is correct (misaligned). Put simply: facts an-
swered correctly in both short and long formats
should exhibit greater internal mechanism overlap
than facts answered correctly in only one format.


in Gm Aligned
“ [5 Misaligned p < 0.001
|
0.8 0.778
[) 0.710
an p < 0.001
5 ol
ee) 0.6 0.552
0.499
ou p < 0.001
0.4

0.316

0.272
IoU

Containment Spearman

MLP

p<0.01
0.909

0.882 p < 0.001

p < 0.001
a
0.673

0.600

Spearman
Attention

Similarity Metrics

Pearson
Attention

Pearson
MLP

Figure 4: Circuit similarity comparison between aligned and misaligned facts across six metrics. Aligned facts
(green) show significantly (p < 0.001) higher mechanistic similarity than misaligned facts (red) for all measures.

5.1 Preliminaries: Component Importance
via Zero-Ablation

We identify critical components by systematically
setting their outputs to zero and measuring how
much that hurts the model’s preference for the gold
token. For each component c and answer token t,
we measure importance using:

ablated

base _ logit’

logit,

importance(c, t) = (6)

base

logit;

This quantifies the change in logit magnitude when
a component c is removed, normalized by the base-
line logit. For each answer token, components are
ranked by importance. We greedily select compo-
nents (highest importance first) until we recover at
least 90% of the baseline logit, yielding the mini-
mal set C, needed for generating token t.

Similarity Metrics. We compare component sets
of responses to SQs and LQs with two metrics:

|Cshort 9 Ciong |

Containment = —
min(|Cshort|; |Ciongl)

(7)

ICshort al Clone
ICshort U Chong

with Cshort and Ciong being the component sets for
short and long responses, respectively. Contain-
ment measures core component sharing relative to
the smaller circuit, while IoU quantifies the overlap
symmetrically. We additionally measure Pearson
Correlation and Spearman Correlation between
the two sets of component importance scores: the
former captures the extent to which importance
scores match/deviate across components, whereas
the latter quantifies the extent to which the two
rankings of components (by decreasing importance
score) match.

Intersection-over-Union =

(8)

Multi-Token Alignment via Earth Mover’s Dis-
tance (EMD) Previous studies on LLMs’ fac-
tual revall (Meng et al., 2022; Geva et al., 2023;
Yao et al., 2024) focused on single-word answers.
However, real answers to factual questions span
multiple tokens, with variable lengths. Because
we extract component sets per token, we need to
aggregate token-level component-based similarity
scores into fact-level (i.e., answer-level) scores, tak-
ing into account different token boundaries (e.g.,

“Paris is capital of France” vs “the capital city Paris

is located in France”). We formulate component
comparison across multi-token answers as an opti-
mal transport problem: we compute pairwise sim-
ilarities between all SQ answer tokens {s;} and
corresponding LQ answer tokens {I;}, creating a
bipartite similarity matrix M!;;. EMD then finds
the transport plan matrix 7* (i.e., coefficients 77;;)
that maximizes the following total similarity:

aw =argmax > 1;;-M;; (9)
a y j j

The final fact-level similarity score is then the
weighted average of pairwise similarities with op-
timal transport weights: sim(k) = )/, ; 7; > Mij-
Unlike exact matching, EMD finds the best possi-
ble alignment of tokens between multi-token an-
swers, reflecting semantic equivalence between to-
kens regardless of their order in the answer.

5.2 Mechanistic Comparison

Experimental Setup We analyze four models span-
ning different scales: Qwen-2.5 (1B, 3B) and
Qwen-3 (1.7B, 4B). For each model, we select
60 short-long fact pairs that are divided into 30
correctly aligned pairs and 30 misaligned pairs.
This yields us 240 short-long fact pairs. For each
fact, we obtain: (1) Minimal component sets for all


(a) Individual Feature Performance

= Accuracy
Gg ROC-AUC

wil

loU Containment Spearman Spearman Peal
MLP Attention LP

Score
Score

Ate ention

(b) Combined Features

0.81
8 0.78
0.76
0.71
. |
6

°“Accuracy AUC Precision Recall

(c) Feature Importance
Spearman Attention 1.36
Containment 0.29
Spearman MLP 0.24
IoU 0.06
Pearson MLP — -0.11

Pearson Attention -0.20

25 0500.75
Coefficient

Figure 5: Predictive modeling performance using circuit similarity metrics. (a) Individual feature performance
shows Spearman Attention as the strongest single predictor of factual alignment (ROC-AUC = 0.83). (b) Combined
features achieve robust performance across all evaluation metrics (ROC-AUC = 0.81, Accuracy = 0.76). (c) Feature
importance reveals Spearman Attention as the dominant predictor (coefficient = 1.36).

answer tokens (2) Importance scores for all atten-
tion heads and MLP layers. We set the the greedy
threshold to 90% are are able to recover the original
token with 100% accuracy.

Results. Figure 4 reveals that aligned facts exhibit
systematically higher mechanistic similarity than
misaligned facts across all six metrics (p < 0.05 for
all comparisons). This provides direct evidence that
behavioral alignment reflects distinct internal mech-
anisms. Node IoU shows the largest relative dif-
ference, with aligned facts achieving 16.2% higher
similarity (0.316 vs 0.272). Containment shows
a 10.6% increase (0.552 vs 0.499). These gaps
suggest that consistent responses recruit overlap-
ping computational pathways, while inconsistent
responses activate different mechanisms. Spear-
man correlations are consistently higher than Pear-
son correlations for both attention (0.909 vs 0.744)
and MLP components (0.778 vs 0.673), suggesting
that components’ rank by importance is more rele-
vant than their raw activation magnitudes. In other
words, factual consistency depends on maintain-
ing stable computational pathways, not identical
activation patterns.

5.3. Model Internals as Predictors

Building on our finding that aligned facts exhibit
significantly higher mechanistic similarity, we in-
vestigate whether these similarity metrics can pre-
dict factual alignment. We train a logistic regres-
sion classifier using the six similarity metrics as fea-
tures on a dataset of 240 fact pairs (120 aligned, 120
misaligned) across four models (Qwen-2.5 1B/3B,
Qwen-3 1.7B/4B). We evaluate performance via
5-fold cross-validation to assess both individual fea-
ture contributions and combined predictive power.

Results and Analysis. The predictive modeling

results in Figure 5 validate the mechanistic basis of
factual alignment and reveal which similarity met-
rics best capture this phenomenon. Spearman cor-
relation over attention components emerges as the
strongest individual predictor of alignment (ROC-
AUC = 0.83), indicating that attention importance
hierarchies provide the most reliable measurable
signal of factual consistency. The combined feature
model achieves robust performance (ROC-AUC =
0.81, Accuracy = 0.76), with logistic regression co-
efficients revealing the underlying computational
logic: Spearman attention dominates with a coef-
ficient of 1.36, whereas Pearson correlation gets a
negative coefficient: this suggests that linear mag-
nitude similarities actually predict misalignment.
This convergence in findings between (i) similarity
analysis and (ii) predictive modeling shows that
mechanistic interpretability can offer not just ex-
planatory insights into model behavior but also
practical tools for predicting inconsistencies in
LLMs responses for the same factual questions.

6 Conclusion

This work establishes factual consistency over
query complexity as a critical dimension of LLM
reliability. Through SLAQ, we demonstrate that
LLMs exhibit systematic misalignment when an-
swering identical factual questions embedded in
queries of varying complexity. Our analysis re-
veals predictable patterns: factual accuracy de-
grades substantially with response position, declin-
ing from 51% for early facts to 30% for later ones,
and correctness exhibits momentum effects. Our
mechanistic analysis provides the first empirical
evidence that behavioral factual alignment corre-
sponds to similar internal mechanisms. Through
zero-ablation, we found that aligned facts exhibit


higher mechanistic similarity. The predictive mod-
eling results demonstrate practical applications,
with mechanistic similarity metrics achieving 78%
accuracy in predicting factual alignment. Future
work should investigate targeted interventions on
circuit patterns to address the consistency failures
identified in LLMs.

Limitations

SLAQ Dataset and Framework The SLAQ dataset
and framework have several limitations. (1) The
dataset is synthetically generated. While a fully
human-annotated dataset would be ideal, it is cost-
prohibitive; using LLMs as data generators of-
fers a more balanced quality—resource trade-off.
We mitigate this limitation by manually verify-
ing each prompt—answer pair and find that Ope-
nAI’s 03-mini-high is a strong data generator for
the SLAQ task (§3.2). (2) The dataset—and this
work—currently covers only English. (3) For eval-
uation, we adopt the LLM-as-a-judge paradigm to
assess factual correctness; in our experiments, how-
ever, we find Gemini-2.5-Flash achieves high
agreement with human annotations (see §4).

Mechanistic Analysis In our mechanistic analysis,
we use zero ablation rather than counterfactual ac-
tivation patching. Although counterfactual activa-
tion patching can yield more precise results (Zhang
and Nanda; Heimersheim and Nanda, 2024), con-
structing counterfactual prompts for complex gen-
eration tasks (short- and long-form) is challenging
and labor-intensive, especially when relevant facts
are distributed across multiple tokens. More im-
portantly, our goal is to demonstrate mechanistic
differences between short- and long-form factual
retrieval under fact misalignment, which we show
in §5 using zero ablation.

7 Acknowledgments

This work was supported by the Alcatel-Lucent
Stiftung and Deutsches Stiftungszentrum through
the grant “Equitably Fair and Trustworthy
Language Technology” (EQUIFAIR, Grant Nr.
T0067/43110/23). The work of Anne Lauscher
is funded under the Excellence Strategy of the Ger-
man Federal Government and States. The authors
gratefully acknowledge the computing time granted
by the John von Neumann Institute for Computing
(NIC) and provided on the supercomputer JURECA
(Jiilich Supercomputing Centre, 2021) at Jiilich Su-
percomputing Centre (JSC).

References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774.

Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch,
Stefan Heimersheim, and Adria Garriga-Alonso.
2023. Towards automated circuit discovery for mech-
anistic interpretability. Advances in Neural Informa-
tion Processing Systems, 36:16318—16352.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Noah A Smith, and Matt Gardner. 2021. A dataset
of information-seeking questions and answers an-
chored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 4599-4610.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al.
2021. A mathematical framework for transformer
circuits. Transformer Circuits Thread, 1(1):12.

Angela Fan, Beliz Gokkaya, Mark Harman, Mitya
Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M
Zhang. 2023. Large language models for software
engineering: Survey and open problems. In 2023
IEEE/ACM International Conference on Software
Engineering: Future of Software Engineering (ICSE-
FoSE), pages 31-53. IEEE.

Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. Eli5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 3558-3567.

Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual associa-
tions in auto-regressive language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing, pages 12216-12235,
Singapore. Association for Computational Linguis-
tics.

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
Alex Vaughan, et al. 2024. The llama 3 herd of mod-
els. arXiv preprint arXiv:2407.21783.

Michael Hanna, Yonatan Belinkov, and Sandro Pezzelle.
2025. Are formal and functional linguistic mech-
anisms dissociated in language models? arXiv
preprint arXiv:2503.11302.

Michael Hanna, Sandro Pezzelle, and Yonatan Belinkov.
2024. Have faith in faithfulness: Going beyond cir-
cuit overlap when finding model mechanisms. arXiv
preprint arXiv:2403.17806.


Stefan Heimersheim and Neel Nanda. 2024. How to
use and interpret activation patching. arXiv preprint
arXiv:2404, 15255.

Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025.
A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions.
ACM Transactions on Information Systems, 43(2):1-
D9:

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

Jiilich Supercomputing Centre. 2021. JURECA: Data
Centric and Booster Modules implementing the Mod-
ular Supercomputing Architecture at Jiilich Super-
computing Centre. Journal of large-scale research
facilities, 7(A182).

Enkelejda Kasneci, Kathrin SeBler, Stefan Ktichemann,
Maria Bannert, Daryna Dementieva, Frank Fischer,
Urs Gasser, Georg Groh, Stephan Giinnemann, Eyke
Hiillermeier, et al. 2023. Chatgpt for good? on op-
portunities and challenges of large language models
for education. Learning and individual differences,
103:102274.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Ilia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics, 7:452—466.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
Truthfulqa: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3214-3252.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2024. Lost in the middle: How language mod-
els use long contexts. Transactions of the Association
for Computational Linguistics, 12:157-173.

Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao
Ding, Gang Chen, and Haobo Wang. 2024. On
LLMs-driven synthetic data generation, curation, and
evaluation: A survey. In Findings of the Associa-
tion for Computational Linguistics: ACL 2024, pages
11065-11082, Bangkok, Thailand. Association for
Computational Linguistics.

Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associa-
tions in gpt. Advances in neural information process-
ing systems, 35:17359-17372.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. Factscore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, pages 12076-12100.

Philipp Mondorf, Sondre Wold, and Barbara Plank.
2024. Circuit compositions: Exploring modular
structures in transformer-based language models.
arXiv preprint arXiv:2410.01434.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.
In-context learning and induction heads. arXiv
preprint arXiv:2209.11895.

Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun
Shi, Ruiyang Zhang, Yinzhao Dong, Kyle Lam,
Frank P-W Lo, Bo Xiao, et al. 2023. Large ai models
in health informatics: Applications, challenges, and
the future. EEE Journal of Biomedical and Health
Informatics, 27(12):6074—6087.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv: 1606.05250.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, Katie
Millican, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805.

Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya
Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,
Tatiana Matejovicova, Alexandre Ramé, Morgane
Riviére, et al. 2025. Gemma 3 technical report. arXiv
preprint arXiv:2503.19786.

Gemma Team, Morgane Riviere, Shreya Pathak,
Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-
raju, Léonard Hussenot, Thomas Mesnard, Bobak
Shahriari, Alexandre Ramé, et al. 2024. Gemma 2:
Improving open language models at a practical size.
arXiv preprint arXiv:2408.00118.

Saad Obaid ul Islam, Anne Lauscher, and Goran Glavas.
2025. How much do Ilms hallucinate across lan-
guages? on multilingual estimation of Ilm hallucina-
tion in the wild. ArXiv, abs/2502.12769.

Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru
Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao,
Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang,
Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang,
and Yue Zhang. 2023a. Survey on factuality in large
language models: Knowledge, retrieval and domain-
specificity. ArXiv, abs/2310.07521.


Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Buck Shlegeris, and Jacob Steinhardt. 2023b. Inter-
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations.

Jason Wei, Nguyen Karina, Hyung Won Chung,
Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John
Schulman, and William Fedus. 2024a. Measuring
short-form factuality in large language models. arXiv
preprint arXiv:2411.04368.

Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,
Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng,
Ruibo Liu, Da Huang, et al. 2024b. Long-form factu-
ality in large language models. Advances in Neural
Information Processing Systems, 37:830756—80827.

Ruiyun Xu, Yue Feng, and Hailiang Chen. 2023. Chat-
gpt vs. google: A comparative study of search
performance and user experience. arXiv preprint
arXiv:2307.01135.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Ly, et al. 2025a. Qwen3
technical report. arXiv preprint arXiv:2505.09388.

Joonho Yang, Seunghyun Yoon, Hwan Chang, Byeong-
jeong Kim, and Hwanhee Lee. 2025b. Hallucinate
at the last in long response generation: A case study
on long document summarization. arXiv preprint
arXiv:2505.15291.

Qwen An Yang, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-
ran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei
Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jun-
yang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin
Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin
Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,
Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang
Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu
Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan,
and Zekun Wang. 2024. Qwen2.5 technical report.
ArXiv, abs/2412.15115.

Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting
Huang, Dong Yu, Nigel Collier, and Deqing Yang.
2025c. Uncle: Uncertainty expressions in long-form
generation. arXiv preprint arXiv:2505. 16922.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D Manning. 2018. Hotpotqa: A dataset for
diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics.

Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang,
Ziwen Xu, Shumin Deng, and Huajun Chen. 2024.
Knowledge circuits in pretrained transformers. Ad-
vances in Neural Information Processing Systems,

37:118571-118602.

Fred Zhang and Neel Nanda. Towards best practices of
activation patching in language models: Metrics and
methods. In The Twelfth International Conference
on Learning Representations.

Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and
Noah A. Smith. 2024. How language model hallu-
cinations can snowball. In Forty-first International
Conference on Machine Learning.

Yuji Zhang, Sha Li, Cheng Qian, Jiateng Liu, Pengfei
Yu, Chi Han, Yi R Fung, Kathleen McKeown,
Chengxiang Zhai, Manling Li, et al. 2025. The law of
knowledge overshadowing: Towards understanding,
predicting, and preventing Ilm hallucination. arXiv
preprint arXiv:2502.16143.

A Appendix

A.1 Dataset Construction

We will release the SLAQ dataset under an open
scientific license.

Scraping Wikipedia We collect articles from 15
curated categories. For each category, we use the
MediaWiki API to list main-namespace pages and
randomly sample candidates. For each candidate,
we download the page, extract the main text, and
remove tables, images, scripts/styles, hatnotes, nav-
igation boxes, and the table of contents. We col-
lapse whitespace and discard pages shorter than
a minimum length of 1000 characters. For every
accepted page, we query the Wikimedia Pageviews
API to get daily counts over the past 1,095 days
and sum them into one popularity score. Within
each category, we sort by pageviews and split the
list in half to form most and least popular sets.

Synthetic Data Generation We _ generate
short/long queries from the scraped Wikipedia
text using OpenAl’s 03-mini-high model (API
name o3-mini-2025-01-31, invoked with
reasoning_effort=high). For each article,
the prompt instructs the model to: (i) produce
exactly five self-contained short questions
(ShortQ1-ShortQ5) with single-fact, single-
answer constraints and no source referencing; (ii)
give concise answers (ShortA1-ShortA5, each
under 10 words) derived strictly from the provided
text (no parametric knowledge); (iii) compose a
long question (LongQ) that explicitly lists those
five sub-prompts; and (iv) write a fluent long
answer (LongA) that synthesizes only the five
short answers—no extra facts. The instruction
emphasizes an “open-domain phrasing” test (the
subject must be uniquely identifiable outside
the Wikipedia article context), bans ambiguous


“pick one of many” list questions, and enforces
a Strict, single-line output format. The total cost
of constructing the dataset via 03-mini-high was
$18.57. The prompt for generating the dataset
is provided in Table 1 and the dataset schema is
provided in Figure 6.

Topic Categories Following is a list of all the cate-
gories of the topics in the SLAQ dataset: Aesthetics,
Algebra, Anthropology, Applied sciences, Artifi-
cial intelligence, Astronomy, Cultural studies, Soci-
ology, Software engineering, Spirituality, Statistics,
Technology, Telecommunications, Theology, Vir-
tual reality

A.2. LLM-as-a-judge

To evaluate LLM-as-a-jduge, we test on 50 samples
(250 short questions and 50 long questions) and in
total we have 500 atomic facts to be evaluate by
LLM. We find LLM to have a an accuracy of 92.%
on short-from and 94% in long-form responses.

Conditions for Incorrect or Correct: If any of the
following conditions were met, the response was
labeled as incorrect: (1) Factual inaccuracy, (2) Se-
mantic dissimilarity, (3) IrrelevanceOmmission, (4)
Contradiction, (5) Hallucinations. For correctness,
following conditions have to be met: (1) Direct
Semantic Equivalance, (2) Subset/Superset Equiv-
alance, (3) World Knowledge Override (Only used
sparsely), (4) Correct Vagueness. These condic-
tions were developed progressively on a validation
set of 50 samples and then tested on 50 samples
separately. To understand the definition of the each
condition, we recommend the reader to go through
the prompts in Table 2 and 3.

A.3 Inference

We generate responses for each short and long
query using the Hugging Face API. All models
are executed on NVIDIA H100 (80GB) GPU with
greedy decoding. The total GPU time required to
generate all responses amounts to 192 hours.

We comply with the licensing agreement and
adhere to the intended use of each of the open-
source and closed-source LLMs.

A.4 Mechanistic Interpretability

We employ the NNsight framework to perform
zero ablation on an NVIDIA H100 (80GB) GPU.
The total compute time for zero ablation is 768
hours. Subsequently, we manually pair each short-
form fact SQ; with the minimal corresponding

span of the fact in the long-form response. This
manual pairing process required a total of 16 hours.

Optimal Transport for Mechanistic Overlap We
compute mechanistic overlap using optimal trans-
port algorithms, including the Hungarian algorithm
and Earth Mover’s Distance (EMD). Both methods
yield consistent results—aligned responses exhibit
higher mechanistic overlap.


"Category”: "History",
"Topic”: "The Punic Wars”,
"URL": "https://en.wikipedia.org/wiki/Punic_Wars”,
"ShortQ1”: "When did the First Punic War occur?”,
"ShortAl": "It took place from 264 to 241 BCE.”,
"ShortQ2": "How long did the Punic Wars last?”,
"ShortA2": "They lasted a total of 43 years.”,
"ShortQ3": "Which two powers fought in the Punic Wars?”,
"ShortA3": "Rome and Carthage.”,
"ShortQ4": "Who was the renowned Carthaginian general in the Second Punic War?”,
"ShortA4”: "Hannibal Barca was the famous general.”,
"ShortQ5”: "What did Hannibal famously cross Italy with?”,
"ShortA5": "Hannibal crossed Italy with war elephants.”,
"LongQ” : "Discuss the Punic Wars by covering (1) when the First Punic War occurred,
(2) how long the Punic Wars lasted, (3) which two powers fought,
(4) who the renowned Carthaginian general in the Second Punic War was,

and (5) what he famously crossed Italy with.”,
"LongA” : "The First Punic War took place from 264 to 241 BCE,

and the three Punic Wars altogether lasted about 43 years between

Rome and Carthage. In the Second Punic War, Hannibal Barca rose as

the famed Carthaginian commander, becoming legendary for crossing Italy with war elephants.”,

"Pageviews”: ...,
"ShortQ1_Entity”:
"ShortQ2_Entity”:
"ShortQ3_Entity”:
"ShortQ4_Entity”:
"ShortQ5_Entity”:

Wi Hk SH

Figure 6: Example SLFA dataset entry instantiated for the Punic Wars.


Prompt

Given a *reference articlex (plain text), generate **xexactlyxx:
1) *x*Fivexx short Q/A pairs: ‘ShortQ1-ShortQ5‘, ‘ShortA1-ShortA5‘
2) *xOnex*x long question: ‘LongQ‘ (explicitly lists the five sub-prompts)
3) *xxOnexx long answer: ‘LongA‘ (coherent synthesis of the five short answers)
## Examples (compressed)
- *xxExample 1 (placeholder) «x*
*Reference article:* ‘[...Punic Wars summary...]‘
xOutput (abbrev) :*
*“ShortQ1: When did the First Punic War occuré
‘ShortA1: 264-241 BCE. ‘
‘LongQ: Discuss by covering (1) ...(2) ...(3) ...(4...()...
‘LongA: [Single paragraph formed only from ShortA1-5]. ‘
- *xxExample 2 (placeholder) «x
*Reference snippet:* ‘[..... a
xOutput (abbrev):* similar structure to Example 1.
## Critical Instructions (strict)

1. *xAbsolute Grounding in Provided Text:** ALL answers (ShortA1-5, LongA) MUST be derived xexclusively*x from the information
present in the reference text provided below. DO NOT use any external knowledge or information not explicitly stated in the text.

2. x*xSelf-Contained & Precise Questions:** Each short question (ShortQ1-5) must be specific, unambiguous, and fully
understandable on its own without needing context from other questions or the article title.

*Precision Example:* Use “When did the First Punic War occur?” instead of the vague “When did the war occur?”.

3. **No Source Referencing in Questions:** Questions MUST NEVER refer to the provided text itself. Avoid phrases like “According
to the article...”, “What does the text mention about...”, “Which item listed...”. Frame questions as standalone, open-domain
factual queries.

4. *xStrict Single-Fact & Single-Answer Rule (MOST IMPORTANT):** This constraint is paramount and must be strictly enforced for
each ShortQ/ShortA pair

*xOne Specific Fact:** Each ShortQ must ask for *xone single, specific piece of informationx.

xxOnly One Correct Answer (within the text):** Critically, based xsolelyx on the provided reference text, there must be xonly
one possible correct answer to the ShortQ.

xxMandatory Verification:** Before finalizing any ShortQ, you MUST verify that no other statement or detail *within the
provided text* could also serve as a correct answer to that specific question.

*xAVOID Ambiguity from Lists/Examples:** If the text presents multiple examples, types, reasons, methods, individuals within a
category, etc. (e.g., “Art mediums include painting, digital tools, and ink,” or “Key figures were X, Y, and Z”), you MUST NOT
formulate a ShortQ asking for xonex of them (e.g., DO NOT ask “What is xonex art medium mentioned?” or “Name *xan* important
figure.”). Such questions inherently violate the single-answer rule because the text itself provides multiple valid options in that
context.

+ x*xEnsure Subject Uniqueness (Open Domain Test):** The specific entity, event, concept, person, or work being asked about in
the question MUST be identifiable *without ambiguity* even when considered outside the context of the source article. Ask yourself:
“If this question were encountered alone, would the subject be clear?”

+ - *xkINVALID Example:**« “Which organization funded Short et al.’s work?” is INVALID if “Short et al.’s work” is not a
globally famous, uniquely identifiable publication/project (like “Einstein’s theory of relativity”). It improperly relies on the
implicit context (“the work mentioned in this article”).

+ - %*xVALID Example:** “What year was the Treaty of Versailles signed?” is VALID because “The Treaty of Versailles” is a
globally unique and identifiable historical event.
+ - x*kGuideline:** Avoid questions where the subject is a vague reference (e.g., “the study’s findings”, “their main

conclusion”, “Smith’s 2020 paper” unless that specific paper is uniquely identifiable globally).

*xTarget Suitable Facts:** Focus ShortQs on unique identifiers (e.g., the *specific namex of the *first* person to do X),
distinct dates/years associated with singular events, uniquely defined terms (like *Pax Romanax, if defined as a singular concept
in the text), precise numerical values or quantities tied to a specific context, or the outcome of a specific, singular event
described, *xensuring the subject meets the Open Domain Test above.**«

5. *xConcise Short Answers:** Each ShortA (ShortA1-ShortA5) must directly state the single fact requested by its corresponding
ShortQ, using fewer than 10 words.

6. *xStructured Longform Composition: **

xxLongQ Construction:** The LongQ must explicitly integrate the five preceding ShortQs, typically by listing them as points to
be covered (e.g., “Discuss [Topic] by addressing: (1) [Content of ShortQ1], (2) [Content of ShortQ2]...”).

xxLongA Synthesis:** The LongA must synthesize *xonly*x the five ShortAs (ShortA1-ShortA5) into a single, coherent,
natural-sounding paragraph. It must flow well and not sound robotic. DO NOT introduce any facts or details not present in the
ShortAs. Avoid using bullet points or numbered lists in the LongA.

7. *xStrict Output Format:** Produce the entire output as a single block of text, strictly adhering to the following
CSV-friendly format on one line. Use commas as separators between key-value pairs. Include NO extra text, commentary,
introductions, or line breaks before or after this formatted string:

### Provide One-Line Output Template

"ShortQ1 : <value>, ShortAl : <value>, ShortQ2 : <value>, ShortA2 : <value>, ShortQ3 : <value>, ShortA3 : <value>,
ShortQ4 : <value>, ShortA4 : <value>, ShortQ5 : <value>, ShortA5 : <value>,

LongQ : <value>,LongA : <value>”

*xReference Article:**

{text}

*xGenerate Now: **

Table 1: Short-Long form query and answer generation prompt for 03-mini-high


Prompt

*xRole and Objective: **

You are a meticulous AI Fact Adherence Evaluator. Your primary objective is to perform a rigorous, objective assessment of a
“Generated Short Answer” by comparing it against a “Ground Truth Short Answer” in the context of a specific “Question”. Your
evaluation must determine if the “Generated Short Answer” accurately and meaningfully conveys the same information as the “Ground
Truth Short Answer” or provides an equally valid factual answer to the “Question”, paying close attention to any specificity
implied by the question and ground truth.

Core Principles for Evaluation:

1. Objectivity: Base your evaluation strictly on the provided information and the criteria outlined below. Avoid personal biases or
assumptions.

2. Factual Accuracy: The paramount consideration is whether the “Generated Short Answer” is factually correct in relation to the
“Question” and “Ground Truth”.

3. Semantic Meaning: Focus on the core meaning and informational content.

4. Contextual Relevance: The “Generated Short Answer” must directly and relevantly answer the “Question”.

Input Data Structure:

You will be provided with the following for each evaluation task:

* Question (ShortQ): SHORT-QUESTION (The specific query the answer should address)

* Ground Truth Short Answer (ShortA): SHORT-ANSWER (The pre-validated correct answer, which also sets the expected level of
specificity for certain types of questions)

* Generated Short Answer (GeneratedShortA): GENERATE-SHORT-ANSWER (The answer to be evaluated)

Detailed Evaluation Criteria and Scoring (Output @ or 1):

A. Score 1 (Correct) if ANY of the following conditions are met:

1. Direct Semantic Equivalence:

* The “Generated Short Answer” conveys the same essential information as the “Ground Truth Short Answer” and accurately answers
the “Question”.

x Differences in phrasing, sentence structure, or the use of synonyms are acceptable as long as the core meaning is preserved.
2. Subset/Superset Equivalence (Strict Application Regarding Specificity):

* If GeneratedShortA is a more specific (subset) version of ShortA (e.g., ShortA: “Dog”, GeneratedShortA: “Labrador Retriever
dog”), it can be correct if it still fundamentally answers ShortQ accurately and doesn’t introduce inaccuracies.

* If GeneratedShortA is a more general (superset) version of ShortA, it can be correct ONLY IF:

* It still fundamentally and accurately answers ShortQ.
* It doesn’t introduce inaccuracies or change the core fact(s) required to answer ShortqQ.
3. World Knowledge Override (Strict Application - Use Sparingly):

This applies ONLY IF:

* The “Generated Short Answer” is factually incorrect OR insufficiently specific when compared directly to the “Ground Truth
Short Answer” based on the criteria above.

* AND The “Generated Short Answer” is a demonstrably true, widely accepted, and commonly known fact that also correctly,
directly, and with appropriate specificity answers the “Question”.

* AND The “Generated Short Answer” is not a niche, controversial, or overly obscure fact.

Checklist before applying World Knowledge Override:

1. Does GeneratedShortA directly and unambiguously answer ShortQ with the necessary specificity? (If no, score Q)

2. Is GeneratedShortA factually true based on broad, verifiable common knowledge? (If no, score @)

3. If ShortQ demands specificity, is GeneratedShortA a better or equally valid specific answer to that demand than ShortA? (If
no, or if ShortA’s specificity is contextually more appropriate, score Q)

4. Does GeneratedShortA introduce ambiguity or miss critical nuances that ShortA captures, especially regarding specificity?
(If yes, score Q)

Example:

ShortQ: Who is considered the primary inventor of the telephone?

ShortA: Alexander Graham Bell.

GeneratedShortA: Antonio Meucci conceived the telephone first. (Score: 1, IF the LLM’s world knowledge strongly supports Meucci
as a more accurate answer to “primary inventor” despite Bell’s common association, and this is a well-established historical
correction. This is a high bar.)

4. Correct Vagueness:

* Sometimes, the answer can be correct but vague. For example, if a question says ’Into what must a geometric shape be divided to
be symmetric?’, the ground truth answer is ’Two or more identical pieces’, the generated answer is ’A shape must be divided into
two halves’. The generated answer here is correct.

* Similarly, for a technical question, the question could be ’How is the fractal-like shape obtained?’, Ground-truth answer
’Finite subdivision rule’ and the Generated answer here ’Fractals are created by repeating a pattern at different scales.’ is
correct here. It is not exactly but the meaning of both is the same.

* For non-technical questions, like ’What is literary criticism?’, The ground truth is ’study, evaluation, and interpretation of
literature’ and the generated answer is ’Literary criticism is the analysis and interpretation of written works.’. The generated
answer here is correct.

* Partial Correction: If the answer is partially correct, Apply World Knowledge Override and see if it can be correct FOR the
question. If so, it is correct.

B. Score @ (Incorrect) if ANY of the following conditions are met:

1. Factual Inaccuracy: The “Generated Short Answer” is factually incorrect.

2. Semantic Dissimilarity: The “Generated Short Answer” conveys a different meaning.

3. Irrelevance: The “Generated Short Answer” does not answer the “Question”.

4. Contradiction: The “Generated Short Answer” contradicts the “Ground Truth Short Answer” and does not meet the stringent
criteria for “World Knowledge Override”

5. Hallucination: The “Generated Short Answer” introduces general knowledge, which alters the answer’s validity and the
hallucination is severe.

For specificity, you have to judge the Question and see if it requires the answer to be exact and specific. These are often
scientific, historical questions where there is only 1 correct answer. If the questions expects a specific answer, only the most
closely related generated answer should be correct.

#4# OUTPUT FORMAT

Return ONE character only: 1 or Q.

#4# INPUT

Question: {q}

Ground-truth: {gt}

Candidate: {cand}

<END_PROMPT>

Table 2: Prompt for evaluating short-form answers against ground-truth short-form answers.


Prompt

*xxxRole and Objective: «**

You are an AI Comprehensive Answer Evaluator. Your task is to dissect a “Generated Long Answer” and meticulously assess its
coverage and accuracy concerning five distinct sub-facts, each defined by a “Short Question” and its “Ground Truth Short Answer”.
The “Generated Long Answer” is intended to synthesize these five pieces of information. You must pay close attention to any
specificity implied by each sub-question and its corresponding ground truth.

*xCore Principles for Evaluation: **

1. Objectivity: Base your evaluation strictly on the provided information and the criteria outlined below. Avoid personal biases
or assumptions.

2. Factual Accuracy: The paramount consideration is whether the “Generated Short Answer” is factually correct in relation to the
“Question” and “Ground Truth”.

3. Semantic Meaning: Focus on the core meaning and informational content.

4. Contextual Relevance: The “Generated Short Answer” must directly and relevantly answer the “Question”.

#4## SUB-QUESTIONS & GT

1. {ql}

— GT-1: {al}
2. {q2}

— GT-2: {a2}
3. {q3}

— GT-3: {a3}
4. {q4}

— GT-4: {a4}
5. {q5}

— GT-5: {a5}
#4## CANDIDATE LONG ANSWER
{cand_long}
*kDetailed Evaluation Task and Scoring (List of 5 scores [@ or 1]):**
For EACH of the 5 Sub-Facts (iterate from Sub-Fact 1 to Sub-Fact 5):
1. **Isolate Focus:** Concentrate on the current Sub-Fact i (defined by ShortQ[i] and ShortA[i]).
2. *xLocate Relevant Information:** Scrutinize the “Generated Long Answer” to identify the sentence(s) or phrase(s) that attempt
to address ShortQ[il.
* If no part of “Generated Long Answer” appears to address ShortQ[i], assign a score of @ for this sub-fact and move to the next.
3. **Evaluate Located Information:** If relevant information is found, compare it against ShortA[i] using the following criteria,
which mirror the detailed logic of the Short Answer Evaluation:
A. Score 1 (Correct) if ANY of the following conditions are met:
1. Direct Semantic Equivalence:
* The “Generated Short Answer” conveys the same essential information as the “Ground Truth Short Answer” and accurately answers
the “Question”.
*x Differences in phrasing, sentence structure, or the use of synonyms are acceptable as long as the core meaning is preserved.
2. Subset/Superset Equivalence (Strict Application Regarding Specificity):
* If GeneratedShortA is a more specific (subset) version of ShortA (e.g., ShortA: “Dog”, GeneratedShortA: “Labrador Retriever
dog”), it can be correct if it still fundamentally answers ShortQ accurately and doesn’t introduce inaccuracies.
* If GeneratedShortA is a more general (superset) version of ShortA, it can be correct ONLY IF:
* It still fundamentally and accurately answers ShortQ.
* It doesn’t introduce inaccuracies or change the core fact(s) required to answer ShortqQ.
3. World Knowledge Override (Strict Application - Use Sparingly):
Applies ONLY IF:
* The “Generated Short Answer” is factually incorrect OR insufficiently specific when compared directly to the “Ground Truth
Short Answer”.
* AND The “Generated Short Answer” is a demonstrably true, widely accepted, and commonly known fact that also correctly,
directly, and with appropriate specificity answers the “Question”.
* AND The “Generated Short Answer” is not a niche, controversial, or overly obscure fact.
Checklist:
1. Does GeneratedShortA directly and unambiguously answer ShortQ with the necessary specificity? (If no, score 0)
2. Is GeneratedShortA factually true based on broad, verifiable common knowledge? (If no, score Q)
3. If ShortQ demands specificity, is GeneratedShortA a better or equally valid specific answer than ShortA? (If no, or if
ShortA’s specificity is more appropriate, score 0)
4. Does GeneratedShortA miss critical nuances that ShortA captures? (If yes, score Q)
Example:
ShortQ: Who is considered the primary inventor of the telephone?
ShortA: Alexander Graham Bell.
GeneratedShortA: Antonio Meucci conceived the telephone first. (Score: 1, IF world knowledge supports Meucci as more accurate.)
4. Correct Vagueness:
* Sometimes the generated answer is correct but vague (e.g., Question: ’Into what must a geometric shape be divided to be
symmetric?’, ShortA: ’Two or more identical pieces’, Generated: ’Two halves’ — correct).
* Similar logic applies for technical and non-technical contexts, as long as meaning is preserved.
* Partial Correction: If partially correct, apply World Knowledge Override to decide.
B. Score @ (Incorrect) if ANY of the following hold:
1 Factual Inaccuracy.
2. Semantic Dissimilarity.
3. Irrelevance.
4. Contradiction not justified by World Knowledge Override.
5. Severe Hallucination.
For specificity, judge whether the Question expects an exact and specific answer (common in science/history). If so, only the most
precise matching Generated answer should be correct.
xxHandling Complexities:«*
* Information may be split across sentences.
* Do not penalize answer order; evaluate each fact independently.
* Prefer explicit statements. If heavily implied, err toward @ unless undeniable.
#4## OUTPUT FORMAT
Return exactly a JSON list of 5 ints, e.g. [1,0,1,1,0]

Table 3: Prompt for evaluating long-form answers against five short-form ground truth facts.


Instruction for Short-form QA

Answer the question with factual single sentence response for the Topic: topic.
Question: question
Instruction for Long-form QA

Answer to the question should answer everything in the question in a clear and concise
manner .Question: long_question

Table 4: Instruction for short and long-form QA.
