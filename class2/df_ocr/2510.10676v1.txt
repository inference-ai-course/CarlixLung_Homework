2510.10676v1 [cs.AR] 12 Oct 2025

arXiv

Bhasha-Rupantarika: Algorithm-Hardware Co-design
approach for Multilingual Neural Machine Translation

Mukul Lokhande’,

ie

Tanushree Dewangan’, io Mohd Sharik Mansoori*,” ©)

Tejas Chaudhari*,"©, Akarsh J., io Damayanti Lokhande', Adam Tena Senior Member, IEEE,
Santosh Kumar Vishvakarma* ©| Senior Member, TEEE.

*NSDCS Research Group, Dept. of Electrical Engineering, Indian Institute of Technology Indore, India.
SEnICS Labs, Faculty of Engineering, Bar Ilan University, Ramat Gan 5290002, Israel.
‘Independent Researcher.

Email: skvishvakarma @iiti.ac.in (Corresponding Author)

Abstract—This paper introduces Bhasha-Rupantarika, a light
and efficient multilingual translation system tailored through
algorithm-hardware codesign for resource-limited settings. The
method investigates model deployment at sub-octet precision
levels (FP8, INT8, INT4, and FP4), with experimental results
indicating a 4.1x reduction in model size (FP4) and a 4.2x
speedup in inference speed, which correlates with an increased
throughput of 66 tokens/s (improvement by 4.8). This under-
scores the importance of ultra-low precision quantization for
real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation
covers bidirectional translation between Indian and international
languages, showcasing its adaptability in low-resource linguistic
contexts. The FPGA deployment demonstrated a 1.96 reduction
in LUTs, a 1.65x decrease in FFs, resulting in a 2.2 enhance-
ment in throughput compared to OPU and 4.6x compared to
HPTA. Overall, the evaluation provides a viable solution based
on quantization-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
and dataset for reproducibility are |publicly available, facilitating
rapid integration and further development by researchers.

Index Terms—Natural Language Processing, Transformers,
Language translation, Model Quantization, AI hardware accel-
eration.

I. INTRODUCTION

The increasing integration of artificial intelligence (AI) has
heightened the emphasis on endowing smart machines with
the capacity to understand, interpret, and generate human-like
language, effectively merging computational capabilities with
human communicative demands. Natural Language Processing
(NLP) has been instrumental in this regard, encompassing
a wide array of tasks such as text classification, sentiment
analysis, speech recognition, among others. Multilingual Neu-
ral Machine Translation (MNMT) has attracted notable in-
terest due to its proficiency in executing automatic, context-
aware, end-to-end translations using sophisticated deep learn-
ing methodologies [I], [2]. In contrast to traditional Statistical
Machine Translation (SMT) systems, which depended on a
dictionary-like approach through discrete components like fea-
ture extractors, translation rule extractors, and word aligners.

+ +Both authors contributed equally to this work.
This work was supported by the Special Manpower Development Program for
Chip to Start-Up (SMDP-C2S), the Ministry of Electronics and Information
Technology (MeitY), Government of India, Grant: EE-9/2/21 - R&D-E

While earlier methods utilized Convolutional Neural Networks
(CNN) or Recurrent Neural Networks (RNN) under this frame-
work, the rise of the “Embed-Encode-Attend-Decode” model
based on the transformer framework has received considerable
attention.

MNMT systems are regarded as more practical because
training across varied language pairs improves translation
quality for less-resourced languages and aids in acquiring
additional linguistic insights, a concept termed translation
knowledge transfer (3). 4]. This approach produces a more
compact and multi-faceted translation model that requires less
computational power, an essential consideration for devices
with limited resources. This study concentrates on translating
between Indian and international languages, with the goal of
implementing simple and cost-effective FPGA solutions in
rural Indian areas. The importance of this work is particularly
pronounced for large populations with limited formal educa-
tion who nonetheless need to engage with foreign languages,
especially in the fields of tourism or business.

The efficiency of resources in MNMT systems becomes
particularly significant when dealing with low-resource trans-
lation scenarios. Conventionally, a single encoder is employed
for multiple languages, while each target language utilizes its
own separate decoder (3). In addition, language divergence can
be mitigated by aligning commonly represented words and
sentences across different languages, though this alignment
requires a comprehensive grasp of multilingual representa-
tions (5)-(7).- Transfer learning across related languages might
be enhanced through the fine-grained clustering of encoder
representations based on language similarity. For example,
representation invariance tends to decrease on the decoder
side while it increases in the upper levels of the encoder.
Thus, the decoder must effectively manage representations
that are specific to a language versus those that are language-
neutral. Another significant factor is lexical transfer on the
source side, which involves mapping pre-trained monolingual
word embeddings from both parent and child languages into
a unified vector space [8}, [9].

The core question of this study is: Is it feasible to create
a single model that manages all language pairs based on
the use case? and extends to another important question: Can


Computer Vision : General Purpose
Nightvision, NVDLA, Systolic Array Deep Learning
Signal Processing

Collaborative
Autonomous Driving
| <

4

lo ] Natural Language Processing (NLP) Engine
2. 2.

RISC-V
CVA6

(b) (c)

Fig. 1: (a) Conventional system-on-chip (SoC) architecture,
showcasing 14 domain-specific accelerators targeting multiple
application domains (including NLP Engine), (b) Real-life
scenario showcasing the emphasis on language translation,
Indian-Italian Diplomats interaction, and (c) Two overseas
students discussing.

shared multilingual machine translation be achieved with
a low-resource NLP mechanism? The answer is affirmative,
as a unified model is indeed possible. However, it encounters
challenges in representation learning and translation quality
across languages, which are influenced by factors such as
data precision, architecture, and learning strategies. For low-
resource inference, there must be a careful equilibrium be-
tween performance and accuracy.

Fig. [Ia] illustrates a multi-application domain SoC designed
for the concurrent execution of various computational kernels,
including those from NLP, deep learning, signal processing,
and cryptography. This highlights that the NLP engine is an
often overlooked area in modern AI hardware developments.
Consequently, the objective of this study is to tackle this
issue within the Indian context. Our approach seeks to achieve
translation between Indian languages and foreign languages
using a unified model, differing from previous methodologies
that employed separate models for translating from Indian

Indian Language
to English

English to
Overseas Language

Uni-directional Model

Bidirectional Model

Speech

Fig. 2: High level algorithm pipeline description, comparison
with (a) prior approach, (b) this work for Indian language to
Overseas language translation.

languages to English and then from English to foreign lan-
guages, with two additional one-directional models for reverse
translation. This unified approach enhances resource efficiency
on edge platforms.

II. THE PROPOSED APPROACH
A. Algorithm Pipeline

The conventional baseline employs OpenAI Whisper Large-
v3 for speech-to-text conversion in foreign languages and
relies on Al4Bharat IndicConformer for Indian languages.
Additionally, text conversion from Indian languages to English
can be achieved using IndicTrans2 (10), a distilled INT4
version of the Bhasha-anuvaad model, and translation from
English to other foreign languages is done via Meta LLaMA
3.2, followed by text-to-speech (TTS) using Coqui XTTS v3
for foreign languages and IndicParler TTS for Indian
languages. The proposed pipeline incorporates:

e NLLB-200 (12), a distilled version of INT4 with 600M
parameters, which are lightweight transformers suitable
for efficient FPGA implementation.

e OpenAI Whisper Large-v3 (73), a recent multilingual
automatic speech recognition (ASR) model, supporting
over 90 languages and excelling in transcribing speech
despite background noise, accents, and various recording
conditions, making it ideal for real-time transcription,
subtitling, and cross-lingual applications in foreign lan-
guages.

e Al4Bharat IndicConformer (10), an ASR model designed
for Indian languages, providing transcription across mul-
tiple Indic languages such as Hindi, Tamil, Telugu, and
Kannada, balancing speed and accuracy to accommodate
India’s linguistic diversity, thus serving applications in
government, education, and healthcare sectors within
India.

e Coqui XTTS v3, a multilingual text-to-speech system
with robust support across global languages, facilitating
cross-lingual voice transfer by replicating one speaker’s
voice in another language, highly effective for dubbing
and personalized speech synthesis.

e IndicParler TTS, which synthesizes natural-sounding
speech in Indic languages, addressing tonal, phonetic, and


prosodic variations, and finds uses in voice assistants, e-
learning platforms, and digital government services.

e IndicTrans2 (Distilled INT4 version), a neural machine
translation model with reduced size while maintaining ac-
curacy, supporting translation between Indian languages
and between Indian and English, thereby enhancing dig-
ital inclusivity in India.

e Meta’s LLaMA 3.2, a large language model used for
advanced reasoning and multilingual understanding, fa-
cilitating general-purpose tasks like summarization, dia-
logue, and content generation in foreign languages.

We have shown a comparative algorithm pipeline in Fig.
We focused on the No Language Left Behind (NLLB-
200), a translation bidirectional model supporting 200 lan-
guages, including many low-resource Indian languages, with
a 600M parameter distilled INT4 lightweight version for
translation across both foreign and Indic languages on
resource-constrained devices. The distilled NLLB-200 version
was implemented with a 600M-parameter Transformer en-
coder—decoder (Fig. |3) with six layers of each Pre-Norm resid-
ual connections, multi-head attention, and two-layer FFNs.
Token order is decided based on positional encoding and per-
language Sentence Piece tokenizers (1,000 tokens) to enable
efficient many-to-many translation based on target language
codes. Pretraining includes auto-encoding (DAE) for bidirec-
tional encoding and causal language modelling (CLM) for
fluency, post-finetune. The dataset involved a custom dataset,
high-quality seed corpora, and generated bi-text with LASER3
embeddings for low-resource performance and balanced expo-
sure across languages. The major resource savings originated
with Mixture-of-Experts (MoE) layers that enabled the most
relevant expert and processed it in parallel, combining outputs
based on gated probabilities. This is crucial for enhancing
parameter cost and comes at no additional per-token compute
cost. Load-balancing loss penalizes skewed expert usage to
avoid collapse on fixed experts, prompting even token distribu-
tion. Thus, the conditional framework scales effectively across
diverse multilingual datasets and ensures robust translation
even in low-resource settings.

B. Hardware Architecture

NLLB utilizes the encoder-decoder architecture (Fig. 3p.
making it essential to design accelerators specifically for trans-
former workloads. Recent developments in natural language
processing (NLP) involving transformer acceleration empha-
size algorithm-hardware co-design to balance performance,
efficiency, and resource constraints effectively. ViTCoD
and ViA have introduced vision transformer accelerators
that prioritize sparsity and data reuse to improve throughput
and reduce power consumption. EdgeBERT and QBERT
have investigated quantized NLP inference, focusing on
minimizing latency and energy consumption. AccelTran
has expanded this capability by exploiting sparsity. Platforms
such as NLP-FPGA-accl [19], [20], OPU [8], [21], HPTA
(22), 23], and EdgeLLM [24] demonstrate the adaptability
and scalability of FPGA and ASIC SoC solutions from NLP

X(N /frace)

xN ;
i | Layer Norm
day
FFN MHA

Layer Norm.

im FEN, FFNe
MoE Gating
I
MHA Layer Norm

Input +
Positional Encodings

(a) (b)

Fig. 3: Illustration of Transformer encoder used, (a) Dense
Transformer, (b) Mixture of Experts (MoE) Transformer.

ute
Positional Encodings

TABLE I: Qualitative comparison between SoTA AI Acceler-
ators and features in Neural Compute Engines.

Precision Design

Use-cases

Design

Datatype Bit-width ‘Approach Overhead
FP 8/16/32/64 | Radix-4 Booth GPU Server
FP/BF16 8/16/32/64 | LPC-DOTP 2 AloT
FP/TF32/BF16 | 4/8/16/32_ | LUT Power Versal MPSoC
INTIEP asi Grouped/Tiled Resources utilization Edge-AI (NLP)

Matrix Compute (Dark-Silicon) Transformers

MXFP/BF16
L. Posit

4/6/8/16
8/16/32

Mixed-precision 2 GPU
Approximation

Accuracy Edge Compute

INTIBF16 4/8/16/32 : Memory Bound Compute _| Mobile PC (NPUs)
INDEP “usiie Parallel Multiplier, | Under-utilization Mobile Soc
Adder Tree Ace. | (Dark-Silicon)
Flexibl
INT/FxP 4/8/16 vor © Latency, Control NLP (Transformers)
FP/BFI6/TE32 | 16/32/64 HPS, CEC Area (Mant. multiplication) High Pettonmaries
Delay (Exp. processing) | Computing
FP/ExP 24/8/16 | Approximation | Accuracy Drop 2
Posit/FP 4/8/16 LPC, PEC Delay XR Perception
This Work INTFP/BF | 4/8/16 RMMEC Run-time adaptivity Edge-NLP

to large language models (LLM) on heterogeneous platforms
(CPU-master) with high-throughput data flow. Meanwhile,
NLP-edge [25], showcases substantial speech and NLP in-
ference capabilities, and LSTM-NLP (9) focuses on recurrent
models through adaptable LSTM architectures. Consequently,
we have concentrated on developing a lightweight transformer-
like architecture with quantized rapid inference, assessed at
both FPGA and ASIC levels, directed towards efficient NLP
deployment at edge nodes. A qualitative overview of the
features present in the current state-of-the-art (SoTA) Neural
compute engines (NPE) is presented in Table [I]

The NLPE architecture (Fig. 4) is composed of modules
including the Control Unit (CU), Memory Read Unit (MRU),
Memory Write Unit (MWU), Matrix Multiply Engine (MME)
(Fig. [5) based on SIMD MAC (Fig. |6), and the Nonlinear
MIMD Vector array (NMV) (Fig. [7). The CU orchestrates
execution by sending instructions to all functional units. The
MRU retrieves data from external memory into the MME,
while the MWU saves results back to external memory. The


| Matrix Multiplication | Non-linear Vector Array
Input 5 '
Memory z i 7
i Non-linear + 2 2
Buffers i 8 Ie | Shared |__| Non-linear | 2) |
i 4 rj M ma ee [*-—|MIMD Vector| e2 i
g i |
1 4 Array | |=a
a
ay a —_ i i
‘ax! | Control . Load/Store Instr.
— Instruction Decoder

Fig. 4: The detailed flow for NLPE, Memory control handles
read/write with off-chip memory, NVU handles non-linear op-
erations, while SIMD Matrix Mult. Engine handles quantized
matrix multiplication.

NPE NPE NPE >
a ma)
=| 3
oO
ne rey ee eee < 2 || & g
f f af z= ||@o
a aye
i + ea ap
NPE | NPE | NPE z 3 3
=
Le ly ly Q.
(tesa [7 a nn 10 S
; resul i a n re)
| | f o a 8
'SIMD Ingut Vector | EY fd eS
g a|| a
=== | = g &
~— Ss =
i
i | i
| | NPE NPE | NPE
i i
| 3 ! |
ly wa ly
a rn (Ai ne je
oer Signal
Input weights Buffer (IN) Interface

Fig. 5: The detailed SIMD Matrix Mult. Engine datapath.

MME performs matrix multiplications with an array of SIMD
NPEs, supported by scratchpad memory. This process involves
selecting data for loading and rearranging operands from
input buffers, with parallel multiplications accumulated in
quantized format. The NMV manages a MIMD array of non-
linear activation functions (NAFs) (Fig. [8) with throughput
supporting 2xFP8/1xBF16 parallel operations for sigmoid,
tanh, ReLU, and SoftMax functions, utilizing shared CORDIC
resources (37). Parallel load/store vector instructions enable
simultaneous handling of multiple operands in each cycle. All
units are pipelined, permitting overlaps between computation
and data movement, thus reducing off-chip memory latency
and optimizing concurrency by facilitating data exchange
between functional units through individual memory buffers.

The parametrized systolic array based on SIMD MAC
(Fig. [6) is designed for multiple attention heads in output-
stationary dataflow, where the partial results of the computa-
tion are held stationary within the PE, and the weights and
embeddings are passed horizontally and vertically. The size
of the systolic array is kept parameterized depending on the
dimensions of the matrices, which also determines cycles for
accumulation. The same systolic array hardware can be used
to perform calculations for multiple attention head layers by
changing the weights and embedding matrices in consecutive
cycles. The MAC is divided in five stages and supports 1 x
BF16, 3 x FP8, 6 x FP4, and 6 x INT4 multiply operations
with one addend in a single clock cycle and output either in
FP8 or BF16 which are precision supported by FASST. The

[avs [psa [eva [ren | |#

Input Processing Decoder

Sign bits | mode

RMMEC3

RMMEC1 | | RMMEC2

Exponent

Difference |g
and

Comparator

rt !

Multi-stage accumulation and Alignment

Normalization, Rounding and Restructuring

RMMEC4 | | RMMECS | | RMMEC6

ARRAY -RMMEC

[-—>

| [15:0] output

Fig. 6: The datapath for SIMD multiply-accumulate unit
supporting precision 6x INT4/FP4, 3xFP8, 1xBF1O6.

Parameters Weight Reg-0
FASST
Input Reg-0
Status & i z
Config Regs
Weight Reg-0
FASST
Tile Reuse Input Reg-0
Control
Scalar
Compute
Reduce,
ALU, Shift-
Mult
Weight Reg-0
+>) Control Unit FASST
Input Reg-0

Fig. 7: The detailed Non-linear MIMD Vector Array datapath.

novel RMMEC 4-bit blocks can be configured to operate as
a multiplier or an exponent comparator by applying a mode
control signal. The input processing decoder unit receives two
input data values, 24-bit A and B, one addend value C (8/16-
bit), and a control signal. The 24-bit input size allows for full
resource utilization in SIMD mode.

The sign, exponent, and mantissa bits are extracted, with the
mantissa divided into 4-bit slices for input to basic multiplier
blocks, while the exponent values are cut into primary blocks
configured with comparators. A 2x3 array of 4-bit RMMEC
blocks, receiving nibble inputs, generates 8-bit partial products
with positional weights; a comparator assesses the maximum


[2:0] [15:0] [1:0]
AF_sel Input ‘ Prec_sel

Softmax
CORDIC
FP Multiplier C1 | c2 | c3 sees Cn

CORDIC
FP Multiplier

Sigmoid +
pee ORES + FP CORDIC
Exponential oe :
Unit tanh} Division Unit

Reconfigurable
Logic

[15:0]
Y Output

Fig. 8: The detailed CORDIC-based FASST (Floating-point
Activation function unit for SoftMax-sigmoid-tanh datapath.

among the three exponents and calculates their differences
for exponent normalization. For radix normalization, each
exponent is adjusted by subtracting the maximum exponent
value using a subtractor array, then used to shift-align the
products. These aligned values are combined using Carry
Select Adders (CSA) to yield the accumulated result, and the
accumulator’s value is recalibrated for further computations,
with the exponent set to the prior maximum value. An XOR
operation determines the sign of the multiplication result,
obtained from the final carry out in the adder stage, with the
radix point adjusted by counting leading zeros and shifting as
necessary. The mantissa result is stored as 8/16-bit depending
on the accumulation mode, and the exponent as 4/8-bit during
the quire stage; it can be truncated, or an exception flag
is set once the entire vector’s dot product is complete. The
upper bits, as per precision, are retained for the mantissa and
recombined with the sign and exponent components.
Another crucial component is the Non-linear MIMD vector
array datapath (Fig. Fig. (7). constructed using a FASST unit
based on CORDIC. This unit predominantly handles computa-
tions of various NAFs in a SIMD manner and accommodates
FP8 and BF16 data precisions, with support for numerous
instructions across different NAFs. It adopts an accumulative
approach, synthesizing methods from [37], [38]. The unit
employs CORDIC floating-point calculations for exponential
functions and division, with reconfigurable logic aiding in
operations such as swish, GeLU, selu, and SoftMax. Our
implementation extends mathematical formulations from (37).
incorporating FP8 support and hardware reuse. Previous re-
search confined functions to GeLU/sigmoid/tanh in SRNNs,
LSTMs, GRUs, Transformers, BERT, and GPT2 [9], [19],
and GeLU/SoftMax in others (17). Solutions like Flex-SFU
and ASTRA provided GeLU/SiLU and SoftMax/GeLU
support for BERT/GPT-2 Language modelling, while PACE
focused on SoftMax/GeLU for Gen-AI, and ReAFM on
swish, GeLU, prelu, etc., utilizing Taylor series, piecewise
linear or logarithmic approximations, LUT-based methods,
stochastic computation, and CORDIC-based reconfigurability.

However, earlier designs did not support most NAFs with
a single, reusable hardware, a gap we addressed to enhance
the resource efficiency and functionality of the unified vector
array. This was essential as NAF hardware can consume up
to 20-25% of the area in a commercial Google TPUV4.

III. PERFORMANCE EVALUATION

The experimental setup included a software evaluation of
the NLLB model, where quantized INT4/INT8 and FP4/FP8
were implemented and compared with the Baseline (FP). For
resource utilization, the hardware setup employed a ZCU104
FPGA. Architectural emulation was aligned with hardware
design for the benefits of co-design. Our approach utilized
precise arithmetic for MAC operations and implemented an
iso-functional arithmetic class for the activation layer. Post-
training quantization (PTQ) was conducted using the BitsAnd-
BytesConfig library, and performance was evaluated on 1000
queries per language, yielding satisfactory results. The setup
included Python 3.0 and Qkeras 2.3 within Jupyter Notebook,
operating on an NVIDIA L4 GPU. The methodology applied
the QLoRA (Quantized Low-Rank Adapter) framework to
fine-tune large language models (LLMs) by combining low-
rank adaptation with 4-bit quantization. This approach main-
tains the original quantized weights while introducing small,
trainable adapter modules instead of updating all model param-
eters. This reduces memory and computational demands and
maintains performance, allowing fine-tuning of very large NLP
models on consumer-grade GPUs. Additionally, 8-bit block-
wise quantization was utilized. We employed the Whisper
model for speech-to-text and Google Text-to-Speech (gTTS)
for the text-to-speech model. Performance comparisons with
various numerical precisions, with an emphasis on quanti-
zation, are illustrated in Fig. [10] demonstrating significant
reductions in memory and latency, with FP4 achieving a
footprint of 0.56 GB and a throughput of 66 tokens/s.

We designed the proposed NLPE using System Verilog
for its components and verified its functionality with the
Questa-Sim Simulator, making a fair comparison with an iso-
functional Python emulator. In addition, FPGA synthesis was
conducted at the level of fundamental modules for MAC and
NAF designs. We compared these designs with SoTA systems
as shown in Table[I] and Table [MI] employing the AMD
Vivado Design Suite. The MAC and NAF units were evaluated
against individual precision units and SIMD compute units
to demonstrate the efficacy of our approach. As indicated in
Table [I] our unit significantly lowers LUT resource usage
by 90% compared to and 94.7% compared to [42};
additionally, power consumption is reduced by a factor of
3x relative to (41), and delay is reduced by up to 1.94x
and 3.66 compared to and [41], respectively. Likewise,
the NAF unit was also compared favorably against SoTA
works, meeting our resource-efficiency goals for accelerat-
ing NLLB, Furthermore, the design was synthesized using
Cadence Innovus in a commercial CMOS 28nm technology,
and we reported pertinent performance metrics, comparing
against SoTA MAC and NAF units as depicted in Fig. [11]


HIS USLALEL Hel Heel

lorsifl eflumy Ly tomb Lago
SAumguL

AIS GAMA BR ARTHRUI

Indic Languages

Bhasha-
Rupantarika

Verifica e convalida del modello

TTIN QI77'N1 Nin'x

Verificacion y validacion de modelos

Modellverifizierung und -validierung

Overseas Languages

Fig. 9: True outputs with Bhasha-Rupantarika (INT4) in different Indic and overseas languages, showing translation for the

phrase “Model Verification and Validation”.

== Model size (GB)
IE Translation time (ms)

—e— Tokens per second

60

50

40

Tokens per second

30

l i lJ lJ
ps ints inta fp4

Fig. 10: Performance comparison for NLLB-600M across
different quantized precision.

Model size (GB) / Translation time (ms)

o_o

fp32 fp16 bflé

TABLE I: FPGA Resource utilization for SoTA MAC designs

100} mmm Area x1000 (mm*)
jms Power (mW)

—e Frequency (GHz)

2
8

a

8

o
g
Frequency (GHz)

3

iS
és

Area x1000 (mm?) / Power (mW)
5
8

8

2

Fig. 11: ASIC performance metrics, comparison with SoTA
MAC units CMOS 28nm, comparison data from (44).

TABLE III: FPGA Resource comparison for NAF functions
used in NLP accelerators.

and Fig. [12] respectively. Our MAC units exhibit notably
increased operational frequency and reductions in area and

Design Precision FPGA Utilization (Virtex 707) Design Function Precision Op. Freq. (MHz) | LUTs | FFs | Energy (pJ)
LUTs FFs Delay | Power | Arith. Intensity APCCAS’ 18 SoftMax 8/16-bit 436 2564 | 2794 1723
(us) | (mW) (pJ/Op) DHS8 2513 1746 | 1386 740
INT4 53 28 | 3.09 | 3.48 10.75 . IDHS8 264 1604 | 1354 698
ARATE CEES) INTS 130 | 44 | 3816 | 7.26 277 THESE 23 SoHMRE DHS12 255.8 1589 | 1235 696
INTI6 369 | 76 | 9.051 | 169 153 IDHS12 267 1450 | 1220 664
ENTS? HA26 | 214 | S93l | 22 130.4 TVLSI'25 SoftMax/GeLU | INTI6 435 1603 |_704 2466
‘Ad-FXP8 256 | 224 | 5.98 | 9.23 55.2 — 500 1446 | 652 1808
vests Ad-FXP16 a27_ | 369 | 65 | 11.78 7657 TVLSP25 SST*, rela | FXP8/16/32 85 897 | 1231 696
Ad-FXP32 681 745 7134 31 227.54 TCAS-IP’ 23 45) SST*, relu 12-bit 284 367 298 -
SIMD-Pipelined 8/16/32 | 897 | 1231 | 11.7 59.4 694 SoftMax FP32 10.88 3217 - 11033
Posit8 467 175 | 2.68 68 182.24 sigmoid FP32 8.27 5101 13189
ISCAS"25 Posit16 2083 | 528 | 435 | 189 822.15 ISQED’24 . tanh. FRS2 17.66 4298 | - 7358
Posit32 6813 | 806 8 347 2776 oftMax BF16 22.18 1263 | - 3472
SIMD-L Posit 4613 | 2078 | 62 276 426 sigmoid BEG 22.5 1856 | - 3694
TCAS-IP'24 SIMD-INT4/FP8/16/32 | 8054 | 1718 | 4.62 | 296 152 _ tani, BEG. 264 1513 | = 3100
TCAS-IP24 SIMD-FP16/32/64 8065 | 1072 | 5.56 | 378 543 This work SST*, rela |_FP8/BF16 192 1434 |_1208 a8
Q-4b 24 16 | 098 | 22 2.16
‘Access’ 24 Q8b 52 38 | 157 | 636 10
Q16b 106 | 168 | 22 | 11.77 26 . . .
SIMD Quan-MAC [150 | oaig | aoa | o138 - power consumption, leading to a conclusion of enhanced
= (RPS) energy efficiency and a compact solution. The NAF was
TCAS-1'22 FXP8 238 | 32 | 275 | 28 7.6 oe 7 :
CORDIC FXP4 35. | 58 | 1406 | 436 6.13 optimized, providing a superior balance between frequency
oe a * a oe — and area-power consumption, as workload characterization in-
Bropased INTAEPA/RBRIG He | bes | 5a | 1556 ARE dicates that the NAF accounts for up to 60% of the operations

in the overall NLLB.

For comparison, we implemented the NLPE architecture
with System Verilog RTL on ZCU104 MPSoC, and reported
the resources in Table[TV] Based on available FPGAs, we


TABLE IV: FPGA resource utilization, comparison with prior NLP accelerator designs.

Design Model FPGA Ri Precision | Power (W) | LUT (K) | FF (K) | DSP (K) | BRAM (K) ee
NPE BERT Zynq Z-7100 200 16-bit 20 156 261 2.02 0.53
ISQED’21 Transformer | Alveo U200 - - 25 472 378 2.34 - 34
TECS21 Multi-30K ZCU102 150 INTS i8 252 161 252 0.912 1870
TCAS-P'22 Yolov3 KCUI5 200 8-bit 4.6 213.3 352 2.24 :
TPDS*22 NMT VCU1I8 100 FP16 30 556 520 4.84 : wD)
TCAD'23 Swin-T Alveo U50 300 FP16 39 258 257 242 i 309.6
HPTA Swin-T ZCU102 200 8-bit 20 210 368 551 0.35 148.8
Q-BERT MNLI ZCU102 214 4/8-bit 98 274 548 252 1.83 :
INT4/EP4
This Work NLLB ZCU104 250 aoe 8.12 79.4 97.24 14 : 684.48

“= Frequency (GHz)

‘Area x1000 (mm?)

25

Frequency (GHz)

Area x1000 (mm?) / Power (mW)

Hy

Fy
Fy
&

Fig. 12: ASIC performance metrics, comparison with SoTA
NAF units CMOS 28nm, comparison with (37|-[41], [45)-

8}.

believe ZCU is a fair comparison of FPGA resources, and
our design showcases a significant resource reduction, approx-
imately 1.96x reduction in LUTs and 1.65 reduction in FFs
compared to the best of SoTA. It was worth noticing improved
throughput by a factor of 2.2 x compared to [8] and 4.6
x compared to (22). The results demonstrate a lightweight
language translation tool for remote areas with quick FPGA

deployment.

The importance of algorithm-hardware codesign in the NLP
edge deployment arena is centered on low-precision quanti-
zation, which results in reduced model sizes and accelerated
inference, ideal for real-time translation on IoT platforms,
with tightly constrained hardware resources (Fig. [9). Addi-
tionally, there is a focus on energy-per-token efficiency for
sustainable deployment. We hypothesise that examining scala-
bility for larger models could enhance high-performance cloud
implementations like chatbots, offering substantial hardware
optimisation. A comprehensive error analysis would highlight
the exact translation performance in terms of grammatical
errors, semantic shifts, and contextual inconsistencies, partic-
ularly with local dialects. We contend that employing mixed-
precision deployment strategies is vital for maintaining a
balance between accuracy and resource constraints.

IV. CONCLUSION

In this study, we evaluated the sub-octet quantization for
multilingual translation, demonstrating significant improve-
ments in memory usage, latency, and throughput, without com-
promising translation capability. Specifically, the FP4 model’s
size was reduced to 0.56 GB, achieving a 4.1x reduction
compared to FP32, which resulted in a throughput of 66
tokens per second, suitable for real-time processing on IoT
platforms. The core components of the accelerator, MAC and
NAF, outperformed previous work at both FPGA and ASIC
levels. FPGA deployment confirmed the algorithm’s benefits,
showing a 1.96x reduction in LUTs, a 1.65 reduction, and
a throughput increase by a factor of 2.2x compared to OPU
and 4.6x compared to HPTA at an operating frequency of
250 MHz. Overall, the findings establish Bhasha-Rupantarika
as an effective solution for multilingual translation systems.

REFERENCES

[1] Q. Li, X. Zhang, J. Xiong, W.-M. Hwu, and D. Chen, “Efficient Methods

for Mapping Neural Machine Translator on FPGAs,” IEEE Transactions

on Parallel and Distributed Systems, vol. 32, pp. 1866-1877, July 2021.

[2] X. Zhang, Y. Wu, P. Zhou, X. Tang, and J. Hu, “Algorithm-hardware

Co-design of Attention Mechanism on FPGA Devices,’ ACM Trans.

Embed. Comput. Syst., vol. 20, Sept. 2021.

[3] R. Dabre, C. Chu, and A. Kunchukuttan, “A Survey of Multilingual

Neural Machine Translation,’ ACM Comput. Surv., vol. 53, Sept. 2020.

[4] A. Joshi, R. Dabre, D. Kanojia, Z. Li, H. Zhan, G. Haffari, and
D. Dippold, “Natural Language Processing for Dialects of a Language:

A Survey,’ ACM Comput. Surv., vol. 57, Feb. 2025.

H. Song, R. Dabre, C. Chu, S. Kurohashi, and E. Sumita, “SelfSeg:

A Self-supervised Sub-word Segmentation Method for Neural Machine
Translation,’ ACM Trans. Asian Low-Resour. Lang. Inf. Process., vol. 22,
Aug. 2023.

C. Guo, F. Cheng, Z. Du, et al., “A Survey: Collaborative Hardware and
Software Design in the Era of Large Language Models,” IEEE Circuits
and Systems Magazine, vol. 25, pp. 35-57, Feb. 2025.

N. Sethiya, S. Nair, P. Walia, and C. Maurya, “Indic-ST: A Large-Scale
Multilingual Corpus for Low-Resource Speech-to-Text Translation,”
ACM Trans. Asian Low-Resour. Lang. Inf. Process., vol. 24, June 2025.
Y. Yu, C. Wu, T. Zhao, K. Wang, and L. He, “OPU: An FPGA-
Based Overlay Processor for Convolutional Neural Networks,” [EEE
Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28,
pp. 35-47, Jan. 2020.

[9] E. Azari and S. Vrudhula, “An Energy-Efficient Reconfigurable LSTM
Accelerator for Natural Language Processing,” in 2019 IEEE Interna-
tional Conference on Big Data (Big Data), pp. 4450-4459, 2019.

S. Jain, A. Sankar, D. Choudhary, D. Suman, N. Narasimhan, M. S. U. R.
Khan, A. Kunchukuttan, M. M. Khapra, and R. Dabre, “Bhasaanuvaad:
A speech translation dataset for 13 indian languages,” arXiv preprint
arXiv:2411.04699, Nov. 2024.

[5]

[6

[7

[8

[10]


{11

[12

{13

[14

[15

[16

[17

[18

[19

[20

[21

[22

[23

[24

[25

[26

[27

[28

[29

[30

G. K. Kumar, S. Praveen, P. Kumar, M. M. Khapra, and K. Nandakumar,
“Towards building text-to-speech systems for the next billion users,” in
Icassp 2023-2023 ieee international conference on acoustics, speech and
signal processing (icassp), pp. 1-5, IEEE, May 2023.

“Scaling neural machine translation to 200 languages,” Nature, vol. 630,
pp. 841-846, June 2024.

A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, “Robust speech recognition via large-scale weak super-
vision,” in International conference on machine learning, pp. 28492-
28518, PMLR, July 2023.

H. You, Z. Sun, H. Shi, Z. Yu, Y. Zhao, Y. Zhang, C. Li, B. Li, and
Y. Lin, “ViTCoD: Vision Transformer Acceleration via Dedicated Algo-
rithm and Accelerator Co-Design,” in IEEE International Symposium on
High-Performance Computer Architecture (HPCA), pp. 273-286, 2023.
T. Wang, L. Gong, C. Wang, Y. Yang, Y. Gao, X. Zhou, and H. Chen,
“ViA: A Novel Vision-Transformer Accelerator Based on FPGA,” IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 41, pp. 4088-4099, Nov. 2022.

T. Tambe, C. Hooper, L. Pentecost, et al., “EdgeBERT: Sentence-Level
Energy Optimizations for Latency-Aware Multi-Task NLP Inference,”
in MICRO-54: 54th Annual IEEE/ACM International Symposium on
Microarchitecture, MICRO ’21, (New York, NY, USA), p. 830-844,
Association for Computing Machinery, 2021.

Z. Liu, G. Li, and J. Cheng, “Hardware acceleration of fully quantized
bert for efficient natural language processing,” 2021 Design, Automation
& Test in Europe Conference & Exhibition (DATE), pp. 513-516, 2021.
S. Tuli and N. K. Jha, “AccelTran: A Sparsity-Aware Accelerator for Dy-
namic Inference With Transformers,’ JEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems, vol. 42, pp. 4038-
4051, Nov. 2023.

S. Hur, S. Na, D. Kwon, J. Kim, A. Boutros, E. Nurvitadhi, and J. Kim,
“A Fast and Flexible FPGA-based Accelerator for Natural Language
Processing Neural Networks,” ACM Trans. Archit. Code Optim., vol. 20,
Feb. 2023.

T. J. Ham, S. J. Jung, et al., “AS: Accelerating Attention Mechanisms
in Neural Networks with Approximation,’ in IEEE International Sym-
posium on High Performance Computer Architecture (HPCA), pp. 328-
341, 2020.

Y. Yu, T. Zhao, M. Wang, K. Wang, and L. He, “Uni-OPU: An FPGA-
Based Uniform Accelerator for Convolutional and Transposed Convo-
lutional Networks,” [EEE Transactions on Very Large Scale Integration
(VLSI) Systems, vol. 28, pp. 1545-1556, July 2020.

Y. Han and Q. Liu, “HPTA: A High Performance Transformer Acceler-
ator Based on FPGA,” in 2023 33rd International Conference on Field-
Programmable Logic and Applications (FPL), pp. 27-33, 2023.

T. J. Ham, Y. Lee, et al., “ELSA: hardware-software co-design for
efficient, lightweight self-attention mechanism in neural networks,” in
Proceedings of the 48th Annual International Symposium on Computer
Architecture, ISCA ’21, p. 692-705, IEEE Press, 2021.

M. Huang, A. Shen, K. Li, et al., “EdgeLLM: A Highly Efficient CPU-
FPGA Heterogeneous Edge Accelerator for Large Language Models,”
IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 72,
pp. 3352-3365, July 2025.

T. Tambe, E.-Y. Yang, G. G. Ko, Y. Chai, et al., “A 16-nm SoC for
Noise-Robust Speech and NLP Edge AI Inference With Bayesian Sound
Source Separation and Attention-Based DNNs,” [EEE Journal of Solid-
State Circuits, vol. 58, pp. 569-581, Feb. 2023.

J. Fowers, K. Ovtcharov, et al., “A configurable cloud-scale DNN pro-
cessor for real-time AI,” in Proceedings of the 45th Annual International
Symposium on Computer Architecture, ISCA ’18, p. 1-14, IEEE Press,
2018.

P. Scheffler, T. Benz, et al., “Occamy: A 432-Core Dual-Chiplet Dual-
HBMZ2E 768-DP-GFLOP/s RISC-V System for 8-to-64-bit Dense and
Sparse Computing in 12-nm FinFET,’ [EEE Journal of Solid-State
Circuits, vol. 60, Apr. 2025.

M. Sinigaglia et al., “Maestro: A 302 GFLOPS/W and 19.8 GFLOPS
RISC-V_ Vector-Tensor Architecture for Wearable Ultrasound Edge
Computing,’ JEEE Trans. on Circuits and Syst.- I, pp. 1-15, 2025.

H. J. Damsgaard, K. J. HoBfeld, and J. Nurmi, “Parallel Accurate
Minifloat MACCs for NN Inference on Versal FPGAs,” IEEE Trans.
Comp.-Aided Des. Integ. Cir. Syst., vol. 44, pp. 2181-2194, June 2025.
A. Tirumala and R. Wong, “NVIDIA Blackwell Platform: Advancing
Generative AI and Accelerated Computing,” in IEEE Hot Chips Sym-
posium (HCS), vol. 36, pp. 1-33, 2024.

31]

32

33]

37]

40]

41]

44

45]

46

47]

48

49

O. Kokane, M. Lokhande, G. Raut, A. Teman, and S. K. Vishvakarma,
“LPRE: Logarithmic Posit-enabled Reconfigurable edge-AI Engine,” in
2025 IEEE International Symposium on Circuits and Systems (ISCAS),
pp. 1-5, May 2025.

A. Rico, S. Pareek, et al., “AMD XDNA NPU in Ryzen AI Processors,”
IEEE Micro, vol. 44, pp. 73-82, Nov. 2024.

J.-S. Park, C. Park, et al., “A Multi-Mode 8k-MAC HW-Utilization-
Aware Neural Processing Unit With a Unified Multi-Precision Datapath
in 4-nm Flagship Mobile SoC,” JEEE Journal of Solid-State Circuits,
vol. 58, pp. 189-202, Jan. 2023.

H. Tan, J. Zhang, X. He, L. Huang, Y. Wang, and L. Xiao, “A Low-Cost
Floating-Point FMA Unit Supporting Package Operations for HPC-AI
Applications,” IEEE Trans. on Circuits and Systems II: Express Briefs,
vol. 71, pp. 3488-3492, July 2024.

S. Venkataramani, V. Srinivasan, et al., “RaPiD: AI Accelerator for
Ultra-low Precision Training and Inference,” ACM/IEEE 48th Annual
International Symposium on Computer Architecture, pp. 153-166, 2021.
T. Chaudhari, T. Dewangan, M. Lokhande, S. K. Vishvakarma, et al.,
“XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing
Engine for Extended Reality Perception Workloads,” arXiv preprint
arXiv:2508.13049, 2025.

M. Basavaraju, V. Rayapati, and M. Rao, “Exploring Hardware Ac-
tivation Function Design: CORDIC Architecture in Diverse Floating
Formats,” in 2024 25th International Symposium on Quality Electronic
Design (ISQED), pp. 1-8, 2024.

O. Kokane, G. Raut, S. Ullah, M. Lokhande, A. Teman, A. Kumar,
and S. K. Vishvakarma, “Retrospective: A CORDIC Based Configurable
Activation Function for NN Applications,’ in 2025 IEEE Computer
Society Annual Symposium on VLSI (ISVLSI), vol. 1, pp. 1-6, 2025.
H. Shao and Z. Wang, “ASTRA: Reconfigurable Training Architecture
Design for Nonlinear Softmax and Activation Functions in Transform-
ers,’ IEEE Transactions on Very Large Scale Integration (VLSI) Systems,
vol. 33, pp. 2054-2058, July 2025.

A. Belano, Y. Tortorella, A. Garofalo, et al., “A Flexible Template
for Edge Generative AI With High-Accuracy Accelerated Softmax and
GELU,” IEEE Journal on Emerging and Selected Topics in Circuits and
Systems, vol. 15, pp. 200-216, June 2025.

M. Lokhande, G. Raut, and S. K. Vishvakarma, “Flex-PE: Flexible
and SIMD Multiprecision Processing Element for AI Workloads,” [EEE
Trans. VLSI Syst., vol. 33, pp. 1610-1623, June 2025.

] B. Li, K. Li, et al., “A Reconfigurable Processing Element for Multiple-

Precision Floating/Fixed-Point HPC,’ JEEE Trans. Circuits Syst. I,
vol. 71, pp. 1401-1405, Mar. 2024.

N. Ashar, G. Raut, V. Trivedi, S. K. Vishvakarma, and A. Ku-
mar, “QuantMAC: Enhancing Hardware Performance in DNNs With
Quantize Enabled Multiply-Accumulate Unit,’ IEEE Access, vol. 12,
pp. 43600-43614, 2024.

M. Lokhande, S. J. Chand, A. Jain, S. Kumar, and S. K. Vishvakarma,
“ReNPU: A Resource-efficient Multi-Mode Neural Processing Unit with
Unified Multi-Precision Datapath for Mobile AI Workloads,” Authorea
Preprints, 2025.

X. Wu, S. Liang, M. Wang, and Z. Wang, “ReAFM: A Reconfigurable
Nonlinear Activation Function Module for Neural Networks,” [EEE
Transactions on Circuits and Systems II: Express Briefs, vol. 70,
pp. 2660-2664, July 2023.

A. Jha, T. Dewangan, M. Lokhande, and S. K. Vishvakarma, “QForce-
RL: Quantized FPGA-Optimized Reinforcement Learning Compute En-
gine,” 29th International Symposium on VLSI Design and Test, July
2025.

R. Andri, E. Reggiani, and L. Cavigelli, “Flex-SFU: Activation Func-
tion Acceleration with Non-Uniform Piecewise Approximation,” [EEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems, pp. 1-1, 2025.

Z. Mei, H. Dong, Y. Wang, and H. Pan, “TEA-S: A Tiny and Efficient
Architecture for PLAC-Based Softmax in Transformers,’ IEEE Transac-
tions on Circuits and Systems II: Express Briefs, vol. 70, pp. 3594-3598,
Sept. 2023.

D. T. Nguyen, H. Je, T. N. Nguyen, et al., “ShortcutFusion: From
Tensorflow to FPGA-Based Accelerator With a Reuse-Aware Memory
Allocation for Shortcut Data,’ IEEE Transactions on Circuits and
Systems I: Regular Papers, vol. 69, pp. 2477-2489, June 2022.
