arX1v:2510.09592v1 [cs.CL] 10 Oct 2025

‘; Mind-Paced Speaking: A Dual-Brain Approach to
Real-Time Reasoning in Spoken Language Models

Donghang Wu'?* Haoyang Zhang!?* Jun Chen! Xiangyu(Tony) Zhang! Hexin Liu?
Eng Siong Chng? Fei Tian!’ Xuerui Yang! Xiangyu Zhang! Daxin Jiang! Gang Yu!
'StepFun 7? Nanyang Technological University * University of New South Wales

Abstract

Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought
(CoT) reasoning due to the prohibitive latency of generating the entire thought pro-
cess sequentially. Enabling SLMs to think while speaking, similar to humans, is
attracting increasing attention. We present, for the first time, Mind-Paced Speaking
(MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning.
Similar to how humans utilize distinct brain regions for thinking and responding, we
propose a novel dual-brain approach, employing a "Formulation Brain" for high-level
reasoning to pace and guide a separate "Articulation Brain" for fluent speech gen-
eration. This division of labor eliminates mode-switching, preserving the integrity
of the reasoning process. Experiments show that MPS significantly outperforms
existing think-while-speaking methods and achieves reasoning performance com-
parable to models that pre-compute the full CoT before speaking, while drastically
reducing latency. Under a zero-latency configuration, the proposed method achieves
an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains
a score of 82.5 on the speech conversation task URO-Bench. Our work effectively
bridges the gap between high-quality reasoning and real-time interaction!.

1 Introduction

Speech has emerged as a more natural and fundamental modality for human-computer interaction,
leading to growing emphasis on spoken language models (SLMs) [1, 2, 3, 4, 5]. These models
facilitate seamless communication by processing and generating audio-based inputs and outputs.
A key component enhancing their capability is the integration of thinking, particularly through
Chain-of-Thought (CoT) processes and its extensions [6, 7, 8, 9], as implemented in frameworks
like Think-Before-Speak (TBS) [6, 10, 11]. This approach enables models to decompose complex
tasks into step-by-step reasoning sequences, thereby improving interpretability and performance in
dialogue systems.

However, generating complete CoT sequences often introduces significant latency, which hinders
real-time applications. Recent efforts to reduce reasoning latency have garnered significant attention
[12, 13]. These methods explore "think-while-speaking" paradigms, where models interleave

“Equal contribution
t corresponding authors: tianfei@stepfun.com
‘Our code is available at https://github.com/stepfun-ai/Step-MPS


Preprint. Work in progress.

thinking and response tokens. The Large Language Model (LLM) continuously switches between
think and response modes. It first generates several think tokens, then produces several response
tokens based on them. These response tokens are sent to the Text To Speech (TTS) system for
speech synthesis. While the speech is synthesizing, the LLM continues to generate more think
tokens. However, this interleaving disrupts semantic coherence by forcing the model to frequently
switch between thinking and response generation, potentially degrading the performance.

In fact, the human brain provides a biological analogy for efficient parallel processing. Cognitive
neuroscience reveals that thinking and speaking involve distinct brain areas [14, 15]. Speech does
not follow a rigid "think-then-speak" sequence or interleaved sequence. Crucially, it exhibits an
incremental nature where later parts of a thought are still being processed while the initial parts of
the utterance are already being spoken [16]. Inspired by this, we introduce Mind-Paced Speaking
(MPS), a novel architecture for enabling SLMs to "think" and "speak" in a concurrent and integrated
manner. The core of MPS is a dual-brain framework that operates analogously to the human
cognitive-speech system. One LLM acts as a central "Formulation Brain", continuously generating
an internal stream of thought. The other functions as an Articulation Brain, which receives this
thought stream in segments and generates the corresponding spoken output. The Formulation Brain
does not need to complete a full reasoning chain before the Articulation Brain begins. Instead,
the ongoing thinking process actively sets the pace and provides the contextual guidance for
the Articulation Brain, allowing it to vocalize fluently even as the underlying thoughts are still
being formed and refined by the Formulation Brain. This mind-paced mechanism ensures that the
spoken output is not only grounded in a thinking process but also maintains semantic coherence,
closely mimicking the natural human process of thinking while speaking. Furthermore, we propose
a think-incomplete Supervised Fine-Tuning (SFT) method to enable the Articulation Brain to
respond based on incomplete thinking content. The experimental results on benchmarks such as
mathematical reasoning, dialogue, and question-answering, prove that compared to methods that
answer directly without thinking, or existing methods that think while speaking, the proposed MPS
method effectively utilizes the thinking process and continuous semantic context, obtaining more
accurate and higher-quality responses. Compared to TBS method, the proposed MPS significantly
reduces response latency while maintaining performance.

Our main contribution can be summarized as follows:

(1) We propose an MPS architecture that enables SLMs to achieve human-like think-while-speaking
capabilities. This method significantly reduces the latency of the CoT process while maintaining
the semantic coherence of the LLM. Consequently, the LLM leverages the CoT content to deliver
superior performance.

(2) We develop a think-incomplete SFT to train LLMs to generate responses based on partial
thinking processes, thereby enabling them to perform think-while-speaking.

(3) We evaluate two distinct MPS architectures: Speak-First and Think-First, against baseline
methods. Experimental results demonstrate that the proposed think-while-speaking MPS method
significantly outperforms both the direct response approach without a thinking process and exist-
ing interleaved think-while-speaking methods. Compared to the TBS architecture, our method
substantially reduces response latency while maintaining the quality of the LLM’s responses.

(4) Our proposed MPS architecture mimics the neuroscientific mechanisms of human thinking
and speaking, transcending the structural limitations of existing interleaved think-while-speaking


Preprint. Work in progress.

methods. It provides the research community with a reference paradigm for subsequent studies on
anthropomorphic, real-time dialogue systems

2 Related Work

2.1 Spoken Language Models

Spoken Language Models (SLMs) accept user speech input and generate speech output, enabling
real-time speech dialogue with users. Since the LLM backbone is typically trained in the text
domain, directly generating speech tokens presents a challenge [12]. Most current SLMs first
generate text tokens and then generate speech. This is achieved through two primary methods: one
approach uses the LLM to output text, which is then synthesized into speech by an additional TTS
model [17, 18]. Another method employs the LLM to generate interleaved text and audio tokens,
where each output chunk contains a fixed number of text tokens and speech tokens, and a speech
decoder directly synthesizes the speech signal [19, 2]. For example, Step-Audio 2 produces output
in the ta4 format, which means it outputs one text token followed by four audio tokens. This chunk
of output is then passed through a speech detokenizer to obtain the speech signal [2].

2.2 Reason for SLMs

Although explicit CoT has been proven helpful in text LLMs, most SLMs still lack CoT capability.
One reason is that audio and text have different structures; another reason is that directly synthesizing
CoT into speech increases the confusion of responses, while generating silent CoT introduces
significant latency, which becomes unreasonable in daily conversations. Some studies introduce
the reasoning ability into Audio LLMs [12]. For example, Xie et al. have proposed an audio CoT
reasoning dataset to fine-tune models [20]. Some studies use reinforcement learning, such as GRPO
[21], to fine-tune models and enhance their reasoning ability [22, 23]. However, these studies remain
limited to audio-in-text-out Audio LLMs, not SLMs that can engage in dialogue with humans. In
[2], Step-Audio 2, which takes speech as input and output, using CoT and reinforcement learning to
improve the response qualities, is proposed. Step-Audio 2 offers a solution for introducing explicit
reasoning into SLMs. Some methods achieve simultaneous thinking and speaking by segmenting
CoT content and response content, using the LLM to generate interleaved think tokens and response
tokens [12, 13]. However, this approach differs from the LLM’s original response generation format.
The LLM needs to continuously switch between think mode and response mode, which disrupts
semantic coherence and affects its performance.

3 Method

This section first outlines the conventional TBS-based SLM. We then present the proposed MPS
method. We also introduce the think-incomplete SFT, which is designed to teach LLMs the
think-while-speaking capability.

3.1 Think Before Speaking

The architecture of TBS-based SLM is shown in Figure 1. To enhance the reasoning ability of
SLM, the TBS paradigm, after receiving user speech X*?* and optional text instructions X ‘*, first
generates step-by-step CoT tokens Y°* € IR”, and then generates the response tokens Y"® € R”",



Preprint. Work in progress.

Hl BEBE EB ERR EBER ERR eee
| LLM Decoder
| EREEEEHE ES ERE E Ew
Audio Adapter
Input Token Audio Detokenizer
Audio Encoder Think Token

Response Token | | | | |

r
WHA HTAII il

Input Audio Output Audio

Figure 1: Architecture of the TBS architecture. For the sake of conciseness, we remove the input text, which is optional
in SLMs. The TBS SLM first generates the full CoT and then produces response tokens.

where 7’, and T7;. denote the number of CoT tokens and response tokens, respectively. This can be
divided into two processes: the thinking process and the speaking process. The thinking process
can be written as:

Te
Poy (Wor|(xPe, XY) = TY Pa (VEN, XP X™)), (1)
t=1

where @; denotes the parameters of the SLM. After that, the LLM generates response tokens for
speaking, which can be formulated as:

T;-
Py (YY, ete Xx) _— [[2.07e\( Fe ats YR Pe xe), (2)
t=1

Through this method, the task is decomposed into a step-by-step process. Additionally, by intro-
ducing CoT tokens, it enables more Transformer forward operations and thus gives LLM a deeper
inference depth [24, 25].

3.2 Architecture

In the human brain, speech production is not a monolithic process but the result of two highly
specialized and collaborative systems. The first, a network centered around the prefrontal-temporal
cortex, is responsible for high-level cognitive functions such as conceptualization, logical reasoning,
and content planning. Subsequently, a second system, primarily involving the motor cortex and
subcortical pathways, translates these abstract thoughts into natural language for articulation,
enabling fluent speech. These two systems operate in parallel, with the cognitive system continuously
supplying thinking content to the articulatory system, creating a natural flow where the mind paces
speech [14, 15].

Inspired by this, we abstract this mechanism of separated "formulation" and "articulation" into our
model architecture. Instead of relying on a single LLM to handle both thinking and speaking, we
propose a dual-brain system composed of two distinct LLMs.

Our proposed framework, illustrated in Figure 2, leverages a dual-LLM architecture consisting
of a Formulation Brain LLM and an Articulation Brain LLM. The Formulation Brain LLM is
dedicated to user intent understanding and performs deliberate CoT reasoning, with its internal
process materialized as "think tokens". Subsequently, the Articulation Brain LLM converts this


Preprint. Work in progress.

Time step i

i Think |

Time step i+1

| Think |

| Gio oi | | | | Un ee
Formulation | LLM Decoder LLM Decoder
Ban SE ESSERE EG LLL Lo
Robin's hair.... Howlong...? _ First, the problem... Robin's hair.... How long...?_ First, the problem ... Then it says ...
Same
Robin's hair.... How long... ? First, the probl Model
Histc | Current |
Thin 1 Think 1, '
[ BHEERETEA fi BEESBS &@ i ow: _
Articulation (nn nena) nn)—“‘(i‘é‘édS upp )
i | LLM Decoder LLM Decoder
Brain L — C ——— =
BEB EEBESB ao LB ER eEeee Oo | [
_ "Audio Adapter ] si { Audio Adapter .
{J Input Token - | Audio Detokenizer | 7 Audio Detokenizer
| Think Token Audio Encoder ] { Audio Encoder |
—_Kesponse Teen i UAT TTT i UAT] HTT
TNT UNIT LI]

Input Audio Output Audio Input Audio Output Audio

Figure 2: Architecture of the proposed MPS. For the sake of conciseness, we remove the input text, which is optional in
SLMs. We demonstrate the process from step i to step i+ 1 when generating think segments and response segments.
The Formulation Brain LLM continuously generates the think segments. The newly generated think segment and
the response segment from the previous step are both added as the prefix to the Articulation Brain LLM, pacing the
Articulation Brain LLM to produce response segment correspondingly.

structured reasoning and the dialogue context into natural language, producing the final "response
tokens" for spoken output.

Formulation Brain: The Formulation Brain’s operating mode is identical to that of TBS Audio
LLMs but with only the thinking process. After receiving user input X*?° and X“*, it aims to
generate the step-by-step CoT tokens Y°°t € IR7°. We use the tokens <think> and </think>
to mark the beginning and end of the CoT. This process can be formulated as (1). In the MPS
architecture, we do not wait for the Formulation Brain to complete the entire CoT before the
Articulation Brain starts speaking. We divide the CoT tokens Y°* into N segments, denoted as
[Sy°", Ss", ..., S§"]. Each time the Formulation Brain produces a think segment °°", we feed the
segment to the Articulation Brain, which then generates a response segment based on the current
think segment and historical thinking and response contents. After the Formulation Brain LLM
finishes CoT segments, it stops generating response tokens as we do not require the Formulation
Brain to speak.

Articulation Brain: The Articulation Brain accepts the same user input as the Formulation Brain.
After obtaining the current think segment S¢* from the Formulation Brain, we concatenate it
with the historical think segments [S§°, SS, ..., S€°,], placing <think> and </think> at the
beginning and the end, and then append the historical response segments, which are defined as
[Sis S5°,..., St, |. This allows the Articulation Brain to continue generating the subsequent
response content. After that, we use a streaming TTS model to synthesize speech in real-time. The
Articulation Brain’s output is incremental. For every think segment that the Formulation Brain
produces, the Articulation Brain generates a segment of the response S7°°. This process can be

written as:

N
Po,(S™|(S°%, XP", KY) = TT Pa (SH |(S1oe 1, Sih, KP, XP), (3)
n=l


Preprint. Work in progress.

When the Formulation Brain just begins its thinking, the Articulation Brain can only generate a
response segment based on a small amount of CoT. The response segment it generates at this stage
may be of lower quality. As the Formulation Brain’s thinking content increases, the Articulation
Brain receives more CoT content, and it subsequently generates responses of increasingly higher
quality.

Compared with existing think-while-speaking methods that use a single LLM to predict interleaved
think and response tokens, thereby forcibly interrupting and splitting the originally continuous
think and response content [12, 13], our method adopts a dual-brain design consisting of the
Formulation Brain and the Articulation Brain. From the perspectives of the Formulation Brain and
the Articulation Brain, both are classic TBS LLMs that, after receiving user input, first generate
step-by-step CoT content and then generate response content conditioned on the CoT, thereby
greatly ensuring the semantic coherence of the LLM output. By allowing the Formulation Brain to
pace the Articulation Brain, our method achieves a human-like think-while-speaking process.

3.3. Think-incomplete SFT

Since the proposed MPS method does not change the input-output patterns of the classic LLM
for the individual Formulation Brain and Articulation Brain, the proposed MPS, unlike existing
think-while-listening methods [12, 13], does not require repretraining the LLM. To ensure that
the Articulation Brain LLM possesses the ability to accept incomplete think content and produce
reasonable output, we introduce think-incomplete SFT. In the construction of training data, we
randomly retain the content of the first L steps of the step-by-step CoT, delete the subsequent CoT
content, then place this incomplete CoT with <think> and </think> tokens at the beginning and
end, concatenate it with the groundtruth response, and use it as the next-token-prediction training
objective for the LLM [26].

During the inference stage, we use segments with a fixed number of tokens. We set 7’. and T;. to 80
and 100 respectively. We use the output format of Step-Audio 2, specifically the ta4 format, which
generates one text token followed by four speech tokens, thus every 100 response tokens contain 20
text tokens and 80 speech tokens. We also attempt to use the same segment division strategy as in
the think-incomplete SFT phase, but we find that it does not bring improvement; on the contrary, it
introduces uncontrollable latency due to the variable length of each CoT step. We also try using a
fixed token count strategy for dividing the CoT during the think-incomplete SFT phase, but it does
not yield performance improvements either.

4 Experiments

4.1 Experimental Settings

The LLM backbone used in this paper is Step-Audio 2, and its parameter settings refer to [2].
The LLMs in Formulation Brain and Articulation brain share the same parameters. To verify
the effectiveness of the proposed method on tasks requiring reasoning, we use Spoken-MQA, a
mathematical reasoning dataset [27]. We use accuracy as the evaluation metric. Furthermore, to
validate the method’s effectiveness on general dialogue tasks, we introduce URO-Bench, which
contains several subtasks such as daily dialogue, emotion recognition, paralinguistic information,
and question-answering [28]. For question-answering tasks, we use accuracy as the metric. For


Preprint. Work in progress.

other tasks, we use GPT-score, generated by GPT-40-mini and ranging from 0 to 100, to evaluate
response quality.

To accommodate the latency requirements of different application scenarios, we implement two
distinct MPS paradigms:

¢ Think-First, denoted as MPS-thkfirst: The Formulation Brain LLM first generates JT”, think
tokens, after which the Articulation Brain LLM generates T;. response tokens and synthesizes
speech. Under this setting, the latency is 7’. plus the buffer size of streaming TTS, which
is significantly lower than the latency required for the TBS structure to generate a complete
Chain-of-Thought.

Speak-First, denoted as MPS-spkfirst: The Articulation Brain LLM first generates 7). response
tokens, while simultaneously, the Formulation Brain LLM begins generating think tokens. The
Formulation Brain LLM completes generating JT’, think tokens before the speech synthesized
from the T;. response tokens finishes playing. In this configuration, the latency is solely the
buffer size of the streaming TTS, meaning the model can be considered to respond directly with
near-zero latency.

Additionally, we compare the proposed method with two approaches that use the same LLM
backbone as in this paper: Think-Before-Speaking (MPS-tbs) and direct response without thinking
(MPS-wo/thk), to validate the effectiveness of our proposed think-while-speaking methodology.

4.2. Data Construction

We begin with real-world user queries as our seed set. To ensure topical diversity and sufficient scale,
we employ GPT-4o [29] for transcription and augmentation of these queries. These augmented
queries are then used as user prompts to distill dialogue data with native CoT from the DeepSeek-R1
model [11].

However, the raw data generated by DeepSeek-R1, a text-centric model, presents two critical
challenges for spoken dialogue applications: (1) Text-specific stylizations, such as Markdown
formatting and emojis, which are incompatible with speech synthesis. (2) The CoT data reflects
complete, turn-based reasoning chains, a format unsuitable for training the model to respond from
partial thoughts. When the CoT generated by the LLM exhibits some incomplete, its performance
is affected.

To address these challenges, we implement a fine-grained data processing pipeline:

¢ Compatibility Processing: We discard samples containing Markdown formatting or multi-item
lists that cannot be naturally rendered in speech. For samples containing emojis, we employ
Qwen-72B-Instruct [30] to remove these elements while preserving the plain text content.

¢ CoT Pruning: To train the model to respond stably with only partial CoT, we augment the data
by randomly deleting some reasoning paragraphs. This operation is performed in a way that
generally preserves the overall logic of the CoT. Crucially, to maintain the stylistic distribution
of the original DeepSeek CoT, we neither delete individual sentences within a paragraph nor use
an LLM to rewrite the content of the remaining parts. This ensures that the preserved paragraphs
are stylistically and distributionally consistent with the source model.


Preprint. Work in progress.

Table 1: Test-set accuracy (%) of different methods on the Spoken-MQA benchmark. The evaluated approaches include:
the direct response baseline without a thinking process (MPS-wo/thk), Think-Before-Speaking (MPS-tbs), Think-First
(MPS-thkfirst), and Speak-First (MPS-spkfirst). Results of baseline systems are taken from [13] except that results of

Step-Audio 2 are reproduced by ourselves.

Method Arithmetic Reasoning Avg
Short Long Avg Single Multi Avg

Whisper-Qwen2.5-7B-Instruct - - 70.0 - - 72.5 72.2

Whisper-Qwen2.5-Math-7B-Instruct - - 77.3 - - 86.7 85.6

LLaMA-Omni 40.0 11.0 23.5 29.5 10.5 16.2 16.8
Mini-Omni 5.0 2.3 3.5 0.8 1.9 1.6 1.7
Freeze-omni 43.0 14.5 26.8 69.0 19.8 344 33.3
GLM-4-Voice 40.0 225 30.1 544 28.5 36.2 35.3
Qwen2-Audio-7B-Instruct 43.0 31.2 363 554 22.5 32.3 32.7
Qwen2.5-Omni-7B 83.00 45.1 61.5 85.2 71.5 75.6 73.6
Qwen2.5-Omni-3B 84.0 43.3 60.1 81.5 57.1 644 63.6
Mini-Omni-Reasoner 92.9 66.1 77.3 85.9 60.5 68.1 68.6
Step-Audio 2 89.0 52.6 65.9 95.6 90.4 91.9 88.8
MPS-wo/thk 71.0 34.1 47.6 88.0 67.8 73.8 70.6
MPS-tbs 90.0 884 89.0 94.4 93.2 93.6 93.0
MPS-thkfirst 89.0 849 864 95.6 94.6 94.9 93.9
MPS-spkfirst 87.0 71.7 77.32 96.0 94.5 94.9 928

4.3 Results

4.3.1 Evaluation on Reasoning Tasks

Table 1 shows the computational accuracy on Spoken-MQA. It can be seen that the proposed MPS-
thkfirst method exceeds the MPS-wo/thk method and all baseline methods, including the think-while-
speaking Mini-Omni-Reasoner, in all evaluation tasks. The results proves that the proposed method
effectively utilizes the thinking process, achieving more intelligent response. Compared to Mini-
Omni-Reasoner, the MPS method maintains semantic coherence and achieves better performance.

Besides, compared to MPS-tbs, the MPS-
thkfirst method demonstrates comparable per-
formance, being slightly weaker in arithmetic
computation tasks but superior in reasoning
tasks. One possible explanation is that reason-
ing tasks require more textual analysis. The
MPS-thkfirst method, by using each think seg-
ment to pace the generation of a corresponding
response segment, implicitly achieves semantic
alignment, enabling the model to better utilize
contextual information for response generation.

The MPS-spkfirst method experiences some per-
formance degradation because it initially out-
puts a response segment without utilizing any

Table 2: The average accuracy of different models with CoT
capability on Spoken-MQA, and the extra tokens gener-
ated by the model before generating the first response token.
The evaluated approaches include: Interleaved Think-While-
Speaking (Mini-Omni-Reasoner), Think-Before-Speaking
(MPS-tbs), Think-First (MPS-thkfirst), and Speak-First
(MPS-spkfirst).

Method Accuracy Extra Tokens
Mini-Omni-Reasoner 68.6% 8
MPS-tbs 93.0% 762
MPS-thkfirst 93.9% 80
MPS-spkfirst 92.8% 0

think segments. This impact is particularly pronounced in tasks involving direct arithmetic compu-
tation. For reasoning tasks, experimental observations indicate that the initial phase of the LLM’s


Preprint. Work in progress.

Table 3: Performance of different methods on the URO-Bench. The evaluated approaches include: the direct response
baseline without a thinking process (MPS-wo/thk), Think-Before-Speaking (MPS-tbs), Think-First (MPS-thkfirst), and
Speak-First (MPS-spkfirst). Results of baseline systems are taken from [2]. The results of Multilingual of URO-Bench
are included in English.

Method Language Basic Pro

U. R. O. Avg’ U. R. O. Avg
GPT-40 Audio 89.4 65.5 85.2 78.6 70.6 57.2 70.2 67.1
GPT-Realtime Chinese 88.8 72.9 90.8 80.6 72.3 62.6 74.2 70.6
Kimi-Audio 79.3 64.7 79.8 73.6 604 59.3 76.2 66.0
Qwen-Omni 59.7 69.7. 77.3 69.0 59.0 59.8 58.7 59.1
Step-Audio 2 91.1 75.5 86.1 83.3 74.8 63.2 65.1 68.3
MPS-wo/thk 916 77.3 87.7 83.4 75.1 74.7 72.9 744
MPS-tbs Chinese 92.6 82.4 93.8 87.8 75.3 84.2 79.5 79.0
MPS-thkfirst 93.6 84.0 94.8 89.1 75.2 84.2 85.2 80.5
MPS-spkfirst 92.5 82.5 93.1 87.6 77.2 848 79.0 79.9
GPT-40 Audio 90.2 75.9 904 845 60.7 644 785 67.5
GPT-Realtime English 874 84.1 941 88.1 59.7 745 76.1 68.9
Kimi-Audio 83.4 42.3 604 60.0 50.3 406 560 49.8
Qwen-Omni 66.3 69.6 76.2 70.6 445 63.9 494 51.0
Step-Audio 2 92.7 76.5 849 83.9 64.9 67.8 66.3 66.1
MPS-wo/thk 91.5 68.7 78.8 774 734 79.2 55.8 65.1
MPS-tbs English 92.3 81.5 875 86.1 764 864 87.1 83.3
MPS-thkfirst 94.2 814 89.0 87.0 765 89.3 89.4 85.0
MPS-spkfirst 94.1 78.5 87.5 85.2 76.0 89.7 69.9 74.8

CoT content primarily involves analyzing the semantic information of the question, often rewriting
the question’s content. Consequently, this initial portion of the CoT content has a limited effect
on the final response. As a result, MPS-spkfirst is minimally affected in reasoning tasks, and its
performance remains nearly identical to that of MPS-thkfirst. Experimental results on Spoken-MQA
demonstrate that the proposed method significantly leverages CoT to achieve more intelligent
responses. Furthermore, compared to TBS methods, our think-while-speaking approach achieves
comparable performance, with significantly lower CoT latency as analysed in Section 4.1.

To demonstrate the latency differences between the proposed method and baseline approaches, we
select four models from Table | that include a thinking process: Mini-Omni-Reasoner, MPS-tbs,
MPS-thkfirst, and MPS-spkfirst. We calculate the number of extra tokens generated by the model
from the end of the user’s question to the generation of the first response token on Spoken-MQA. The
results are shown in Table 2. It can be observed that compared to MPS-tbs, MPS-thkfirst achieves
higher accuracy while exhibiting significantly lower response latency. Although the accuracy of
MPS-spkfirst is slightly lower than that of MPS-tbs and MPS-thkfirst, its response is without latency.
Furthermore, compared to Mini-Omni-Reasoner, which uses interleaved think and response tokens
to achieve think-while-speaking, the proposed MPS methods achieve higher accuracy. Notably, the
MPS-spkfirst attains this superior accuracy with zero latency. This indicates that MPS-spkfirst can
play a more critical role in real-time dialogue scenarios with low-latency requirements. An example
of the output of MPS-spkfirst is shown in Appendix A.1.


Preprint. Work in progress.

4.3.2 Evaluation on Speech-to-speech conversation

Table 3 shows the results of different methods on URO-Bench. It can be observed that MPS-thkfirst
achieves higher performance than MPS-tbs on nearly all tasks and on average, under lower response
latency. This may also be related to the implicit semantic alignment performed by MPS-thkfirst.
Due to generating an initial response segment without prior thinking, MPS-spkfirst performs slightly
worse than MPS-thkfirst, but still significantly outperforms the direct response method MPS-wo/thk.
Nevertheless, MPS-spkfirst features lowest response latency as analysed in Section 4.1, and its
response performance remains close to or even better than that of MPS-tbs in some tasks, making it
more suitable for scenarios requiring faster feedback. The experimental results demonstrate that
the proposed MPS method maintains high performance on dialogue tasks, achieving performance
comparable to TBS models while operating at significantly lower latency.

5 Conclusion

This paper proposes the MPS method, which enables SLMs to possess the ability to think while
speaking. Inspired by the human thinking and response mechanism, we use a Formulation Brain
LLM to continuously generate think segments, pacing the Articulation Brain LLM to utilize
historical and current think segments, as well as historical responses, to generate current response
segment, ensuring semantic coherence. Experimental results on mathematical reasoning and speech
conversation tasks show that the proposed method significantly outperforms direct response methods
and existing think-while-speaking methods. It achieves performance comparable or even better than
methods that complete thinking before responding, while greatly reducing response latency. The
proposed method breaks through the limitations of existing interleaved thinking and response-based
think-while-speaking methods and provides an effective reference for researching real-time dialogue
consistent with human thinking and response mechanisms.

6 Acknowledgement

We would like to express our sincere gratitude to Liang Zhao and Chengyuan Yao for their insightful
suggestions and constructive discussions regarding the design of the model’s thinking mechanism.
Their expertise greatly contributed to the development of the Formulation Brain LLM in this work.

References
[1] Wengian Cui et al. “Recent advances in speech language models: A survey”. In: arXiv preprint arXiv:2410.03751
(2024).
[2] Boyong Wu et al. “Step-audio 2 technical report”. In: arXiv preprint arXiv:2507. 16632 (2025).

[3] Ke Huetal. “Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model”. In: arXiv preprint
arXiv:2505.15670 (2025).

[4] Wenyi Yu et al. “Salmonn-omni: A codec-free Ilm for full-duplex speech understanding and generation”. In:
arXiv preprint arXiv:2411.18138 (2024).

[5] Alexandre Défossez et al. “Moshi: a speech-text foundation model for real-time dialogue”. In: arXiv preprint
arXiv:2410.00037 (2024).

[6] Jason Wei et al. “Chain-of-thought prompting elicits reasoning in large language models”. In: Advances in neural
information processing systems 35 (2022), pp. 24824-24837.

10


Preprint. Work in progress.

Xuezhi Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models”. In: The
Eleventh International Conference on Learning Representations. 2023. URL: https: //openreview.net/
forum? id=1PLiNIMMrw.

Shunyu Yao et al. “Tree of thoughts: Deliberate problem solving with large language models’. In: Advances in
neural information processing systems 36 (2023), pp. 11809-11822.

Luyu Gao et al. “Pal: Program-aided language models’. In: International Conference on Machine Learning.
PMLR. 2023, pp. 10764-10799.

Jingran Xie et al. “Leveraging chain of thought towards empathetic spoken dialogue without corresponding
question-answering data”. In: ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). EEE. 2025, pp. 1-5.

Daya Guo et al. “Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning”. In: arXiv
preprint arXiv:2501.12948 (2025).

Cheng-Han Chiang et al. “STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken
Language Models”. In: arXiv preprint arXiv:2507.15375 (2025).

Zhifei Xie et al. “Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models”. In: arXiv
preprint arXiv:2508. 15827 (2025).

Nancy J Nersessian. The cognitive basis of model-based reasoning in science. na, 2002.

Gregory Hickok and David Poeppel. “The cortical organization of speech processing”. In: Nature reviews
neuroscience 8.5 (2007), pp. 393-402.

Peter Indefrey. “The spatial and temporal signatures of word production components: a critical update”. In:
Frontiers in psychology 2 (2011), p. 255.

Jin Xu et al. “Qwen2. 5-omni technical report”. In: arXiv preprint arXiv:2503.20215 (2025).

Qingkai Fang et al. “Llama-omni2: Llm-based real-time spoken chatbot with autoregressive streaming speech
synthesis”. In: arXiv preprint arXiv:2505.02625 (2025).

Aohan Zeng et al. “Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot”. In: arXiv
preprint arXiv:2412.02612 (2024).

Zhifei Xie et al. “Audio-reasoner: Improving reasoning capability in large audio language models”. In: arXiv
preprint arXiv:2503.02318 (2025).

Zhihong Shao et al. “Deepseekmath: Pushing the limits of mathematical reasoning in open language models”. In:
arXiv preprint arXiv:2402.03300 (2024).

Cheng Wen et al. “Sari: Structured audio reasoning via curriculum-guided reinforcement learning”. In: arXiv
preprint arXiv:2504. 15900 (2025).

Gang Li et al. “Reinforcement learning outperforms supervised fine-tuning: A case study on audio question
answering”. In: arXiv preprint arXiv:2503.11197 (2025).

Sachin Goyal et al. “Think before you speak: Training language models with pause tokens”. In: arXiv preprint
arXiv:2310.02226 (2023).

Jacob Pfau, William Merrill, and Samuel R Bowman. “Let’s think dot by dot: Hidden computation in transformer
language models”. In: arXiv preprint arXiv:2404.15758 (2024).

Tom Brown et al. “Language models are few-shot learners”. In: Advances in neural information processing
systems 33 (2020), pp. 1877-1901.

Chengwei Wei et al. “Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over
Multi-faceted Math Problems”. In: arXiv preprint arXiv:2505.15000 (2025).

Ruiqi Yan et al. “Uro-bench: A comprehensive benchmark for end-to-end spoken dialogue models”. In: arXiv
preprint arXiv:2502.17810 (2025).

11


Preprint. Work in progress.

[29]

[30]

OpenAl. GPT-40 System Card. 2024. arXiv: 2410.21276 [cs.CL]. URL: https://arxiv.org/abs/2410.
21276.

An Yang et al. Qwen2 Technical Report. 2024. arXiv: 2407 . 10671 [cs.CL]. URL: https: //arxiv.org/
abs/2407. 10671.

12


Preprint. Work in progress.

A Appendix

A.1 Example of MPS-spkfirst

Figure 3 shows an example of MPS-spkfirst on Spoken-MQA. After receiving the user input, the
Articulation Brain LLM first generates a response segment S7°°. Simultaneously, the Formulation
Brain LLM produces the first think segment S‘"*, S'"* is then prefixed to the Articulation Brain
LLM along with S{° to pace the Articulation Brain LLM in generating the second response segment
S3°*. During this period, the Formulation Brain LLM generates the second think segment S%"*.
S"* is further prefixed to the Articulation Brain LLM, where S#"*, S2’*, St°s, and $3° collectively
pace the Articulation Brain LLM to produce $3°°. This process repeats until the Formulation Brain
LLM generates the complete think content, after which the Articulation Brain LLM continues
generating content until completion.

Question:

"Robin's hair was 16 inches long. He cut off 11 inches. Then it again grew by 12 inches. How long is his hair now?"
Groundtruth:

"47"

Response:

Step1:

Formulation Brain:

Articulation Brain:

"Robin's hair started at 16 inches. He cut off 11"

Step1:

Formulation Brain:

"First, the problem states that Robin's hair was initially 16 inches long. He cut off 11 inches. So, after cutting, the length would
be 16 inches minus 11 inches. That gives 5 inches. Then"

Articulation Brain:

"Robin's hair started at 16 inches. He cut off 11 inches, so 16 minus 11 is 5 inches. Then, his "

Step2:

Formulation Brain:

"First, the problem states that Robin's hair was initially 16 inches long. He cut off 11 inches. So, after cutting, the length would
be 16 inches minus 11 inches. That gives 5 inches. Then, it says his hair grew by 12 inches. So, starting from the 5 inches
after cutting, adding 12 inches would be 5 + 12 = 17 inches. The problem says "

Articulation Brain:

"Robin's hair started at 16 inches. He cut off 11 inches, so 16 minus 11 is 5 inches. Then, his hair grew by 12 inches, so 5 plus
12 equals "

Step3:

Formulation Brain:

"First, the problem states that Robin's hair was initially 16 inches long. He cut off 11 inches. So, after cutting, the length would
be 16 inches minus 11 inches. That gives 5 inches. Then, it says his hair grew by 12 inches. So, starting from the 5 inches
after cutting, adding 12 inches would be 5 + 12 = 17 inches. The problem says \"he cut off 11 inches,\" which clearly indicates
cutting, so subtracting is correct. So, the final answer should be 17 inches."

Articulation Brain:

"Robin's hair started at 16 inches. He cut off 11 inches, so 16 minus 11 is 5 inches. Then, his hair grew by 12 inches, so 5 plus
12 equals 17 inches. Therefore, Robin's hair is now 17 inches long. "

Figure 3: An example of the output of MPS-spkfirst on the Spoken-MQA dataset. The Articulation Brain first generates
a response segment. Simutaneously, Formulation Brain continuously generates new think segments, and each newly
generated think segment is prefixed to the Articulation Brain, pacing it to generate new response segment.

13
