arXiv:2501.12826v1 [cs.CL] 22 Jan 2025

Open or Closed LLM for Lesser-Resourced Languages?
Lessons from Greek

John Pavlopoulos

AUEB & Archimedes/Athena RC, Greece

annis@aueb.gr

Kanella Pouli
ILSP/Athena RC, Greece

Abstract

Natural Language Processing (NLP) for lesser-
resourced languages faces persistent chal-
lenges, including limited datasets, inherited
biases from high-resource languages, and the
need for domain-specific solutions. This study
addresses these gaps for Modern Greek through
three key contributions. First, we evaluate
the performance of open-source (Llama-70b)
and closed-source (GPT-40 mini) large lan-
guage models (LLMs) on seven core NLP tasks
with dataset availability, revealing task-specific
strengths, weaknesses, and parity in their per-
formance. Second, we expand the scope of
Greek NLP by reframing Authorship Attribu-
tion as a tool to assess potential data usage by
LLMs in pre-training, with high 0-shot accu-
racy suggesting ethical implications for data
provenance. Third, we showcase a legal NLP
case study, where a Summarize, Translate, and
Embed (STE) methodology outperforms the tra-
ditional TF-IDF approach for clustering long le-
gal texts. Together, these contributions provide
a roadmap to advance NLP in lesser-resourced
languages, bridging gaps in model evaluation,
task innovation, and real-world impact.

1 Introduction

Natural Language Processing (NLP) tasks have
advanced significantly with the help of deep learn-
ing, and more recently with large language models
(LLMs), the creation of which demands immense
volumes of digital data (Brown et al., 2020). While
multilingual NLP has benefited from these ad-
vances, the progress in lesser-resourced languages
significantly lags behind that of well-supported
languages. As a result, NLP for the myriad of
languages worldwide relies heavily on research
conducted for well-established languages, often
inheriting their assumptions, biases, and other char-
acteristics that may not align with the features of
less supported languages (Bakagianni et al., 2024).

Juli Bakagianni
University of Ioannina, Greece

Maria Gavriilidou
ILSP/Athena RC, Greece

NLP for lesser-resourced languages is further
hindered due to the scarcity of open-access, high-
quality language resources. Such resources in-
clude datasets, which could be used to train and/or
evaluate multilingual models on downstream tasks
across languages or language models per se. Com-
prehensive NLP surveys for resource-lean lan-
guages, besides providing evidence on the digi-
tal readiness of the language, can also serve as
a sound basis to promote NLP research for these
languages, and to lead to available language re-
sources for downstream tasks, opening the way to
new benchmarks. Our work focuses on one of these
languages, Greek,! the official language of Greece
and one of the two official languages of Cyprus.

In this work, based on the recent Greek NLP
survey of Bakagianni et al. (2024), we extracted
publicly available and open-access Greek datasets
that permit derivatives, aggregating them into a uni-
fied collection that is easy to re-compile and use.”
Using this collection, we make three contributions:

¢ We benchmarked seven NLP tasks in Greek
using a closed- (GPT-40 mini) and an open-
source (Llama-70b) LLM, revealing task-
specific strengths and weaknesses. LLAMA is
better in Named Entity Recognition (NER)
and Summarization while GPT is better in
Grammatical Error Correction (GEC), Ma-
chine Translation (MT; el-jpn), Intent Clas-
sification and Part-of-Speech (POS) Tagging.
The two models perform on par on Toxicity
Detection, MT (el-en, el-fa).

¢ We reframe Authorship Attribution by eval-
uating LLMs in a 0-shot setting and hypoth-
esising that high accuracy suggests potential
inclusion of the authored texts in pre-training

‘Tn our study, with the term Greek we refer only to Modern
Greek, excluding previous stages of the language’s history.

°The code is available at: https: //github.com/
greek-nlp/benchmark


data. In lesser-resourced languages, which are
not well supported in the training data, our
hypothesis is even likelier.

We introduce the first Text Clustering bench-
mark for Greek legal texts, demonstrating its
value in organizing long, complex documents.
Using the Summarize, Translate, and Embed
(STE) methodology, we show improved clus-
tering performance over a traditional baseline,
offering a practical solution for navigating le-
gal corpora in lesser-resourced settings.

2 The data

Based on the systematic literature review of Bak-
agianni et al. (2024), covering 142 research NLP
studies for Greek from 2012 to 2023, we extracted
and analysed information on the 94 available Greek
datasets. Most of these resources were monolin-
gual (84%), but bilingual and multilingual datasets
were also present (i.e., with Greek being one of
the languages). The most frequent domains were
politics and news (20%) while the most frequent
task was sentiment analysis (33%).

Lmt, with technical steps
Err, n/a

Lmt, n/a

Lmt, upon request

Err, HTTP Error

Availability-Accessibility

Yes, downloadable

Lmt, downloadable

No, n/a

te) 5 10 15 20 25 30 35

Figure 1: Availability and accessibility of extracted
datasets, one tuple per bar. Availability is classified to
open (yes), constrained (Imt), hindered by an error (err),
or not available via a URL (no). Accessibility reflects
the outcome we observed when accessing the resource.

2.1 Availability, Popularity and Licensing

Figure 1 shows the availability status of the ex-
tracted datasets. The most common status is that
the datasets are not available, meaning no informa-
tion is provided regarding their availability. The
second most frequent category concerns datasets
reported as downloadable, but actually not publicly
available due to restrictions, such as missing license
or subscription limitations. This limitation contra-
dicts the principles and benefits of open access.
Studies with publicly available datasets (serving as

Yes No Err Lmt

Availability

Figure 2: Whisker and box plot with the number of
citations of studies per availability type.

a reference point) can attract more citations, as is
shown by the higher third and fourth quartile of the
green box in Figure 2. Historically (Figure 3), stud-
ies attracting citations while presenting data with
limited (Juola and Stamatatos, 2013), or with no
availability Giatsoglou et al. (2017), belong to the
past. The study of Juola and Stamatatos (2013), for

Availability
© No

© Er
© Lmt
Yes

_-- 25 -e

2012 2013.—=2/«2014—S 20152016 2019 2020. ©-2021«=S 2022S 2023

2017-2018
Publication Year

Figure 3: Citation counts per year of publication of
studies developing datasets, based on the availability
classified as yes, no, limited (Imt), or erroneous (err).

example, shown with a blue peak in 2013, concerns
a shared task on authorship identification (hence,
the citations) whose data lack a license (hence, the
limitation). Our work reveals an alternative dataset
for this task ($4.2).

2.2. Distilling FAIR data

We extracted findable, accessible, interoperable
and re-usable (FAIR) datasets from the survey of
Bakagianni et al. (2024). We focused on the 14
Greek annotated datasets that were found as li-
censed, accessible, machine-actionable, and with
no hidden costs. We excluded the dataset of Fit-
silis and Mikros (2021), which is licensed under
CC BY-NC-ND 4.0 and does not allow derivatives
(i.e., a limitation of re-usability). Also, Korre et al.
(2021) provided two datasets, one of which is too
small in size (100 sentences). By disregarding these
datasets, we end up with the 12 datasets presented
in Table 1. There is a variety of domains and the
largest dataset is that of Barzokas et al. (2020) fol-
lowed by that of Dritsa et al. (2022).


AUTHORS AT HOST SIZE SIZE UNIT DOMAIN LICENSE
Papantoniou et al. (2023) A Zenodo 18,615 Doc EVENTS Cc BY 3.0
Koniaris et al. (2023) C Hugging Face 8,395 DOC LEGAL CC BY-NC 4.0
Rizou et al. (2023) M  msensis.com 2,176 SENT ADMIN CC-BY-NC-SA 4.0
Dritsa et al. (2022) A Zenodo 1.28M DOC POLITIC cc BY 4.0
Papaloukas et al. (2021) C Hugging Face 47,563 DOC LEGAL cc BY 4.0

Korre et al. (2021) M_ GitHub 227 SENT ESSAYS MIT

Zampieri et al. (2020) M_ Hugging Face 10,287 TWEET GENERAL cc BY 4.0
Bartziokas et al. (2020) H GitHub 21,153 SENT NEWS CC-BY-NC-SA 4.0
Prokopidis and Piperidis (2020) N  olp.ilsp.gr 101,857 Doc WEB CC-BY-NC-SA 4.0
Barzokas et al. (2020) C GitHub 1,734 DOC E-BOOKS MIT

Prokopidis and Papageorgiou (2017) H GitHub 2,521, SENT NEWS, POLITIC CC BY-NC-SA 3.0
Prokopidis et al. (2016) U — olp.ilsp.gr 17,018 DOC PAIR NEWS cc BY 4.0

Table 1: Greek datasets with their annotation type (AT) defined as manual (M), automatic (A), curated (C; metadata
provided by the distributor), hybrid (H; manual and automatic), user-generated (U; from user-edits, not curated) or
no (N). We include also the host, size, size unit, domain and license of each dataset.

MIN MAX AVG _ SIZE (#)
Papantoniou et al. (2023) 34 1,484 172.9 18,615
Prokopidis and Piperidis (2020) 36 67,963 2,430 101,857
Dritsa et al. (2022) 1 291,949 900 1,280,927

Table 2: Greek FAIR datasets appropriate for pre-
training tasks with min, max, average length in char-
acters and size in documents.

2.3 Datasets with no supervision signal

Table 1 comprises three datasets with no ground
truth or whose ground truth is automatically ex-
tracted (A/N; i.e., of doubted quality). Although
excluded from our benchmarking, these datasets
may still serve pre-training purposes and are further
discussed next. Dritsa et al. (2022) gathered 1.28M
political speeches spanning from July 1989 to July
2020. Papantoniou et al. (2023) conducted NER
and entity linking on a dataset derived from Greek
Wikipedia event pages. Prokopidis and Piperidis
(2020) provided 101,857 files, which comprise the
cleaned version of websites containing open con-
tent, including online archives of Greek newspapers
from 2003 to 2020, and the Greek part of the W2C
corpus (Majlis and Zabokrtsky, 2012).

2.3.1 Dataset statistics

As is shown in Table 2, the text length (measured
in characters) greatly varies between the datasets.
The dataset with the shortest texts on average is
that of Papantoniou et al. (2023) and that with the
lengthiest is that of Prokopidis and Piperidis (2020).
The dataset size (counted in number of files) also
varies, starting from ten thousands (Papantoniou
et al., 2023), to one hundred thousands (Prokopidis
and Piperidis, 2020), to more than a million (Dritsa
et al., 2022). The aggregated text totals up to 1.4
billion characters and 211 million tokens (using

5.0

3.65+0.75

prokopidis

45

-4.0

dritsa

4.7 + 1.08 2.61 +4 1.04 5,15 0.95

LM trained on
BPC

3.5

3.79 + 0.64

4.6 + 0.53 2.85 + 0.76

papantoniou

dritsa
Unseen text from

prokopidis papantoniou

Figure 4: Heatmap of BPC measured on a sample per
dataset (average across the sample; shown horizontally)
per trained LM (vertically). Distance in warm colours.

white space split).

2.3.2 Linguistic distance

We investigate the linguistic associations between
these datasets, using statistical language modelling
(LMing) as a proxy. For each dataset, we trained
one Statistical character-level LM on the first 100
characters of 1,000 randomly selected texts per
dataset.> Then, we computed the Bits per Charac-
ters (BPC) across 500 randomly selected texts per
dataset, reporting the mean. The result is displayed
as a heatmap in Figure 4. High scores (in red) re-
flecting a greater linguistic distance between the
dataset the LM is trained on and the one BPC is
computed on. Low scores (in blue) reflect a low lin-
guistic distance, with the lowest scores taking place
in the diagonal, when predicting unseen text taken

3Character-level statistical LMing is an efficient way to
capture stylistic features, employed in authorship analysis
(Pavlopoulos and Konstantinidou, 2023).


from the same dataset (self-distance). Based on
this heatmap, we find that the dataset of Prokopidis
and Piperidis (2020) shows a higher self-distance
compared to the rest while the datasets of Dritsa
et al. (2022) and Papantoniou et al. (2023) are far
away from each other, especially when the LM is
trained on the former and predicting the latter.

2.4 Datasets with supervision signal

The datasets that can be used as benchmarks for
downstream tasks in Greek are presented in Table 3.
They cover supervised tasks (i.e., Intent, Toxicity,
Authorship) and unsupervised tasks (i.e., Cluster-
ing) learning; text-to-text tasks (i.e., MT, GEC,
Summarization); and sequence learning tasks (i.e.,
NER, POS tagging). The task with the lengthiest
texts is Clustering and the one with the shortest
is POS tagging. We chose Clustering over legal
text classification because the dataset serves as a
valuable resource for Clustering, a task currently
lacking a benchmark. The dataset in this case has
47 clusters (classes), and it also comprises more
fine-grained labels, increasing this number to 374
and 1,685. The task with the highest number of
classes besides Clustering is NER (18), followed by
Authorship (17) and POS tagging (16). Intent clas-
sification is the only balanced task (CIR = 1.0).

3 The Greek NLP Benchmark

We used licensed, accessible, machine-actionable,
and with no hidden costs datasets, summarised in
Table 3, to set up a benchmark per task. The ground
truth is derived either by human annotators or by
metadata sourced from the distributors. We exper-
imented with 0-shot learning using LLAMA (70b-
instruction)* and GPT (40-mini).> We used the
test splits, when they were available,° and we ran-
domly sampled 175 instances per test set otherwise,
mainly to limit the cost for GPT.

3.1 Toxicity Detection

The dataset In 2020, a subtask of offensive lan-
guage identification for Greek was introduced as
part of the SemEval-2020 Task 12 on Multilingual
Offensive Language Identification in Social Media

4We also experimented with a 4-bit quantized version of
Meltemi, a Mistral-7B Instruct model that was continually
pre-trained on Greek (Voukoutis et al., 2024), but the model
failed to understand the task at hand (i.e., Toxicity and GEC).

>In the sequence learning tasks of POS tagging and NER,
each class/tag was provided along with explanations.

We created and provide our own splits, where these were
not available; marked with a dagger in Table 3.

(OffensEval-2020)’ (Zampieri et al., 2020). The
task focused on four other languages besides Greek:
Danish, English, Turkish, and Arabic. Overall, 145
teams submitted official runs on the test data, 37
of which made an official submission on Greek,
while the submissions for English were approx. the
double, i.e. 81. The Greek dataset used for the
SemEval subtask is an extended version of the Of-
fensive Greek Tweet Dataset (OGTD), which was
developed by Pitenis et al. (2020). As can be seen
in Table 3, texts in this dataset are relatively short
and there is only slight class imbalance.

The results Table 4 shows that LLAMA is better
in F1 for the better-supported NOT TOXIC class
(due to its better recall) and worse for the mi-
nor TOXIC class. Hence, GPT appears superior
in macro-averaged F1 but the two are equal in per-
formance in weighted-F1.

3.2. Grammatical Error Correction

The dataset This task concerns the correction
of grammatical errors that vary from grammatical
mistakes to punctuation, spelling, and morphol-
ogy of word. Korre et al. (2021) listed 18 main
categories of grammatical errors that systems can
correct. They developed two datasets, of which
we consider only the one that is annotated by hu-
man experts, i.e., the Greek Native Corpus (GNC).
This corpus is comprised of essays written by stu-
dents who are native speakers of Greek, totalling
227 sentences. Each sentence within this dataset
may contain zero, one, or multiple grammatical
errors, all annotated by human experts with the cor-
responding grammatical error types as defined in
the provided annotation schema.

The results Using character (CER) and word
(WER) error rate, we find that GPT performs sig-
nificantly better than LLAMA in correcting gram-
matical errors. It is halved in WER compared to
LLAMA and only 1.74 in CER. Unless GPT has
already used the data of Korre et al. (2021) during
training, a possibility we cannot exclude for this
dataset, this is a very low rate. To gain a better in-
sight, we experimented by also adding two shots in
the prompts. Although the performance of LLAMA
halved, that of GPT remained unchanged.

Thttps ://sites.google.com/site/
of fensevalsharedtask/offenseval-2020


TASK DATASET MIN MAX AVG TESTSIZE UNIT TASK CLASSES# = CIR
TOXICITY Zampieri et al. (2019) 8 289 107 1,544 sent CLF 2 0.36
AUTHORSHIP Barzokas et al. (2020) 1,000, 1,000 1,000 179+ text CLF 17 0.18
INTENT Rizou et al. (2023) a 174 59 436 t sent CLF 6 1.0
CLUSTERING Papaloukas et al. (2021) 17 931,736 42 9,516 text CLF 47 0.07
GEC Korre et al. (2021) 19 482 134 22] sent T2T —- -
MT Prokopidis et al. (2016) 2 976 132 1,500t doc T2T - -
SUMMARIZATION Koniaris et al. (2023) 2,571 6,983 4,899 1,238 text T2T - -
NER Bartziokas et al. (2020) 2 2,210 175 299 + sent SL 4/18 0.02
POS Prokopidis and Papageorgiou (2017) 1 96 24 456 sent SL 16 0.03

Table 3: Statistics (counting in chars) for datasets related to downstream NLP tasks in Greek: classification (CLF),
sequence labelling (SL), text to text (T2T). When applicable, the class imbalance ratio (CIR: the smallest to the
largest class) is reported. Datasets split by us are marked with a dagger.

F1-Score Support
LLAMA GPT
NOT TOXIC 0.82 0.80 148
TOXIC 0.37 0.50 27
Macro Avg 0.60 0.65 175
Weighted Avg 0.75 0.75 175

Table 4: Fl in Toxicity using (0-shot) LLAMA and GPT.
Precision and Recall shown in the Appendix (Table 13).

LLAMA -2s | GPT -2s
WER 14.51 (-38%) | 7.55  (-1%)
CER 8.69  (-46%) | 1.74 (418%)

Table 5: Word (WER) and character (CER) error rate of
LLAMA and GPT using 0-shot learning and the percent-
age change when adding 2 (static) shots.

3.3. Machine Translation

The dataset Prokopidis et al. (2016) created
bilingual corpora (756 language pairs) from con-
tent available in Global Voices, where volunteers
translate news stories in 41 languages. The 3,629
Greek documents are translated into 40 languages.
However, not every document is translated into
all languages, resulting in 17,018 bilingual docu-
ment pairs involving Greek. In this work, we fo-
cus on three Greek language pairs: Greek-English,
Greek-Japanese, and Greek-Farsi. These target
languages were selected as the most supported
languages within their respective language sup-
port tiers, as defined in Bakagianni et al. (2024).
The tiers are based on the number of ACL An-
thology studies (2012-2024) referencing (in their
titles/abstracts) each respective language.* Well-
supported languages are referenced in more than
1,000 studies; moderately-supported languages are
referenced in 100 to 1,000; less-supported ones
are referenced in less than 100. Based on this
classification, the English language represents the

8s listed in the IETF BCP 47 standard.

well-supported tier (6,915 studies), Japanese repre-
sents the moderately-supported tier (808 studies)
and Farsi the less-supported tier (98 studies). We
use this dataset to benchmark machine translation,
experimenting with Greek as the source language.

The results Table 6 presents the BERTScore
Fl (Zhang et al., 2019) per model, along with
CER and WER. By translating to English, a well-

LLAMA GPT
EL-EN: Greek to English
WER 65.53 (41.60) 53.13 (33.09)
CER 48.68 (31.91) 36.63 (24.25)
BERTScore F1 0.80 0.81

EL-JPN: Greek to Japanese

WER 267.57 (541.60) 109.05 (41.13)
CER 153.82 (194.99) 129.69 (186.95)
BERTScore F1 0.49 0.54
EL-FA: Greek to Farsi

WER 92.67 (38.83) 88.81 (34.41)
CER 71.01 (31.07) 67.00 (32.70)
BERTScore F1 0.54 0.54

Table 6: The average (standard deviation) error rate
(WER, CER) is computed across the three source-target
language pairs, along with the BERTScore Fl. We use
LLAMA and GPT 0-shot learning, and we report the
best score achieved when multiple gold translations are
available.

supported language, we report the best results
across all metrics. By translating to Farsi, a less-
supported language, we observe the worst per-
formance, slightly below those for translations
to Japanese, a moderately-supported language.
Across all languages, GPT demonstrates a lower
translation error rate, in terms of WER and CER,
compared to LLAMA, but its (BERTScore) F1 is
only higher for Japanese. For English and Farsi,
both models achieve similar BERTScore F1. This
suggests that while LLAMA produces translations


of comparable quality, it tends to use different
wording than the ground truth. Overall, we ob-
serve that the translation quality from Greek to
the selected languages reflects their classification
within the language support tiers.

3.4 Summarization

The dataset Koniaris et al. (2023) created a le-
gal corpus of 8,395 court decisions from Areios
Pagos, the Supreme Civil and Criminal Court of
Greece. This corpus includes the decisions, their
summaries, and related metadata, all sourced from
the Areios Pagos website.’ Using the (provided)
test set of 1,238 Greek legal texts, we find an av-
erage text length of 18,541 characters and a stan-
dard deviation of 18,351 (min: 2,571 and max:
303,538). When tokenised (white-space split), this
is 381 (min) to 46,232 (max) tokens. We only kept
texts of 1,000 tokens or fewer, to limit the time and
cost of our experiments. This totals 192 texts of
4,899 characters on average with a standard devia-
tion of 1,319 characters (max: 6,983). To remain
consistent with the benchmarking of other NLP
tasks, we sampled 175 texts for the experiments.

The results In addition to BERTScore (Zhang
et al., 2019), we also report variants of ROUGE
(Lin, 2004). As is shown in Table 7, LLAMA out-
performs GPT across most metrics for this task,
demonstrating stronger overall performance in gen-
erating high-quality summaries. However, GPT
slightly surpasses LLAMA in recall, as measured
by ROUGE (across variants), suggesting that it bet-
ter captures content overlap, though at the expense
of precision.

Metric LLAMA GPT

BERTScore 0.517+0.108 0.509+0.101
ROUGE-1 0.192 + 0.089 0.186+0.076
ROUGE-2 0.048 + 0.055 0.039 + 0.037
ROUGE-L 0.167 + 0.080 0.164+0.066

Table 7: Mean (+SD) BERTScore and ROUGE F1 of
LLAMA and GPT. More metrics are shown in Table 14.

3.5 Intent Classification

The dataset Rizou et al. (2023) collected student
queries to two University help desks and manually
annotated each of these with three entity tags and
six intents. The dataset is balanced regarding the
intents. The average text length in characters is
58.6, the maximum is 174 (minimum of 7), and the
standard deviation is 30.3.

https ://www.areiospagos.gr/

The results As shown in Table 8, GPT is overall
(F1 of 0.93) and consistently outperforms LLAMA
(F1 of 0.70). LLAMA faces more challenges with
Recall than with Precision. In certain classes, the
Recall score for LLAMA was as low as 0.24, which
significantly reduces its macro-average to 0.72.

Intent LLAMA’ GPT Support
AVAILABLECOURSESB YEXAMPERIOD 0.86 0.91 30
COEFFICIENTSB YCOURSENAME 0.39 0.98 29
GRADEB YCOURSENAME 0.85 0.93 29
PASSEDCOURSESB YEXAMPERIOD 0.80 0.93 29
TEACHERINFOBYNAME 0.69 0.92 29
TEACHERNAMEB YCOURSENAME 0.60 0.89 29
Macro Avg 0.70 0.93 175
Weighted Avg 0.70 0.93 175

Table 8: F1 in intent using (0-shot) LLAMA and GPT.
Precision and Recall shown in the Appendix (Table 15).

3.6 NER

The dataset Bartziokas et al. (2020) provided
an annotated dataset with two levels of granular-
ity for entity annotation. The first level uses 4
label tags akin to the CONLL-2003 dataset (Sang
and De Meulder, 2003), while the second incor-
porates 18 entity tags, as in the OntoNotes 5 En-
glish dataset (Pradhan et al., 2007). The dataset
was developed during a Google Summer of Code
project in 2018,'°, where initial automatic annota-
tion was followed by manual curation. The text
length varies from 2 to 2,210 characters (Table 3).
Both annotation levels exhibit heavy imbalance,
with the four-tag level having an imbalance of 0.02
and the 18-tag level showing near-zero imbalance.
We opted for the four-entity-tag level of annotation
with the lower imbalance.

The results Table 9 shows that LLAMA achieves
a better Fl score overall compared to GPT. How-
ever, the low macro-average scores indicate that
both models fail to capture specific entity classes,
such as E-MISC and S-MISC (see Appendix (Ta-
ble 16)). Both models perform well in detecting the
O class (i.e., not an entity), yielding an F1 of 0.92,
which means that both models can detect entities.
The low macro average scores, on the other hand,
reflect how both models struggle to distinguish be-
tween the different types of entities.

3.7. POS Tagging

The dataset Prokopidis and Papageorgiou (2017)
provided the Greek treebank as part of a project that

https ://github.com/eellak/gsoc2018-spacy


Metric LLAMA’ GPT Support
Macro Avg 0.14 0.14 5447
Weighted Avg 0.85 0.84 5447

Table 9: Fl in NER using (0-shot) LLAMA and GPT.
Results per entity tag shown in the Appendix (Table 16).
offers standardized treebanks with consistent anno-
tations across languages (Nivre et al., 2016). The
dataset includes syntactic dependencies, POS tags,
morphological features, and lemmas. It contains
2,521 sentences split into train (1,622), develop-
ment (403), and test (456) sets, and was manually
validated and corrected.

The results Table 10 shows that GPT is consis-
tently better than LLAMA. For the POS tags DET
(Determiner) and VERB (Verb), LLAMA achieves
higher recall, meaning it identifies more instances
of these tags (see Appendix (Table 17)). However,
GPT has a better overall F1 score, indicating a bet-
ter balance between precision and recall. Neither
model is able to correctly handle unknown or miss-
ing tags (represented as _) or other foreign words,
abbreviations, etc. (denoted as X).

Metric LLAMA’ GPT Support
Macro Avg 0.35 0.47 4131
Weighted Avg 0.48 0.60 4131

Table 10: F1 in POS tagging using (0-shot) LLAMA and
GPT. Results per tag shown in the Appendix (Table 17).

4 Novel Benchmark Tasks for Greek

4.1 Long Legal Text Clustering

The dataset Papaloukas et al. (2021) developed
a dataset for multi-class legal topic classification,
derived from a collection of Greek legislative doc-
uments titled “Permanent Greek Legislation Code
- Raptarchis”. The dataset includes classifications
that range from broader categories to more special-
ized ones. There are annotations at the volume,
the chapter and the subject levels. This is a heav-
ily imbalanced dataset, and the text lengths vary
significantly, from 17 to approx. 1 million charac-
ters. The high number of classes in this task make
in-context learning approaches impractical while
our benchmark already covers text classification
as a task (Table 3). Therefore, we use this dataset
to benchmark LLMs for text clustering, a task of
unsupervised learning that is currently missing for
Greek data in literature. We employ the entire test
set of 38,052 texts for our experiments, comprising
47 volumes, 374 chapters, and 1,685 subjects.

The results The direct use of LLM models for
this task is prohibitive due to the substantial com-
putational cost and processing time required for
handling the long legal texts (Table 3). Therefore,
we opted for K-Means with TF-IDF features.'! Ad-
ditionally, we used LLAMA for summarization (the
best-performing model for this task; see $3.4) and
for translation into English (performing on par with
GPT; see §3.3). This allowed us to compute Instruc-
tor embeddings (Su et al., 2022), which we refer
to as STE (Summarized, Translated, Embedded),
as an alternative to TF-IDF for text representation.
We set the number of clusters, k, according to the
ground truth number of topics, i.e., 47 for the vol-
ume, 374 for the chapter, and 1,685 for the subject.
We evaluated the results based on the normalised
mutual information score (NMI), the adjusted mu-
tual information score (AMI), and the adjusted rand
index (ARI). All three measures use the ground
truth labels to assess the clustering solution while
being independent of the absolute label values. As
shown in Table 11, STE (2nd row per level) gives
the best results overall, and the best ones across
metrics consistently in two levels. When ablating
the embedding step, computing the TF-IDF fea-
tures of the English summaries (3rd row per level),
we see that the results deteriorate. This means that
information is lost during translation and summa-
rization. The superiority of the Instructor embed-
dings, however, make up for this drop.

k REP LAN’ LEN METRICS

NMI AMI ACC

TF-IDF Gr Full 0.161 0.133 0.124

47 Dense En Sum 0.244 0.219 0.194
TF-IDF En Sum _ 0.136 0.121 0.129

TF-IDF Gr Full 0.479 0.194 0.082

374 Dense En Sum 0.524 0.225 0.175
TF-IDF En Sum _ 0.321 0.156 0.118

TF-IDF Gr Full 0.723 0.202 0.057

1,685 Dense En Sum 0.725 0.162 0.246
TF-IDF En Sum _ 0.510 0.111 0.162

Table 11: Clustering evaluation for long legal Greek doc-
uments (Papaloukas et al., 2021), comparing KMeans
and STE (Dense-En-Sum) at three granularity levels
(k = 47, k = 374, k = 1, 685).

4.2 Authorship Attribution

The dataset Barzokas et al. (2020) collected
1,734 open-access e-books from Project Guten-
berg,'* and from Open Library. Due to the con-

‘We used scikit-learn with default parameters.
https ://www. gutenberg.org/


version from various format to plain text, the 1,105
e-books have content in plain text that corresponds
to the original file content. For our experiments,
we filtered out books with more than 1,000 tokens
(the number of tokens was reported in the dataset).
From the remaining 985 books, to yield a test set
(of 175 instances, as in other tasks), we selected
books from authors with many books, resulting in
175 books from 17 authors. For each book, we used
1,000-character excerpts taken from the middle of
the text to detect the author of a given passage.

Theresults Table ?? shows the results of LLAMA
and GPT, with the latter performing overall better.
LLAMA outperforms GPT for three authors - Tzou-
valis, Amanatidou, and Aeschylus - in terms of
Fl-score, with both models achieving perfect Pre-
cision for these authors, suggesting prior exposure
to their works during the pre-training phase. Sim-
ilar indications arise for Papavasileiou and Plato,
who achieve the best results for both models. How-
ever, GPT achieved a zero Fl-score for Voulazeris,
Kritsotaki, Semertzidou, Fourouklas, and Kappas,
indicating that data leakage or contamination is
unlikely for these authors, as the models fail to cor-
rectly classify their works. LLAMA also achieved
a zero Fl-score for these authors, as well as for
Koliopoulos, Andamis, and Zervas.

Author LLAMA GPT Support
Thanasis Triaridis 0.32 0.34 28
Rania Synodinou 0.11 0.41 12

0.00 0.00 17
0.47 0.33 13
0.40 0.46 7
0.00 0.00 5)
0.00 0.10 17
0.00 0.06 23

Kostas Voulazeris
Dimitris Tzouvalis
Evridiki Amanatidou
Frinta Kritsotaki
Panos Koliopoulos
Yiannis Andamis

Giorgos S. Kokkinos 0.29 0.42 8
Manos Kounougakis 0.25 0.33 5)
Eleni Semertzidou 0.00 0.00 5
Panos A. Zervas 0.00 0.15 7
Lakis Fourouklas 0.00 0.00 )
Vasileios Kappas 0.00 0.00 5
Paschalis Papavasileiou 0.80 0.89 5
Plato 0.75 0.82 8
Aeschylus 0.57 0.33 5
Macro Avg 0.23 0.27 175
Weighted Avg 0.20 0.25 175

Table 12: Fl in Authorship with (0-shot) LLAMA and
GPT. Precision and Recall in Table 18 of the Appendix.

5 Related work
Our study provides a unified cross-task Greek NLP

benchmark, based on pre-existing datasets. Our
work is not the first to reveal existing datasets.

(Nikiforos et al., 2021), for example, reviewed
methods and datasets related to the Greek so-
cial web. Alexandridis et al. (2021) focused on
sentiment analysis in Greek social media while
Krasadakis et al. (2022) surveyed NLP studies re-
lated to legislative and Greek documents. These
studies, however, have not assessed the employed
datasets regarding their licensing, public availabil-
ity, and (machine) actionability, which hinders the
impact of existing benchmarks. We provide an
extensive benchmark on publicly available and ac-
tionable Greek NLP data, hence filling this gap.

Overviews that focus on Greek NLP, such as the
work of Papantoniou and Tzitzikas (2020, 2024),
cover a very wide period of study (e.g., comprising
Ancient Greek), and lack classification for licens-
ing and actionability of the resources. The system-
atic literature review of Bakagianni et al. (2024)
followed that path and provided an exhaustive list
of publicly available and actionable datasets. Our
work was based on that study, providing code to
compile a unified collection of appropriate datasets,
to ease usability and re-production of the results.

Data leakage and contamination is a challenge
for the NLP community (Balloccu et al., 2024),
which recent benchmarks employing LLMs at-
tempt to avoid (White et al., 2024). Benchmarks
can assist towards that path, by providing spe-
cific evaluation sets and documentation. Although
benchmarks exist for Greek dialects (Faisal et al.,
2024) and Ancient Greek (Stopponi et al., 2024),
this is the first benchmark across Greek NLP tasks
using publicly available and actionable data.

6 Conclusions

Based on a monolingual survey, we compiled a col-
lection of publicly available and accessible Greek
datasets, based on their licensing schemas. We used
this collection to benchmark an open- (LLAMA)
v. a closed-source (GPT) LLM on seven core
NLP tasks, showing that the former best performs
in NER and Summarisation while the latter best-
performs in POS tagging, Intent Classification and
GEC. We further observe and address two weak-
nesses in Greek NLP. First, the available data for au-
thorship attribution are of limited availability. We
tackled this by introducing an alternative dataset
and by using 0-shot learning. High accuracy in this
task could indicate usage of the respective mate-
rial during pre-training. Second, there is no text
clustering benchmark, which we addressed by in-


troducing the first long legal text clustering bench-
mark. By summarising, translating and embedding
we showed, then, that better representations can be
resulted compared to TF-IDF representations.

Acknowledgments

This work has been partially supported by project
MIS 5154714 of the National Recovery and Re-
silience Plan Greece 2.0 funded by the European
Union under the NextGenerationEU Program.

7 Limitations

We compiled a collection of existing datasets,
providing the code to re-compile our collection.
Dataset developers, however, may update their
datasets, which may yield slightly altered results in
one or more tasks. To reflect any such changes, we
will regularly re-compile the collection and re-run
our benchmark, reporting the results over time in
our repository online. A limitation of our work lies
in the sustainability of our code, to incorporate new
repositories and adjust to changes in the current
one. This is addressed by unit tests set to download
periodically the data and evaluate the functionality.
We also note that our method is applicable to other
languages, if a systematic literature review for that
language exists.

8 Ethical considerations

Greek NLP datasets present a range of legal and
licensing challenges primarily related to copyright,
permissions, and data protection, crucial for com-
pliant and ethical research use.

Copyright The dataset of Papantoniou et al.
(2023), for example, relies on Greek Wikipedia,
which may include third-party contributions un-
der different licensing terms. This could lead to
potential copyright issues if the dataset includes
trademarked names or logos, not covered under the
CC license and requiring separate permissions. Bar-
zokas et al. (2020) created a dataset using content
from Project Gutenberg and Open Library under
an MIT license, but content of the former may not
be free of copyright in other countries. Also, Open
Library restricts its content use to non-commercial
purposes and for research only, which may conflict
with the broad permissions of the MIT license if
the content is used beyond these limits.

Re-identification The judicial rulings of Ko-
niaris et al. (2023) may include copyrighted an-

notations by court staff, posing a risk of re-
identification. Similarly, in the student essays of
Rizou et al. (2023) there could be potential re-
identification risks of student data (even if they are
fully anonymised) while in the handwritten essays
from high school students of Korre et al. (2021)
there may be concerns about obtaining proper con-
sent from students or their guardians for dataset in-
clusion. On the other hand, the data of Bartziokas
et al. (2020) may include named entities tied to
individuals and there is potential for privacy viola-
tions.

Conflicting terms Derived from the Permanent
Greek Legislation Code - Raptarchis, the dataset of
Papaloukas et al. (2021) could encounter legal con-
cerns due to the Ministry of Digital Governance re-
taining all intellectual property rights. The dataset
of Dritsa et al. (2022) sourced from the Hellenic
Parliament, conflicts with its public terms, which
require proper attribution without alterations (e.g.,
3rd party content). Zampieri et al. (2020) applied
a CC-BY-4.0 license to tweets, presenting legal
challenges due to Twitter’s restrictive policies on
content redistribution. Also, regarding the data
of Prokopidis and Piperidis (2020), while derived
from web content labeled as “open”, the terms of
the original websites must explicitly allow crawling
and reuse to avoid potential copyright infringement.

References

Georgios Alexandridis, Iraklis Varlamis, Konstantinos
Korovesis, George Caridakis, and Panagiotis Tsanti-
las. 2021. A survey on sentiment analysis and opin-
ion mining in greek social media. Information, 12(8).

Juli Bakagianni, Kanella Pouli, Maria Gavriilidou, and
John Pavlopoulos. 2024. Towards systematic mono-
lingual nlp surveys: Gena of greek nlp. arXiv
preprint arXiv:2407.09861.

Simone Balloccu, Patricia Schmidtova, Mateusz Lango,
and Ondrej Dusek. 2024. Leak, Cheat, Repeat:
Data Contamination and Evaluation Malpractices in
Closed-Source LLMs. In Proceedings of the 18th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 67-93, St. Julian’s, Malta. Associa-
tion for Computational Linguistics.

Nikos Bartziokas, Thanassis Mavropoulos, and Constan-
tine Kotropoulos. 2020. Datasets and performance
metrics for greek named entity recognition. In //th
Hellenic Conference on Artificial Intelligence, SETN
2020, page 160-167, New York, NY, USA. Associa-
tion for Computing Machinery.


Vasileios Barzokas, Eirini Papagiannopoulou, and Grig-
orios Tsoumakas. 2020. Studying the evolution of
greek words via word embeddings. In //th Hellenic
Conference on Artificial Intelligence, SETN 2020,
page 118-124, New York, NY, USA. Association for
Computing Machinery.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing

systems, 33:1877-1901.

Konstantina Dritsa, Aikaterini Thoma, Ioannis
Pavlopoulos, and Panos Louridas. 2022. A greek
parliament proceedings dataset for computational
linguistics and political analysis. Advances in Neural
Information Processing Systems, 35:28874—28888.

Fahim Faisal, Orevaoghene Ahia, Aarohi Srivastava,
Kabir Ahuja, David Chiang, Yulia Tsvetkov, and
Antonios Anastasopoulos. 2024. Dialectbench: A
nlp benchmark for dialects, varieties, and closely-
related languages. arXiv preprint arXiv:2403.11009.

Fotios Fitsilis and George Mikros. 2021. Development
and validation of a corpus of written parliamentary
questions in the hellenic parliament. Journal of Open
Humanities Data, 7.

Maria Giatsoglou, Manolis G. Vozalis, Konstantinos
Diamantaras, Athena Vakali, George Sarigiannidis,
and Konstantinos Ch. Chatzisavvas. 2017. Sentiment
analysis leveraging emotions and word embeddings.
Expert Systems with Applications, 69:214-224.

Patrick Juola and Efstathios Stamatatos. 2013.
Overview of the author identification task at pan 2013.
CLEF (Working Notes), 1179.

Marios Koniaris, Dimitris Galanis, Eugenia Giannini,
and Panayiotis Tsanakas. 2023. Evaluation of auto-
matic legal text summarization techniques for greek
case law. Information, 14(4):250.

Katerina Korre, Marita Chatzipanagiotou, and John
Pavlopoulos. 2021. Elerrant: Automatic grammatical
error type classification for greek. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP 2021), pages
708-717.

Panteleimon Krasadakis, Evangelos Sakkopoulos, and
Vassilios S. Verykios. 2022. A natural language pro-
cessing survey on legislative and greek documents.
In 25th Pan-Hellenic Conference on Informatics, PCI
2021, page 407-412, New York, NY, USA. Associa-
tion for Computing Machinery.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74-81, Barcelona, Spain.
Association for Computational Linguistics.

Martin Majlis and Zdenék Zabokrtsky. 2012. Language
richness of the web. In Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC’ 12), pages 2927-2934.

Maria Nefeli Nikiforos, Yorghos Voutos, Anthi
Drougani, Phivos Mylonas, and Katia Lida Kermani-
dis. 2021. The modern greek language on the social
web: A survey of data sets and mining applications.
Data, 6:52.

Joakim Nivre, Marie-Catherine De Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
vl: A multilingual treebank collection. In Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC’16), pages
1659-1666.

Christos Papaloukas, Ilias Chalkidis, Konstantinos Athi-
naios, Despina Pantazi, and Manolis Koubarakis.
2021. Multi-granular legal topic classification on
Greek legislation. In Proceedings of the Natural
Legal Language Processing Workshop 2021, pages
63-75, Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.

Katerina Papantoniou, Vasilis Efthymiou, and Dimitris
Plexousakis. 2023. Automating benchmark genera-
tion for named entity recognition and entity linking.
In European Semantic Web Conference, pages 143-—
148. Springer.

Katerina Papantoniou and Yannis Tzitzikas. 2020. NLP
for the Greek language: a brief survey. In //th Hel-
lenic Conference on Artificial Intelligence, pages 101-
109.

Katerina Papantoniou and Yannis Tzitzikas. 2024. NIp
for the greek language: A longer survey. arXiv
preprint arXiv:2408.10962.

John Pavlopoulos and Maria Konstantinidou. 2023.
Computational authorship analysis of the homeric
poems. International Journal of Digital Humanities,
5(1):45-64.

Zeses Pitenis, Marcos Zampieri, and Tharindu Ranas-
inghe. 2020. Offensive language identification in
greek. CoRR, abs/2003.07459.

Sameer S Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007. Ontonotes: A unified relational semantic repre-
sentation. In International Conference on Semantic

Computing (ICSC 2007), pages 517-526. TEEE.

Prokopis Prokopidis and Harris Papageorgiou. 2017.
Universal dependencies for greek. In Proceedings of
the nodalida 2017 workshop on universal dependen-
cies (udw 2017), pages 102-106.

Prokopis Prokopidis, Vassilis Papavassiliou, and Stelios
Piperidis. 2016. Parallel Global Voices: a collection
of multilingual corpora with citizen media stories. In


Proceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC’ 16),
pages 900-905, Portoroz, Slovenia. European Lan-
guage Resources Association (ELRA).

Prokopis Prokopidis and Stelios Piperidis. 2020. A neu-
ral nlp toolkit for greek. In //th Hellenic Conference
on Artificial Intelligence, pages 125-128.

Sofia Rizou, Angelos Theofilatos, Antonia Paflioti,
Eleni Pissari, Iraklis Varlamis, George Sarigiannidis,
and K Ch Chatzisavvas. 2023. Efficient intent classi-
fication and entity recognition for university admin-
istrative services employing deep learning models.
Intelligent Systems with Applications, 19:200247.

Erik F Sang and Fien De Meulder. 2003. Introduction
to the conll-2003 shared task: Language-independent
named entity recognition. arXiv preprint cs/0306050.

Silvia Stopponi, Saskia Peels-Matthey, and Malvina
Nissim. 2024. Agree: a new benchmark for the
evaluation of distributional semantic models of an-
cient greek. Digital Scholarship in the Humanities,
39(1):373-392.

Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,
Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A
Smith, Luke Zettlemoyer, and Tao Yu. 2022. One
embedder, any task: Instruction-finetuned text em-
beddings. arXiv preprint arXiv:2212.09741.

Leon Voukoutis, Dimitris Roussis, Georgios
Paraskevopoulos, Sokratis Sofianopoulos, Prokopis
Prokopidis, Vassilis Papavasileiou, Athanasios
Katsamanis, Stelios Piperidis, and Vassilis Katsouros.
2024. Meltemi: The first open large language model
for greek. arXiv preprint arXiv:2407.20743.

Colin White, Samuel Dooley, Manley Roberts, Arka
Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv,
Neel Jain, Khalid Saifullah, Siddartha Naidu, et al.
2024. Livebench: A challenging, contamination-free
Ilm benchmark. arXiv preprint arXiv:2406.19314.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Sara Rosenthal, Noura Farra, and Ritesh Kumar.
2019. Predicting the type and target of of-
fensive posts in social media. arXiv preprint
arXiv: 1902.09666.

Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa
Atanasova, Georgi Karadzhov, Hamdy Mubarak,
Leon Derczynski, Zeses Pitenis, and Cagri Coltekin.
2020. Semeval-2020 task 12: Multilingual offensive
language identification in social media (offenseval
2020). CoRR, abs/2006.07235.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv: 1904.09675.

Appendix


Table 13: Fl in Toxicity Detection with 0-shot LLAMA and GPT.

Precision Recall F1-Score Support
LLAMA GPT LLAMA GPT LLAMA GPT

NOT TOXIC 0.90 0.98 0.76 0.67 0.82 0.80 148
TOXIC 0.29 0.34 0.52 0.93 0.37 0.50 27
Macro Avg 0.59 0.66 0.64 0.80 0.60 0.65 175
Weighted Avg 0.80 0.88 0.73 0.71 0.75 0.75 175

Metric LLAMA GPT

BERTScore F1 0.517+0.108 0.509+0.101

BERTScore Precision 0.441+0.093 0.435 + 0.087

BERTScore Recall 0.627 +0.132 0.616+0.126

ROUGE-1 F1 0.192 + 0.089 0.186+0.076

ROUGE-1 Precision 0.185+0.107 0.152 + 0.083

ROUGE-1 Recall 0.249+0.129 0.299+0.129

ROUGE-2 F1 0.048 + 0.055 0.039 + 0.037

ROUGE-? Precision 0.046 + 0.054 0.032+0.031

ROUGE-? Recall 0.069 +0.094 0.074 + 0.103

ROUGE-L FI 0.167 + 0.080 0.164 +0.066

ROUGE-L Precision 0.159 + 0.092 0.134+0.071

ROUGE-L Recall 0.221+0.129 0.269+0.125

Table 14: Mean (+ SD) BERTScore and ROUGE F1,

Precision, Recall of LLAMA and GPT.


Intent Precision Recall F1-Score Support

LLAMA GPT LLAMA GPT LLAMA ~ GPT

GETAVAILABLECOURSESBYEXAMPERIOD 0.75 0.83 1.00 1.00 0.86 0.91 30
GETCOEFFB YCOURSENAME 1.00 1.00 0.24 0.97 0.39 0.98 29
GETGRADEB YCOURSENAME 0.83 0.93 0.86 0.93 0.85 0.93 29
GETPASSEDCOURSESB YEXAMPERIOD 0.95 1.00 0.69 0.86 0.80 0.93 29
GETTEACHERINFOB YTEACHERNAME 0.55 0.90 0.93 0.93 0.69 0.92 29
GETTEACHERNAMEB YCOURSENAME 0.61 0.93 0.59 0.86 0.60 0.89 29
Macro Avg 0.78 0.93 0.72 0.93 0.70 0.93 175
Weighted Avg 0.78 0.93 0.72 0.93 0.70 0.93 175

Table 15: Evaluation of intent classification, comparing LLAMA and GPT with 0-shot learning.

NE Precision Recall F1-Score Support
LLAMA GPT LLAMA GPT LLAMA GPT

S-LOC 0.25 0.13 0.08 0.03 0.12 0.05 104
B-LOC 0.06 0.03 0.19 0.19 0.09 0.05 27
I-LOC 0.03 0.07 0.12 0.25 0.05 0.11 8
E-LOC 0.07 0.04 0.15 0.19 0.09 0.07 27
S-ORG 0.20 0.07 0.09 0.05 0.12 0.06 44
B-ORG 0.07 0.16 0.09 0.30 0.08 0.21 66
J1-ORG 0.08 0.14 0.09 0.23 0.08 0.17 35
E-ORG 0.13 0.17 0.12 0.23 0.12 0.20 66
S-PERSON 0.07 0.04 0.08 0.08 0.08 0.06 12
B-PERSON 0.14 0.03 0.18 0.07 0.16 0.05 28
J-PERSON 0.08 0.04 1.00 1.00 0.15 0.07 1
E-PERSON 0.22 0.26 0.21 0.39 0.22 0.31 28
S-MISC 0.00 0.00 0.00 0.00 0.00 0.00 12
B-MISC 0.03 0.00 0.07 0.00 0.04 0.00 14
I-MISC 0.03 0.00 0.06 0.00 0.04 0.00 17
E-MISC 0.00 0.00 0.00 0.00 0.00 0.00 14
O 0.93 0.95 0.91 0.89 0.92 0.92 4944
Macro Avg 0.14 0.13 0.20 0.23 0.14 0.14 5447

Weighted Avg 0.86 0.87 0.83 0.82 0.85 0.84 5447

Table 16: Evaluation of NER per entity, comparing LLAMA and GPT with 0-shot learning. The number of evaluation
instances per class is shown rightmost.

POS Precision Recall F1-Score Support
LLAMA GPT LLAMA’ GPT LLAMA GPT

ADV 0.45 0.57 0.29 0.54 0.35 0.55 164
PUNCT 0.57 0.70 0.55 0.67 0.56 0.69 406
DET 0.62 0.78 0.59 0.58 0.61 0.67 774
ADJ 0.57 0.67 0.46 0.58 0.51 0.62 357
NOUN 0.60 0.70 0.49 0.65 0.54 0.67 863
PRON 0.33 0.34 0.18 0.59 0.23 0.43 131
VERB 0.50 0.76 0.66 0.74 0.57 0.75 361
ADP 0.40 0.41 0.46 0.56 0.43 0.47 325
_ 0.05 0.08 0.01 0.01 0.02 0.02 98
CCONJ 0.51 0.64 0.56 0.64 0.53 0.64 137
NUM 0.24 0.36 0.57 0.63 0.34 0.46 54
PROPN 0.26 0.34 0.70 0.84 0.38 0.48 114
AUX 0.31 0.60 0.12 0.41 0.17 0.49 190
SCONJ 0.41 0.56 0.44 0.70 0.42 0.62 64
xX 0.00 0.00 0.00 0.00 0.00 0.00 64
PART 0.00 0.01 0.00 0.03 0.00 0.02 29
Macro Avg 0.36 0.47 0.38 0.51 0.35 0.47 4131

Weighted Avg 0.50 0.62 0.47 0.59 0.48 0.60 4131

Table 17: POS tagging per entity, comparing LLAMA and GPT with 0-shot learning.


Author Precision Recall F1-Score Support

LLAMA GPT LLAMA GPT LLAMA~ GPT

Thanasis Triaridis 0.22 0.23 0.57 0.61 0.32 0.34 28
Rania Synodinou 0.14 0.35 0.08 0.50 0.11 0.41 12
Kostas Voulazeris 0.00 0.00 0.00 0.00 0.00 0.00 17
Dimitris Tzouvalis 1.00 0.36 0.31 0.31 0.47 0.33 13
Evridiki Amanatidou 0.67 0.50 0.29 0.43 0.40 0.46 7
Frinta Kritsotaki 0.00 0.00 0.00 0.00 0.00 0.00 5
Panos Koliopoulos 0.00 0.33 0.00 0.06 0.00 0.10 17
Yiannis Andamis 0.00 0.12 0.00 0.04 0.00 0.06 23
Giorgos S. Kokkinos 0.18 0.31 0.75 0.62 0.29 0.42 8
Manos Kounougakis 0.33 1.00 0.20 0.20 0.25 0.33 5
Eleni Semertzidou 0.00 0.00 0.00 0.00 0.00 0.00 5
Panos A. Zervas 0.00 0.17 0.00 0.14 0.00 0.15 7
Lakis Fourouklas 0.00 0.00 0.00 0.00 0.00 0.00 5
Vasileios Kappas 0.00 0.00 0.00 0.00 0.00 0.00 5
Paschalis Papavasileiou 0.80 1.00 0.80 0.80 0.80 0.89 5
Plato 0.75 0.78 0.75 0.88 0.75 0.82 8
Aeschylus 1.00 1.00 0.40 0.20 0.57 0.33 5
Macro Avg 0.30 0.36 0.24 0.28 0.23 0.27 175
Weighted Avg 0.25 0.30 0.24 0.29 0.20 0.25 175

Table 18: Authorship Attribution evaluation, comparing LLAMA and GPT with 0-shot learning.
