arXiv:2109.01211v1 [cs.CL] 2 Sep 2021

Quantifying Reproducibility in NLP and ML

Anya Belz
ADAPT Centre
Dublin City University, Ireland
anya.belz@adaptcentre.ie

Abstract

Reproducibility has become an intensely de-
bated topic in NLP and ML over recent years,
but no commonly accepted way of assessing
reproducibility, let alone quantifying it, has so
far emerged. The assumption has been that
wider scientific reproducibility terminology
and definitions are not applicable to NLP/ML,
with the result that many different terms and
definitions have been proposed, some diamet-
rically opposed. In this paper, we test this
assumption, by taking the standard terminol-
ogy and definitions from metrology and apply-
ing them directly to NLP/ML. We find that we
are able to straightforwardly derive a practical
framework for assessing reproducibility which
has the desirable property of yielding a quanti-
fied degree of reproducibility that is compara-
ble across different reproduction studies.

1 Introduction

Reproducibility of results is coming under increas-
ing scrutiny in the machine learning (ML) and nat-
ural language processing (NLP) fields, against the
background of a perceived reproducibility crisis in
science more widely (Baker, 2016), and NLP/ML
specifically (Pedersen, 2008; Mieskes et al., 2019).
There have been several workshops and checklist
initiatives on the topic,! conferences are promot-
ing reproducibility via calls, chairs’ blogs,” and
special themes, and the first shared tasks are being
organised, including REPROLANG’20 (Branco
et al., 2020) and ReproGen’21 (Belz et al., 2020a).
The biggest impact so far has been that sharing

‘Reproducibility in ML Workshop at ICML’17, ICML’18
and ICLR’19; Reproducibility Challenge at ICLR’18,
ICLR’19, NeurIPS’ 19, and NeurIPS’20; reproducibility track
and shared task (Branco et al., 2020) at LREC’20; repro-
ducibility programme at NeurIPS’ 19 comprising a code sub-
mission policy, a reproducibility challenge for ML results, and
the ML Reproducibility checklist (Whitaker, 2017), later also
adopted by EMNLP’20 and AAAI’21.

*https://2020.emnlp.org/blog/
2020-05-20-reproducibility

of code, data and supplementary material provid-
ing increasingly detailed information about data,
systems and training regimes is now expected as
standard. Yet early optimism that "[r]eproducibility
would be quite easy to achieve in machine learning
simply by sharing the full code used for experi-
ments" (Sonnenburg et al., 2007) is now giving
way to the realisation that even with full resource
sharing and original author support the same scores
can often not be obtained (see Section 2.2).

Despite a growing body of research, no consen-
sus has so far emerged about standards, terminol-
ogy and definitions. Particularly for the two most
frequently used terms, reproducibility and replica-
bility, divergent definitions abound, variously con-
ditioned on same vs. different team, methods, arti-
facts, code, software, and data. E.g. for the ACM
(Association for Computing Machinery, 2020), re-
sults have been reproduced if obtained in a different
study by a different team using artifacts supplied
in part by the original authors, and replicated if
obtained in a different study by a different team
using artifacts not supplied by the original authors.
Drummond (2009) argues that what ML calls repro-
ducibility is in fact replicability which is the ability
to re-run an experiment in exactly the same way,
whereas true reproducibility is the ability to obtain
the same result by different means. For Rougier
et al. (2017), “/rJeproducing the result of a com-
putation means running the same software on the
same input data and obtaining the same results. [...].
Replicating a published result means writing and
then running new software based on the description
of a computational model or method provided in
the original publication”. Wieling et al. (2018) tie
reproducibility to “the same data and methods,’ and
Whitaker (2017), followed by Schloss (2018), tie
definitions of reproducibility, replicability, robust-
ness and generalisability to different combinations
of same vs. different data and code.

Underlying this diversity of definitions is the as-


Museum number
1991,0501.129

Description
Fragment of silver toro: of twelve strands
‘twisted in pairs and then coiled with buffer-
terminal.

Cultures/periods
tron Age

Production date
150 BC - 50 BC (circa)

eS SE 5 Findspot
J J Excavated/Findspot: Ken Hill (Hoard F)

Europe: British Isles: England: Norfolk
(England): Snettisham: Ken Hill

Figure 1: A torc fragment from the British Museum
and some of the information provided about it in the
museum’s collection catalogue® (image © The Trustees
of the British Museum).

sumption that general reproducibility terminology
and definitions somehow don’t apply to computer
science for which different terminology and defi-
nitions are needed. To the extent that terminology
and definitions have been proposed for NLP/ML,
these have tended to be sketchy and incomplete,
e.g. the definitions above entirely skirt the issue of
how to tell if two studies are or are not ‘the same’
in terms of code, method, team, etc.

In this paper we take the general definitions (Sec-
tion 3) of the International Vocabulary of Metrol-
ogy (VIM) (JCGM, 2012), and explore how these
can be mapped to the NLP/ML context and ex-
tended into a practical framework for reproducibil-
ity assessment capable of yielding quantified as-
sessments of degree of reproducibility (Section 4).
We start with two informal examples (Section 2),
one involving weight measurements of museum
artifacts, the other weighted F1 measurements of
a text classifier, and conclude with a discussion of
limitations and future directions (Sections 6, 7).

2 Example Physical and Non-physical
Measurements

2.1 Mass of a torc

The first example involves physical measurement
where general scientific reproducibility definitions
apply entirely uncontroversially. The torc (or neck
ring) shown in Figure 1 is from the British Museum
collection.* The information provided about it in
the collection catalogue (some of it also shown in
Figure 1) includes a measurement of its weight
which is given as 87.20g.

Museum records contain two other weight mea-

Shttps://www.britishmuseum.org/
collection/object/H_1991-0501-129

surements of tore 1991,0501.129, and museum
staff performed four additional weighings for this
study, yielding a set of seven measured quantity
values for the mass of this torc, shown in the last
column of Table 1. Details such as the scales used,
whether they were calibrated, placed on a flat sur-
face, etc., are not normally recorded in a museuym
context, but the general expectation is nevertheless
that the same readings are obtained. The values
in Table 1 range from 87.2g to 92g, a sizable dif-
ference that implies there must have been some
differences in the conditions under which the mea-
surements were performed that resulted in the dif-
ferent measured values.

Museum records show‘ that the measurements
were taken by different people (Jeam) at differ-
ent times, up to 30 years apart (Date); staff nor-
mally place the object on ordinary electronic scales
noting down the displayed number and rounding
to one tenth of a gramme (Measurement method);
calibration is sometimes checked with a 10g stan-
dard weight, but scales are not recalibrated (Mea-
surement procedure). The museum catalogue also
records details of a conservation treatment in 1991
for torc 1991,0501.129 (for full details see the link
in Footnote 3) which included dirt removal with a
scalpel, and immersion in acid (Object conditions).

Table 1 provides an overview of the conditions of
measurement mentioned above (Team, Date, etc.),
alongside the corresponding values for condition
in each of the seven measurement where available.
Despite some missing information, the information
indicates that the treatment for dirt and corrosion
removal in 1991 led to the biggest reduction in
weight, with the exact scales and use of a standard
weight potentially explaining some of the remain-
ing differences.

2.2 Weighted F1-score of a text classifier

The second example involves non-physical mea-
surements in the form of a set of eight weighted
F1 (wF1) scores for the same NLP system variant
of which seven were obtained in four reproduction
studies of Vajjala and Rama (2018). In the original
paper, Vajjala and Rama report variants of a text
classifier that assigns grades to essays written by
second-language learners of German, Italian and
Czech. One of the multilingual system variants, re-

“Information about museum records and practices in this
and other sections very kindly provided by Dr Julia Farley, cu-
rator of the British Museum’s European Iron Age and Roman
Conquest Period collections.


Object Measurement Measurement pro- Measured
Object Measurand || conditions method conditions cedure conditions || Date | Team || quantity
Treatments Scales Standard weight value

1991,0501.129 mass 0 ? 2 1991 ? 92g
1991,0501.129 mass 0? 2 ? ? JF? 92.0g
1991,0501.129 mass 1 ? 10g 2012) JF 87.2g
1991,0501.129 mass 1 SWS pocket scales none 2021] CM 87.47g
1991,0501.129 mass 1 SWS pocket scales none 2021] CM 87.37 g
1991,0501.129 mass 1 CBD bench counting scales none 2021] CM 88.1g
1991,0501.129 mass 1 CBD bench counting scales none 2021] CM 88.1g

Table 1: Some of the conditions of measurement for the seven weight measurements of torc 1991,0501.129. SWS
= Smart Weigh Digital Pocket scale SWS100; CBD = Adam CBD 4 bench counting scales.

ferred to below as multPOS ~, uses part-of-speech
(POS) n-grams, and doesn’t use language identity
information. The information provided about it in
the paper includes a wF1 score of 0.726 (Table 3
in the paper).

We have seven other wF1 scores for multPOS~
from subsequent research which intended, in at
least the first experiment in each case, to repeat the
original experiment exactly. Arhiliuc et al. (2020)
report a score of 0.680 wF1 (Table 3 in the paper).
Huber and Céltekin (2020) report 0.681 (Table 3
in the paper). Bestgen (2020) reports three wF1
scores: 0.680 and 0.722 (Table 2 in the paper) and
0.728 (Table 5). Caines and Buttery (2020) report
2 scores: 0.680 and 0.732 (Table 4 in the paper).
Four of the measured quantity values (those shown
first for each team in Table 2) were obtained in con-
ditions as close to identical as the teams could man-
age, but nevertheless range from 0.680 to 0.726, a
difference in wF1 score that would be considered
an impressive improvement in many NLP contexts.

From the papers we know that the scores were
produced by five different teams (Jeam), up to two
years apart (Date). wF1 measurements are nor-
mally performed by computing the wF1 score over
paired system and target outputs, but no informa-
tion about implementation of the wF1 algorithm
(Measurement method) is given. In most cases, the
original system code by Vajjala & Rama (V&R)
was used, but Caines & Buttery (C&B) reimple-
mented it, and Bestgen (B) and Huber & Coltekin
(H&C) produced cross-validated results with 10
random seeds (Object conditions), rather than a
single fixed seed. The measured values were all
obtained in different compile-time (CT) (Object
conditions), and run-time (RT) environments (Mea-
surement procedure), except for two results where
a Docker container was used. The test set was the
same as the original in all but three cases (marked
as B, C&B and H&C in the Inputs column) (Mea-

surement procedure).

Table 2 provides an overview of the conditions
mentioned above (Team, Date, etc.) under which
the eight measurements were obtained. Conditions
can be seen as attribute-value pairs where the at-
tribute name is here shown in the (lower) column
heading, and the values for it in the table cells.
We know the reason for some of the differences,
because the authors controlled for them: e.g. the
differences in compile time and run time environ-
ments explain the difference between Bestgen’s
first and second results, and reimplementation of
the system code in R explains the difference be-
tween Caines & Buttery’s first and second results.
Despite missing information, we can see that differ-
ent CT/RT, and performing 10-fold cross-validation
vs. a single run with fixed seed, account for some
of the remaining differences.

2.3 Comparison

In presenting and discussing the physical and non-
physical measurements and conditions in the pre-
ceding two sections, we introduced common ter-
minology for which full VIM definitions will be
provided in the next section. The intention in using
the same terms and layout in Tables 1 and 2 was to
bring out the similarities between the two sets of
measurements, and to show that measurements can
generally, whether physical or not, be characterised
in the same way on the basis of the same general
scientific terms and definitions.

The two fields, curating of museum artifacts
and NLP/ML, also have in common that (i) condi-
tions of measurement are not traditionally recorded
in any great detail, making exact repetition of a
measurement virtually impossible, (ii) despite this,

Shttps://github.com/nishkalavallabhi/
UniversalCEFRScoring
Shttps://github.com/cainesap/
EFRgrader

Q



Meas. method Measurement pro- Measured
Object | Measurand Object conditions conditions cedure conditions Date | Team quantity
Code* Seed CT env_ | Method | Implem. | Procedure | Inputs | RT Env. value

multPOS— wF1 V&R° | V&R 1 fixed V&R WF1(0,t)| V&R One V&R V&R 2018) V&R_ || 0.726 wF1
multPOS— wF1 V&R ? A et al. Win | wF1(0,t)| V&R? OTE V&R | A etal. Win || 2020 | A et al. || 0.680 wF1
multPOS — wF1 V&R | V&R 1 fixed | BMacOS | wF1(o,t) ? OTE V&R | BMacOS || 2020 B 0.680 wF1
multPOS — wF1 V&R | V&R 1 fixed | B Docker | wF1(o,t) ? OTE V&R | B Docker || 2020 B 0.722 wF1
multPOS — wF1 V&R+s| B10avg B Docker | wF1(o,t) ? OTE B B Docker || 2020 B 0.728 wF1
multPOS— wF1 V&R | V&R 1 fixed C&B1 WF 1(0,t) ? One V&R C&B1 2019] C&B_ || 0.680 wF1
multPOS— wF1 C&B° ? C&B2 WF 1(0,t) ? OTE C&B C&B2 2020) C&B || 0.732 wF1
multPOS — wF1 V&R | H&C 10 avg H&C wF1(0,t) ? OTE H&C H&C 2020 | H&C || 0.681 wF1

Table 2: Some conditions of measurement for eight weighted Fl measurements of Vajjala & Rama’s multilin-
gual POS-ngram CEFR classifier system without language category information variant (multPOS~ ). *Code here
shown separately from random seed. OTE = outputs vs. targets evaluation, i.e. the standard procedure of obtaining
outputs for a set of test inputs and computing metrics over system and target outputs.

there is nevertheless an expectation that repeat
measurements should yield the same results, and
(ili) historically, practitioners have not cared about
these issues very much.

3. VIM Definitions of Repeatability and
Reproducibility

The International Vocabulary of Metrology (VIM)
(JCGM, 2012) defines repeatability and repro-
ducibility as follows (all defined terms in boldface,
see Appendix for full set of verbatim VIM defini-
tions including subsidiary defined terms):

2.21 measurement repeatability (or repeatability,
for short) is measurement precision under a
set of repeatability conditions of measure-
ment.

2.20 a repeatability condition of measurement
(repeatability condition) is a condition of mea-
surement, out of a set of conditions that in-
cludes the same measurement procedure,
same operators, same measuring system,
same operating conditions and same location,
and replicate measurements on the same or

similar objects over a short period of time.

2.25 measurement reproducibility (reproducibil-
ity) is measurement precision under repro-

ducibility conditions of measurement.

2.24 a reproducibility condition of measure-
ment (reproducibility condition) is a condi-
tion of measurement, out of a set of condi-
tions that includes different locations, opera-
tors, measuring systems, etc. A specification
should give the conditions changed and un-

changed, to the extent practical.

The VIM definitions are squarely focused on mea-
surement: repeatability and reproducibility are

properties of measurements (not objects, scores,
results or conclusions), and are defined as measure-
ment precision, i.e. both are quantified by calcu-
lating the precision of a set of measured quantity
values. Moreover, both concepts are defined rela-
tive to a set of conditions of measurement, in other
words, the conditions have to be known and speci-
fied for assessment of either concept (repeatability,
reproducibility) to be meaningful.

4 Mapping the VIM Definitions to a
Metrological Framework for NLP/ML

The VIM definitions provide the definitional ba-
sis for reproducibility assessment. However, for a
framework that tells us what to do in practice in
an NLP/ML context, we also need (i) a method
for computing precision, (ii) a specification of the
practical steps in performing assessments, and (iii)
a specific set of repeatability/reproducibility condi-
tions of measurement. In this section, we present
the fixed, discipline-independent elements of the
proposed framework (Section 4.1), methods for
computing precision in NLP/ML (Section 4.2), a
series of steps for practical assessment of repro-
ducibility (Section 4.3), and, utilising previous
work on reproducibility in the NLP/ML field, start-
ing points for identifying suitable conditions of
measurement (Section 4.4).

4.1 Fixed elements of the framework

Table 2 gave a first indication what (some) con-
ditions might look like in NLP/ML, grouped into
Object conditions C° , Measurement method condi-
tions CN , and Measurement procedure conditions
C?. These conceptually useful groupings are re-
tained to yield a skeleton framework shown in dia-
grammatic form in Figure 2, which corresponds to


™m

M,
Corio!

Precision R°

Figure 2: Diagrammatic overview of repeatability as-
sessment of measurements M1, Mo,... M, of object
O, with measurand m, and repeatability conditions of
measurements C (same for all /;). Repeatability R° is
defined as the precision of the set of values vu; returned
by M;. (For C°, CN, and C?, see in text.)

the following definition of repeatability R°:

R°(M,, Mg, ...Mn) := Precision(v1, v2, ...Un)
where M;: (m, O,ti,C) 9;

and the M; are repeat measurements for measurand
m performed on object O at different times t; under
(the same) set of conditions C = C2? UCN U
C?. Below the coefficient of variation is used
as the precision measure, but other measures are
possible. The members of each set of conditions
are attribute/value pairs each consisting of a name
and a value. Reproducibility R is defined in the
same way except that condition values differ for at
least one condition in the M;;:

R(Mi, Mp, ..
where M;: (m, O,t;, Ci)  v;

Mn) := Precision(v1, v2, ...Un)

4.2 Computing precision

Precision in metrological studies is reported in
terms of some or all of the following: mean, stan-
dard deviation with 95% confidence intervals, coef-
ficient of variation, percentage of measured quan-
tity values within n standard deviations.

In reproducibility assessment in NLP/ML, sam-
ple sizes tend to be very small (a sample size of 8
as in Section 2.2 is currently unique). We therefore
need to use de-biased sample estimators: we use
the unbiased sample standard deviation, denoted
s*, with confidence intervals calculated using a
t-distribution, and standard error (of the unbiased
sample standard deviation) approximated on the ba-
sis of the standard error of the unbiased sample vari-
ance se(s”) as se,2(s*) & 528e(s”) (Rao, 1973).
Assuming measured quantity values are normally

distributed, we calculate the standard error of the

A
2c.
Finally, we also use a small sample correction for
the coefficient of variation: CV* = (1 + Gm)CV
(Sokal and Rohlf, 1971).

Equipped with the above, the reproducibility of
WF 1 measurements of Vajjala and Rama (2018)’s
MultPOS~ system can, for example, be quantified
based on the eight replicate measurements from
Table 2 (disregarding the role of measurement con-
ditions for the moment) as being CV* = 3.818.’
This and two other example applications of the
framework is presented in more detail in Section 5.

sample variance in the usual way: se(s?) =

4.3 Steps in reproducibility assessment

In order to relate multiple repeatability and repro-
ducibility assessments of the same object and mea-
surand to each other and compare them, they need
to share the same conditions of measurement (same
in terms of attribute names, not necessarily val-
ues, see next section). Repeatability is simply the
special case of reproducibility where all condition
values are also the same.

For a true estimate of the variation resulting from
given differences in condition values, the baseline
variation, present when all condition values are the
same, needs to be known. It is therefore desirable
to carry out repeatability assessment prior to repro-
ducibility assessment in order to be able to take
into account the baseline amount of variation. E.g.
if the coefficient of variation is oo under identical
conditions C°, and xc under varied conditions C,
then it’s the difference between xo and x¢ that es-
timates the effect of varying the conditions. Finally,
for very small sample sizes, both baseline repeata-
bility assessment, and subsequent reproducibility
assessment, should use the same sample size to
ensure accurate assessment of variation due to the

7Code for computing CV* available here: https: //
github.com/asbelz/coeff-var


2-PHASE REPRODUCIBILITY ASSESSMENT
REPEATABILITY PHASE

1. Select measurement to be assessed, identify
shared object and measurand.

2. Select initial set of repeatability conditions of
measurement C® and specify value for each
condition.

3. Perform n>2 reproduction measurements to
yield measured quantity values v?, v9, ...v2.

4. Compute precision for v?, v9, ...v®, giving re-

peatability score R°.

5. Unless precision is as small as desired, iden-
tify additional conditions that had different
values in some of the reproduction measure-
ment, and add them to the set of measurement
conditions, also updating the measurements
to ensure same values for the new conditions.
Repeat Steps 3-5.

REPRODUCIBILITY PHASE

6. From the final set of repeatability conditions,
select the conditions to vary, and specify the
different values to test.

7. For each combination of differing condition
values:
(a) Carry out n reproduction tests, yielding
measured quantity values v1, V2, ...Un
(b) Compute precision for v1, v2, ...Un, gIV-
ing reproducibility score R.

Report all resulting R scores, alongside baseline R°
score.

Figure 3: Steps in 2-phase reproducibility assessment
with baseline repeatability assessment. Step 5 is obso-
lete if a field has standard conditions of measurement.

varied condition values (alone).

Figure 3 translates what we have said in this and
preceding sections into a 2-phase framework for re-
producibility assessment. If a field develops shared
standard conditions of measurement, Step 5 is ob-
solete. In situations where reproduction studies
have been carried out without a pre-defined, shared
set of conditions of measurement, reproducibility
assessment can still be carried out (as we did for the
Vajjala & Rama system in Section 2.2), but in this
situation a baseline assessment of variation under
repeatability conditions of measurement is not pos-
sible, and the only option is to use a single-phase
version of the framework as shown in Figure 4. See
Section 6 for more discussion of this issue.

4.4 Conditions of measurement

The final component needed for a metrologi-
cal framework for reproducibility assessment in
NLP/ML is a specific set of conditions of measure-
ment. As mentioned in Section 4.1, individual con-
ditions consist of name and value, and their role is
to capture those attributes of a measurement where
different values may cause differences in measured
values. As indicated by Step 5 in Figure 3, for
repeatability, conditions should be selected with
a view to reducing baseline variation. As it’s not
practically feasible (or, normally, theoretically pos-
sible) to specify and control for all conditions that
can be the same or different in a measurement,
some judgment is called for here (see Section 6 for
discussion). The idea is that a discipline evolves
shared standard conditions that address this.

It so happens that much of the reproducibility
work in ML and NLP has so far focused on what
standard conditions of measurement (information
about system, data, dependencies, computing en-
vironment, etc.) for metric measurements need to
be specified in order to enable repeatability assess-
ment, even if it hasn’t been couched in these terms.
Out of all reproducibility topics, pinning down the
information and resources that need to be shared
to enable others to obtain the same metric scores
is the one that has attracted by far the most discus-
sion and papers. In fact, reproduction in NLP/ML
has become synonymous with rerunning code and
obtaining the same metric scores again as were
obtained in a previous study.

Reproducibility checklists such as those pro-
vided by (Pineau, 2020) and the ACL are lists
of types of information (attributes) for which au-
thors are asked to provide information (values),
and these can directly be construed as conditions
of measurement. In this section, the intention is
not to propose definitive sets of conditions relating
to object, measurement method and measurement
procedure that should be used in NLP/ML. Rather,
in each subsection, we point to existing research
such as the above that can be used to provide a
“starter set’ of conditions of measurement.

4.4.1 Object conditions

The ML Code Completeness Checklist’ (adopted
as part of the NeurIPS’21 guidelines) consists of

Snttps://2021.aclweb.org/calls/
reproducibility-checklist/

https: //github.com/paperswithcode/
releasing-—research-—cod



1-PHASE REPRODUCIBILITY ASSESSMENT

1. For set of m measurements to be assessed,
identify shared object and measurand.

2. Identify all conditions of measurement C’' for
which information is available for all measure-
ments, and specify values for each condition.

3. Gather the nm measured quantity values
U1, V2, +--Un

4. Compute precision for v1, v2, ...Un, giving re-
producibility score R.

Report resulting R score.

Figure 4: Steps in 1-phase reproducibility assessment
for assessing a set of existing measurements where
baseline repeatibility assessment is not possible. See
also discussion in Section 6.

five items: specification of dependencies, train-
ing code, evaluation code, pre-trained models,
README file including results and commands.
These provide a good starting point for object con-
ditions of measurement (note we group evaluation
code under measurement method conditions, see
next section):

1. Dependencies

Training code

Pre-trained models

Precise commands to run code/produce results
Compile-time environment

Aw es &

Run-time environment

In human evaluation of system outputs, object con-
ditions don’t apply, as the object of measurement
is fully specified by a sample of its outputs.

4.4.2 Measurement method conditions

In metric-based measurement where the measurand
is defined mathematically, a measurement method
is an implementation of a method for (or, conceiv-
ably, a manual way of) computing a quantity value
for the measurand. In this case, the method, like
the Object of measurement, is a computational arti-
fact, so the same conditions can be used as in the
preceding section (albeit with different names to
mark the difference; values will clearly also differ).

In human-evaluation-based measurement, a dif-
ferent set of conditions is needed. Here, the measur-
and is identified by (ideally standardised) name and
definition for the quantity being assessed. This has

been termed ‘quality criterion’ !° in previous work

(Howcroft et al., 2020; Belz et al., 2020b) which
took a first stab at a set of standardised names and
definitions. These, in conjunction with a checklist
for human evaluation such as the one proposed by
Shimorina and Belz (2021), can provide a start-
ing point for measurement method conditions (as
follows), and measurement procedure conditions
(next section), for human evaluation.

1. Name and definition of measurand

2. Evaluation mode!!

3. Method of response elicitation

4

. Method for aggregating or otherwise process-
ing raw participant responses

5. Any code used (conditions as for Object)

4.4.3 Measurement procedure conditions
Measurement procedure conditions capture any in-
formation needed to apply a given measurement
method in practice. In metric-based measurement,
this includes:

1. Test set

2. Any preparatory steps taken, such as prepro-
cessing of text

3. Any code used (conditions as for Object)
In human-evaluation-based measurement, some of

the remaining properties from Shimorina and Belz
(2021) and Howcroft et al. (2020) can be used:

1. If test set evaluation, test set and prepro-
cessing code/method(s); if system interaction,
specification of system set up

Responses collection method
Quality assurance code/method(s)
Instructions to evaluators

Evaluation interface

Aw sr WL

Any code used (conditions as for Object)

5 Examples

In this section we apply the framework to the two
sets of measurements from Sections 2.1 and 2.2,
and an additional set of measurements from a re-
cent reproduction study (Mille et al., 2021).

'0Tn this and subsequent sections we use five closely related
terms which relate to each other as follows (Belz et al., 2020b):
(i) quality criterion + evaluation mode = evaluation measure;
(ii) evaluation measure + experimental design = evaluation
method.

'l Absolute vs. relative, intrinsic vs. extrinsic, objective vs.
subjective.


5.1 Mass of a Torc

In the case of the torc mass measurements, the re-
producibility analysis is performed post hoc, Le.
we cannot obtain more information for the three
older measurements than was recorded at the time.
We therefore have to use the 1-phase assessment
(Figure 4), and for the complete set of seven weigh-
ings specify all conditions (Table 1) as different.
CV* results can then be reported as follows:

Mass measurement reproducibility under re-
producibility conditions of measurement as de-
tailed in Table 1, was assessed on the basis of
seven measurements of torc 1991,0501.129 as
follows: the unbiased coefficient of variation
is 2.61, for a mean of 88.89, unbiased sample
standard deviation of 2.24 with 95% CI (0.784,
3.696), and sample size 7. All measured val-
ues fall within two standard deviations, 71.43%
within one standard deviation.

If we used just the four measurements for which all
condition values are known, we would get CV* =
0.519 under reproducibility conditions where only
the scales used differ. For a sample size of 4 we
can still be reasonably confident that this is a good
estimate of CV* for the whole population (stdev
95% CI [-0.04, 0.90]).

Repeatability assessment can be performed for
the two subsets of measurements for the SDS scales
and the CBD scales, which gives CV* = 0.11 for the
former and CV* = 0 for the latter, for similar-size
stdev confidence intervals.

5.2 wF1 of a Text Classifier

The situation is similar for the text classifier wF1
measurements (Table 2) in that we are restricted
to the information made available by the authors,
which is however, more complete than in the torc
example. Here too we have to use the 1-phase
assessment (Figure 4), and for the complete set of

8 wF1 values specify all conditions as different.

CV* results can then be summed up as above:

wFI measurement reproducibility under repro-
ducibility conditions of measurement as in Ta-
ble 2, was assessed on the basis of eight mea-
surements reported by Vajjala and Rama (2018),
Arhiliuc et al. (2020), Bestgen (2020), Caines
and Buttery (2020), and Huber and Céltekin
(2020) (Table 2): CV* = 3.818, mean = 0.7036,
unbiased sample standard deviation = 0.0261,
95% CI [0.01, 0.04], sample size = 8. All mea-
sured values fall within two standard deviations,

87% within one standard deviation.

5.3. Clarity and Fluency of an NLG system

The third example comes from a recent reproduc-
tion study (Mille et al., 2021) which repeated the
human evaluation of a Dutch-language football re-
port generation system (van der Lee et al., 2017).
There were two main measurands, mean Clarity
ratings and mean Fluency ratings, and a single ob-
ject (the report generator) was evaluated. There
were two scores for each of the measurands, one
from the original study, one from the reproduction
study. Here the situation was that a repeatability
study was intended, but not possible.!? Therefore a
different set of evaluators and a different evaluation
interface had to be used.

Both Clarity and Fluency ratings were obtained
on a 7-point scale (1..7). Computed on the scores
as reported, for Clarity, CV* = 10.983, for Flu-
ency, CV* = 13.525. However, if the 7-point scale
had been 0..6, higher CV* values would have been
obtained, i.e. results are not comparable across dif-
ferent rating scales. To address this, rating scales
should be mapped to range with lowest score 0.
Results can then be reported as follows:

Clarity measurement reproducibility under re-
producibility conditions of measurement as de-
tailed in Mille et al. (2021), was assessed
on the basis of 2 measurements reported by
van der Lee et al. (2017) and Mille et al. (2021),
rescaled to 0..6: CV* = 13.193, mean = 4.969,
unbiased sample standard deviation = 0.583,
95% CI [-2.75, 3.92], sample size = 2. Mea-
sured values fall within one standard deviation.

Fluency measurement reproducibility under re-
producibility conditions of measurement as de-
tailed in Mille et al. (2021), was assessed
on the basis of 2 measurements reported by
van der Lee et al. (2017) and Mille et al. (2021),
rescaled to 0..6: CV* = 16.372, mean = 4.75,
unbiased sample standard deviation = 0.691,
95% CI [-3.26, 4.645], sample size = 2. Mea-
sured values fall within one standard deviation.

5.3.1 Notes on Examples

CV* is fronted in the examples above as the ‘head-
line’ result. It can take on this role, because it
is a general measure, not in the unit of the mea-
surements (unlike mean and standard deviation),

"Original evaluators could not be used, and COVID-19
pandemic restrictions prevented real-world interaction.


providing a quantification of precision (degree of
reproducibility) that is comparable across studies
(Ahmed, 1995, p. 57). This also holds for percent-
age within n standard deviations but the latter is a
less recognised measure, and likely to be the less
intuitive for many.

CV is a measure of precision and as such, sample
size should be > 3. For a sample of 2 (as in the
human evaluation in the last section), CV is still
meaningful as a measure of the variation found in
the sample. However, it will generally provide a
less reliable estimate of population CV.

6 Discussion

Specifying conditions of measurement in repro-
ducibility assessment is important so results can
be compared across different measures and assess-
ments. We have not attempted to come up with
a definitive set of conditions, but pointed to other
research as a starting point. One important role
the conditions play is to facilitate estimation of
baseline variation via repeatability testing. It could
be argued that if the goal is to ensure as near as
possible identical measurements in repeatability
testing, then a straightforward way to achieve that
is containerisation. However, firstly the purpose
of repeatability testing is to assess variation un-
der normal use and it’s not realistic to always run
systems in Docker containers even in a research
context. Secondly, human evaluation can’t be run
in a container.

What counts as a good level of reproducibility
can differ substantially between disciplines and
contexts. E.g. in bio-science assays,'> precision
(reported as coefficient of variation) ranges from
typically <10% for enzyme assays, to 20-50% for
in vivo and cell-based assays, and >300% for virus
titer assays (AAH/USFWS, n.d.). For NLP, such
typical CV ranges would have to be established
over time, but it seems clear that we would expect
typical CV for metric-based measurements to be
much lower (better) than for human-assessment-
based measurements. For a set of wF1 measure-
ments, the 3.8% CV* above seems high.

There are many ways in which results from simi-
lar studies can be compared and conclusions drawn
from comparisons. For example, to make (subjec-
tive) judgments of whether the same conclusions
can be drawn from a set of comparable experimen-

3 An investigative analytic procedure in the physical sci-
ences e.g. laboratory medicine.

tal results, or to ask a group of assessors to make
such judgments, is a valid and informative thing
to do, but it’s not reproducibility assessment in the
general scientific sense of the term. It can also be in-
formative to consider the ease with which systems
can be recreated, but reproducibility is not a prop-
erty of systems. Computer science has a history of
departing from standard scientific reproducibility
definitions, e.g. the ACM changed its definitions
after NISO asked it to “harmonize its terminol-
ogy and definitions with those used in the broader
scientific research community.” (Association for
Computing Machinery, 2020). The question is, if
the standard scientific terminology and definitions
work for computer science, why would we not use
them exactly as they are, rather than adapt them in
often fundamental ways?

7 Conclusion

The reproducibility debate in NLP/ML has long
been framed in terms of pinning down exactly what
information we need to share so that others are
guaranteed to get the same metric scores (e.g. Son-
nenburg et al., 2007). What is becoming clear,
however, is that no matter how much of our code,
data and ancillary information we share, residual
amounts of variation remain that are stubbornly re-
sistant to being eliminated. A recent survey (Belz
et al., 2021) found that just 14% of the 513 origi-
nal/reproduction score pairs analysed were exactly
the same. Judging the remainder simply ‘not repro-
duced’ would be of limited usefulness, as some are
much closer to being the same than others, while
assessments of whether the same conclusions can
be drawn are prone to low levels of agreement.
Quantifying closeness of results, and, over time, es-
tablishing expected levels closeness, seems a better
way forward.

In this paper our aim has been to challenge the as-
sumption that the general scientific reproducibility
terms and definitions are not applicable or suitable
for NLP/ML by directly mapping them to a prac-
tical framework that yields quantified assessments
of degree of reproducibility that are comparable
across different studies.

The NLP/ML field certainly needs some way of
assessing degree of reproducibility that is compa-
rable across studies, because the ability to assess
reproducibility of results, hence the trustworthiness
of evaluation practices, is a cornerstone of scientific
research that the field has not yet fully achieved.


Acknowledgements

The contribution to this work made by Dr Julia Far-
ley, curator (European Iron Age and Roman Con-
quest period collections) at the British Museum,
and colleagues, is gratefully acknowledged. In
particular their time and patience in obtaining the
seven historical and new weighings of a 2,000 year
old torc, as well as providing detailed information
about the weighings.

References

AAH/USFWS. n.d. Assay validation methods: Defini-
tions and terms. Aquatic Animal Health Program,
U.S. Fish & Wildlife Service.

SE Ahmed. 1995. A pooling methodology for coeffi-
cient of variation. Sankhya: The Indian Journal of
Statistics, Series B, pages 57-75.

Cristina Arhiliuc, Jelena Mitrovié, and Michael Gran-
itzer. 2020. Language proficiency scoring. In Pro-
ceedings of The 12th Language Resources and Eval-
uation Conference, pages 5624-5630, Marseille,
France. European Language Resources Association.

Association for Computing Machinery. 2020.
Artifact review and badging Version
1.1, August 24, 2020. https://www.
acm.org/publications/policies/

artifact-review-—and-badging-current.

Monya Baker. 2016. Reproducibility crisis. Nature,
533(26):353-66.

Anja Belz, Shubham Agarwal, Anastasia Shimorina,
and Ehud Reiter. 2021. A systematic review of re-
producibility research in natural language process-
ing. In Proceedings of the 16th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Main Volume, pages 381-393.

Anya Belz, Shubham Agarwal, Anastasia Shimorina,
and Ehud Reiter. 2020a. ReproGen: Proposal for a
shared task on reproducibility of human evaluations
in NLG. In Proceedings of the 13th International
Conference on Natural Language Generation, pages
232-236, Dublin, Ireland. Association for Computa-
tional Linguistics.

Anya Belz, Simon Mille, and David M. Howcroft.
2020b. Disentangling the properties of human eval-
uation methods: A classification system to support
comparability, meta-evaluation and reproducibility
testing. In Proceedings of the 13th International
Conference on Natural Language Generation, pages
183-194, Dublin, Ireland. Association for Computa-
tional Linguistics.

Yves Bestgen. 2020. Reproducing monolingual, multi-
lingual and cross-lingual CEFR predictions. In Pro-
ceedings of The 12th Language Resources and Eval-
uation Conference, pages 5595-5602, Marseille,
France. European Language Resources Association.

Anténio Branco, Nicoletta Calzolari, Piek Vossen,
Gertjan Van Noord, Dieter van Uytvanck, Joao Silva,
Luis Gomes, André Moreira, and Willem Elbers.
2020. A shared task of a new, collaborative type
to foster reproducibility: A first exercise in the
area of language science and technology with RE-
PROLANG2020. In Proceedings of The 12th Lan-
guage Resources and Evaluation Conference, pages
5539-5545, Marseille, France. European Language
Resources Association.

Andrew Caines and Paula Buttery. 2020. RE-
PROLANG 2020: Automatic proficiency scoring
of Czech, English, German, Italian, and Spanish
learner essays. In Proceedings of The 12th Lan-
guage Resources and Evaluation Conference, pages
5614-5623, Marseille, France. European Language
Resources Association.

Chris Drummond. 2009. Replicability is not repro-
ducibility: nor is it good science. Presented at
4th Workshop on Evaluation Methods for Machine
Learning held at ICML’09.

David M. Howcroft, Anya Belz, Miruna-Adriana
Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad
Mahamood, Simon Mille, Emiel van Miltenburg,
Sashank Santhanam, and Verena Rieser. 2020.
Twenty years of confusion in human evaluation:
NLG needs evaluation sheets and standardised def-
initions. In Proceedings of the 13th International
Conference on Natural Language Generation, pages
169-182, Dublin, Ireland. Association for Computa-
tional Linguistics.

Eva Huber and Cagri Coltekin. 2020. Reproduction
and replication: A case study with automatic es-
say scoring. In Proceedings of The 12th Language
Resources and Evaluation Conference, pages 5603—
5613, Marseille, France. European Language Re-
sources Association.

JCGM. 2012. International vocabulary of metrology-
basic and general concepts and associated terms
(VIM).

Margot Mieskes, Karén Fort, Aurélie Névéol, Cyril
Grouin, and Kevin Cohen. 2019. Community per-
spective on replicability in natural language process-
ing. In Proceedings of the International Conference
on Recent Advances in Natural Language Process-
ing (RANLP 2019), pages 768-775, Varna, Bulgaria.
INCOMA Ltd.

Simon Mille, Thiago Castro Ferreira, Anya Belz, and
Brian Davis. 2021. Another pass: A reproduction
study of the human evaluation of a football report
generation system. In Proceedings of the 14th Inter-
national Conference on Natural Language Genera-
tion (INLG 2021).


Ted Pedersen. 2008. Empiricism is not a matter of faith.
Computational Linguistics, 34(3):465—470.

Joelle Pineau. 2020. The machine learning repro-
ducibility checklist v2.0.

Calyampudi Radhakrishna Rao. 1973. Linear statisti-
cal inference and its applications. Wiley.

Nicolas P Rougier, Konrad Hinsen, Frédéric Alexan-
dre, Thomas Arildsen, Lorena A Barba, Fabien CY
Benureau, C Titus Brown, Pierre De Buyl, Ozan
Caglayan, Andrew P Davison, et al. 2017. Sustain-
able computational science: the rescience initiative.
PeerJ Computer Science, 3:e142.

Patrick D Schloss. 2018. Identifying and overcoming
threats to reproducibility, replicability, robustness,
and generalizability in microbiome research. MBio,
9(3).

Anastasia Shimorina and Anya Belz. 2021. The hu-
man evaluation datasheet 1.0: A template for record-
ing details of human evaluation experiments in nlp.
arXiv preprint arXiv:2103.09710.

R.R. Sokal and FJ. Rohlf. 1971. Biometry: The Prin-
ciples and Practice of Statistics in Biological Re-
search. WH Freeman.

Soren Sonnenburg, Mikio L Braun, Cheng Soon Ong,
Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann
LeCunn, Klaus-Robert Muller, Fernando Pereira,
Carl Edward Rasmussen, et al. 2007. The need for
open source software in machine learning.

Sowmya Vajjala and Taraka Rama. 2018. Experiments
with universal CEFR classification. In Proceedings
of the Thirteenth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
147-153, New Orleans, Louisiana. Association for
Computational Linguistics.

Chris van der Lee, Emiel Krahmer, and Sander
Wubben. 2017. Pass: A dutch data-to-text system
for soccer, targeted towards specific audiences. In
Proceedings of the 10th International Conference on
Natural Language Generation, pages 95-104.

Kirstie Whitaker. 2017. The MT Reproducibility
Checklist. https://www.cs.mcgill.ca/
~jpineau/ReproducibilityChecklist.
pd.

Martijn Wieling, Josine Rawee, and Gertjan van Noord.
2018. Reproducibility in computational linguistics:
Are we willing to share? Computational Linguistics,
44(4):641-649.


A Verbatim VIM Definitions

Primary term (synonyms)

Definition

2.21 measurement repeatability (re-
peatability)

measurement precision under a set of repeatability conditions of measure-
ment

2.20 (Note 1) repeatability condition
of measurement (repeatability condi-
tion)

condition of measurement, out of a set of conditions that includes the same mea-
surement procedure, same operators, same measuring system, same operating
conditions and same location, and replicate measurements on the same or similar
objects over a short period of time

NOTE 1 A condition of measurement is a repeatability condition only with
respect to a specified set of repeatability conditions.

2.25 measurement reproducibility
(reproducibility)

measurement precision under reproducibility conditions of measurement

2.24 (Note 2) reproducibility condi-
tion of measurement (reproducibility
condition)

condition of measurement, out of a set of conditions that includes different
locations, operators, measuring systems, and replicate measurements on the
same or similar objects

NOTE2 A specification should give the conditions changed and unchanged, to
the extent practical.

1.1 quantity

property of a phenomenon, body, or substance, where the property has a magnitude
that can be expressed as a number and a reference

1.19 quantity value (value of a quan-
tity, value)

number and reference together expressing magnitude of a quantity

2.1 measurement

process of experimentally obtaining one or more quantity values that can reason-
ably be attributed to a quantity

2.3 measurand

quantity intended to be measured

2.5 measurement method

method of measurement generic description of a logical organization of operations
used in a measurement

2.6 measurement procedure

detailed description of a measurement according to one or more measurement
principles and to a given measurement method, based on a measurement
model and including any calculation to obtain a measurement result

NOTE 1 A measurement procedure is usually documented in sufficient detail to
enable an operator to perform a measurement.

2.9 measurement result (result of
measurement)

set of quantity values being attributed to a measurand together with any other
available relevant information

2.10 measured quantity value (value
of a measured quantity, measured
value)

quantity value representing a measurement result

2.15 measurement precision (preci-
sion)

closeness of agreement between indications or measured quantity values ob-
tained by replicate measurements on the same or similar objects under specified
conditions

4.1 indication

quantity value provided by a measuring instrument or a measuring system

Table 3: Verbatim VIM definitions of repeatability, reproducibility and related concepts (JCGM, 2012). (in VIM,
definitions also give the earlier definition number from the second edition in parentheses which we omit here. In
Definition 2.20, Note 2 relating only to chemistry is omitted. In 2.5 and 2.6, less important notes are omitted for
space reasons.)
