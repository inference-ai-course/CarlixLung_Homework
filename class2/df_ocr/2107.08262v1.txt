arX1v:2107.08262v1 [cs.SE] 17 Jul 2021

Tea: Program Repair Using Neural Network Based on Program
Information Attention Matrix

Wenshuo Wang
Microsoft Asia-Pacific Technology Company Ltd.
University of Chinese Academy of Sciences
TCA, Institute of Software, Chinese Academy of Sciences
wangwenshuol8@mails.ucas.ac.cn

Chen Wu
Microsoft Asia-Pacific Technology Company Ltd.
wu.chen@microsoft.com
Liang Cheng
University of Chinese Academy of Sciences
TCA, Institute of Software, Chinese Academy of Sciences
chengliang@iscas.ac.cn
Yang Zhang
University of Chinese Academy of Sciences
TCA, Institute of Software, Chinese Academy of Sciences
zhangyang@iscas.ac.cn

Abstract

The advance in machine learning (ML)-driven
natural language process (NLP) points a
promising direction for automatic bug fixing
for software programs, as fixing a buggy pro-
gram can be transformed to a translation task.
While software programs contain much richer
information than one-dimensional natural lan-
guage documents, pioneering work on using
ML-driven NLP techniques for automatic pro-
gram repair only considered a limited set
of such information. We hypothesize that
more comprehensive information of software
programs, if appropriately utilized, can im-
prove the effectiveness of ML-driven NLP ap-
proaches in repairing software programs. As
the first step towards proving this hypothesis,
we propose a unified representation to capture
the syntax, data flow, and control flow aspects
of software programs, and devise a method to
use such a representation to guide the trans-
former model from NLP in better understand-
ing and fixing buggy programs. Our prelimi-
nary experiment confirms that the more com-
prehensive information of software programs
used, the better ML-driven NLP techniques
can perform in fixing bugs in these programs.

1 Introduction

Finding and fixing bugs accounts for a significant
portion of maintenance cost for software (Britton
et al., 2013), and the cost of bug fixing increases ex-
ponentially with time (Dawson et al., 2010). Hence,

automatic program repair has long been a focus of
software engineering, with the goal of lowering
down the cost and reducing the introduction of new
bugs during bug fixing.

Traditional automatic program repair approaches
utilize certain aspects of information of a program
to repair its bugs. For example, GenProg (Goues
et al., 2011) used genetic algorithms to mutate the
abstract syntax tree (AST) of a C program for bug
fixing, while Nopol (Xuan et al., 2016) used trace
and data types collected from the target programs
to repair assertion condition errors and assertion
statement missing errors. This type of approaches,
however, suffered low accuracy in bug detection
and repair due to the lack of a good code generation
model.

Recent advance in natural language processing
(NLP), especially machine learning (ML) based
NLP technologies, has inspired automatic program
repair research in that ML techniques designed for
NLP tasks such as automated translation have been
revised to improve the understanding of buggy pro-
grams and/or to derive correct program repairs that
achieve that greatest repairing goals. For example,
ACS (Xiong et al., 2017) sorts variables through
the principle of locality and then uses NLP tech-
nologies to analyze the content of open-source pro-
grams to generate correct patches. CoCoNut (Lutel-
lier et al., 2020) used the neural network translation
model to generate correct code based on the contex-


tual information (e.g., the adjacent code) of bugs.

However, these ML-based approaches only con-
sider and utilize a limited set of information of the
target program. For example, neither data flow nor
control flow was considered by CoCoNut, even
though it suggested that the contextual informa-
tion used in its model can be replaced by any other
forms of contextual information. This has limited
the applicability, accuracy, and effectiveness of pro-
gram repair that these technique can achieve.

In addition, there are distinct differences be-
tween software programs and natural language doc-
uments: entities in software programs (e.g., vari-
ables, statements, and functions) are related to each
other not only in syntactic or semantic bindings,
but also for data- and control-dependencies, which
makes software program more complex than one-
dimension text. As a result, a software program
can often execute in different orders (i.e., traces),
most of which are different than the syntactic or-
der of its statement. Hence, we hypothesize that
the syntax, semantics, data flow and control flow
information of software programs, if appropriately
utilized, can improve the accuracy and effective-
ness of automatic program repair techniques based
on NLP technologies.

As the first step towards proving this hypoth-
esis, we propose a unified representation, called
Program Information Attention Matrix (PIAM), to
capture the syntax, data dependency, and control
dependency of software programs to assist program
repair tasks. We also develop a prototype called
TEA (Transformer Code Attention) that replaces
the attention mechanism of the transformer model
from NLP with the PIAM, so that it can quickly
grasp the comprehensive relationship between the
entities in a buggy program and ’translate’ into a
correct program in a more accurate manner.

We have evaluated the effectiveness of TEA in
bug fixing over the Tufano dataset!, and the eval-
uation results suggested that the more aspects of
program information is considered and the more
granular such information is, the better TEA gener-
ally performs in bug fixing.

2 Background: Transformer in NLP

The transformer in NLP (Vaswani et al., 2017) is
a novel architecture that revolutionized machine
learning based sequence-to-sequence (seq2seq)

‘https: //github.com/micheletufano/
NeuralCodeTranslator/tree/master/dataset.

translation, since it ’relies entirely on self-attention
to compute representations of its input and output
without using sequence-aligned RNNs or convolu-
tion’.

Like other seq2seq models, the transformer also
incorporates an encoder and a decoder. However,
in the transformer, the encoder has one layer of a
multi-head attention followed by a layer of feed
forward neural network. The decoder has a similar
setup but with an extra masked multi-head atten-
tion layer. The encoder transforms the word em-
beddings of the input sequence into an intermediate
vector, while the decoder translate the intermediate
vector into an output sequence with the greatest
probability of achieving the translation goals.

In both encoder and decoder, the attention mech-
anism helps to relate the positions of a target se-
quence to better understand it (a notion known
as ’self-attention’) and the multi-head mechanism
helps to calculate self-attention multiple times in
a parallel and independent manner. Self-attention
ensures that information from different representa-
tion subspaces at different positions of the target
sequence has been attended. Lastly, the masked
multi-head mechanism in the decoder helps it fo-
cus on appropriate parts of the input sequence.

The multi-head attention of the transformer can
be calculated according to the black font part of
Equations 1. In Equation 1, Q, K, V are three
vectors representing the same sequence, dj is the
dimension of K, and softmaz is a softmax acti-
vation function to normalize the attention scores.
The multi-head mechanism is used to concatenates
different attention matrices to calculate attention in
different spaces.

When applied to automatic program repair, the
transformer takes as an input a buggy program and
translates’ it into a correct one, during which the
self-attention mechanism guides the repair with the
important relationships between program entities
(e.g., tokens) to improve repair accuracy.

3 Method

3.1 Program Information Attention Matrix

As aforementioned, software programs have richer
information than natural language text that should
be collected and utilized for program repairing. In
particular, the relationship between tokens in soft-
ware programs are determined not only by their
relative positions, but also by many other aspects
of features of these programs.


AADNHRWNRE

In this work, we consider control flow, data flow,
and syntax of software programs, because they are
representative program features that have been well
studied in the program analysis area (i.e., there are
a vast body of techniques and tools available for
collecting these features from software programs).
Apparently, other program features may also be
considered depending on the needs of program re-
pair tasks.

The syntax structure of a software program is
typically captured by its AST. For example, for the
following code fragments, Figure 1 illustrates its
AST, in which leaf nodes represent statements in
the code and non-leaf nodes represent the relation-
ship between leaves.

void foo(int param)
{

++param ;

for (int i = 0;

{
}

i < 100000; ++i)

var temp = LargeAlloc (param );

Thus, the relationship between any pair of
leaf nodes in an AST can be determined by: 1)
their nearest common ancestor (NCA) on the
AST; 2) the marked types of these two nodes
and their NCAs (denoted as NCALabel); and
3) the nodes along the shortest path between
these two nodes through it NCA (denoted as
NCAPath). For example, for tokens int and
var in Figure 1, their NCA is token FOR, their
NCALabel feature is TYPE->FOR->TYPE (red
circles in Figure 1); and their NCAPath fea-
ture is TYPE->VAR->INIT->FOR->BLOCK->
TYPE (the red line in Figure 1).

Similarly, the control flow feature of a program
is captured by its CFG, which characterizes its ex-
ecution paths; and its data flow feature (in other
words, data dependency) is characterized by its
DFG, which reveals how variables and parameters
are accessed and manipulated across the program.

The AST, CFG, and DFG representations of
a program each reveals a specific aspect of the
program. We propose the notion of PIAM to
unify these different representations of software
programs, so that they can be used by the trans-
former model for program repair, avoiding the over-
reliance on a specific representation. More specifi-
cally, Definition | provides the formal definition of
PIAM for software programs.

Definition 1 (PIAM). Given a program P that can
be tokenized to a set of tokens, its PIAM is a matrix

token: int token: var

Figure 1: Labeled AST.

M = 2X4jx that satisfies:

°1<i< |tokens|, 1 <j < |tokens| and
1<k < |features|, where features are
the set of features of P considered.

If the indexes of tokens t1 and t2 are respec-
tively id1 and id2, and t1 and t2 are the last
word and the first word of two adjacent nodes
in the CFG of P, then x|id1]|id2]|q| = 1,
where q is the index of the CFG feature.

If the indexes of tokens t3 and t4 are respec-
tively id3 and id4, and t3 and t4 are the to-
kens of two different leaves in P’s AST, then
x|id3][id4] |p] = 1, where p is the index of the
AST feature.

Apparently, PLIAM conforms to the definition of
the attention matrix of the transformer model.

In this work, we design three versions of the
PIAM, each representing a different way of captur-
ing the AST, CFG, and DFG information of soft-
ware programs:

¢ Version |: relations considered include direct
neighbor tokens in program syntax, direct ad-
jacent nodes in the CFG and DFG, and up
to x NCA features in the AST 7 ; and p in
Definition 1 is set to 32.

Version 2: relations considered include direct
neighbor tokens in program syntax, direct ad-
jacent nodes in the CFG and DFG, up to x
NCA features in the AST, and 125-x NCALa-
bel features of the program; and p in Defini-
tion | is set to 128.

Version 3: relations considered include di-
rect neighbor tokens in program syntax, direct
adjacent nodes in the CFG and DFG, up to x
NCA features in the AST, and 125-x NCAPath

Based on the current data sets considered, we configure x
to a value between 22 and 25.


features of the program; and p in Definition 1
is set to 128.

It is apparent that version 1, 2, and 3 of PIAM fol-
low an increasing order in the comprehensiveness
and granularity of information they consider for
software programs.

3.2 Revised Transformer Using PIAM

We revise the attention mechanism in (Vaswani
et al., 2017) using the PIAM, which marked by red
in Equation 1, in which W is the coefficient for
linear transformation of PIAM.

Attention(Q, K,V) =
T

K
xa + W *« PIAM «(1—a))V
Jae ( ))
(1)

softmax(

a in Equation | is a parameter in the range of
(0, 1], which can be learned through training to rep-
resent the proportional relationship between the
original attention matrix and the version of PLAM
used. Given an input program x, a can be calcu-
lated using Equation a = sigmod(wz + b), where
are w and 6 are constant coefficients.

3.3 Implementation

We have implemented a prototype called TEA that
utilizes the revised transformer model for bug fix-
ing. Figure 2 illustrates the architecture of TEA, in
which different versions of PIAM can be incorpo-
rated.

In particular, TEA tokenizes the input program
using the bpe algorithm (Sennrich et al., 2015) and
utilizes traditional static analysis techniques to re-
trieve its AST, CFG, and DFG information. TEA
revises an open-source pytorch implementation *
of the Transformer in (Vaswani et al., 2017) as
the baseline transformer, and replaces its attention
mechanism with one of three versions of PIAM
defined in section 3.1 based on configuration.

4 Evaluation

We have evaluated the performance of TEA, with
different versions of PIAM incorporated, in bug re-
pair using a small data set called Tufano. Figure 3
summarizes the evaluation results, in which Trans-
former, TEA1, TEA2, and TEA3 correspond to no

Shttps://github.com/SamLynnEvans/
Transformer.

8 Attention(Q, K,V)

= ar R
ooo = softmax Qk s+ W + PIAM + (1—a) Vv ai
AST Vax =>
oO fixed cod
ooo -- fe oes
CEG ¢ t t f
> i ENCODER §& DECODER
is |
gd inal ENCODER DECODER
ENCODER DECODER

DECODER
DECODER
DECODER

ENCODER
ENCODER
ENCODER

=>

ett ft
oe

Figure 2: Architecture of the TEA Prototype.

PIAM (i.e., only the baseline transformer is used),
version 1, 2, or 3 of PIAM in section 3.1 is incor-
porated in TEA. It is obvious from Figure 3 that
these four configurations of TEA fixed more bugs
(345, 417, 455, and 484 respectively) when the
incorporated PIAM captures more comprehensive
and granular information of the subject programs.

600
500
400
300
200
100

0
Transformer TEAI TEA2 TEA3

Figure 3: Comparision with different models.

5 Related Work

The advance in NLP, especially ML-driven NLP,
has also greatly improved the accuracy of transla-
tion tasks. This advance has influenced the auto-
matic program repair (Lutellier et al., 2020; Chen
et al., 2019). However, existing work on using NLP
technologies to tackle automatic bug fixing tasks
only considers a limited set of information of the
subject programs. In contrast, our work proposes to
collect and utilize different aspects of information
of software programs in improve the accuracy and
effectiveness of bug fixing.

There are many forms of representation having
been proposed for software programs, beside AST,
CFG, and DFG considered in this work. For exam-
ple, Wei Le et al. proposed a data structure called
Multiversion Interprocedural Control Flow Graph
(MVICFG) (Le and Pattison, 2014) for patch veri-
fication. This thread of work offer possibilities to


expand the types of information of software pro-
grams considered by the PIAM so that it can better
meet the needs of different program repair tasks.

6 Conclusion and Future Work

We have presented a prototype called TEA as the
first step towards proving our hypothesis that uti-
lizing different aspects of features of programs can
improve the accuracy and effectiveness of the trans-
former model from NLP in automatically fixing
bugs in software programs. Our preliminary ex-
periment demonstrates that the more aspects of
program features and the more granular of program
features considered, the better the transformer gen-
erally performs in repairing program bugs.

As next steps, we plan to train and evaluate the
TEA prototype with large-scale real world program
datasets, and conduct a more throughout experi-
ment to compare its performance with other ML-
driven program repair methods.

References

D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural
machine translation by jointly learning to align and
translate. arXiv preprint arXiv: 1409.0473.

T. Britton, L. Jeng, G. Carver, P. Cheak, and
T. Katzenellenbogen. 2013. Reversible debugging
software. Judge Bus. School, Univ. Cambridge,
Cambridge, UK, Tech. Rep.

Z. Chen, S. J. Kommrusch, M. Tufano, L. Pouchet,
D. Poshyvanyk, and M. Monperrus. 2019. Se-
quencer: Sequence-to-sequence learning for end-to-
end program repair. IEEE Transactions on Software
Engineering.

K. Cho, B. van Merriénboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Bengio.
2014. Learning phrase representations using rnn
encoder-decoder for statistical machine translation.
arXiv preprint arXiv: 1406.1078.

M. Dawson, D. N. Burrell, E. Rahim, and S. Brewster.
2010. Integrating software assurance into the soft-
ware development life cycle (sdlc). Journal of Infor-
mation Systems Technology and Planning, 3(6):49-
D3;

E. F. de Souza, C. L. Goues, and C. G. Camilo-Junior.
2018. A novel fitness function for automated pro-
gram repair based on source code checkpoints. In
Proceedings of the Genetic and Evolutionary Com-
putation Conference, pages 1443-1450.

C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer.
2011. Genprog: A generic method for automatic
software repair. [eee transactions on software engi-
neering, 38(1):54—72.

W. Le and S. D. Pattison. 2014. Patch verification via
multiversion interprocedural control flow graphs. In
Proceedings of the 36th International Conference on
Software Engineering, pages 1047-1058.

T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and
L. Tan. 2020. Coconut: Combining context-aware
neural translation models using ensemble for pro-
gram repair. In Proceedings of the 29th ACM SIG-
SOFT International Symposium on Software Testing
and Analysis, pages 101-114.

n

. Nielebock, R. Heumiiller, J. Kriiger, and F. Ort-
meier. 2020. Using api-embedding for api-misuse
repair. In Proceedings of the IEEE/ACM 42nd Inter-
national Conference on Software Engineering Work-
shops, pages 1-2.

R. Sennrich, B. Haddow, and A. Birch. 2015. Neu-
ral machine translation of rare words with subword
units. arXiv preprint arXiv: 1508.07909.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and L. Polosukhin.
2017. Attention is all you need. In Advances in
Neural Information Processing Systems, volume 30.
Curran Associates, Inc.

Y. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang,
and L. Zhang. 2017. Precise condition synthesis for
program repair. In 2017 IEEE/ACM 39th Interna-
tional Conference on Software Engineering (ICSE),
pages 416-426. IEEE.

J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L.
Marcote, T. Durieux, D. Le Berre, and M. Monper-
rus. 2016. Nopol: Automatic repair of conditional
statement bugs in java programs. [EEE Transactions
on Software Engineering, 43(1):34—55.
