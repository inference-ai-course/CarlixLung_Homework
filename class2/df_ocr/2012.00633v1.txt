Meta-Embeddings for Natural Language
Inference and Semantic Similarity tasks

Shree Charran R, Rahul Kumar Dubey, Senior Member IEEE

Abstract— Word Representations form the core component for almost all advanced Natural Language Processing
(NLP) applications such as text mining, question-answering, and text summarization, etc. Over the last two
decades, immense research is conducted to come up with one single model to solve all major NLP tasks. The major
problem currently is that there are a plethora of choices for different NLP tasks. Thus for NLP practitioners, the task
of choosing the right model to be used itself becomes a challenge. Thus combining multiple pre-trained word
embeddings and forming meta embeddings has become a viable approach to improve tackle NLP tasks. Meta
embedding learning is a process of producing a single word embedding from a given set of pre-trained input word
embeddings. In this paper, we propose to use Meta Embedding derived from few State-of-the-Art (SOTA) models
to efficiently tackle mainstream NLP tasks like classification, semantic relatedness, and text similarity. We have
compared both ensemble and dynamic variants to identify an efficient approach. The results obtained show that
even the best State-of-the-Art models can be bettered. Thus showing us that meta-embeddings can be used for
several NLP tasks by harnessing the power of several individual representations.

Index Terms— Meta Embedding, Natural Language Inference, Semantic relatedness, Textual similarity

1. INTRODUCTION

rguably one of the fundamental tasks in
An! language processing (NLP) is to

represent the meanings of individual words. If
one can correctly represent the meanings of individual
terms as some representations then we can further use
such representations to compose the meanings of
broader lexical units, such as sentences, paragraphs,
or even the entire document. To tackle this issue, over
the last decade the NLP community has proposed
numerous different approaches for learning word
representations/embeddings. However, all these
word embedding are learned using different methods,
thus they report varying degrees of success on various
tasks. Simply put, when it comes to word embedding,
there is no single method proposed so that solves all
NLP problems efficiently. The search for better word
embedding is never-ending and new methods for
learning word embedding are being proposed
frequently which capture different aspects of word
semantics and industry issues.

However, this plethora of choices can become a bit
of a challenge for NLP practitioners. How does one
choose the best embedding model out of several
available to train for a specific NLP application? Due
to time constraints or a sheer number of different word
embedding methods available, Trial and error don't

¢ Shree Charran R is with the Department of Management Studies, Indian
Institute of Science, Bengaluru 560076, and India (E-mail: shreer@tisc.ac.in)
¢ Rahul Kumar Dubey is with Robert Bosch Engineering and Business
Solutions Private Limited, Bengaluru, Karnataka 560095, India (E-
mail:rahul.dubey2011@ieee.org)

seem like a feasible solution.

Thus combining multiple  pre-trained word
embeddings and forming meta embeddings has
become a viable approach to improve word
representations. Meta Embedding can use several
good individual embeddings and ensemble them so
that they cover various aspects of word semantics thus
making it even richer and form a larger vocabulary
that provides an immense advantage in cross-domain
tasks. To be clear, Meta embedding learning is a
process of producing a single word embedding from a
given set of pre-trained input word embeddings. Two
key aspects make meta embedding learning problems
different and better from that of learning source
embedding.

Firstly, we do not re-train any of the source word
embedding learning algorithms during meta
embedding. One does not need to assume the
availability of the implementations of the methods that
were used to produce the source word embeddings.
We can directly use pre-trained source word
embeddings. This makes the meta embedding learning
algorithms independent of the source word
embedding learning algorithms, which is great
because we can use the same meta embedding
learning algorithm to meta embed any given set of
word embeddings.

Secondly, we do not assume the availability of the
text corpora that were used to train the source word
embeddings. This is attractive because sometimes all
that you get with source word embeddings is the pre-
trained vectors and not the language resources such as
text corpora or dictionaries that were used to train


ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS 2

those source word embeddings. This could be because,
for example, the language resources are proprietary
and the authors of the source embedding learning
algorithms are not allowed to release the language
resources they used. It also makes the meta embedding
learning algorithms attractive because they do not
require language resources during training. Moreover,
even if you had access to the language resources, it
might be computationally costly to re-train the source
word embedding learning methods on those language
resources by yourself, especially if you do not have
access to the hardware such as GPU units. In this
paper, we evaluate the performance of Ensemble and
Dynamic Meta-embeddings on Natural Language
Inference and Semantic Similarity tasks.

2 RELATED WORK

2.1 SENTENCE EMBEDDINGS:

Sentence embeddings is a single representation for
a sentence rather than a word. The first major
approach is Word embedding average sentence
encoders where it takes the weighted average of word
embeddings. The major drawback of this approach is
that word order and polysemy are compromised,
Hence particularly ineffective in sentiment analysis
and but effective in sentence similarity tasks[23]. The
second major approach is Complex contextualized
sentence encoders, such as Long Short Term Memory
Networks (LSTM) [24] or Transformers [25]
Contextualized encoders can be pre-trained as
unsupervised language models [26], but they are
usually improved on supervised transfer tasks such as
Natural Language Inference [21].

2.2 SENTENCE EMBEDDINGS EVALUATION:

With continuous developments of word and
sentence representations, there is a need for a
standardized evaluation oof representations.
SentEval[12], is an open-sourced toolkit available for
evaluating the quality of universal sentence
representations. [12] consists of a variety of tasks,
including binary and multi-class classification,
Natural Language Inference (NLI), etc. This work
aimed to provide a more centralized way to evaluate
sentence representations. In [13], the authors used the
evaluation tasks suggested by [12] to probe the quality
of major sentence representations to assess the state-
of-the-art performers in each category. ELMo [3], USE
[4] & Infersent [5] came out as the leaders in all ten
classes of assessment.

2.3 META-EMBEDDINGS:
The performance of the word embedding depends

on different factors such as the size and genre of
training corpora as well as the type of training

methodology used. This led to the development of
meta-embedding approaches from ensemble from pre-
trained embeddings[7][9][10][11].

Diverse approaches to learning meta-embedding
have been proposed and can be broadly classified into
two categories: (a) basic linear methods such as
averaging or concatenation, or low-dimensional
projection; [7] (b) non-linear framework [9][10][11]
which use auto-encoding or transformation to learn
meta-embedding as a shared representation between
each and a common representation

In [7], and effective a meta-embedding learning
framework namely 1TON was proposed. 1TON
projects a meta-embedding of a word into the source
embeddings using separate projection. These
projection matrices were learned by minimizing the
sum of squared Euclidean distance between the
projected source embeddings and the corresponding
source embeddings for all the words in the vocabulary.
The authors also proposed an extension (1ITON+) to
their meta-embedding learning framework that first
predicts the source word embeddings for out-of-
vocabulary words in a particular source embedding,
using the known word embeddings. Next, the 1TON
method is applied to learn the meta-embeddings for
the union of the vocabularies covered by all of the
source embeddings. Experimental results in semantic
similarity prediction, word analogy detection, and
cross-domain POS tagging tasks show the
effectiveness of both 1TON and 1TON+.

In [8], they proposed a geometric framework for
learning meta-embedding of words from various
sources of word embeddings. The geometric
averaging and geometric concatenation outperformed
the plain averaging and the plain concatenation
models.

In [9], the authors extended their work on
autoencoders and developed three variants i.e.
Decoupled Autoencoded, Concatenated Autoencoded
& Average Autoencoded. Of these, the Average
Autoencoded outperformed the other two and
baseline models

3 EMBEDDING METHODS

In this section we explore three ensemble
methods: Concatenation, Singular Value
Decomposition, Generalized Canonical Correlation
Analysis, and two Dynamic methods: Dynamic meta-
embeddings and Contextualized dynamic meta-
embeddings.

3.1 Concatenation (CON):

In Concatenation, the meta-embedding M is the
concatenation of all the individual embeddings Ei of
dimension |Ei| (i=1,2...n) or concatenation of select
individual embeddings. The main intuition behind
Concatenation is that: by combining different


ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

embedding sets, we can capture all of the information
in the individual embeddings.
Moo" = [E1; , E>; peeeys Enj ] (1)

The dimensionality of CON meta-embeddings
M,©™ is k. where,

k=) |Ei| (2)

The major drawback we need to keep in mind
while concatenating embeddings it becomes inefficient
as we combine more and more embeddings.

3.2. SINGULAR VALUE DECOMPOSITION (SVD):

The major disadvantage of concatenation as a
method for creating meta-embeddings is the curse of
dimensionality [1] proposed the use of SVD to reduce
the dimensionality of the meta-embeddings created
from concatenation. Given a_ set of CON
representations for n-words, each of dimensionality k,
we compute an SVD decomposition C = USVT of the
corresponding nxk matrix C. We then use Us, the first
d dimensions of U, as the SVD meta-embeddings of
the n-words. L2-normalization is performed to
embeddings; similarities of SVD vectors are computed
as dot products. SVD is guaranteed to produce the best
(in the sense of least square error) rank k
approximation of a matrix.

3.3 Generalized Canonical Correlation
Analysis (GCCA):

Given three or more random vectors x1,x2,x3;
GCCA finds linear projections such that
01 x,,03x2,0$x3 are maximally correlated. [14] showed
a variant of GCCA can be reduced to a generalized
eigenvalue problem on block matrices

a 0 0

p} oO > 0 |... (3)

0 0 >,

Oy Lay
“pid 0D @)
a

SD

SD

SD

S

where,
dy = Eses [(mj(s) — Hy) (M;(s) a By) | (5)
Hy = Eses[(M;(s)] (6)

For stability, we add pian diag(Xj,j-)n to diag(d; j-)
Where 7
eigenvectors of the top-d eigenvalues into matrices.

We define the GCCA meta-embedding of sentence s’
as:

is a hyperparameter. We _ stack the

MBs") =y)_,© (M,(s') — 4) (7)

As suggested by [26]The value 7 is set as 10
for best results.

3.4 DYNAMIC META-EMBEDDINGS (DME):

In DME, the embeddings are projected onto a
common d’-dimensional space using learned linear
functions

Mi; = Piwij + b; (i = 1,2.., n) where P; € R?** and bj
€ R* The combination of the weighted sum of
projected embeddings give

MRM = Raj Mi; (8)

where, a; = g({M ',,;}j=1 are the scalar weights from a
self-attention mechanism:

where a € R® and be R® are the learned parameters
and @is a softmax.

DME uses a mechanism that is dependent only on
the word projections themselves. Each word
projection gets multiplied by a learned vector a which
results in a scalar — so for N different projections
(which corresponds N different embedding sets), we
will have N scalars. These scalars are then passed
through the softmax function and the results are the
attention coefficients. Then these coefficients are used
to create the meta-embedding, which is the weighted
sum (weighted using the coefficient) of the projections.
Figure 1 helps visualize DME operations.

Shape: Nxd' dxt 1XN Nxd ind’

=n : | - SS =

FIGURE 1: DME EMBEDDING


ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

3.5 | CONTEXTUALIZED
EMBEDDINGS (DME):

DYNAMIC META-

CDME, adds context into the mix, using a
Sentence encoder. The difference from DME is in way
attention coefficients are calculated. Just like DME,
first, the sequences get projected, and then passed
through the Sentence encoder. Then, instead of using
the word itself, the concatenated hidden state of the
forward and backward LSTMs (Sentence encoder) (in
the word’s corresponding index) and the vector are
used to calculate the attention coefficient.

where hj € R2™ is the j'" hidden state of a BILSTM
taking (M';;) as input, a € R?" and b E R™. In [27]
authors suggest we take m = 2, which makes the
contextualization very efficient.

Sentence encoder : a standard bidirectional LSTM
encoder with max-pooling (BILSTM-Max), is used:

hj = LSTM,(w4,W2,....,W;) (11)

The hidden states are concatenated at each timestep
to obtain the final hidden states, after which a max-
pooling operation is applied over their components to
get the final sentence representation:

h= max{(hj, h,)}j=1.2..n (13)

Figure 2 helps visualize the CDME operations.

Shape: NxTxd NxTxds
orresponding to Concatenated hidden states (hi
Nxds did 1XN Nxd ind

= 2sS=

FIGURE 2: CDME EMBEDDING

4 EXPERIMENTAL SETUP

4.1 SOURCE EMBEDDINGS:

In this section, we explain in brief about the
selected pre-trained models which we will use as
source models:

4.1.1 EMBEDDINGS FROM LANGUAGE MODELS
(ELMo)[3]:

ELMo is a deep contextualized representation
of words that models both (1) complex word use
characteristics like syntax and semantic structure and
(2) polysemy modeling. Such word vectors are
learned functions of a deep bidirectional language
model 's internal states (biLM), which is pre-trained on
a large text corpus. ELMo can easily be applied to
existing models and greatly improve the state of the
art across a wide spectrum of difficult NLP issues,
including answering questions, classification textual
entailment, etc. We shall use the ELMo (BoW, all
layers, 5.5B) embedding whose model was trained on
a dataset with 5.5B tokens consisting of Wikipedia
(1.9B) and WMT (3.6B).

4.1.2 UNIVERSAL SENTENCE ENCODER (USE)[4] :

USE encodes text into vectors of large
dimensions that can be used for semantic similarity,
clustering, and other natural language tasks. Both
have accuracy and computational need trade-off.
Although the one with a Transformer encoder has
higher accuracy, it is more expensive in computational
terms. The one with DNA encoding is less costly
computationally and with comparatively lesser
accuracy.

TABLE 1
KEY POINTS OF THE SOURCE EMBEDDINGS
Model Training Embedding size
method
ELMo (BoW, all layers, | Self-supervised 3072
5.5B)
4096
InferSent (AIINLI) Supervised
USE (Transformer) Supervised 512

4.1.3. INFERSENT[5]:

InferSent incorporates supervised training to
obtain sentence embeddings. The sentence encoders
are trained using the Stanford Natural Language
Inference (SNLI) dataset, which consists of 570k
human-generated English sentence-pairs and it is


ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

considered one of the largest high-quality labeled
datasets for building sentence semantics
understanding. The authors found the best results
were achieved with a bi-directional LSTM (BiLSTM)
encoder.

In [6] the authors observed ELMo achieved the
best results in 5 out of 9 tasks downstream tasks. While
InferSent achieved the best results in the paraphrase
detection as well as in the SICK-E (entailment)., the
Universal Sentence Encoder (USE) using the
Transformer model achieved excellent results in the
semantic relatedness and textual similarity tasks. Thus
we consider these as the SOTA to use as source
embeddings. Table 1 gives the key points of
outsourcing embeddings

4.2 EVALUATION TASKS:

In this section, we explain the Natural
Language Inference and Semantic Similarity tasks
employed to evaluate the performance of the Meta-
Embeddings.

4.2.1 SENTENCES INVOLVING COMPOSITIONAL
KNOWLEDGE SEMANTIC ENTAILMENT/
RELATEDNESS (SICK_E & SICK_R)[15]:

It is a crowdsourced dataset consisting of
10,000 English sentence pairs. Each sentence pair in the
SICK dataset is annotated to mark the following two
aspects (i) the degree relatedness between a sentence
pair (on a scale of 1-5), and (ii) whether one sentence
entails or contradicts or is neutral the other
(considering both directions). We learn to predict the
probability distribution of relatedness scores
4.2.2. SEMANTIC TEXTUAL SIMILARITY

BENCHMARK (STS-B)[16-20]:

These datasets consist of sentence pair taken
from news snippets, headlines, news conversations,
headlines, image descriptions, etc. and is labeled with
a similarity score between 0 and 5. The task is to
evaluate how the cosine distance (similarity) between
two sentences correlate with a human-labeled
similarity score by using Pearson correlations. We use
STS tasks from 2012 to 2016.

LANGUAGE

4.2.3. STANFORD NATURAL

INFERENCE (SNLI)[21]:

The SNLI dataset [21] is a collection of 570k
human-written English supporting the task of natural
language inference (NLI), also known as recognizing
textual entailment (RTE) which consists of predicting
whether two input sentences are entailed, neutral or

contradictory. SNLI was specifically designed to serve
as a benchmark for evaluating text representation
learning methods.

4.2.4 PARAPHRASE DETECTION THE MICROSOFT
RESEARCH PARAPHRASE CorPus (MRPC)[22]:

This data - set consists of pairs of sentences that
were extracted from Web news sources. The Sentence
pairs are human-annotated depending on how they
can capture the relation of paraphrase / semantic
equivalence. It only has 2 classes, i.e. the purpose is to
predict whether or not the sentences are paraphrases.
Table 2 gives a sample of all the datasets used for
evaluation.

TABLE 2
Samples from all evaluation dataset
Name | Size Sentence Sentence Score
1 2
“A girlis | "A girl opens 1.6
SICK- | 10000 playing a package
R the that contains
guitar” headphones"
SICK- | 10000 | “A manis “There isno | Contradiction
E sitting on man sitting
a chair on a chair
and and rubbing
rubbing his eyes”
his eyes”
Liquid Liquid 4.6
8700 ammonia ammonia
STS-B leak kills leak kills at
15in least
Shanghai 15in
Shanghai
SNLI | 570000 | “Asmall | “The carousel entailment
girl is moving.”
wearing a
pink
jacket
is riding
ona
carousel.”
MRPC | 5700 “The “The paraphrase
procedure | technique is
is used during
generally the second
performed and,
in the occasionally,
second or third
third trimester of
trimester” pregnancy.”



ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS 6

4.3 EVALUATION METRICS:

4.3.1 COSINE SIMILARITY

Cosine similarity is a metric used to determine
how similar two embeddings (text). It is given as the
dot product of two vectors(A, B). Its denoted as :

Similarity(A,B) = = A.B

=———__ (14)
I All| BI

i=l

(15)

Where:

A, B are two token/sentence embedding in vector
form.

The Cosine similarity produces a score between 0-
1. Where 1.00 indicates the maximum similarity the
0.00 the least similarity. The SICK_R & STS-B dataset
is scored between 0-5. Thus we scale the cosine
similarity score accordingly.

4.3.2. PEARSON'S CORRELATION COEFFICIENT

(ro):

Pearson's correlation coefficient measures the
strength of the association between the two continuous
variables. In our case, it measures the strength of the
association between the cosine similarity scores of the
original dataset and the prediction.

It is given by the formula :

YX —nXY

i
i=l

Joa ~¥Y-7) (16)

Where:

e x,y are two sets of the mean of X & Y.
e Nis the sample size

4.3.3 ACCURACY:
Accuracy is a straight forward metric and is measured as
a percentage of total correct.

4.4 EXPERIMENTAL DETAILS:

a) All
individual

the Evaluations are conducted for

source embeddings and_ all
combinations of source embeddings.

b) The dimension of concatenated embeddings
will be the sum of individual embeddings.

c) The latest pretrained version of the Source
embeddings is used and indicated in Table 1.

d) The dimension of SVD & GCCA is set to 3072,
the median size of embeddings.

e) The value of 7 (GCCA) as 10

f) tuned on the STS Benchmark development

g) To
parameters

maintain uniformity the
for Logistic
considered for
suggested by [12]:

following
Regression is

classification tasks as

params['classifier'] ={'nhid': 0,
‘optim’: ‘adam’,
‘batch_size': 64,

‘tenacity’: 5,
‘epoch_size': 4}

h) We have conducted all experiments by
SentEval[18] standards.

5 RESULTS & DISCUSSION:

We have conducted the evaluation tasks initially for
the source models so that we have a benchmark for
performance. There are 4 combinations of source
embeddings possible (ELMo +USE+ InferSent , ELMo
+USE , USE+ InferSent, ELMo + InferSent). These 4
variations of source embeddings are fed to the
ensemble models and corresponding results are
tabulated.

Table 3: Results of the semantic relatedness and textual similarity tasks
(Pearson correlation coefficient)

sTs-12 | sTs-13 | STS-14 | STS-15 | STS-16 | STS-B SICK-R
Source ELMo 0.54 0.52 0.62 0.67 0.59 0.66 0.83
Embeddings | Usg 0.60 0.63 0.70 0.73 0.73 0.77 0.85



ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

InferSent 0.60 0.55 0.67 0.70 0.70 0.76 0.88
All 0.59 0.57 0.67 0.71 0.68 0.74 0.86
- ELM 61 al 7 73 0.73 0.78 0.88
CON @) 0.6 0.60 0.70 0
- USE 0.58 0.55 0.66 0.70 0.66 0.72 0.87
- InferSent 0.60 0.61 0.69 0.73 0.69 0.75 0.88
All 0.59 0.58 0.68 0.71 0.69 0.74 0.87
- ELMo 0.62 0.61 0.71 0.74 0.74 0.79 0.89
SVD
- USE 0.59 0.55 0.66 0.71 0.66 0.73 0.88
- InferSent 0.60 0.60 0.68 0.74 0.70 0.76 0.89
All 0.63 0.61 0.72 0.76 0.71 0.81 0.92
- ELM y 3 74 A a 81 0.94
GCCA () 0.65 0.63 0 0.78 0.75 0.8
- USE 0.63 0.61 0.72 0.76 0.73 0.79 0.92
- InferSent 0.63 0.64 0.73 0.78 0.73 0.79 0.93
All 0.67 0.70 0.78 0.81 0.81 0.85 0.92
- ELM vi / ah We We 84 0.90
DME 0 0.65 0.69 0.76 0.79 0.79 0.8
- USE 0.65 0.68 0.75 0.79 0.79 0.83 0.89
- InferSent 0.63 0.67 0.74 0.77 0.77 0.81 0.87
All 0.69 0.67 0.75 0.78 0.78 0.82 0.93
CDME - ELMo 0.65 0.69 0.76 0.79 0.79 0.84 0.90
- USE 0.65 0.68 0.75 0.79 0.79 0.83 0.89
- InferSent 0.63 0.67 0.74 0.77 0.77 0.81 0.87
1.00
0.90
0.80 1.00
0.70 0.90

0.60
a 0.50
0.40
0.30
0.20
0.10

STS- 12 STS- 13 STS- 14 STS- 15 STS- 16 STS-B SICK-R
Evaluation Tasks

lm Source Embeddings ELMo m Source Embeddings USE = Source Embeddings InferSent

Figure 3: Results of the semantic relatedness and
textual similarity tasks for the Source embeddings
(Pearson correlation coefficient)

As can be seen in Table 10, where we report the
results for the semantic relatedness and_ textual
similarity tasks, the source embedding Universal
Sentence Encoder (USE) using Transformer model
achieved excellent results on all five STS tasks, except
for the SICK-R (semantic relatedness) where InferSent
achieved better results. This is in line with the results
of [6]. Thus showing that the following results can be
used as a benchmark with which the Meta-
Embeddings can be compared with.

0.80

0.70

P O60 |

oso Hl

oo |

030 MINT
ao AM |)
| |

STS- 12 STS- 13 STS- 14 STS- 15 STS- 16 STS-B SICK-R

N

w
So

Evaluation Tasks

BCON All CON -ELMo CON - USE |CON - InferSent
BSVD All mSVD -ELMo WSVD - USE WSVD - InferSent
BGCCA All MGCCA-ELMo m®GCCA - USE BGCCA - InferSent

Figure 4: Results of the semantic relatedness and
textual similarity tasks for the Ensemble embeddings
(Pearson correlation coefficient)

From Table 3 it can be seen CON, the simplest
ensemble, has a robust performance. The all-CON
embeddings are the weakest variant of CON
Embeddings. This is solely due to the curse of


ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

dimensionality i.e. 3072+4096+512=7680 sized input
which is too many parameters. But the variant
ELMo+USE CON performs the best as its size of 3584
seems to be performed efficiently. The ELMot+USE
CON outperforms all the three source embeddings in
all semantic relatedness and textual similarity tasks.

The SVD beats the CON variant as expected. SVD
reduces CON's dimensionality dramatically and still
keeps competitive performance,
especially on word similarity. SVD maintains a
dimension of 3072. The USE+InferSent substantially
performs better than all previous variants. As its a
combination of two very good representations. It can
be seen that the correlation of the other variants is very
similar to each other this is because we have reduced
all the dimensions. Nonetheless, the process easily
beats the benchmark.

From Table 3 it’s clear that the GCCA is the best
ensemble approach. All four variants perform better
than the source embedding. This is due to the fact the
GCCA integrates information from data samples that
are acquired at multiple feature spaces to produce low-
dimensional representations. The best performing
GCCA setting was again the ensemble of
USE+InferSent. The USEt+InferSent =GCCA
outperforms all the ensembles in all the tasks emerging
as a clear winner. Figure 4 gives the comparison of all
the Ensemble settings on semantic relatedness and
textual similarity tasks.

Moving forward, it can be seen that the dynamic
variant, in general, performs better to the ensemble
models for the semantic relatedness and _ textual
similarity tasks. Its better performance is due to Bi-
LSTM architecture and SNLI training. The DME and
CDME variant performs similarly to the GCCA
ensemble. Unlike the ensemble models, the total
intersection of all the source embeddings performs
better to the subset variants. This is because the DME
and CDME efficiently capture the information across
more dimensions without the issue of dimensionality.
The ALL- DME performs best overall in five out of the
six textual similarity tasks. And CDME performs the
best for the semantic relatedness task. Figure 5 gives
the comparison of both the dynamic settings on
semantic relatedness and textual similarity tasks. The
results of the Natural Language Inference i.e. SICK-E
and MRPC dataset results are tabulated in Table 4.

1.00
0.90
0.80

0.60 }
a 0.50 } | | |
0.40 | |
0.30 | |
0.20 |
0.10 | | |

STS 12 STS 13 STS-14 — STS-15 ss STS- 16 STS-B SICK-R

Evaluation Tasks

= DME All DME - ELMo DME - USE © DME - InferSent
mCDME All mCDME - ELMo CDME - USE CDME - InferSent
mCON All m@CON - ELMo CON - USE |CON - InferSent
mSVD All mSVD - ELMo wSVD - USE WSVD - InferSent
BGCCA All @GCCA-ELMo =GCCA - USE BGCCA - InferSent

Figure 5: Results of the semantic relatedness and
textual similarity tasks for the Dynamic embeddings
(Pearson correlation coefficient )

Table 4: Results of the Natural Language
Inference tasks (Accuracy )
SICK-E MRPC
Source ELMo 80.16 69.33
Embeddings USE 79.99 70.89
InferSent 85.09 73.46
All 82.56 71.94
CON - ELMo 84.19 73.62
- USE 84.28 72.82
- InferSent 84.08 73.61
All 83.38 72.65
SVD - ELMo 85.02 74.34
- USE 85.10 73.54
- InferSent 84.88 74.31
All 90.74 79.06
GCCA - ELMo 89.10 77.64
- USE 88.29 76.93
- InferSent 88.88 77.82
All 89.35 77.14
DME - ELMo 87.56 75.59
- USE 86.67 74.82
- InferSent 84.88 73.28
All 90.20 77.87
CDME - ELMo 87.56 75.59
- USE 86.67 74.82
- InferSent 84.88 73.28



ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

90.00
80.00
70.00
60.

8

50.

8

Accuracy

40.

8

30.

8

20.

8

10

8

SICK-E MRPC

Evaluation Task

m Source Embeddings ELMo m Source Embeddings USE m Source Embeddings InferSent

Figure 5: Results of the Natural Language Inference for
the Source embeddings (Accuracy )

From Table 4 and Figure 5 it can be seen that the
Source embedding InferSent is substantially a better
performer than the other two in NLI tasks. Figures 6 &
7 provide a comparison for the NLI task on the
ensemble and dynamic Meta-Embedding.

100.00
90.00
80.00

70.00 |
60.00 :
50.00 |
40.00 |
30.00 ]
20.00 i
10.00 q

SICK-E MRPC

Accuracy

Evaluation Task

@CON All ™CON -ELMo CON - USE CON - InferSent

SVD AIl ™SVD -ELMo MSVD - USE SVD - InferSent

MGCCA All ™GCCA -ELMo = =M™GCCA - USE MIGCCA - InferSent

Figure 6: Results of the Natural Language Inference for
the Ensemble embeddings (Accuracy ).

For the Ensemble models both SVD and GCC
perform better than the SOTA models. This is largely
due to the better representation achieved by the
dimension reduction techniques. Even a naive method
of CON produces close to SOTA results especially
(ELMO+InferSent). The GCCA- All variant is the best
ensemble for NLI technique. The GCCA outperforms
the dynamic models too in the entailment task
displaying the immense power of dimension
reduction techniques. For the Dynamic models both
the contextualized DME performs better than the
DME, unlike the previous task. This shows the need
for a self-attention mechanism that is incorporated in
the CDME. The All CDME performs better than all the
remaining dynamic models.

We can finally conclude that the GCCA method
of Ensemble performs better than Concatenation and
singular value decomposition for Natural Language
Inference and Semantic Similarity tasks especially a
complete ensemble. For dynamic ensembles, there is
no clear winner as DME performs best on Semantic
Similarity tasks while the contextualized upgrade
performs better in NLI tasks.

100.00
90.00

80.00
70.00
60.00
50.00
40.00
30.00
20.00
10.00

SICK-E MRPC
Evaluation Task

Accuracy

= DME All m@ DME - ELMo DME - USE @ DME - InferSent

= CDME All ™ CDME - ELMo MCDME - USE @CDME - InferSent

Figure 7: Results of the Natural Language Inference for
the Dynamic embeddings (Accuracy )

6 CONCLUSIONS:

In this paper, we demonstrated how the use of
Meta-Embeddings can better the results of SOTA
models on standard NLI tasks. In this paper, we see
how the power of multiple individual representations
can be aggregated to form one strong representation.
For future work, we can _ utilize several other
representations and try to find the upper limit of
aggregation possible. Furthermore, we can try to see
the effect of Meta-Embeddings various other NLP
tasks and try to build a single powerful representation

REFERENCES

[1] Wenpeng Yin and Hinrich Schutze, "Leaming Word Meta-
Embeddings” Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics, pages 1351-1360, 2016.

[2] Joshua Coates and Danushka Bollegala. 2018. “Frustratingly easy
meta-embedding — computing meta-embeddings by averaging source
word embeddings’. In NAACL-HLT, pages 194-198, New
Orleans

[3] Tharindu Ranasinghe, Constantin Or‘asan and Ruslan Mitkov,
"Enhancing Unsupervised Sentence Similarity Methods with Deep",
Proceedings of Recent Advances in Natural Language Processing,
pages 994-1008, Varna, Bulgaria, Sep 2-4, 2019.

[4] D.Cer, Y. Yang, S. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant,
M. Guajardo-Cespedes, S. Yuan, C. Tar, Y. Sung, B. Strope, and R.
Kurzweil. “Universal sentence encoder.” CoRR, abs/1803.11175, 2018.
URL http://arxiv.org/abs/1803.11175

[5] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes.
“Supervised learning of universal sentence representations from
natural language inference data.” In EMNLP, pages 670-680.



ENSEMBLE META-EMBEDDINGS FOR NATURAL LANGUAGE INFERENCE AND SEMANTIC SIMILARITY TASKS

[6]

[8]

19]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

Association for Computational Linguistics, 2017.

Christian S. Perone, Roberto Silveira, and Thomas S. Paula. 2018.
“Evaluation of sentence embeddings in downstream and linguistic
probing tasks”. CoRR, abs/1806.06259.

Wenpeng Yin and Hinrich Sch"utze. 2016. “Learning meta-
embeddings by using ensembles of embedding sets.” In Proc.
of ACL, pages 1351-1360

Pratik Jawanpuria, N T V Satya Dev, Anoop Kunchukuttan, Bamdev
Mishra, “Learning GeometricWord Meta-Embeddings”,
arXiv:2004.09219

C. Bao and D. Bollegala. 2018. “Learning word meta-embeddings by
autoencoding”. In Proceedings of the 27th International Conference on
Computational Linguistics, pages 1650-1661.

D.Bollegala, K. Hayashi, and K. Kawarabayashi. 2018. “Think globally,
embed locally —locally linear meta-embedding of words”. In JCAI.

J. N. Coates and D. Bollegala. 2018. “Frustratingly easy meta-
embedding computing meta-embeddings by averaging source word
embeddings.” In Proceedings of NAACL-HLT 2018, pages 194-198.
Conneau, A., Kiela, D.: SentEval: An Evaluation Toolkit for Universal
Sentence Representations. In: Calzolari, N. etal. (eds.) LREC. European
Language Resources Association (ELRA) (2018).

Christian S. Perone and Roberto Silveira and Thomas S. Paula,
"Evaluation of sentence embeddings in downstream and linguistic
probing tasks”, CoRR,abs/1806.06259, 2018
http://arxiv.org/abs/1806.06259.

Bach, Francis & Jordan, Michael. (2003). Kernel Independent
Component Analysis. Journal of Machine Learning Research. 3. 148.
10.1162/153244303768966085.

Marelli, M., Menini, S., Baroni, M., Bentivogli, L.,Bernardi, R., and
Zamparelli, R. (2014). A SICK cure for the evaluation of compositional
distributional semantic models. In Proceedings of LREC

Agirre, E.,, Diab, M., Cer, D,, and Gonzalez-Agirre, A. (2012). Semeval-
2012 task 6: A pilot on semantic textual similarity. In Proceedings of
Semeval-2012, pages 385-393.

Agirre, E., Cer, D., Diab, M., Gonzalez-agirre, A., and Guo, W. (2013).
sem 2013 shared task: Semantic textual similarity, including a pilot on
typed-similarity. In*SEM 2013: The Second Joint Conference on Lexical
and Computational Semantics. Association for Computational
Linguistics.

Agirre, E.,, Baneab, C,, Cardiec, C., Cerd, D., Diabe, M., Gonzalez-
Agirre, A., Guof, W., Mihalceab, R., Rigaua, G., and Wiebeg, J. (2014).
Semeval-2014 task 10: Multilingual semantic textual similarity.
SemEval 2014, page 81.

Agirre, E., Banea, C., Cardie, C.,, Cer, D. M., Diab, M. T., Gonzalez-
Agirre, A., Guo, W., Lopez-Gazpio, I., Maritxalar, M., Mihalcea, R,, et
al. (2015). Semeval-2015 task 2: Semantic textual similarity, English,
Spanish, and pilot on interpretability. In SemEval@ NAACL-HLT,
pages 252-263.

Agirre, E, Baneab, C., Cerd, D, Diabe, M., Gonzalez- Agirre, A,
Mihalceab, R., Rigaua, G., Wiebef, J., and Donostia, B. C. (2016).
Semeval-2016 task 1: Semantic textual similarity, monolingual, and
cross-lingual evaluation. Proceedings of SemEval pages 497-511.
Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large
annotated corpus for learning natural language inference. In
Proceedings of EMNLP

Dolan, B., Quirk, C., and Brockett, C. (2004). Unsupervised construction
of large paraphrase corpora: Exploiting massively parallel news
sources. In Proceedings of ACL, page 350.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but
tough-to-beat baseline for sentence embeddings. In ICLR, Toulon,

[24]

[25]

[26]

[27]

10

France.

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term
memory. Neural computation,9(8):1735-1780.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
Attention is all you need. In NeurIPS, pages 5998-6008, Long Beach,
USA.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2019. BERT: Pre-training of deep bidirectional transformers for
language understanding. In NAACL, New Orleans, USA.

Douwe Kiela and Changhan Wang and Kyunghyun Cho.2018.
Dynamic Meta-Embeddings for Improved Sentence Representations.
Arxiv-e-print1804.07983
