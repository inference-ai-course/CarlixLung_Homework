arX1v:2510.09599v1 [cs.CL] 10 Oct 2025

@4 TECHNICAL REPORT

Prompting Test-Time Scaling Is A Strong LLM
Reasoning Data Augmentation
— 90 Samples Can Beat 1K in the Wild

Sondos Mahmoud Bsharat and Zhiqiang Shen*

VILA Lab, MBZUAI

* E-Mail: zhigiang.shen @ mbzuai.ac.ae (Correspondence)

Abstract

Large language models (LLMs) have demonstrated impressive reasoning capabilities when provided with
chain-of-thought exemplars, but curating large reasoning datasets remains laborious and resource-intensive. In
this work, we introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective inference-time data
augmentation strategy for enhancing LLM reasoning through finetuning. Rather than collecting thousands or
even millions of examples, P-TTS leverages a small pool of only 90 manually selected reasoning instances and
systematically varies exemplar augmentation through principled instruction prompting intensities at test time
to synthesize diverse reasoning trajectory contexts. Then we finetune the various sizes of Qwen-2.5 models
on P-TTS data. Across a suite of mathematical reasoning AIME2024 & 25, MATHS00, and GPQA-Diamond,
our P-TTS-7B and 32B models outperform the prior competitive baselines like S1 and $1.1 (1K-shot), achieving
absolute accuracy gains of +26.66% and +30.00% on AIME’ 24 (7B), and +13.34% and +6.67% on AIME’25
(7B); P-TTS-32B yields gains of +23.33% and +16.63% on AIME’ 24, and +26.63% and +3.33% on AIME’25
(vs. S1 and S1.1, respectively), with comparable or better performance on MATHS500 and GPQA-Diamond. We
further show that P-TTS enhances zero-shot generalization accuracy on out-of-domain reasoning benchmarks of
Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Ablation studies confirm that both
exemplar diversity and scaled sampling schedules are critical drivers of improvement. Our analysis suggests that
test-time scaling effectively explores the latent space of reasoning patterns, amplifying LLM problem-solving
with minimal annotation overhead, and further unlocking the reasoning potential and capabilities of LLMs.
Prompting Test-Time Scaling offers a practical, low-cost way to elicit LLM reasoning in resource-constrained
or rapidly evolving domains. Our code and data are available at https://github.com/VILA-Lab/PTTS.

Keywords: Prompting Test-Time Scaling; LLM Reasoning; Large Language Models; Principled Instructions;

80, AIME 2024 100.0 MATH500 GPQA-Diamond
64,
75) | P-TTS-32B Ri-pistill-qwen-328 27°?
-

70; 95.0 P-TTS-32B Saati wermsee R1-Distill-Qwen-32B
g OpenThipker-32B | oacaa _Bespoke-32B 62; OpenThinker-32B +
S65] Bespoke-32B 92.5; 8 P-TTS-32B
> y OpenThipker-32B * s
© 60 90.0 60]
3
Z 55] 87.5 38. Bespoke-32B

50 /535328 350) sel

1-32B %
Sky-T1 e
45) Sky-T1 82.5 y 56,
1K 17K 114K 800K 1K 17K 114K 800K 1K 17K 114K 800K
Dataset Size Dataset Size Dataset Size

Figure 1. Comparison of 32B-scale models on AIME 2024 (left), MATH500 (middle), and GPQA Diamond
(right). Model performance on AIME 2024, MATHS00, and GPQA-Diamond benchmarks as a function of dataset
size. Each point represents a different model, with our P-TTS-32B (red star) showing competitive performance from
a significantly smaller dataset. The x-axis scale highlights the differences in training data sizes across models.

Technical Report | October 13,2025 | 1-29


1. Introduction

Large language models (LLMs) [RNS*18, AAA*23, TAB*23, Ant25] attain strong deductive
and quantitative reasoning once equipped with curated chains of thought (Col) [WWS*22b]
or tool-augmented exemplars [MGH*24]. However, constructing thousand-scale reasoning
corpora is costly: it requires prompt engineering, human verification of multi-step solutions, and
continuous refresh to track dataset shifts in the wild. Moreover, static large-shot prompts are
brittle—fixed exemplars can inadvertently bias the model toward spurious solution templates
or fail under domain shift, limiting generalization despite high in-domain scores. Prior work
has largely scaled pre-/post-training time data (pre-training, instruction tuning, supervised
CoT) or model size, while inference-time strategies typically vary only in decoding parameters
(temperature, sampling) or rerank multiple outputs from a single prompt. The combinatorial
space of which exemplars to show, how to order them, and how to perturb them remains mostly
unexploited. We argue that the prompt itself is a stochastic control knob whose systematic scaling
at test time can simulate the effect of large reasoning datasets, without actually collecting them.

In this work, we propose Prompting Test-Time Scaling (P-TTS) as an LLM data augmen-

tation method: given a compact seed pool of just 90 high-quality math reasoning exemplars,
we algorithmically expand the prompt space at inference by 1) exemplar subsampling under
diversity constraints using various principled instructions [BMS23], 2) ordering perturbations
that modulate inductive biases (recency, primacy), and 3) pseudo-sampling of paraphrased
rationales and solution skeletons via the model itself. Each seed exemplar question is paired
with a prompt ensemble, a set of independently constructed prompt contexts, whose answers are
collected. This converts test-time prompting into a scalable augmentation pipeline.
Why can 90 beat 1K? The key insight of our framework is that, conventionally, a fixed 1 K-shot
sample or prompt provides only one (or a few) points in the prompt-combinatorial space, whereas
our P-TTS explores a far larger manifold of reasoning cues. From a bias—variance perspective,
prompt ensembles for the same question reduce variance in reasoning trajectories and increase
coverage of latent solution schemas. Information-theoretically, diverse promptings expose the
model to a richer set of conditional priors over intermediate steps, effectively approximating a
mixture-of-experts CoT without extra training. Empirically, we find that P-TTS surpasses the gap
to 1K-shot baselines across reasoning and zero-shot generalization on out-of-domain tasks, and
substantially improves robustness on naturally occurring, shifted test distributions.

We verify the effectiveness of P-TTS through two orthogonal dimensions: 1) Semantic/knowl-
edge diversity in Col responses — measuring how P-TTS expands the coverage of knowledge
and concepts. We compare the trade-off between accuracy and knowledge diversity gain across
four principled prompting strategies: Reward, Correctness, Penalty, and Think. 2) Language
trigram diversity — quantifying lexical and phrasing variety using distinct n-gram ratios and
entropy, applied both to full responses and isolated reasoning traces. Results show that reward
framing achieves the greatest lexical diversity, reflecting stronger surface-level variation from the
base examples. More broadly, we observe synergistic benefits: semantic/knowledge diversity re-
duces overfitting to narrow reasoning templates and enhances transfer; trigram diversity mitigates
lexical/template lock-in and strengthens robustness; ordering perturbations counter position bias
slightly; and our model-driven self-augmentation introduces novel but still on-manifold rationales.
Finally, scaling curves reveal diminishing returns beyond roughly 6 prompt-augmentations per
base question, suggesting this as a practical deployment point.

Contributions of this work. (1) We introduce Prompting Test-Time Scaling, a simple yet
effective framework for LLM inference-time reasoning data augmentation. (2) We demonstrate
that only 90 seed samples, when leveraged through our P-TTS, can outperform 1|K-shot static

Technical Report | October 13,2025 | 2


prompts, reducing curation cost by an order of magnitude. (3) We provide empirical evidence that
prompt-space exploration is an underutilized scaling dimension for LLM reasoning. We release
code and the augmented exemplar pool to facilitate reproducibility and facilitate rapid transfer to
new domains. Collectively, P-TTS reframes test-time prompting from a one-shot design choice
into a scalable, stochastic process, unlocking robust reasoning without additional data collection
or massive labeled datasets.

2. Related Work

LLM Reasoning. The growing availability of frontier autoregressive pretrained large language
models that expose long-form rationales (e.g., GPT-ol [JKL*24], Gemini [TAB*23], Claude
3.7 Sonnet [Ant25], DeepSeek-R1 [GYZ*25]) has integrated RL or SFT pipelines that explicitly
incorporate intermediate reasoning. Diffusion LLM [LCGS25, ZGZG25] also shows potential in
reasoning capability recently. In many open-sourced reasoning LLM settings [MYS*25, Bes25,
GMK* 25], a teacher model is usually prompted to produce chain-of-thought (CoT) explanations
for curated inputs, and the resulting (prompt, rationale, answer) triples supervise a student
model. For instance, BEsPOKE-STRATOS [Lab25] collects teacher-generated explanations from
DeepSeek-R1, while the OpenThinker models [GMK*25] train on the OpenThoughts corpora
built with rationales elicited from teachers such as DeepSeek-R1 or QwQ-32B. These methods
demonstrate that model-generated reasoning is a scalable supervision signal, but they typically
rely on tens to hundreds of thousands of exemplars and substantial collection costs. Our work
targets a complementary regime: we exploit instructional wrapping to elicit diverse, high-utility
rationales from only 90 seeds, yielding data efficiency competitive with 1 K-scale baselines.

Inference-Time Scaling. Orthogonal to parameter or dataset scaling [BMR*20, KMH*20],
inference-time strategies improve performance without updating model weights. Classic ap-
proaches tune decoding hyperparameters (temperature, top-k, nucleus sampling) to modulate
diversity [HBD* 19, FLD18, MPWC23], or generate multiple completions from a fixed prompt
and aggregate answers, as in few-shot CoT [_WWS*22b] and Self-Consistency [WWS* 22a].
Recent “think-more” style methods (e.g., sl/s1.1) adjust the reasoning budget—allocating
more deliberate tokens at test time—without altering exemplars or performing additional train-
ing [MYS*25]. We build on this line by treating the prompt itself as a first-class scaling axis:
P-TTS systematically varies instructional framing to create an ensemble of prompt contexts that
can be aggregated at test time or distilled into compact training sets.

Data Augmentation for Reasoning Tasks. A complementary line synthesizes new samples or
rationales to expand training corpora. METAMATH [YJS*23] bootstraps diverse math problems
via generate-and-verify loops, while REASONINGMIx [ZCH*25] composes traces by interleaving
steps across tasks. Such methods operate at the data level by creating novel items or reasoning
sequences. In contrast, P-TTS operates at the prompt level: it preserves the original problems
but injects controlled diversity through principle-guided wrappers (reward/penalty framing,
correctness emphasis, step-by-step cues), which we show can be leveraged both at test time
(ensemble prompting) and at training time (SFT over wrapper-elicited rationales).

Principled instructions and prompt engineering. General-purpose prompting frameworks catalog
instruction patterns that improve reliability and adherence [BMS23]. Our approach instantiates
a small subset of such patterns that are directly compatible with math-style reasoning (reward,
penalty, correctness, step-by-step) and formalizes them as deterministic wrap operators. This

Technical Report | October 13,2025 | 3


Rewards Variants

| con (Ri) Tconp (Flow (RD)

Reward: | am going to tip $200000 for a better solution! Find }
|! “SS.

the number of collections of 16 distinct subsets of .....
Feuer (Ri)

|

| :

| g Reasoning esponse

| a |
I

|

|

Full P-TTS

Question : Find the number of
collections of 16 distinct
subsets of {1,2,3,4,5} with the

Penalty: You will be penalized if you provide the wrong answer.
, Fow(Ri) FeeasonFracn(®)

FFind the number of collections of 16 distinct subsets of .....
mntima> P-TTS

|
“|
a
JI
Reasoning ning) gsvos
a
q
l
|
(‘Response | Fine-Tune
|
al
|
|
|
|
7!
|
|
JJ

Correctness: Your Task is to solve the following: Find the
number of collections of 16 distinct ..... You must provide the

property that for any two
subsets X and Y in the

collection, XN Y=. correct answer! (Reasoning Lgereore)
@ gy Pretrained
oe . Step-by-Step Reasoning: Think Step by Step: Find the number off Language
Original Data collections of 16 distinct subsets of .. Model

_————~
Reasoning | | Response

2. Find the number of collections of 16 distinct subsets of

——_——_— — SY

Figure 2. Overview of the P-TTS data augmentation process. Starting from a small set of high-quality math
problems (AIME-style), we generate diverse prompt variants through instruction reframing, such as reward-based
encouragement, penalty warnings, and step-by-step guidance. These augmented prompts are used to elicit high-
quality LLM completions, which are then collected as synthetic reasoning data to fine-tune.

yields a semantically invariant augmentation space with clear ablation handles (template choice,
placement, and paraphrase strength).

Low-Resource Supervision. Recent efforts show that carefully curated, small datasets can deliver
strong reasoning performance. S1 [MYS*25] and LIMO [YHX%*25] claim training on ~1K
high-quality, challenging prompts as a competitive alternative to massive corpora. Our results
complement these findings: with only 90 seeds, P-TTS converts principled prompt-space variation
into supervision that matches or surpasses 1K-shot baselines, highlighting prompt-level scaling
as a practical lever for low-resource regimes.

3. Methodology

3.1. Overview

We propose Prompting Test-Time Scaling (P-TTS), a reasoning-centric data augmentation
framework that expands a compact seed set via instructional wrapping. Rather than modifying
task semantics, P-TTS applies a family of fixed textual wrappers (“‘principles”) [BMS23] to each
seed example, producing prompt variants that preserve the original problem while modulating the
instructional framing (e.g., reward/penalty cues, correctness emphasis, or step-wise guidance).
Concretely, a principle p is realized by a template t, that deterministically wraps the raw question
q, yielding g‘?) = Tp || g (string concatenation ||). The union of the original prompts and their
principle-conditioned variants forms a P-TTS augmented corpus. Unlike large-scale supervision
or domain-specific curation, P-TTS relies on principle-guided prompt reformulation to elicit
high-quality reasoning traces from a teacher model, which are then used for supervised fine-tuning
(SFT) of a student. In Sec. 4.3 we show that individual principles already yield measurable gains
over the null prompt, and that training on the full P-TTS corpus (from only 90 seeds) can match
or exceed models fine-tuned on substantially more data (Table 8).

Technical Report | October 13,2025 | 4


3.2. Dataset Seed Selection Strategy

We adopt a seed-based construction paradigm [ZAD*25]: a small, vetted set of problems serves
as the substrate for systematic instructional variation. Our seeds comprise N =90 problems from
AIME 2022-20724, selected for (i) reasoning density across algebra, combinatorics, number
theory, geometry, and probability; (ii) format and label reliability (professionally authored
items with definitive three-digit answers); and (iii) contamination mitigation—restricting to
recent editions reduces overlap with widely scraped, earlier AIME corpora [HYS*25, AAA*23].
This compact, high-quality set enables controlled scaling via instructional wrappers without
compromising semantic fidelity.

3.3. Selection of Instructional Principles
We instantiate four core principles

Peore = {Reward, Penalty, Correctness, StepByStep},

chosen for their direct applicability to math reasoning and prior evidence of consistent gains
across model families [BMS23]. Each principle p is bound to a fixed template tT, and applied as
a wrap operator without modifying q’s tokens:

(p)
L

q; =Tp ll gi. (1)

This guarantees semantic invariance: removing the principled template 1, deterministically
recovers question g;. Table | summarizes the templates and induced operators.

Instructional Principle p Template t, (excerpt) Operator

Reward “IT am going to tip $200,000 for a better solution!” gltew) = Trew || g

Correctness “Your task is... You MUST...” gO cs ree || @

Penalty “You will be penalized if you provide the wrong q(Pen) = Tpen II 4
answer.”

StepByStep “Think step by step.” g"P) = Tstep || q

Table 1. Core P-TTS instructional wrappers. Each operator preserves problem semantics by concatenating a fixed
template to the unmodified q.

3.4. P-TTS Dataset Construction

Our primary dataset is drawn from the AIME benchmarks (2022-2024) [Art] and consists of
N = 90 unique problems with gold answers,

Oseed = {(qi, ayn, (2)

where gq; is the original seed question or problem without any additional prompting, and
a; € {0,1,...,999} is the integer-style ground-truth for the associated question. Oseeq is the
seed question-answer pairs.

Original question (null prompts, i.e., 2). | We first query a teacher model T (DeepSeek-R1 [GY Z*25])
on the unmodified question text, which we view as a null principle p = @ with

qs = qi. (3)

Technical Report | October 13,2025 | 5


(2)
L

(2)

(2)

where q; “ is its null-prompt form (unchanged). For each q;

(2)

the teacher returns a reasoning

trace r; ’ and a full response y;"’ yielding
N
Dseed = {(a\”. yi), 7), ai)} i (4)
l=
where A is the teacher-generated reasoning trace, 3” is the full teacher response, a; is the

ground-truth answer, and N = 90 is the total number of seed problems.

Selected/Core principle transformations. As shown in Sec. 3.3 and Table 1, we select four
principles as our core set. Each principle p € Peore is implemented as a deterministic operator
fp(-) that wraps the original text with a fixed instructional template t, while leaving the tokens
of g; unmodified:

gi = fp(qi) =Tp || qi_ (string concatenation ||). (5)

where gi ) is the wrapped question under principle p, and T, is the fixed instructional template.

This construction preserves the mathematical content, since removing T, recovers the original

question g;. Querying T with each qi? ) produces (ri? ) yl? ), and we collect

Deore = {(a\” 7), yi, aj)

l

i= 1,...,N5 p € Peore}. (6)

where 7? ) is the teacher reasoning trace, yl? ) is the teacher response, a; is the ground-truth

answer, N is the number of seed problems, and Pore is the set of four selected principles.

Reward framing variants for reward principle. Our single-principle ablation (Sec. 4.3) shows
that Reward Framing yields the largest accuracy gain among the core strategies. To test whether
this effect depends on exact wording, we create six paraphrased Reward prompts that vary in
incentive magnitude, placement, and phrasing strength.! Veeward = {R1,R2,R3, Ra, Rs, Ro}.
Applying each Reward variant to all N problems and querying T gives :

DReward = (4; rR yi? a; ) | P= Ipsan pj R; € Vaeward}- (7)

fi
where gis ) is the problem g; wrapped with Reward variant R; and VReward the set of six Reward
Framing variants. Table | summarizes the four core principles, and Table 2 lists the Reward
Framing paraphrases in full.

Prompt order impact. We also study how the ordering of prompts affects LLM responses and,
in turn, their reasoning ability. Specifically, we experiment with placing the additional prompt
either before (R1) or after (R4) the original question, as shown in Table 2. In general, positioning
prompts at the beginning leads the model to focus more on the prompt’s instructions and therefore
achieves better accuracy, as shown in Table 13. Intriguingly, we further notice that when the
question is relatively short, the difference in accuracy is actually small.

Dataset configurations and scale. We construct four dataset families from the same N seeds,
distinguishing single—principle sets from their unions:

1s opendix D reports model accuracy across five Reward Framing variants (R1—RS5) that differ in reward amount (e.g., $20 vs. $200,000) and location (e.g., beginning
vs. end of prompt). Results are presented for ol-mini, Gemini, and DeepSeek.

Technical Report | October 13,2025 | 6


Variant ID Incentive Phrase Transformation

R1 I am going to tip $200,000 for a better solution! I(R})

R2 I am going to tip you $200,000 if you provide the correct answer! Tconv (R11)

R3 I am going to tip you $200 if you provide a better solution! Tconp (FLow (R1))
R4 Add at the end of the prompt: I am going to tip $200,000 for a better solution! Tsureix(R1)

RS5 I am going to tip $200 for a better solution! FLow(R1)

R6 I am going to tip you $500,000 for a better solution and step-by-step reasoning! JReason(TaicH(R1))

Table 2. Reward Framing variants in P-TTS. Each variant is generated from the baseline R; via an operator:
R; = 7;(R1), where £ denotes identity. Operator definitions: Sconp : C (add conditionality); FLow : M|
(reduce magnitude in USD); Taicu: MT (increase magnitude in USD); Tsurrrx : T=Suffix (change placement);
TREASON: P#M (add reasoning cue, e.g., step-by-step).

(i) Single-P-TTS (per principle). For each p € Peore we build a separate dataset

single _ (Pp) \(p) (Pp) *) NV single} _ y7 _
Diy = {(4;"" 93", 73", aj) Faas loo | =N=90.

Thus, there are four Single-P-TTS datasets (one per principle). When we report “Single”,
we train one model per p (and report results per p or their mean, as specified in Sec. 4.3).

(ii) Core-P-TTS (union of singles). The core set is the (disjoint) union over all four principles:

Deore = |_| on, Deore =4N = 360.
PEP core

(iii) Seed combined with the core P-TTS. We add the null-prompt (seed) Deeg and DRewara to
obtain
Daseed+core = Daeeed U Deores |Dseedxvore! = 5N = 450.

(iv) Full P-TTS. Let Vreward denote the set of reward paraphrases K total variants applied to
all seeds, producing Drewara. Because one variant is already used in the Core set, the
additional Reward set has size (K — 1)N. The full corpus is

Deun P-TTS = Dseed U Deore U DrReward> |Deutt P-TTS| = (1 +4+ (K _ 1))N.
In our experiments we use K=6, so |Dguti p-rrs| = 1ON = 900.

We parameterize the corpus size by the augmentation multiplier m := |D|/N, where D is
the training corpus. In our study |D| € {90, 360, 450, 900} with N=90, so m € {1,4,5, 10},
corresponding to Single, Core, Seed+Core, and Full, respectively, enabling controlled comparisons
as a function of prompt diversity (Fig. 3).

3.5. Fine-Tuning with P-TTS Dataset Augmentations

We evaluate whether principle-guided wrapping improves supervised reasoning under constrained
data. We fine-tune Qwen2.5-Instruct (7B/14B/32B) [YLY*25] separately on each configuration
from Sec. 3.4, following an SFT recipe adapted from sl [MYS*25]. The student is trained to
predict full assistant outputs (reasoning + answer) with token-level cross-entropy computed on
assistant tokens only (user tokens masked). Each dataset configuration (Original, Single, Core,
Mix, and Full P-TTS) is used to train a separate model, enabling us to isolate the contribution of
each prompting strategy and the effect of data scaling, i.e., isolating the impact of instructional
diversity and corpus scale (m € {1,4,5, 10}) while keeping optimization and decoding fixed
across runs.

Technical Report | October 13,2025 | 7


Algorithm 1: Prompting Test-Time Scaling (P-TTS) Dataset Construction

Input: Seeds Oseeq = {(Gi, a ii 5 core principles Prore; reward variants Veewara; teacher
T
Output: Augmented dataset D fullP-TTS

<_
Di up-rrs 0

foreach (gj, 4;) € Oseeq dO
Query T with g; to obtain (r'”, y(™)

Add (gir, yy, di) tO Dy p-rrs

foreach p € Peore do
= Tp II Gi // wrap; preserve g;

2)
qj
Query T with qe ) to obtain (rt? ) ye ))

| Add (9)? 7)”, ¥;”,ai)

foreach R; € Vpeward do
(Rj)

qj <—_ TR; I qi

Query T with qh ’ to obtain cr a ys ay

| Add (g\"?, r\®?, ya)

return
ety D fullP-TTS

Models are trained to predict the full assistant output—reasoning trace and final answer. Our
dataset scale (90 — 900 examples) is intentionally small, allowing us to directly measure how
principle-guided prompt reformulations affect supervised reasoning performance relative to
models trained on much larger datasets.

4. Experiments

4.1. Experimental Setup

Datasets. We evaluate our P-TTS models on four public reasoning benchmarks: AIME24 [AIM24]
(30 problems) and AIME25 [AIM25] (15 problems) from the American Invitational Mathematics
Examination; AIME includes problems from algebra, arithmetic, geometry, number theory, com-
binatorics, and probability. MATH500 [HBK*21] is a 500-problem competition-math subset;
we adopt the publicly released OpenAI selection used in prior work. GPQA-Diamond [RHS* 24]
contains 198 PhD-level science questions from Biology, Chemistry, and Physics with re-
ported expert performance of 69.7%. We evaluate using the 1m-evaluation-harness frame-
work [GBB*21, BSS*24]. To make results comparable across models and ablations, we disable
sampling by setting the temperature to 0 (greedy decoding), so each input yields a deterministic
output. Reported scores are accuracy (equivalent to pass@1). In addition to these four core
benchmarks, we further assess cross-domain and multilingual generalization using a broader set
of reasoning tasks spanning Chinese exams, U.S. school math, olympiad-style problems, and
scientific quantitative reasoning. These evaluations, shown in Table 9, include Gaokao, Kaoyan,
OlympiadBench [HLB* 24], AMC23, GradeSchoolMath, and Minerva.

Baselines. We benchmark P-TTS against three categories of reasoning models. (i) Closed-source
(API-only) models: OpenAI’s o1 series [Ope24, Ope25] and Google’s experimental Gem-
ini 2.0 Flash Thinking variant [Clo24]. (ii) Open-weight models: DeepSeek-R1 series

Technical Report | October 13,2025 | 8


[GYZ*25] and Qwen’s QwQ-32B-preview [Qwe24, YLY*25] . (iii) Open-weight SFT mod-
els on Qwen2.5-Instruct with public data on openly available reasoning corpora: including
Bespoke-Stratos-32B [Bes25], OpenThinker-32B [Tea25b, GMK*25], Sky-T1-32B-Preview [Tea25al],
and the S1/ $1.1 w/o BF [MYS*25]checkpoints.

Diversity Metrics. We compute two complementary metrics—semantic and surface—level—to
quantify how each single-principle variant in Deore adds information beyond the seed set (Dseeq).
Semantic diversity (Diversity Gain). Following [YJS*23, Bil22], we compute diversity gain
to quantify knowledge-level novelty. Given a seed dataset Dyeeq and a new dataset Deore, We
define DG = W Lixj€ Deore MINX /E Decca | f (xi) — f(x Alb where f(-) is an embedding function and
M = |Deore|. We use OpenAl’s text-embedding-ada-002 as f for feature extraction. Higher
values indicate greater semantic divergence from the base data. Surface—level diversity (trigram
diversity). We compute trigram diversity [LYH* 22], defined as the ratio of non-overlapping
trigrams between two texts”. We average this score over all sample pairs between each P-TTS
principle variant in Deore and its corresponding baseline instance in Deeea.

Effect of Scaling Principle-Based Augmentation on Model Accuracy

804
70 4
@ GPQA-Diamond
6 m AIME24
sS A AIME25
2 50 @ MATHS00
5
U
U
< 40
304
204
Tf)
x1 x4 x5 x10

Augmentation Multiplier (x)

Figure 3. Accuracy improvement with increased principled augmentation on 7B model. We evaluate how model
accuracy scales with the number of augmented training examples. Here, x1 refers to P-TTSrewara (90 examples), x4
to P-TTScore (360 examples), x5 to P-TTScore+orig (450 examples), and X10 to P-TTS-y11 (900 examples). Accuracy
improves consistently across all evaluation sets with larger, principle-guided augmentations.

4.2. Teacher Model for Data Construction

Our objective is to construct a compact yet high-quality mathematical reasoning corpus for
supervised fine-tuning. Specifically, we aim to identify the possible smallest training corpus that
still yields the highest downstream accuracy. To obtain both full responses and explicit reasoning
traces, we consider large language models that expose chain-of-thought generation through their
public APIs. We benchmark three reasoning models: Claude-3-Opus, DeepSeek-R1, and OpenAI

| Tri(x) ATri(y)|

2 For texts x, y, TD(x, y) =1- MITEIIENTIODIE where Tri(x) denotes the set of distinct word-level trigrams in x.

Technical Report | October 13,2025 | 9


Omni-4 on four tasks. Claude-3-Opus and DeepSeek-R1 natively return aligned answer—reasoning
pairs, while Omni-4 requires an augmented prompt to elicit full reasoning. From each model, we
collect 90 answer—reasoning pairs and fine-tuned a Qwen?2.5-7B-Instruct on the resulting dataset.
Table 12 reports accuracy averaged over all four benchmarks. The Qwen2.5-7B-Instruct model
fine-tuned on DeepSeek-R1 outputs consistently outperforms counterparts trained on Claude
and Omni-4 outputs under the same small corpus. Based on this, we adopt DeepSeek-R1 as the
teacher for all subsequent data construction.

35
—e— Accuracy —e— Accuracy
66) , --=-. Diversity Gain [9.23 34] 4 -=- Diversity Gain [9.23
x (os : ic
_ 64 0.22.5. 32 0.225
° ES) = ve
= 2 2
> v  >31 g
3 62 0.216 & 0.216
= %
5 o 5 30 o
o an Oo a
< B 229 3
60 0.20 2 0.20 2
(a 28 S
--- “ ood sae “
xe yer 0.19 27 eo “a bo.1g
6.
P_TTSreward P_TTScorrectness P_TTSpenaity P_TTS think PoTTSrewara P_TTScorrectness P_TTSpenaity P_TTSthink
P_TTS Models P_TTS Models
(a) MATHS00 (b) GPQA-Diamond
30 0.24 30 0.24
—e— Accuracy —e— Accuracy
-=- Diversity Gain --#- Diversity Gain
257 » 0.23 ¢ 25] 4 0.23 ¢
\ oO \ o
() (0)
=> 20 0.222 +20 0.222
> & 2
wo 15 0.21qQ «615 0.214
= fi
: BS 8
G ne] G xe]
10 0.20% 10 0.200
= =
° °
(a (a
5 0.19% 5 0.19%
0, 0.18 0. 0.18
P_TTSreward P_TTScorrectness P_TTSpenaity P_TTS think P_TTSreward P_TTScorrectness P_TTSpenaity P_TTSthink
P_TTS Models P_TTS Models
(c) AIME24 (d) AIME25

Figure 4. Knowledge Diversity Gain vs. Accuracy for different P-TTS variants across four benchmarks. We
compare the trade-off between Accuracy (blue solid line, left y-axis) and Knowledge Diversity Gain (gray dashed
line, right y-axis) on 7B model for four principled prompting strategies: Reward, Correctness, Penalty, and Think.
Diversity Gain is computed relative to the original P-TTS baseline.

4.3. Ablation Studies

4.3.1. Single-P-TTS: Measuring the Impact of Each Principle

Single-P-TTS. To assess the impact of each principle independently, we fine-tune separate
Qwen2.5-7B-Instruct models on the Single P-TTS subsets—90 examples per model—each
applying only one core principle p € Pore. We compare these models to a baseline trained on the
same seed problem without any instructional framing (P-TTSseeq). As shown in Table 3, all Single
P-TTS variants outperform the baseline on average. The Reward-based model (P-TTSpewara) yields
the highest overall gain (+6.67%), improving accuracy across all benchmarks. Penalty-based
model (P-TTSpenaity) also delivers strong results, especially on MATHS00, though with a drop

Technical Report | October 13, 2025 | 10


on AIME25. Correctness-based model (P-TTScorrectness) Offers modest improvements, while
Think-based model (P-TTStpink) increases the average but underperforms on MATHS00. These
findings highlight that even minimal augmentations, 1.e., just 90 principle-guided examples, can
yield measurable improvements. Among the four principles, Reward and Penalty framings are
the most effective when applied independently.

Model #Ex. AIME-24 AIME-25 MATHS500 GPQA-D Avg.
Single-Principle P-TTS Ablation (90 Examples Each)
P-TTSseeq (baseline) 90 3.33 6.67 60.20 27.78 24.50
P-TTSreward 90 13.33 13.33 65.20 32.83 31.17
P-TTScorrectness 90 3.33 6.67 61.80 28.28 25.02
P-TTSpenalty 90 13.33 0.00 63.20 30.30 26.71
P-TTSthink 90 6.67 13.33 58.20 30.81 2725
Pairwise Principle Ablation Centered on Reward Framing (180 Examples Each)
P-TTSRewarduPenalty 180 10.00 20.00 75.20 32.32 34.38
P-TTSRewarduCorrectness 180 23.33 20.00 75.40 37.37 39.02
P-TTSpewardUThink 180 13.33 13.33 72.80 31.82 32.82

Table 3. Accuracy (%o) of Single and Pairwise Principle Ablation using 7B model. Top: Single-principle P-TTS
variants each trained on 90 instructional prompts. Bottom: Pairwise ablations centered on Reward framing, trained
on 180 prompts combining Reward with one other principle. All models use Qwen2.5-7B-Instruct fine-tuning.

Pairwise-P-TTS. To further evaluate the usefulness of each principle and whether they can be
effectively combined, we explore the case where a principle that is not highly effective on its
own might still contribute positively when paired with another. Specifically, since P-TTSrewara
shows the strongest performance in the Single P-TTS ablation, we fix the Reward framing and
incrementally add one additional principle at a time, resulting in datasets of 180 examples.
Table 3 shows the performance of each pairwise combination. We observe that combining
Reward framing with either Correctness or Penalty significantly boosts performance across
most benchmarks, particularly on MATHS500 and AIME2024. Also, the P-TTSRewarduCorrectness
combination achieves the highest overall accuracy (39.02%), suggesting a synergistic effect
between reward framing and correctness emphasis. In contrast, combining Reward framing with
Thinking yields only modest improvements, indicating diminishing returns when both principles
primarily influence the reasoning process.

Core-P-TTS. To assess the relative importance of each principle when used in combination, we
conduct a leave-one-out ablation over the full Core P-TTS dataset Deore (360 examples). We
fine-tune a separate Qwen2.5-7B-Instruct model after removing one principle at a time, reducing
the training set to 270 examples. As shown in Table 4, the exclusion of the Reward framing leads
to the largest performance drop (from 39.06% to 35.40%), confirming its central role in driving
improvements. Removing Correctness or Penalty framing causes moderate degradation, while
the absence of Step-by-Step thinking has minimal or slightly positive effects. These results
indicate that Reward-based cues are the most impactful when principles are used in combination,
whereas Step-by-Step prompting contributes the least in multi-principle settings. Fig. 5 further
supports this finding, showing that the incremental addition of principles leads to consistent
accuracy improvements.

4.3.2. Measuring the Effects of Augmentation Size

Table 5 presents the results for three configurations: (1) P-TTScore with 360 examples, (2)
P-TTScore+seeqd With 450 examples, and (3) P-TTScore+seed+Rewardvar With 900 examples, which

Technical Report | October 13,2025 | 11


Model #Ex. AIME2024 AIME2025 MATHS00 GPQA-D Avg.

P-TTScore (all 4) 360 20.00 20.00 80.40 35.86 39.06
P-TTScoie\ Reward 270 16.67 13.33 78.80 32.83 35.40
P-TTScore\Correctness 270 13.33 20.00 79.80 35.86 37.24
P-TTScore\ Penalty 270 20.00 20.00 79.80 34.34. 38.53
P-TTScore\ Think 270 20.00 26.67 78.20 32.32 39.29

Table 4. Leave-one-principle-out ablation. We fine-tune Qwen2.5-7B-Instruct on the full Core-P-TTS set
(Reward+Correctnesst+Penalty+Think; 360 prompts) and then re-train after omitting one principle (270 prompts).

represents the full dataset (core, seed, and paraphrastic reward variants). Performance improves
consistently with dataset size: the model trained on the full 900-example dataset achieves
an average accuracy of 49.03%, outperforming all other configurations and surpassing the
1k-example S1.1 baseline (38.99%). The largest average gain occurs between 450 and 900
examples (+6.23%), with improvements on all benchmarks. The increase from 360 to 450
examples is particularly notable on AIME24 (+13.33%), indicating that mixing seed and wrapped
questions with the selected core principles is beneficial. Fig. 3 shows the same trend: scaling
augmentation from x1 to X10 yields gains across all four test sets, especially on MATHS00 and
AIME?24. These results demonstrate that principled prompt augmentation scales effectively and
enables competitive performance with far fewer training examples than traditional supervised
fine-tuning.

Performance Impact of Incremental Principle Addition

50 400
—e Average Accuracy
Num. Examples L350
45
_ 7300 |,
& Lv
~ 40 Q
o 250 €
S x
a Lu
9 35 200 —
x °
a oD
© 1502
3g 30 E
< Zz
, L00
25
50
20 0

P6 +P10 +P12 +P9
Cumulative Prompting Principles

Figure 5. Impact of incremental principle addition on average accuracy. As additional prompting principles are

cumulatively incorporated into training (P6 — +P10 — +P12 — +P9), both the number of training examples and

model accuracy increase. Bars (right axis) denote the total number of examples after each addition; the green line

(left axis) shows the resulting average accuracy across evaluation benchmarks. This highlights the compounding

benefit of principled augmentation.

4.4. P-TTS Dataset Analysis

Diversity. The accuracy results in Table 3 show that each Single P-TTS variant outperforms
the seed baseline on most benchmarks and on average. P-TTSprewara yields the largest absolute
gain, improving performance by approximately 6.7% on AIME25 and 5.0% on GPQA-D1amonb,

Technical Report | October 13, 2025 | 12


Model #Ex AIME24 AIME25 MATHS00 GPQA-D Avg.

Qwen?2.5-7B-Instruct (base) - 13.33 6.67 76.40 36.36 33.19
S1-7B 1K 16.67 13.33 77.20 41.41 37.15
S1.1-7B 1K 13.33 20.00 81.20 41.41 38.99
P_TTScore-7B 360 20.00 20.00 80.40 35.86 39.07
P_TTScCore+ Seed-7B 450 33.337 20.00 81.007 36.87! 42.807
P-TTScore+ Seed+ Rewardvar-7B 900 43.337 26.67! 84.207 41.927 49.037

Table 5. Data-volume ablation. Accuracy (%) when fine-tuning (i) on Core P-TTS only (360 prompts), (ii) Core+
Seed (450), and (iii) Core+ Seed+ six Reward—variant prompts (900). Best values in each column are bold; T marks
a gain over the immediately preceding configuration.

while P-TTSpenalty achieves the strongest improvement on MATHS500. Even the weakest variant,
P-TTScorrectness, Matches or surpasses the baseline on three out of four datasets. These accuracy
trends correlate with the diversity analysis in Fig. 4. Single P-TTS variants with higher
semantic diversity (Diversity Gain), such as P-TTSReward and P-TTSpenalty, also exhibit larger
accuracy improvements. Table 6 complements this by quantifying surface-level diversity (trigram
diversity): P-TTSpewara attains the highest scores across both final responses and reasoning traces,
consistent with the observed performance gains. This pattern is consistent with prior findings that
semantically diverse supervision and diversity-promoting augmentations improve generalization
and robustness [BMZ*24, QSS*20].

Prompt Variant Responses | Reasoning
Reward Framing 0.8363 0.9280
Correctness Framing 0.8227 0.9264
Penalty Framing 0.8223 0.9266
Step-by-Step Thinking 0.8295 0.9254

Table 6. Trigram diversity of teacher outputs under different prompting strategies. We report average lexical
diversity (+) for full responses and isolated reasoning traces. Reward framing yields the highest diversity, indicating
stronger surface-level variation from original examples.

Response Tokens Reasoning Tokens
Prompt Variant Mean Min Max Mean Min Max
Seed 579.6 314 950 11,872 1129 31604
Reward Framing 597.5 376 1,029 12,436 1745 34509
Correctness Framing 575.7 322 962 10,463 1420 34514
Penalty Framing 583.1 268 962 11,321 1133 29723

Step-by-Step Thinking 577.6 312 936 11,113 1343 28270

Table 7. Teacher output verbosity across prompting strategies. We report token count statistics (mean, min,
max) for both full responses and extracted reasoning traces produced by the teacher model across single-principle
prompt variants. These numbers reflect raw outputs used to construct the training data and illustrate how different
instructional cues influence explanation length.

Prompting Impact on Output Length. To further understand how each principle influences
model behavior, we compare the token lengths of generated responses and reasoning traces
in Table 7. We observe that Reward framing consistently produces longer and more detailed
reasoning outputs, which aligns with its observed accuracy gains. While longer generations
may imply increased inference costs, the improved reasoning quality may justify this tradeoff in
high-stakes tasks.

Technical Report | October 13, 2025 | 13


Question

If the Sun’s absolute magnitude is +5, find the luminosity of a star of magnitude 0 in
ergs/s. A useful constant: the luminosity of the sun is 3.83
times 10°> ergs/s

Response

Okay, so I need to find the luminosity of a star with magnitude O, given that the
Sun’s absolute magnitude is +5 and its luminosity is 3.83 x 10°? ergs/s. Hmm, let’s
start by recalling the relationship between magnitude and luminosity. First, absolute
magnitude given as +5. The star we’re interested in has an absolute magnitude
of 0. The difference in magnitude between the two stars is 5 - 0 = 5 magnitudes.
In astronomy, the difference in magnitudes relates to the ratio of luminosities. The
formula is: ml - m2 =-2.5 log10(L1/L2) indeed m1 - m2 =-2.5 log10 (L1/L2).
So if we know the difference in magnitudes, we can find the ratio of luminosities. In our
case, ml (Sun) is +5, m2 (star) isO. Soml -m2=5-0=+5. Plugging into the formula:
5 = —2.5 logj9(Lsun/Lstar) Wait, hold on. Let me make sure I have the formula right.
Then LI is luminosity of Sun, L2 is luminosity of star. So ml - m2 =5 -

0 =+5. Therefore: 5 = —2.5 log;g(Lsun/Lstar) Let me solve for log)g(Lsun/Lstar):
Divide both sides by -2.5: 3 = 108i g( Leun/ Leer) Which 1§ —2°= 108) 9(leunj Lester)
Therefore, Lisn/ Lear = 10-2 = 0.01 So Letar = Leun {0-01 = Lon & 100 Therefore,
is 3.83 x 107° x 100 = 3.83 x 10*° ergs/s. Wait, let me check that calculation

ASAI, secur: Yep, same result. Therefore, the answer should be | 3.83 x 10° ergs/s.

Final Answer | 3.83 x 10° |.... Alternatively, since each 5 magnitudes corresponds to
a factor of 100 in luminosity, a difference of 5 magnitudes (from +5 to 0) means the star
is 100 times more luminous than the Sun. So again, 3.83 x 10°? x 100 = 3.83 x 10°
ergs/s. Therefore, the luminosity of the star is 3.83 x 10*° ergs/s. Final Answer

383 x 10>

Figure 6. Example of a P-TTS-32B response. See Appendix F for comparison with baseline model outputs.

4.5. Main Results

Overall Performance Comparison. We compare our P-TTS-32B model, trained on just 900
principle-augmented examples, against a diverse set of competitive baselines, including closed-
source APIs, open-source models, and open-weight instruction-tuned variants of Qwen2.5-32B.
While closed models like 01-preview and 01-mini perform strongly on certain tasks, P-TTS-
32B outperforms them on AIME2024 and MATHS00, and delivers competitive results on
GPQA-Diamond despite using less data. Compared to open-weight models that finetuned on
Qwen2.5-instaruct-32B like OpenThinker-32B (114K examples) and Bespoke-32B (17K), our
model achieves superior or comparable performance across all benchmarks. P-TTS-32B achieves
an average accuracy of 70.35%, exceeding all open-weight instruction-tuned baselines, and
narrowing the gap with large-scale models like DeepSeek-R1, which require over 800K training
examples. These findings highlight the efficiency of principled instructional data and demonstrate

Technical Report | October 13,2025 | 14


Model # Train Size AIME2024 AIME2025 MATHS500 GPQA-Diamond Avg

closed-source models

ol-preview[Ope25] - 56.7 - 85.5 78.3 -
ol-mini [Ope25] - 63.6 - 90.0 60.0 -
Gemini 2.0 Flash Think = 60.0 - - - =
open-source models
Qwen2.5-32B-Instruct [Y YZ* 24] - 26.7 - 84.0 49.0 -
QwQ-32B [Tea25c] - 50.0 - 90.6 54.5 -
DeepSeek-R1 [GYZ*25] > 800K 79.8 - 97.3 71.5
DeepSeek-R 1 -Distill-Qwen-32B [GYZ* 25] 800K 72.6 - 94.3 62.1 -
Open-weight & open-data SFT on Qwen2.5-Instruct

OpenThinker-32B [Tea25b] 114k 66.0 53.3 90.6 61.6 67.9
Bespoke-32B [Bes25] 17K 63.3 - 93.0 58.1 -
Sky-T1-32B-Preview [Tea25a] 17K 43.3 - 82.4 56.8 -
S1-32B [MYS*25] 1K 50.0 26.7 92.6 56.6 56.5
S1.1-32B [MYS*25] 1K 56.7 50.0 94.4 60.6 65.4
P-TTS-32B (ours) 90 900 73.3 53.3 94.2 60.6 70.4

Table 8. Accuracy comparison of 32B-scale models on four reasoning benchmarks: AIME2024, AIME2025,
MATHS00, and GPQA-Diamond. Models are grouped into closed-source APIs, open-source baselines, and
open-weight fine-tuned variants of Qwen2.5-Instruct. Our method, P-TTS-32B, leverages only 90 seed examples
augmented via principled prompting strategies to generate up to 900 training examples. Despite the small training
size, P-TTS-32B achieves competitive or superior performance, outperforming several models trained on datasets
with hundreds of thousands of examples. Notes: Results for Gemini and Qwen are taken from [MYS*25] (we follow
their evaluation settings).

Model OlympiadBench Gaokao Kaoyan Minerva GradeSchool AMC23_ Avg.
OpenAI-o1-preview 52.1 62.1 51.5 47.1 62.8 81.8 59.6
Qwen2.5-32B-Instruct 45.3 72.1 48.2 41.2 56.7 64.0 54.6
OpenThoughts (114K) 56.3 63.2 54.7 41.1 39.0 80.5 55.8
NuminaMath (100K) 36.7 49.4 321 24.6 36.2 40.6 36.7
S1 dK) 56.9 32.9 59.3 46.7 61.4 775 55.8
P-TTS (Ours) 63.9 51.9 52.3 51.5 53.8 87.5 60.2

Table 9. Zero-shot generalization accuracy (%o) on out-of-domain reasoning benchmarks. P-TTS is trained
only on AIME22-24.

that high accuracy can be attained through lightweight, targeted supervision.

Cross-Domain and Multilingual Generalization. Although P-TTS is trained only on 90 English
AIME-style problems (AIME22-—24), it exhibits robust zero-shot transfer to benchmarks that
differ in language, curriculum, and problem format. In Table 9, we evaluate on Chinese exam
datasets (Gaokao, Kaoyan), U.S. competition and school math (OlympiadBench, AMC23,
GradeSchoolMath), and scientific quantitative reasoning (Minerva). These tasks introduce
shifts in linguistic style (Chinese vs. English), assessment design (competition vs. entrance exams
vs. textbook problems), and reasoning presentation (concise Olympiad proofs vs. step-by-step
classroom narratives). Despite no multilingual supervision and no direct exposure to these
benchmarks during finetuning, P-TTS attains competitive accuracy across the board, narrowing
the gap with models trained on one to two orders of magnitude more data. This suggests that
principled prompt augmentation through varying instructional framing and exemplar structure
encourages prompt-space coverage that translates into language and curriculum robustness, elicits
knowledge from pre-trained models, rather than overfitting to a single benchmark family.

Scaling across model sizes. We evaluate how P-TTS performance scales with model size across
the 7B, 14B, and 32B model variants. At every scale, P-TTS outperforms both S1 and S1.1 on
most benchmarks, as shown in table 11. For instance, P-TTS-7B surpasses S1.1-7B by +30.0% on

Technical Report | October 13, 2025 | 15


Benchmark 7B Models 14B Models 32B Models
P-TTS S1 S11 P-TTS S11 P-TTS S1 S1.1

AIME2024 43.33 16.67 13.33 53.33 33.33 73.33 50.00 56.70
AIME2025 26.67 13.33 20.00 26.67 33.33 53.33 26.70 50.00
MATHS00 84.20 77.20 81.20 90.40 91.60 94.20 92.60 94.40

GPQA-Diamond 41.92 4141 4141 51.01 51.01 60.61 5660 60.60

Table 10. Accuracy (%) on Four Benchmarks with Grouped Model Sizes. Each group shows results for core
M1, S1, and S1.1.

AIME2024 and achieves comparable results on MATHS500 with fewer examples. At 14B level,
P-TTS continues to lead across all tasks, reaching 53.33% on AIME2024 and 90.4% on MATHSO0.
Notably, at the 32B scale, P-TTS achieves 73.33% on AIME2024 and 94.20% on MATHS00,
outperforming both S1 and S1.1 despite their larger training sizes. These results highlight the
robustness and efficiency of principled instruction tuning (P-TTS), demonstrating that even with
minimal data (90 — 900), it scales effectively and consistently enhances performance across
diverse benchmarks.

5. Conclusion

We presented Prompting Test-Time Scaling (P-TTS), a lightweight yet effective framework
that converts a compact seed set into a high-utility reasoning corpus by wrapping each problem
with principled instructional prompts. Without changing task semantics, P-TTS systematically
explores prompt-space via reward/penalty framing, correctness emphasis, and step-by-step
guidance, eliciting high-quality rationales from a teacher model to supervise a student. Across
configurations with augmentation multipliers m € {1,4,5,10}, P-TTS consistently improves
supervised reasoning relative to the null prompt and even starting from only 90 seeds, matches
or surpasses models trained on substantially larger, static datasets. P-TTS demonstrates that
instructional prompt reformulation is a powerful and overlooked scaling lever. With only 90
carefully chosen seeds, principled wrapping and its paraphrased variants produce supervision
that competes with (and at times exceeds) 1 K-shot baselines in the wild, substantially lowering
the data curation burden for robust LLM reasoning.

Future Work. We see several promising directions: (1) adaptive, per-instance selection of
instructional wrappers via learned policies; (2) integration with retrieval and verifier/reranker
pipelines to couple wrapper diversity with factual grounding; (3) principled scheduling of wrapper
mixtures over training epochs to mimic curriculum learning; and (4) systematic study of wrapper
transfer across tasks, languages, and modalities.

6. Limitations

There are several potential limitations. First, the evaluation is concentrated on math-style problems
with single numeric answers (AIME22-24), so external validity to open-ended, multimodal, or
multilingual reasoning remains to explore further. Second, P-TTS depends on a single family
to generate rationales; any bias, error, or stylistic artifact in the teacher can be amplified by our
wrappers and propagated to the student, especially since rationales were not human-audited.
Third, while wrappers are designed to be semantically invariant, some templates (e.g., extreme
reward/penalty framings) may shift reasoning behavior in undesired ways and could introduce
ethical or calibration issues; sensitivity to wrapper mixture, placement, and decoding settings
also suggests latent hyperparameter fragility. Fourth, despite contamination mitigation, residual
leakage from publicly available AIME material cannot be conclusively ruled out. Finally, P-TTS

Technical Report | October 13, 2025 | 16


requires compute at data collection through inference generation (via wrapper ensembles).

References

AAA*23.

AIM?24.
AIM25.

Ant25.

Art.

Bes25.

Bil22.

BMR*20.

BMS23.

BMZ?*24.

BSS*24.

Clo24.

FLD18.

GBB*21.

GMK*25.

GYZ*25.

HBD*19.

HBK?21.

HLB*24.

HYS*25.

Technical Report

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774, 2023. (cit. on pp. 2 and 5.)

AIME. 2024 aime i. https://artofproblemsolving.com/wiki/index.php/2024_AIME_I, 2024. Art of
Problem Solving Wiki, accessed July 2025. (cit. on p. 8.)

AIME. 2025 aime i. Art of Problem Solving Wiki, 2025. Held February 6, 2025. URL: https:
//artofproblemsolving.com/wiki/index.php/2025_AIME_I. (cit. on p. 8.)

Anthropic AI. Claude 3.7 sonnet and claude code. Anthropic blog, February 2025. First hybrid reasoning
large language model generally available. URL: https://www.anthropic.com/news/claude-3-7-sonnet.
(cit. on pp. 2 and 3.)

Art of Problem Solving (AoPS). Aime problems and solutions. https://artofproblemsolving.com/wiki/
index.php/AIME_Problems_and_Solutions. (cit. on p. 5.)

Bespoke Labs. Bespoke-stratos-32b. https://huggingface.co/bespokelabs/Bespoke- Stratos-32B,
2025. Hugging Face model card, Apache-2.0 license. Fine-tuned Qwen2.5-32B-Instruct on
Bespoke-Stratos-17k dataset derived via DeepSeek-R1 distillation. (cit. on pp. 3, 9, and 15.)

Jeff Bilmes. Submodularity in machine learning and artificial intelligence. arXiv preprint
arXiv:2202.00132, 2022. (cit. on p. 9.)

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877—-1901, 2020. (cit. on
p. 3.)

Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang Shen. Principled instructions are all you
need for questioning llama-1/2, gpt-3.5/4. arXiv preprint arXiv:2312.16171, 2023. (cit. on pp. 2, 3, 4,
and 5.)

Alexander Bukharin, Jiachang Mu, Zhengbao Zhang, Seyeon Lee, Kai-Wei Chang, Noah A. Smith,
and Daniel Khashabi. Data diversity matters for robust instruction tuning. In Findings of the
Association for Computational Linguistics: EMNLP 2024, pages 2871-2885, 2024. URL: https:
//aclanthology.org/2024.findings-emnlp.195. (cit. on p. 13.)

Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi,
Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et al. Lessons from the
trenches on reproducible evaluation of language models. arXiv preprint arXiv:2405. 14782, 2024. (cit.
on p. 8.)

Google Cloud. Flash thinking with generative ai. https://cloud.google.com/vertex-ai/generative-ai/
docs/thinking, 2024. (cit. on p. 8.)

Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint
arXiv: 1805.04833, 2018. (cit. on p. 3.)

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Samuel Weinbach, and Connor Leahy. EleutherAI/Im-
evaluation-harness: Evaluation Harness for Language Models, 2021. doi: 10.5281/zenodo. 5371628.
(cit. on p. 8.)

Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna
Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning
models. arXiv preprint arXiv:2506.04178, 2025. (cit. on pp. 3 and 9.)

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in IIms via
reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. (cit. on pp. 3, 5, 9, and 15.)

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. arXiv preprint arXiv: 1904.09751, 2019. (cit. on p. 3.)

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint
arXiv:2103.03874, 2021. (cit. on p. 8.)

Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han,
Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench:
A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific
problems. arXiv preprint arXiv:2402. 14008, 2024. URL: https://arxiv.org/abs/2402.14008, doi:
10.48550/arXiv. 2402.14008. (cit. on p. 8.)

Shulin Huang, Linyi Yang, Yan Song, Shuang Chen, Leyang Cui, Ziyu Wan, Qingcheng Zeng, Ying

| October 13, 2025 | 17


JKL*24.

KMH?*20.

Lab25.

LCGS25.

LYH*22.

MGH?*24.

MPWC23.

MYS?*25.
Ope24.
Ope25.

QSS*20.
Qwe?24.

RHS*24.

RNS*18.

TAB*23.

Tea25a.
Tea25b.
Tea25c.
WWS?* 22a.

WWS?*22b.

YHX*25.

YIS*23.

YLY*25.

Technical Report

Wen, Kun Shao, Weinan Zhang, et al. Thinkbench: Dynamic out-of-distribution evaluation for robust
Ilm reasoning. arXiv preprint arXiv:2502.16268, 2025. (cit. on p. 5.)

Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec
Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai ol system card. arXiv preprint
arXiv:2412.16720, 2024. (cit. on p. 3.)

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361, 2020. (cit. on p. 3.)

Bespoke Labs. Bespoke-stratos-17k: A synthetic reasoning dataset of questions, reasoning traces, and
answers. Hugging Face Dataset, 2025. Derived from DeepSeek-R1 via the Sky-T1 pipeline using
Bespoke Curator. URL: https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos- 17k. (cit. on
p. 3.)

Tianyi Li, Mingda Chen, Bowei Guo, and Zhigiang Shen. A survey on diffusion language models.
arXiv preprint arXiv:2508.10875, 2025. (cit. on p. 3.)

Wenhao Li, Xiaoyuan Yi, Jinyi Hu, Maosong Sun, and Xing Xie. Evade the trap of mediocrity:
Promoting diversity and novelty in text generation via concentrating attention. In Yoav Goldberg,
Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 10834-10858, Abu Dhabi, United Arab Emirates,
December 2022. Association for Computational Linguistics. URL: https://aclanthology.org/2022.
emnlp-main.745, doi:10.18653/v1/2022.emnlp-main. 745. (cit. on p. 9.)

Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang,
Yixin Cao, and Aixin Sun. Sciagent: Tool-augmented language models for scientific reasoning. In
EMNLP, 2024. (cit. on p. 2.)

Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling. Transactions
of the Association for Computational Linguistics, 11:102—121, 2023. (cit. on p. 3.)

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Candés, and Tatsunori Hashimoto. s1: Simple test-time scaling.
arXiv preprint arXiv:2501.19393, 2025. (cit. on pp. 3, 4, 7, 9, and 15.)

OpenAI. Learning to reason with Ilms, 2024. URL: _https://openai.com/index/
learning-to-reason-with-Ilms/. (cit. on p. 8.)

OpenAI. Openai 03-mini. https://openai.com/index/openai-03-mini/, 2025. (cit. on pp. 8 and 15.)
Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Jiawei Han, and Weizhu Chen. Coda:
Contrast-enhanced and diversity-promoting data augmentation for natural language understanding.
arXiv preprint arXiv:2010.08670, 2020. (cit. on p. 13.)

Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/
qwq-32b-preview/, November 2024. QwQ-32B-Preview is an experimental reasoning model with
open weights. (cit. on p. 9.)

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,
Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In
First Conference on Language Modeling, 2024. (cit. on p. 8.)

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018. (cit. on p. 2.)

Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805, 2023. (cit. on pp. 2 and 3.)

NovaSky Team. Sky-tl: Fully open-source reasoning model with o1-preview performance in $450
training cost, 2025. URL: https://novasky-ai.github.io/posts/sky-tl. (cit. on pp. 9 and 15.)
OpenThoughts Team. Openthinker-32b. https://huggingface.co/open-thoughts/OpenThinker- 32B,
2025. (cit. on pp. 9 and 15.)

Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. (cit. on p. 15.)
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv
preprint arXiv:2203.11171, 2022. (cit. on p. 3.)

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824—24837, 2022. (cit. on pp. 2 and 3.)

Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for
reasoning. arXiv preprint arXiv:2502.03387, 2025. (cit. on p. 4.)

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo
Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large
language models. arXiv preprint arXiv:2309.12284, 2023. (cit. on pp. 3 and 9.)

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,

| October 13,2025 | 18


YYZ*24.

ZAD*25.

ZCH*25.

ZGZG25.

Technical Report

Chengen Huang, Chenxu Ly, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.
(cit. on pp. 7 and 9.)

An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng
Liu, Fei Huang, Haoran Wei, et al. Qwen?2.5 technical report. arXiv preprint arXiv:2412.15115, 2024.
(cit. on p. 15.)

Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E
Gonzalez, and Matei Zaharia. Bare: Leveraging base language models for few-shot synthetic data
generation. arXiv preprint arXiv:2502.01697, 2025. (cit. on p. 5.)

Tong Zheng, Lichang Chen, Simeng Han, R Thomas McCoy, and Heng Huang. Learning to reason
via mixture-of-thought for logical reasoning. arXiv preprint arXiv:2505.15817, 2025. (cit. on p. 3.)
Siyan Zhao, Devaansh Gupta, Qinging Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion
large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. (cit. on
p. 3.)

| October 13,2025 | 19


Appendix

A. Extended Experiments on Deore

We further evaluate models trained on the Deore dataset across multiple parameter scales.
Table 11 reports grouped results for 7B, 14B, and 32B variants, highlighting consistent gains
from principled data augmentation.

Benchmark 7B Models 14B Models 32B Models
P-TTScore S1 S110 P-TTScore = SUL — PTT Score S1 S1.1
AIME2024 20.00 16.67 = 13.33 36.67 33.33 - 56.67 56.70 56.70
AIME2025 20.00 13.33 20.00 33.33 33.33 - 46.67 26.70 60.00
MATHS00 80.40 77.20 81.20 89.80 91.60 —- 94.00 93.00 95.40
GPQA-Diamond 35.86 41.41 41.41 45.96 51.01 —- 53.03 59.60 63.60

Table 11. Accuracy (%o) on Four Benchmarks with Grouped Model Sizes. Each group shows results for core
P-TTS, S1, and S1.1.

B. Training Details

We fine-tuned the Qwen2.5-Instruct family at three scales—7B, 14B, and 32B—using our
P-TTS datasets. All models were trained for 5 epochs with an effective global batch size of 16
(micro-batch size of 1 with gradient accumulation). We used the AdamW optimizer (6; = 0.9,
Bo = 0.95, weight decay = 1 x 1074) and a base learning rate of 1 x 10~°, warmed up linearly for
the first 5% of steps and decayed to zero following a cosine schedule. Training was conducted in
bfloat16 precision with fully sharded data parallelism (FSDP) enabled. We set the maximum
sequence length to 20k tokens to avoid truncation of reasoning traces. For supervision, loss was
applied only to the reasoning and answer tokens, not the input question text. Across model scales,
this consistent setup allowed us to directly compare how principled data augmentation transfers
to different parameter sizes.

0.890 Training Loss 6.65 Gradient Norm Learning Rate Schedule
cnn ‘ — Original 10-5 — Original
0.823 acm aaa — Reward — Reward
. — = Penalty 5.38 — Correctness — Correctness
— Step-by-Step . —— Penalty —— Penalty
0.756 === Original Final (0.607) — Step-by-step | W — Step-by-Step
Pu}
E 2
tn 0.689 6 412
fe)]
wn 2 c
& 3 = 10°
eal emer emeeeeel Yom \ ems © 2.85 =
Oo ®
0.555 a
1.58
0.488
0.422 0.32
25 5.0 7.55 10.0 12.5 15.0 25 5.0 7.5 100 12.5 15.0 25 5.0 7.5 100 12.5 15.0
Training Steps Training Steps Training Steps

Figure 7. Training dynamics of P-TTS-32B.

C. Additional Results on Other Models

We also report benchmark accuracy for additional baselines beyond the Qwen?2.5-Instruct family.
Table 12 presents results for Claude, O4-mini-high, and hybrid settings with DeepSeek.

Technical Report | October 13, 2025 | 20


Model Data Points AIME24 AIME25 GPQA-Diamond MATH

Claude 810 16.67 0.00 34.34 72.80
04-mini-high 810 36.67 0.00 41.41 60.00
DeepSeek + Claude 1620 40.00 20.00 37.88 83.20
DeepSeek + O4mini 1620 36.67 13.33 35.86 74.00

Table 12. Performance comparison across benchmarks. Values represent accuracy (%) per dataset.

D. Evaluation of Reward Framing Variants

We evaluate variants of the reward-framing across different models. Table 13 summarizes
the accuracy of Ol-mini, Gemini, and DeepSeek on a fixed subset of math problems (AIME
2022-2024) under each variant.

Rewards Ol-mini Gemini DeepSeek
R1 (Large Reward) 60.0% 33.3% T4.4%
R2 (Reward2) 52.2% 32.2% 73.3%
R3 (Reduced Reward2) 57.8% 32.2% 75.6%
R4 (Reward at End) 51.1% 28.9% 72.2%
R5 (Reduced Reward 1) 56.7% 33.3% 67.8%

Table 13. Model Accuracy Across Five Variants of the Reward. Each row corresponds to a different formulation of
the reward-based prompt, evaluated on a fixed subset of math questions. Results are reported as the percentage of
correct answers for each model: O1l-mini, Gemini, and DeepSeek.

Technical Report | October 13,2025 | 21


E. Example from Dpy-p-rrs

Question

Let O = (0,0), A = (3.0). and B = (0, $) be points in the coordinate plane. Let

F be the family of segments PQ of unit length lying in the first quadrant with P on
the x-axis and Q on the y-axis. There is a unique point C on AB, distinct from A
and B, that does not belong to any segment from F other than AB. Then OC? = oD
where p and q are relatively prime positive integers. Find p + q.

Response

Given points O = (0,0), A = (+. 0), and B = (0, 8), we need to find the unique

point C on AB that does not belong to any segment from the family ¥ other than AB.
The family ¥ consists of segments PQ of unit length with P on the x-axis and Q
on the y-axis. Each segment in ¥ can be parametrized as x/cos(@) + y/sin(@) = 1
where @ is an angle parameter. The segment AB itself corresponds to 6 = 77/3.

To find the point C, we parametrize AB as x = 44 andy = ae for t € [0,1].
Substituting these coordinates into the equation of the line segment from #, we
derive the condition for C to lie only on AB.

By solving the resulting equations and ensuring that the derivative condition (to check
for a double root) is satisfied, we find that t = i. This gives the coordinates of C as

Calculating OC?:

= + = =
64 64 64 16

8 8

2
Ly +S) 1 27 28 7

oc? =|

Thus, OC? = i where p = 7 and gq = 16. Therefore, p + g = 7+ 16 = 23.

23

Reasoning Trace

Okay, so I need to find the square of the distance from the origin O to this special
point C on segment AB. The problem says that C is the unique point on AB, other
than A and B, that isn’t on any other segment from the family ....

Figure 8. Example of an original AIME-style math problem, along with its model-generated response and reasoning
trace. In our methodology, these original seed examples are later paired with principle-based reformulations (e.g.,
Reward, Penalty, Correctness, Step-by-Step), enabling the construction of the full P-TTS augmented corpus

Technical Report | October 13, 2025 | 22


Question

I am going to tip $200000 for a better solution! Let O = (0,0), A = (4 0), and

B= (0, 8) be points in the coordinate plane. Let ¥ be the family of segments PO
of unit length lying in the first quadrant with P on the x-axis and Q on the y-axis.
There is a unique point C on AB, distinct from A and B, that does not belong to any
segment from F other than AB. Then OC? = - where p and gq are relatively prime
positive integers. Find p + q.

DeepSeek Response

Given points O = (0,0), A = (3. 0), and B = (0, ¥), we need to find the unique

point C on AB that does not belong to any segment from the family F of unit length
segments PQ with P on the x-axis and Q on the y-axis.

1. **Equation of AB**: - The line segment AB has endpoints A and B. - The
equation of AB is y = —V3x + xa)

2. **Parametrization of AB**: - Points on AB can be parametrized as x = it and
y = 8 fort € [0,1].

3. **Envelope of the family F**: - The family F consists of segments PO with
P = (p,0) and Q = (0,q) where p* + q* = 1. - The envelope of these segments
forms an astroid given by x7/3 + y?/3 = 1.

4. **Finding the point of tangency**: - The point C where AB is tangent to the
astroid is found by solving the tangency condition. - The slope of AB is — V3, which

must match the slope of the astroid at the point of tangency. - The point of tangency
1 3v3

is found to be (cos? 4, sin? @) where 6 = a leading to coordinates (i. 7

5. **Calculating OC?**: - The coordinates of C are (4 38), - Squaring the distance
from O to C:

2 2
2_(1 3v3\" _ 1 4 27 8 _ 7
Oe = (3) + (38) = 64 + 64 = 64 = 16

- The fraction z has p = 7and g = 16,so p+ gq = 23.

Thus, the final answer is | 23 |.

Reasoning Trace

Okay, so I have this geometry problem here. Let me try to parse it step by step.
First, they give me three points: O at (0,0), A at (1/2, 0), and B at (0, sqrt(3)/2). Then
there’s this family of segments F, which consists of segments PQ of unit length lying
in the first quadrant, with P on the x-axis and Q on the y-axis. The problem states
that 252

Technical Report | October 13, 2025 | 23


Question

Think step by step: Let O = (0,0), A = (3.0). and B = (0, 3) be points in the

coordinate plane. Let F be the family of segments PQ of unit length lying in the first
quadrant with P on the x-axis and Q on the y-axis. There is a unique point C on AB,
distinct from A and B, that does not belong to any segment from ¥ other than AB.
Then OC? = 7 where p and gq are relatively prime positive integers. Find p + q.

DeepSeek Response

Given points O = (0,0), A = (;. 0), and B = (0, 8), we need to find a unique point

C on AB that does not belong to any other segment from the family ¥ of unit length
segments PQ with P on the x-axis and Q on the y-axis.
1. **Equation of AB**: - Parametrize AB with ¢ € [0, 1]:

ro= Sh y= 8,

2. **Family F**: - Segments PO with P = (p,0) and Q = (0,q) such that
247 =

po t+q =1.

3. **Envelope of F**: - The envelope of the family F is the astroid x2/> + y?/3 = 1.

4. **Intersection of AB with the Astroid**: - Substitute x = it and y = By into the

astroid equation:
= ae (F
— +|—t =a
2 2

- Solving this equation, we find t = 7.
5. **Coordinates of C**: - For t = 3:

6. **Distance OC**: - Calculate OC?:

8 g | 64°64 64 16

Thus, OC? = iz where p = 7 and g = 16. Therefore, p + g = 7+ 16 = 23.

23

Reasoning Trace

Alright, let’s try to tackle this problem step by step. First...

Technical Report | October 13, 2025._| 24


Question

You will be penalized if you provide the wrong answer. Let O = (0,0), A = (3. 0),

and B = (0, ) be points in the coordinate plane. Let F be the family of segments

PQ of unit length lying in the first quadrant with P on the x-axis and Q on the y-axis.
There is a unique point C on AB, distinct from A and B, that does not belong to any
segment from F other than AB. Then OC? = a where p and gq are relatively prime
positive integers. Find p + q.

Response

Given points O = (0,0), A = (4 0), and B = (0, 8), we need to find the unique

point C on AB that does not belong to any other segment from the family F of unit
length segments PQ with P on the x-axis and Q on the y-axis. We need to determine
OC? and express it as i where p and q are coprime positive integers, then find p + q.

1. **Parametrization of AB**: - The parametric equations for AB are x =

and y = se: for t € [0,1]. Thus, any point C on AB can be written as (54 Br),

1
xf
)

2. **Equation of line PQ**: - For a segment PQ in F with endpoints P = (p,0
and Q = (0, q); the equation is > + = 1 where Vite a q = 1.

3. **Condition for point C**: - Substituting C (44, Br) into the line equation

£42 =1 gives CO? 4 VR
pig ® P q

equation a + nev =rie

4. **Solving for r**: - Simplifying and solving for ¢ using trigonometric identities
and ensuring the equation has a unique solution at @ = 3 (corresponding to AB) leads
tot = 3.

5. **Coordinates of C**: - Substituting t = 3 into the parametric equations, C is

. - Using p = cos 6 and g = sin 6, we derive the

2 2
6. **Calculating OC?**: - OC = i) + (48) = L + s _ ze _ a

Thus, p = 7 and g = 16, leading to p + q = 23.

23

Reasoning Trace

Okay, let’s try to tackle this problem. First, let me make sure I understand what’s
being asked here.We have three points: O at (0,0), A at (1/2, 0), and B at (0, sqrt(3)/2).
The line segment AB is part of the ....

Technical Report | October 13, 2025 | 25


Question

Your Task is to solve the following: Let O = (0,0), A = (3, 0), and B = (0, $) be

points in the coordinate plane. Let F be the family of segments PQ of unit length
lying in the first quadrant with P on the x-axis and Q on the y-axis. There is a unique
point C on AB, distinct from A and B, that does not belong to any segment from F
other than AB. Then OC? = 2, where p and q are relatively prime positive integers.
Find p + q.You must provide the correct answer!

DeepSeek Response

Given points O = (0,0), A = (4 0), and B = (0, 8), we need to find the point C

on AB that does not belong to any other segment from the family F of unit length
segments in the first quadrant with endpoints on the axes. __

1. **Parametrization of AB**: - The parametric equations for AB are x = i and
y = 8+ fort € [0,1].

2. **Envelope of the family #**: - The envelope of the family of segments PQ with
P on the x-axis and Q on the y-axis, where PQ has unit length, forms an astroid given
by x2/3 yee = ll.

3. **Finding the intersection of AB with the astroid**: - Substitute the parametric
equations of AB into the astroid equation:

- Solve for ¢ by ensuring the gradient of AB matches the gradient of the astroid at the
point of tangency. This leads to the point C where t = 3.
4. **Coordinates of C**: - Substituting t = ; into the parametric equations:

v3 3v3
2 4° 8

_™3 3
y= 4

5. **Calculating OC?**: - The coordinates of C are (5. 38), - Compute OC?:

- 64764. 64. 16

8 8

2
) FR 1 27 28 7

oc? = |

Thus, OC? = iz and the sum p+ g = 7+ 16=23.

ZB

Reasoning Trace

Okay, let’s try to tackle this problem step by step. First,

et’s make sure ....
Technical Report 26

]
October 13, 2025 |


F. P-TTS-32B vs. S1-32B

Question : If the Sun's absolute magnitude is +5, find the luminosity of a star of magnitude O in ergs/s. A
useful constant: the luminosity of the sun is 3.83*10* ergs/s

P-TTS-32B
Okay, so | need to find the luminosity of a star with
magnitude 0, given that the Sun's absolute magnitude is +5
and its luminosity is 3.83x10** ergs/s. Hmm, let's start by
recalling the relationship between magnitude and
luminosity.
First, absolute magnitude is a measure of a star's intrinsic
brightness, as it would be seen from a standard distance.
The Sun's absolute magnitude is given as +5. The star we're
interested in has an absolute magnitude of 0. The
difference in magnitude between the two stars is 5-0=5
magnitudes.
In astronomy, the difference in magnitudes relates to the
ratio of luminosities. The formula is:
m1 - m2 = -2.5:logio(L1/L2)
Where m1 and m2 are the magnitudes, and L1 and L2 are the
corresponding luminosities. Wait, but here we have
absolute magnitudes, so this formula should apply. Let me
confirm.
Yes, the formula for the difference in magnitudes is indeed
m1 - m2 = -2.5-logio(L1/L2). So if we know the difference in
magnitudes, we can find the ratio of luminosities.
In our case, m1 (Sun) is +5, m2 (star) is O. Som1-m2=5-0=
+5,
Plugging into the formula:
5 = -2.5-logio(L_sun/L_star)
Wait, hold on. Let me make sure | have the formula right.
The formula is m1 - m2 = -2.5-logio(L1/L2). So if m1 is the
magnitude of the first star (Sun), m2 is the magnitude of the
second star (star of interest). Then L1 is luminosity of Sun,
L2 is luminosity of star.
Som1-m2=5-0=+5.
Therefore:
5 = -2.5-logio(L_sun/L_star)
Let me solve for logio(L_sun/L_star):
Divide both sides by -2.5:
5 / (-2.5) = logio(L_sun/L_star)
Which is -2 = logio(L_sun/L_star)
Therefore, L_sun/L_star = 10-? = 0.01
So L_star = L_sun/ 0.01 =L_sunx100
Therefore, the luminosity of the star is 100 times that of the
Sun.
Given that the Sun's luminosity is 3.83x10** ergs/s, then the
star's luminosity is 3.83x10* x 100 = 3.83x10* ergs/s.
Wait, let me check that calculation again. 3.83x10**
multiplied by 100 is indeed 3.83x10*. Yes.
But let me verify the steps again to make sure | didn't make
a mistake.
Starting with the magnitude difference: Sun's absolute
magnitude is +5, star's is 0, so difference is +5.
Using the formula m1 - m2 = -2.5-logi0(L1/L2)
Plugging in: 5 = -2.5-logio(L_sun/L_star)
Divide both sides by -2.5: 5 / (-2.5) = -2 = logio(L_sun/L_star)
Therefore, L_sun/L_star = 10-? = 0.01
Thus, L_star = L_sun / 0.01 = L_sun x 100
Therefore, 3.83*10** x 100 = 3.83x10* ergs/s.
That seems correct. So the luminosity of the star is
'3.83*x10** ergs/s.
But let me check if there's another way to approach this to
confirm.

think

The problem asks for the luminosity of a star with a given
apparent magnitude, given the absolute magnitude of the
Sun and the Sun's luminosity.

We are given:

Sun's absolute magnitude, M_sun = +5

Magnitude of the star, m=O

Luminosity of the Sun, L_sun = 3.83x10* ergs/s

We need to find the luminosity of the star, L_star.

The relationship between absolute magnitude and
luminosity is given by:

M= -2.5-logi0(L/Lo)

where M is the absolute magnitude, L is the luminosity, and
Lo is a reference luminosity.

For the Sun, we have:

M_sun = -2.5-logio(L_sun/Lo)

5 = -2.5-logio(3.83%10** / Lo)

We can solve this equation for Lo:

5/(-2.5) = logio(3.83*10*? / Lo)

-2 = logio(3.83*10** / Lo)

10-7 = 3.83x10** / Lo

0.01 = 3.83*10** / Lo

Lo = 3.83*10** / 0.01

Lo = 3.83x10** x 100

Lo = 3.83x10* ergs/s

Now, we need to find the luminosity of the star. The
apparent magnitude m is related to the absolute magnitude
M by the distance modulus:

m-M= 5-logio(d) -5

where d is the distance to the star in parsecs.

The absolute magnitude of the star can be found using the
same formula as for the Sun:

M_star = -2.5-logo(L_star/Lo)

We are given the apparent magnitude of the star, m = 0. We
need to find the absolute magnitude of the star. We can use
the fact that the apparent magnitude of the Sun is about
-26.74.

Let's use the relationship between apparent magnitude,
absolute magnitude, and distance. For the Sun, which is at a
distance of approximately 1 astronomical unit (AU), we
have:

m_sun - M_sun = 5-logio(d_sun) - 5

The average distance from Earth to the Sun is about
1.496x10* cm.

1 parsec = 3.086~10** cm.

d_sun in parsecs = (1.496*10**)/(3.086*10**) = 4.85x10-°
parsecs.

Using the apparent magnitude of the Sun, m_sun * -26.74:
-26.74 - 5 = 5-logio(4.85*10-*) - 5

-31.74 = 5-(logi0(4.85) + logio(10-*)) - 5

-31.74 = 5-(logio(4.85) - 6) - 5

-31.74 = 5-logio(4.85) - 30-5

-31.74 = 5x0.6856 - 35

-31.74 = 3.428 - 35

-31.74 = -31.572

This is close enough considering the approximations used.
Now, let's consider the star with magnitude O. We need to
find its absolute magnitude. We can use the Sun as a
reference.

Figure 9. Qualitative comparison between P-TTS-32B and the baseline model.

Technical Report

| October 13, 2025 | 27


G. Comparative Example: Original vs. Reward-Framed Response

Let ABCD be a parallelogram with ZBAD < 90°. A circle is tangent to DA, AB, and
BC and intersects diagonal AC at points P and Q with AP = 3, PQ = 9, and QC = 16.
Suppose that AP* + 3PQ* + 9QC? = 16n. Show that the area of ABCD can be written as
m

—vn where m,n € Zyo and n is squarefree. Find m + n.
n

Response: Response: Given a triangle
ABC with sides AB and BC, we have two
configurations of circles: eight circles of
radius 34 and 2024 circles of radius 1, each
arranged sequentially tangent with two
circles tangent to AB and BC respectively.
We need to find the inradius of triangle
ABC.

Key steps: 1. **Understanding the Con-
figuration**: The circles are arranged in
a chain, sequentially tangent, with two cir-
cles tangent to AB and BC. This implies
a relationship between the number of cir-
cles, their radii, and the angle at vertex
B. 2. **Angle Bisector and Inradius**:
The inradius R of the triangle is related to
the distance from the incenter to the sides.
The chain of circles forms a path along
the angle bisector of ZABC. 3. **Re-
lating Configurations**: The product of
the number of circles and their radii for
both configurations must be proportional
to the inradius. The key insight is that the
ratio of the number of circles (253) and
the inverse ratio of the radii (34) gives the
inradius. 4. **Calculating the Inradius**:
The inradius R is found by recognizing the
proportional relationship between the two
configurations. The ratio of the number
of circles (2024/8 = 253) and the ratio of
the radii (34/1 = 34) leads to the inradius
being 43 a

Thus, the inradius R is 33 and the sum of
the numerator and denominator is 253 +

Technical Report

Response: Given triangle ABC with sides
AB and BC, we have two configurations
of circles: eight circles of radius 34 and
2024 circles of radius 1, each sequentially
tangent and tangent to AB and BC respec-
tively. We need to find the inradius of
triangle ABC.

1. Key Relationship: - The inradius R of
triangle ABC can be related to the radius
r and the number of circles N through the
angle at vertex B. - The formula derived
is: R=r+2r(N — 1)sin(6/2).

2. Setting Up Equations: - For the config-
uration with 8 circles of radius 34:

R =344+2-34-7-sin(6/2)
3. Equating the Expressions: - Equate the
two expressions for R:

34 + 476 sin(@/2) = 1 + 4046 sin(@/2)

4. Calculating the Inradius: - Substitute

sin(@/2) = a back into the equation for
R using the smaller radius configuration:
11
= 404
R=1+ 4046 - 1190

- Simplify the calculation: ....

5. Final Result: - The inradius R is an
where m = 192 andn = 5. Thus, m+n =
197.

197

| October 13,2025 | 28



34 = 287.

287

Reasoning Trace: Okay, let me try to

work through this problem step by step.
The problem is about a triangle ABC with
sides AB and BC. There are eight circles
of radius 34 that are sequentially tangent,
and two of these circles are tangent to
AB and BC respectively. Similarly, 2024
circles of radius 1 can be arranged in the
same manner. We need to find the inradius
of triangle ABC, expressed as a reduced
fraction m/n, and then compute m + n. ...
However, this is a guess based on the
ratio. The problem likely involves this
ratio, leading to the answer 287. **Final
Answer** | 287

Technical Report

Reasoning Trace: Alright, let me try to

wrap my head around this problem. So, we
have triangle ABC with sides AB and BC.
There are eight circles of radius 34 that are
sequentially tangent, meaning each one is
tangent to the next, and the first and last
ones are tangent to AB and BC respectively.
Similarly, we can arrange 2024 circles of
radius | in the same manner. The question
is asking for the inradius of triangle ABC,
expressed as a reduced fraction m/n, and
then we need to find m + n. ...

So, 22,253 / 595 = 37 + 2/5 = 37.4
Therefore, R = 1 + 37.4 = 38.4 = 192/5.
Thus, the inradius is 192/5, so m=192,
n=5,m+n=197.

This seems to check out, and the steps
make sense. Therefore, the answer is 197.
**Final Answer** | 197

| October 13, 2025 | 29

