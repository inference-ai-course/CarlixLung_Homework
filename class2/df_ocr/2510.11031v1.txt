arXiv:2510.11031v1 [cs.CL] 13 Oct 2025

LOGINUMSYNTH: SYNTHESIZING JOINT LOGICAL-
NUMERICAL REASONING PROBLEMS FOR LANGUAGE
MODELS

Yiwei Liu Yucheng Li Xiao Li Gong Cheng*
State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
{ywliu, ycli,xiaoli.nju}@smail.nju.edu.cn gcheng@nju.edu.cn

ABSTRACT

Joint logical-numerical reasoning remains a major challenge for language mod-
els, yet existing datasets rely on fixed rule sets and offer limited control over
task complexity, constraining their generalizability for evaluation and training.
We present LogiNumSynth, a flexible natural language problem synthesizer that
synthesizes tasks requiring proficiency in joint logical reasoning (e.g., rule-based
reasoning) and numerical reasoning (e.g., arithmetic computation). LogiNum-
Synth supports fine-grained control over reasoning world richness, logical rea-
soning depth, and the complexity of numerical computations, enabling flexible
data synthesis across difficulty levels. We demonstrate three key contributions:
(1) Synthesizer—synthesizing fully controllable joint reasoning tasks over natural
language; (2) Evaluation & Process Analysis—evaluating both process accuracy
and answer accuracy; (3) Targeted Training—using synthesized data to enhance
LLMs’ reasoning performance. Experiments with multiple LLMs highlight per-
sistent weaknesses in logical-numerical reasoning, showing that LogiNumSynth
can serve as both a diagnostic tool and a source of targeted supervision for ad-
vancing integrated reasoning skills.

1 INTRODUCTION

Comprehending the world and adeptly applying knowledge in practical scenarios pose fundamental
challenges for natural language processing (NLP). At the core of this cognitive process lie logical
reasoning and numerical reasoning, which jointly enable solving complex problems and deriving

meaningful insights 2019}|Clark et al.|{2020 2020}|/Hendrycks et al.|/2021).

Motivation. Although considerable efforts have been made to advance various aspects of reason-

ing (Lample & Charton| [2019 2022 2022 2022} [Lightman et al.
existing datasets typically focus on either logical reason-

ie gs ol et aa
Chen et al. 2025) in isolation (see Appendix |C} for related
work). In practice, however, many real-world problems require these abilities to be integrated. Ta-
ble[I|shows a biology example where solving the task demands both the application of genetic rules

(logical reasoning) and probabilistic computation of phenotypic ratios (numerical reasoning).

RuleArena (Zhou et al.|/2025) is, to our knowledge, the only existing dataset explicitly integrating
logical and numerical reasoning, covering tasks such as taxation, airline luggage fees, and NBA

trade validation. However, it has the following limitations. First, it offers limited control over logi-
cal and numerical complexity, and its reliance on fixed rules leads to repetitive task patterns, which
limit its extensibility and generalizability for evaluation and targeted training. Second, while the
original work reports rule-level recall and precision (a coarse form of process evaluation), it does
not provide fine-grained assessment of intermediate reasoning steps. These limitations motivate the
need for a flexible synthesizer that can (1) synthesize joint logical-numerical problems of adjustable

“Corresponding author


Table 1: Example of joint logical-numerical reasoning in biology.

Problem Gene A is dominant over a (A = red, a = white). Dominant B suppresses A, producing
white regardless of A/a. Two loci assort independently. Given Aa Bb x AaBb, determine
the offspring phenotypic ratio.

Reasoning Logical reasoning: B_—> white; bb: A_ — red, aa + white.
Numerical reasoning: From Mendelian segregation, Pr(B_) = 3 (all white), Pr(bb) =

1. 3 1 whi inine: whi B841y1 13%. ly 3 3
q: among them $ red, ¢ white. Combining: white = 7+ 4x4 = 7gsted= 4x7 = ¥-

Ratio = 3:13.

complexity for evaluation and further serving as a targeted training resource to improve reason-
ing performance, (2) incorporate structured intermediate steps to enable fine-grained process-level
diagnosis.

Our Work. We introduce LogiNumSynth, a controllable natural language problem synthesizer that
synthesizes tasks requiring integrated logical reasoning (e.g., rule-based reasoning) and numerical
reasoning (e.g., arithmetic computation). Our synthesizer allows fine-grained control over reason-
ing world richness (e.g., the number of rules and facts), logical reasoning depth, and the complex-
ity of numerical computations (e.g., computation range), enabling flexible synthesis of customized
datasets across difficulty levels. It is also highly extensible: the rule language supports easy in-
clusion of new mathematical expressions and richer logical operators, allowing the construction of
more diverse and challenging reasoning processes tailored to specific evaluation needs. It operates
independently of domain-specific background knowledge, ensuring that evaluation reflects a model’s
inherent reasoning capability rather than its memorized factual knowledge.

To synthesize a problem instance, LogiNumSynth first constructs a formal world of automatically
synthesized facts and rules, which incorporate numerical expressions, paired with a query, and then
translates this structure into natural language descriptions. Models must reason over the provided
natural language facts and rules to answer the query. In addition to answer accuracy, we evaluate
process accuracy by comparing a model’s generated reasoning steps with the automatically derived
gold-standard reasoning process.

Our Main Contributions are:

¢ Synthesizer: A customizable and extensible synthesizer that synthesizes natural language
problems requiring joint logical and numerical reasoning with controllable complexity.

¢ Evaluation & Process Analysis: A large-scale evaluation of 29 models, including process-
level reasoning diagnostics.

¢ Targeted Training: Demonstrating that our synthetic data can enhance model performance
on external reasoning tasks.

Code and Data are available at https://github.com/nju-websoft/LogiNumS ynth

2 DATA SYNTHESIZATION

Our synthesization starts from synthetic formal representations on which reasoning combines logical
and numerical operations. Then we convert formal representations into diverse templated natural
language descriptions and use an LLM to improve their fluency. We summarize the features of our
synthesizer in Appendix |[D] Now we elaborate on its design.

2.1 FORMAL REPRESENTATION

To synthesize a problem instance (called a sample in the rest of the paper) that requires reasoning,
we first define its underlying formal representation (Facts, Rules, Query). Facts and Rules
jointly establish a world model where formal reasoning can be executed to derive the gold-standard
answer for the given Query. Specifically, facts that match the condition of a rule will trigger this
rule to infer new facts. An example is shown in the “Reasoning DAG” part of Figure[I]


Pools

* Entity pool: Alice, Bob, Charlie, Dave, ...
+ Attribute pool: pretty, kind, furry, cold...

* Relationship pool: visit, like, hug, eat, ...

Formal Representation (shuffled)
Facts:
1. is(Thatch, english, 10)

15. average(Suki, Katleen)

Rules:

1. average(entity_1, entity_2) and

is(entity_3, blamed, 3) => slow(entity_3, entity_2)

15. is(entity_1, upcoming, 6) and

form(entity_2, entity_3) and is(entity_1, blamed, 3)
=> is(entity_3, pursued, entity_1[conventional])

Natural Language Description

1. Initializer World Elements: E, A, R
Query: is(Tootsie, blamed, ?)

Configuration

¢ Entity num: 10

¢ Attribute num: 15

¢ Relationship num: 10

3. Distraction || * Fact num: 15
Synthesizer *Rule num: 15

* Reasoning depth: 4

¢ Numerical range: [-10, 10]

aD
“4. Natural
Language
Generator

(continued)

2. Reasoning DAG
Constructor

Reasoning DAG

Facts:

is(Electra, blamed, 3)
form(Suki, Cassi)

is(Electra, upcoming, 6)
is(Katleen, confidential, 8)
is(Tootsie, agricultural, 4)
is(Hali, agricultural, 1)
gain(Kalinda, Tootsie)
is(Electra, conventional, 6)
nominate(Cassi, Katleen)
Rules:

1. is(entity_1, agricultural, 4) and
is(entity_2, agricultural, 1) and
gain(entity_3, entity_1) =>
reallocate(entity_3, entity_1)

2. segment(entity_1, entity_2) and
is(entity_1, pursued, 6) => is(entity_2,
blamed, 4)

No) (oe) Satlicay ea Bae)

3. nominate(entity_1, entity_2) and
reallocate(entity_3, entity_4) and
is(entity_2, confidential, 8) =>
segment(entity_1, entity_4)

Facts: 3. rule_3 & fact_i2 & fact_9 &

1. The value 10 is associated with Thatch under the
English attribute.
2. Electra's blamed attribute is set to 3.

fact_4 => fact_i3: segment(Cassi,
Tootsie)
4. rule_2 & fact_i3 & fact_il =>

1. Given the average relationship between entity_1
and entity_2, and the fact that the blamed attribute
of entity_3 is set to 3, it follows that there is a slow
relationship between entity_3 and entity _2.

2. If entity_1 hugs entity 2, entity_3 slowly relates
to entity_1, and entity_3 is identified as 6 under the
suburban attribute, then it follows that entity_1
forms a relationship with entity_3.

fact_i4: is(Tootsie, blamed, 4)

Templates
+ Attribute fact:
* the value of {a_j} for {e_i} is {num}
*the {a_j} field of {e_i} is represented by the
value {num}

4. is(entity_1, upcoming, 6) and
form(entity_2, entity_3) and is(entity_1,
blamed, 3) => is(entity_3, pursued,
entity_1[conventional])

Reasoning process:

1. rule_4 & fact_2 & fact_3 & fact_l1 &
fact_8 => fact_il: is(Cassi, pursued, 6)
2. rule_1 & fact_6 & fact_S & fact_7 =>
fact_i2: reallocate(Kalinda, Tootsie)

(to be continued on the left)

Figure 1: Overview of sample synthesization.

Entities, Attributes, and Relationships. Each sample is constructed from elements that include a
set of entities E = {e,,e2,--- } anda set of attributes A = {a 1, a2,---}. Each entity is associated
with a number of attributes. The value of an attribute is numerical and characterizes the degree
of the attribute in practical scenarios. For example, when we declare that the cold attribute of the
entity Dave has a value of 4, it signifies the level or intensity of coldness that he experiences, with
4 indicating a certain degree of severity. We also include binary relationships between entities. The
set of all relationships is denoted by R = {rj,r2,--- }.

Facts include all known values of the attributes of the entities and their relationships. They provide
the foundational information needed for reasoning. Facts come in two types:

° rp (ei, 5) denoting the existence of a directed relationship r, € R between two entities
e;,¢e; © E, and

* is(e;,a,;, num), which specifies a numerical value num for a specific attribute a; € A of
an entity e; € E.

Rules are expressed in an implication form, composed of a condition anda conclusion:

Condition 4 Conclusion.

If condition is true, then conclusion is inferred to be true.

In our current implementation (which can be easily extended), the condition is formulated as a
conjunction of one or more facts, while the conclusion is derived as a corresponding fact. Cru-
cially, all entities within these facts are represented as anonymized placeholder variables, denoted
using Greek letters as subscripts (e.g., €g, eg). These placeholders are dynamically instantiated
with concrete entities during rule application. To ensure logically valid inferences, any placeholder
variable appearing in the conclusion must also appear in the corresponding condition. Formally, we
define the condition using regular expression as:


lis(€q, ai, num) | 7; (eg, ey)| [A lis(e<, ax, num’) | ri(ec,&)] |

Similarly, the conclusion is implemented as a factual statement of the form:

* ri(€q, eg), OF

* is(e,,a;,expr(...)), where expr(...) is a function over placeholder variables defined in
the condition.

Unlike facts, conclusions involving expr(...) introduce numerical computations. In the current
implementation, we support four types of expressions:

* Constant expression: a parameter-free expression representing a fixed constant value.

* Retrieval expression: parameterized by an entity e, and an attribute a,;; its value is simply
the attribute of the entity itself, denoted as e,[a;].

* Calculation expression: parameterized by an entity e, and an attribute a,; its value is
computed as a linear form k x e,[a;] + b, where & and 0 are intrinsic parameters of the
expression sampled from a specified range, as detailed in Appendix [E.1|

¢ Aggregation expression: defined over two sub-expressions (each being a constant, re-
trieval, or calculation expression). Its parameters are inherited from the sub-expressions,
and its value is obtained by aggregating the results of the two sub-expressions us-
ing one of the supported operators: max, min, addition, or subtraction (e.g.
max(k x €q[a;] + b, eg[aj])).

Query finally asks the model to determine the value of a specific attribute for a given entity and
can be formulated as is(e;, a;,?).

A reasoning model capable of answering such queries requires proficiency in joint rule-based log-
ical reasoning and arithmetic-based numerical reasoning. Note that the above implementation can
be easily extended by, for example, designing more diverse forms of rules and providing more ex-
pressive arithmetic operations. These extensions are incremental and are left for future work.

2.2 SAMPLE SYNTHESIZATION

We develop an initializer, a reasoning DAG constructor, a distraction synthesizer, and a natural
language generator to synthesize samples described in natural language, as illustrated in Figure
These four components equip our synthesizing process with diversity, purity (i.e., independent of
background knowledge), and extensibility.

Initializer randomly selects entities, attributes, and relationships from predefined pools accord-
ing to the given configurations (i.e., the numbers of entities, attributes, and relationships), thereby
constructing the world elements of the sample inference world. The selection process for A and R
explicitly excludes synonyms, thereby mitigating the risk of semantic ambiguities that could con-
found the results. Specifically, we use an entity pool of 7,944 items from nltk.corpus.names,
an attribute pool of 1,366 items from nltk.corpus.treebank, and a relationship pool of 976
items from nltk.corpus.wordnet. These resources are provided by the Natural Language

Toolkit (NLTK) (Bird et al.}/2009), a widely used library for natural language processing. After that,
the initializer randomly selects one entity and one attribute as the query.

Reasoning DAG Constructor _ takes as input the world elements, the query, and the configuration
parameters (e.g., the reasoning depth). It then attempts to construct a reasoning directed acyclic
graph (DAG) by working backward from the query as the target node, with each node representing
either a fact or a rule along the reasoning path. Specifically, the constructor first takes the query as
the target and synthesizes a rule that can derive this target. The number of conditions and the form

‘ie., [is(ea, ai,num) | rj(eg,ey)] A--- A [is(ee, ax, num’) | ri(ec, e:)]


Table 2: Examples and numbers of templates used for converting formal representation into natural language.

Formal representation Example of natural language template #Templates
Attribute fact is(e;, aj, num) the value of {a;} for {e;} is {num} 222
Relationship fact Th(Eiy C7) {ei} {re} {es} 20
Implication condition + conclusion whenever {condition}, {conclusion} 26
Retrieval expression €a [ai] the value of {a;} for {e.} 16
Calculation expression (addition) k x ea[ai] +b multiplying {eq [ai]} by {k} and adding {b} 15
Calculation expression (subtraction) k x ea[ai] — b multiplying {e[ai]} by {k} and subtracting {b} 15
Aggregation expression (max) max(expr1, expr2) the greater of {exprl} and {expr2} 6
Aggregation expression (min) min(expr1, expr2) the minimum value of {exprl} and {expr2} 6
Aggregation expression (addition) addition(expr1, expr2) the total of {expr1} and {expr2} 6
Aggregation expression (subtraction) subtract ion(expr1, expr2) the difference between {exprl} and {expr2} 6

of the conclusion in this rule are randomly determined according to the configuration parameters.
Next, it synthesizes the corresponding facts that satisfy the rule’s conditions and treats these facts as
the subsequent targets. For each new target, the constructor either directly produces a supporting fact
or synthesizes another rule together with additional targets as conditions to support it. This process
continues until the reasoning depth specified in the configuration is reached. During the generation
of each fact or rule, the constructor ensures that no conflicts occur, i.e., respecting attribute value
uniqueness, thus guaranteeing the uniqueness of the query’s answer. Finally, it performs forward
reasoning across the DAG to derive both the gold-standard reasoning process and final answer.
Further implementation details are provided in Appendix{E. 1]

Distraction Synthesizer produces the remaining distracting facts and rules based on the con-
structed reasoning DAG and the configuration parameters (e.g., the numbers of facts and rules).
Similar to the construction of reasoning DAG, the facts and rules are randomly synthesized accord-
ing to the configurations, while ensuring that no conflicts arise with the existing facts and rules.

Natural Language Generator operates in two steps. First, templates are used to convert the
formal representation of the synthesized sample into diverse, template-based natural language de-
scriptions, laying the foundation for the subsequent natural language optimization. The number
of predefined templates is summarized in Table|2} In particular, to describe a rule, we randomly
select and combine templates for its components. For example, describing the rule r;(€a,e8) >
is(eg,a;,3 X €a[ax] — 9) requires one template for describing the implication, two templates to
describe the two facts, one template to describe the calculation expression, and one template to
describe the retrieval expression. In the second step, it takes the template-based descriptions and
further refines them using an LLM, making the text more fluent and natural. This refinement en-
sures that the generated text is syntactically accurate and semantically coherent, making it suitable
for processing by both humans and language models. More examples of templates and detailed
settings are provided in Appendix[E.2]

3. EXPERIMENT SETUP

We synthesized datasets of varying difficulty (detailed in Section B.1) to evaluate language models
of different architectures and scales (detailed in Section (3-2), with evaluation metrics described in
Section [3.3] as well as to serve as training resources (detailed in Section[3.4).

3.1 SYNTHETIC DATASETS

We first synthesized four datasets using our data synthesizer, named EL-EN, EL-HN, HL-EN and
HL-HN. The first part of the name represents the logical reasoning difficulty (Easy or Hard Logical),
while the second part indicates the numerical reasoning difficulty (Easy or Hard Numerical). To
further assess the reasoning capabilities of state-of-the-art models, we also synthesized exHL-HN
(extremely Hard Logical-Hard Numerical). Lastly, for the training resources for reasoning, we
synthesized two datasets: EN-Train and EL-Train. These are specifically designed to lower the
difficulty in numerical (Easy Numerical) or logical (Easy Logical) reasoning, respectively, to aid
model training toward the other reasoning capability. We controlled the difficulty of the samples
in each dataset by varying factors such as reasoning depth, the number of conditions in a logical


tule, the range of numerical computations, and the types of expressions used. More details on the
synthesizing configurations of these datasets are provided in Appendix[E.3|

3.2 TESTED MODELS

We evaluated 29 LLMs, including models with long reasoning chains: GPTS-mini (based on GPTS-
mini-2025-08-07) (OpenAT]/2025), DeepSeek-R1 (based on DeepSeek-R1-0528) (Guo et al.|/2025);
models specialized for reasoning: Phi4-mini-reasoning 2025), Phi4-reasoning and Phi4-
reasoning-plus (Abdin et al.| 2025); hybrid reasoning models: Qwen3 series (0.6B, 1.7B, 4B, 8B,
14B, 32B) (Yang et al.||2025); models with MoE (Mixture of Experts) architectures: Qwen3-30B-
A3B (Yang et al.||2025); and common instruct models: Llama3.1-8B-instruct (Dubey et al.|/2024),
Llama3.2-1B-instruct, Llama3.2-3B-instruct (AI||2024), Qwen2.5-instruct series (0.5B, 1.5B, 3B,
7B, 14B, 32B) (Team||2024), GLM4-chat GLM4-airx, -0520, -plug?] Phi4-mini-
instruct (Abouelenin et al.||2025), Phi4 (Abdin et al.||2024), DeepSeek-V3 (based on DeepSeek-
V3-0324) , and GPT-4o (based on GPT-40-2024-11-20) (2023), with

parameter sizes ranging from 0.5B to 671B.

Large models with long reasoning chains and significant parameter sizes, such as GPT5-mini and
DeepSeek-R1, demonstrate the current upper bound of reasoning capabilities in LLMs. On the
other hand, models with moderate parameter sizes, such as Qwen3-14B and Phi4-reasoning-plus
(14B), represent a balanced scale, offering a trade-off between computational efficiency and reason-
ing capability. Smaller models, such as Qwen3-1.7B and Llama3.2-1B-instruct, further emphasize
computational efficiency, but with a reduction in reasoning depth and complexity. We prompted
them using Chain of Thought (CoT), in the zero-shot and few-shot settings 2022). Im-
plementation details are provided in Appendix |F.1]

3.3. EVALUATION METRICS

Due to the nature of our synthesized datasets, each question’s reasoning can be decomposed into
multiple steps, enabling a fine-grained evaluation of a model’s process accuracy—the extent to
which its reasoning steps match the gold-standard process—and answer accuracy—the correctness
of the final result. To compute process accuracy, we prompted models to explicitly summarize
their reasoning at the end and used the structured output mode of Qwen3-8B to extract structured
reasoning steps from their output. Both the raw and structured outputs were compared with the
gold-standard reasoning process, allowing partial credit for intermediate steps that were correct. In
cases where the model outputs a different reasoning path but still arrives at the correct final answer,
the process is considered correct. This step-level evaluation is particularly important in scenarios
such as automated theorem proving, multi-step medical diagnosis, and financial auditing, where the
transparency, rigor, and correctness of each step are as critical as the final result. The implementation
of process accuracy is detailed in Appendix|F.2|

3.4 TRAINING SETUP

To demonstrate the potential of our synthetic data as an additional training resource for enhancing
model reasoning ability, we conducted supervised fine-tuning on Qwen3-1.7B and Llama3.2-1B-
instruct using LoRA (Hu et al.|/2021), and, in some experiments, the Recall Adam Mk
(2020). The models were evaluated on numerical reasoning benchmarks: GSM8K (Cobbe|
2021), MATH , SVAMP (Patel et al.
2021), MAWPS (Koncel-Kedziorski et al.||2016) ; formal deductive log-
ical reasoning benchmarks: RuleTaker [2020), FO-
LIO (Han et al.|[2024a), FLD (Morishita et al.|/2023); complex logical and joint logical-numerical

, ReClor (2020), AbductionR (Young
2022), RuleArena a

Detailed training configurations are provided in Ap-
pendix

‘https://open.bigmodel.cn/


Table 3: Performance of LLMs on datasets of different difficulty levels. ‘Proc’ refers to process accuracy and

‘Ans’ refers to answer accuracy.

Model #Params. #Shots EL-EN EL-HN HL-EN HL-HN Average
Proc Ans Proc Ans Proc Ans Proc Ans Proc Ans

. O-shot 60.37 8540 1243 22.00 020 480 0.05 260 18.26 28.70
Qwen?.5-32B-instruct 32.8B 3-shot 36.43. = 78.80 »=—:16.73 35.80 ~—0.66 6.80 0.52 1.20 13.59 30.65
O-shot 11.67 43.20 080 21.80 0.00 24.20 0.00 560 3.12 23.70

eB OM _Sshot_ 12.70 4720 _6.33__-27.00__0.05__—221.20 0.16 840481 25.95
Qwen3-1.7B 3.038 O-shot 1297 7680 4.47 62.20 230 4360 089 1920 5.16 50.45
Soe eee emp pmmnen caw med 3-shot 3153. 80.60. 22.50 64.20 139 45.80 0.82 20.40 14.06 _ 52.75
Qwen3-4B L038 O-shot 56.83 90.60 39.20 78.80 251 37.00 1.25 11.00 24.95 54.35
See ceenuenenm naw 3-shot 58.70 88.20. 53.27 _ 79.80 2.26 22.60 0.57 6.80 28.70 49.35
Qwen3-8B a.i0B O-shot 69.30 90.80 27.80 60.80 5.05 3140 448 1140 26.66 48.60
ee en umeuncct aucune 3-shot 65.67, __87.40_ _ 56.37_ _ 79.80 _ = 2.55__ 23.00_ 1.64 4.803156 _ 48.75
Qwen3-14B ee O-shot 3040 69.80 24.13 54.60 15.01 60.20 1645 54.60 21.50 59.80
oe shot 43.50 54.80 34.13 42.20 7-15 53.20 9.59 61.00 23.59 _ 52.80
O-shot 51.27. 66.40 51.00 66.40 1822 77.40 15.28 75.00 33.94 71.30

Ee eee S088 __Bsshot 46.33 5940 43.73__ 57.60 __15.77 72.80 __ 17.03 __74.60__ 30.72 __ 66.10
Qwen3-32B oe O-shot 75.97 91.60 68.43 87.00 14.96 81.20 1349 72.20 43.21 83.00
sales ; 3-shot 78.50 84.80 72.00 79.00 15.59 8140 15.06 74.00 45.29 79.80
O-shot 0.40 4.40 0.00 020 020 380 000 0.00 015 2.10

a ee shot __0.13___9.00__ 0.00 __ 1.20 __0.00___3.60__ 0.00 __0.20__ 003 __ 3.50
. O-shot 37.13 63.80 25.43 43.00 238 24.20 0.24 5.80 6.30 34.20
Oe 2B Bsshot 31.77 64.60 12.50 28.00 1.38 (39.80 0.56 8.20 1SS__35.15
O-shot 45.50 68.80 27.83 40.00 1.03 27.40 040 620 8.69 35.60

ae 21008 __Beshot 42.27, 61.20 25.57__ 37.80__ 1.56 _ 28.00 _ 0356.40 17.43__33.35
O-shot 79.43 93.00 59.50 68.60 1953 69.20 8.96 36.00 41.86 66.70

GLM4-plus >100B 3-shot 75.90 93.80 54.50 61.60 14.09 69.80 7.61 30.80 38.02 64.00
pitlmsnbieome _— O-shot 17.97 5220 7.90 3340 160 1720 049 760 699 27.60
re Zshot __6.20__ 38.40 _ _0.00__ 0.20 __ 0.00 441 __ 0.00 __0.00__ 1.55 _ _ 10.75
Phid-mini-reasonin 3.848 O-shot 480 65.20 117 50.20 238 4000 147 2060 2.46 44.00
oe B ecoouenux 3-shot 5.60 63.60, 147 49.60 3.24 43.20 136 2140 2.92 44.45
Pia sae O-shot 65.63 89.60 57.73 7640 231 36.20 2.01 2080 31.92 55.75
ee oe 3-shot 63.10 90.20 S117 _ 74.40 2.52 36.20 2.06 10.80 29.71 _ 52.90
Phil censori idee O-shot 0.90 2260 020 9.60 0.00 1020 0.07 540 0.29 11.95
eee a 3shot __0.30__ 24.20 0.17 __ 8.20 0.44 10.00 0.08 2.20 0.25 AS
Phid-reasonine-plus 7B O-shot 63.87 97.60 57.73 89.60 2744 83.40 20.56 65.40 42.40 84.00
tee ORINe Ee , 3-shot 64.17 97.60 = 57.90 89.80 27.75 = 83.60 20.47: «Ss «65.40 = 42.57_~—s 84.10
DecriSeckv3 x58 O-shot 90.07 9840 81.80 89.60 43.97 89.60 30.93 68.00 61.69 86.40
Occ ceuemeunemecee 3-shot 88.37 95.20, 74.77_ _ 84.20 25.56 79.20 19.87 52.40 52.14 77.75
HeerSeckil Aes O-shot 97.87 99.00 91.93 92.60 86.77 «98.20 76.35. «492.60-—Ss«88.23 (95.60
ho 3-shot. 97.27 =—-99.80 «92.33: 91.80 = 83.73 498.20 75.39 «92.80 87.18 95.65
oor O-shot 77.43 92.00 59.97 79.00 960 5940 5.12 3880 38.03 67.30
wee oe 3-shot _ _76.73_ 90.20, 64.33 76.20 14.55 68.60 8.60 43.40 41.05 _ 69.60
GPTs O-shot 98.40 99.60 92.93 92.00 92.50 99.40 79.72 94.40 90.89 96.35
mint - 3-shot 98.87 100.00 93.50 9280 91.66 99.80 79.72 94.80 90.94 96.85



Table 4: Performance of two top-performing models on the exHL-HN dataset in the 0-shot setting. ‘Proc’ refers
to process accuracy and ‘Ans’ refers to answer accuracy.

Model EL-EN EL-HN HL-EN HL-HN exHL-HN
Proc Ans Proc Ans Proc Ans Proc Ans Proc Ans
DeepSeek-R1 97.87 99.00 91.93 92.60 86.77 98.20 76.35 92.60 12.27 38.50

GPTS5S-mini 98.40 99.60 92.93 92.00 92.50 99.40 79.72 94.40 23.23 54.75

4 EXPERIMENT RESULTS

4.1 EVALUATION RESULTS

Most Models Perform Poorly. The evaluation results of LLMs are shown in Table Due to
the inherent difficulty of the synthesized tasks, many commonly used models perform poorly. For
instance, Llama3.1-8B-instruct achieves only 0.83 process accuracy and 5.00 answer accuracy in the
0-shot setting on HL-HN, while GPT-40 scores 5.12 and 38.80, respectively. In contrast, stronger
reasoning models like DeepSeek-R1 and GPTS-mini achieve 76.35 and 92.60, and 79.72 and 94.40,
respectively.

Analysis of Two Best Models. For GPT5-mini and DeepSeek-R1, transferring from EN to HN
results in a noticeable decrease in both process accuracy and answer accuracy, consistent with other
models. However, transferring from EL to HL causes a more significant drop in process accuracy
(e.g. from 91.93 on EL-HN to 76.35 on HL-HN for DeepSeek-R1 in the 0-shot setting), while
answer accuracy remains relatively unchanged (e.g. from 92.60 on EL-HN to 92.60 on HL-HN for
DeepSeek-R1 in the 0-shot setting). This phenomenon is observed only in these two models. This
suggests that these two models are generally capable of handling the logical reasoning complexity of
the tasks to produce correct answers, but struggle with presenting clear and logically sound reasoning
processes.

Given the strong reasoning abilities demonstrated by the two top-performing models, we further
stress-tested them using exHL-HN in the 0-shot setting. As shown in Table[4] they fail to perform
well (e.g., GPT5-mini only scores 23.23 and 54.75 as process and answer accuracy respectively),
and the results highlight GPT5-mini’s stronger reasoning ability compared to DeepSeek-R1. This
difference is less apparent in the previous four datasets, showcasing how our synthesizer can syn-
thesize datasets of varying difficulty levels to evaluate the capabilities of different models.

EL-HN v.s. HL-EN Comparison. ara 700s Process Accuracy v

Figure [2a] shows the process accu- 0 21+ _¥ sulle |}

racy and answer accuracy of differ- : e he “

ent models on EL-HN and HL-EN, 2°) ;°.°* 4 * eo :

. . . . . . <= Ve <= s

indicating that while HL slightly in- gy}, + - ae Pd

creases the difficulty for answer ac- he kg ge

curacy, models perform significantly 20) Process Accuracy canirer: i

worse on process accuracy under HL, oe? ed ol #

compared with HN. This is due to 0 20 40 60 80 100 0 20 ' 60 80 100
: -| = t

the larger reasoning depth and more MICEN “ne

complex logical rules, which signif- (a) EL-HN v.s. HL-EN (b) 0-shot v.s. 3-shot

icantly hinder the models’ ability to

provide accurate reasoning processes. Figure 2: Analyses of evaluation results.

0-shot v.s. 3-shot Comparison. Figure [2b] shows the comparison of average accuracy across dif-
ferent datasets for 29 models in the 0-shot and 3-shot settings, indicating that, across all models,
3-shot learning generally has little effect, with some models showing improvement and others ex-
periencing a decline in performance. This is mainly because the synthesized tasks are inherently
complex reasoning problems. While the provided examples can help guide the model on how to
approach the problem, they may also divert the model’s attention, leading to performance changes.
We also conducted experiments with varying numbers of few-shot examples, as detailed in Ap-
pendix{G.1] and the results remain consistent.


Table 5: Performance on external reasoning benchmarks before and after SFT on our synthetic data.

Model GSM8k MATH MATHQA — SVAMP MAWPS AIME Average
Llama3.2-1B-instruct 35.50 27.33 22.23 61.00 74.72 2.85 37.27

+ LogiNumSynth (EL-Train) 37.37 (41.87) 28.63 (+1.30) 23.00 (40.77) 61.66 (+0.66) 75.78 (+1.60) 4.22 (41.37) 38.44 (+1.17)
Qwen3-1.7B 88.77 67.16 66.52 94.33 93.99 24.49 72.54

+ LogiNumSynth (EL-Train) 89.23 (+0.46) 67.77 (40.61) 66.80 (+0.28) 95.33 (41.00) 94.52 (40.53) 25.55 (+1.06) 73.20 (+0.66)

Model RuleTaker ProofWriter FOLIO FLD Average
Llama3.2-1B-instruct 46.18 24.84 35.47 32.60 34.77

+ LogiNumSynth (EN-Train) 46.60 (+0.42) 25.80 (+0.96) 35.96 (+0.49) 32.90 (+0.30) 35.32 (+0.54)
Qwen3-1.7B 70.29 75.30 52.36 60.17 64.53

+ LogiNumSynth (EN-Train) 71.90 (41.61) 79.50 (+4.20) 62.56 (+10.20) 63.90 (+3.73) 69.47 (+4.94)

Model LogiQA ReClor AbductionR RuleArena
Llama3.2-1B-instruct 12.85 28.43 47.30 1.96

+ LogiNumSynth (EN-Train) 23.35 (+10.50) 30.20 (+1.77) 51.80 (44.50) 2.62 (+0.66)
Qwen3-1.7B 56.41 31.80 40.80 8.18

+ LogiNumSynth (EN-Train) 58.09 (+1.68) 35.80 (+4.00) 43.00 (+2.20) 10.30 (+2.12)

Case Study and Error Analysis. We conducted a case study that reveals several interesting ob-
servations. We found instances where the answer is wrong but the process accuracy is non-zero,
demonstrating that process accuracy can capture partial correctness in reasoning even when the final
answer fails. We also observed a few rare cases where a model discovers a shorter yet correct reason-
ing path that differs from the gold-standard reasoning path. This is possible and considered correct
because our synthesizer guarantees logical consistency (i.e., no derived facts, including the query,
contradict each other) while allowing the existence of multiple valid reasoning paths. For cases
where the answer accuracy equals | but the process accuracy is below 1, the errors in the reasoning
process typically fall into three categories: (1) incorrect application of rules: misapplication or
omission of relevant facts or rules, yet resulting in a correct intermediate conclusion; (2) incorrect
numerical computations: errors in intermediate numerical calculations, e.g., the final answer is
calculated as max(a, b) where a is incorrect but b is correct and larger; (3) incorrect intermediate
results: producing wrong intermediate conclusions—commonly caused by lexical errors in entities,
attributes and relationships or by reversing the direction of relationships—but ultimately yielding
the correct final answer. Representative examples are provided in Appendix |G.2|

4.2 TRAINING RESULTS

To further assess the utility of our synthetic data as an additional training resource for improving
reasoning capabilities, we fine-tuned two models on the synthesized datasets and directly evaluated
them on existing reasoning benchmarks. The results are reported in Table [5]

We first present the pre- and post-fine-tuning performance of Llama3.2-1B-instruct and Qwem3-
1.7B across multiple out-of-domain numerical and formal deductive logical reasoning benchmarks,
along with their average scores. Results show consistent improvements across all tasks, ranging from
relatively simple datasets such as GSM8k to more challenging benchmarks like AIME. Notably,
Qwen3-1.7B demonstrates a substantial 10.20 point gain on FOLIO.

We also report results on datasets that involve more diverse reasoning types—LogiQA (diverse log-
ical reasoning and reading comprehension), ReClor (natural language inference), AbductionR (ab-
ductive logical reasoning) and RuleArena (rule-guided numerical reasoning). These results illustrate
the potential of our synthetic data to serve as an effective training resource for enhancing reasoning
abilities in out-of-domain settings.

5 CONCLUSION

Recent NLP research has increasingly focused on equipping language models with robust logical
and numerical reasoning capabilities. To contribute to this direction, we introduced LogiNumSynth,
a data synthesizer designed to benchmark models’ integrated logical-numerical reasoning abilities.


With four synthetic datasets of varying difficulty levels, we evaluated 29 models covering diverse
architectures and scales, and conducted a detailed analysis of their joint reasoning performance.
In addition, we synthesized a more challenging dataset to assess the strongest two models, reveal-
ing that there remains substantial room for improvement. Beyond evaluation, LogiNumSynth also
provides synthetic training resources that can effectively enhance models’ reasoning abilities.

The extensibility of LogiNumSynth makes it a living benchmark: researchers can easily extend it
to new domains and reasoning settings by incorporating additional operations or logical constructs,
thereby enabling continuous and targeted evaluation of ever-advancing large language models. We
further discuss the limitations of our work in Appendix |B]

REPRODUCIBILITY STATEMENT

To facilitate the reproducibility of our work, we provide a detailed description of the experimen-
tal settings in Section |3| Appendix Appendix and Appendix|F} In addition, our code and

datasets are included at https://github.com/nju-websoft/LogiNumSynth|to enable independent veri-

fication of our results.

REFERENCES

Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,
Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 techni-
cal report. arXiv preprint arXiv:2412.08905, 2024.

Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao
Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning
technical report. arXiv preprint arXiv:2504.21318, 2025.

Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin
Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical
report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint
arXiv:2503.01743, 2025.

Meta AI. Llama 3.2 model card, 2024. URL https://github.com/meta-llama/
llama-models/blob/main/models/llama3_2/MODEL_CARD.md) Accessed: 2025-

09-13.

Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. Mathqa: Towards interpretable math word problem solving with operation-based for-
malisms. arXiv preprint arXiv: 1905.13319, 2019.

Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python. O’ Reilly
Media Inc., 2009.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877-—1901, 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 202 1a.

Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and
learn: Fine-tuning deep pretrained language models with less forgetting. In Bonnie Webber,
Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP), pp. 7870-7881, Online, November
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.634. URL

https://aclanthology.org/2020.emnlp-main.634/

10


Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Jana Borova, Dylan Langdon,
Reema N Moussa, Matthew I. Beane, Ting-Hao ’Kenneth’ Huang, Bryan R. Routledge, and
William Yang Wang. Finga: A dataset of numerical reasoning over financial data. In Confer-

ence on Empirical Methods in Natural Language Processing, 2021b. URL|https://api.
semanticscholar.org/CorpusID:235399966

Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language.
arXiv preprint arXiv:2002.05867, 2020.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In
North American Chapter of the Association for Computational Linguistics, 2019. URL|https:|

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.

Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao
Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghao-
ran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao
Chang. Omni-MATH: A universal olympiad level mathematic benchmark for large language
models. In The Thirteenth International Conference on Learning Representations, 2025. URL

Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu
Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng,
Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu,
Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao,
Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu,
Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan
Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang,
Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: A family of large language
models from glm-130b to glm-4 all tools, 2024.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in lms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.

Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James
Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alexander Wardle-Solano, Hannah
Szab6, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm
Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander Fabbri, Wo-
jciech Maciej Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou,
Caiming Xiong, Rex Ying, Arman Cohan, and Dragomir Radev. FOLIO: Natural language rea-
soning with first-order logic. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.),
Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,
pp. 22017-22031, Miami, Florida, USA, November 2024a. Association for Computational Lin-

guistics. doi: 10.18653/v1/2024.emnlp-main.1229. URL https: //aclanthology.org/

Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, Wenfei Zhou, Yujie Qiao, Yilun
Zhao, Semih Yavuz, Ye Liu, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Dragomir Radev, Rex
Ying, and Arman Cohan. P-FOLIO: Evaluating and improving logical reasoning with abun-
dant human-written reasoning chains. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen
(eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 16553-
16565, Miami, Florida, USA, November 2024b. Association for Computational Linguistics.

11


doi: 10.18653/v1/2024.findings-emnlp.966. URL https://aclanthology.org/2024.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300, 2020.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874, 2021.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685, 2021.

Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
Mawps: A math word problem repository. In North American Chapter of the Association for Com-

putational Linguistics, 2016. URL. https: //api.semanticscholar.org/CorpusID:
2228 719

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles, 2023.

Guillaume Lample and Francois Charton. Deep learning for symbolic mathematics. arXiv preprint
arXiv:1912.01412, 2019.

Xiao Li, Gong Cheng, Ziheng Chen, Yawei Sun, and Yuzhong Qu. Adalogn: Adaptive logic graph
network for reasoning-based machine reading comprehension. In Annual Meeting of the Associa-

tion for Computational Linguistics, 2022. URL https://api.semanticscholar.org/
Corpus1ID:247518855

Xiao Li, Yin Zhu, Sichen Liu, Jiangzhou Ju, Yuzhong Qu, and Gong Cheng. Dyrren: A dynamic
retriever-reranker-generator model for numerical reasoning over tabular and textual data. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13139-13147, 2023.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023a.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth
International Conference on Learning Representations, 2023b.

Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint
arXiv:2412.19437, 2024.

Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the
logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023.

Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A
challenge dataset for machine reading comprehension with logical reasoning. In Christian
Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial In-
telligence, IJCAI 2020, pp. 3622-3628. ijcai.org, 2020. doi: 10.24963/IJCAI.2020/501. URL

https://doi.org/10.24963/ijcai.2020/501

Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu,
Wenwei Zhang, Derek F Wong, Songyang Zhang, et al. Compassverifier: A unified and robust
verifier for llms evaluation and outcome reward. arXiv preprint arXiv:2508.03686, 2025.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Dangi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv: 1907.11692, 2019.

12


Jiangiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.
Autopsv: Automated process-supervised verifier. Advances in Neural Information Processing
Systems, 37:79935—79962, 2024.

Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for
mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022.

Man Luo, Shrinidhi Kumbhar, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya,
Chitta Baral, et al. Towards logiglue: A brief survey and a benchmark for analyzing logical
reasoning capabilities of language models. arXiv preprint arXiv:2310.00836, 2023.

Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing
English math word problem solvers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel
Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, pp. 975-984, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.92. URL{https://aclanthology .org/2020.acl-main.|

Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. Learning deductive
reasoning from synthetic corpus based on formal logic. In International Conference on Machine
Learning, pp. 25254-25274. PMLR, 2023.

Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. Enhancing reason-
ing capabilities of Ilms via principled synthetic logic corpus. In A. Globerson, L. Mackey,
D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neu-
ral Information Processing Systems, volume 37, pp. 73572-73604. Curran Associates, Inc.,
2024. URL

OpenAI.  Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL

semanticscholar.org/CorpusID: 257532815

OpenAI. Gpt-5 system card, 2025. URL

gpt-—5-system-card.pdf

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple
math word problems? In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094,
2021.

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea-
soning abilities of neural models. arXiv preprint arXiv: 1904.01557, 2019.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615, 2022.

Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs,

and abductive statements over natural language. In Findings, 2020. URL https://api.
semanticscholar.org/CorpusID: 229371222

Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL|https: //qwen1m.|

github.io/blog/qwen2.5/

Jidong Tian, Yitian Li, Wenging Chen, Ligiang Xiao, Hao He, and Yaohui Jin. Diagnosing the
first-order logical reasoning ability through logicnli. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing, pp. 3738-3747, 2021.

Hemish Veeraboina. Aime problem set 1983-2024, 2023. URL https://www.kaggle.com/
datasets/hemishveeraboina/aime-—problem-set-—1983-2024

13


Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan
Duan. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions
on Audio, Speech, and Language Processing, 30:2201—2216, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824—24837, 2022.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. As-
sociation for Computational Linguistics. URL/https://www.aclweb.org/anthology/|

Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin
Kim, Yunsheng Li, Liliang Ren, Yelong Shen, et al. Phi-4-mini-reasoning: Exploring the limits
of small reasoning language models in math. arXiv preprint arXiv:2504.21233, 2025.

Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. Faithful logical
reasoning via symbolic chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar
(eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics
(Volume I: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 13326-13365.
Association for Computational Linguistics, 2024. doi: 10.18653/V 1/2024.ACL-LONG.720. URL
https://doi.org/10.18653/v1/2024.acl-long.720

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint
arXiv:2505.09388, 2025.

Nathan Young, Qiming Bao, Joshua Bensemann, and Michael Witbrock. AbductionRules: Training
transformers to explain unexpected inputs. In Smaranda Muresan, Preslav Nakov, and Aline
Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp.
218-227, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/

v1/2022.findings-acl.19. URL https://aclanthology.org/2022.findings-acl.

19/

Fei Yu, Hongbo Zhang, and Benyou Wang. Natural language reasoning, a survey. 2023. URL
https://api.semanticscholar.org/CorpusID:257766470

Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset
requiring logical reasoning. arXiv preprint arXiv:2002.04326, 2020.

Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu,
Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical rea-
soning. arXiv preprint arXiv:2412.06559, 2024.

Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou,
and Nan Duan. Ar-lsat: Investigating analytical reasoning of text, 2021.

Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, and William Yang
Wang. RuleArena: A benchmark for rule-guided reasoning with LLMs in real-world scenar-
ios. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar
(eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pp. 550-572, Vienna, Austria, July 2025. Association for Com-
putational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.27. URL

https://aclanthology.org/2025.acl-long.27

14


A THE USE OF LARGE LANGUAGE MODELS

We used large language models to polish the writing.

B~ LIMITATIONS

The limitations of our work are as follows.

¢ Better process evaluation: ae we take nape to Beata ae e SOS [ea oran ar] O02

[Liu et al.}[2025} {Zhou et al.} [2025 055) . Current oo See ae are Senin aa ok

and a small number of such errors are inevitable, especially when intermediate reasoning
steps are lengthy, interleaved, or involve intricate cross-references that disrupt the logical
flow.

Training for more models and benchmarks: Due to limitations in computational resources,
we only experimented with fine-tuning Llama3.2-1B-instruct and Qwen3-1.7B using our
synthetic data, and evaluated them primarily on logical and numerical reasoning bench-
marks. Extending to a wider range of models, including larger-scale LLMs and diverse
architectures, as well as more benchmarks such as standard general-purpose ones, would
further validate the generality of our synthesizer and potentially yield more substantial im-
provements in reasoning performance.

Data-related limitations: First, the relationships in our synthesized worlds (e.g., “a visits
b”) do not carry real-world semantic meaning, a limitation also common in prior work on
(2024). Currently, there is no effective solution to this issue other than relying on expert
annotation [2024aJb). To mitigate ambiguity, we avoid introducing synonyms
within the same sample, though deeper semantic grounding remains an open challenge.
Second, our current synthesizer includes only conjunction and implication as logical struc-
tures, and four basic types of numerical expressions. While the synthesizer is inherently
extensible, incorporating more complex logical constructs and numerical operations is left
to future work.

C RELATED WORK

C.1 LOGICAL AND NUMERICAL REASONING DATASETS

Logical ene a au neon represent two of the most fundamental reasoning types
in NLP (Wang et al. 2023), and considerable effort has been made in logical reason-
ing (Clark et al.][2 ee and numerical reasoning (Wei et al.|{2022}{Li et al.|/2023).
Datasets involving a single type of reasoning, such as logical reasoning, including Ruletaker
(2020), ProofWriter (T: -}[2020), AR-LSAT
2021), FOLIO (Han et al.||2024a), PFOLIO
FLD (Morishita et al][2 3
as well as numerical reasoning, on OT MAWPS
2019), MathQA (Amini eta
2021b), SVAMP (P. mie NATE d AIME (
aio. Omni-MATH ¢ 2025) require models to identify Teal or numerical structures within

text and to perform ar or ao eee operations. These datasets do not provide scenarios that
simultaneously require logical and numerical reasoning.

A notable exception is RuleArena (Zhou et al.|{2025), which, to the best of our knowledge, is the only
existing benchmark that explicitly combines logical and numerical reasoning in natural language. It
includes three real-world inspired tasks (taxation, airline luggage fee calculation, and NBA trans-
action validation) that require applying logical rules together with numerical computations. While
RuleArena makes an important contribution by integrating these two forms of reasoning, it adopts a
fixed set of rules and scenarios with limited linguistic variation, resulting in repetitive task patterns
and offering little control over the logical and numerical complexity of problem instances. This

15


limits its suitability for probing models at varying levels of difficulty or for covering a broader di-
versity of reasoning scenarios. Furthermore, although the original work reports rule-level recall and
precision (providing a coarse form of process evaluation), it does not perform fine-grained, step-
by-step assessment of intermediate reasoning stages. Finally, RuleArena is positioned purely as an
evaluation benchmark, with no intended support for targeted training data generation, which further
constrains its extensibility.

These limitations motivate the development of more flexible and scalable synthesis frameworks.
Our proposed LogiNumSynth addresses these gaps by supporting precise control over reasoning
complexity, synthesizing richer and more varied natural language formulations, and incorporating
structured intermediate reasoning steps for fine-grained process-level diagnosis. The dual appli-
cability of LogiNumSynth for evaluation and targeted training enables systematic improvement of
models’ joint logical-numerical reasoning capabilities.

C.2 BENCHMARKING REASONING CAPABILITIES

RuleTaker (Clark et al.|/2020) evaluated RoBERTa (Liu et al.|/2019) using synthetic logical reason-
ing data. Based on this, FLD (Morishita et al.|/2023) and LogicNLI 2021) extended

the expressiveness of logical reasoning by introducing more deduction rules and evaluated more
models. There are also works that evaluated LLMs such as ChatGPT 2020) and GPT-
using logical reasoning problems from standard exams (Liu et al.||2023|
. HumanEval (Chen et al. /2021a), MMLU (Hendrycks et al.

(2022), and AREA a have become standard

benchmarks to evaluate the foundational language and reasoning capabilities of language models.

In contrast, we incorporate joint logical-numerical reasoning while minimizing confounding factors
such as background knowledge, to ensure that the evaluation results more purely reflect models’
reasoning capabilities. Furthermore, we conduct an evaluation of 29 models, with parameters from
0.5B to 685B, to observe the differences in their reasoning capabilities. In addition, unlike existing
(2024), which used a limited set of templates for natural language generation, we employ hundreds
of templates and an LLM to generate diverse, syntactically accurate, and semantically coherent
natural language descriptions for problem instances.

D_ FEATURES OF THE DATA SYNTHESIZER

Our data synthesizer exhibits diversity, purity, and extensibility, as described below.

D.1 DIVERSITY

Our data synthesizer achieves diversity through controllable scale and difficulty, specifically in world
richness, depth of reasoning, and arithmetic computation.

World Richness has two aspects: scale and density. Scale describes the size of the world through
counts of entities, attributes, and relationships. Density reflects the complexity of interconnections
among entities’ attribute values and relationships, as established by the numbers of facts and rules.
In worlds with higher density, these elements are more tightly intertwined, enabling richer and more
extensive inference chains. By varying scale and density, our synthesizer can produce knowledge
bases ranging from simple, sparsely connected worlds to highly interwoven reasoning environments.

Depth of Reasoning. The difficulty of reasoning can be controlled by specifying the required
depth of logical reasoning.

Arithmetic Computation. The difficulty of arithmetic computation is tuned through types of ex-
pressions (from constant expression to nested aggregation expression), their sampling distribution
ratios, and ranges of operands.

16


D.2 PURITY

Unlike existing datasets derived from real applications which essentially evaluate a hybrid of reason-
ing capability and knowledge capacity of a reasoning model, we seek to decouple reasoning from
knowledge so that our evaluation can be focused on reasoning capability. This is reflected in our
synthesizing process, where world elements are combined randomly so that the knowledge stored in
a reasoning model (e.g., commonsense knowledge learned during pre-training) is unlikely to help.
Only the facts and rules provided should be useful. It helps to support less biased evaluation and
offer a clearer view of the reasoning performance of models, representing an aspect that is pivotal
for advances in areas where reasoning is prioritized over knowledge retention and retrieval.

D.3. EXTENSIBILITY

Our data synthesizer has a high degree of extensibility. In particular, the expressions in the rules
are designed to be extensible. As mentioned in Section [2.1] we currently incorporated only four
types of expressions. However, our synthesizer is adaptable to the inclusion of additional mathe-
matical expressions such as exponentiation and trigonometric functions. We recommend selecting
different expressions based on the specific requirements to evaluate the model’s capabilities in the
corresponding scenarios.

Also, as mentioned in Section [2.1] our rule condition is a conjunction of one or more atoms. It
can be extended to support richer logical operators (e.g., disjunctions V and negations —), thereby
inducing more complex reasoning process.

E_ DETAILS ON DATA SYNTHESIZATION

We present the method for synthesizing formal representations in Appendix [E. | additional tem-
plates and details in Appendix and the configuration settings for dataset synthesis in Ap-

pendix|E.3]

E.1 DETAILS ON FORMAL REPRESENTATION S YNTHESIZATION

The sample synthesization process outlined in Section [2.2] consists of four sequential stages. Fol-
lowing the initialization of world elements and query, the subsequent two stages construct the formal
representation (Facts, Rules, Query) as defined in Section 2.1] which will be illustrated in the
following. The final stage generates corresponding natural language descriptions, with implementa-
tion details provided in Section[2.2|and Appendix|E.2|

Overall DAG Construction. Starting from world elements, query and configurations, we first
construct a reasoning directed acyclic graph (DAG) to represent the reasoning process required to
answer the query. The DAG is built in a backward, top-down, and recursive manner. The root
node is the query, and each non-leaf node represents an intermediate conclusion that can be inferred
by applying a rule to its child nodes. Each leaf node corresponds to a fact or a rule that can be
directly used to infer its parent node. The depth of the reasoning process is controlled by specified
configuration, and the DAG is constructed until the desired depth is reached.

Rule Synthesis. To synthesize a rule for inferring an intermediate conclusion or the query, we
first randomly decide the number of atoms in the rule condition, which is sampled from a range
predefined by configuration. Each atom can be either an attribute fact or a relationship fact, where
the entities are anonymized as a variable and can be instantiated with concrete entities during rule
application. For an attribute fact, we randomly select an attribute from the attribute set A of world
elements. For a relationship fact, we randomly select a relationship from the relationship set R. The
conclusion of the rule is anonymized by the target intermediate conclusion or the query, so that it
can be inferred when the rule is applied. Specially, for an attribute fact in the rule conclusion, an
expression is needed to compute the attribute value.

To synthesize an expression, we first randomly select the type from the supported expression types
with probabilities predefined by configuration, and then sample the parameters or other components

17


Algorithm 1 Reasoning DAG Construction (Part 1/2)

Require:

1: World elements E, A, R

2: Query q

3: Configuration con fig
Ensure:

4: DAGG

5: Facts Facts
6: Rules Rules
Zi
8

: function CONSTRUCTDAG(E, A, R, g, con fig)

; statey + (0,0,0,0) > Initialize empty state: (G, Facts, Rules,C)

9: state final ~- BUILDNODE(q, 0, stateo, con fig) > Build DAG starting from query

10: Extract (G, Facts, Rules,C) from state final > Extract final components
11: return G,Facts,Rules

12: end function

13: function BUILDNODE(node, depth, state, con fig)
14: Extract (G, Facts, Rules,C) from state

15: if depth = con fig.depth then > At maximum depth, must synthesize fact

16: return S YNTHESIZEFACT(node, state)

17: else

18: if currently last node to synthesize then > Not at required depth, must synthesize rule
for last node

19: return S YNTHESIZERULE(node, depth, state, con fig)

20: else

21: p + random((0, 1]) > Roll dice for synthesis strategy

22: if p < 0.5 then

23: return S YNTHESIZERULE(node, depth, state, con fig)

24: else

25: return S YNTHESIZEFACT(node, state)

26: end if

D7 end if

28: end if

29: end function

required by the selected expression type. For calculation expressions, we randomly select the pa-
rameters k and b from the ranges specified in the configuration. For aggregation expressions, we
uniformly select the aggregation operator from the supported set and recursively synthesize the two
sub-expressions based on the predefined probabilities of expression types in the configuration.

It is important to ensure that the anonymized entities in the rule conclusion also appear in the rule
condition, to avoid introducing new entities during rule application.

Recursive Node Building. After synthesizing a rule to infer an intermediate conclusion or the
query, more intermediate conclusions may be needed to infer the atoms in the rule condition. We
recursively synthesize rules or direct facts for these intermediate conclusions until the desired depth
is reached. The decision to synthesize a rule or a fact is made uniformly at random, except at the
maximum depth (where a fact must be synthesized) or for the last node at a given level (where a rule
is required to ensure progression toward the desired depth).

The algorithm for constructing the reasoning DAG is shown in Algorithm[I]and Algorithm [2] The
main procedure begins with the CONSTRUCTDAG function, which initializes an empty state com-
prising the DAG G, facts Facts, rules Rules, and inferable conclusions C. It then recursively
builds the DAG starting from the query node via BUILDNODE. In BUILDNODE, the synthesis strat-
egy depends on the current depth: at the maximum depth, a fact is synthesized; otherwise, a rule is
mandatory for the last node at that level to extend the depth, while for others, the choice between
tule and fact is randomized with equal probability.

18


Algorithm 2 Reasoning DAG Construction (Part 2/2)
30: function S YNTHESIZEFACT(node, state)

31: Extract (G, Facts, Rules,C) from state

32; repeat > Generate facts until finding a valid one
33: Generate fact f for node

34: Ctemp <- UPDATECONCLUSIONS(C, Facts U{f}, Rules)

35: valid < (no conflict between f and Cremp)

36: until valid

37: Facts’ ¢+ FactsU{f}

38: Update G with f to get G’

39: Ce Ctemp

40: return (G’, Facts’, Rules,C’)
41: end function

42: function S YNTHESIZERULE(node, depth, state, con fig)
43: Extract (G, Facts, Rules,C) from state

44: repeat > Generate rules until finding a valid one
45: Generate rule r for node

46: Ctemp <- UPDATECONCLUSIONS(C, Facts, Rules U {r})

47: valid < (no conflict in Czemp and no cycle with r in G)

48: until valid

49: Rules’ ¢ Rules U {r}

50: Update G with r to get G’
Sls Cl & Chem

52: final_state + (G’, Facts, Rules’,C’)

53: for each atom a required in r’s condition do > Build prerequisite atoms recursively
54: final_state <- BUILDNODE(a, depth + 1, final_state, con fig)

55: end for

56: return final_state

57: end function

58: function UPDATECONCLUSIONS(C, Facts, Rules)

59: Courrent <- CUFacts > Start with existing conclusions and facts
60: repeat > Apply rules until fixed point is reached
61: Cota <— Ceurrent

62: for each rule r in Rules do > Try to apply each rule
63: if r can be triggered by Coy pene then

64: Derive new conclusion c from r

65: Ceurrent — Courrent U {c}

66: end if

67: end for

68: until Ceurrent = Cota

69: return Couprent

70: end function

Conflict Detection and Validation. Every time we synthesize a rule or a fact, we ensure that
it does not lead to multiple, inconsistent derivations when combined with the existing rules and
facts, thereby maintaining uniqueness of the inferred results. Specifically, we maintain a set of all
conclusions that can be inferred by the existing rules and facts, and ensure that the new rule or fact,
along with its triggered conclusions, does not conflict with them. Additionally, we also ensure that
the reasoning DAG remains acyclic, i.e., no intermediate conclusion needs to be used to infer itself.
If a conflict is detected or a cycle is introduced, we discard the newly synthesized rule or fact and
re-synthesize it until no conflict or cycle is found.

The SYNTHESIZEFACT function generates a fact for the node and iteratively checks for conflicts by
temporarily updating the conclusions using UPDATECONCLUSIONS. Once a valid fact is found, it
updates the state accordingly. Similarly, SYNTHESIZERULE generates a rule, verifies it for conflicts

19


and cycles, and upon validation, recursively calls BUILDNODE for each atom in the rule’s condition
to build prerequisite substructures.

Termination of UPDPATECONCLUSIONS. UPDATECONCLUSIONS computes the iterative closure
of a monotone consequence operator T’ : 2 —> 2“, where one step updates the current conclusion
set C by T(C) = CUInfer(C, Rules). The universe U/ consists of (i) attribute assignments is(e, a, v)
and (ii) binary relations r(e;,e;). We impose the restriction that for any fixed entity-attribute pair
(e, a), at most one value v is admissible. Given finite sets of entities E, attributes A, relations R, and
numeric values V (bounded by current facts and rule parameters), U/ is finite, with / = {is(e, a, v) |
e€ Bac A,ve V}U {r(ei,e;) | r € R,e;,e; € E} and cardinality at most |E||A] + |R||E|?
for binary r. Since (4, C) is a finite lattice and T is monotone, the ascending chain Cy CC; C-:-
must stabilize after finitely many steps, yielding the least fixed point reachable from the initial state.
Conflicts are detected during closure and rejected, ensuring the result is both comprehensive for
conflict checks and consistent.

Distraction Synthesization. We synthesize additional, potentially irrelevant, facts or rules to dis-
tract the reasoning process. These are randomly generated and added to the number specified in
the configuration, while ensuring they do not conflict with existing knowledge. This increases the
complexity of the knowledge base, making it harder for models to locate and use the information
needed to answer the query.

E.2 DETAILS ON NATURAL LANGUAGE GENERATOR

Example templates used in the first step of natural language generator for different formal represen-
tations are as follows:

1. Attribute fact (i.e. is(e;,a;, num)):
* the value of {a;} for {e:} is {num}.
¢ {num} is recorded for {e;} in the {a,} attribute.
* the {a,;} property for {e;} is given by {num}.
* the {a;} field of {e;} is represented by {num}.
2. Relationship fact (i.e., rx (ei, €;)):
* it can be said that {e:} {rx} {e;}.
* the {rx} correlation is present between {e;} and {e;}.
* in the context of {rx}, {e;} and {e,} share a connection.
* the relationship {r;,} defines a connection between {e;} and {e;}.
3. Implication (i.e., condition > conclusion):
¢ {conclusion} can be deduced from {condition}.
¢ {conclusion} is a natural consequence of {condition} being true.
* given {condition}, {conclusion} follows.
¢ if {condition}, {conclusion}.
4. Retrieval expression (i.e., €4,[a;]):
¢ the value of {a;} for {eq}.
* property {a;} of {eq}.
¢ the value associated with the attribute {a;} of {eq}.
¢ the value corresponding to {a;} within {e,}.
5. Calculation expression (addition, i.e., k x eq[a;] + b):
* multiplying {e.[a;]} by {k} and adding {0}.
* multiplying {e.[a;|} by {k} with an addition of {b}.
* scaling {e,[a;]} by {k}, then adding {b}.
* adding {b} to {k} times {e,[a;]}.
6. Aggregation expression (max, i.e., max(expr1, expr2)):

¢ the larger of {expr1} and {expr2}.

20


Table 6: Prompt template and examples we used in the second step of natural language generator.

Prompt. Given a formal representation in logical form, optimize the corresponding template representation
into a fluent, grammatically correct natural language expression that clearly conveys the meaning of the formal
representation. Keep the following guidelines in mind:

1. When transforming the template into natural language, ensure that the key terms and relationships (such as
specific attributes of entities and the nature of the relationships between them) are preserved exactly. Do not
alter critical terms like attribute names, relationships, or the order in which relationships appear.

2. For rule-based formal representations, make sure the causal inference is clearly expressed in the template,
highlighting the cause-effect relationship where appropriate.

3. You don’t need to strictly follow the template of the examples. Prioritize fluent and diverse language
expression, and simply provide the optimized sentence as your answer.

Examples:

{examples }

formal representation: {formal representation}

template representation: {template-based natural language description}

output:

Examples for facts refinement.

formal representation: is(Susana, low, -8)

template representation: The low of Susana is recorded as -8.
output: Susana’s low attribute is recorded as -8.

formal representation: is(Lynn, under, 10)

template representation: The under property for Lynn is represented by the value 10.
output: The under property for Lynn is represented by the value 10.
formal representation: sacrifice(Cecilla, Terrianne)

template representation: Cecilla sacrifice Terrianne.

output: Cecilla sacrifices Terrianne.

formal representation: flood(Jorie, Woodrow)

template representation: It can be said that Jorie flood Woodrow.
output: It can be said that Jorie flooded Woodrow.

Examples for rules refinement.

formal representation: defuse(entity_1, entity_2) => is(entity_2, technical, subtraction(3 * entity_2[retained]
+ 8, 5 * entity_2[proven] + 2))

template representation: The difference between multiplying the characteristic retained of entity_2 by 3 with
a subtraction of 8 and increasing entity_2’s proven 5 times, followed by adding 2 is associated with entity_2
for the technical attribute is a natural consequence of there exists a relationship defuse between entity_1 and
entity_2 being true.

output: The relationship between entity_1 and entity_2, denoted as "defuse," implies that entity_2’s technical
attribute is the result of subtracting the value of five times entity_2’s proven characteristic plus 2 from three
times entity_2’s retained characteristic plus 8.

formal representation: is(entity_1, asian, -1) and is(entity_1, entire, -10) and revise(entity_2, entity_3) =>
block(entity_3, entity_1)

template representation: Given the asian of entity_1 is -1 and the value -10 is logged for entity_1 under the
entire attribute and the revise link is observed between entity_2 and entity_3, entity_3 and entity_1 form a
connection of the block relationship follows.

output: Considering that the value -1 is recorded for entity_1 under the asian attribute, and -10 is logged
for entity_1 under the entire attribute, along with the existence of a revise relationship between entity_2 and
entity_3, it follows that a block relationship is formed between entity_3 and entity_1.

formal representation: is(entity_1, rental, -9) => is(entity_1, monthly, min(5 * entity_1[interstate], 1 * en-
tity_1[deep]))

template representation: The minimum value of scaling the property denoted by interstate for entity_1 by 5 and
1 times the value corresponding to deep within entity_1 is ascribed to entity_1 for the monthly attribute can be
safely inferred from within entity_1, the rental attribute is noted as -9.

output: Given that the rental attribute for entity_1 is recorded as -9, it can be inferred that the monthly attribute
for entity_1 is assigned the minimum value between 5 times the value of interstate and the value of deep within
entity_1 times 1.

formal representation: shop(entity_1, entity_2) and is(entity_3, operating, -6) => is(entity_2, foreign, 0 *
entity_2[weakened])

template representation: Given the relationship shop defines a connection between entity_1 and entity_2 and
entity_3 is described by -6 within the operating context, the value decreasing the property denoted by weakened
for entity_2 by 0 times is ascribed to entity_2 for the foreign attribute follows.

output: Given that the shop relationship exists between entity_1 and entity_2, and entity_3 is described as -6
within the operating context, it follows that the foreign attribute for entity_2 is assigned the value of 0 times the
weakened property of entity_2.

21


¢ the maximum value between {expr1} and {expr2}.
¢ the greater of {expr1} and {expr2}.
¢ the maximum of {expr1} and {expr2}.

In the second step, we used Qwen3-8B with thinking mode disabled, running on NVIDIA RTX 3090
and RTX 4090 GPUs with the vLLM framework and PyTorch 2.x. The sampling
parameters were: temperature = 0.7, top p = 0.8, and top k = 2. The prompt template and few-shot
examples are shown in Table[6]

E.3. DETAIL ON SYNTHESIZATION CONFIGURATIONS

We synthesized 7 datasets for distinct purposes; their detailed configurations are summarized in
Table [7] The first four (EL-EN, EL-HN, HL-EN, HL-HN) target the evaluation of general model
reasoning. The exHL-HN dataset is designed to stress-test the best two LLMs: GPT5-mini and
DeepSeek-R1; for this dataset, we explicitly synthesized four subsets with reasoning depths from 7
to 10, each containing 100 samples, rather than sampling depths uniformly. The last two (EL-Train,
EN-Train) are training resources for strengthening reasoning capabilities of smaller models.

The sizes of the world element sets E, A, R are governed by #Entities, #Attributes, and #Relation-
ships. The distraction module expands the knowledge base until the specified #Facts and #Rules
are reached. World richness (scale and density) is thus jointly shaped by world sizes and the counts
of facts and rules. The Reasoning Depth parameter defines the sampling range for the target infer-
ence depth that the DAG constructor is required to realize. #Condition gives either a fixed number
or a uniform range for the atoms in a rule body (e.g., [2,3] means sample uniformly from {2, 3}).
Expression Weights define the sampling distribution over expression types (constant, retrieval, cal-
culation, aggregation). For example, “0 0 1 1” permits only calculation and aggregation, equally
likely. Aggregation Weights analogously define the distribution of sub-expression types (constant,
retrieval, calculation) inside an aggregation (e.g., “1 1 1” = uniform). The intrinsic parameters of
calculation expressions (k,b in expr(eg,a;) = k X e€q[a;] + 6) and the attribute value ranges in
facts are controlled by Oprand Range. For instance, “[-100, 100] means uniform sampling from
{—100, —99,...,99, 100}; negative b implicitly yields a subtraction form. Finally, Size is the num-
ber of synthesized samples in the dataset.

Our synthesizer offers independent control over structure, interaction density, reasoning depth, rule
complexity, numeric difficulty, and dataset size, which supports precise diagnosis during evaluation
and serves as a targeted training resource to enhance reasoning capabilities.

F IMPLEMENTATION DETAILS

We summarize additional implementation details, including the evaluation settings (Appendix [Fip,
the process accuracy implementation (Appendix |F.2), and the training settings (Appendix|F.3).

F.1 EVALUATION SETTINGS

We accessed the GPT series (GPT-40 and GPT5-mini), DeepSeek series (DeepSeek-V3 and -R1),
and three models from the GLM4 series (GLM4-airx, -0520, and -plus) via API calls with default
hyperparameters. For other LLMs, experiments were conducted on NVIDIA RTX 3090 and 4090

GPUs using vLLM and PyTorch 2.x. The sampling parameters were set to
temperature = 0.3, top p = 0.8, and top k = 20. The prompt used is listed in Table|8| and the few-shot
examples, synthesized along with the corresponding datasets, can be found Tae aE aS

Table 7: Detailed configurations of the synthetic datasets in our experiments.

Dataset #Entities #Attributes #Relationships #Facts #Rules Reasoning Depth #Condition Expression Weights Aggregation Weights Oprand Range Size
EL-EN 10 15 10 15 15 (1, 3] 1 1110 111 [1, 10] 500
EL-HN 10 15 10 15 15 {1, 3] 1 OO11 111 [-100, 100] 500
HL-EN 10 15 10 15 15 [4, 6] [2, 3] 1110 111 [1, 10] 500
HL-HN 10 15 10 15 15 [4, 6] (2, 3] OoO11 111 [-100, 100] 500
exHL-HN 30 40 40 15 x Depth 5 x Depth [7, 10] [3, 6] OO11 111 [-100, 100] 400
EL-Train 10 15 10 15 15 (1, 3] 1 1122 112 [-100, 100] 5000
EN-Train 10 15 10 15 15 [4, 6] 23] 1110 111 [1, 10] 5000



Table 8: Prompt template we used for evaluating LLMs.

# Task
Analyze a logical scenario with entities, their attributes, and relationships. Use the given facts and
rules to answer the query through step-by-step reasoning.
## Key Components
- **Entities**: Objects in the scenario
- ** Attributes**: Properties of entities (with specific values)
- **Relationships**: Asymmetric connections between entities (direction matters)
- **Facts**: Given information about attributes and relationships
- **Rules**: If-then statements for logical deduction
- **Query**: Question to answer
## Instructions
1. **Natural Analysis**: First, think through the problem freely using clear, natural language.
Explain your reasoning process, identify relevant entities, attributes, and relationships, and work
toward the solution.
2. **Final Summary**: After your analysis, provide a structured reasoning summary that shows the
key logical steps that lead to the answer.
3. ** Answer Format**: End with “Answer:
boxed { [value] }””
## Final Summary Requirements
Your summary should list only the complete reasoning steps in this format:

[dependencies] => int_[n]: [conclusion]

**For relationships**: ~rule_X & fact_Y & int_Z => int_n: [relation] exists between [A] and [B]~
**For attributes**: “rule_X & fact_Y & fact_Z & int_W => int_n: [Entity]’s [attribute] is
[final_value]~

### Critical Requirements for AttributeFacts:

- Must show the **final calculated value** (e.g., “is 22”), not intermediate expressions

- Must list **ALL dependencies**: the triggering rule and conditions + all facts/intermediates that
provide values for the calculation

### Summary Example

Reasoning:

rule_15 & fact_13 & fact_4 => int_1: reject exists between Sterne and Beilul
rule_5 & fact_1 & fact_10 & fact_11 & fact_5 => int_2: Nils’s prior is 22
rule_8 & int_1 & int_2 & fact_7 => int_3: final_entity’s target_attribute is 15

Answer: \boxed{37}
{examples }
{querying sample: facts, rules, assertion}

F.2.) IMPLEMENTATION OF PROCESS ACCURACY

Our data synthesizer synthesizes tasks whose reasoning processes can naturally be represented as
a reasoning DAG, where each node denotes a known fact, a rule, or an intermediate conclusion
(also treated as a fact). As shown in Figure[I] a non-leaf node is derived jointly from all the nodes
pointing to it. With the gold-standard reasoning steps produced during synthesis, our dataset is
inherently suitable for evaluating a model’s reasoning process and gaining deeper insights.

To achieve this, we adopt a dual-extraction approach that combines a text-based parser with an LLM-
based structured extractor. Both components operate in parallel and cross-reference each other to
accurately reconstruct the reasoning process in a structured form. For the LLM-based extraction,
we employ vLLM [2023)’s structured output mode, which enforces JSON-formatted
outputs during decoding. Specifically, we use Qwen3-8B with the temperature set to 0 and the
thinking mode disabled. The prompt used for this structured extraction is provided in Table[9]

23


Table 9: Prompt template for extracting structured reasoning outputs, where {attributes_list} and {rela-
tions_list} denote the attribute set A and relationship set R, respectively, provided as guidance for the ex-
traction.

You are a logical reasoning assistant. Your task is to analyze the given reasoning process and refor-
mat it into a specific structured format.

Requirements:

1. After completing your analysis, summarize the key reasoning steps in the specified structured
format

2. The structured format is only required at the end as a summary - your main explanation can be in
natural language

3. For the final answer, always use: “Answer: \boxed{[value]}”

Structured Summary Requirements:

“Reasoning:

rule_15 & fact_13 & fact_4 => int_1: relation_name exists between first_entity and second_entity.
rule_5 & fact_1 & fact_10 & fact_l1 & fact_S => int_2: entity_name’s attribute_name is at-
tribute_value.

Answer: \boxed{answer_value}”

Format Guidelines:

- Each reasoning step should be expressed as: [rule/fact combinations] => int_[n]: [intermediate
conclusion]

- Express relationships as “[relation] exists between [X] and [Y]”

- Express attributes as “[X]’s [attribute] is [value]”

- Use logical operators: & (and)

- Number intermediate conclusions sequentially (int_1, int_2, etc.)

Please analyze the following reasoning process and reformat it into the structured format specified
above.

Original Answer:

{raw output of Ilm}

Available Attributes: {attributes_list}

Available Relations: {relations_list}

After extracting the structured reasoning process of the tested model, we verify each reasoning step
by checking: (1) whether the facts and intermediate conclusions used satisfy the conditions of the
corresponding rule, and (2) whether the derived conclusion is correct. We then compare all verified
correct steps with the gold-standard reasoning steps.

If the model fails to produce the correct final answer, its score is determined by the proportion of
intermediate conclusions from the gold-standard reasoning that it correctly derives (i.e., the reason-
ing steps are verified), regardless of whether it follows the exact same reasoning path. If the model
produces a fully correct reasoning chain that leads to the correct final answer, even if its steps differ
from the gold-standard sequence, we assign the maximum score.

Process accuracy requires a model not only to produce the correct final answer, but also to present
a clear, logically sound, and verifiable reasoning process. This is particularly important in scenarios
such as automated theorem proving, multi-step medical diagnosis reasoning, and financial auditing,
where the transparency, rigor, and correctness of each step are as critical as the final result. Unlike
answer accuracy, which is either 0 or 1, process accuracy can award partial credit based on the
correctness of intermediate steps. Thus, even if the final answer is wrong, models demonstrating
partially correct reasoning can still receive proportionate scores.

F.3. TRAINING SETTINGS

We fine-tuned the models using PyTorch 2.x, Transformers and Peft (Wolf et al.||2020), adopting
the LoRA configuration of r = 32, lora_alpha = 64, and lora_dropout = 0.05. The target modules

included q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj. The batch size was set
to 8, and the learning rates for different benchmarks are listed in Table [10] All experiments were
conducted on NVIDIA RTX 3090 and RTX 4090 GPUs.

24


For the Llama model, we utilized the Recall Adam Optimizer with the following
hyperparameters: (6; = 0.9, 82 = 0.999, «=1 x 10-8, anneal_fun = sigmoid, anneal_k = 0.2,
anneal_t0 = 100, anneal_w = 1.0, and pretrain_cof = 3000 for the benchmarks: GSM8k, MATH,
MATHQA, SVAMP, MAWPS, AIME, RuleTaker, ProofWriter, LogiQA, ReClor and AbductionR.
For all other benchmarks (FOLIO and FLD), the standard Adam Optimizer, with 6; = 0.9, 62 =
0.999, and e = 1 x 10-8, was employed.

For the Qwen model, the Recall Adam Optimizer was applied to the benchmarks: GSM8k, MATH,
MATHQA, SVAMP, MAWPS, AIME, LogiQA, and RuleArena with identical hyperparameters.
The standard Adam Optimizer was used for the remaining benchmarks, including RuleTaker,
ProofWriter, FOLIO, FLD and AbductionR.

We used different synthesized training datasets depending on the benchmark category: (1) mod-
els evaluated on mathematical reasoning benchmarks were fine-tuned on the synthesized EL-Train
dataset to enhance numerical reasoning skills, whereas (2) models evaluated on logical reasoning
benchmarks (including RuleArena) were fine-tuned on the synthesized EN-Train dataset to enhance
logical reasoning skills.

During fine-tuning, we used the validation set provided with each benchmark. If no validation set
was available, we randomly sampled a subset from the train split. For the AIME and MAWPS bench-
marks, which do not include official validation and train splits, we instead used the test set of AS-
Div as the proxy validation set. Similarly, for the RuleArena benchmark, which
does not include official validation and train splits, we utilized the test set of AR-LSAT

2021) as the proxy validation set.

For evaluation, we used vLLM 2023) with sampling parameters set to temperature =
0.3, top_p = 0.8, and top_k = 20.

G ADDITIONAL EXPERIMENT RESULTS

G.1 EFFECT OF FEW-SHOT EXAMPLE QUANTITY COMPARED TO ZERO-SHOT

To further investigate how varying the number of in-context examples affects model accuracy, we
conducted additional experiments with k € {0,1, 2,3, 4,5} shot settings across 6 selected evaluated
models. The results are summarized in Table Consistent with the observations reported in
Section|4.1] the performance differences across different / do not exhibit a consistent trend: in some
cases, adding examples yields notable improvements, whereas in others it leads to performance
drops. This indicates that, for the highly complex reasoning tasks, increasing the number of in-
context examples does not guarantee better performance and may sometimes even be harmful.

G.2 REPRESENTATIVE CASES
As described in Section[4.1| we conducted a case study. Below, we present representative examples

illustrating both the robustness and advantage of process accuracy, as well as three typical types of
process errors.

Robustness and Advantage of Process Accuracy. Case [I] and Case [2] demonstrate situations
where the model’s reasoning process followed a different order from the gold-standard reasoning

Table 10: Benchmark-specific learning rates applied during fine-tuning on our dataset for evaluation purposes.

Model GSM8k MATH MATHQA SVAMP MAWPS' AIME

Llama3.2-1B-instruct 2e-8 8e-9 2e-8 8e-9 8e-9 8e-9

Qwen3-1.7B le-7 2e-8 Se-8 Se-8 8e-8 8e-8
Model RuleTaker ProofWriter FOLIO FLD LogiQA ReClor AbductionR RuleArena
Llama3.2-1B-instruct 2e-8 Se-7 5e-8 2e-7 Se-7 2e-8 5e-7 5e-8
Qwen3-1.7B 2e-8 2e-7 2e-7 Se-7 Se-7 Se-7 2e-8 2e-7

25


Table 11: Performance of LLMs under varying few-shot settings. ‘Proc’ refers to process accuracy and ‘Ans’
refers to answer accuracy.

Model #Params. #Shots EL-EN EL-HN HL-EN HL-HN Average
Proc Ans Proc Ans Proc Ans Proc Ans Proc Ans

0-shot 4.07 33.20 0.10 11.60 0.20 14.00 0.00 3.00 1.09 15.45
-shot 4.30 36.60 0.47 9.20 0.20 19.80 0.00 2.80 1.24 17.10
2-shot 1.60 22.80 0.10 4.20 0.4 22.20 0.07 2.60 0.55 12.95
3-shot 1.57 27.80 0.40 4.60 0.4 20.40 0.00 4.00 0.61 14.20
4-shot 1.50 26.60 0.10 4.40 0.5 23.20 0.00 3.80 0.53 14.50
5-shot 2.27 31.00 0.40 9.60 1; 25.80 0.00 3.40 0.95 17.45

20.87 50.20 5.90 16.00 2.31 34.00 0.83 5.00 7.48 26.30
12.33 44.60 127 7.40 0.18 27.80 0.13 4.80 3.48 21.15
14.10 46.60 Sd 9.80 0.55 32.00 0.22 3.20 4.61 22.90
12.60 48.60 2.97 13.20 0.29 34.60 0.17 5.20 4.01 25.40
5
1

Llama3.2-3B-instruct 3.21B

ReNwW

Llama3.1-8B-instruct 8.03B

13,13 50.60 2.80 11.20 0. 30.80 0.37 5.00 4.11 24.40
30.60 0.04 5.20 4.00 22:55

69.30 90.80 27.80 60.80 5:05 31.40 4.48 11.40 26.66 48.60
72.13 87.80 34.23 56.80 0.45 5.80 0.20 1.60 26.76 38.00
78.83 90.20 62.53 81.00 0.57 13.60 0.65 4.20 35.65 47.25

Qwen3-8B S108 65.67 87.40 56.37 79.80 2.55 23.00 1.64 4.80 31.56 48.75
79.97 92.80 46.90 62.00 0.05 9.80 119 4.00 32.03 42.15

80.20 92.60 68.37 81.60 0.55 12.80 1.99 6.40 37.78 48.35

7597 91.60 68.43 87.00 14.96 81.20 13.49 72.20 43.21 83.00

80.03 86.00 7177 81.00 16.05 83.40 13.77 73.60 45.40 81.00

Qwen3-32B 32.8B 79.13 85.20 74.23 81.40 15.76 83.20 14.77 74.20 45.97 81.00
. 78.50 84.80 72.00 79.00 15.59 81.40 15.06 74.00 45.29 79.80

71.70 78.00 75.80 82.60 15.16 83.80 17.31 78.40 44.99 80.70

79.10 85.00 74.27 80.00 15.74 84.20 15.48 75.20 46.15 81.10

65.63 89.60 S773 76.40 2.31 36.20 2.01 20.80 31.92 59.13,

65.10 89.80 55.13 77.20 3.02 19.80 1.43 10.60 31.17 49.35

Phi4 14.7B 65.17 88.40 55.90 74.00 1:55 22.40 1,53 10.40 31.04 48.80

63.10 90.20 51.17 74.40 2.52. 36.20 2.06 10.80 29.71 52.90
67.50 92.20 54.50 76.40 2.39 25.80 2.12 21.80 31.63 54.05

63.87 97.60 3273 89.60 27.44 83.40 20.56 65.40 42.40 84.00
61.53 96.80 55.50 89.60 28.30 83.60 20.14 65.20 41.37 83.80
62.50 96.80 55.13 89.60 28.05 83.20 20.39 65.40 41.52 83.75
64.17 97.60 57.90 89.80 27.75 83.60 20.47 65.40 42.57 84.10
62.10 97.00 55.53 89.60 28.10 83.20 20.18 65.60 41.48 83.85
61.50 96.80 54.90 89.60 28.79 83.20 20.28 65.40 41.37 83.75

Phi4-reasoning-plus 14.7B

process, yet process accuracy was correctly awarded. Case [3 illustrates that when the model pro-
duced the relationship hampers, which differs from the gold-standard relationship hamper but shares
the same lemma, the evaluation correctly handled this case. Case |4|presents an example where the
model derived the correct answer through an entirely different reasoning path than the gold-standard
one, essentially finding a shortcut. This is possible and considered correct because our synthesizer
guarantees logical consistency (i.e., no derived facts, including the query, contradict each other),
while allowing the existence of multiple valid reasoning paths. These cases collectively highlight
the robustness of our process accuracy evaluation.

Moreover, in Case|2| the model made an error only in the final computation step. As a result, answer
accuracy assigned a score of 0, whereas process accuracy awarded 2. Similarly, in Case(5| the model
correctly derived an intermediate step that contributed toward the final answer, and process accuracy
appropriately gave partial credit. In Case[I] however, the model miscalculated 25913 as 26047, but
due to the presence of an expression min(88 x 26047 — 96, —69) = —69, the final answer happened
to be correct. In this case, answer accuracy assigned a score of 1, while process accuracy awarded
0.6, successfully identifying the error. These cases collectively highlight the advantage of process
accuracy over answer accuracy. Whereas answer accuracy provides a coarse, binary evaluation,
process accuracy offers a more fine-grained and informative assessment: it not only recognizes
partially correct reasoning steps but also distinguishes between coincidentally correct answers and
genuinely correct reasoning, thus offering a more faithful measure of a model’s reasoning ability.

Cases of Typical Process Errors. Case|6]and Case[]illustrate incorrect application of rules. In
Case [6] the model mistakenly applied an irrelevant fact, while in Case [it omitted a relevant fact;
nevertheless, both cases still led to the correct intermediate or final conclusion. Case[l]is an example
of incorrect numerical computations, where the model made an error in an intermediate calculation,
but the mistake was masked by a subsequent min operation, resulting in the correct final answer.
Additionally, Case [8] and Case [9] demonstrate incorrect intermediate results. In Case [8] the error

26


arose from a lexical mistake in an entity, while in Case[)| it resulted from reversing the direction of
a relationship.

Process Accuracy = 0.6, Answer Accuracy = 1

Gold-Standard Reasoning Process:
rule_6 & fact_2 & fact_8 => fact_il: resubmit exists between Arvie
and Ruperta.
rule_11 & fact_5 & fact_6 & fact_3 & fact_12 => fact_i2: Arvie's
whole is -71 * -92 ST 6475.
rule_14 & fact_11 & fact_i2 & fact_1 & fact_7 => fact_i3: Arvie's
imposing is 70 * 99 —- 82 = 6848.
rule_13 & fact_10 & fact_14 & fact_15 & fact_i2 => fact_i4: Arvie's
canadian is subtraction(4 * 6475 - 67, -80) = 25913.
rule_2 & fact_4 & fact_i3 & fact_il & fact_i4 & fact_13 => fact_i5d5:

Granville's withdrawn is min(88 * 25913 - 96, -69) =

-69.

Structured Reasoning Process:

rule_1ll1 & fact_3 & fact_6 & fact_5 & fact_12 => fact_il: Arvie's

whole is 6475.

rule_14 & fact_1l & fact_il & fact_11 & fact_7 => fact_i2: Arvie's

imposing is 6848.

rule_6 & fact_8 & fact_2 => fact_i3: resubmit exists between Arvie

and Ruperta.

rule_13 & fact_15 & fact_14 & fact_10 & fact_il => fact_i4: Arvie's

canadian is 26047.

rule_2 & fact_i2 & fact_i3 & fact_4 & fact_i4 & fact_13 => fact_id:
Granville's withdrawn is -69.

Process Accuracy = 2, Answer Accuracy = 0

Gold-Standard Reasoning Process:
rule_14 & fact_18 & fact_1l & fact_9 & fact_16 & fact_2 => fact_il:
Suki's acquired is addition(-66 * 53 - 82, 60 * -79 + 67) =

=8253..

rule_5 & fact_3 & fact_14 & fact_11 => fact_i2: Katrinka's
impossible is 7 * -52 - 74 = -438.

rule_10 & fact_13 & fact_6 & fact_15 => fact_i3: Marlyn's acquired
is -65 * 86 + 39 = -5551.

rule_7 & fact_12 & fact_il & fact_17 => fact_i4: Suki's executive
is 80 « 42 - 8 = 3352.
rule_11 & fact_8 & fact_i2 & fact_5 & fact_i3 & fact_7 => fact_id:

Jeramie's soaring is subtraction(-33 * -5551 12, =23)
183199.

rule_6 & fact_i4 & fact_4 & fact_10 & fact_i5 => fact_i6: Suki's
alternative is -84 * 183199 — 48 = -15388764.

Structured Reasoning Process:

rule_5 & fact_3 & fact_14 & fact_11 => fact_il: Katrinka's

impossible is -438.

rule_10 & fact_6 & fact_13 & fact_15 => fact_i2: Marlyn's acquired

is S555),

rule_11 & fact_5 & fact_il & fact_8 & fact_i2 & fact_7 => fact_i3:
Jeramie's soaring is 183199.

27


rule_14 & fact_18 & fact_1 & fact_9 & fact_16 & fact_2 => fact_i4:
Suki's acquired is -8253.

rule_7 & fact_12 & fact_i4 & fact_17 => fact_i5: Suki's executive
is 3352.

rule_6 & fact_4 & fact_10 & fact_i5d & fact_i3 => fact_i6é: Suki's
alternative is -15388668.

Case 3: DeepSeek-R1 in 0-shot setting on EL-EN-184

Process Accuracy = 1, Answer Accuracy = 1

Gold-Standard Reasoning Process:

rule_13 & fact_15 => fact_il: stack exists between Ed and Claresta.

rule_8 & fact_il => fact_i2: hamper exists between Ed and Claresta.

rule_4 & fact_i2 & fact_2 => fact_i3: Ed's reported is 9 * 3+9 =
36.

Structured Reasoning Process:
rule_13 & fact_15 => fact_il: stack exists between Ed and Claresta.
rule_8 & fact_il => fact_i2: hampers exists between Ed and Claresta

rule_4 & fact_i2 & fact_2 => fact_i3: Ed's reported is 36.

Case 4: DeepSeek-R1 in 0-shot setting on HL-EN-9

Process Accuracy = 1.0, Answer Accuracy = 1

Gold-Standard Reasoning Process:

rule_14 & fact_14 & fact_5 & fact_13 => fact_il: Charlton's side is

4x 6+ 10 = 34.

rule_11 & fact_3 & fact_15 & fact_7 => fact_i2: assume exists

between Tedie and Charlton.

rule_10 & fact_4 & fact_9 & fact_il => fact_i3: Charlton's regional

is 2 x« 34 4+ 10 = 78.

rule_9 & fact_i3 & fact_8 & fact_i2 => fact_i4: whipsaw exists

between Astrid and Charlton.

rule_5 & fact_2 & fact_i4 & fact_il => fact_i5: Charlton's unusual
is 34.

Structured Reasoning Process:
rule_14 & fact_14 & fact_5 & fact_13 => fact_il: Charlton's side is
34.
rule_5 & fact_2 & fact_9 & fact_il => fact_i2: Charlton's unusual
is 34.

Case 5: Phi4-reasoning-plus in 0-shot setting on EL-EN-489

Process Accuracy = 0.5, Answer Accuracy =

Gold-Standard Reasoning Process:

rule_5 & fact_10 => fact_il: finalize exists between Bengt and
Maryann.

rule_13 & fact_il => fact_i2: Maryann's iranian is 2.

Structured Reasoning Process:
fact_3 => fact_il: Maryann's annual is 4.

28


rule_5 & fact_10 => fact_i2: finalize exists between Bengt and
Maryann.

rule_13 & fact_i2 => fact_i3: Bengt's iranian is 2.

rule_4 & fact_3 => fact_i4: Maryann's iranian is 4.

Case 6: DeepSeek-R1 in 0-shot setting on EL-HN-153

Process Accuracy = 0.5, Answer Accuracy =

Gold-Standard Reasoning Process:

rule_5 & fact_4 & fact_7 => fact_il: Hetty's legal is -95 * 33 - 16
= —3151.

rule_6 & fact_3 & fact_il => fact_i2: Hetty's willing is addition
(-3151, -46) = -3197.

Structured Reasoning Process:
rule_5 & fact_4 & fact_7 => fact_il: Hetty's legal is -3151.
rule_6 & fact_2 & fact_il => fact_i2: Hetty's willing is -3197.

Case 7: DeepSeek-R1 in 0-shot setting on HL-HN-5

Process Accuracy = 0.5, Answer Accuracy = 1

Gold-Standard Reasoning Process:

rule_7 & fact_10 & fact_2 & fact_1l &
fact_3 => fact_il: Garey's liquid is max(52, 1) = 52.

rule_3 & fact_8 & fact_4 => fact_i2: scrutinize exists between
Garey and Aura.

rule_13 & fact_12 & fact_9 => fact_i3: waive exists between Aura
and Lonnie.

rule_1l & fact_il & fact_i2 & fact_i3 & fact_13 => fact_i4: Garey's
mean is -31 * -17 + 76 = 603.

Structured Reasoning Process:

fact_4 & fact_8 & rule_3 => fact_il: scrutinize exists between
Garey and Aura.

fact_12 & fact_9 & rule_13 => fact_i2: waive exists between Aura
and Lonnie.

fact_10 & fact_1 & fact_2 &
rule_7 => fact_i3: Garey's liquid is 52.

fact_il & fact_i2 & fact_13 & fact_i3 & rule_l => fact_i4: Garey's
mean is 603.

Case 8: GPT5-mini in 0-shot setting on EL-HN-162

Process Accuracy = 0, Answer Accuracy = 1

Gold-Standard Reasoning Process:

rule_15 & fact_9 & fact_4 => fact_il: Bradly's accepting is 42 «
-29 + 86 = -1132.

rule_12 & fact_3 & fact_il => fact_i2: Bradly's conditional is -93
* -1132 — 25 = 105251..

Structured Reasoning Process:
rule_15 & fact_9 & fact_4 => fact_il: Bradley's accepting is -1132.

29


rule_12 & fact_3 & fact_il => fact_i2: Bradly's conditional is
1052515

Case 9: GLM4-0520 in 0-shot setting on EL-EN-53

Process Accuracy = 0.3, Answer Accuracy = 1

Gold-Standard Reasoning Process:

rule_14 & fact_9 => fact_il: unsettle exists between Joscelin and
Clementia.

rule_2 & fact_5 => fact_i2: Joscelin's go is 7.

rule_1l & fact_il & fact_i2 => fact_i3: Clementia's first is 7.

Structured Reasoning Process:

fact_9 & rule_9 => fact_il: unsettle exists between Clementia and
Joscelin.

rule_2 & fact_9 => fact_i2: Joscelin's go is 7.

fact_il & rule_l & fact_i2 => fact_i3: Clementia's first is 7.

30
