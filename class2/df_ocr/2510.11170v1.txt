2510.11170v1 [cs.LG] 13 Oct 2025

arXiv

Preprint. Under review.

EAGER: ENTROPY-AWARE GENERATION FOR
ADAPTIVE INFERENCE-TIME SCALING

Daniel Scalena!? Leonidas Zotos!
Elisabetta Fersini2 Malvina Nissim! Ahmet Ustiin®+

University of Groningen University of Milano - Bicocca °Cohere Labs “Cohere

ABSTRACT

With the rise of reasoning language models and test-time scaling methods as
a paradigm for improving model performance, substantial computation is of-
ten required to generate multiple candidate sequences from the same prompt.
This enables exploration of different reasoning paths toward the correct solu-
tion, however, allocates the same compute budget for each prompt. Grounded on
the assumption that different prompts carry different degrees of complexity, and
thus different computation needs, we propose EAGER, a training-free generation
method that leverages model uncertainty through token-wise entropy distribution
to reduce redundant computation and concurrently improve overall performance.
EAGER allows branching to multiple reasoning paths only in the presence of high-
entropy tokens, and then reallocates the saved compute budget to the instances
where exploration of alternative paths is most needed. We find that across multi-
ple open-source models on complex reasoning benchmarks such as AIME 2025,
EAGER can reallocate the budget without accessing target labels, achieving the
best efficiency-performance trade-off in terms of reasoning length and Pass@k.
When target labels are accessible, EAGER generates up to 65% fewer tokens
(hence saving compute) and achieves up to 37% improvement in Pass@k com-
pared to the FULL PARALLEL sampling. Our results show that EAGER consis-
tently maximizes the efficiency-performance trade-off by enabling dynamic con-
trol over computation expenditure.

Sequence Qwen3 4B GPT-oss 20B
budget 0.85 1.00

T T T
0 08 16 3.3 () 03 06 12
# Tokens (1e5) # Tokens (1e5)

by flip ...

Prompt 1: To solve this probelm | need to subtract the two num... 4 “lees tokon uso N\ ; 4
: 7 5 5 improve
z sum ofa the first step is to define the li... 4 vn 0:80 ft eo © @ | 0.95 parormants
integer anew eleme... ax
Reasoning 0.75 0.90 e 2 ©
Tasks ze -@- Full Parallel Sampling Y
EAGer-init ’
0:70) EAGer-adapt 0:85
M=32 EAGer
Prompt 2: First, let's declare a new variable that will be used to... 0.65 : 0.80 r r
On Sitriangle a i = 8 14 29 5.7 0 08 17 33
‘ABCS points try to solve the main issue by following t.... ‘j 100
3A, D, ES, 3 Fi °
ph Die iol. setup a solution that is simpler to sol... i: a
equation step by st... gz O84 / 0.95 4 =
§
7 ag 2
easier to solv... E 6 e@ 0.90 °
ay avo74 e
"x M=32 <e
Ok the best way to address this problem itto start... g * 0.85
5 y
solution involves careful division of the two... 0.6 d
simplify the given problem is to ch... | |_| 19:89 H
x

Figure 1: Left: We introduce EAGER, a generation method that dynamically allocates the per-
prompt budget during decoding, branching only when high-entropy peaks are detected. For each
prompt, the total number of allowed sequences is capped at /, and we track the actual budget
consumed by our preparatory stage, EAGER-init. The remaining budget is evenly allocated among
prompts reaching the IV cap (EAGER-adapt) or, in case targets labels are accessible, prompts not
reaching a correct final solution (i.e. with Pass@k = 0; our full EAGER), in contrast to the fixed-
budget allocation of FULL PARALLEL sampling. Right: Our approaches (EAGER -init, -adapt and
full EAGER) consistently reduce token usage compared to the standard FULL PARALLEL sampling
approach when scaling the M limit € [4, 8, 16, 24, 32]. In addition, EAGER always achieves a clear
performance advantage over all other decoding methods.


Preprint. Under review.

1 INTRODUCTION

Recent advances in large language models (LLMs) have led to substantial improvements in complex

reasoning tasks, particularly with the adoption of chain-of-thought (CoT) prompting

(2022). Such tasks often admit multiple valid reasoning paths that converge to the same correct

solution (Stanovich & West]/2002). Rather than relying on a single greedy decoding path, the single
on can b

generati e replaced by multiple sampled candidate sequences, thereby producing a diverse
set of reasoning paths and corresponding final answers (2023). This strategy has been
shown to enhance performance on challenging reasoning problems: by exploring multiple reasoning
paths, the model reduces its reliance on the stochasticity of a single greedy generation and increases
the likelihood of arriving at a correct solution.

Despite its success, CoTs introduce an inherent computational inefficiency: reasoning sequences
tend to be long, and a large portion of the tokens generated are predictable continuations rather than
genuine decision points (Wang et al.|{2025). This inefficiency is amplified in approaches that explore
multiple reasoning paths in parallel, where each path independently regenerates identical prefixes
before diverging. For prompts with simple problems, many of these paths converge to the same solu-
tion with little variation, resulting in redundant computation. For more complex prompts, however,
the diversity of reasoning paths becomes crucial, and additional generations may be necessary to
discover a correct solution (Snell et al.| {2025} [Muennighoff et al.||2025). This observation suggests
that a per-problem decision to let or not let the model explore alternative paths would be desirable.
We argue that such decision can be guided by monitoring model uncertainty during generation to-
wards an adaptive allocation of computating budget. Intuitively, when the model’s predictions are
confident and stable, only a few candidate sequences are needed, while at points of high uncertainty,
where multiple reasoning paths are plausible, additional exploration becomes critical.

To address these issues, we introduce EAGER, an Entropy-Aware Generation method that mon-
itors token-level uncertainty during decoding to guide where new parallel reasoning traces should
start. By branching only at high-entropy tokens, we avoid regenerating identical low-entropy con-
tinuations, substantially reducing computation overhead without sacrificing coverage of diverse rea-
soning traces. Furthermore, reducing the parallel samples for easy prompts, EAGER dynamically
allocates the unused sampling budget towards more challenging ones, maximizing the benefits of
inference-time scaling for difficult prompts|"|

We evaluate EAGER on a diverse set of benchmarks, spanning from complex math problems to
science-related questions and code generation tasks. All the tested LMs, from the smallest 3B to
the biggest 20B parameter model, show a performance boost of up to 37% when using EAGER
compared to our baseline FULL PARALLEL sampling setting.

Our main contributions are as follows:
¢ We empirically show that token-wise entropy peaks as a form of online (i.e., measured

during generation) uncertainty is a good proxy that shows when more exploration is needed
during the generation, hence reflecting the difficulty of a prompt for the model used.

We introduce, a novel, training-free decoding method that leverages entropy distribution
during generation to dynamically reduce compute cost while maintaining the benefits of
inference-time scaling. EAGER generates up to 65% fewer tokens and saves up to 80% of
the entire generation budget across all our benchmarks and models.

To maximize the benefits of inference-time scaling, we show that EAGER enables adaptive
use of the given sampling budget, where it spends more compute on the hard problems.
This yields up to 13% improvements in test-time settings without access to target labels,
and up to 37% when labels are available, on the AIME 2025 reasoning benchmark.

2 PRELIMINARIES

In the inference-time scaling paradigm, a language model generates multiple parallel sequences so

that it can explore various reasoning paths to find a correct solution (Welleck et al.| |2024

‘Code and data: https://github.com/DanielSc4 /EAGer|,



Preprint. Under review.

2025). This is oftentimes facilitated by sampling completions from the model with a relatively
high temperature using methods such as nucleus sampling (Holtzman et al.|{2020). We refer to this

standard approach as the FULL PARALLEL sampling generation procedure. This approach is useful
in various settings, including for the generation of diverse solutions to a problem, or in large-scale

reinforcement learning (RL) pipelines such as in RLVR (DeepSeek-AJ} |2025), where among the

diverse set of generated sequences, only the correct ones are selected to update the policy.

Our objective is to optimize this process by exploiting uncertainty during generation, allowing an
efficient allocation of resources (in terms of number of generated sequences) to solve a given prompt.

2.1 UNCERTAINTY IN LLMS’ GENERATIONS

Among the different techniques for uncertainty quantification in LLMs, we focus our attention on
top-K token entropy, as token entropy has been shown to be a powerful uncertainty quantification

measure (Fomicheva et al.||2020). We define top-K token entropy as:

Hy = — SO pil log pt, (1)
iene

where a C {1,...,|V]|} is the index set of the K tokens with highest p,,; probability with V
denoting the vocabulary of the LM and t € N* indexes the current generation step. Specifically,
the quantity p;; represents the probability assigned by the model to token z € V at step ¢ after

the softmax computation. We denote rie C {1,...,|V]} the index set of the kK tokens with the
highest probability p,; and pr? their re-normalized probabilities, given by:

p®) __ Pt
tf —
: yjert™ Pt,

, ie Th, (2)

K
where ier ®) pe? =1.

Compared to more precise and computationally intensive uncertainty quantification methods found
in the literature e.g.,), top-K token
entropy provides a strong approximation to the entropy of the full-vocabulary, as it computes the
dominant contributions from the most probable tokens with minimal computational overhead/’|

2.2 ENTROPY IN LONG CHAIN-OF-THOUGHT REASONING

For our goal of saving resources in parallel sampling by leveraging model uncertainty, we first need
to determine whether and how token entropy values relate to the model’s final performance. To
this end, we analyze the entropy patterns of the CoT sequences generated by an LLM to solve
challenging problems. We monitor the entropy of each token during generation, and rather than
analyzing the entire entropy sequence, we focus on identifying significant spikes as signals of higher
uncertainty. We hypothesize that this peak-entropy measure can serve as a proxy for the model’s
perceived difficulty of a problem and thus its (in)ability to solve it: high peaks indicate moments
where the model is highly uncertain about the next step in its reasoning chain, low entropy indicates
that the model is more confident about what to generate next.

Given the input prompt zx, we sample / independent candidate sequences fm) \M_, from the lan-
guage model. During generation, for each token position t in each sequence m, we record the token

entropy HY) (y°™), with K = 20. For each sequence, we define the peak entropy value Ve) as

the mean of all entropy values that lie in the p" percentile of the sequence’s entropy distribution:

~ lm, 1 Kp. Gm
Fyeak (D") = 17 PR) YS HOY), (3)
| m (p ) | te Tek (pt)

?Bull-vocabulary entropy computation can be costly due to large vocabulary sizes, often in the tens of thou-


Preprint. Under review.

where
TR (p) = {t: HELO y™) > p® {HL Y™)}u)}, (4)

and p"(-) denotes the p" percentile of the entropy sequence.

We run Qwen3 4BP| a strong open-source LLM with

Pass Rate VS Token Entropy Peaks average

long CoT reasoning capabilities, on five standard S. corr: -0.547, p-value: 2.6e-34
reasoning benchmarks for math, science and code 1.0 4 a ie
generation tasks (see Section |4] for the benchmarks’ %
details), allowing for 1/ = 32 parallel sequences to 084
be generated. £064
a

Figure[2|shows the Pass Rate accuracy, i.e., the pro- 3 4, | —
portion of correct answers out of MZ = 32 gener- BE Science
ations, for each prompt and the corresponding aver- 0.24 Seas Gen

= th Peas gent ~
age peak entropy Ao ) We focus on the top per- 004 Melina eer
centile, specifically p'" = 99.9, to isolate the high- 2.0 2.2 2.4 2.6

Token Entropy Peaks average

est entropy peaks. We observe a statistically signif-
icant negative correlation (p ~ —0.55), between the
peak entropy during generation and model Pass Rate Figure 2: For each sequence generated by
accuracy. This suggests that higher entropy peaks, Qwen3 4B with FULL PARALLEL sampling
indicative of greater uncertainty during generation, (/ = 32), we report its Pass Rate accu-
are associated with lower performance. Thus, ad- racy and the average entropy peak (p"" =
ditional path exploration during these phases may 99.9). The results reveal a negative correla-
help to improve performance. Conversely, when en- tion (r = —0.547) between Pass Rate and the
tropy remains low, the model is more confident on average entropy peak across sequences. No-
the generated solutions (hence, the long CoT reason- _ tably, sequences exhibiting higher entropy at
ing sequences), suggesting that further exploration any generation step are less likely to yield a
may be less likely to yield significant improvements. correct answer.

This observation is in line with recent work which

found that high-entropy tokens disproportionately contribute to performance gains during RL train-

ing (Wang etal 2025).

Given this evidence, we ask: Can token entropy be leveraged to develop a decoding adaptive strategy
that allocates more compute to uncertain regions while limiting effort in more confident segments?

3. ENTROPY-AWARE GENERATION EXPLAINED

We introduce EAGER, a training-free inference-time scaling approach aimed at optimizing parallel
sampling by leveraging token entropy to guide resource allocation. EAGER consists of two stages:
in the first one, EAGER-init dynamically adjusts the generation process to focus on sequences where
the most effort is needed, while pruning unnecessary generations. In the second stage, the saved
computational budget is reallocated to enhance performance on the remaining challenging prompts.

3.1 EAGER-INIT: SAVE COMPUTE VIA TOKEN ENTROPY

EAGER-init represents the first stage of our approach and operates by identifying potentially easy
questions during generation. Instead of sampling constant / generations for every prompt, EA-

GER-init computes token entropy HY at each step t, and compares it to a predefined threshold
§\"|If the observed entropy exceeds this threshold, the current sequence is branched, creating a new
candidate continuation at that position. If the entropy is below the threshold, the generation contin-
ues with the existing sequence. During the branching step, we reuse the token distribution from the
model but adopt a temporally greedy approach; we select the top two most likely tokens to ensure

sands. By restricting calculations to the top-k most probable tokens, the token entropy significantly reduces
computational overhead while maintaining efficiency during generation.

https://huggingface.co/Qwen/Qwen3-4B

“We empirically find the best threshold for a model. See Section|4. 1]for a detailed analysis of the threshold.

4


Preprint. Under review.

that the two new sequences always start with different tokens. This process continues until the to-
tal number of active sequences reaches a predefined limit 1/7, at which point no further branching
occurs. A detailed overview of the EAGER-init algorithm is provided in AlgorithmT]

This process yields a generation tree where the root is the initial sequence, and each branching node
corresponds to a high-entropy token (Figure[I); the generation stops when the total number of nodes
is equal to MM. For implementation efficiency, we restrict the branching procedure to long CoT
sequences only, and entropy monitoring is halted if no branching has occurred within the previous
1000 tokens from the last branch.

Algorithm 1: EAGER-init sequence generation

Input: Prompt «x, entropy threshold 6 > 0, max active sequences /, temperature 7, top-K for
entropy K, maximum steps T’
Output: Completed set of sequences Y

Notation: H. us is the top- token entropy at step ¢ under distribution p(- | x, y).

Initialize active set. A + {yP} ; // initial continuation from prompt
Initialize completed set YV + © ;

fort + 1toT do

if A = © then

break

foreach sequence y € Ado

Compute next-token distribution p(- | x, y) with temperature T ;

Compute entropy 1. (x) from top-K probabilities ;
if H!) > 6 and |.A| < M then

a, + arg max, p(a | x,y) ; // most likely token
a2 + second-most-likely token under p ;
Update y + yoai; // greedy continuation

Create branch y’ + yo ag, addy’ to A;

else

Sample a ~ p(- | x, y) and update y + yoa,;
if y ends with EOS or length limit then

Move y from A to ¥ ;

return ) ;

Reducing test-time compute through EAGER-init. LEEAGER-init, saves computational budget
through two mechanisms. The first arises directly from the branching logic: if a branch occurs at
token position t, all preceding tokens (0,...,¢ — 1) are reused across branches rather than being
regenerated independently. The second, and more substantial source of savings occurs when the
generation process does not saturate the maximum number of sequences M set per prompt. For
easy queries, the model’s default sampling converges to identical or near-identical completions, so
that EAGER-init may terminate with only a single sequence, saving (/—1 full generations compared
to a fixed-budget baseline which would let the model generate M/ sequences for any given prompt.
This surplus capacity can then be reallocated where it is most needed.

3.2 EAGER: DYNAMICALLY ALLOCATE THE SAVED COMPUTE

The key challenge is to devise the best strategy to reallocate the compute which has been saved. We
start by defining an additional budget b, computed as:

b= min(Mneoretical _ Mactuals 2M) (3)

where Miheoretical = MM x |D| is the maximum possible number of sequences that could be generated

for the dataset D, and Mactuar = yl? # Seq, is the total number of sequences actually produced
under entropy-aware generation.


Preprint. Under review.

Algorithm 2: Full EAGER algorithm

Input: Dataset D = { (aj, 2:)}%1, initial generations {V;}3~, (from EAGER-init), max
sequences per prompt //, entropy threshold @

Output: Augmented generations {7/}*_,

Compute Miheoretical <— M - |D|;

Compute Mactal — 0, |Vil ;

Set remaining budget b — Mheoreticat — Mactual 3

Identify challenging prompts Z = {i | Pass@k(¥j, z:) = 0} ;

if b = 0orZ = © then

return {V;}

Assign additional budget b = min(b, 2M) uniformly across all 2 € TZ ;

foreach i € Z do

if |Vi| < M then

// underutilizing prompt

Set 0’ + 0.8-6;

Generate up to M + b sequences for x; using Algorithm[I}with 6’;

else

// prompt already saturated at M

Set 0’ + 0;

Generate up to M + b sequences for x; using Algorithm[I}with 6’;

Append new sequences to Y; ;

return {V/}%, ;

The term Miheoretical —Mactuat represents the surplus budget created by early stopping in easy prompts.
We cap b at 2M to avoid pathological cases where extremely large surpluses would lead to dispro-
portionately high generation budgets for single prompts}

We consider two scenarios: (i) a test-time setting where target labels are unavailable and realloca-
tion must rely solely on model signals, and (ii) a setting where target labels are accessible, as in
reinforcement learning pipelines where only correct generations are used for policy updates.

Budget reallocation without target labels. In the absence of target labels, we identify saturating
prompts, those that hit the maximum branching cap M, as candidates for additional budget. The
rationale is that when M serves as a hard limit, promising reasoning paths may remain unexplored.
To mitigate this, we start from a low branching threshold (6 = 2.0, see Section |4. Ip and reallocate
saved budget exclusively to these prompts. We denote this strategy as EAGER-adapt.

Fine-grained budget reallocation with target labels. When target labels are available, we instead
focus on challenging prompts, defined as those that fail to achieve Pass@k accuracy under EAGER-
init (i.e., none of the generated sequences match the correct answer).

For underutilizing prompts (fewer than M sequences under EAGER-init), we lower the entropy
threshold 6 by 20%, encouraging earlier and more frequent branching. For saturating prompts
(exactly MM sequences under EAGER-init), we extend generation up to a new per-prompt limit 1/+0,
thereby deepening exploration where additional sequences may yield correct solutions.

Reallocation in this setting is uniform across all failing prompts, while the generation strategy adapts
based on each prompt’s prior behavior. By redirecting unused capacity from easier prompts to
harder ones, this approach increases coverage without exceeding the global budget Miheoretical (See
Algorithm 2}. Importantly, savings from branch-based token reuse persist even when b > 0, and all
additional sequences remain governed by Algorithm|[I] ensuring that total token usage stays below
an equivalent fixed-budget FULL PARALLEL sampling baseline.

Especially in larger datasets, budget savings for easy prompts were large enough to allocate hundreds, if not
thousand, of additional sequences to single failing prompts; this cap prevents excessive unbalanced allocation.


Preprint. Under review.

Model Samplin AIME 2025 GPQA-Diamond HumanEval Plus
ping p@k c@k PR | p@k c@k PR | p@k c@k PR
FULL PARALLEL 0.53 0.00 0.06 | 0.49 0.00 0.03 | 0.00 0.00 0.00
SmolLM3B — EAGER-INIT 0.53 0.07 O11 | 059 0.10 0.15] 0.68 0.46 0.44
EAGER 0.73 0.33 0.31 | 0.85 0.12 0.18 | 0.75 0.56 0.52
FULL PARALLEL 0.80 0.70 0.62 | 0.75 0.51 0.43 | 0.91 0.82 0.78
Qwen3 4B EAGER-INIT 0.77 0.70 0.61 | 0.79 0.51 0.43] 0.86 0.86 0.86
EAGER 0.83 0.73 0.69 | 0.81 0.59 0.54 | 0.94 0.87 0.86
FULL PARALLEL 0.80 0.67 0.65 | 0.95 0.15 0.18 | 0.95 0.90 0.86
DeepSeek 8B = EAGER-INIT 0.70 0.63 0.64] 0.93 0.25 0.24) 0.96 0.85 0.77
EAGER 0.77 0.67 0.67 | 0.96 0.25 0.25 | 0.97 0.90 0.89
FULL PARALLEL 0.90 0.83 0.67 | 0.96 0.68 0.65 | 0.95 0.83 0.79
GPT-Oss 20B EAGER-INIT 0.93 0.80 0.66] 0.97 0.71 0.66 | 0.97 0.88 0.85
EAGER 0.97 0.80 0.68 | 0.99 0.72 0.66 | 0.97 0.89 0.85

Table 1: Comparison of FULL PARALLEL, EAGER-INIT and EAGER in AIME-2025, GPQA-
Diamond and HumanEval Plus. We report pass@k, cons@k and Pass Rate where k is number
of samples generated (while always 32 for the baseline, differs per prompt for EAGER-init and EA-
GER). EAGER consistently achieves the best results and EAGER-init performs very competitive
with FULL PARALLEL sampling while saving significant amount of compute as shown in Figure]

4 EXPERIMENTAL SETTING AND RESULTS

Models. We evaluate multiple reasoning models from different model families and sizes to test
EAGER in comparison to the FULL PARALLEL sampling baseline: SmolLM-3B (HuggingFaceTB|
2025), Qwen3-4B (Team||2025), DeepSeek-R 1 -0528-Qwen3-8B (DeepSeek-Al}|2025) and GPT-oss
. Additional generation parameters and EAGER hyper-parameters are available
in Appendix

Benchmarks. We evaluate our approach saved resources (compute metrics) for generation and
performance on a set of diverse reasoning benchmarks on various tasks: AIME 2024 and 2025, and
the 2025 Harvard MIT Math Tournament (Balunovié et al.|/2025) for math, GPQA-Diamond

2023) for scientific domains, and HumanEval Plus (Liu et al.|[2023}|2024) for code generation.

Compute metrics. We evaluate efficiency improvements using two complementary metrics: The
first is the average sequence Count (#Seq). FULL PARALLEL sampling uses a fixed budget of
M sequences, in contrast, EAGER uses a dynamic #Seq that depends on the branching behavior.
The second metric is the average token Count (#Token) generated. While #Seq provides a gen-
eral measure of computational efficiency, #Tokens is a more precise indicator since, branching at
step t, reuses previously generated tokens (0,...,¢ — 1) as prefix across new branches rather than
regenerating them, that can lead to substantial savings even when #Seq is comparable.

Performance metrics. We evaluate performance using three complementary metrics. Pass@k
shows whether the model produces at least one correct final solution, Cons@k aggregates responses
through majority voting across k generations. Lastly, Pass Rate measures the proportion of correct
answers over all generated outputs. We report the average metric across each entire benchmark.

4.1 RESULTS

EAGER-init and EAGER yield significant savings in computation. Figure |3} (top row) illus-
trates the efficiency advantages of EAGER-init and EAGER across all benchmarks and model scales.
Starting with EAGER-init, the total number of generated tokens is typically less than half of that re-
quired by FULL PARALLEL sampling. Building on this, EAGER (and EAGER-adapt) leverages a
small fraction of the saved budget to further improve accuracy, while still generating substantially
fewer sequences than FULL PARALLEL sampling. On the performance side, EAGER consistently
achieves higher Pass Rate accuracy than FULL PARALLEL sampling, indicating superior perfor-
mance per unit of computation. It is worth noting that the performance of SmolLM 3B is 0.0 across
all metrics in the parallel-sampling setting. This is caused by the generation of sequences in which
the same tokens are repeatedly produced (e.g., “The answer is: The answer is: The ...”). This effect,
along with the effect of temperature is discussed in Appendix[B]


Preprint. Under review.

lm Default EAGER-init EAGER
AIME 2024 AIME 2025 HMMT 2025 GPQA Diamond HumaneEval Plus

36%
45%
1

—\_—_ 4%
<+<—— -38%
+ _-36%

1.0M 4
0.5M 4

o.1M 4 |
0.0M F ; :

# Tokens

<+—___—§_ -64%
<<——_ 54%
L
+ 54%
<—— _-60%

-28%

——$—@_$_$_<<—_ 2%

<< -51%

| -42%
+ -45%
| -32%

eT
<+——____-1%
a
<— 52%
44%
L L
ee
—\__—_-43%
————
—_-46%
ee
49%
+ -46%
L
a
<
+ -47%

1.09 +15% +11% — +4% 4794
20754 eg 4 +9% 43% 41%) 1 41%] j
& +26%
ra 0.50 |  +365% 7 7 +28% 6% 43% 7 7

+454% 9

& 0.25 4 4 4 al | | 4 =| in” 1

0.00 LB, — T T = T T ™— T T T T — T LE T T T T T

3B 4B 8B 20B 3B 4B 8B 20B 3B 4B 8B 20B 3B 4B 8B 20B 3B 4B 8B 20B

Figure 3: Compute and performance trade-offs of EAGER-init and EAGER. Across all benchmarks
and model size, the efficiency of EAGER-init and EAGER consistently outperforms FULL PARAL-
LEL sampling, requiring only half as many tokens in most cases (top). In addition, they achieve
higher pass rate accuracy (bottom). For issues specific to the smallest 3B model, see Appendix |B]

SmolLM 3B Qwen3 4B DeepSeek 8B GPT-Oss 20B

Data EP EP EAGER EP EAGER EP EAGER

init adapt init & Ee init £ init adapt |
AIME 2024 1 P@K 0.60 053 0.63 083 | 0.90 080 085 0.90 | 0.93 0.90 0.90 0.93 | 0.93 0.93 0.95 1.00
#T 27 #218 19 19] 4 6 6 6| 20 7 9 9] 8 4 55
an @k 053 053 0.60 0.73 | 0.80 0.77 080 083 | 0.80 0.73 0.77 080] 0.90 090 095 0.97
#T 2 19 8 8] 7 8 2 wl 2 +8 Bb BI 0 5 5 6S
P@k 0.23 033 0.33 0.40 | 050 043 047 053] 057 043 050 0.57] 0.63 0.70 0.70 0.70
HMMT2025 ler 28 20 415 is| 18 7 15 | 24 8 I 6] 3 6 6 6
GPOA.Di P@k 049 068 0.76 085 | 0.75 0.79 O81 085] 0.95 0.93 093 0.96] 0.96 093 097 0.99
QADia eT igs. 137, s«d33,'sa9 | 65S—(is—“(tsé‘CsSC<“‘<‘“z | SCOTCH | CdS
HE Fie P@k 0.00 0.04 0.68 0.75| 0.91 0.86 0.92 0.94] 0.95 0.96 0.96 0.98] 0.95 0.96 097 0.97
: #T ll 4 94 2] 27 10 «+1 2) 23 2 8 Bi 5 4 55

Table 2: Reallocation of the additional budget (EAGER-adapt) only on Saturating prompts (i.e.,
prompts that reach Z = 32 generated sequences). All experiments use a threshold of 2.0, which
we found to provide a good balance between number of tokens (# T x le5) used and performance
(p@k) across models and benchmarks. Bold are best results, underline second best.

Saturation is a good proxy for budget reallocation. In the absence of target labels, EAGER-
adapt reallocates additional budget to saturating prompts as a_proxy for identifying challenging
cases. Table [2}reports results across all benchmarks and models} On average, this strategy not only
achieves substantial token savings during generation, but also improves exploration, yielding higher
Pass @k compared to the FULL PARALLEL sampling baseline. In other words, EAGER-adapt saves
a large fraction of compute while at the same time uncovering more successful reasoning paths.

For comparison, Table [2] also reports the full EAGER approach. While it benefits from the unfair
advantage of access to target labels, redirecting budget to saturating sequences still achieves the
second-best performance in most settings.

EAGER always achieves better performances than FULL PARALLEL sampling. As shown in
Figure 3] EAGER consistently outperforms FULL PARALLEL sampling in terms of Pass Rate. Ta-
ble[I]shows a more comprehensive overview using Pass@k, Cons @k, and Pass Rate. While Pass @k
is highest under EAGER, Pass Rate is consistently equal or better even for EAGER-init compared to
FULL PARALLEL sampling. This suggests that EAGER-init effectively prunes unproductive gener-
ations (higher Pass Rate) at the cost of reduced exploration (lower Pass@k). In general, Pass @k is

°The EAGER-init results in Table[2| differ from those reported elsewhere because we fix the threshold to
@ = 2.0 for the EAGER-adapt experiments. In this context, EAGER-init is treated as the first step of the
overall process. In other sections, EAGER-init is evaluated as an independent decoding strategy with its optimal
threshold selected for both efficiency and performance (see Section[4. I]for details and Appendix[C]for threshold
selection transparency).


Preprint. Under review.

10 AIME 2025 GPQA Diamond HumaneEval Plus
¥ 0.8 | lai
g 0.6 fad
s yf
= 0.4 fa

0.2

0 O0.8MO 0.5MO 0.6M0O 0.3M 0 0O8MO 0.3MO 0.3MO_ 0.1M 0 0.3MO0 0.1MO0 0.2MO_ 0.0M
3B 4B 8B 20B 3B 4B 8B 20B 3B 4B 8B 20B
# Tokens # Tokens # Tokens
=@- Full Parallel Sampling EAGer-init EAGer-adapt EAGer

Figure 4: Performance comparison with scaling the total allowed sequences for generating (VM €
{1, 4, 8, 16, 24, 32}). As M increases (line’s markers), EAGER consistently improves Pass @k (y-
axis) while reducing the number of tokens needed to find the correct solution (x-axis), further shift-
ing the Pareto frontier of the performance-efficiency trade-off.

particularly useful in scenarios where obtaining at least one correct answer is critical, for example,
when the user prioritizes correctness and exploration over efficiency as per in Reinforcement Learn-
ing applications. In contrast, Pass Rate and Cons@k capture a different dimension of quality: (i)
higher values indicate that EAGER focuses computation more effectively on promising generations,
and (ii) given the extreme efficiency gains of EAGER-init compared to FULL PARALLEL sampling,
the trade-off is often strongly favorable.

EAGER scales effectively under budget constrains. We evaluate the effect of scaling the max-
imum number of allowed generations, (7, on overall performance. As shown in Figure |4] increas-
ing M improves the probability of obtaining at least one correct solution (Pass@k). This trend
is expected, as a larger generation budget naturally enables more extensive exploration. Notably,
EAGER-init — and even more so EAGER — achieve superior Pass@k under the same constraints,
often with significantly fewer tokens. In other words, EAGER not only benefits from larger M but
also allocates its computational budget more efficiently, resulting in a consistent shift of the Pareto
frontier, where higher accuracy is achieved at lower token cost.

Threshold guides the trade-off between performance and compute. Efficiency metrics (# To-
kens, # Seq) are directly shaped by the choice of entropy threshold 6. In our experiments, we
explore values in the interval [1.8, 2.7], which captures the majority of observed entropy peaks (see
Section 2.2). Across different model families and sizes, we find consistent efficiency improvements
relative to the FULL PARALLEL sampling baseline throughout this range. The optimal setting of
@ remains task- and model-dependent. Under EAGER-init, lower thresholds encourage more fre-
quent branching, which increases both the number of generated sequences (#Seq) and total tokens
(#Token). Higher thresholds, in contrast, restrict branching, yielding fewer continuations and lower
computational cost. The balance between these regimes varies across architectures, scales, and
datasets. Full results are available in Appendix [A]

5 RELATED WORKS

Since the recent introduction of test-time scaling (Snell et al.|/2025}|Welleck et al.}/2024), multiple
approaches have been proposed to improve its efficiency and performance. (2025) propose

REBASE (REward BAlanced SEarch) a branching method that expands reasoning trajectories that
are evaluated as being of high quality by a reward model. While powerful, REBASE is significantly
more computationally expensive compared to directly computing token entropy at test-time. Deep-
Conf (Deep Think with Confidence, is amethod that also leverages local confidence
measures to increase performance and efficiency during generation. DeepConf uses this confidence
measure to truncate sequences where it is lower than a pre-defined threshold (determined during a
warm-up stage). This is in contrast to our proposed approach where the certainty measure drives
branching, instead of truncation. introduce self-certainty, a sequence-level mea-
sure closely related to cross-entropy. The authors demonstrate that self-certainty discriminates well
between correct and incorrect answers and is robust to reasoning length. The authors additionally
illustrate that self-certainty driven answer selection (through a voting mechanism) leads to improve-


Preprint. Under review.

ments in reasoning benchmarks. While the the current work is closely related to the work by [Kang]
we demonstrate that token-level certainty (in contrast to sequence-level) can function as a
useful tool to modulate performance and efficiency in reasoning LLMs.

6 CONCLUSION AND FUTURE DIRECTIONS

By leveraging token-level entropy, EAGER-init proves to be a highly performant training-free gen-
eration method with significantly higher efficiency compared to FULL PARALLEL sampling. In
applications such as RLVR, where the correct answer is known, EAGER surpasses the Pass @k per-
formance by up to 37% compared to FULL PARALLEL sampling while using up to 65% fewer tokens.
Even without a verifier at test time, EAGER-adapt improves exploration, surpassing FULL PARAL-
LEL sampling performance while generating up to 40% fewer tokens. In both cases, the methods
demonstrate the ability to save large amounts of compute and simultaneously enhance exploration.
Finally, we show that the approaches are domain (math, science and coding) and temperature (see
Appenidx[B} agnostic.

While our current work uses token-level entropy to create branching reasoning streams, future
research could explore other methods for quantifying uncertainty. For example, using Kullback-
Leibler (KL) Divergence to measure token uncertainty is a promising direction, inspired by the
work of (2025). At the same time, a key consideration is that the uncertainty quantifi-
cation method must be lightweight, as a computationally expensive approach would undermine the
goal of improving generation efficiency.

REFERENCES

Mislav Balunovi¢, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi¢, and Martin Vechev. Math-
arena: Evaluating Ilms on uncontaminated math competitions, February 2025. URL |https:)
//matharena.ai/

DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning,

2025. URLihttps://arxiv.org/abs/2501.12948

Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura,
and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantifica-
tion of free-form large language models. In Lun-Wei Ku, Andre Martins, and Vivek Sriku-
mar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Lin-
guistics (Volume I: Long Papers), pp. 5050-5063, Bangkok, Thailand, August 2024. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.276. URL

aclanthology.org/2024.acl—-long.276

Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzman, Mark Fishel,
Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised Quality Estimation for
Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8:
539-555, September 2020. ISSN 2307-387X. doi: 10.1162/tacl_a_00330.

Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025.
URL https://arxiv.org/abs/2508.15260

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis

Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL|https: //openreview.|

HuggingFaceTB. Smollm3: smol, multilingual, long-context reasoner, 2025. URL
huggingface.co/blog/smol1lm3

Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language

models via self-certainty, 2025. URL|https://arxiv.org/abs/2502.18581

10


Preprint. Under review.

Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances
for uncertainty estimation in natural language generation. In The Eleventh International Confer-

ence on Learning Representations, 2023. URL\|https://openreview.net/forum?id=
VD-AYtPOdve

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by
chatGPT really correct? rigorous evaluation of large language models for code generation. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL {ht tps: //|

Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evalu-
ating language models for efficient code generation. In First Conference on Language Modeling,

2024. URL/https://openreview.net/forum?id=IBCBMeAhmC

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Candés, and Tatsunori Hashimoto. s1: Simple test-time

scaling, 2025. URL https://arxiv.org/abs/2501.19393

OpenAl. Introducing gpt-oss, 2025. URL https://openai.com/index/

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien
Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a

benchmark, 2023. URL https: //arxiv.org/abs/2311.12022

Charlie Victor Snell, Jachoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute
optimally can be more effective than scaling parameters for reasoning. In The Thirteenth In-
ternational Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.

OpenReview.net, 2025. URL https: //openreview.net/forum?id=4FWAwZtd2n

Keith E. Stanovich and Richard F. West. Individual Differences in Reasoning: Implications for the
Rationality Debate ?, pp. 421-440. Cambridge University Press, 2002.

Qwen Team. Qwen3 technical report, 2025. URL/https://arxiv.org/abs/2505.09388

Roman Vashurin, Maiya Goloburda, Albina Ilina, Aleksandr Rubashevskii, Preslav Nakov, Artem
Shelmanov, and Maxim Panov. Uncertainty Quantification for LLMs through Minimum Bayes
Risk: Bridging Confidence and Consistency. arXiv e-prints, art. arXiv:2502.04964, February
2025. doi: 10.48550/arXiv.2502.04964.

Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen,
Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen
Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive

effective reinforcement learning for Ilm reasoning, 2025. URL|https://arxiv.org/abs/
2506.01939

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations, ICLR 2023,

Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL/https: //openreview.net/

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V
Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in
Neural Information Processing Systems, volume 35, pp. 24824-24837. Curran Associates, Inc.,
2022. URL

Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig,
Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms
for large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856.

URL https://openreview.net/forum?id=eskQMcIbMS) Survey Certification.

11


Preprint. Under review.

Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws:
An empirical analysis of compute-optimal inference for LLM problem-solving. In The Thirteenth
International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.
OpenReview.net, 2025. URL https: //openreview.net/forum?id=VNckp7JEHn

12


Preprint. Under review.

A COMPLETE RESULTS

Table|3|presents a complete overview of the results of our experiments.

0 SmolLM3-3B Qwen3-4B Deepseek 8B GPT-oss 20B

tp@1 tc@32 TPR |#T |#S|tp@l tc@32 +PR [#T |#S|tp@l tc@32 PR |#T |#S | tp@l tc@32 +PR |#T |#S

AIME 2024 (math)
_— 0.60 0.03 0.09 27/00N32:0 0.90 0.80 0.74 14 32.0 0.93 0.80 0.73 20 32.0 0.93 0.80 0.71 8 32.0
2.0 0.53 0.03. 0.10 18 28.8 | 0.80 0.73 0.70 6 153 | 0.90 0.87 0.85 7 158} 0.93 0.77 0.70 4 307
22, ‘0:53: 0.10 0.15 18 28.4 0.70 0.67 0.68 1 1.7 0.80 0.80 0.79 2, 45 0.90 0.80 0.70 4 29.7
23 0.52 0.10 0.18 1726.8 0.67 0.67 0.65 0.3 1.0 0.77 0.77 0.76 04 13) 0.90 0.80 0.68 4 30.2
24 0.67 0.20 0.25 14 (22.7 0.77 0.77 0.77 1 10 0.70 0.70 0.70 0.3 1.0 0.93 0.83 0.75 4 27.6
DSN 057 0.50 0.40 5 165 | 0.73 073 073 02 10] 0.73 0.73 073 03 10] 0.90 0.83 0.69 4 28.0
20 0.73 0.10 0.13 19 32.0 | 0.90 0.80 0.74 9 23.4) 0.93 0.87 0.85 9 20.0 | 0.93 0.77 (0.70 5 32.0
2:2 0.73 0.10 0.16 19 52. 0.87 0.77 0.76 5 11.3 0.90 0.80 0.80 EI 12.0 0.93 0.83 0.71 5 322
23 0.83 O17 0.18 19 33.0] 0.90 0.80 0.75 5 124] 0.87 0.83 0.81 5 101 | 0.93 0.80 0.68 5 321
24 0.77 0.20 0.28 19 32.8 0.83 0.80 0.79 5 10.5 0.90 0.80 0.78 6 13.0 0.97 0.83 0.75 5 32.0
25 0.67 0.50 0.42 14 313 | 0.87 0.83 0.81 5 10.7 | 0.90 0.77 (0.77 7 140] 1.00 0.87 0.71 5 32.0
AIME 2025 (math)
— 0.53 0.00 0.06 28 =32.0 0.80 0.70 0.62 17 32.0 0.80 0.67 0.65 22 «32.0 0.90 0.83 0.67 10 32.0
1.8 = = = = - | 077 0.70 060 0.90 245 z 5 = = = = = = = =
2.0 0.53 0.00 0.05 19 28.8 0.77, 0.70 0.61 8 18.4 0.73 0.60 0.59 8 17.4 0.90 0.83 0.66 5 3ll
2 0.43 0.00 0.07 19 29.9 0.67 0.63 0.64 1 Sail 0.70 0.63 0.64 2 49 0.93 0.80 0.67 =) 30
Py 8) 0.37 0.10 0.14 1727.8 0.60 0.60 0.60 0.3 11 0.70 0.67 0.66 1 2.2 0.93 0.73 0.64 5 314
24 0.53 0.07 0.11 17 27.6 0.70 0.70 0.70 0.3 ares 0.63 0.60 0.61 0.5 12 0.93 0.80 0.66 5 29.8
25 0.43 0.23 0.26 10 17.5 | 0.60 060 060 03 1.0] 0.63 0.60 0.61 04 12] 0.90 0.83 0.69 5 261
2.0 - - - - - - - - - - - - - - - - - - - -
18 - - - - - 0.80 0.70 0.63 13 32.8 - - - - - - - - - -
2.0 0.63 0.00 0.06 20 32.0 0.83 0.73 0.63 12 30.0 0.80 0.63 0.63 12. 28.0 0.90 0.83 0.66 5 32.0
2.2 0.57 0.00 0.08 20 32.0 0.80 0.70 0.70 6 14.7 0.80 0.63 0.68 7 16.1 0.97 0.80 0.68 S29
23) 0.57 0.10 0.15 19 32.1 0.80 0.77 0.71 8 17.5 0.80 0.73 0.71 6 13.1 0.93 0.73 0.64 5 32.2
2.4 0.67 0.07 0.13 193272 0.80 0.73 0.71 5 10.7 0.80 0.70 0.68 pi Leys) 0.93 0.80 0.66 6 33.0
25 0.73 0.33 0.31 15 33.6 0.83 0.73 0.69 7 15.0 0.80 0.70 0.68 6 14.0 0.93 0.83 0.69 7 32.0
HMMT (math)
— 0.23 0.00 0.03 28 32.0 0.50 0.37 0.34 18 32.0 0.57 0.43 0.37 24 32.0 0.63 0.53 0.38 13, 32.0
18 0.23 0.03 0.06 20 31.7 0.43, 0.37 0.33 10 22.7 - - - - - - - - - -
2.0 0.33 0.03 0.07 20 30.8 0.43, 0.37 0.35 y 15.5 0.43 0.37 0.33 8 18.1 0.70 0.43 0.37 6 319
Pipe 0.23 0.00 0.06 18 28.5 0.37 0.37 0.36 1 a8 0.40 0.37 0.38 3 6.2 0.67 0.43 0.36 6 32.0
2.3 0.27 0.10 0.12 17) (27.5 0.40 0.40 0.40 1 23 0.40 0.40 0.35 2 47 0.63 0.47 0.40 6 30.9
24 0.27 0.17 0.18 14 23.8 0.33 0.33 0.33 0.4 1 0.37 0.37 0.35 1 15 0.63 0.43 0.38 6 30.8
Dap) 0.23 0.17 0.17 10 17.4 0.43 0.43 0.43, 0.3 10 0.37 0.37 0.37 0.4 11 0.67 0.53 0.40 6 30.5
2.0 - - - - - - - - - - - - - - - - - - - -
18 0.27 0.03 0.06 20 0.47 0.37 0.33 15 S219; - - - - - - - - - -
20 0.40 0.03 0,09 20 0.53 0.37 0.36 14. 32.0 | 0.57 040 0.35 15 32.1 | 0.70 0.43 © 0.37 6 32.0
2:2 0.33 0.00 0.06 19 0.53 0.37 0.37 11 24.5 0.57 0.43 0.43 13 27.5 n/a n/a n/a n/a n/a
23 0.33 0.10 0.13 19 0.57 0.40 0.41 10 22.6 0.53 0.43 0.38 di 24.0 0.67 0.47 0.40 7 32.0
24 0.40 0.17 0.19 17 0.50 0.40 0.37 Il 25.2 | 0.57 0.43 0.39 li 240} 0.63 0.43 0.38 7 32.0
25 0.40 0.17 0.19 15 0.60 0.43 0.44 10 21.9 0.50 0.37 0.39 ial 23.5 0.67 0.53 0.40 7 = 32.0
GPQA-Diamond (science)
_— 0.49 0.00 0.03 185 32.0 0.75 0.51 0.43 65 32.0 0.95 0.15 0.18 68 32.0 0.96 0.68 0.65 24 32.0
2.0 0.68 0.04 0.09137 0.79 051 0.43 46 26.8 | 0.93 0.25 0.24 37 28.5 | 0.93 0.72 0.66 13 29.7
2, 0.62 0.07 0.11 130 0.71 0.47 0.43 38 «22.1 0.91 0.18 0.22 a3) 25.0 0.97 0.71 0.66 14 27.6
23 0.61 0.02 0.07 133 0.64 0.49 0.43 23 14.7 0.81 0.21 0.18 23 17.4 0.95 0.66 0.65 14 30.9
24 0.61 0.06 0.10 126 0.56 0.45 0.44 12 74 = = S = = 0.94 0.70 0.66 14 25.8
25 0.59 0.10 O15 113 0.49 046 © 0.45 3 25] 0.66 0.20 © 0.21 4 34] 0.95 0.68 0.65 1B 24.0
2.0 0.79 0.04 0.09 137 0.85 0.51 0.43 59 32.4 0.96 0.25 0.26 43 32.5 0.99 0.72 0.66 15 32.3
22 0.81 0.07 0.12 132 0.81 0.50 0.45 55 32.4 0.97 0.18 0.25 43 33.8 0.98 0.71 0.66 15 29.9
2.3 0.75 0.03 0.09 135 0.81 0.53 0.47 44 27.3 0.97 0.25 0.25 46 35.4 0.97 0.66 0.65 14 30.9
24 0.79 0.08 0.12 128 0.81 0.51 0.50 37 22.4 - - - - - 0.99 0.70 0.66 1529.9
25 0.85 0.12 018 = 119 0.81 0.59 0.54 34 19.9 | 0.94 0.26 = 0.33 45 35.6 | 0.98 0.68 0.66 14° 273
HumanEval Plus (code)

_— 0.00 0.00 0.00 161 32.0 0.91 0.82 0.78 27 32.0 0.95 0.90 0.86 25 32.0 0.95 0.83 0.79 5 32.0
18 - - - - - 0.87 0.76 0.76 16 919) 0.95 0.82 0.79 18 25.0 2 = = 2 =
2.0 0.04 0.01 0.01 94 = 30.7 0.86 0.79 0.80 10 6.20 0.96 0.85 0.77 21 23.0 0.96 0.88, 0.83 4 24.7
2.2 0.04 0.01 0.01 65 26.3 0.86 0.86 0.86 1 1.1 0.94 0.86 0.83 13 14.0 0.97 0.88, 0.82 3 233
23 0.68 0.46 0.44 33 17.3 0.84 0.82 0.82 0.5 Ll 0.87 0.82 0.80 6 TA 0.93 0.81 0.74 3 22.8
24 0.52 0.37 0.38 13 9.0 0.81 0.81 0.81 0.5 el 0.92 0.88 0.88 3 3.5) 0.97 0.88, 0.85 3 203
25 0.52 0.48 0.47 3 2.6 | 0.82 082 082 04 1.0] 0.88 0.87 0.86 1 1s | 095 086 0.82 3 18.6
18 - - - - - - - - - - - - - - - - - - - -
2.0 - - - - - 0.94 0.79 0.82 17 11.2 0.97 0.77 0.74 22 (24.7 0.97 0.89 0.84 5 6205
2:2 - - - - - 0.92 0.86 0.87 uy pul 0.96 0.87 0.83 15 15.9 0.97 0.88 0.82 5 29.0
23 5 E - 5 - | 0.92 0.87 0.86 11 9.9 | 0.98 0.88 0.86 13 16.9 | 0.97 0.90 0.84 5 28.8
24 0.75 0.52 0.56 20 193] 0.94 0.87 0.86 12 109] 097 0.90 0.89 8 85 | 0.97 0.89 0.85 5 264
25 z E : : - | 0.93 0.86 0.86 12 118 | 0.96 0.91 0.90 8 93] 097 0.88 0.83 5 272

Table 3: All models, benchmarks and entropy-thresholds 6 configurations. Higher is better for
Pass@k (p@k), Cons@k (c@32) and Pass Rate (PR); lower is better for # Token. # Token are
in led unit. Results for FULL PARALLEL sampling generations, EAGER-init generations, and full
EAGER. Bold is best overall, underline is best within each category always including the FULL
PARALLEL sampling one.

13


Preprint. Under review.

B EFFECT OF TEMPERATURE

The temperature hyperparameter, 7, plays a critical role during autoregressive decoding by scal-
ing the logits used by the sampling method (decoding becomes more greedy as 7 — 0). In this
section, we conduct a short exploration on the effect of temperature on EAGER. This is espe-
cially important in the current context, where a higher diversity among the generated sequences
can intuitively have an effect on the performance metrics. For this exploration, we focus on two
LLMs, SmolLM 3B & DeepSeek 8B, two temperature settings, 7 € {0.6,0.9} and AIME 2025 as
the evaluation dataset. Furthermore, we conduct the analysis for varying entropy threshold levels
6 € {2.0, 2.2, 2.3, 2.4, 2.4, 2.5}.

Effect of Temperature and EAGER on Model Performance (AIME 2025)

SmolLM3-3B Deepseek 8B

0.8

: é 0.67
x [= -0.63- = -0.63-— === 0.63 --.0,62-—— 0.63 --
x x
E06 -
©
a
5 %
js eee i: a es 933 —---—----— 0.33, -------- a
0.2
0.10
0.0 a-===focces === === = == =
2 2.2 2.3 ‘ 2.4 as
EAGER (varying 6) EAGER Gaim 6)
lm EAGER (t=0.6) lm EAGER (t=0.9) --- Default (t=0.6) --- Default (t=0.9)

Figure 5: Pass@k and Cons @k at low (7 = 0.6) and high(7 = 0.6) temperature settings. Horizontal
lines show the performance for the default sampling method, while the bars show EAGER’s perfor-
mance for varying entropy threshold levels 0.

As shown in Figure [5| SmolLM 3B generally performs best at the high temperature setting while
the opposite is true for DeepSeek 8B. Importantly, at both temperature levels, EAGER is competi-
tive with the corresponding default baselines, often surpassing them. A direct comparison between
the low and high temperature setting including all metrics for default, EAGER and EAGER-init
generations is presented in Table[4]

SmolLM3-3B Deepseek 8B
(7) Low Temperature (7 = 0.6) High Temperature (7 = 0.9) Low Temperature (7 = 0.6) High Temperature (7 = 0.9)
+p@l tc@32 TPR {#T |#S | tp@l +c@32 +PR |#T #8 | tp@1 tc@32 +PR |#T {#8 | tp@l tc@32 +PR |#T #8

0.53 0.00 0.06 28 = 32.0 | 0.63 0.37 0.25 44 32.0 | 0.80 0.67 0.65 22.0 32.0 | 0.70 0.60 0.57 78 32.0

2.0 0.53 0.00 0.05 19 28.8 | 0.67 0.30 0.25 26 «631.0 | 0.73 0.60 0.59 8.0 17.4 | 0.77 0.63 0.58 32 22.7
2.2 0.43 0.00 0.07 19 29.9 | 0.53 0.37 0.25 2730.9 | 0.70 0.63 0.64 2.0 49 | 0.70 0.60 0.58 19 113
239) 037 0.10 0.14 17 27.8 | 0.57 0.33 0.26 26 29.9 | 0.70 0.67 0.66 1.0 2.2 | 0.57 0.53 0.53 0.7 44
24 0.53 0.07 0.11 17 27.6 | 0.60 0.40 0.32 22 25.8 | 0.63 0.60 0.61 0.5 1.2 | 0.57 0.53 0.53 0.7 4.0
25 0.43 0.23 0.26 10 «17.5 | 0.50 0.37 0.26 21 24.7 | 0.63 0.60 0.61 04 1.2 | 0.63 0.60 0.61 0.3 2.0

2.0 0.63 0.00 0.06 20 32.0 | 0.67 0.30 0.25 27 32.0 | 0.80 0.63 0.63 12.0 28.0 | 0.77 0.63 0.58 44 30.2
2.2 0.57 0.00 0.08 20 = 32.0 | 0.53 0.33 0.25 28 = 32.0 | 0.80 0.63 0.68 7.0 16.1 | 0.77 0.63 0.62 3.5 21.0
28 0.57 0.10 0.15 19 32.1 0.70 0.33 0.27 28 = 32.1 0.80 0.73 0.71 6.0 13.1 | 0.73 0.63 0.63 AZo
24 0.67 0.07 0.13 19 32.2 | 0.70 0.40 0.33 29 32.0 | 0.80 0.70 0.68 7.0 15.3 | 0.73 0.60 0.59 3.1 18.9
25 0.73 0.33 0.31 15 33.6 | 0.63 0.37 0.29 29° 32.4 | 0.80 0.70 0.68 6.0 14.0} 0.77 0.67 0.66 24 149

Table 4: AIME 2025 results for default, EAGER-init, and EAGER generations for low and high
temperature 7 and varying entropy threshold 0. Best results per temperature and threshold setting
are marked in boldface.

Notably, the performance of SmolLM 3B is particularly higher in the high temperature setting when
measured by the Cons @ max rate. We find that this is a result of the reduction of generations in which

14


Preprint. Under review.

the same tokens are repeatedly produced (e.g., “The answer is: The answer is: The ...”). Specifically,
in the high temperature setting, this phenomenon occurs, on average, 59.1% less compared to the low
temperature setting. This behaviour was only observed with SmolLM 3B, suggesting it results from
the smaller model size. An exception arises with the HumanEval Plus benchmark, where SmolLM
3B failed to solve any tasks, resulting in all metrics being zero under the FULL PARALLEL sampling
setting. In contrast, EAGER-init and EAGER appeared to partially mitigate this issue.

Lastly, we also find that the temperature has an effect on the number of tokens generated which, by
extension, impact performance. For example, when EAGER is used at the high temperature setting,
Deepseek 8B generates, on average, less than half the number of tokens compared to the low tem-
perature setting. In contrast, SmolLM3-3B generates more tokens at the high-temperature setting.
In both cases, and in line with the test-time scaling paradigm, we find that higher performance is
achieved in whichever temperature setting more tokens are generated.

C GENERATION PARAMS

All models are used with their longest thinking configuration to get their best performances. Further-
more we limit their context window to 32k tokens. All sequences are generated with a temperature of
7 = 0.60 and a top-p of 95%. The effect of temperature is discussed in Appendix |B] Table|5|reports
the thresholds used for each benchmark and model. Following the discussion in Section |4.1} we
select thresholds independently based on their intended use. The EAGER-init sampling method is
designed to save budget without significantly compromising performance (lower threshold), whereas
EAGER aims to preserve as much performance as possible for later reuse, higher threshold are pre-
ferred in such scenario.

SmoLM 3B Qwen3 4B DeepSeek 8B GPT-oss 20B
EAGER-init EAGER EAGER-init EAGER EAGer-init EAGER EAGER-init EAGER
AIME 2024 25 2.5 2.0 2:3 2.0 2.0 2.4 25
AIME 2025 2.4 25 2.0 20 22 25 2.4 25
HMMT 2025 2.5 2.5 2.5 2.5 2.4 2.5 2.5 2.5
GPQA-Diamond 25 2.5 2.0 2.5 2.0 2.3 2.2 2.0
HumanEval Plus 2.3 - 2.2 2.4 2.0 2.4 2.4 2.4

Table 5: Best thresholds for every benchmark and model.

15
