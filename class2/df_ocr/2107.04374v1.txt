arXi1v:2107.04374v1 [cs.CL] 9 Jul 2021

Benchmarking for Biomedical Natural Language
Processing Tasks with a Domain Specific ALBERT

Usman Naseem!’, Adam G. Dunn”, Matloob Khushi!, and Jinman Kim!

'School of Computer Science, The University of Sydney, Sydney, 2006, Australia
*School of Medical Science, The University of Sydney, Sydney, 2006, Australia
“corresponding author: Usman Naseem (usman.naseem@sydney.edu.au)

ABSTRACT

The availability of biomedical text data and advances in natural language processing (NLP) have made new applications in
biomedical NLP possible. Language models trained or fine-tuned using domain-specific corpora can outperform general models,
but work to date in biomedical NLP has been limited in terms of corpora and tasks. We present BioALBERT, a domain-specific
adaptation of A Lite Bidirectional Encoder Representations from Transformers (ALBERT), trained on biomedical (PubMed
and PubMed Central) and clinical (MIMIC-III) corpora and fine-tuned for 6 different tasks across 20 benchmark datasets.
Experiments show that BioALBERT outperforms the state-of-the-art on named-entity recognition (+11.09% BLURB score
improvement), relation extraction (+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document classification
(+0.62% F1-score), and question answering (+2.83% BLURB score). It represents a new state-of-the-art in 17 out of 20
benchmark datasets. By making BioALBERT models and data available, our aim is to help the biomedical NLP community
avoid computational costs of training and establish a new set of baselines for future efforts across a broad range of biomedical
NLP tasks.

Background & Summary

The growing volume of the published biomedical literature, such as clinical reports! and health literacy* demands more precise
and generalized biomedical natural language processing (BioNLP) tools for information extraction. The recent advancement of
using deep learning (DL) in natural language processing (NLP) has fueled the advancements in the development of pre-trained
language models (LMs) that can be applied to a range of tasks in the BioNLP domains’.

However, directly fine-tuning of the state-of-the-art (SOTA) pre-trained LMs for bioNLP tasks, like Embeddings from
Language Models (ELMo)*, Bidirectional Encoder Representations from Transformers (BERT) and A Lite Bidirectional
Encoder Representations from Transformers (ALBERT)®°, yielded poor performances because these LMs were trained on
general domain corpus (e.g. Wikipedia, Bookcorpus etc.), and were not designed for the requirements of biomedical documents
that comprise of different word distribution, and having complex relationship’. To overcome this limitation, BioNLP researchers
have trained LMs on biomedical and clinical corpus and proved its effectiveness on various downstream tasks in BioNLP
tasks®-}>,

Jin et al.? trained biomedical ELMo (BioELMo) with PubMed abstracts and found features extracted by BioELMo
contained entity-type and relational information relevant to the biomedical corpus. Beltagy et al.!! trained BERT on scientific
texts and published the trained model as Scientific BERT (SciBERT). Similarly, Si et al.'° used task-specific models and
enhanced traditional non-contextual and contextual word embedding methods for biomedical named-entity-recognition (NER)
by training BERT on clinical notes corpora. Peng et al. '* presented a BLUE (Biomedical Language Understanding Evaluation)
benchmark by designing 5 tasks with 10 datasets for analysing natural biomedical LMs. They also showed that BERT models
pre-trained on PubMed abstracts and clinical notes outperformed other models which were trained on general corpora. The
most popular biomedical pre-trained LMs is BioBERT (BERT for Biomedical Text Mining)!* which was trained on PubMed
and PubMed Central (PMC) corpus and fine-tuned on 3 BioNLP tasks including NER, Relation Extraction (RE) and Question
Answering (QA). Gu et al.'* developed PubMedBERT by training from scratch on PubMed articles and showed performance
gained over models trained on general corpora. They developed a domain-specific vocabulary from PubMed articles and
demonstrated a boost in performance on the domain-specific task. Another biomedical pre-trained LM is KeBioLM!> which
leveraged knowledge from the UMLS (Unified Medical Language System) bases. KeBioLM was applied to 2 BioNLP tasks.
Table | summarises a number of datasets previously used to evaluate Pre-trained LMs on various BioNLP tasks. Our previous
preliminary work has shown the potential of designing a customised domain-specific LM outperforming SOTA in NER tasks!°.

With all these pre-trained LMs adopting BERT architecture, its’ training is slow and requires huge computational resources.
Further, all these LMs were demonstrated with selected BioNLP tasks, and therefore their generalizability is unproven.


Table 1. Comparison of the biomedical datasets in prior language model pretraining studies and ours (Bio ALBERT) — a
biomedical version ALBERT language model

Datasets BioBERT® SciBERT!! BLUE! PubMedBERT™ KeBioLM!®  BioALBERT

Share/Clefe!”
BCS5CDR (Disease)!8
BCS5CDR (Chemical)!®
JNLPBA!?
LINNAEUS2°
NCBI (Disease)?!
Species-800 (S800)?
BC2GM?3
DDI“*
ChemProt?
i2b225
Euadr26
GAD27

BIOSSES®
MedSTS2?

MedNLI-?
Hoc?!

BioASQ 4b
BioASQ 5b"2
BioASQ 6b"2

KAA XI KIRK RK A KIRK RRR AK x
x xX KITx])xtlx xilx «x «Kx < xiix x < «Kx <\< x
x x xX|ISIIAR SIP XK AA AIK KKK KARR K
KA ATA XID AIR KK AAR K RK RR KX
x xX KITX]])xIlx xlla «Kx x K <IR XK KK KR KR KX

KA AINIAIS AIRS SSSR SRNR SS

Furthermore, these LMs are trained on limited domain-specific corpora, whereas some tasks contain both clinical and
biomedical terms, so training with broader coverage of domain-specific corpora can improve performance. ALBERT has been
shown to be a superior model compared to BERT in NLP tasks®, and we suggest that this model can be trained to improve
BioNLP tasks as shown with BERT. In this study, we hypothesize that training ALBERT on biomedical (PubMed and PMC)
and clinical notes (MIMIC-IIT) corpora can be more effective and computationally efficient in BioNLP tasks as compared to
other SOTA methods.

We present biomedical ALBERT (BioALBERT), a new LM designed and optimized to benchmark performance on a range
of BioNLP tasks. BioALBERT is based on ALBERT and trained on a large corpus of biomedical and clinical texts. We
fined-tuned and compared the performance of BioALBERT on 6 BioNLP tasks with 20 biomedical and clinical benchmark
datasets with different sizes and complexity. Compared with most existing BioNLP LMs that are mainly focused on limited
tasks, our BioALBERT achieved SOTA performance on 5 out of 6 BioNLP tasks in 17 out of 20 tested datasets. BioALBERT
achieved higher performance in NER, RE, Sentence similarity, Document classification and a higher Accuracy (lenient) score
in QA than the current SOTA LMs. To facilitate developments in the important BioNLP community, we make the pre-trained
BioALBERT LMs and the source code for fine-tuning BioALBERT publicly available!.

Methods

BioALBERT has the same architecture as ALBERT. The overview of pre-training, fine-tuning, variants of tasks and datasets
used for BioNLP is shown in Figure 1. We describe ALBERT and then the pre-training and fine-tuning process employed in
BioALBERT.

ALBERT

ALBERT? is built on the architecture of BERT to mitigate a large number of parameters in BERT, which causes model
degradation, memory issues and degraded pre-training time. ALBERT is a contextualized LM that is based on a masked
language model (MLM) and pre-trained using bidirectional transformers** like BERT. ALBERT uses an MLM that predicts
randomly masked words in a sequence and can be used to learn bidirectional representations.

ALBERT is trained on the same English Wikipedia and BooksCorpus as in BERT; however, reduced BERT parameters by
87 percent and could be trained nearly twice as fast. ALBERT reduced parameters requirement by factorizing and decomposing

"https://github.com/usmaann/BioALBERT

2/10


Pre-training of BioALBERT Fine-tuning of BioALBERT

¢ ‘

BioALBERT Named entityrecognition

BC5CDR (Disease) GAD
BC5CDR (Chemical) Euadr
NCBI (Disease) ; DDI
JNLPBA ‘Benchmarking : ChemProt
BC2GN : 12B2-2010
LINNEAEUS
ShARe/Clefe

eal ‘ BENCHMARKING

(PublMed 4.5B words

t
'

'

f

'

'

: '

'

j '

'

\ 1
'

b 1
'

'

'

'

'

'

'

'

: PM (C1358 word

'
1
1
1
'
1
1
1
1
1
1
1
\

Weights Initialization
(ALBERT (Lan et al., 2019)

oF
a y

Wikipedia BookCorpus

“i Sentence Similarity ‘ BioASQ 4b-factoid
Wikipedia BookCorpus BioNLP BioASQ 5b-factoid

MedSTS
BioSSES BioASQ 6b-factoid

Pre-trained
| BioALBERT Models
' with domain corpora 1 Inference

MedNLI

Figure 1. An overview of pre-training, fine-tuning and the diverse tasks and datasets present in Benchmarking for BioNLP
using BioALBERT

large vocabulary embedding matrix into two smaller matrixes. ALBERT’s other improvements are introducing sentence order
prediction (SOP) loss instead of next sentence prediction (NSP) and the introduction of cross-layer parameter sharing, which
prevents parameters from growing with the depth of the network. In the following section, we describe the steps involved in
training BioALBERT.

Pre-training BioALBERT

We first initialized BioALBERT with weights from ALBERT during the training phase. Biomedical terminologies have terms
that could mean different things depending upon its context of appearance. For example, ER could be referred to ‘estrogen
receptor’ gene or its product as protein. Similarly, RA may represent ‘right atrium’ or ‘rheumatoid arthritis’ depending upon
the context of appearance. On the other hand, two terminologies could be used to refer to a similar concept, such as ‘heart
attack’ or ‘myocardial infarction’. As a result, pre-trained LM trained on general corpus often obtains poor performance.

Table 2. List of text corpora used for BioALBERT

Corpus Number of Words Domain
English Wikipedia 2.5 Billion General
BooksCorpus 0.8 Billion General
PubMed Abstracts 4.5 Billion Biomedical
PMC Full-text articles 13.5 Billion Biomedical
MIMMIC-II 0.5 Billion Clinical

BioALBERT is the first domain-specific LM trained on biomedical domain corpus and clinical notes. The text corpora
used for pre-training of BioALBERT are listed in Table 2. BioALBERT is trained on abstracts from PubMed, full-text
articles of PMC and clinical notes (MIMIC) and their combination. These unstructured and raw corpora were converted to
structured format by converting raw text files into a single sentence in which: (i) within a text, all blank lines were removed and
transformed into a single paragraph, and (ii) any line with a length of less than 20 characters was excluded. Overall, PubMed
contained approximately 4.5 billion words, PMC contains about 13.5 billion words, and MIMIC contained 0.5 billion words.

We used sentence embeddings for tokenization of BioALBERT by pre-processing the data as a sentence text. Each line
was considered as a sentence keeping the maximum length to 512 words by trimming. If the sentence was shorter than 512
words, then more words were embedded from the next line. An empty line was used to define a new document. 3,125 warm-up
steps are used for training all of our models. We used LAMB optimizer during the training process of our models and kept the
vocabulary size to 30K. GeLU activation is used in all variants of models during the training process. For BioALBERT base
models, a training batch size of 1,024 was used, whereas, in BioALBERT large models, training size was reduced to 256 due to
computational resources limitations. Summary of parameters used in the training process is given in Table 3.

3/10


Table 3. Summary of parameters used in the Pre-training of BioALBERT

Summary of All parameters used: (Pre-Training)

Architecture ALBERT
Activation Function GeLU
Attention Heads 12
No. of Layers 12
Size of Hidden Layer 768
Size of Embedding 128
Size of Vocabulary 30k
Optimizer Used LAMB
Training Batch Size 1024
Evaluation Batch Size 16
Maximum Sentence Length 512
Maximum Predictions per Sentence 20
Warm-up Steps 3,125

We present 8 models (Table 4) consisting of 4 base and 4 large LMs. We identified that on V3-8 TPU, both base and large
LMs were successful with a larger batch size during training. The base model contained 128 embedding size and 12 million
parameters, whereas the large model had 256 embedding size and 16 million parameters.

Table 4. BioALBERT trained on different training steps, different combinations of the text corpora given in Table 2, and
BioALBERT model version and size

Model BioALBERT Size Combination of corpus Number of
Version. used for training training steps
1 Basel Wikipedia + BooksCorpus + PubMed 200K
2 Base2 Wikipedia + BooksCorpus + PubMed+ PMC 470K
3 Largel Wikipedia + BooksCorpus + PubMed 200K
4 Large2 Wikipedia + BooksCorpusPubMed + PMC 470K
5 Base3 Wikipedia + BooksCorpus + PubMed + MIMIC-III 200K
6 Base4 Wikipedia + BooksCorpus + PubMed + PMC + MIMIC-III 200K
7 Large3 Wikipedia + BooksCorpus + PubMed + MIMIC-III 270K
8 Large4 Wikipedia + BooksCorpus + PubMed + PMC + MIMIC-III 270K

Fine-tuning BioALBERT
Similar to other SOTA biomedical LMs *, BioALBERT was tested on a number of downstream BioNLP tasks which required
minimal architecture alteration. BioALBERT’s computational requirements were not significantly large compared to other
baseline models, and fine-tuning only required relatively small computation compared to the pre-training. BioALBERT
employed less physical memory and improvised on parameter sharing techniques, and learned the word embeddings using the
sentence piece tokenization, which gives it better performance and faster training compared to other SOTA biomedical LMs.

Table 5. Summary of parameters used in fine-tuning

Summary of All parameters used: (Fine-Tuning)

Optimizer used AdamW
Training Batch Size 32
Checkpoint Saved 500

Learning Rate 0.00001
Training Steps 10k
Warm-up Steps 320

During fine-tuning, we use the weights of the pre-trained BioALBERT LM. We used an AdamW optimizer and a learning

2We followed the same architectural modification as previous studies in the downstream task.

4/10


rate of 0.00001. A batch size of 32 was used during training. We restricted sentence length to 512 for NER and 128 for all other
tasks and lower-cased all words. Finally, we fine-tuned our pre-trained models for 10k training steps and used 512 warm-up
steps during the fine-tuning process. The test datasets were used for prediction, and the evaluation metric was compared with
previous SOTA models. Table 5 summaries all fine-tuning parameters.

Tasks and Datasets

We fine-tuned BioALBERT on 6 different BioNLP tasks with 20 datasets that cover a broad range of data quantities and
difficulties (Table 6). We rely on pre-existing datasets that are widely supported in the BioNLP community and describe each
of these tasks and datasets.

Table 6. Statistics of the datasets used

Dataset Task Domain Train Dev Test Metric
BCSCDR (Disease) NER Biomedical 109853 121971 129472  Fl-Score
BCSCDR (Chemical) NER Biomedical 109853 117391 124676  Fl-Score
NCBI (Disease) NER Clinical 135615 =. 23959 24488 — Fl-Score
JNLPBA NER Biomedical 443653 117213 114709  Fl-Score
BC2GM NER Biomedical 333920 70937 118189  Fl-Score
LINNAEUS NER Biomedical 267500 87991 134622 Fl-Score
Species-800 (S800) NER Biomedical 147269 22217 42287 —_Fl-Score
Share/Clefe NER Clinical 4628 1075 5195 Fl-Score
GAD RE Biomedical 3277 1025 820 Fl-Score
Euadr RE Biomedical 227 71 57 Fl-Score
DDI RE Biomedical 2937 1004 979 Fl-Score
ChemProt RE Biomedical 4154 2416 3458 Fl-Score
i2b2 RE Clinical 3110 11 6293 Fl-Score
HoC Document Classification Biomedical 1108 157 315 Fl-Score
MedNLI Inference Clinical 11232 1395 1422 Accuracy
MedSTS Sentence Similarity Clinical 675 75 318 Pearson
BIOSSES Sentence Similarity Biomedical 64 16 20 Pearson
: : : . Accuracy
BioASQ 4b-factoid QA Biomedical 327 - 161 (Lenient)
3 3 : ‘ Accuracy
BioASQ 5b-factoid QA Biomedical 486 = 150 {Lentient)
: : : . Accuracy
BioASQ 6b-factoid QA Biomedical 618 - 161 Cenient)

Named entity recognition (NER): Recognition of proper domain-specific nouns in a biomedical corpus is the most
basic and important BioNLP task. Fl-score was used as an evaluation metric for NER. BioALBERT was evaluated on 8
NER benchmark datasets (From Biomedical and Clinical domain): We used NCBI (Disease)*!, BC5CDR (Disease)!®,
BCSCDR (Chemical)!*, BC2GM??, JNLPBA!”, LINNAEUS”, Species-800 (S800)? and Share/Clefe!” datasets.

Relation Extraction (RE): RE tasks aim to identify relationship among entities in a sentence. The annotated data were
compared with relationship and types of entities. Micro-average Fl-score metric was used as an evaluation metric. For
RE, we used DDI™, Euadr*°, GAD?’, ChemProt’ and i2b2”° datasets.

Document Classification: Document classification tasks classifies the whole document into various categories. Multiple
labels from texts are predicted in the multi-label classification task. For the document classification task, we followed the
common practice and reported the Fl-score. For document classification, we used HoC (the hallmarks of Cancers)*!
dataset.

Inference: Inference tasks predict whether the premise sentence entails the hypothesis sentence. It mainly focuses on
causation relationships between sentences. For evaluation, we used overall standard accuracy as a metric. For inference,
we used MedNLI°® dataset.

Sentence Similarity (STS): STS task is to predict similarity scores by estimating whether two sentences deliver similar
contents. Following common practise, we evaluate similarity by using Pearson correlation coefficients. We used
MedSTS”? and BIOSSES”® datasets for sentence similarity task.

5/10


Table 7. Comparison of BioALBERT v/s SOTA methods in BioNLP tasks

ee SOTA BioALBERT Difference over
Basel Base2 Largel Large2 Base3 Base4 Large3  Large4 SOTA
Named Entity Recognition Task
Share/Clefe 75.40 94.27 9447 93.16 94.30 94.84 9482 94.70 94.66 19.44 ft
Disease) 87.15 97.66 97.62 97.78 97.61 90.03 90.01 90.29 91.44 10.63 fT
Fhenieal 93.47 97.90 98.08 97.76 97.79 89.83 90.08 90.01 91.48 4.61 t
JNLPBA 82.00 82.72 83.22 84.01 83.53 86.74 86.56 86.20 85.72 4.74 t
Linnaeus 93.54 99.71 99.72 99.73 99.73, 95.72, 98.27 98.24 98.23 6.19 F
Diseane) 89.71 95.89 95.61 97.18 95.85 85.82 85.93 85.86 85.83 TAT
$800 75.31 98.76 98.49 99.02 98.72 93.53 93.63 93.63 93.63 23.71 tT
BC2GM 85.10 96.34 96.02 96.97 96.33 83.35 83.38 83.44 84.72 11.87 ¢
BLURB 84.61 95.41 95.41 95.70 95.48 89.98 90.34 90.30 90.71 11.09t
Relation Extraction Task
DDI 82.36 82.32 79.98 83.76 84.05 76.22 75.57 76.28 76.46 1.69 ft
ChemProt 77.50 78.32 76.42 77.77 77.97 62.85 62.34 61.69 57.46 0.82 +
i2b2 76.40 76.49 76.54 76.86 76.81 73.83 73.08 72.19 75.09 0.46 +
Euadr 86.51 82.32 74.07 84.56 81.32 62.52 76.93 70.41 70.48 -1.95 |
GAD 84.30 73.82 66.32 76.74 69.65 72.68 69.14 71.81 68.17 -7.56 |
BLURB 79.14 78.66 74.67 79.94 77.96 69.62 71.41 70.50 69.53 0.80¢
Sentence Similarity Task
BIOSSES 92.30 82.27 73.14 92.80 81.90 24.94 55.80 47.86 30.48 0.50 t
MedSTS 84.80 85.70 85.00 85.70 85.40 51.80 56.70 45.80 42.00 0.90
BLURB 88.20 83.99 79.07 89.25 83.65 38.37 56.25 46.83 36.24 1.05¢
Inference Task
MedNLI 84.00 77.69 76.35 79.38 79.52 78.25 77.20 76.34 751 -4.48 |
Document Classification Task
HoC 87.30 83.21 8452 87.92 84.32 64.20 75.20 61.00 81.70 0.62
Question Answering Task
BioASQ 4b 47.82. 47.90 48.34 = 48.90 48.25 47.10 47.35 45.90 46.10 1.08 ft
BioASQ 5b 60.00 61.10 61.90 62.31 61.57 58.54 59.21 58.98 58.50 2.31 fT
BioASQ 6b 57.77. 559.80 62.00 ~— 62.88 61.54 56.10 56.22 56.60 56.85 5.11 t
BLURB 55.20 56.27 57.41 58.03 57.12 53.91 54.26 53.83 53.82 2.83¢

Note: The ‘difference over SOTA’ indicate the absolute change (t for increase and | for decrease) in metric performance over SOTA. Bold
has the best results. We report the SOTA model results on various datasets as follows: (i) JNLPBA, BC2GM, ChemProt, and GAD from Yuan
et al.!> (KeBioLM), (ii) DDI ad BIOSSES are from Gu et al.!* (PubMedBERT), (iii) Share/Clefe, i2b2, MedSTS, MedNLI and HoC from
Peng et al.!2 (BLUE), (iv) BC5CDR (disease), BC5CDR (chemical), NCBI (Disease), $800, Euadr, BioASQ 4b, BioASQ 5b, and BioASQ
6b, from Lee et al.!> (BioBERT), and (v) LINNAEUS from Giorgi and Bader**. The Biomedical Language Understanding & Reasoning
Benchmark (BLURB) is an average score among all tasks used in previous studies!*!5,

* Question Answering (QA): QA is a task of answering questions posed in the natural language given related passages.
We used accuracy as an evaluation metric for the QA task. For QA, we used BioASQ factiod>? datasets.

Results and Discussion

* Comparison with SOTA biomedical LMs: Table 7 summarizes the results* for all the BioALBERT variants in
comparison to the baselines +. We observe that the performance of BioALBERT? is higher than SOTA models on 17

3Refer to Table 4 for more details of BioALBERT size and training corpus and Table 6 for the evaluation metric used in each dataset.
4Baseline results were acquired from the respective original publication.
5Here, we discuss the best model of BioALBERT Out of 8 versions of BioALBERT.

6/10


Table 8. Comparison of run-time (in days) statistics of BioALBERT v/s BioBERT. Refer to Table 4 for more details of
BioALBERT size. BioBERT gas) and BioBERT gase2 refers to BioBERT trained on PubMed and PubMed+PMC respectively

Model Training time
BioBERT gase1 23.00
BioBERT gase2 10.00

BioALBERT gose1 3.00
BioALBERT gose2 4.08
BioALBERT arge1 2.83
BioALBERT arge2 3.88
BioALBERT gosc3 4.02
BioALBERT Bose 4.45
BioALBERT arge3 4.62
BioALBERTyarge4 4.67

datasets out of 20 datasets and in 5 out of the 6 tasks. Overall, a large version of ALBERT trained on PubMed abstract
achieved the best results among all the tasks.

For NER, BioALBERT was significantly higher compared to SOTA methods on all 8 datasets (ranging from 4.61% to
23.71%) and outperformed the SOTA models by 11.09% in terms of micro averaged Fl-score (BLURB score). For,
Share/Clefe dataset, BioALBERT increased the performance by 19.44%, 10.63% for BCS5CDR-disease, 4.61% for
BCS5CDR-chemical, 4.74% for JNLPBA, 6.19% for Linnaeus, 7.47% for NCBI-disease, 23.71% and 12.25% for S800
and BC2GM datasets, respectively.

For RE, BioALBERT outperformed SOTA methods on 3 out of 5 datasets by 1.69%, 0.82%, and 0.46% on DDI,
ChemProt and i2b2 datasets, respectively. On average (micro), BioALBERT obtained a higher Fl-score (BLURB score)
of 0.80% than the SOTA LMs. For Euadr and GAD performance of BioALBERT slightly drops because the splits of data
used are different. We used an official split of the data provided by authors, whereas the SOTA method reported 10-fold
cross-validation results; typically, having more folds increase results.

For STS, BioALBERT achieved higher performance on both datasets by a 1.05% increase in average Pearson score
(BLURB score) as compared to SOTA models. In particular, BioALBERT achieved improvements of 0.50% for BIOSSES
and 0.90% for MedSTS.

Similarly, for document classification, BioALBERT slightly increase the performance by 0.62% for the HoC dataset and
the inference task (MedNLI dataset), the performance of BioALBERT drops slightly, and we attribute this to the average
length of the sentence being smaller compared to others.

For QA, BioALBERT achieved higher performance on all 3 datasets and increased average accuracy (lenient) score
(BLURB score) by 2.83% compared to SOTA models. In particular, BioALBERT improves the performance by 1.08%
for BioASQ 4b, 2.31% for BioASQ 5b and 5.11% for BioASQ 6b QA datasets respectively as compared to SOTA.

We note that the performance of ALBERT (both base and large), when pre-trained on MIMIC-III, in addition to PubMed
and combination of PubMed and PMC, drops as compared to the same pre-trained ALBERT without MIMIC-III,
especially in RE, STS and QA tasks. Since MIMIC consists of notes from the ICU of Beth Israel Deaconess Medical
Center (BIDMC) only, the data size was relatively smaller than PubMed and PMC, and therefore we suggest that
the BioALBERT performance was poor when compared to models trained on PubMed only or PubMed and PMC.
BioALBERT (large), trained on PubMed with dup-factor as five, performed better.

Table 9. Prediction samples from ALBERT and BioALBERT. Bold entities are better recognised by BioALBERT

Dataset Model Sample

ALBERT Number of glucocoticoid receptors in lymphocytes and their sensitivity to. ..

SRLPEH BioALBERT Number of glucocoticoid receptors in lymphocytes and their sensitivity to...

ALBERT The mitral valve leaflets are mildly thickened . There is mild mitral annular calcification .TRICUSPID VALVE...

Share/Clefe 5: .4LBERT The mitral valve leaflets are mildly thickened . There is mild mitral annular calcification .TRICUSPID VALVE...

ALBERT In contrast , 15 Gy increased the expression of p27 in radiosensitive tumors and reduced it in radioresistant tumors.

Ho BioALBERT _ Incontrast , 15 Gy increased the expression of p27 in radiosensitive tumors and reduced it in radioresistant tumors.

7/10


We compared pre-training run-time statistics of BioALBERT with BioBERT with all variants of BioALBERT outper-
forming BioBERT. The difference in performance is significant, identifying BioALBERT as a robust and practical model.
BioBERT pase trained on PubMed took 10 days, and BioBERT gas¢ trained on PubMed and PMC took 23 days, whereas
all models of BioALBERT took less than 5 days for training an equal number of steps. The run time statistics of both
pre-trained models are given in Table 8.

mmm ALBERT
@@™ BioALBERT

— = omy | C4 | Bo} Be} 2a
7 a 5 FJ a oh 8 * Q e a 5 g vf} 2 = 3 + in Tr)
wv a uy a oD ba O jas) Po N 0 & aA in 2 r
o iy = ra c] 5 a N p a Z a 2 5 fe} 9 fe;
o ¥ o z = Y 8 0 fe) Z 2 4 < 2
5s 2 5 38 - a 3 9 3
= w 2 = oO [va] [va]
Ww [oa

S 4 g

wy U <

U wy

a fe)

a
Dataset

Figure 2. Comparison of BioALBERT v/s ALBERT

* Comparison with SOTA general LM (ALBERT): We compared the performance of ALBERT trained on general
corpora to BioALBERT with the results shown in Figure 2. BioALBERT consistently achieved higher performance on
all 6 tasks (20 out of 20 datasets) as compared to ALBERT. Further, as shown in Table 9, we sampled predictions from
ALBERT and BioALBERT to see the effect of pre-training on downstream tasks. BioALBERT can better recognise the
biomedical entities compared to ALBERT both in NER (JNLPBA and Share/Clefe) and document classification (HoC)
datasets.

Conclusion

We present BioALBERT, the first adaptation of ALBERT trained on both biomedical text and clinical data. Our experiments
show that training general domain language models on domain-specific corpora leads to an improvement in performance across
a range of biomedical BioNLP tasks. BioALBERT outperforms previous state-of-the-art models on 5 out of 6 benchmark tasks
and on 17 out of 20 benchmark datasets. Our expectation is that the release of the BioALBERT models and data will support
the development of new applications built from biomedical NLP tasks.

References

1. Meystre, S. M., Savova, G. K., Kipper-Schuler, K. C. & Hurdle, J. F. Extracting information from textual documents in the
electronic health record: a review of recent research. Yearb. medical informatics 17, 128-144 (2008).

2. Maartensson, L. & Hensing, G. Health literacy—a heterogeneous phenomenon: a literature review. Scand. journal caring
sciences 26, 151-160 (2012).

3. Storks, S., Gao, Q. & Chai, J. Y. Recent advances in natural language inference: A survey of benchmarks, resources, and
approaches. arXiv preprint arXiv: 1904.01172 (2019).

4. Peters, M. et al. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume I (Long Papers),
2227-2237, 10.18653/v1/N18- 1202 (Association for Computational Linguistics, 2018).

5. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language
understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume I (Long and Short Papers), 4171-4186 (2019).

8/10


10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.
22.

23.
24.

25.

26.

27.

28.

29.

. Lan, Z. et al. Albert: A lite bert for self-supervised learning of language representations. In International Conference on

Learning Representations (2019).

. Krallinger, M. et al. Overview of the biocreative vi chemical-protein interaction track. In Proceedings of the sixth

BioCreative challenge evaluation workshop, vol. 1, 141-146 (2017).

. Pyysalo, S., Ginter, F, Moen, H., Salakoski, T. & Ananiadou, S. Distributional semantics resources for biomedical text

processing. In Proceedings of LBM 2013, 39-44 (2013).

. Jin, Q., Dhingra, B., Cohen, W. & Lu, X. Probing biomedical embeddings from language models. In Proceedings of the

3rd Workshop on Evaluating Vector Space Representations for NLP, 82-89 (2019).

Si, Y., Wang, J., Xu, H. & Roberts, K. Enhancing clinical concept extraction with contextual embeddings. J. Am. Med.
Informatics Assoc. 26, 1297-1304, 10.1093/jamia/ocz096 (2019).

Beltagy, I., Lo, K. & Cohan, A. Scibert: A pretrained language model for scientific text. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 3606-3611 (2019).

Peng, Y., Yan, S. & Lu, Z. Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on
ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and Shared Task, 58-65 (2019).

Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics
36, 1234-1240 (2020).

Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing. arXiv preprint
arXiv:2007.15779 (2020).

Yuan, Z., Liu, Y., Tan, C., Huang, S. & Huang, F. Improving biomedical pretrained language models with knowledge.
arXiv preprint arXiv:2104.10344 (2021).

Naseem, U. et al. Bioalbert: A simple and effective pre-trained language model for biomedical named entity recognition.
arXiv preprint arXiv:2009.09223 (2020).

Suominen, H. et al. Overview of the share/clef ehealth evaluation lab 2013. In International Conference of the Cross-
Language Evaluation Forum for European Languages, 212-231 (Springer, 2013).

Li, J. et al. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database : journal biological
databases curation 2016 (2016).

Kim, J.-D., Ohta, T., Tsuruoka, Y., Tateisi, Y. & Collier, N. Introduction to the bio-entity recognition task at jnlpba. In
Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications,
JNLPBA ’04, 70-75 (Association for Computational Linguistics, USA, 2004).

Gerner, M., Nenadic, G. & Bergman, C. M. Linnaeus: a species name identification system for biomedical literature. BMC
bioinformatics 11, 85 (2010).

Doundefinedan, R. I., Leaman, R. & Lu, Z. Ncbi disease corpus. J. Biomed. Informatics 47, 1-10 (2014).

Pafilis, E. et al. The species and organisms resources for fast and accurate identification of taxonomic names in text. PLOS
ONE 8, 1-6, 10.1371 /journal.pone.0065390 (2013).

Ando, R. K. Biocreative ii gene mention tagging system at ibm watson (2007).

Herrero-Zazo, M., Segura-Bedmar, I., Martinez, P. & Declerck, T. The ddi corpus: An annotated corpus with pharmacolog-
ical substances and drug—drug interactions. J. biomedical informatics 46, 914-920 (2013).

Uzuner, O., South, B. R., Shen, S. & DuVall, S. L. 2010 i2b2/va challenge on concepts, assertions, and relations in clinical
text. J. Am. Med. Informatics Assoc. 18, 552-556 (2011).

Van Mulligen, E. M. et al. The eu-adr corpus: annotated drugs, diseases, targets, and their relationships. J. biomedical
informatics 45, 879-884 (2012).

Bravo, A., Pinero, J., Queralt-Rosinach, N., Rautschka, M. & Furlong, L. I. Extraction of relations between genes and
diseases from text and large-scale data analysis: implications for translational research. BMC bioinformatics 16, 1-17
(2015).

Sougancioglu, G., Ozturk, H. & Ozgur, A. Biosses: a semantic sentence similarity estimation system for the biomedical
domain. Bioinformatics 33, i49-158 (2017).

Wang, Y. et al. Medsts: a resource for clinical semantic textual similarity. Lang. Resour. Eval. 54, 57-72 (2020).

9/10


30. Romanov, A. & Shivade, C. Lessons from natural language inference in the clinical domain. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, 1586-1596 (2018).

31. Baker, S. et al. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics
32, 432-440 (2016).

32. Tsatsaronis, G. et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering
competition. BMC bioinformatics 16, 1-28 (2015).

33. Vaswani, A. et al. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, 6000-6010 (2017).

34. Giorgi, J. M. & Bader, G. D. Transfer learning for biomedical named entity recognition with neural networks. Bioinformatics
34, 4087-4094 (2018).

Code Availability

Pre-trained weights of BioALBERT models and reproduced results of the benchmarks presented in this paper is at https:
//github.com/usmaann/BioALBERT.

Competing interests

The authors declare no competing interests.

10/10
