arX1v:2510.10787v1 [cs.CL] 12 Oct 2025

REVIEW OF INFERENCE-TIME SCALING STRATEGIES:
REASONING, SEARCH AND RAG

Zhichao Wang* Cheng Wang Dong Nie
Inflection AI Georgia Institute of Technology ChatAlpha AI
zcwang0201@gmail.com

ABSTRACT

The performance gains of LLMs have historically been driven by scaling up model size and training
data. However, the rapidly diminishing availability of high-quality training data is introducing a
fundamental bottleneck, shifting the focus of research toward inference-time scaling. This paradigm
uses additional computation at the time of deployment to substantially improve LLM performance on
downstream tasks without costly model re-training. This review systematically surveys the diverse
techniques contributing to this new era of inference-time scaling, organizing the rapidly evolving field
into two comprehensive perspectives: Output-focused and Input-focused methods. Output-focused
techniques encompass complex, multi-step generation strategies, including reasoning (e.g., CoT, ToT,
ReAct), various search and decoding methods (e.g., MCTS, beam search), training for long CoT (e.g.,
RLVR, GRPO), and model ensemble methods. Input-focused techniques are primarily categorized by
few-shot and RAG, with RAG as the central focus. The RAG section is further detailed through a
structured examination of query expansion, data, retrieval and reranker, LLM generation methods,
and multi-modal RAG.

Keywords Inference-Time Scaling - Reasoning - Search - Decoding - Training for Long CoT - Few-Shot - RAG

1 Introduction

The rapid advancement of large language models (LLMs) transformed the landscape of natural language processing
(NLP), enabling breakthroughs in tasks from text generation to complex problem-solving. Historically, researchers
focused on scaling training processes by expanding computational resources—measured in floating-point operations
(FLOPs)—and enlarging both model size and training dataset diversity. This strategy, known as pre-training scaling,
drove remarkable progress in systems such as GPT-4 and its successors [1]. However, the supply of high-quality and
diverse training data became a critical bottleneck. Curating massive, reliable, and unbiased datasets was resource-
intensive and challenging, rendering further gains through larger models unsustainable. As a result, researchers began
to explore new directions for improving LLM capabilities without increasing model size.

In recent years, a paradigm shift emerged toward inference-time scaling, which enhanced model performance by
allocating additional computation during inference. Unlike pre-training scaling, which consumed vast resources during
model training, inference-time scaling optimizes the use of computation after deployment. This approach allowed
models to refine outputs or process inputs more effectively, leading to higher accuracy, improved reasoning, and greater
adaptability without costly retraining. This paper comprehensively reviews inference-time scaling techniques, organized
into output-focused and input-focused methods as shown in Figure[]]

From an output perspective, we examined techniques that improve the quality of model-generated results during
inference. These included reasoning-based approaches such as Chain-of-Thought (CoT), which encouraged step-by-
step problem-solving; Tree-of-Thought (ToT), which explored multiple reasoning paths; and Reason+Act (ReAct),
which combined reasoning and action in interactive settings. Search-based methods such as Monte Carlo Tree Search
(MCTS) and beam search balanced exploration and exploitation in sequence generation. We also reviewed decoding

“Corresponding author


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

. CoT / ToT / SC
Reasoning

Iterative Reasoning

Monte Carlo Tree Search

Beam Search

Search
Best-first Search
Tree Search
Best-of-N
‘ Modified Probability Decoding
Decoding

Speculative Decoding
Output» Constrained Decoding
Reinforcement Learning with Verifiable Reward

Training for Long CoT Process Reward Model

Generative Reward Model

Multi-modal Reasoning eee nee
Image Modification
Different LLM Architectures: Token-Level Ensemble
Model Ensemble Different LLM Architectures: Response-Level Ensemble
Same LLM Architectures Ensemble

. Query Expansion by Rewriting
Query Expansion

Query Expansion by Generation

[ nfere nce Chunking and Storage Strategies
e Data Graph RAG
Time rep
e Other Data Representations
Sca l ] ng Retrieval Refinement

Retrieval and Reranker Retrieved Document Refinement

Reranker
Input: RAG | — =

Multi-step RAG with Planning

‘ Self-Reflection RAG
LLM Generation

MDP by search

RAG with Tools and Rules

Collaborative RAG

Multi-modal RAG

Input: Few Shot

Figure 1: From the output side, different techniques will be discussed, including: 1) reasoning methods like CoT, ToT,
and ReAct; 2) search methods like MCTS and beam search; 3) decoding methods like Best-of-N, speculative decoding,
and constrained decoding; 4) training for long CoT like RLVR and GRPO; 5) multi-modal reasoning; and 6) model
ensemble. For the input side, it is further divided into RAG and Few-Shot. In RAG, it will be discussed from the
perspectives of: 1) query expansion, 2) data, 3) retrieval and reranker, 4) LLM generation, and 5) multi-modal RAG.


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Standard Prompting Chain-of-Thought Prompting

Q: Roger has 5 tennis balls. He buys 2 more cans of Q: Roger has 5 tennis balls. He buys 2 more cans of
tennis balls. Each can has 3 tennis balls. How many tennis balls. Each can has 3 tennis balls. How many
tennis balls does he have now? tennis balls does he have now?

A: The answer is 11. A:
The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to

make lunch and bought 6 more, how many apples Q: The cafeteria had 23 apples. If they used 20 to

do they have? make lunch and bought 6 more, how many apples
do they have?

A: The answer is 27. x A:

answer is 9. «/

Figure 2: CoT: LLM is asked to generate chain of thought before generating the final answer with few-shot examples of
prompt, CoT and answer. For zero-shot case, it will use "Let’s think step by step" to encourage LLM to think before
generating the final answer.

strategies—Best-of-N sampling, speculative decoding, and constrained decoding—that optimized fluency, efficiency, or
structure. In addition, we discussed fine-tuning strategies such as Reinforcement Learning with Verifiable Rewards
(RLVR) and Group Relative Policy Optimization (GRPO), which trained models to produce more coherent reasoning
chains. Finally, we included multi-modal reasoning and model ensembling, both of which expanded LLM capabilities
across tasks and modalities.

From the input perspective, we analyzed methods that increased the length of prompt to scale inference FLOPs and
improved the quality of inputs. These included few-shot learning, which enabled rapid adaptation from limited examples,
and Retrieval-Augmented Generation (RAG), which integrated external knowledge into generation. While few-shot
learning often appeared as an engineering practice rather than a core research focus, RAG became central to input-based
scaling. We reviewed its major components—query expansion, data, retrieval and reranker, LLM generation, and
multi-modal extensions.

Finally, we selected topics based on whether they increased computational FLOPs during inference to enhance
downstream task performance. Traditional inference-time scaling focused mainly on output-level methods like CoT
or MCTS. In this review, we also included input-based approaches, such as few-shot prompting and RAG, as longer
prompts proportionally increase inference FLOPs within the context window. Although some techniques—like
speculative decoding for acceleration, constrained decoding for structured output, or model merging for performance
enhancement—did not directly increase FLOPs, they nonetheless improved inference-time capability and thus warranted
inclusion. Through this synthesis, we aimed to establish a clearer understanding of inference-time scaling as a unifying
framework for advancing LLM efficiency and intelligence.

2 Output

2.1 Reasoning Techniques in LLMs

2.1.1 Prompting-Based Reasoning Techniques

Prompting-based reasoning techniques enable LLMs to perform multi-step reasoning through structured prompt design,
without modifying model parameters. These methods progressively evolve from simple step-by-step prompting to more
sophisticated frameworks that enhance reasoning diversity, structure, and consistency. Starting with Chain-of-Thought
(CoT), which elicits explicit intermediate reasoning steps, subsequent approaches such as Self-Consistency (SC),
Tree-of-Thought (ToT), and Graph-of-Thought (GoT) introduce mechanisms like ensemble reasoning, tree search, and
graph-based deliberation to improve accuracy, robustness, and scalability in complex reasoning tasks.


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

i i Chain-of- Tree of Thoughts (ToT Graph of Thoughts (GoT ;
Oumut G0) hain-o! Multiple CoTs (CoT-SC) ree of Thoughts (ToT) raph of Thoughts (GoT) [This work]

-Thought
(CoT)

Ly
Ly
Ly
Input Input Backtracking 4 Refining Input
Input Branching out ZS A from a chain ry ip
Y ys Lo y \ T iL : J ‘ ) a ¢
Output y y y y " ae a : Backtracking |
Q e .
Ss | Oded aN \ KS: @
"So | a ‘ Aggregatin Aggregating
Ce) Negative { » H kins! 3 thoughts
score r)
'
LJ
Ly
Ly
e
Ly

Output Abandon a chain Output Output ]
Dependencies
between thoughts Selecting Intermediate
achain with thoughts are
a = Sh. — —_
*. Backtrack

Figure 3: Comparison among: 1. CoT, 2. SC, 3. ToT, and 4. GoT

CoT As one of the earliest prompting-based reasoning techniques, CoT laid the foundation for structured reasoning
in LLMs. CoT guides models to solve problems by presenting examples of step-by-step reasoning that lead to
the final answer, as illustrated in Figure[2| In Zero-shot CoT, the prompt appends a simple instruction, such as "Let
us think step by step," to encourage the model to reason sequentially. In Few-shot CoT, the prompt includes a few
examples of queries, intermediate reasoning steps, and final answers, followed by the same verification instruction.
When faced with new problems, the model generates its own reasoning steps before producing the answer, which
improves interpretability. Experiments on arithmetic, commonsense, and symbolic reasoning tasks show that CoT
significantly boosts performance for large-scale models (over 100 billion parameters) on complex tasks. However, for
simpler tasks, improvements are minimal, as the model may skip unnecessary steps.

SC Building on CoT, the SC approach refines the reasoning process by incorporating diversity and consensus during
decoding. In {3], the authors extend CoT into CoT-SC by replacing conventional greedy decoding with a majority
voting mechanism to enhance reasoning reliability:

¢ Paths Generation: produces multiple paths using (temperature) sampling.

¢ Majority Voting: selects the final answer by aggregating the most frequent outcome.

This sample-and-marginalize approach leverages the insight that complex problems often have multiple valid reasoning
paths, and convergence on the same answer across independent paths increases confidence in its correctness. Building
on this, [4] proposes universal self-consistency (USC), extending the self-consistency framework to free-form generation
tasks where no single correct answer exists. USC generates multiple reasoning paths using a LLM and employs the
LLM as a judge to evaluate responses. The model selects the most consistent response based on a prompt, such as
"Evaluate these responses and select the most consistent based on majority consensus". Further advancing the concept,
[5] introduces a two-stage framework to improve zero-shot reasoning by dynamically generating few-shot examples.
In the first stage, an LLM produces multiple answers in a zero-shot setting, retaining only those that align with the
majority prediction (self-consistency). These answers are further filtered based on three criteria: low entropy, low
repetitiveness, and high diversity, resulting in a set of top-k samples. In the second stage, a new prompt incorporates
these top-k samples to generate a fresh set of answers, with the final answer selected via majority voting. This approach
enhances the robustness and adaptability of zero-shot reasoning in LLMs.

ToT Moving beyond linear reasoning, the ToT framework extends CoT into a structured search paradigm that explores
multiple reasoning paths in parallel. Proposed by Yao et al. [6], ToT generalizes the CoT approach by representing
reasoning as a tree search over coherent intermediate steps (“thoughts’’) rather than a linear, top-down sequence, as
illustrated in Figure] At each step, multiple candidate thoughts are generated and self-evaluated, which allows for:

¢ Lookahead: anticipates future steps to guide decision-making effectively.

¢ Pruning: eliminates less promising steps to focus on viable steps.

¢ Backtracking: revisits earlier steps to correct or explore alternative steps.

This deliberate problem-solving process significantly enhances performance on tasks requiring planning, exploration,
or global consistency. In [7], the ToT framework is enhanced by integrating a thought validator to filter out invalid


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

branches. When a query is received, multiple reasoner agents perform parallel ToT processes to decompose the query
and generate diverse next steps. Each generated thought is evaluated for (1) logical consistency, (2) factual accuracy, and
(3) completeness. Flawed paths are discarded, and feedback is provided for refinement. A consensus voting mechanism
then determines the final answer: if consensus is reached, the answer is returned; otherwise, additional iterations of
reasoning are conducted.

The Graph of Thoughts (GoT) framework, proposed in [8], further extends CoT and ToT, as shown in Figure[3] GoT
introduces two key operations: (1) refinement, where a thought is iteratively improved, and (2) aggregation, where
multiple thoughts are combined into a new thought. Lastly, a comparison among CoT, SC, ToT, GoT is in Table[I]

Technique | Key Mechanism Advantages

CoT[2] Step-by-step prompting Improves interpretability on complex tasks
SCB} Ensemble voting on diverse paths Enhances reliability and robustness

ToT (6) Tree search with lookahead/pruning —_ Better for planning and exploration
GoT[B] Graph with refinement/aggregation Scalable for complex deliberation

Table 1: Comparison of Prompting-Based Reasoning Techniques

2.1.2 Iterative Reasoning Paradigms in LLMs

This subsection examines iterative reasoning approaches in LLMs, which improve problem-solving through repeated
cycles of reasoning, action, and refinement. These methods address challenges in complex tasks, such as mathematical
reasoning, decision-making, and long-term planning, by breaking them into manageable steps. Key approaches include:
ReAct, which combines reasoning and actions in a loop with environmental feedback; self-improving techniques
like Self-Refine and Self-Debug, where models critique and enhance their own outputs; memory-based systems like
Reflexion and Cumulative Reasoning, which store and reuse past knowledge; planning frameworks (e.g., LLM+P and
Plan-and-Solve) which structure tasks into sequential steps; and selection-inference models or tool-using methods (e.g.,
PAL, ToolLLM), which leverage external computation or APIs for accuracy. Together, these paradigms enhance LLMs’
ability to generalize, reduce errors, and produce reliable, interpretable results across diverse applications.

Iterative Reasoning with Task Decomposition

Building on the foundation of reasoning techniques, iterative reasoning with task decomposition enhances complex
problem-solving in LLMs by breaking a query into manageable sub-tasks and progressively refining intermediate
results. Zhou et al. [9] introduced least-to-most prompting to address limitations in CoT prompting for complex problem
generalization. This two-stage framework includes::

¢ Decompose: decomposes a query into sub-queries using (a). few-shot examples of query decomposition and
(b). the current query.

¢ Solve: solves sub-queries sequentially with a prompt containing (a). constant examples of sub-problem
solutions, (b). previously answered sub-questions and their solutions and (c). the next sub-question to address.

This progressive approach enables LLMs to tackle tasks requiring symbolic manipulation, compositional generalization,
and mathematical reasoning. Wang et al. developed an iterative prompting framework to enhance multi-step
reasoning in complex tasks. A context-aware prompter, trained while keeping the LLM fixed, generates a new sub-query
based on the query and previously retrieved knowledge. The LLM then uses this sub-query to produce a sub-answer;
the loop continues until a stopping condition is met. This method improves performance on tasks requiring iterative
refinement of reasoning steps.

Iterative Reasoning with Feedback

Extending beyond decomposition, iterative reasoning with feedback introduces an adaptive loop where LLMs continually
refine their outputs through self-assessment or environmental responses. As proposed by Yao et al. [11], REACT
(Reason+Act) is a paradigm that integrates reasoning and action in large language models (LLMs) through a reasoning-
action-observation loop. The process involves three steps:

1. Reason: The LLM generates free-form thoughts, such as decomposing tasks, creating or updating plans,
injecting commonsense knowledge, or handling uncertainty.

2. Act: The LLM interacts with an external environment, such as querying Wikipedia or navigating text-based
systems.

3. Observe: Feedback from the environment informs subsequent reasoning and actions.


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Agent

External feedback f
———*|_ Self-reflection (LM)
eee

Internal Reflective

text

feedback

Evaluator (LM) Experience
(long-term memory)

sil i

—Obs / Reward 4 Environment

Use M to get feedback on its own output Use M to refine its previous output, given its feedback

-— Action
(a). Self-Refine: the same LLM for generate response and refine (b). Reflexion: iterative reasoning with memory

Figure 4: (a). Self-Refine: the same LLM is utilized for generating the response and providing the feedback, (b).
Reflexion: iterative reasoning with memory

This cycle continues until a final answer is reached. ReAct, a prompt-based approach, leverages few-shot in-context
examples and outperforms reasoning-only (Chain-of-Thought, CoT) and action-only baselines on various tasks. By
producing grounded, fact-driven reasoning traces, ReAct enhances human interpretability and reduces hallucinations.
Optionally, ReAct can be combined with CoT-SC to effectively integrate internal knowledge and external feedback.
In {12}, Self-Refine was proposed to enhance the capabilities of an LLM, as shown in Figure fa). Initially, the LLM
is guided by a prompt to generate an initial response based on a user query. The same LLM then takes the input and
initial response to generate feedback. Finally, the response is refined based on the feedback, starting another iteration.
The process stops when the maximum number of iterations is reached or a stop token is output. In Self-Debug [13],
inspired by rubber duck debugging, the authors propose a method for teaching LLMs to iteratively debug their own
code without human intervention or additional model training. Self-Debug involves three main steps: 1. generate
candidate code, 2. code execution, 3. explain the code line-by-line in natural language, and 4. produce a feedback
message based on the code explanation and execution results for debugging. In REFINER [14], the authors propose an
interaction-based framework where a generator language model produces intermediate reasoning steps, and a critic
model provides fine-grained structured feedback on errors. During inference, the generator produces a response, which
is sent to the critic for feedback. The feedback is then incorporated into the generator to refine the response, and
this iteration continues until the critic outputs “no hint” or the maximum iteration threshold is reached. Lastly, in
Self-Correct [15], a corrector is trained using self-corrective learning. In inference, a generator generates a response and
the corrector iteratively improves the response until the stopping condition. A comparison of these different iterative
reasoning with feedback is in Table [2]

Table 2: Comparison of Key Iterative Reasoning Paradigms with Feedback

Technique Feedback Source Loop
REACT External Environment (Observation) Reason-Act-Observe
Self-Refine Same LLM (Self-critique) Reason-Assess-Refine
Self-Debug External Execution & LLM (Explanation) Reason-Execute-Explain-Debug
REFINER Separate Critic LLM Reason-Assess-Refine

Iterative Reasoning with Memory

While feedback-based methods focus on short-term refinement, memory-augmented reasoning extends this concept,
enabling models to accumulate, recall, and leverage past experiences over time. In Reflexion [16], the authors propose a
verbal reinforcement framework that enables language agents to learn from experience without parameter updates as
shown in part (b) of Figure/4] The agent consists of four modules

¢ Actor: generates responses based on prompt and context.

¢ Evaluator: provides external feedback from environment or internal feedback from the model.

¢ Self-Reflection Model: transforms sparse signals (e.g., binary success/fail) into rich, semantic guidance.


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

¢ Memory: stores reflections episodically.

Finally, this memory is appended to the prompt in the next iteration, allowing the agent to iteratively improve its
performance. Cumulative Reasoning (CR), proposed by [17], enhances LLM problem-solving capability by emulating
human-like cumulative reasoning. CR contains four modules: 1. Proposer for idea generation, 2. Verifier for fact
checking via another LLM or external tools such as code interpreters or theorem provers and 3. Reporter for generating
final responses from accumulated knowledge and 4. Memory to store verified knowledge in a direct acyclic graph
(DAG) for subsequent reasoning. In [18], the authors propose In-Context Policy Iteration (ICPI), a method that uses
a LLM to implement policy iteration without gradient updates or expert demonstrations. A memory buffer stores
past trajectories (state, action, reward, next state), which are used to prompt the LLM to act as both a world model
(predicting next states and rewards) and a rollout policy (choosing actions). At each step, the LLM simulates rollouts
based on sampled trajectories from the buffer to compute Q-values. Actions are selected greedily with respect to these
Q-values, and new experiences are added to the buffer. This iterative process continues until convergence, improving
the policy solely through in-context learning.

Iterative Reasoning with Plan

In parallel, planning-based iterative reasoning introduces structured decision-making, where LLMs devise and refine
plans before executing them to handle long-horizon or multi-step tasks. LLM+P [19] is a framework that enables
large language models (LLMs) to solve long-horizon planning problems by leveraging classical planners. The system
translates a natural language description of a planning problem into PDDL using an LLM (with in-context learning),
solves it with a classical planner, and then converts the solution back into natural language. In Plan-and-Solve prompting
{20}, the authors improve CoT by addressing three common error types: 1. calculation errors, 2. missing-step errors and
3. semantic misunderstanding errors by introducing two instruction prompts: 1. PS Prompting and 2. PS+ Prompting.
PS Prompting first instructs the LLM to devise a plan and then execute it step by step: "Let’s first understand the
problem and devise a plan to solve the problem. Then, let’s carry out the plan and solve the problem step by step." PS+
Prompting adds detailed guidance based on PS Prompting: “extract relevant variables and their corresponding numerals”
and “calculate intermediate results (pay attention to calculation and commonsense)”. AdaPlanner first utilizes
an LLM to generate an initial plan based on task description, candidate actions and optional expert demonstrations to
generate an intial plan with Python-style function to 1. decompose tasks, 2. generate actions, 3. check results and 4.
parse information. During execution, AdaPlanner adapts the plan using two refinement strategies: in-plan refinement,
which extracts useful information from environmental feedback for upcoming actions without modifying the current
plan, and out-of-plan refinement, which revises the plan when unexpected errors occur (refine-then-resume). Lastly, the
system also uses skill discovery to store successful plans, improving sample efficiency.

The following works focus on iterative reasoning with plan on embodied tasks. In [22], the authors propose Inner
Monologue, a framework where a pretrained LLM acts as an interactive planner for embodied robotic tasks. The LLM
iteratively plans actions, executes them via pre-trained robotic skills, receives textual feedback from the environment
including success detection, passive, and active scene descriptions, and human feedback, and replans as needed. This
closed-loop reasoning allows the LLM to adapt to failures, gather information, and accomplish complex long-horizon
tasks without additional training. Text2Motion combines LLMs with motion planning for robots. The LLM
translates a language instruction into possible goal states, and then a planner searches for a sequence of skills to reach
one of these goals. The planner uses two approaches: generating full candidate plans and checking if they are feasible,
or building a plan step by step while checking each action. By performing verification on both the language and motion
aspects, Text2Motion achieves much higher success rates on long-horizon tasks than earlier LLM-based methods. In
[24], the authors propose DEPS (Describe, Explain, Plan, Select), an interactive LLM-based planning framework for
multi-task embodied agents in open-world environments. DEPS addresses two challenges of open-world planning:
(1) long-horizon tasks with strict sub-goal dependencies and (2) state-dependent feasibility of alternative sub-goals.
The framework works by (a) generating an initial plan via an LLM, (b) using a descriptor to summarize execution
outcomes, (c) prompting the LLM as an explainer to diagnose failures, and (d) replanning accordingly. To further
improve efficiency, DEPS introduces a learned goal selector that ranks parallel candidate sub-goals based on predicted
completion horizon.

Iterative Reasoning with Selection

Complementing planning approaches, selection-based reasoning focuses on identifying the most relevant information
or hypotheses across multiple inference cycles. In [25], selective inference (SI), a two-stage framework, was proposed
to enhance LLM’s capability on multiple-steps reasoning. In the first stage, as a selection module, LLM with few-shot
examples of good selection, scores the probability of facts and the facts with the highest scores will be chosen. In
the second inference stage, an LLM, provided with few-shot examples, is prompted to generate conclusion from the
selected facts. The new generated conclusion is added to the knowledge base for a new round of selection, and this
cycle repeats until reaching the final answer. In [26], the authors propose a faithful multi-step reasoning framework


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Chain-of-Thought (Wei et al., 2022) Program-aided Language models (this work)

~ ~
Q: Roger has 5 tennis balls. He buys 2 more cans of

tennis balls. Each can has 3 tennis balls. How many

Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls does he have now?

tennis balls. Each can has 3 tennis balls. How many

tennis balls does he have now? A: Roger started with 5 tennis balls.
tennis balls = 5

bought balls = 2 * 3
. The answer is

Q: The bakers at the Beverly Hills Bakery baked 200 answer = tennis balls + bought balls

loaves of bread on Monday morning. They sold 93 loaves ~ ~

in the morning and 39 loaves in the afternoon. A grocery Q: The bakers at the Beverly Hills Bakery baked 200
store returned 6 unsold loaves. How many loaves of loaves of bread on Monday morning. They sold 93 loaves
bread did they have left? in the morning and 39 loaves in the afternoon. A grocery

store returned 6 unsold loaves. How many loaves of bread

yy, \ did they have left? yy,
\ <

A:
loaves baked = 200

(

Model Output

>

loaves_sold_ morning = 93

The answer is 62. x loaves _sold_ afternoon = 39

loaves returned = 6

The answer is

answer = loaves baked - loaves_sold_ morning
- loaves_sold_afternoon + loaves returned

J

Figure 5: Program aided language model transforms the natural language problem into Python programs so that the
results are obtained from the execution of the Python program.

built on a Selection—Inference (SI) backbone with |. the Halter and 2. the Value Function. The Halter is a LLM to
decide when to stop the cycle through answering "Given inference, can you answer question?". If yes, the answer will
be generated from the last inference. Otherwise, an answer of "Unknown" will be returned. The Value Function predicts
a score for the (selection, inference) pair and prunes the ones with lower score given multiple generations.

Iterative Reasoning with Tool

Finally, tool-augmented reasoning extends LLM capabilities beyond text, allowing models to interact with external
computation engines, APIs, or symbolic systems for more accurate and interpretable results. In program-aided language
model (PAL) [27], the LLM is used to translate natural language problems into Python programs that define variables
and operations as intermediate reasoning steps to avoid calculation errors during CoT, as shown in Figure ae the
Python interpreter executes the code to find the final answer. In Program of Thoughts (PoT) prompting [28], LLMs
are prompted to generate reasoning steps in the form of Python programs, which are then executed by an interpreter.
In Faithful CoT [29], a query was sent to LLM to output a reasoning chain that interleaves 1. natural language (NL)
like a sub-query, dependency among sub-queries and rationale to answer each sub-query and 2. symbolic language
(SL) which is the executable program to solve each sub-query. Next, a deterministic external solver will execute
the code in SL to generate the final answer. In [30], the authors show that OpenAI’s Codex can solve, explain, and
generate university-level math problems at scale. Their method takes a math question, synthesizes a Python program
via zero-shot or few-shot prompting, executes it to obtain the solution, and then produces step-by-step explanations. In
Hypothesis search [31], given a prompt and multiple samples of input-output pairs, the authors proposed to utilize LLM
to generate, filter, and test multiple hypotheses. For each remaining hypotheses, LLM generates a Python code to realize
the hypothesis and tests it on the prepared examples, where programs that pass all tests are selected and the ones that
incurred errors will be sent back to LLM for debugging. Eventually, the best program is selected. In [32], ViperGPT
handles complex visual queries by using a code-generation LLM that decomposes the query into interpretable steps,


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

step 1. Selection step 2. Expansion step 4. Backpropagation

—_ en ©) Nod
\, or update O one

menel ll >

<<

\
1
!

tj

‘
LC()

O Terminal Node (w/ Result 1)

step 3. Evaluation wa i
step 5. Termination i @) Terminal Node (w/ Result 2)
State Prompt

Task description V Predicted Result

Action. ...
Observation: ...
Action: ...

Observation. ...

2

Figure 6: MCTS: Monte Carlo Tree Serarch includes four stages 1. selection, 2. expansion, 3. evaluation, 4.
backpropagation and 5. termination

Guidelines

E\ Open-source Instruction LLM

EX Fine-tuned LLM

Format requirements

Exemplars

= Database Environment

generates executable Python code to call pretrained vision and language modules, and sequentially executes each step
to produce the final answer.

Another direction is calling API tools to solve different problems. In ToolLLM [33], the initial LLM was fine-tuned on
ToolBench, a dataset of 16k APIs over 49 categories and muti-step tool reasoning. Eventually, given a query from the
user, relevant APIs are retrieved from the pool through a pretrained retriever and the ToolLLM can build the API call
with suitable parameters and derive the final answer through multi-tool and multi-step reasoning. TORA introduces
Tool-integrated Reasoning Agents for mathematical problem-solving via an iterative loop: 1. natural language rationale,
2. program/tool call, 3. tool output and 4. refined rationale, blending analytical reasoning with computational tools like
SymPy solvers. In ToolChain* [35], the A* algorithm is employed to search through a decision tree of possible actions.
The root node corresponds to the user query and each node is a possible tool, i.e., API function call. At each step, the
algorithm evaluates candidate leaf nodes by computing their total cost as the sum of the cumulative past cost and a
heuristic estimate of future cost. The node with the lowest total cost is then expanded, and its past cost functions are
updated accordingly. To improve reliability, long-term memory and self-consistency mechanisms are incorporated to
penalize erroneous branches. This iterative process continues until the termination condition is satisfied.

2.2 Search

This section provides an overview of search strategies used to guide LLMs in reasoning and sequence generation. We
cover tree-based methods such as MCTS and value-function-guided search, which systematically explore multiple
reasoning paths. Sequential decoding strategies, including beam search and stochastic beam search, are discussed for
generating high-quality and diverse token sequences. We also examine best-first that adaptively allocate computational
resources to promising reasoning paths. Finally, multi-agent LLM frameworks are introduced, highlighting how iterative
debate and collaboration among multiple models can improve reasoning accuracy and robustness.

2.2.1 Monte Carlo Tree Search

Monte Carlo Tree Search (MCTS) is a heuristic search algorithm used to make optimal decisions in large, uncertain,
or partially explored decision spaces—most famously in games like Go, Chess, and general reinforcement learning
settings. It balances exploration (trying new actions) and exploitation (using known good actions) through repeated
simulations as shown in Figure|6]

MCTS operates in four main steps, repeated many times to grow and refine a search tree:

1. Selection: Starting from the root (initial state), MCTS traverses the existing tree by selecting child nodes
according to a policy that balances exploitation and exploration commonly using the Upper Confidence Bound


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

(UCB) formula:

InN
UCT =r; + 0,{— (1)
ig
where 1; is the average reward of node j, N is the total number of visits to the parent, n; is the number of
visits to node j, and C controls the level of exploration. This step continues until a leaf node (a node not fully
expanded or terminal) is reached.

2. Expansion: If the selected node is not a terminal state, one or more new child nodes (representing possible
next actions) are added to the tree. This introduces unexplored actions for future simulations.

3. Simulation / Rollout: From the newly expanded node, the algorithm performs a random or heuristic-based
simulation until a terminal state (or a predefined depth) is reached. The outcome of this simulation (e.g.,
win/loss, score, reward) provides an estimate of the node’s value.

4. Backpropagation: The simulation result is then propagated back up the tree. Each node along the path from the
expanded node to the root updates its statistics (e.g., visit count and average reward). This gradually improves
the estimated value of each action and helps guide future selections.

After many iterations, MCTS converges toward optimal decisions: the action corresponding to the child of the root
with the highest estimated value is chosen. In short, MCTS efficiently builds a partial search tree guided by stochastic
sampling, making it powerful for problems with large or complex state spaces where exhaustive search is infeasible.

MCTS for LLM reasoning

In MCTS for LLMs, step-by-step reasoning is modeled as a Markov decision process (MDP), where states are
intermediate reasoning steps and actions are next-step generations from LLMs. In reasoning as planning (RAP) [B6],
a LLM is utilized as both a reasoning agent to predict the next action and a world model to predict the next state.
Next, planning is performed using MCTS guided by rewards including: 1. likelihood of action, 2. confidence of state,
3. self-evaluation to check the correctness of the reasoning step, and 4. task-specific heuristics, i.e., custom reward
signals based on the nature of the task to encourage goal-directed reasoning. Eventually, multiple reasoning paths
are generated and the final answer is derived by aggregating them. Building upon this framework, LE-MCTS
ensembles multiple LLMs to improve reasoning robustness. In each iteration, a leaf node is selected by balancing
exploration and exploitation, expanded by generating candidate next steps from multiple LLMs, and evaluated using
a preference reward model (PRM). The step with the highest reward is then propagated upward through optimistic
backpropagation, which prioritizes the best child rather than averaging over all children. This iterative process continues
until a predefined depth or computational budget is reached.

A different line of work explores value-function-guided MCTS, where the reward evaluation relies on learned value
estimators. In [38], probability tree state abstraction (PTSA) was applied to speed up MCTS by reducing the size of
the search tree by grouping or clustering nodes with similar states as a single node. When a new path comes in, it is
compared with the saved paths using PTAS function through Jensen-Shannon (JS) divergence between the probability
distributions of their predicted action values (Q-values). If the new path is merged into the old path list, the path
with the lower estimated value will be removed. Eventually, PTSA can reduce the computational cost by 10%-45%
without deteriorating the performance of MCTS. In PPO-MCTS [39], a decoding algorithm leverages the value model
from PPO to guide token-level MCTS. Specifically, the Q-value of each expanded node is initialized using the parent
node’s V-value to encourage exploration. After running multiple simulations, the probability of selecting a token
is made proportional to its visit count, which reflects higher expected rewards. In [40], a shared LLM-based value
function and outcome reward model (ORM) are trained. Then, tree search optimizes cumulative rewards by one of five
algorithms—BFS-V/DFS-V, classic MCTS (Monte Carlo backpropagation from terminals), MCTS-a (AlphaZero-like
with value approximations and visit-count selection), or MCTS-Rollout (hybrid restarting from root with intermediate
backups)—to explore paths. Lastly, different reasoning paths are aggregated via majority vote, ORM-max, or ORM-vote
to select high-reward outputs.

While the above methods rely on structured search guided by external or model-based reward signals, subsequent
works introduced self-refinement and human-like evaluation mechanisms into MCTS. Building upon this structured
reasoning paradigm, Monte Carlo Tree Self-refine (MCTSr) built a reasoning algorithm that augments LLMs with a
structured search-and-refinement process [41]. MCTSr includes five stages: 1. selection, 2. expansion, 3. self-refine, 4.
self-evaluation and 5. backpropagation. In self-refine, candidate solutions are improved through iterative refinement. In
self-evaluation, candidates were scored with self-reward mechanism using |. prompt constraint 2. full score suppression
and 3. repeated sampling. This cycle iterates until termination and yields the final reasoning paths. Complementary
to these self-refining approaches, Cooperative Reasoning (CoRe) proposed a human-like dual-system approach
including: 1. a generator to generate potential reasoning paths and 2. a verifier to score the individual step and the whole

10


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

response for solving math word problems. In inference, CoRe utilized MCTS to balance exploration and exploitation
through the verifier’s score on the generator’s response.

Finally, while the above approaches treat each response or reasoning step as a single unit of action, PPL-MCTS
[43] shifts the focus to token-level search and evaluation. This method enables constrained text generation without
fine-tuning by applying MCTS at the token-level, where leaf nodes are selected using the PUCT formula, allowing
fine-grained control over the generated content.

Tree-based Search

Unlike linear reasoning, tree search-based approaches—distinct from MCTS—expand into a branching structure that
examines multiple reasoning paths and systematically assesses alternative explanations and outcomes. In Maieutic
Prompting [44], the authors proposed "logical integrity": a proposition is integral if the LM gives opposite truth values
to Q and !Q consistently as shown in part (a) of Figure [7] For example, in Proposition Q: “The sky is blue.” and
Negation !Q: “The sky is not blue.’, the LLM demonstrates logical integrity if it assigns 1. True for Q and False for !Q
or 2. False for Q and True for !Q. Instead of directly predicting True/False or relying on a single explanation, the method
builds a maieutic tree: for each statement, the LM generates abductive explanations for both "True" and "False", then
recursively expands these explanations into deeper ones. At each step, explanations are checked for logical integrity, i.e.,
whether the LM gives consistent opposite labels to a statement and its negation. Non-integral branches are pruned and
the resulting tree contains only logically integral propositions, which are scored by the LM’s belief in each proposition
and its consistency with the original question. Eventually, these scores are formulated as weighted constraints and
the MAX-SAT solver assigns truth values across the tree to maximize overall consistency. In this way, the workflow
transforms noisy, inconsistent generations into a structured reasoning process that yields more reliable truth judgments.

2.2.2. Beam Search

Unlike greedy search, which selects the single most probable token at each step, beam search keeps track of the top k
tokens (called the beam width) at each generation step. By exploring multiple candidate sequences simultaneously, it
balances exploration and exploitation, increasing the likelihood of finding higher-probability overall sequences.

Beam Search

The following works introduced minor modifications to beam search to enhance its performance. In [45], the authors
discovered that sampling based method produced less repetitive, more creative, and more human-like text, while
likelihood strategies like beam search produced more factual and verifiable text. To make a balance between sampling
and likelihood strategies, the authors proposed delayed beam search, and it started the first L tokens using sampling
based method and the remaining tokens through beam search. In [46], the authors proposed to force the LLM to decode
the top-k tokens for the first token and then decode greedily for the remaining tokens to generate different responses
and select the best one for answering. To begin with, the authors discovered that if the LLM’s generated responses
utilized CoT, the top-1 candidate will have a much larger probability over the top-2 candidate for each token. However,
this is reversed if the LLM response did not utilize CoT. By forcing the model try all different top-k candidates at the
first token can increase the probability of CoT. In addition, the author proposed to utilized the confidence score:

janswer|

1

= Janswer| Ss? (P(yj le, y1,-*> »Yi-1) — P(yil@, yt.» Yi-1)) (2)
i=1
where P(y}|”,¥1,--+ ,Yi—1) referred to the top-1 candidate and P(y?|x, y1,--- , y;—1) referred to the top-2 candidate

for each token and the confidence score will be larger if the model is more confident. Eventually, the answer with the
highest confidence score will be selected.

Beyond improving token-level probability, recent work has focused on integrating external evaluators to guide beam
search toward correct reasoning paths. In [47], the authors propose DIVERSE (Diverse Verifier on Reasoning Step), a
method to improve language model reasoning on GSM8K. The approach has three components: (1) generating diverse
prompts to elicit multiple reasoning paths, (2) using a verifier to score each path and perform weighted voting instead of
naive majority voting, and (3) introducing a step-aware verifier that labels and evaluates each reasoning step individually,
allowing the model to identify partially correct reasoning paths and filter out faulty steps. In [48], the authors introduce
Deductive Beam Search (DBS), which integrates beam search with a pretrained deductive verifier to check factual
consistency. This ensures that each step in the reasoning chain is logically deducible from the previous one, thereby
mitigating error accumulation.

Verifier-guided beam search has also been extended to physically grounded planning tasks, where language models
must generate sequences that satisfy real-world constraints, such as robot actions and safety considerations. In SayCan
[49], LLM was combined with a robot’s affordance function to ground LLM’s plan with reality. In "Say", given the

11


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

current state in physical world, available skills of the robot and the final goal, the LLM predicted the probability of each
action. In the meantime, the robot estimated the possibility of each candidate action through the affordance function,
which was referred as "Can". Eventually, the probability of "SayCan" was multiplication of the probabilities of "Say"
and "Can", and the highest scored action was selected. The robot executed this action and moved to the next state for the
next action selection until the goal is achieved. In [50], SayCanPay framed LLM planning as a heuristic search problem
where Say and Can are the same as before while Pay referred to Long-Term Value Estimation to avoid short-visoned
actions. Eventually, the final score was the multiplication of probabilities of "Say", "Can" and "Pay" and it was utilized
in the beam search for generating the best possible responses. In [51], the authors propose Grounded Decoding, a
method that guides large language models with grounding functions to ensure generated plans are both semantically
meaningful and physically realizable for robots. At each decoding step, token probabilities from the LLM are combined
with probabilities from grounded models (e.g., affordances, safety constraints, preferences), effectively framing the
problem as probabilistic filtering. In Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE)
[52], a stepwise approach was proposed to solve the problem that LLMs often assign high probability to incorrect
reasoning steps. To begin with, a correctness discriminator was trained through 1. sampling steps, 2. finding the exact
mistakes like missing or incorrect steps and 3. discriminator training with the contrastive max-margin loss. After
training, a stepwise decoding strategy was applied with the score of each step as the multiplication of 1. LM likelihood
and 2. discriminator score for the candidate step.

Stochastic Beam Search

Another line of research addresses the lack of diversity in conventional beam search by introducing stochasticity into
the decoding process, enabling the generation of more varied yet high-quality sequences. In [53], the authors introduce
stochastic beam search (SBS), an algorithm that implicitly applies the Gumbel-Top-k trick to sequence models without
enumerating the exponentially large sequence space. The Gumbel-Max trick samples one element from a categorical
distribution, adds independent Gumbel noise to each element’s log-probability and takes the argmax. Here, replacement
refers to whether a sampled element is put back into the pool before the next sampling. Gumbel-Top-k samples the top-k
elements rather the argmax. In stochastic beam search, for each step, the log probability based on its parental sequence
was predicted, sampled with a Gumbel perturbation and finally pruned to maintain the top-k subsequences. Empirically,
SBS yields more diverse yet high-quality translations. In [54], the LLM played two roles: 1. next step generation
and 2. next step generation’s self evaluation as shown in part (b) of Figure[7| In each step, multiple next steps were
generated and each candidate was evaluated by the combination of 1. LLM’s probability of generating the response and
2. correctness confidence through self-evaluation. Then, a constrained SBS is utilized to sample answers by utilizing
the weighted score rather than greedily selecting the top-k next paths. The procedure proceeds step by step until the full
reasoning chain is constructed for producing the final answer. In [55], Conditional Poisson Stochastic Beam Search
(CPSBS) was utilized to address two limitations of standard beam search: high candidate overlap and biased expectation
estimates. Rather than selecting the top-k using argmax, it selects the next tokens through Conditional Poisson (CP)
sampling. The CP sampling procedure begins with an empty set. At each iteration, every token in the vocabulary is
considered for inclusion. For each token, the probability of being added to the set is determined by its weight, the
number of remaining slots, and a precomputed normalization factor. The process continues until the set reaches its
predefined size.

2.2.3 Best-first Search

To further enhance reasoning without retraining, best-first search methods scale inference-time computation by exploring
reasoning paths adaptively—allocating more compute to the most promising trajectories rather than treating all reasoning
chains equally. In [56], GPT-f was utilized to prove mathematical theorems by starting with a goal and generates
multiple proof steps, and these proof steps are first verified by a theorem verifier, i.e., Metamath, and then search
the next steps by either 1. cumulative log probability or 2. value function. Instead of merely searching for the next
steps sequentially, the system expands a priority queue of sub-goals, repeatedly applying candidate proof steps until
the entire proof tree is closed. In Q* [57], a Q-value model was trained via offline RL, rollouts, or stronger LLM
completions which predicts future potential of a given immediate reasoning step, where the optimal Q value represents
the total expected future reward from taking action a, in state s, and then acting optimally thereafter, where states refer
the partial reasoning traces and actions are next reasoning steps. At inference, candidate next steps are scored by (1)
aggregated utility (process-based rewards collected so far) and (2) heuristic value (the estimated optimal Q-value of
future steps). Using an A*-style best-first search, Q* guides LLMs toward the most promising reasoning trajectories.

2.2.4 Multi-Agent LLMs

In [58], the authors propose a multi-agent debate framework where multiple instances of LLM independently generate
answers to a query, critique each other’s reasoning, and iteratively update their responses across several rounds until

12


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

If you travel west f h from th
Q: If you travel wes far enough from the — Fake,
west coast, you will reach the east cost? 0
True, because / E, :You cannot reach the east True, because False, because
coast by going west?

iz: The Earth is round and if you travel

: in any direction long enough, you will Fale, because { , E Er
eventually return to where you started. : Epp: You can reach the east coast Logically Integral X
: integzal (£7) = 1 : by going west by traveling around True, because
; the world.
Ey *You cannot reach the | “integral (Bpg)= 1“ Err
east coast by going west. ; my Logically Integral

Err: If you travel in a specific
straight line, you will eventually

Depth 1 generation Depth 2 generation reach the other side, Prune non-integral Branch
integral (Epp)= 0 ~~

“integral (Ep)= 0

(a). Tree Search: Maieutic Prompting

Multi-step Reasoning Chain

Question Basic Greedy Decoding mueriiny copes = 88000
Marilyn's 1% record sold answer = marilyn_copies // ratio
10 times as many ‘ .

marilyn_copies = 88000 |

copies as Harald's. If
they sold 88,000 copies
combined, how many
copies did Harald sell?

copies_mar = total_sold / 10

ratio = 10

Multi-step Reasoning Chain

: j : total_sold = 88000
Self-Evaluation Guided Stochastic Beam Search marilyn_harald_ratio = 10

answer = total_sold / (marilyn_harald_ratio + 1)

ony 0 - Generation |
O LLM [EEE Evaluation

04°05 06 07 08 09 1.0
(b). Beam search with evaluator

Figure 7: (a). Tree search through Maieutic Prompting to ensure integrity, (b). Beam search with evaluator

reaching a consensus. Results showed that debate between LLMs can correct hallucinations and inconsistencies by
filtering out uncertain facts. They also discovered that "stubborn" prompting encouraged LLMs to defend their reasoning
to make the debates longer.

Building on this idea of iterative debate, subsequent works investigate how increasing the number of cooperating
LLM agents can systematically improve performance. In AgentForest [59], the authors show that LLM performance
systematically scales with the number of agents via a simple sampling-and-voting approach. For close-ended tasks,
majority voting is used, while for open-ended tasks, the answer with the highest BLEU score is selected. Experiments
across reasoning, generation, and coding tasks reveal that this brute-force scaling can match or exceed larger models,
yields greater gains on harder tasks and weaker models, and can further boost existing methods like CoT and Debate. In
multi-agent LLM training (MALT) [60], the authors propose a multi-agent post-training strategy with three specialized
roles: a generator, a verifier, and a refiner. Training data is automatically produced by expanding a multi-agent
search tree, where outcome rewards are propagated back to each agent using value iteration. Next, the generator is
fine-tuned through SFT and the verifier and refiner are fine-tuned through SFT and DPO. At inference, the generator
produces candidate answers, the verifier critiques them, and the refiner integrates the critiques to generate improved
responses, with majority voting over multiple iterations yielding the final answer. Beyond general reasoning, multi-agent
frameworks have also been applied to structured tasks such as code generation, where specialized agent roles coordinate
to produce correct outputs. In AgentCoder [61], a multiple-agent framework consisting of 1. programmer agent for
generating the code and refining the code based on error, 2. test designer agent for independently generating diverse and
unbiased test cases, 3. test executer agent that runs codes in Python. If all tests are passed, the final answer is derived.
Otherwise, the error messages are sent to the programmer agent for new code generation.

13


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

2.3 Decoding

The most frequently used decoding strategies in generation are broadly categorized into deterministic and probabilistic
approaches. Deterministic methods like greedy decoding select the most probable tokens, ensuring high quality and
reproducibility but often yielding less diverse text [62]. In contrast, sampling-based methods such as temperature
sampling [63], top-k sampling and nucleus (top-p) sampling [64] [65] introduce stochasticity to enhance diversity and
naturalness. In this part, more methods of decoding for inference-time computation scaling are discussed.

2.3.1 Best-of-N

Building on the idea of generating multiple candidate outputs to improve correctness and confidence, Best-of-N decoding
selects the most promising solution from a set of candidates, effectively balancing exploration and reliability during
inference. In [66], a generator—verifier framework was proposed where the generator produces multiple candidate
solutions, and the verifier—trained to predict correctness of (problem, solution) pairs—selected the best one. At
inference time, multiple solutions are generated and the highest-scoring one is selected for the final answer. In V-STaR
[67], the generator is iteratively fine-tuned on correct solutions, while a verifier is trained with Direct Preference
Optimization (DPO) on both correct and incorrect solutions. At inference, the verifier ranks multiple candidate
generations to select best-of-N. Another complementary approach focuses on task-specific evaluation through functional
correctness rather than learned verifiers. In CODET [68], the LLM is prompted to generate multiple answers and
multiple test cases to evaluate these answers, where all the answers were tested over all test cases. Based on their
results, the answers were clustered into different sets, where the score of the set is computed based on: 1. the number of
answers in the cluster and 2. the number of cases that they pass. Eventually, the answer with the highest score was
selected.

Beyond the use of verifiers, researchers have also investigated how scaling computation at inference can improve
performance. In [69], the authors proved the benefits of scaling the computation budge in inference. The authors
tested: (1) search with verifiers like Best-of-N, beam search, and lookahead search, and (2) iterative refinement. Easier
problems benefited more from iterative refinement, while harder problems benefited more from search with verifier
where the prompt hardness is defined by the pass@1. Lastly, using this adaptive strategy yields up to 4 times efficiency
improvements over best-of-N baselines. In Large Language Monkeys [70], the authors explored scaling inference
computation through repeated sampling to enhance LLM performance. Using positive temperature sampling, they
generate multiple candidate outputs, selecting the best via automatic verifiers (e.g., unit tests, proof checkers) or
heuristic methods (majority voting, reward models) for tasks without verifiers. Coverage—the likelihood of generating
a correct answer—scales log-linearly with sample size, modeled as an exponential power law.

2.3.2 Majority Voting

Majority voting is regarded as a special case of Best-of-N with a count-based scoring function. In [71], the authors
utilized majority voting on the fine-tuned model and observed significant improvement on performance in many
benchmarks. In [72], the authors discovered that LLMs have poor performance in estimating the confidence of their
answers. They propose Prompt Agreement, which estimates confidence by comparing responses across multiple diverse
prompts. Two approaches are introduced: 1. Multi-Prompt Log Probability utilized the majority voting to select
answers and compute the average log probability and 2. Multi-Prompt Agreement (Rand Index) clustered answers
into different clusters and applied Rand Index to compute the confidence on the answer. In QALIGN [73], an initial
response is generated, and then a Markov chain Monte Carlo (MCMC) process is applied using the QUEST proposal.
At each step, a random index in the sequence is chosen, the suffix is regenerated, and a Metropolis-Hastings acceptance
probability—based on reward difference and model likelihood—is computed to accept or reject the new sequence. This
produces a set of candidate sequences that approximate the optimal aligned distribution for the prompt. The final answer
is selected via majority voting for tasks with definite answers or Minimum Bayes Risk (MBR) for open-ended tasks.

2.3.3 Adjusted Probability Decoding

Beyond generating multiple candidates, another line of work focuses on adjusting token probabilities during decoding
to steer the model toward more accurate or desirable outputs. In KNN-LM [74], a pre-inference datastore is constructed
where the key is the embedding of previous tokens and the value is the next token. In inference time, the normal
sampling is realized by P(y|x). In contrast, datastore sampling retrieves the top 1024 nearest embeddings from the
datastore, along with their corresponding next tokens. Each retrieved token is assigned a probability proportional to the
negative exponential of the distance between its embedding in the datastore and the embeddings of the previous tokens
in the current inference, followed by normalization to obtain Pxnn(y|x). Eventually, the final sampling is a weighted

14


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[START] japan i s benchmark bend n

[START] japan (| s benchmark nikkei 22 75

[START] japan | § benchmark nikkei 225 index rose 226 . 69 » points

[START] japan . § benchmark nikkei 225 index rose 226 , 69 points , or @ 1

[START] japan  $ benchmark nikkei 225 index rose 226 , 69 points , or 1. 5 percent , to 10 , 9859

[START] japan  $ benchmark nikkei 225 index rose 226 , 69 points , or 1, § percent , to 10, 989 . 72» in

[START] japan  § benchmark nikkei 225 index rose 226 , 69 points , or 1. 5 percent , to 10 , 989 . 79 in tokye Late

[START] japan  $ benchmark nikkei 225 index rose 226 , 99 points , or 1, 5 percent , to 10 , 989. 79 in late morning trading . [END]

(a). Speculative Decoding: generate token by a draft model and verified by a target model

|

"house": "(Gryffindor] Slytherin |Ravenclaw| Hufflepuff)",

\}

Re ne Wa
Regular Expression Finite State Machine x) Ya.
yy
m W
Please fill in the following }Decode + FSM /piease fill in the following Decode + FSM FG,
information about Harry Potter. information about Harry Potter.| —Em> cee
3 — | O
"name": "Harry", sage J name": "k rov \ } JV A
= Age x ge" iv — ——
= hou x * fif * f Y,
Zi ). g .
Decoding Status Decoding Status V allowed next token Y CA

x not allowed next token ( (ae,

Constrained Decoding With Logits Mask

(b). Constrained Decoding: generate responses by masking invalid tokens in JSON and SQL

Figure 8: (a). Speculative Decoding to accelerate the generation process by generating with a small draft model in
parallel, verified by a target model simultaneously, filtering invalid tokens. (b). Constrained decoding: generates valid
tokens that satisfy constraint rules by masking invalid tokens.

summation of P(y|a) and Pyyn(y|) as:

Padjustea(y|2) = a x P(y|x) + (1 — a) x Penn (yl) (3)

In Proxy tuning [75], the authors propose a lightweight decoding-time method for adapting LLMs without modifying
their weights, making it applicable even to black-box LMs. Three models are required: a base large model M and two
smaller model, one base m— and one base but fine-tuned on the specific domain m+. Eventually, the probability for
next tokens’ generation will be

Padjustea (yl) = Paur(yl) + (Pm+ (yl) — Pm—(yl2)) (4)
during the inference stage.

Another representative work is Future Discriminators for Generation (FUDGE) [76], where a binary future discriminator
is trained to guide the model toward outputs with specific attributes. To begin with, the binary attribute discriminator
was trained to predict P(a|x1.;), i.e., the probability that the eventually completed sequence 21.,, will have the
desired attribute a. Then, FUDGE multiplies the generator’s original probabilities with the classifier’s predictions and
renormalizes to sample tokens, corresponding to a Bayesian factorization

Pay | Ts 352) x Pla | Diss) . P(x; | wei) (5)

Eventually, the updated probability distribution was utilized to sample the response. One more benefit of FUDGE lay
in its ability to extend to various attributes, where the log probabilities from different classifiers are summed before
adjusting the P(a; | 71:-1,4@).

2.3.4 Speculative Decoding

While probability modification refines token selection, speculative decoding accelerates the generation process by
leveraging a smaller draft model in tandem with the target model, maintaining output fidelity while improving efficiency.
In [77], the authors propose speculative decoding, a method to accelerate inference from LLM without altering their
output distribution as shown in part (a) of Figure[p] A smaller draft model g generates multiple candidate tokens which

15


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

are then verified by the target model p. The acceptance rule is derived from speculative sampling: for a proposed
token x, accept with probability min(1, vt) , reject otherwise, and if rejected, resample from the adjusted distribution
p'(x) «x max(0, p(x) — g(x)). This ensures that the final sample is exactly distributed according to p, i.e., identical to
standard decoding.

2.3.5 Constrained Decoding

Constrained Decoding

Constrained decoding ensures that generated sequences adhere to strict syntactic, semantic, or logical rules, making
it indispensable for structured output formats like JSON, SQL, or formal reasoning tasks. This is realized by setting
the logits of invalid tokens to negative infinity as shown in part (b) of Figure[8] In the small example, each time, three
candidate next tokens are given, while the invalid ones that violate the constraints will be filtered out as shown in the
red corssing, and the left one will be sampled by based on logits until the final response generation terminates.

To ensure specific words or phrases appear in the generated output, researchers first explored lexically constrained
decoding techniques. In [78], grid beam search (GBS) extends standard beam search to handle lexical constraints
by decoding over a 2D grid, where one axis represents time steps (t) and the other represents the number of covered
constraint tokens (c). At each step, candidate hypotheses (partial generations) can expand in three ways:

1. Generate: open hypotheses can freely generate tokens from the model’s distribution.
2. Start: begin a new lexical constraint.

3. Continue: closed hypotheses (those in the middle of a constraint) must continue generating tokens within that
constraint.

Lastly, the highest-scoring finished response is returned. Constrained beam search (CBS) augments beam search
with a finite-state machine (FSM) that encodes the constraints. Each FSM state represents which constraints have been
satisfied and has its own beam of candidate sequences. When a candidate sequence starts or ends a constraint, the FSM
transitions accordingly, placing the sequence into the beam of the appropriate next state. After each step, only the top-k
sequences in each FSM are selected. Eventually, one beams corresponding to accepting states can output valid answers
and the one with the highest probability will be selected as the final answer. In [80], Dynamic Beam Allocation (DBA)
is proposed as an efficient improvement over Grid Beam Search (GBS) for lexically constrained decoding. Unlike GBS,
DBA keeps the total beam slots fixed and dynamically redistributes these beam slots among different constraint banks at
each decoding step. This design ensures that the decoding cost remains constant regardless of the number of constraints.

Beyond lexical control, syntax-constrained decoding enforces grammatical and structural validity, especially important
for formal or machine-readable outputs such as SQL queries. In [81], parsing incrementally for constrained auto-
regressive decoding (PICARD) was proposed for constrained auto-regressive decoding that ensures the outputs of
language models remain valid in formal languages such as SQL. PICARD uses incremental parsing to constrain
decoding by selecting top-k token candidates and pruning invalid ones by assigning negative infinity to their logits. It
employs three checks: (1) lexing to reject lexically invalid tokens, (2) parsing without guards to enforce grammatical
correctness, and (3) parsing with guards to add semantic checks, ensuring valid SQL and correct abstract syntax tree
structures. In [82], Language Model Programming (LMP) extends traditional text prompting by integrating three
elements: (1) natural language prompts, (2) programming constructs such as loops and conditions, and (3) explicit
constraints. The process begins by constructing a query that specifies the decoder, decoding strategy (e.g., argmax or
beam search), the prompt (including fixed text, holes like [VAR] for model generation, and placeholders like VAR for
substitutions), control flow elements (such as conditions and loops), the target model, and any decoding constraints
(e.g., output length or stopping phrases). This query is then parsed and executed like a Python program, where an
interaction trace is built. During decoding, constraints are enforced immediately by masking invalid tokens, ensuring
the generated output always satisfies the specified rules.

Moving from syntax to reasoning, logic-constrained decoding integrates symbolic constraints into the generation
process to ensure logical soundness. In neurologic decoding [83], a beam search-based algorithm that enforces logical
constraints during text generation was proposed, where the constraints are expressed in Conjunctive Normal Form
(CNF). In each step, the algorithm tracks each constraint’s state, classifying it as reversible (can still be satisfied/violated)
or irreversible (permanently satisfied/violated), prunes dead-end hypotheses that violate constraints irreversibly, groups
the remaining candidates by their unique sets of satisfied constraints to ensure diverse solutions, and selects the best
candidates from each group based on a score that balances language model likelihood and progress towards fulfilling
unmet constraints. The beam search process continues until reaching out the final answer where all the constraints
are satisfied. In [84], the authors propose NEUROLOGIC A*esque (NEUROLOGIC*) decoding, an extension of

16


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Verifiable Reward

O otherwise

- Scalar
ac Reward

10 if correct
ee

Or41 = O + AVoI(T)
Policy Update

Reference
Model
Reward
Model

Frozen

Models
r Group A
LJ "| Conpttin|* ;

(b). GRPO: reinforcement learning with verifiable reward

Trained
Models

Figure 9: (a). RLVR: reinforcement learning with verifiable reward to enhance the model’s capability on reasoning, (b).
GRPO: group relative preference optimization to simplify PPO through alleviating the usage of value model

NEUROLOGIC decoding that incorporates A*-style lookahead heuristics. Unlike standard beam search, which only
considers past probabilities, NEUROLOGICF augments candidate scoring with heuristic estimates of future cost.
Several lookahead strategies are introduced—greedy, beam, and sampling—that approximate future continuations
efficiently. For constrained generation, NEUROLOGICF builds on NEUROLOGIC’s logical CNF formulation and
integrates estimates of future constraint satisfaction into the scoring function.

Finally, constrained decoding has been applied to formal reasoning and theorem proving, where step-by-step logical
validity is essential. In NATURALPROVER [85], the model was fine-tuned to generate proof with the context of
reference through stepwise reference reconstruction objective. In inference, the reference is provided by either human
or retriever, and stepwise constrained decoding is utilized to enforce reference usage for final prover generation. In
[86], the authors utilized LogicGuide, i.e., an external prover tool, with constrained decoding to guarantee reasoning
reliability. To begin with, the LLM parses the query to generate assumptions, whose syntax is checked by LogicGuide.
During inference, the LLM generates actions (e.g., [[object:]], [[axiom:]], [[infer:]]), and constrained semantic decoding
(CSD) restricts outputs to valid logical steps based on prior context. This step iterates until reaching the answer or
maximum number of threshold.

Template+Placeholder

Constrained decoding realizes a strict format by modifying next tokens’ logits based on different constraints. In
comparison, the following methods firstly generated a template with placeholder, and then fill the placeholder with true
content in the second iteration. Although they are different from constrained decoding, they are summarized here for
straightforward comparison. In AutoTemplate [87], the authors propose a simple two-stage framework for lexically
constrained text generation: (1) generate a template containing placeholders conditioned on the input and constraint
lexicons, and (2) replace the placeholders with the given lexicons.

17


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

2.4 Training for Long CoT

This section reviews methods for enhancing LLMs’ reasoning capabilities through training to generate long CoT before
generating the final answer. We first discuss DeepSeek-R1, which leverages RLVR to automatically generate long CoT.
Next, we examine Process Reward Models (PRMs), which provide stepwise evaluation of reasoning, and contrast them
with outcome-based reward models (ORMs). Finally, we present Generative Reward Models (GRMs), which extend
reward modeling with CoT reasoning.

2.4.1 DeepSeek R1

The authors propose two reasoning-focused LLMs—DeepSeek-R1-Zero and DeepSeek-R 1— trained primarily with
large-scale RL with verifiable reward as shown in part (a) of Figure [9 First, DeepSeek-R 1-Zero applies RL
directly to a base model, i.e., DeepSeek- V3-Base without SFT. Using GRPO as shown in part (b) of Figure [9]and
rule-based verifiable rewards that check results’ format and accuracy, the base model gradually self-evolves into a
strong reasoner with emergent behaviors like reflection, longer chains of thought and “aha” moments. However, it
suffers from readability and language mixing issues. To improve usability and accelerate convergence, DeepSeek-R1
introduces a multi-stage pipeline starting from the same base model:

¢ Step 1-Cold Start: Collect thousands of high-quality, human-readable long CoT examples to fine-tune the base
model with better readability as the starting point for RL.

¢ Step 2-RL: Apply RL on the Cold Start SFT model with rewards for answer format, accuracy, and language
consistency.

¢ Step 3-RFT: The checkpoint from previous step generates around 800k high-quality samples, including
reasoning and non-reasoning data, via rejection sampling. The base model is fine-tuned on the collected
dataset for 2 epochs.

¢ Step 4-RL: Apply RL again on the model derived from previous step, including rewards for helpfulness and
harmlessness.

Last, reasoning ability is distilled into smaller open-source dense models using the 800k curated samples from
DeepSeek-R1’s SFT stage via SFT.

2.4.2 Process Reward Model

In [89], a process reward model (PRM) was trained using the PRM800K dataset and it could provide stepwise reward
evaluation for LLM response. The results showed that the model trained with PRM outperformed outcome reward model
(ORM). In [90], the authors apply RLHF with both outcome-supervised reward models (ORMs) and process-supervised
reward models (PRMs) to improve LLM reasoning in mathematics. They find that PRMs, which provide fine-grained
stepwise rewards, significantly boost performance on simpler tasks but reduce performance on complex ones. In
addition, in Best-of-N or critic-based methods, PRM can act the role of scoring function to rank different generated
responses.

2.4.3. Generative Reward Model
LLM-as-a-Judge and SFT

In Reinforcement Learning from Human Feedback (RLHF), a reward model r(x, y) is employed to assign a scalar score
to aresponse y given a prompt x. This model is typically trained using the Bradley—Terry (BT) framework to reflect
human preference comparisons [91]. While effective, such reward models provide only a numerical score without
offering reasoning or interpretability. To address this limitation, the LLM-as-a-Judge paradigm was introduced, where
LLMs themselves serve as scalable evaluators of conversational quality in open-ended, multi-turn settings [92]. Instead
of outputting a score directly, a model such as GPT-4 first generates a CoT rationale and then produces a final judgment.
Later works further trained open-source LLMs on preference data with CoT using SFT to align reasoning with human
evaluations [93] {94]. Overall, this methodology improves the fidelity and robustness of evaluations through inference
time scaling.

Generative Reward Model by RLVR

Previous studies leveraged LLMs’ inherent CoT reasoning abilities or fine-tuned them via SFT to strengthen their
CoT-based response evaluation. Subsequent works introduced RLVR, where the reward signal is based on the alignment
between the LLM’s judgments and human judgments, thereby enhancing the model’s capacity for long CoT reasoning
and improving its overall judgment performance. In [95], the authors introduce Generalist Reward Modeling (GRM),

18


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

uestion: If the FED buys bonds in the secondary market Principle 1: Instruction Adherence (Weight: 4); Response 1 is better than Too Easy/
Lys ry W

(A) the money supply will increase. (B) the money supply will decrease. (C) the money supply will not be Principle 2: Level of Detail (Weight: 3); Principle :..._ [| Response 2. Final Scores: {{2, 41) Incorrect
affected. (D) the discount rate would be affected. Output the answer only. Principle 1: Safety (4); Principle 2: Clarity (2); |_J Response 1 is not as well as RFT
Assistant A: Assistant B: Principle 3: Accuracy (2); Principle 4: Relevance (2). Response 2. Final Scores: [[6, 11] Dataset
The Federal Reserve buys (A) the money supply will increase. i “ = ia :
bonds in the secondary Qttonab round Truth eee
market to increase the RL [Q&R . argmaxi{r))
money supply. Rolling [Principle 4: Logic Chain Correctness (35%): The Extract memaxitry)

[crm ] Out response should induce each step with evidence ...; — >| 2/10, 7/10 Rules
Follow-up Question: How does it affect my daily life? Give 3 examples. (ale ||
Assistant A: Assistant Be J Online Update a
The Federal Reserve buys When the Federal Reserve buys bonds in the secondary market, it canhave | ------~--~-------------- = <
bonds in the secondary several effects on the economy that can impact your daily life. Here are Inference eS
market to increase the three examples: Principle 1: Technical Accuracy (Weight: 30%): The B sseaee
poney supply. This response should accurately describe the technical Overall, Response 2 is better than
affects your daily life 1. Interest rates: The Federal Reserve's actions in the bond market can steps ..; For example, ....j Response 1 according to
by: influence interest rates, which in turn can affect the cost of borrowing ‘iple 2: Practical Implementation (Weight: 25%)...; principles and the weights.
4. Increasing the money money: For exeaple; if the Federal Reserve buys bonds:andidrives‘up demand | 8 ait (<tésésd Final Scores: [{1, 5]] (0) wien
supply. for them, interest rates on things like mortgages, car loans, and credit
2. Increasing the money cards may go up, making it more expensive to borrow money. Principle 1: Clarity and Organization (Weight: 40%): Analysis: ....
supply and lowering the 2. Inflation: When the money supply increases, it can lead to inflation, The response should be well-organized ...; Scores: (4, 5, 8) and (7, 5, 5)
discount rate. which is the general rise in prices of goods and services. This can impact QaR Principle 2: Compliance with Human Value (35%)...; However, Principle 1 outweighs Voting
3. Increasing the money your daily life by making it more expensive to buy things like groceries, Parallel | Principte 3: Inductive Reasoning Correctness (25%)...; |_| Principle 3, resulting in...
supply and lowering the gas, and other necessities. Sampling. [= Final Scores: (5, 6]] Extract
discount rate and 3. Employment: The Federal Reserve's actions in the bond market can also [crm } —_
increasing the money have an impact on employment. For example, if the Federal Reserve's Principle 1: Practicality (Weight: 30%): The response
supply. actions lead to economic growth and increased demand for goods and provide steps can easily implement...

services, it can create more job opportunities and boost employment. Principle 2: Logical Coherence (Weight: 30%)...; 4/10, 8/10
Principle 3: Risk Awareness (Weight: 20%)...;
GPT-4 Judgment: ae meee : 6) 0808
Assistant A provided an incorrect response to the user's question about how the Federal Reserve ———— et
buying bonds’ in the secondary narket affects daily life, The answer given 1s repetitive ‘and 1acks) Principle 1: Technical Accuracy (Weight: 30%): Analysis: ....
Clear exanples of how the action inpacts daily life. Principle 2: Language Proficiency (Weight: 25%): The For Response 1, score: (8, 7, 7, 4)
On the other hand, Assistant B provided a relevant and accurate response to the user's question about ieaponibe shoidlltie mite sveciic lang aot Forhasponse2 scores (6.5.6 4) 7/0, 6/10 5/20, 13/20
the Federal Reserve buying bonds. The answer includes three clear examples of how the action impacts Principle 3: Engagement and Appeal (Weight: 25%): Considering the overall weights,
daily life, such as interest rates, inflation, and employment. Ualaigremrucs nisreaincend renioabie se. Final Scores: {17, 6]] ®
Assistant A should improve by providing relevant, clear, and accurate examples to address the user's ‘
question. Assistant B's response is more helpful, relevant, and detailed. Principles Critiques Rewards
(a). LLM-as-a-Judge (b). GRM: generalist RM

Figure 10: (a): LLM-as-a-Judge: utilize LLM to judge by outputting CoT and final evaluation, (b). GRM: generalist
reward model to output principle and then judge based on the generated principle through RL.

designed to handle flexible inputs—such as single, paired, or multiple responses—and enable inference-time scaling of
reward signals across diverse domains. To initialize the model, they employ Rejective Fine-Tuning (RFT). For each
query-response pair with ground-truth labels, multiple trajectories of principles (evaluation criteria for judgment) and
critiques are generated. Trajectories that are incorrect are discarded, while correct ones are used to fine-tune the GRM,
ensuring proper alignment and output format. Following this, a rule-based online RL phase is conducted using GRPO.
In this phase, the GRM first generates principles adaptively based on the prompt, then produces critiques conditioned
on both the prompt and the principles. A binary reward is assigned (+1 if the pointwise score matches the ground truth,
-1 otherwise), along with a KL penalty to avoid reward hacking. This approach encourages scalable behavior, such as
producing diverse principles and accurate critiques. During inference, parallel sampling is used to generate multiple sets
of principles and critiques. Final rewards are aggregated through voting or enhanced using a meta reward model (meta
RM) that filters out low-quality samples. Experiments show that this method outperforms baselines on reward modeling
benchmarks and demonstrates strong inference-time scaling. In [96], a similar workflow was tested on Helpsteer v3
dataset and observed a clear improvement in reward modeling.

In [97], Reward Reasoning Models (RRMs) take a query with two candidate responses and are prompted to reason
across multiple evaluation criteria, such as helpfulness, accuracy, and harmlessness before deciding the preferred
response. To train RRMs via RLVR and GRPO, the model generates multiple reasoning paths based on different
evaluation criteria and the reward is derived based on consistency with human evaluation. In addition, RRMs employ
multi-response rewarding strategies, i.e., selecting the best response from multiple candidates. It is realized through
(i) an ELO rating system that converts round-robin pairwise comparisons into numeric ratings, and (ii) a knockout
tournament that progressively eliminates weaker responses.

2.5 Multi-Modal Reasoning

Recent advances in multi-modal reasoning have aimed to extend the CoT paradigm beyond text-only reasoning,
enabling LLMs to jointly process and reason over heterogeneous modalities such as vision, language, and audio. These
studies generally follow two complementary directions: (1) multi-modal input reasoning, which integrates visual or
auditory signals directly into the reasoning process through multi-modal LLMs (MLLM) or modular expert like vision
encoder; and (2) image modification approaches, which transform visual inputs—via marking, sketching, or coordinate
scaffolding—to enhance grounding, spatial understanding, and iterative reasoning.

2.5.1 Multi-Modal Input
MLLM

Recent works have explored enhancing CoT reasoning in LLMs by incorporating visual inputs, enabling models to
jointly reason over text and images in a two-stage framework:.

19


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Language
Question: Will these magnets attract or repel each
1 other?

| Context: Two magnets are placed as shown. Hint:
Magnets that attract pull together. Magnets that
' repel push apart.

: Options: (A) attract (B) repel

Rationale: Will these magnets attract or repel? To find out, look at which poles are
closest to each other. The north pole of one magnet is closest to the south pole of the
other magnet. Poles that are different attract. So, these magnets will attract each other.
Answer: The answer is (A).

(a). VLLM takes image and text as input for CoT and
final answer generation

User Whatis on the left side of the right laptop?
GPT-4V On the left side of the right laptop, there is a

cup or mug. <

User | want to find a seat close to windows,
where can | sit?
You can sit on either of the two black

GPT-4V __ chairs in front of the white desks, as they

Ke are closest to the windowed wall. X

@ Conversation + Pg

What is on the left side of the right laptop?

GPT-4V On the left side of the right laptop
(numbered as 9), there is a lamp, which is
numbered as 12. \/

User _I want to find a seat close to windows,
where can | sit?

You can sit on the chair numbered as 7. It's
the closest seat to the windows
(represented by the area numbered as 1). \/

GPT-4V

(b). VLLM generate marker on images and then
conduct CoT and and answer generation

Figure 11: (a). VLLM takes image and text as input to generate CoT and final answer, (b). VLLM adds markers on the
images to enhance VLLM’s capability on multi-modality.

1. Rational Generation: MLLM takes multi-modality inputs to generate rationales.

2. Answer Generation: MLLM takes multi-modality inputs and rationale to generate answer.

In [98], the authors propose multi-modal CoT (MCoT) through MLLM, a two-stage framework for CoT reasoning that
incorporates both language and vision inputs as shown in part (a) of Figure[I I] In the first stage, the MLLM generates
rationales by fusing textual inputs (question, context, options) and images. In the second stage, these rationales are
concatenated back with the original inputs to infer the final answer. In Image-of-Thought (IoT) [99], a two-stage
framework was proposed |. generate chain-of-multi-modal-rationales and 2. hybrid-rational-based answer refinement
to extend CoT reasoning into the multi-modal setting. In the first stage, the image and query were sent to the MLLM
to break down into sub- tasks, to select action for each sub-task and execute the task to obtain the textual and visual
rationale to form the final <Sub-Goal, Visual Rationale, Textual Rationale> triplet, i.e., Multi-modal Rationale Series
(MRS). In the second stage, MRS were sent back to MLLM for the hybrid-rational-based final answer refinement.

Expanding beyond rationale generation, introduces compositional CoT (CCoT), a zero-shot prompting method
designed to improve compositional reasoning in MLLMs. CCoT first prompts the MLLM itself to generate a structured
scene graph (objects, attributes, and relations) in JSON format from an input image and task prompt. In the second step,
the MLLM model is prompted with the original image, task, and generated scene graph to produce the final response.
This structured intermediate step encourages MLLMs to move beyond “bag-of-objects” reasoning and better capture
compositionality.

Unlike traditional CoT, which relies solely on text, Multi-modal Visualization-of-Thought (MVoT) enables
models to generate interleaved verbal thoughts (text reasoning steps) and visual thoughts (image visualizations of
reasoning traces). All the previous steps’ information is utilized for the next step’s generation until the termination of
the generation. To improve visual quality, they introduce a token discrepancy loss that bridges the gap between text and
image tokenizers in autoregressive MLLMs.

Building on these multi-modal reasoning frameworks, Contrastive CoT (CoCoT) specifically addresses multi-
image tasks, aiming to resolve two common failure modes in LMMs: insufficient fine-grained perception and unwanted
blending of information across images. In the first stage, the MLLM is prompted to identify the similarities and
differences among these images. Then, in the second stage, the MLLM is prompted to utilize the comparison analysis
among these images to answer the question with details.

LLM+multi-modal experts

To enable multi-modal reasoning for LLM without additional training, recent studies have explored modular composition
frameworks that orchestrate multiple pretrained foundation models—such as LLMs, VLMs, and ALMs—through
language-based prompting interfaces. In Socratic Models (SMs) [103], a framework for zero-shot multi-modal reasoning
was proposed to compose multiple pretrained foundation models through language-based prompting as an intermediate
representation. The query is firstly decomposed into sub-queries that foundational models could answer. Then, for each
step, a suitable model like LLM, VLM like CLIP or ALM like Whisper was selected and the previous step’s output

20


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

is combined with the current sub-query into a suitable hudrated sub-query and sent the selected foundation model to
for generating the response. Here, VLM utilized CLIP for image-text pair and ALM like Whisper to transform audio
into transciption. During this iteration, it may include "closed-loop" feedback to verify previous steps. This modular
prompting approach leverages complementary knowledge across modalities without finetuning. In MM-ReAct [104],
a LLM is combined with vision experts to conduct multi-modal complex reasoning tasks. To begin with, a query is
broken down into sub-queries by the LLM. If action is needed to extract relevant information, different vision tools will
be called iteratively via prompts using file paths and watchwords. Lastly, the extracted information, i.e., observation
will be sent back to LLM for further reasoning and final answer generation. In [105], Duty-Distinct Chain-of-Thought
(DDCoT) decomposes multi-modal query into sub-queries and explicitly separates visual recognition from textual
reasoning. Sub-queries that cannot be answered without the image are tagged as Uncertain via negative-space prompting
which encourage LLM not to answer if not confident and sent to visual model to obtain visual complements. Then, the
LLM performs joint reasoning over the query, generated rationales, and visual complements while being prompted to
critically evaluate and select valid information.

2.5.2 Image Modification

To further enhance multi-modal reasoning, recent studies have begun to move beyond directly feeding unprocessed
multi-modal inputs into LLMs, focusing instead on transforming or augmenting visual inputs to improve the model’s
ability to ground and interpret image features. In Set-of-Mark (SoM) [106], a image was firstly segmented into different
meaningful regions using external segmentation model, and each region is added with an identifiable mark to form a
new image as shown in part (b) of Figure[I] This marked image, together with either plain or interleaved text prompts,
is fed into GPT-4V. By explicitly associating regions with “speakable” marks, SoM unleashes GPT-4V’s emergent
visual grounding ability. In [107], the authors propose visual prompt engineering for VLLM by directly editing images
instead of only manipulating text. To begin with, an object detection model is applied to detect a set of candidate region
with bounding boxes and red circles are added within each bounding box to mark the objects to generate different
images with each image having one object stressed. Lastly, a CLIP model will be utilized to find the object that has
the largest score, i.e., most relevant to the query. In Visual Sketchpad [108], the authors propose to equips MLLMs
with a visual sketchpad, enabling them to draw intermediate sketches (e.g., lines, boxes, masks, plots) as part of their
reasoning process to improve performances on math and vision tasks. In each step, the multi-modal LLM is utilized to
generate the next step planning based on the query, previous text and visual reasoning, then synthesize Python code to
plot new figures or utilize vision models to detect or segment objects and lastly the updated figure will be sent back to
the multi-modal LLM for the next step iteration.

While the aforementioned methods perform a single round of image modification before reasoning, subsequent research
has explored iterative image refinement to progressively improve performance. In prompting with iterative visual
optimization (PIVOT) [109], the MLLM is shown with candidate actions to choose from in a modified image iteratively
to improve robots’ performance on choosing suitable actions. In each iteration, multiple candidate actions were sampled
and they were then labeled on the original image and sent to MLLM to choose the most potential action. After that, the
selected regions are focused to sample more actions in this more fine-grained regions to start the next iteration. After
several iterations, the final action will be selected and executed by the robot.

The previous works focus on image modification on 2D domain, while the following work aims at estimating 3D
distances and relationships based on modification on 2D images. In 3DAxiesPrompts [110], a 3D Cartesian coordinate
was applied to the image through 1. origin determination, 2. axis orientation based on object’s natural orientation
and 3. scale marking by adding tick marks and numerical labels along each axis. Then, the modified image is
combined with user query and send to LLM for answer generation to handle tasks such as measuring object dimensions,
2D-to-3D point reconstruction, 2D-to-3D point matching, and 3D object detection. In [II], the authors propose
SCAFFOLD prompting, a simple visual prompting scheme to enhance vision-language coordination in multi-modal
LLMs. SCAFFOLD overlays a dot matrix on the input image, with each dot labeled by Cartesian coordinates (2D for
single images, extended to 3D for image sequences). These coordinates serve as visual anchors to reference positions
explicitly and are also included in the textual prompt for reasoning about spatial and compositional relationships.

2.6 Model Ensemble

In this section, we discuss two approaches: model ensemble and model merging. In model ensemble, the output
distributions of different models are combined with weights to improve performance on downstream tasks. In
comparison, model merging merges different models of the same architecture. Model merging is not directly related to
inference-time scaling. However, it is a counterpart of model ensemble and should be introduced.

21


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

2.6.1 Different LLM Architectures: Token-Level Ensemble

Token Vocabulary Alignment

The following works focus on ensembling models of different architectures by aligning their token vocabularies. In
112], the vocabulary of different LLMs are matched through minED and the probability for next token across different
LLMs is merged through either 1. Minimum Cross-Entropy (MinCE), i.e., for each token, selecting the probability
distribution from the source model that had the lowest cross-entropy loss or 2. Average by Cross-Entropy (AvgCE), i.e.,
taking a weighted average of all source models’ distributions, weighted by their confidence which is the inverse of cross
entropy. Eventually, the combined response and probability was utilized for better performance in fine-tuning a model.
In Pack of LLMs [113], a set of LLMs was chosen, and their token vocabulary was aligned to a specific vocabulary
through minimum edit distance (MinED). MinED is the minimum number of single-character edits (insertions, deletions,
or substitutions) required to change one string into another. Then, the prompt are sent to the set of LLMs to compute
perplexity which measures the confidence of a specific LLM on this query. Next, the weights to combine these LLMs are
generated. Two methodologies were proposed: 1. simple PackLLM and 2. optimized PackLLM. In simple PackLLM,
the weights were computed through softmax based on the perplexity of the prompt. In optimized PackLLM, it is realized
iteratively by first combining with weights the two most reliable LLMs and then combining them to minimize perplexity,
continuing with the remaining LLMs. Lastly, in inference, the probability of each token is a weighted combination
of the pack of LLMs and it iterates until the termination condition. In [114], the authors propose EVA (Ensemble via
Vocabulary Alignment), a method to ensemble LLMs with different vocabularies at a fine-grained token-level during the
generation process. Based on the embedding of the shared tokens across different LLMs, a linear transformation matrix
is constructed and some noise reduction and variance reduction techniques are applied to build the transition dictionary.
In the inference stage, the set of LLMs predicts the probability of the next token and a filtering technique is applied
where if a model’s top-1 probability token is not in the top-n tokens of any other LLM, this LLM will be filtered out and
the remaining LLMs’ next token probabilities are averaged to predict the token. Cool-Fusion is a training-free
approach to fuse heterogeneous LLMs with different vocabularies through 1. fine-grained fusion and 2. coarse-grained
fusion. In each iteration of fine-grained fusion, every LLM generates a candidate text segment—ceither the shortest
decodable unit or an aligned segment that is decoded by all models. All candidate segments are then evaluated by every
LLM using perplexity, and the one with the lowest average perplexity is selected to extend the generation. This process
alternates between generation and evaluation until the completion of fine-grained fusion. Lastly, each LLM generates a
separate response and combine with the fine-grained fusion response for response-level reranking by perplexity.

Token Probability Ensemble

The following works focus on ensembling the next token prediction probabilities of different LLMs. In DEXPERTS

116], a new decoding methodology was proposed utilizing three models: 1. a large base model M, 2. a small fine-tuned
model with specific domain knowledge m-+ and 3. a small anti-fine-tuned model with anti-domain knowledge m—
where our desire lies in injecting the knowledge difference between m-+ and m-— to improve the performance of M/.
Given the prefix context, the probability of M, m+ and m— were Py,(y|x), P;(y|x) and P,, (y|x) respectively. Then,
the modified probability was

P(yl|x) = Pu(yla) + a x (Pa (y\x) — Pr (y\a)) (6)

Using this modified probability for iteratively decoding the next token derives the final response. This product-of-experts
formulation ensures tokens are favored when they are likely under the expert but unlikely under the anti-expert. In
[117], a simple, provably effective method was proposed to purify LLMs from negative effects of uncurated training
data—copyright infringement, data poisoning, and personally identifiable information (PII) leakage—by ensembling
them with a small, benign language model at the logit level using the scaled logits ensemble, i.c., CP — Ax yz algorithm.

2.6.2 Different LLM Architectures: Response-Level Ensemble

The following works focus on ensembling LLMs of different architectures by comparing and combining their individual
responses. In LLM-Blender [118], a two-stage framework was proposed to combine the responses from multiple LLMs.
To begin with, given a set of LLMs, each LLM generates a response to the query, and a pairwise ranker, PAIRRANKER,
is trained to predict the winner. Given N responses, N? comparisons were conducted to build a comparison matrix and
the methodology of MaxLogits was applied to derive the final ranking of these responses. Next, the query and the top-k
responses were sent toa LLM, GENFUSER, for response improvement. In [119], the authors investigate ensembling
strategies for LLM-based machine translation by combining translation candidates generated from multiple LLMs.
They explore multiple strategies for generating translation candidates and aggregating them into a final translation via
methods including quality-based reranking, ChooseBest, GenerateBest, and Minimum Bayes Risk (MBR) decoding.
MBR decoding, which selects the hypothesis with the highest average similarity to all other candidates, is shown to be
the most effective.

22


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Moving beyond direct response comparison, another line of research ensembles models at the knowledge level,
integrating domain-specific expertise into a base LLM through retrieval and selection. In Knowledge card [120], the
authors propose a modular framework to fill LLMs’ knowledge gaps by integrating specialized domain-specific LMs
called knowledge cards. Each card serves as a parametric repository trained on targeted corpora, and at inference time,
selected cards generate background documents for the base LLM. To ensure quality, 1. relevance selector, 2. pruning
selector and 3. factuality selector are utilized to filter and condense the knowledge. Lastly, two different methodologies
were proposed 1. bottom-up approach and 2. top-down approach for knowledge integration. In the bottom-up approach,
all the knowledge cards were utilized to generate relevant documents and all the integrated knowledge were sent to
LLM for answer generation. In comparison, in the top-down approach, the LLM is firstly asked if it needs external
information. If no external information is needed, LLM can directly generate the answer. Otherwise, the most relevant
knowledge card is selected by either 1. automatic selection or 2. explicit selection. Then, the selected knowledge card
will generate new relevant document and another round of iteration starts until LLM thinks that it can generate the
response without further information.

Finally, some approaches aim to learn an explicit fusion model that adaptively combines multiple expert LLMs with
complementary strengths. In [121], the authors propose the fusion of experts (FoE) framework, which learns a fuser to
combine outputs of multiple expert models with complementary domain knowledge in both classification and generative
tasks. In inference, it is realized in standard FoE or FrugalFoE. In standard FoE, all Kv experts compute the final layer’s
embedding and they are concatenated to send to the FoE for final generation. In FrugalFoE, it starts with the most
promising expert and utilizes KNN on validation to estimate the benefit of incorporating the next most promising expert
over the cost. This iteration continues until the termination of the incorporating the next expert. The FrugalFoE can
reduce the number of experts queried while maintaining high accuracy.

2.6.3 Same LLM Architectures Ensemble

Models Merge into One Dense Model

The most straightforward way to merge models of the same LLM architecture is through direct parameter averaging

122]. In Ensemble of Averages [123], the authors propose using a simple moving average (SMA) of model parameters
to stabilize performance and improve correlation between validation and test accuracy. Building on this, they introduce
an ensemble of average models (EoA), which outperforms traditional ensembles of raw models. Their approach is
hyperparameter-free and computationally efficient. In model soups [124], the models derived from the combination
of different hyperparameters were combined rather than selecting the best from them. Three merging methods were
proposed. The first method, uniform soup took average of these models. The second method, greedy soup firstly sorted
these models based on the performances on the validation set and then iteratively merge the next best model with
the previous selected models as long as the merged model did not deteriorate the performance on the validation set.
The final method, learned soup, optimizes the mixing coefficients and the temperature scaling parameter to combine
multiple models on a validation set using AdamW. Similar ideas are found in Diverse Weight Averaging (DiWA)
to average models derived from different hyperparameters, where the authors showed that model weight averaging can
reduce variance in out-of-distribution (OOD) generalization through bias—variance—covariance-locality decomposition.

Beyond simple averaging, structured merging frameworks explicitly model and resolve parameter interference between
different models. In TIES-MERGING [126], a three-step framework was proposed to merge multiple task-specific
models. To begin with, Trim: the changes of the each LLM’s parameters are calculated and only changes larger than a
specific threshold were maintained while the others were set back to 0. Then, Elect Sign: by summing up different
models, each parameter could move in the positive or the negative direction and this will be recorded. Lastly, Merge:
for each parameter, if the changes in each LLM is in the same direction as the recorded direction, the changes will
be averaged and updated. In FUSECHAT [127], a set of K LLMs are selected and one of them will be utilized as
reference and the remaining fy — 1 LLMs will be utilized for distillation where the knowledge of the original LLM will
be transferred to the reference LLM. Then, the obtained A LLMs utilizing the architecture of the reference LLM will
be merged to obtain the final merged LLM through SCE, i.e., select, calculate and erase. In the "select", the top 7%
parameters that have been changed will be recorded. Then in "calculate", the merging weights are calculated based
on the magnitude of squared changed magnitude. Lastly, in "erase", if two LLMs change in different directions, the
one with smaller magnitude will be erased to avoid cancellation and noise. In RegMean [128], the authors address the
challenge of dataless knowledge fusion, where multiple fine-tuned language models are available but their training
data is private. The new merged model was derived by 1. directly averaging the non-linear layers and 2. using

-1
Wu = (oh G:) (mi GiW;) for weighted summation in linear layers where K is the number of models,

G; is the Gram matrix for model 7, and W;; is its weight matrix. This formulation minimizes prediction differences
between merged and individual models without requiring training data.

23


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Models Merge into One MoE Model

To combine the strengths of multiple domain-specific models into a unified framework, several works have explored
merging dense models into a single MoE architecture. In Branch-Train-Mix [129], a dense model is copied multiple
times, each copy is fine-tuned on domain-specific data and finally merged as a sparse MoE model , where the feedforward
sub-layers serve as experts while the remaining parameters, i.e., self-attention are averaged and shared acorss experts.
Lastly, a router is initialized and then fine-tuned to distribute different queries to different experts. In inference, the
derived MoE model will follow the inference process of a normal MoE model. In [130], ULTRAFUSER was proposed
to fuse three LLMs focusing on text, code and math by training and utilizing a token-level gating model. In inference,
for each token, all three LLMs are activated which is different from MoE. The gating model will output a score between
0 and | to combine the token prediction from these three different LLMs.

Building on this idea of integrating model specialization across different domains while maintaining inference efficiency,

131] introduced Cluster-Branch-Train-Merge (c-BTM). The method first clusters documents into k clusters, then trains
k copies of the model on the data within each cluster. Eventually, in inference, the top-k similar clusters were searched
by TF-IDF and weighted-ensembled based on context-cluster similarity for final answer generation.

LLM with PEFT The following works focus on improving LLM’s performances on multiple PEFT adapters. In
AdaMix [132], different adapter were trained by randomly routing and training a batch of data. During training,
input batches are stochastically routed through different adaptation modules, and consistency regularization via KL
divergence is applied to stabilize learning. To keep inference efficient, the multiple modules are merged into a single
adaptation per layer, matching the cost of the underlying PEFT method. In [133], the authors propose a training-free
method to compose parameter-efficient finetuning modules like LoRA through linear arithmetic in parameter space.
They define two basic operators — addition (to aggregate skills) and negation (to retract skills) — and show that these
are composed to create new modules for tasks such as distribution generalization, multi-task learning, unlearning, and
domain transfer. In LoraHub [134], LoRA adapters trained on diverse upstream tasks are dynamically composed via
element-wise weighted combination to enable efficient cross-task generalization. For unseen tasks, few-shot examples
guide coefficient optimization using gradient-free methods like Covariance Matrix Adaptive Evolution Strategies
(CMA-ES), requiring no additional parameters or gradients.

3 Input: RAG

RAG Background

Prior to the era of retrieval augmented generation (RAG), there was some work focused on Q&A with retrieval and
generative models. REALM included the retrieval as an augment process for the pretraining of LLM. In [136], a
hybrid retriever, combining a sparse retriever (BM25 [137]) and a dense retriever (DPR [138]), was utilized to retrieve
relevant documents to answer the question. Afterward, the question and retrieved materials were combined into an
encoder-decoder seq2seq model. The encoder extracted information from different materials and concatenated before
being fed into the decoder for question answering. This workflow is nearly the same as the latest RAG, except that the
encoder-decoder seq2seq model was replaced by a decoder-only LLM.

RAG Workflow: Sequence and Token

The name of RAG comes from [139] as shown in Figure[12] This paper proposed a sequence-level RAG model as
shown in Eq. [7Jand a token-level RAG model as shown in Eq.

N
PRAG-Sequence (Y|X) y S- Pn (z|2) . [ [vot | Z,z, Yisi—1) (7)
z€top-K(p(-|x)) i=1
N
prac-toren(yle) “TT = S$) pg(zle) - po(yi | @, 2, yri-1) (8)

i=1 z€top-K(p(-|x))

Here, x refers to the user’s query, and y refers to the responses generated by the LLM. z refers to retrieved documents, and
they are retrieved from different retriever like BERT [140], SentenceBERT [141], DPR [138], E5 [142], Deberta (143).
Pn (z|x) referr to the probability of the retriever to retrieve this document z based on the query x and po(y: | v, 2, y14—-1)
represents the probability of the generator generating the z-th token based on the prompt 2, the retrieved documents z
and previous generated tokens y;.;_1. The probability of RAG-sequence generating a specific response is based on
summing up the probability of generating this response over each single document. By contrast, RAG-token generates
each token by summing the probabilities over different documents and iterates for all tokens.

24


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Define "middle ear" (x) < The middle ear includes

End-to-End Backprop through q and pp the tympanae cavity and

Question Answering: the three ossicles. (y)

Question Que! - ; ee
a man Retriever p, eee Generator pg Sa
>
ncoder (Non-Parametric) po (Parametric)
Barack Obama was d(z) to
born in Hawaii. (x) q(x) SUBROEES MY:
Fact Verification: Fact Query Margin- Fact Verification:

Label Generation
The Divine

This 14th century work
Comedy (x)

is divided into 3
sections: "Inferno",
"Purgatorio" &
"Paradiso" (y)

Jeopardy Question
Generation:
Answer Query

Question Generation

Figure 12: The initial pipeline in the RAG paper

RAG with Beam Search

When decoding with beam search, RAG-token is directly transformed into maximizing each token’s probability, i.e.,
ecw K@@) Pn(z|z) + po(ys | ©, 2%, Yi—1). On the contrary, RAG-sequence cannot solve this problem directly.
Instead, RAG-sequence proposes 1. Thorough Decoding (Exact Marginalization) and 2. Fast Decoding (Approximate
Marginalization) to generate multiple responses with beam search on each retrieved single document and choose the
final response with the maximum probability.

In practice, RAG-token is more frequently utilized where given a query, multiple chunks of different documents are
collected and combined into a hydrated prompt and sent to LLM for generation, where the attention scores in the
transformer layers serve as the role of weight combination on different documents and change dynamically for each
token. Another work similar to RAG from that period is found in [144].

In this section, we will review RAG from seven perspectives: 1. Query Expansion, 2. Data, 3. Retriever and Reranker,
4, LLM Generation, 5. RAG with Tools and Rules, 6. Multi-Modal RAG, 7. Collaborative RAG.

3.1 Query Expansion

For Q&A in RAG, the initial query provided might not have conveyed all the required information, so it might not have
been able to make full use of the power of the LLM. To solve this problem, two types of methodologies have been
proposed to expand the query. The first methodology directly enriched the query by the LLM. The second methodology
augmented the query with a generated document or draft answer to help retrieve relevant materials.

3.1.1 Query Expansion by Rewriting
Query Rewriting: Text

The following papers directly enrich the query, especially when it is ambiguous, to facilitate the retrieval process and
final answer generation in RAG. In (145], RAG-Fusion generated multiple rephrased or related queries, and multiple
documents are retrieved based on each individual query as shown in part (a) of Figure[13] For each retrieved document,
it was retrieved by multiple queries at different ranks, and Reciprocal Rank Fusion (RRF) is utilized to assign reciprocal
scores to each document from multiple query results as shown in Eq. [9] where X refers to the set of query, x refers
to one single query of X and z refers to a specific retrieved documents and k is a smoothing parameter. Based on
RkRFscore, the retrieved documents are reranked and the top-K documents are selected to send to LLM for final answer
generation.

1
RRF “) = 2, eyreake 1D)
score(z | X) ps k + rank(z | x) i.

In Diverse Multi-Query Rewriting (DMQR)-RAG [146], the authors proposed that the diversity of the rewritten queries
in RAG-Fusion was low, and they proposed four rewriting strategies: 1. General Query Rewriting (GQR) to denoise and
refine the query without losing information, 2. Keyword Rewriting (A WR) to extract key nouns/subjects and facilitate
searching queries on search-engine, 3. Pseudo-Answer Rewriting (PAR) to use the LLM to generate a “fake” answer
and 4. Core Content Extraction (CCE) to strip away unnecessary details, keeping only the essential parts. In addition,

25


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

LLM generates n _ @. generates n queries

Query 1 Query 2 ome Query n write a passage to answer the question Tia ae
( Ge ) | ry ) ( 2 ) ities $ How wisdom teeth are removed...
how long does it take to remove Some ... a few minutes, whereas
Vector Search | __Yedtor Search Neckar Search wisdom tooth aN H y DE Itusuall takes between 30 others can take 20 minutes or
for m relevant minutes and two hours to longer...
documents Socuments Soca remove a wisdom tooth... =

write a scientific paper passage to answer
a 1 (Query Query n
( <2te3,., ) auras {documents} the question

How has the COVID-19 pandemic impacted

neva mental health?
‘eran! Rerank
with rf ‘eS with rf i

«depression and anxiety had ) . ... two studies investigating
jieased by 20% since the Contriever COVID-19 patients ... significantly
start of the pandemic... J \ higher level of depressive ...

2120] BS AS 7S F

write @ passage in Korean to answer the BOOP EY LEEHHICY. NY SSASASTATE SO i
question in detail Ee OPAEA7| 98 1409 Ho
2122 OF BB ASSET tale] gat wae Be
Send n queries
Seamer instruction query = _) generated document real document
(a). Query rewriting for document retrieval (b). Query enrichment by Hypothetical Document Embeddings (HyDE)

Figure 13: Query expansion: (a). rewrite the query to different versions and extract different corresponding documents;
(b). use the query to generate HyDE to enhance retrieval

adaptive rewriting strategy selection was applied to prompt LLM with few-shot examples to select suitable strategies to
generate multiple rewritten prompts to improve efficiency. In Diversify-verify-adapt (DIVA) [147], diverse queries,
pseudo-interpretations are generated and their relevant documents are retrieved and top relevant documents are selected.
Next, LLM is utilized to verify the usefulness of the retrieved documents on answering the query as a classifier: 1.
"Useful", 2. "PartialUseful" and 3. "Useless". If it is classified as "Useful" or "PartialUseful", the retrieved documents
were utilized for generation. Otherwise, no retrieved document will be utilized and LLM will directly generate the
response. In Insight-RAG [148], insight identifier detects critical question and insights from the query and insight miner
retrieves the passages that address these insights before being sent to LLM for final response generation.

Query Rewriting: Embedding

The previous works directly rewrite the query, while another direction applies different intermediate embeddings of
LLM as new query embeddings to represent different aspects of the original query. In [149], Multi-Head RAG (MRAG)
utilizes each head of embedding in the LLM’s last attention layer to retrieve documents. For each document in each
head, a relevance score is computed based on the cosine similarity of embeddings combined with the ranking selection
through reranker. For each document, the scores in different heads are weighted aggregated and reranked to produce a
final relevance score for each document. Finally, the top-k document chunks are selected and passed to the LLM for the
final response generation.

3.1.2. Query Expansion by Generation

Query Expansion by Hypothetical Document Generation

The other category of methods generate hypothetical documents or draft answer to augment the query for retrieving
relevant documents. In [150], the authors proposed Hypothetical Document Embeddings (HyDE) as shown in part (b)
of Figure[13}

1. Document Generation: leverages a LLM to create multiple hypothetical documents tailored to address the
user’s query accurately.

2. Retrieval: calculates embeddings for each generated document and computes their average to enhance retrieval
precision.

3. Response Generation: utilizes the LLM to produce a concise and accurate final response based on the retrieved
documents.

Even though the generated hypothetical document may have incorrect information, it captures the relevance pattern for
the correct answer.

Query2Doc also utilizes the hypothetical document generation idea with few-shot examples to enrich the query for
retireval process. In particular, for sparser retirever, the initial query is repeated multiple times 2 = concat(x x n,d )

26


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

while for the dense retriever, the query is not repeated and separated with the generated document through a "[SEP]"
token «+ = concat(a,[SEP},d') where d’ refers to the generated documents. In {152}, the authors compared
Query2Doc, Query2Expansion and CoT. Query2Doc utilizes the LLM to generate documents and Query2Expansion
generates expansion terms similar to Pseudo-Relevance Feedback (PRF) [153], while CoT generates chain-of-thought
reasoning and final answer to break the query down and provide expansion terms. For Query2Doc, Query2Expansion
and CoT, different settings including zero-shot, few-shot and PRF are tested. Their results showed that CoT had the
best performance compared with Query2Doc and Query2Expansion.

Query Expansion by Draft Answer Generation Previous works focus on query expansion by hypothetical document
generation, while the following works focus on query expansion by draft answer or initial summaries. In Language
Model as Retriever (LameR) [154], given the query and an initial set of retrieved documents, an LLM first generates a
draft answer. This draft answer is then used to augment the query, enabling a second round of document retrieval. In
(155], the authors iterated between 1. RAG and 2. generation-augmented retrieval (GAR) to generate better responses.
In FoRAG [156], a two-stage framework is applied for 1. outline generation to ensure coherent and multifaceted
responses and 2. answer expansion based on outline to enhance the factuality and logicality of web-search enhanced
long-form Q&A. In RAT [157], an initial zero-shot CoT response is generated based on the query. For each step in
the second round, a new retrieval sub-query is generated based on 1. the query, 2. previous reasoning steps in the
second round and 3. the current reasoning step from the zero-shot CoT response in the first round. Next, relevant
documents will be retrieved based on the retrieval sub-query in the second round and eventually the current reasoning
step in the first round will be revised. After revising all the steps in the first round, the final answer will be generated.
In [158], the authors improved query expansion and document retrieval by incorporating metadata-guided synthetic
Q&A pairs and meta knowledge (MK) summaries through a prepare—rewrite—retrieve—read (PR3) pipeline. To begin
with, different documents were classified to predefined categories, and Claude was utilized to extract Q&A pairs from
these documents. Then, MK summaries are generated for metadata-based clusters of QA pairs. When a query comes,
relevant MK summaries are retrieved to augment the query into multiple more-specific sub-queries. The augmented
sub-queries are applied to retrieve relevant Q&A pairs and the original document titles. Eventually, the original query,
augmented sub-queries, and retrieved Q&A pairs are provided to the LLM for answer generation.

The O1 embedder introduces a special token <emb> that allows LLMs to not only embed queries but also
generate candidate reasoning paths. The embedder is jointly fine-tuned with contrastive loss for retrieval and next-token
prediction for supervised fine-tuning (SFT), enabling it to align reasoning with embedding quality. At inference,
the query is expanded into multiple reasoning chains, each paired with the original query and re-encoded by the O1
embedder, where the resulting embeddings are aggregated—via average pooling or voting—into a single representation
for retrieval. The enriched query embedding is then passed into the standard RAG pipeline.

3.2 Data

In RAG, the chunking and storage of the data play an important role. Smart chunking and storage strategies will greatly
facilitate RAG. In addition, the previous RAG focuses on unstructured data, i.e., text, while other types of structured
data like graph, table and tree deserve careful research. The following works deal with these problems.

3.2.1. Chunking and Storage Strategies

Adaptive Chunking For RAG with static documents, the chunking and storage of these documents in the database
deserve some consideration. The most frequently utilized way to store documents is to split them into chunks, and
use the embedding of each chunk as the key and the chunk as the value. To avoid the information loss, there is
overlap between each chunk. However, random chunking will cause information loss and performance degradation. In
ChunkRAG [160], variable-length semantic chunking strategies were utilized to alleviate this problem. To be more
specific, a two-stage approach is proposed including |. variable-length semantic chunking and 2. advanced filtering
to reduce hallucination and irrelevant responses in RAG. In variable-length semantic chunking, documents were split
into sentences using the NLTK library, the consecutive sentences with large embedding similarity were concatenated
to form variable-length chunks with dynamic greedy aggregation and lastly the embeddings of the variable-length
chunks were stored in the database. Then, hybrid retrieval strategy with advanced filtering was applied by 1. query
rewriting using GPT4o, 2. initial retrieval with term frequency-inverse document frequency (TF-IDF) and embedding
cosine similarity,redundancy removal and removal of irrelevant content through LLM scoring, self-reflection and critic
model with dynamic threshold, 3. hybrid retrieval by BM25 and LLM scoring, 4. coherence rerank to deal with the
"lost-in-the-middle" problem. Eventually, LLM generated responses based on the selected documents.

Map Chunking back to Documents Another way of dealing with information loss in chunking stores the chunks with
the mapping back to the whole documents. In [161], LongRAG was proposed to address “lost-in-the-middle” issues in
generation [162] and retrieval noise/irrelevance using dual perspectives: global (whole document) and local (chunk),

27


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

linked through a mapping from chunk to document. LongRAG consists of four components: (1) a hybrid retriever, (2)
an LLM-augmented information extractor, (3) a CoT-guided filter, and (4) an LLM-augmented generator. The hybrid
retriever performs coarse retrieval via a dual encoder and fine-grained retrieval via a cross-encoder. The information
extractor maps retrieved chunks back to their documents, where an LLM derives global information. The CoT-guided
filter classifies chunks as supported or unsupported for the query, combining supported ones as local information.
Finally, global and local information are fused to generate the response.

Store and Compute the Whole Database In another extreme, when the database is small enough, the whole database
is loaded in memory and its corresponding KV cache is pre-computed and re-utilized to avoid the retrival process
and speed the whole process up. In [163], Cache-Augmented Generation (CAG) was proposed to replace RAG for a
medium knowledge base to reduce latency and avoid retrieval error. The whole knowledge base is preloaded into the
cache through precomputation. During inference, the user’s query is processed conditioned on this cached knowledge,
enabling the model to generate responses without additional retrieval. After generation, the KV cache is efficiently reset
for subsequent queries. A potential problem with CAG stems from noise or distraction when utilizing the database for
answer generation.

3.2.2 Graph RAG
GraphRAG

RAG excels at answering local questions by retrieving a small set of relevant documents, but it struggles with global
questions like summarization (e.g., “What are the main themes in the dataset?”’), since the documents are splitted into
chunks, retrieval alone cannot capture and synthesize the full breadth of information across the entire corpus and the
sequence information of these chunks are lost. To solve these problems, GraphRAG has been proposed as shown
in part (a) of Figure[14] It includes two-stages: "Indexing Time" and "Query Time".

In the "Indexing Time", it is composed of the following stages:

1. Chunking and Extraction: Source documents are divided into chunks, from which entities (nodes, \V),
relationships (edges, /’), and associated claims are extracted.

2. Knowledge Graph Construction: Entities and relationships form a graph as triplets (s, 7, 0) (subject, relation-
ship, object).

3. Community Detection: The Leiden algorithm [165] partitions the graph into sub-graphs (communities) for
scalable summarization.

4. Summary Generation: Summaries are created bottom-up. Leaf-level summaries prioritize key nodes, edges,
and claims within token limits. Higher-level community summaries use sub-community summaries if token
limits are exceeded, ensuring scalability.

In the "Query Time", it is composed of the following stages:

1. Query Processing: Community summaries are shuffled, divided into chunks, and combined with the query for
LLM evaluation of helpfulness.

2. Reranking and Selection: Chunk summaries are reranked by helpfulness score and iteratively selected until the
token limit is reached.

3. Answer Generation: Selected chunk summaries are sent to the LLM to produce the final global answer.

Modification on GraphRAG

Later, Some modification works have been done based on GraphRAG with minor differences on graph construction,
community detection and LLM reasoning. In CommunityKG-RAG [166], the community is detected by Louvain
algorithm and the embedding of each community is the average of each node’s embedding within the sub-graph
extracted from the BERT encoder [140]. In HippoRAG and [169], a knowledge graph and a passage—node
incidence matrix are constructed through offline indexing. Then, when a query comes, seed entities are extracted and an
online relevant entity search is conducted through Personalized PageRank (PPR) where node specialty is applied to
modify the weights based on inverse document frequency-like signals for question answering. In addition, hierarchical
information is utilized to improve GraphRAG. In tree-RAG (T-RAG) [170], An entity hierarchy tree structure was
constructed to represent organizational hierarchies. During inference, if entity is detected from the query, its hierarchical
information will be extracted from the tree. If the hierarchical information has been detected, it along with retrieved
documents will be sent to LLM for answer generation. GraphRAG can also run together with traditional RAG to obtain
more detailed information. In HybridRAG {171}, the traditional RAG, i.e., VectorRAG is combined with GraphRAG

28


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

(SY an) A. Tree Traversal Retrieval =
Source Documents Global Answer SS =
a Pee! _— Query el e _ _ oe Query — > Answer
text extraction query-focused see ENcoder Cc) —_— — nee wre Sa
5 Retrieved Context
yand chunking summarization =
EN (oo —_
Text Chunks Community Answers Tree Structure
Se ET TENET B. Collapsed Tree Retrieval
domain-tailore query-focused
ysummarization summarization on > |} OOO Do OHOOOO- ee er
Entities & Relationships Community Summaries — Betiever come
SSE |
domain-tailored + domain-tailored (b). RAPTOR: RAG over hierarchical tree for long documents
y summarization community summarization Tabular Query Expansion Schema & Cell Retrieval
(rn detection (7. ~*Y (caida: ap Tenth eadaeuecswncasasteecwecaaseeesees,
Knowledge Gragh =}+————>|_ Graph Communities a) >
) Schema Query 1 ! rogram- ay.
Question “5 mg oe Aided LM newer
5 " eae Qi v
Indexing Time Pipeline Stage Query Time CellQueryt J2]3= | cee Relevant
: (Col-Cell Pairs) Schema & Cells;
Table H

(a). GraphRAG: "Indexing Time" and "Query Time" fe

(c). TableRAG: RAG over table data format

Figure 14: Data: (a). GraphRAG: "Indexing Time" and "Query Time", (b). RAPTOR: transforms long documents into
tree with summary and answers query based on the tree, (c). TableRAG: retrieves relevant contents from table and
utilizes the content for answer generation

to retrieve semantically relevant text chunks and structured relationship-aware sub-graphs of financial data and they
are combined and sent to LLM for generation. To accelerate GraphRAG, LightRAG retrieves only relevant
entities and relations via vector search, instead of retrieving large text chunks or doing heavy graph traversal. During
inference, LightRAG first extracts local keywords (for entity-level details) and global keywords (for broader themes)
from the query. It then uses a vector database to match local keywords with candidate entities and global keywords with
higher-level relations. Finally, it incorporates high-order relatedness by retrieving one-hop neighboring nodes and edges
from the graph, enriching the results with structural context before sending to LLM for answer generation.

Knowledge Graph Construction

There have been some works to improve the quality of the constructed knowledge graph to reduce repetition and
error, utilize information embedded within dataset and update with the latest knowledge or relationship extraction.
In iText2KG [173], the authors propose a method to construct a knowledge graph to extract entity and relationship
progressively while ensuring semantic uniqueness, i.e., no overlapping or duplicate meanings. To begin with, document
distiller rewrites documents into semantic blocks in JSON. Then, incremental Entities Extractor extracts and deduplicates
entities through cosine similarity in the embedding space, where the documents are processed sequentially. Lastly,
semantic blocks and global document entities are sent to relation matcher to extract global document relations and
the extracted knowledge graph is saved in graph database Neo4j. In MedGraphRAG [174], predefined medical tags
was utilized for hierarchical clustering after the initial construction of the knowledge graph. A U-retrieval is also
proposed to move top-down to search the most relevant sub-graph and utilize that sub-graph for draft answer generation
and then move bottom-up to utilize higher level summary tags to further refine the answer. In Retrieval-Augmented
Editing (RAE) [175], incorrect and latest knowledge is updated and inserted to the existing knowledge graph. To begin
with, relevant knowledge sub-graph are selected based on maximizing mutual information and form the fact chain.
Next, uncertainity-based fact pruning, i.e., entropy is utilized to reduce redundency and sent to LLM for final answer
generation. In AgentRE [176], the authors employed a LLM as an agent to interact with three modules: 1. retrieval
module, 2. memory module and 3. extraction module to perform relation extraction (RE) in complex tasks, producing
structured triples that are used to build a knowledge graph. Firstly, retrieval module store and retrieve relatively static
knowledge and it is utilized for sample retrieval and relevant information retrieval, Then, memory module contains
shallow memory to summarize extraction result and deep memory to summarize and reflect on historical actions. Lastly,
extraction module turns text into structured triplets using ReAct style following 1. thought, 2. action and 3. observation
cycle until it has enough information to generate the final response [L]].

Iterative GraphRAG

The following works focus on enhance the reasoning capability of LLM in GraphRAG by iteratively breaking down
the query into sub-queries, retrieving relevant documents for the sub-query and generating answers based on retrieved

29


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

documents. In ChainRAG [177], it mimics human-like reasoning 1. decomposing the complex question, 2. retrieving
relevant context via sentence graph, 3. rewriting vague sub-questions and 4. synthesizing the final answer. To begin
with, the sentence graph with entity indexing is constructed. The complex question is then broken down into multiple
simpler ones, with vague pronouns such as “it” or “this” replaced by concrete terms from previous sub-queries or
answers to prevent confusion. Each sub-question is retrieved with a two-step strategy with the first level seed sentence
retrieval and the second level to explore the neighbor of the seed sentences until the context is sufficient or the maximum
context window is reached. The retrieved documents are then utilized for answering the sub-query. Finally, the
retrieved sub-documents will be reranked at sentence level and retrieved and integrate with all sub-question to generate
the final answer. In [178], think-on-graph (ToG-2) iteratively reasons between 1. context-enhanced graph search
to explore the neighborhood of the current entities and prune unrelated entities and 2. knowledge-guided context
retrieval to retrieve related documents, build context pool, rank by relevancy and select the top-k chunks during the
reasoning process to generate the final answer. After each iteration, the accumulated knowledge will be checked
whether it is enough for answering the question or conducting another round of iteration. In knowledge graph based
retrieval-augmented reasoning (KR-RAR) [179], process level reasoning capability of GraphRAG is explored. To begin
with, a process-oriented mathematical knowledge graph is constructed from PRM800 dataset to enable multi-step
reasoning.

Next, a hierarchical retrieval strategy is used where (a). step-level problem retrieval is realized by 1. filtering, 2.
semantic similarity scoring and 3. context retrieval using depth-first search (DFS) and (b). step retrieval is realized by
1. restricting search to the steps of retrieved problems, 2. semantic similarity scoring and 3. context retrieval using
breadth-first search (BFS). After every step of iteration, both step verification and end-of-reasoning detection will be
conducted and retrieved documents will be refined by LLMs until the termination condition is met. In [180], Knowledge
Augmented Generation (KAG) boosts LLM reasoning in logic, computation, and semantics via three modules: KAG-
Builder, KAG-Solver, and KAG-Model. KAG-Builder constructs a LLM-friendly knowledge framework with raw
chunks, graph information, and a knowledge layer linked via mutual indexing. KAG-Solver uses logical-form-guided
hybrid reasoning, combining LLM, knowledge, and mathematical logic reasoning iteratively. KAG-Model fine-tunes
the system to improve knowledge representation and retrieval accuracy.

Adaptive GraphRAG to Improve Performances

The following works train a separate classifier to identify the complexity of the query or the relevancy of a triplet to a
query and apply different strategies based on the classifier’s result. In flexible modular KG-RAG framework (FRAG)
[181], three modules: reasoning-aware module, flexible-retrieval module and reasoning module are included to adapt
the retrieval strategy based on the complexity of the query. Reasoning-aware module is utilized to classify the query
as simple or complex based on the estimated minimum hop count of reasoning paths in a knowledge graph through a
trained binary classifier. Flexible-retrieval module includes a preprocessing-retrieval-postprocessing pipeline where
preprocessing obtains the knowledge sub-graph, retrieval utilizes different strategies to derive reasoning paths based on
query complexity and postprocessing removes redundant and irrevalent reasoning paths through Path Ranking Model.
If the query is simple, breadth-first search will be utilized to retrieve all reasoning paths, while Dijkstra algorithm
will be applied to find the shortest reasoning path when the query is complex. Lastly, reasoning module generates the
final answer based on query and selected reasoning paths. In [182], Sub-graphRAG represents the graph as triplets,
and directional distance encoding (DDE) is utilized to compute the relationship between the query and the triplet and
eventually, the query, triple and DDE is sent to a pretrained MLP for classifying if the triple is relevant to the query
improve the efficiency and effectivenss of GraphRAG.

The retrieved knowledge sub-graph is generally not suitable for directly input to LLM for generation. The following
papers discuss how to deal with the problem to make the knowledge sub-graph more suitable for LLM. In CoTKR [183],
knowledge rewriting is incorporated to transform question-related triples into natural language that is consumed by
LLMs. When a knowledge sub-graph is retrieved, it is transformed into contextual knowledge by iterations between 1.
reasoning: decomposing the question and generating a reasoning trace and 2. summarization: summarizing the relevant
knowledge. Lastly, the reasoning trace and summaries are sent to LLM for generating answers. In GRAG [184], the
knowledge sub-graph is transformed to hierarchical text description and use GNN for embedding extraction to faciliate
its performance on LLM. To begin with, K-hop ego-graph of all nodes are extracted and based on the query, the top
K relevant sub-graphs are retrieved, merged and pruned using soft pruning using MLP-learned scaling factors based
on their distance to the query. Lastly, the merged and filtered graph will be sent to LLM for answer generation with
1. text view (hard prompt) where the graph is transformed into a hierarchical text description and 2. graph view (soft
prompt) where the GNN generates its embedding in the prompt. In GraphReader [185], knowledge graph is utilized
to handle limited context window of LLM by transforming the original long documents into knowledge graph and
corresponding atomic facts summarized by the LLM. Then, a notebook to record supporting facts and a rational plan to
identify missing information and the logic to identify them are constructed. Next, exploration are guided by the rational

30


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Straightforward Query: _ Answer
gp | Paris is the capital of what? _

Simple Query: 000 Ee, " Documents

When is the birthday Rerriev™

of Michael F. Phelps? } Answer

—_ ji
wl k times
Classifier Ee)

OM “ Complex Query: ) renee Documents
| What currency is in etrieval ,
_Billy Giles’ birthplace? y) (Intermediate)

Answers

(b). Summary and Q&A with consideration of diversity

(a). Diversity RAG: consider diversity in retrieval (b). Open-RAG: Retrieving based on query complexity

Figure 15: (a) Dvierse RAG: retrieve relevant and diverse documents; (b) The intuition behind Adaptive-RAG and
Open-RAG: determine whether to retrieve based on the complexity of the query

plan and recorded by the notebook at three levels: atomic fact for coarse information, chunks for detailed information
and neighbors until enough information is collected to answer the query.

3.2.3, Other Data Representations

In this subsection, different types of structured data—including trees, tables, and metadata—will be discussed in the
context of RAG. In Recursive Abstractive Processing for Tree-Organized Retrieval (RAPTOR) [186], the authors
enhanced RAG by structuring long documents into a hierarchical tree. The tree is built by chunking documents,
embedding through SentenceBERT, clustering through Gaussian Mixture Models (GMM) and dimension reduction
through Uniform Manifold Approximation and Projection (UMAP) and summarizing each chunk. During inference,
the relevant documents are queried through either tree traversal or collapsed tree as shown in part (b) of Figure[14]
In tree traversal, it is started from the root node and traverse from top to bottom while keeping the top-k documents.
For collapsed tree, the tree is flatten into a single layer, and the top-k item is retrieved. In [187], the authors proposed
TableRAG to improve the performances of RAG for table format. Generally, a table is too large to put into the context
window of a LLM. To begin with, tabular query expansion is applied to obtain schema queries to retrieve important
column-level information and cell queries to retrieve key cell values by prioritizing distinct and frequent values within a
cell encoding budget. With encoder, the schema queries and cell queries are transformed into embedding and the top-k
entries are selected. Lastly, a program-aided solver inspired by the ReAct framework is applied to reason and generate
executable programs—e.g., SQL or Python—by leveraging the retrieved schemas and cells for answer generation. In
188], HtmlRAG was proposed to handle long HTML contexts while maintaining their important information through
utilizing its semantic and structural information. HtmIRAG included four steps in the pipeline: 1. html cleaning to
remove unnecessary css and js information, 2. block tree construction to transfrom the html into a tree structure where
tiny nodes were merged to reduce compute cost, 3. embedding-based coarse block pruning to remove the irrevalent
nodes of the tree in the embedding space and 4. generative fine-grained pruning, where a path-aware generative model
scores fine-grained block paths to select the most relevant content. After the block tree is constructed, cleaned and
pruned, it will be tranformed back to html format and formatted as prompt to send to LLM for answer generation. In
Multi-Meta-RAG [189], metadata such as article sources and publication dates is first extracted from user queries by a
lightweight LLM using few-shot prompting. This metadata is then used as a filter during chunk retrieval in the vector
database, ensuring that only relevant sources and dates are considered before RAG.

3.3 Retrieval and Reranker
In this section, the retriever and reranker will be discussed. Given the query, how to retrieve the most relevant documents
to improve downstream questions answering will be the first question. Then, there may be some errors in the documents

or inconsistency with LLM’s internal knowledge. Lastly, is it possible to use cross-encoder to further improve the
relevancy of the retrieved documents to the query? The following sections will discuss these questions.

3.3.1 Retrieval Refinement

Retrieval Considering Diversity

31


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

In the normal retrieval process, only paying attention to the relevance between query and documents will cause
redundency and miss important materials. In DiversityRAG [190], maximal marginal relevance (MMR) and modified
farthest point sampling (mFPS) were applied to retrieve documents by balancing relevance and diversity and improve
the answer accuracy in downstream tasks as shown in part (a) of Figure[15] The formula of MMR is in Eq. [10] where k
refers to the k-th selected document, r; is the relevancy between the query and the i-th document and max,;ew cos(i, j)
measures the similarity between the current document and previously selected documents in W to contribute diversity
in a greedy manner.

Dex ry —(1—@)- ij 10
arg max | ar (1—a) nae Boal 9) (10)

Retrieval based on Prompt Complexity

Another direction of retrieval refinement is to retrieve based on the complexity of query. In Adaptive-RAG [191], the
authors proposed to utilize different retrieval strategy for different prompt complexity to balance efficiency and accuracy
as shown in part (b) of Figure[15] They trained a classifier using cross-entropy loss to classify the complexity of the
prompt. When the prompt was simple, it directly generated response without retrieval. If the prompt is moderate, it will
generate response with one time retrieval. If the prompt is really complex, it will iteratively generated responses with
multi-hop retrieval. In Open-RAG [192], [RT] or [NoRT] tokens were predicted for deciding whether to apply retrieval.
If [NoRT] is predicted, it will first generate a response without a retrieval and compute the confidence score. If the
confidence score is below a threshold, it will retrive relevant knowledge. If [RT] is predicted, relevant knowledge will be
retrieved at the beginning. In unified active retrieval (UAR) [193], the authors proposed a unified active retrieval where
active retrieval refers to automatically determine when to retrieve including 1. Self-aware classifier, 2. Time-aware
classifier, 3. Knowledge-aware classifier and 4. Intent-aware classifier. When a query came, these following steps
were executed sequentially. To begin with, intent-aware classifier check if the user has explicitly ask for retrieval, and
retrieval if true otherwise proceed. Then, the knowledge-aware classifier checks if the query requires factual knowledge.
If true , retrieve otherwise proceed to the check. Next, the time-aware classifier checks if the query is time-sensitive. If
so, retrieve otherwise proceed to the next check. Lastly, the self-aware check is conducted to check if the LLM has
already known the answer. If so, proceed otherwise retrieve.

Hierarchical Retrieval

Next, hierarchical retrievers are applied to retrieve information from different granualities like chunks and documents.
In Hierarchical RAG with rethink (HiRAG) [194], hierarchical retriever including document-level sparse retrival and
chunk-lever dense retrieval is utilized to select single candidate rather than the top-k multi-candidates to reduce the
noise of irrevalent documents and the pressure of context window limitation. HiRAG contained five components: 1.
decomposer to decompose complex question into simpler sub-questions, 2. definer to determine if current sub-answers
can solve the original query, 3. hierarchical retriever including document-level sparse retrieval and chunk-level dense
retrieval to select single candidate rather than the top-k multi-candidates, 4. verifier to check if the current retrieval is
sufficient for answer sub-questions, which will trigger rethinking if the retrieval is not sufficient and 5. summarier to
combine all sub-answers into the final answer. By the combination of hierarchical retriever, single-candidate selection
and rethinking process, HiRAG outperformed on multi-hop question answering tasks. Another work on hierarchical
retriever is in and it has been discussed in GraphRAG.

3.3.2 Retrieved Document Refinement

After retrieval, there may exist conflicts between internal and external knowledge. The conflicts between internal and
external knowledge should be resolved firstly before generating the final answer. In FilCo [195], the authors trained a
filter model /.; to extract the text span, i.e., sentence from the retrieved document and send the text span to LLM for
final answer generation. In CRAG [196], the authors proposed to correct the problem in the retrieved documents to
improve the performance of RAG. CRAG utilized a light-weight evaluator to judge the relevance score of the retrieved
document as 1. correct, 2. incorrect and 3. ambiguous. If the judgment is correct, the retrieved document will be refined
by splitting into smaller chunks, irrelevant chunks will be filtered out and the remaining chunks will be recomposed and
then be applied for output generation. If the judgment is incorrect, the retrieved document will be discarded and will rely
on web search on rewritten queries to get correct information. If the judgment is ambiguous, both document refinement
and web search on rewritten queries will be conducted for the final generation. In ASTUTE RAG [197], the authors
desired to solve the imperfect retrieval and conflicts between internal and external knowledge. To begin with, the
LLM is elicited to generate accurate, relevant and hallucination-free passages based on the query where the number of
generated messages is determined by the LLM. Then, the retrieved external knowledge, internal knowledge and source
labels are consolidated by iteratively comparing between them, identifying consistencies and conflicts, discarding
irrelevant information and regroup the remaining contents based on consistency. Eventually, the most trustworthy,

32


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

top-K
passages sort based on the log-likelinood
score over question

Evidence

re-ranked
passages

Question (q) Retriever

Re-Ranker

Passage: {pi}. Please write a question
based on this passage.

Figure 16: Rerank to select the most important document for a given query

source-creditable documents are sent to LLM for final answers generation and the best answer was further selected
based on reliability. In [198], FlexRAG was proposed to compress the retrieved context to improve the performance of
RAG on long context problem. The retrieved context is firstly sent to a compressive encoder, a middle layer of an LLM
for compatibility, to extract embedding. Then, selective compression mechanism through 1. an importance estimation
at token or sentence level and 2. an embedding down-sampling are applied to extract condensed embedding with a
flexible compression ratio. Lastly, query is tokenized, represented in embeddings and concatenated with the extracted
condensed embedding for final answer generation.

3.3.3. Reranker

The retrieval of documents assessed relevancy in a fairly coarse way—usually. Reranker improves this by using a
cross-encoder: the query and each retrieved document are jointly processed to produce a more precise relevance score
rather than cosine similarity between embedding the query and the document. In this way, the most relevant documents
rise to the top, giving a finer-grained ordering than the initial retrieval. In addition, the layout of these documents in the
context plays an important role on correctly answering the query as shown in "lost-in-th-middle" [162]. As a result,
after the retrieval of the top-K documents, it is desired to rerank these documents for downstream generation task.

In [199], each document was utilized to predict the probability of the query, and the document with the highest probability
to predict the query will be more important as shown in Figure[16] By Bayes’s law, logP(z;|x) + logP(a|z;) given a
uniform P(z;) assumption. Thus, the scoring mechanism is r(x, 2;) = al >>, logP(x:|v<, 2) which represents the
probability of LLM generating the query based on the retrieved document, and it is applied to rank the importance of
different documents before sending to LLM for generation. In RankRAG [200], a prompt "For the question question,
access whether the passage is relevant to the question." is utilized, and the probability of outputting "True" is utilized as
metric for ranking documents. Lastly, the reranked documents are sent to LLM for final answer generation. In RE-RAG
{201}, the authors enhance the RAG framework by introducing a Relevance Estimator (RE) that evaluates the relevance
of retrieved contexts to a given question. RE outputs a confidence score for each context, which is used to rerank
contexts and guide the answer generation process.

3.4 LLM Generation

In this subsection, the process that LLM utilized retrieved documents for final answer generation will be investigated
with the focus on multi-step RAG that decompose the query into multiple sub-queries and answer them iteratively.

Table 3: Summary of Important Works in RAG

Method Key Mechanism

IRCoT Interleaves retrieval and reasoning steps until final answer or maximum iterations reached.
Search-ol Agentic RAG with special tokens for query injection; employs two-stage document refinement.
SELF-RAG Fine-tuned generator with Retrieve, ISREL, ISSUP, ISUSE tokens; evaluates document
relevance and response quality.

Active RAG Forward-looking mechanism; evaluates provisional responses and triggers retrieval when
confidence is low.

33


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Q Ours: Self-reflective Retrieval-Augmented Generation (Self-RAG)
WW ie eebnty wae —_%_5 Prompt How did US states get their names? Step 1: Retrieve on demand
Lost Gravity manufactured? j
[=| ve —> US states got their names from a variety of sources. Q
cumulate docs
qeumulate docs LS Step 2: Generate segment in parallel oO [2] [3]
- P t Prompt +
The Lost Gravity was Q Prompt + @ rompt + @ L3
: ————

manufactured by Mack Rides. = By

2 e [Fem] 11 of 50 state names Texas is named [Reever Calitornia's name has its
0 _fumulate docs fone froth Persons: [Sipporea | after a Native American tribe. a a 16th-century ieee
Las Sergas de Esplandian. [ rariaty_|

Step 3: Critique outputs and select best segment
Mack Rides is a company

from Germany. (=) oo — |_| (2
= US states got their names from a variety of sources. 11 of 50
= [= 2 , —> Repeat... — states names are come from persons. 26 states are named
cumulate docs ee
| after Native Americans, including Utah.

Prompt: Write an essay of your best summer vacation
The answer is Germany. — v
y. @) ae — My best summer vacation is when my family and | embarked on a road trip along ..
(a). Multi-Step RAG (b). Self-Reflection RAG

Figure 17: (a). Multi-Step RAG to divide the query into sub-queries and answer them sequentially based on previous
sub-query’s answer, (b). Self-Reflection RAG to classify retrieved documents as relevant, irrelevant or partially

3.4.1 Multi-step RAG

In the muti-step Q&A, it is difficult to retrieve all relevant documents within one retrieval process. For example, for
the question, "Where is the author of Harry Porter born?", it needs to 1. firstly identify the author of "Harry Porter"
is J. K. Rowling and 2. then identify J. K. Rowling is born in Yate, United Kingdom. For such kind of queries, it is
supposed to firstly split the muti-step question into multiple steps during the reasoning process where each reasoning
step identify a sub-question that needs retrieval to be answered, a retrieval process is conducted based on the sub-query
and the retrieved documents are utilized to answer this sub-query and move onto the next reasoning step. In different
literature, multi-step Q&A can also be named muti-hop Q&A or muti-aspect Q&A, while the essence of these problems
are the same. The summary of some important works on multi-step RAG is in Table.

Interleaved Retrieval and Chain-of-Thought (IRCoT) [202], as shown in part (a) of Figure[I7| proposes to interleave
between

¢ Retrieval: retrieving more documents based on the previous reasoning steps.

¢ Reasoning: generating the next reasoning step.

Iterations of IRCoT continued until a final answer or the maximum number of iterations was reached. In Search-
ol [203], it iteratively included 1. agentic RAG mechanism to automatically determine when to start query and
inject knowledge with special tokens like < |begin_search_query| > query_teat < |end_search_query| > and
< |begin_search_result| > docs < |end_search_result| > and 2. reason-in-documents module through a two-
stage refinement by firstly generating an intermediate document analysis and then distilling key insights into a concise
summary to avoid noisy or verbose insertions for the reasoning process.

In previous Iterative RAG works, the decision of the retrieval query is a black box, and inner monologues RAG
(IM-RAG) makes the retrieval decision intrepetible, modular and optimizable through inner monologues. There
are four modules: |. reasoner including questioner and answerer, 2. retriever, 3. refiner and 4. progress tracker. To
begin with, a query is sent to the reasoner to evaluate if more evidence is needed, and if yes, the reasoner will play the
role of questioner to generate a query to retrieve more information from external knowledge source. Then, the retriever
will retrieve relevant knowledge from external source and the refiner will re-rank and filter more relevant documents.
Progress tacker evaluates how close the retrieved evidence supports the correct answer. Lastly, the reasoner evaluates
and reasons why the evidence is sufficient or not. If the evidence is not sufficient, it will go back to the retriever with a
new sub-query and its corresponding reasoning to retrieve more documents. Otherwise, the reasoner will play the role
of answerer to generate the final answer.

34


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Previous works divided the query into sub-queries, rewrote them into higher quality based on dense retriever’s embedding
and then utilized for the embedding for retrieval and reasoning for the next iteration. However, this workflow of tight
coupling between query rewriting and the dense retriever limits its compatibility with hybrid retrieval, impeding further
RAG performance improvements. To solve this problem, LevelRAG proposed a two-tiered search architecture
with 1. High-level searcher and 2. Low-level searchers for decoupling the query rewriting process from the retrieval
and reasoning process. The high-level searcher performs four key operations for query rewriting: 1. decompose: breaks
the complex query into simpler atomic sub-queries, 2. summarize: condenses retrieved documents into direct answers
to each sub-query, 3. verify: checks if collected information sufficiently answers the original query and 4. supplement:
adds new sub-queries if initial results are insufficient to break down the complex query into simpler ones. The low-level
searcher for hybrid retrieval contained 1. sparse searcher: conducts keyword-based retrieval with Lucene syntax for
query rewriting, 2. dense searcher: focuses on semantic similarity and 3. web Searcher: Interfaces with commercial
search engines to refine the query for optimal retrieval.

Lastly, is it possible to retrieve all relevant documents even if the query is muti-hop? Alignment-oriented retrieval
(ARM) solved this problem using a three-step methodology: indexing, alignment (information + structure), and
self-verification. The indexing process organized the data with embedding and N grams for search. In alignment, it
contained information alignment and structure alignment. In information alignment, keywords were extracted by LLM
and constrained decoding rephrased the keywords into N grams in the database by scoring them based on average logit
probability. Next, the matched keywords were utilized to retrieve relevant documents based on BM25 and embedding
similarity. For structure alignment, mixed-integer linear program (MIP) is applied to select different 1. the top-k subset
and 2. their connection to form multiple drafts for maximizing relevancy to the question and compatibility between
selected subsets. In the third process, the drafts are verified by LLM to get rid of irrelevant retrieved documents. Lastly,
beam search is conducted and the best answer is voted by weighted aggregation.

3.4.2 Self-Reflection RAG

In muti-step RAG, the query is divided into sub-query, and each sub-query is utilized to retrieve relevant documents
and sent to LLM for generation. However, there is no judgment whether each step is reasonable and the error in each
step is accumulated and eventually gets a wrong answer. In SELF-RAG, the generator is fine-tuned to generate four
types of tokens: Retrieve (decides when to retrieve), ISREL (check if retrieved documents provide useful information
to solve the query), ISSUP (check if the response is supported by the retrieved document), and ISSUE (check if
the response is useful for the query) [204]. As shown in Figure [17{b), given a query 2, the retrieved document z
and the previous responses y;, the generator firstly generates a "Retrieve" token. If "Retrieve" is no, the response
is generated directly. Otherwise, A’ documents will be retrieved and for each document, a response will generated,
i.e., AY responses in total. For each retrieved document and generated response, it will be evaluated using a score
function from the perspectives of "ISREL", "ISSUP", and "ISUSE". The best top-B of the top K responses by the
scoring function will be utilized and continue for the next step. Similar ideas are in when the similarity of the
generated response with an expert response lies below a threshold, the monitoring module will activate the evaluating
module to check if 1. the knowledge is sufficient, 2. is there any conflicting knowledge and 3. is there any erroneous
reasoning. If not enough knowledge is available, a targeted new query will be generated to retrieve again. If the
knowledge is conflicting, the correct information, either internal or external knowledge will be utilized. If the reasoning
is wrong, the reasoning process will be checked for each step to see if it is supported and give suggestions to the error
for the next round of reasoning. RetroRAG constructed a graph-based framework including two modules: 1.
evidence-collation-discovery (ELLERY) and 2. Answer to discard erroneous past evidence and include new evidence.
The ELLERY framework operates through an iterative process involving two core modules: 1. evidence collation
and 2. evidence discovery. In evidence collation, missing documents are retrieved based on prior queries, merged
with the current set of retrieved documents, and then scored by an LLM where the top-k documents are retained to
ensure relevance and quality. In evidence discovery, the LLM derives new inferential evidence based on the collected
source evidence, and the inferential envidence needs to pass question-relevance and reference-attribution checks. This
cycle continues until the Answer module can produce responses that meet self-consistency criteria, with consistency
measured by an LLM-based evaluator.

3.4.3 Multi-step RAG with Planning

In multi-step RAG, the previous works focus on decomposing the query into sub-queries and answer them step by step.
Another methodology introduces an explicit planning module and utilizes the initial plan to guide the retrieval and
generation process for the final answer. In PlanRAG [211], the LLM takes a decision-making question Q, a database
D with schema S, and business rules R as input, and generates an initial plan describing the data analyses needed for
decision making. Based on this plan, the LLM generates structured queries to retrieve relevant data from the database.
After executing these queries, the model evaluates whether the retrieved data are sufficient to answer the question. If

35


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

TE Seem cememm mm m em m em eme mem  e m  Om o m MCTS-RAG Framework
Search results: Dy LM eve ; Retriever A textile dye containing an

1

1

1

T

. Q The answer is:
Retrieved = yt" [ays fe)
documents 1 2): Search results: Dg, ( & 7
1 Pe tye.
I

light with energy of 2.3393 eV.
a
ce 00 --

What color of light is absorbed
s. Base LLM

by the organic compound?
g Finding 1: MCTS-RAG achieves better performance than baselines
Evaluation Datasets Compared Baselines

$4! Joe Biden (born November 20, 1942) is the 46th president of the United States. QA: ComplexWebQA, GPQA rStar, ReAct, Self-Ask, Self-RAG, FLARE, Iter-RetGen,
! Fact Checking: FoolMeTwice Search-O1, AutoRAG, TC-RAG, IterDRAG, DeepRAG

PL)

!
Step 2 3 ; Joe Biden attended the University of Pennsylvania, where he earned a law

: @ Finding 2: Concurrency + Pruning = Faster Tree Search
1 degree.

wie ey s
fi * 3 5 Parallel action expansions & retrieval
a s) we ) pruning greatly improve efficiency
aw aw

Parallel Expansion Retrieval Pruning

1
s! He graduated from the University of Delaware in 1965 with a Bachelor of Arts
- in history and political science.
1

1
Step3 9,1 Joe Biden announced his candidacy for the 2020 presidential election on August L
31 18, 2019, 1 g Finding 3: In-depth Analysis on Effectiveness of MCTS-RAG
! I AS Human analysis & Case Study
1
i

Q Impact of different actions MCTS-RAG demonstrates (1) enhanced external
¢ Impact of different rollout strategies knowledge utilization and (2) reduced hallucination

53} Joe Biden announced his candidacy for the 2020 presidential election on April
25, 2019.
1 25,

(a). Active RAG (b). MCTS RAG

Figure 18: (a). Active RAG: retrieve based on generation that has low probability, (b). MCTS RAG: model the generator
through MCTS with the capability of retrieval and reasoning.

not, a replanning procedure iteratively updates the plan until a confident decision is made. In Plan*RAG [212], the
original workflow of RAG is modified into a two-phase execution: 1. plan generation by decomposing the main query
into a direct acyclic graph (DAG) of sub-queries which are stored out of the context window of LLM and 2. plan
execution with retrieval where independent nodes in the DAG are executed in parallel and the answers from the parent
nodes are passed to the child nodes. Similar ideas are in [213]. In [214], the authors propose a heterogeneous recursive
planning framework for long-form writing with language models. Starting from the top-level writing goal, the planning
framework recursively decomposes the goal into three type of tasks: 1. retrieval for information gathering, 2. reasoning
for extracting information and resolving logical issues and 3. composition to generate or revise text. The planning
process interleaves with the reasoning process with the memory and state updated until all goals are reached.

Another direction research is applied planning to think ahead, i.e., guessing what LLM needs to in the future steps of
generation. In Active RAG , the authors enhance retrieval-augmented generation by introducing a forward-looking
mechanism with three key components: (1) generating provisional responses before producing the final output, (2)
evaluating the quality of these provisional responses, and (3) triggering retrieval when confidence in the provisional
responses is low as shown in Figure [18fa). To implement this idea, they propose two variants of Forward-Looking
Active Retrieval-Augmented Generation (FLARE): FLAREinstruct and FLAREdirect. FLAREinstruct makes retrieval
decisions explicitly by embedding search calls in the generated text. However, this approach can suffer from unreliable
or poorly formed queries. FLAREdirect instead adopts a more implicit strategy, leveraging the language model’s
confidence to decide whether retrieval is necessary. To begin with, an initial answer is generated without retrieval. If
the model’s probability for the next sentence is sufficiently high, the sentence is accepted directly without retrieval.
If confidence is low, the uncertain sentence—or its refined version—is converted into a query, which is then used to
retrieve supporting evidence before generating the final response.

In the previous work of multi-step RAG and multi-step RAG with planning, the performances of RAG will be improved.
However, two more questions may come into being: 1. context overload as the retrieved documents overwhelm the
LLM’s context window and 2. over-planning as continuing planning even if information is enough and redundant
planning as new sub-queries will be very similar to previous sub-qeury leading to repetition problems. To solve this
problem, the authors proposed to utilize a dual-function query-focused summarizer after retriever to record 1. global
evidence memory to record documents relevant to the initial query and 2. local pathway memory to record documents
relevant to the current sub-query in [215]. Through the summarizer, the overwhelming context length problem is solved
and with the memory, the repetition problem of sub-query is alleviated.

3.4.4 Tree-based Search for RAG
Tree based Search for RAG

Tree-based methods introduce explicit search structures into the reasoning process, allowing RAG systems to explore,
evaluate, and refine multiple reasoning paths before reaching a final answer. In [216], SolutionRAG explored the

36


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

reasoning path by alternating between solution node and comment node which mimic CoT for solution review, i.e.,
bi-point thinking in a tree structure to improve reliability. Because of the tree structure, different reasoning paths are
explored and low quality paths are pruned through 1. reliability score to measure how well a solution solves a technical
challenge and 2. helpfulness score to measure the relevance of a feedback comment. Similarly, in ReARTeR [217],
the authors included process reward model and process explaination model to improve the performance of RAG. In
each step, the reasoning model generates multiple reasoning steps, the process reward model provides reward for every
response. If any reward is above a threshold, it will be accepted. Otherwise, the process explaination model will provide
explaination for the generator to refine its responses.

Critic-based MDP for RAG

By modeling the decision process as a Markov decision process (MDP), we can include search in the inference stage
by either generating multiple responses and select the best one using critic based method or balance exploration and
exploitation using MCTS.

To begin with, the best response is selected using critic based method. In [218], CRPlanner models the planning as
a MDP process where retrieval is a candidate action and multiple execution paths are generated and the best one is
selected by a critic model. In the MDP, states refers to the full history of actions and observations. Actions include
sub-goal selection and execution by LLM. To begin with, a sub-goal is selected like "RETRIEVE", "REASON" and
"GENQUERY" by a sub-goal selection model. Then, the sub-goal will be sent to LLM to generate multiple possible
execution paths and a critic model will evaluate the paths that most likely to reach the selected sub-goal as the next
action. The workflow of sub-goal selection, execution, and critic with retrieval iterates until reaching out the final
answer.

MCTS for RAG

The other direction leverages MCTS to search for the optimal reasoning path, where retrieval is incorporated in the
action space. To understand this line of work, it is helpful to firstly introduce Self-Play muTuAl Reasoning (rStar)
(219). In rStar, a generator-discriminator framework using MCTS is proposed, while it does not involve retrieval. For
the generator, it is realized through MCTS with five action options: 1. propose an one-step thought, 2. propose the
remaining thought steps, 3. propose next sub-question along with its answer, 4. answer the sub-question again and 5.
rephrase the question/sub-question. The discriminator utilizes another LLM to start from an intermediate reasoning
path and see if it can reach the same answer to evaluate the reasoning path.

Building upon rStar, subsequent works extended MCTS to RAG. In MCTS-RAG shown in Figure [18]b), the
generator is modeled as MCTS with six actions including 1. direct answer, 2. quick reasoning, 3. decompose question
from rStar, 4. retrieval reasoning, 5. retrieval decompose and 6. summarized answer, allowing dynamic interleaving of
reasoning and retrieval. The retrieval process includes four steps: 1. query generation, 2. query execution, 3. knowledge
reflection and 4. summary answer. Lastly, candidate answers from multiple reasoning paths are scored based on
accumulated rewards along their trajectories and selected via a voting mechanism, ensuring consistency and factual
accuracy rather than a discriminator. In RARE [221], two more actions are included: 6. search query generation and
information retrieval and 7. sub-question retrieval and re-answering based on the five actions in rStar for MCTS to
incorporate the retrieval capability. After the generation of different solution paths, retrieval-augmented factuality scorer
(RAFS) evaluates each statement of each path against retrieved evidence, assigning Supported/Not Supported labels
and a factuality score to select the most factually reliable reasoning trajectory.

3.4.5 RAG with Tools and Rules

RAG with Tools

With some tools, the performances of RAG are greatly improved by avoiding letting LLM conduct problems that they
are not good at. In this section, different tools will be discussed including 1. memory and 2. coding. When addressing
long-context or multi-hop problems, incorporating a dedicated memory module to store refined information significantly
enhances RAG performance. In the Retriever-and-Memory approach, an adaptive note mechanism iteratively retrieves
relevant documents, refines the extracted information, and updates the note [222]. Firstly, with the initial query xo and
initial retrieved documents zo, an initial memory note No is constructed. Then in the process of "Note-Centric Adaptive
Retrieval", for the ¢-th iteration 1. the query x; is refined based on previous queries 71,..., £;—; and current memory
note N;_1, and 2. the refined query x; is utilized to retrieve documents z;, and 3. the memory note N; is refined by
the retrieved documents z; and evaluated whether further retrieval is required. Eventually, the converged optimized
memory note Nop; is ultimately combined with the initial query xg to generate accurate responses. Similarly, TC-RAG

223] employs a Last-In-First-Out (LIFO) stack for memory management, where new information is added to the stack,
and irrelevant or incorrect data is removed. MemoRAG adopts a dual-system framework. First, a lightweight
global memory module—trained with Reinforcement Learning from Generation Feedback (RLGF)—compresses the

37


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

full context into compact representations and provides draft answer clues. Second, a more expressive LLM generates
the final response using evidence retrieved based on these clues. In practice, the database is pre-compressed into the
memory module. At inference time, a query is first answered in draft form using only the memory, without retrieval.
This draft answer, together with the original query, then guides retrieval of relevant documents, from which the LLM
produces the final output.

RAG with Rules

In RAG, rules are frequently applied manually in the prompt template which will concatenate the initial query and
retrieved documents before being sent to LLM for generating the final answer. Instead, rules can also be selected from
a provided rule pool or learned from dataset. Some research has been done on this topic. In [225], RuleRAG select
specific rules from a rule pool to enrich query for retrieval documents and guide LLM for Q&A in a specific manner.
The authors proposed 1. RuleRAG-ICL and 2. RuleRAG-FT. In RuleRAG-ICL, a query is applied to retrieve relevant
tules through cosine similarity in the embedding space by SentenceBERT. Then, for each rule, it is concatenated with
the query to retrieve its most important documents. Next, all the rules and retrieved documents are mixed with the query
for the final answer generation. RuleRAG-FT enhances RuleRAG-ICL with another fine-tuning stage with paired data
using contrastive learning.

Another direction directly generates rules from LLM for answer generation. In [226], Rule-Augmented Generation
(RuAG) was proposed to automatically generate logic rules from dataset and utilized the learned logic rules in LLM
prompt to generate the answer. To begin with, boolean-valued predicates were transformed from features, and then
impossible or irrevalent predicates were removed. Next, logic rules were searched from the remained predicates using
MCTS until the length or reward threshold was reached where state referred to partial logic rules, action referred to
adding a predicate and reward referred to evaluating the logic rules. Lastly, the generated logic were 1. cleaned by
removing replicate or low quality, 2. translated to natural language, 3. selected to fit given context window and 4.
applied for final answer generation.

3.4.6 Collaborative RAG

Lots of high quality retrievers have been pretrained. However, the matching of the retriever and the generator has not
been optimized. Some of the works in this subsection reply on fine-tuning, a little deviated from inference-time scaling
topic. However, it is too important to remove them.

RePlug proposed to utilize the LLM’s response generation probability based on query and retrieved documents, i.e.,

P(y|z, x) as information to improve the performances of the retriever. To be more specific, the probabaility of retrieving
‘ 3(2,2)/¥

documents z from x, ie., P(z|z) = Sop ee where s(z,x) = cos(E(z), E(x)) measured the relevancy

between z and x through cosine similarity between embedding. The probability of generator generating document z

ePulz.)/B

Yovte ps ere 2/8"
Lastly, the KL divergence between P(z|x) and Q(z|x, y) is minimized to optimize the retriever so that it will have a
better performance when working with the generator. In the inference, RePlug firstly retrieves multiple documents, and
then apply each document with the query and send to LLM to compute a next token probability. By ensembling across
different retrieved documents, the final next token probability is obtained and then sampled to obtain the final response.
In [228], RAG is trained using SFT for warmup and reinforcement learning to coordinate different modules and balance
retrieval and answering as shown in Figure[19](a). In the RL setting, the action includes "Answer" to generate answer
or "Retrieval" to rewrite query and retrieve and the reward includes the correctness of the final answer and each retrieval
penalty. When the action is predicted or when the maximum retrieval quota is reached, the answer will be generated
and the reward will be collected to update the policy. In [229], the authors model RAG in a multi-agent reinforcement
learning framework to optimize them together. The system is mainly composed of 1. query rewriter, 2. retriever, 3.
selector and 4. generator. A common LLM is utilized as the backbone of these modules, and a share reward is utilized
for these different modules and each module has their own reward like a query rewriting quality measurement. Lastly,
PPO is utilized to optimize the LLM to maximize the accumulated rewards.

In CoRAG [230], the authors extend RAG using local datasets of different clients into a collaborative setting. For
each agent, it continuously trains the retriever and generator based on its own private data. Then, after training, local
updates are aggregated into a global model for the next iteration of collaborative training. In the inference time, the
same global retriever and generator models will be utilized across different agents, the retrieval process will happen in
both the shared central dataset and agent-specific dataset before being sent to LLM for final answer generation. This
methodology preservers the data privacy of different agents and improve the performances of RAG for different clients.
In ActiveRAG [231], the authors propose to collaborate among multiple agents to improve performances on downstream
tasks as shown in Figure[19](b). The multi-agents include 1. self-inquiry agent for generating initial response based on

from query x and response y, i.e., Q(z|x, y) was computed through Bayes theorem Q(z|x, y) =

38


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

©) Question: Who is the spouse of the performer of
@ song Missing My Baby?
|_ Documents: “Missing My Baby” was one of the first
==]) Selena songs to be played on radio stations...
[Answer] John Adams Ground Truth: Chris Pé
[Question] Who was the first president arp Reward pound eer
who lived in the white house? oe Action ActiveRAG @ Chain of Thought
[Observation] None / [Searched Text] Ksgerre [Search] First president The song is “Missing My Baby” and
icy Tg : : i @ >> we need to know who is the
State lived in the white house cisticn Selfnquiry (Pedformer and who his/her spouse Is.
Reward Assimilated Knowledge
update @) a (@ The performer ofthe
@ + a song “Missing My Baby”
Question Documents Knowledge is Selena. The knowledge
[Searched] John Adams became the first = Ga Assimilation \_can help us ...
resident to move into the White House ... = . nswer
g =) retrieve Selena was married to Chris Pérez,
. a guitarist for Selena Los Dinos.
Observation Database (From Parametric Memory) a7 a @
So Chris Pérez is the spouse of the
performer of the song "Missing My Thought Question
Baby", which Is Selena. LenS ah co
(a). Improve module collaboration through RL (b). Improve module collaboration through multi-agent

Figure 19: Collaborative RAG: (a) fine-tune all modules through RL to improve collaboration and (b) use multi-agent
to improve collaboration

Visual information-seeking Queries What shape is the pediment used by Capitol Hill in DC?

Q: What can be found on the White House
balconies at Christmas?

Backbone Encoder

A: Wreath and garlands are decorated on the balconies. Maximum Inner Product Search
Q: What can you find on the roof of the White House? f omens as
A: The flag of the United States is on the roof.

Dense Index Index Encoder Multimodal Corpus
Q: What’s the color of Capitol Hill building in DC? = i

A: Capitol Hill is mostly white colored. ¢ Backbone &
Q: What shape is the pediment used by Capitol Hill, DC? Qh Encoder a :

A: triangular pediments is used.

A: Capitol Hill is built with domed roof. Sani + Capitol Hill: front arch ... + What shape is ..

| Retrieval Augmentation

- 3 Ve -
F ¥ fo satel Ett Backbone Encoder

White House Capitol Hill, Donald Trump, White House

during Christmas WashingtonDC —Chilren to testify .. suspended tourist ... Decoder

Multimedia World Wide Web

triangular-shaped pediment ...

(a). Queries need to answered through multi-modality (b). The workflow of multi-modal RAG

Figure 20: Muti-modal RAG: (a) some example questions that need to be answer using muti-model RAG and (b) the
general workflow of multi-modal RAG

LLM parameters, 2. knowledge assimilation agent for analyzing the retrieved documents to extract useful insights and
3. thought accommodation agent to refine initial responses by resolving the confilicts with external knowledge.

3.5  Muti-modal RAG

Previous work focuses on Q&A in the text domain. However, lots of questions need to be answered across multiple
modalities like text, speech and image as shown in Figure[20] To extend RAG to multi-modality, a multi-modal encoders
is utilized to extract embedding from different modalities into a common embedding space for retrieval. Then, a
multi-modal LLM is applied to generate responses based on query and retrieved multi-modal documents. Different
encoders are utilized for embedding extraction from images and texts [232], [233], 234]. To mitigate the fairness of
image generation, FairRAG utilize a fair retrieval system to retrieve demographically balanced reference images
and then apply these reference images in the output generation.

In LLM-based ASR RAG (LARAG) where ASR refers to automatic speech recognition, the authors applied RAG
in the audio domain for speech transcript correction. To begin with, a speech tokenizer like (CTC/AED) is utilized to
obtain speech tokens from speech which is aligned with a corresponding text. Then, a speech database of speech-text
pairs is constructed using a pre-trained ASR model, and it is stored as a speech inverted index from speech token to
speech transcription to facilicate term frequency search. When given an audio, the speech tokens are utilized obtain
speech token and then retrieve similar speech tokens from the previously built database with token-level KNN. The

39


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

retrieved results are grouped and scored at the sequence-level, with low-similarity matches filtered and error-prone
tokens pruned using N-best list analysis. Next, a feedforward network transforms the speech embedding to produce
candidate text embeddings. Lastly, speech embeddings, corresponding text embeddings, current input speech tokens
and N-best candidate text embeddings are sent to LLM for generating the final correct text transcription.

4 Input: Few-Shot

Few-shot prompt tuning is one of the most commonly used methods to improve LLM performance. In zero-shot
prompt tuning, the model is provided only with a task description or instructions, without any labeled examples [237].
Surprisingly, LLMs often perform well in zero-shot settings, demonstrating strong reasoning capabilities even without
examples. In few-shot prompt tuning, the model is given a small number of labeled examples (typically 2-10) alongside
the task description. A special case is one-shot prompt tuning, where exactly one labeled example is included. The
success of GPT-3 demonstrated the effectiveness of few-shot prompting, particularly in industry scenarios where a
specific output format is desired without full model fine-tuning [238].

Research has investigated how and why few-shot prompting improves LLM performance [239]. To begin with, they
discover that replacing in-distribution examples with out-of-distribution examples significantly reduces performance.
Besides, they discovered that direct models, which predict labels directly, rely heavily on correct labels in few-shot
examples. In contrast, channel models, which predict the likelihood of input given a label, are less sensitive to the
correctness of the examples. Lastly, they observed that the structure of input-output demonstrations matters: leaving
only the input or only the output substantially decreases performance.

Another research direction focuses on automatically generating prompts to further improve LLM performance. In
Automatic Prompt Engineer (APE) [240], prompts are generated using two strategies:

¢ Forward generation: The model is given task examples and instructed to continue a story-like setup, completing
the prompt.

¢ Reverse generation (infilling): The model fills in blanks in the middle of input-output examples.
Generated prompts are then scored on a small validation set using metrics like answer accuracy and log probability. The
top prompts are selected, and the process iterates for multiple epochs to optimize prompt quality (Figure 21}. Similarly,
Automate-CoT [241] automates the generation of prompts with a focus on: 1. Order Sensitivity, 2. Complexity
Sensitivity, 3. Diversity Sensitivity and 4. Style Sensitivity. Here, the LLM generates multiple reasoning paths,
low-quality paths are filtered, and a Variance-Reduced Policy Gradient (VR-PG) method estimates the importance of

specific examples for downstream tasks. Finally, the highest-quality examples are selected and ordered to form the final
prompt for the LLM.

5 Conclusion

Our review has explored the shift in LLM research from the traditional focus on scaling training FLOPs and data—a
path increasingly constrained by the scarcity of high-quality data—to inference-time scaling. This paradigm uses
increased computational resources during inference to significantly enhance LLM performance on downstream tasks.

We systematically organized the techniques contributing to this shift based on two key perspectives: the output and the
input.

On the output side, we discussed a range of methods for more sophisticated generation, including:
* Reasoning techniques like CoT, ToT, and ReAct.
¢ Search methods such as MCTS and beam search.
¢ Advanced decoding strategies, including Best-of-N, speculative decoding, and constrained decoding.
* Methods focused on training for long CoT, such as RLVR and GRPO.
* The emerging fields of multi-modal reasoning and model ensemble.

Regarding the input side, we examined few-shot learning and dedicated significant attention to RAG. Our analysis of
RAG covered its critical components and latest advancements:

* Query expansion.
¢ The role of high-quality data.

40


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

G Keep the high score candidates

LLMs as Inference Models

Professor Smith was given the
following instructions: <INSERT>

Here are the Professor’s responses:

# Demostration Start
Input: prove Output: disprove

®

Proposal

xX Discard the low score candidates

* Final selected prompt with highest score

LLMs as Scoring Models

Instruction: write the antonym of the
word.

<LIKELIHOOD>

Input: direct Output{indirect}
®@ Scoring iff @

Log
Probability

Input: on Output: off

# Demostration End

write the antonym of the word.

[Optional] High Score
LLMs as Resampling Models Candidates
Generate a variation of the following
instruction while keeping the semantic ()
meaning. Similar
Candiates

Input: write the antonym of the word.

Output: <COMPLETE>

list antonyms for the given word.

Figure 21: APE: automatic prompt engineer to generate lots of prompts and scoring based on a small set of test examples

using answer accuracy and log probability

¢ Improvements in retrieval and reranker mechanisms.

* Optimization of the LLM generation process.
* The integration of multi-modal RAG.

In summary, inference-time scaling provides a powerful, practical avenue for improving LLM capabilities in the face of
data limitations. The rich landscape of techniques reviewed—from complex reasoning and search at the output level to
sophisticated RAG architectures at the input level—underscores the depth and potential of this rapidly evolving area of

research.

6 Symbols

G: knowledge graph

EE: Edges in knowledge graph

N: Nodes in knowledge graph

rj: the average reward of node 7 in MCTS

N: the total number of visits to the parent in MCTS

n,: the number of visits to node 7 in MCTS

P(y|a): the probability of generating response y given prompt

Sz: State at step t for MDP

dz: action at step t for MDP

p: draft model in speculative decoding
q: target model in speculative decoding
M: large base model

m*: fine-tuned domain model

m+ anti-domain model

KL: KL divergence

41


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

z: retrieved documents in RAG
KNN: K nearest neighborhood

Nopt: the optimal memory note

References

[1] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,
AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel,
Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard
Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison
Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej
Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew
Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia,
Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger,
Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby
Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian
Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson,
Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen
Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim,
Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki,
Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy,
David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris
Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth
Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo,
Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie
Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao,
Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner,
Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian
O’Connell, Ian O’ Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya
Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung,
James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang,
Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh,
Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe
Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato,
Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan,
Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi,
Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren
Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry
Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien
Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas
Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine
Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan
Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet
Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael
Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo
de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin,
Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan
LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige,
Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk,
Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel
Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet,
Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan
Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani,
Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet,
Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam
Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith,
Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino

42


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee,
Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders,
Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell,
Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan
Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo,
Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba,
Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing
Dai, and Yury Malkov. Gpt-40 system card, 2024.

[2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny
Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.

[3] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.

[4] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton,
Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation, 2023.

[5] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan O. Arik, and Tomas Pfister. Better zero-shot reasoning with
self-adaptive prompting, 2023.

[6] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.
Tree of thoughts: Deliberate problem solving with large language models, 2023.

[7] Fatemeh Haji, Mazal Bethany, Maryam Tabar, Jason Chiang, Anthony Rios, and Peyman Najafirad. Improving
Ilm reasoning with multi-agent tree-of-thought validator agent, 2024.

[8] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna
Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving
elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence,
38(16):17682—17690, March 2024.

[9] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,
Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language
models, 2023.

[10] Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt pre-trained language models for chain of thought,
2022.

[11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:
Synergizing reasoning and acting in language models, 2023.

[12] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha
Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,
Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.

[13] Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug,
2023.

[14] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. Refiner: Reasoning feedback on intermediate representations, 2024.

[15] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi.
Generating sequences by learning to self-correct, 2022.

[16] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
Reflexion: Language agents with verbal reinforcement learning, 2023.

[17] Yifan Zhang, Jinggin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language
models, 2025.

[18] Ethan Brooks, Logan Walls, Richard L. Lewis, and Satinder Singh. Large language models can implement policy
iteration, 2023.

[19] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+p:
Empowering large language models with optimal planning proficiency, 2023.

[20] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve
prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023.

[21] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from
feedback with language models, 2023.

43


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[22] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine,
Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language
models, 2022.

[23] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: from natural
language instructions to feasible plans. Autonomous Robots, 47(8):1345-—1365, November 2023.

[24] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and
select: Interactive planning with large language models enables open-world multi-task agents, 2024.

[25] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models
for interpretable logical reasoning, 2022.

[26] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022.

[27] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham
Neubig. Pal: Program-aided language models, 2023.

[28] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling
computation from reasoning for numerical reasoning tasks, 2023.

[29] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris
Callison-Burch. Faithful chain-of-thought reasoning, 2023.

[30] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen,
Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul
Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math
problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of
Sciences, 119(32), August 2022.

[31] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D. Goodman. Hypothesis
search: Inductive reasoning with language models, 2024.

[32] Didac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning,
2023.

[33] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill
Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,
and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.

[34] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen.
Tora: A tool-integrated reasoning agent for mathematical problem solving, 2024.

[35] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and
Chao Zhang. Toolchain*: Efficient action space navigation in large language models with a* search, 2023.

[36] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning
with language model is planning with world model, 2023.

[37] Sungjin Park, Xiao Liu, Yeyun Gong, and Edward Choi. Ensembling large language models with process
reward-guided tree search for better complex reasoning, 2024.

[38] Yangqing Fu, Ming Sun, Buging Nie, and Yue Gao. Accelerating monte carlo tree search with probability tree
state abstraction, 2023.

[39] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz.
Don’t throw away your value model! generating more preferable text with value-guided monte-carlo tree search
decoding, 2024.

[40] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang.
Alphazero-like tree-search can guide large language model decoding and training, 2024.

[41] Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical
olympiad solutions via monte carlo tree self-refine with llama-3 8b, 2024.

[42] Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu
Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, 2023.

[43] Antoine Chaffin, Vincent Claveau, and Ewa Kijak. Ppl-mcts: Constrained textual generation through
discriminator-guided mcts decoding, 2022.

44


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[44] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi.
Maieutic prompting: Logically consistent reasoning with recursive explanations, 2022.

[45] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktaschel, Vassilis Plachouras, Fabrizio
Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text, 2020.

[46] Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024.
[47] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making large
language models better reasoners with step-aware verifier, 2023.

[48] Tinghui Zhu, Kai Zhang, Jian Xie, and Yu Su. Deductive beam search: Decoding deducible rationale for
chain-of-thought reasoning, 2024.

[49] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,
Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,
Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan
Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina
Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet,
Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022.

[50] Rishi Hazra, Pedro Zuidberg Dos Martires, and Luc De Raedt. Saycanpay: Heuristic planning with large
language models using learnable domain knowledge, 2024.

[51] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey
Levine, Karol Hausman, and Brian Ichter. Grounded decoding: Guiding text generation with grounded models
for embodied agents, 2023.

[52] Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Grace: Discriminator-
guided chain-of-thought reasoning, 2023.

[53] Wouter Kool, Herke van Hoof, and Max Welling. Stochastic beams and where to find them: The gumbel-top-k
trick for sampling sequences without replacement, 2019.

[54] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Self-evaluation
guided beam search for reasoning, 2023.

[55] Clara Meister, Afra Amini, Tim Vieira, and Ryan Cotterell. Conditional poisson stochastic beam search, 2023.

[56] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020.

[57] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving
multi-step reasoning for IIms with deliberative planning, 2024.

[58] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and
reasoning in language models through multiagent debate, 2023.

[59] Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. More agents is all you need, 2024.

[60] Sumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, Rafael Rafailov, Ivan Laptev, Philip H. S. Torr,
Fabio Pizzati, Ronald Clark, and Christian Schroeder de Witt. Malt: Improving reasoning with multi-agent Ilm
training, 2025.

[61] Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. Agentcoder: Multi-
agent-based code generation with iterative testing and optimisation, 2024.

[62] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advances in
neural information processing systems, 27, 2014.

[63] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann machines.
Cognitive science, 9(1):147-169, 1985.
[64] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.

[65] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration,
2020.

[66] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to
solve math word problems, 2021.

[67] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal.
V-star: Training verifiers for self-taught reasoners, 2024.

45


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[68] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet:
Code generation with generated tests, 2022.

[69] Charlie Snell, Jachoon Lee, Kelvin Xu, and Aviral Kumar. Scaling Ilm test-time compute optimally can be more
effective than scaling model parameters, 2024.

[70] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini.
Large language monkeys: Scaling inference compute with repeated sampling, 2024.

[71] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari,
and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.

[72

“

Gwenyth Portillo Wightman, Alexandra Delucia, and Mark Dredze. Strength in numbers: Estimating confidence
of large language models by prompt agreement. In Anaelia Ovalle, Kai-Wei Chang, Ninareh Mehrabi, Yada
Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista Cao, Anoop Kumar, and Rahul Gupta,
editors, Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages
326-362, Toronto, Canada, July 2023. Association for Computational Linguistics.

[73] Gongalo Faria and Noah A. Smith. Sample, don’t search: Rethinking test-time alignment for language models,
2025.

[74] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through
memorization: Nearest neighbor language models, 2020.

[75] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning language
models by proxy, 2024.

[76] Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. In Proceedings of
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics, 2021.

[77] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding,
2023.

[78] Chris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using grid beam search,
2017.

[79] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open vocabulary image captioning
with constrained beam search. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings
of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 936-945, Copenhagen,
Denmark, September 2017. Association for Computational Linguistics.

[80] Matt Post and David Vilar. Fast lexically constrained decoding with dynamic beam allocation for neural machine
translation, 2018.

[81] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. Picard: Parsing incrementally for constrained
auto-regressive decoding from language models, 2021.

[82] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting is programming: A query language for large
language models. Proceedings of the ACM on Programming Languages, 7(PLDI):1946-1969, June 2023.

[83] Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Neurologic
decoding: (un)supervised neural text generation with predicate logic constraints, 2021.

[84] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin,
Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. Neurologic a*esque decoding: Constrained text
generation with lookahead heuristics, 2021.

[85] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded
mathematical proof generation with language models, 2022.

[86] Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah D. Goodman. Certified deductive reasoning with
language models, 2023.

[87] Hayate Iso. Autotemplate: A simple recipe for lexically constrained text generation, 2024.

[88] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao,
Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin,
Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng

46


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang,
Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai
Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang,
Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang,
Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,
Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou,
Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou,
Shaoging Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen
Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan
Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li,
Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang
Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang,
Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong,
Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan
Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou,
Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun
Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen
Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song,
Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning
capability in Ilms via reinforcement learning, 2025.

[89] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John
Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023.

[90] Sarah Pan, Vladislav Lialin, Sherin Muckatira, and Anna Rumshisky. Let’s reinforce step by step, 2023.

[91] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie
Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models
to follow instructions with human feedback, 2022.

[92

“4

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging Ilm-as-a-judge with
mt-bench and chatbot arena, 2023.

Aliyah R. Hsu, James Zhu, Zhichao Wang, Bin Bi, Shubham Mehrotra, Shiva K. Pentyala, Katherine Tan,
Xiang-Bo Mao, Roshanak Omrani, Sougata Chaudhuri, Regunathan Radhakrishnan, Sitaram Asur, Claire Na
Cheng, and Bin Yu. Rate, explain and cite (rec): Enhanced explanation and attribution in automatic evaluation
by large language models, 2025.

[93

“4

[94

sy

Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. Direct judgement preference optimiza-
tion, 2025.

Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time
scaling for generalist reward modeling, 2025.

[95

“4

[96

=

Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans,
Yi Dong, and Oleksii Kuchaiev. Helpsteer3-preference: Open human-annotated preference data across diverse
tasks and languages, 2025.

[97] Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward reasoning
model, 2025.

[98] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-
thought reasoning in language models, 2024.

[99] Qii Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for
visual reasoning refinement in multimodal large language models, 2024.

[100] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting
for large multimodal models, 2024.

[101] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli¢, and Furu Wei.
Imagine while reasoning in space: Multimodal visualization-of-thought, 2025.

[102] Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin, Yuan Yao, Mingkai Chen, and Jiebo Luo. Cocot: Contrastive
chain-of-thought prompting for large multimodal models with multiple image inputs, 2024.

47


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[103] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico
Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.
Socratic models: Composing zero-shot multimodal reasoning with language, 2022.

[104] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,
Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action, 2023.

[105] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought
prompting for multimodal reasoning in language models, 2023.

[106] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting
unleashes extraordinary visual grounding in gpt-4v, 2023.

[107] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle?
visual prompt engineering for vlms, 2023.

[108] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, and Ranjay
Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models, 2024.

[109] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess,
Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu,
Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian
Ichter. Pivot: Iterative visual prompting elicits actionable knowledge for vlms, 2024.

[110] Dingning Liu, Xiaomeng Dong, Renrui Zhang, Xu Luo, Peng Gao, Xiaoshui Huang, Yongshun Gong, and Zhihui
Wang. 3daxiesprompts: Unleashing the 3d spatial task capabilities of gpt-4v, 2023.

[111] Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. Scaffolding coordinates to promote vision-
language coordination in large multi-modal models, 2024.

[112] Fangi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large
language models, 2024.

[113] Costas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time via perplexity
optimization, 2024.

[114] Yangyifan Xu, Jinliang Lu, and Jiajun Zhang. Bridging the gap between different vocabularies for Ilm ensemble,
2024.

[115] Cong Liu, Xiaojun Quan, Yan Pan, Liang Lin, Weigang Wu, and Xu Chen. Cool-fusion: Fuse large language
models without training, 2025.

[116] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin
Choi. Dexperts: Decoding-time controlled text generation with experts and anti-experts, 2021.

[117] Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, and Min Lin. Purifying large language models
by ensembling a small language model, 2024.

[118] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise
ranking and generative fusion, 2023.

[119] Antdénio Farinhas, José G. C. de Souza, and André F. T. Martins. An empirical study of translation hypothesis
ensembling with large language models, 2023.

[120] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowledge
card: Filling Ilms’ knowledge gaps with plug-in specialized language models, 2024.

[121] Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, and Mikhail Yurochkin. Fusing models
with complementary expertise, 2024.

[122] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park.
Swad: Domain generalization by seeking flat minima, 2021.

[123] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model
selection and boosting performance in domain generalization, 2022.

[124] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S.
Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups:
averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022.

[125] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonyjy, Patrick Gallinari, and Matthieu

Cord. Diverse weight averaging for out-of-distribution generalization. Advances in Neural Information Processing
Systems, 35:10821-10836, 2022.

48


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[126] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving
interference when merging models, 2023.

[127] Fanqi Wan, Longguang Zhong, Ziyi Yang, Ruijun Chen, and Xiaojun Quan. Fusechat: Knowledge fusion of chat
models, 2024.

[128] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging
weights of language models, 2025.

[129

—“

Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Roziere, Jacob Kahn,
Daniel Li, Wen tau Yih, Jason Weston, and Xian Li. Branch-train-mix: Mixing expert Ilms into a mixture-of-
experts Ilm, 2024.

[130

=

Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, and
Maosong Sun. Mastering text, code and math simultaneously via fusing highly specialized language models,
2024.

[131] Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer.
Scaling expert language models with unsupervised domain discovery, 2023.

[132

“Ko

Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and
Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning, 2022.

[133] Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic
operations, 2023.

[134] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task
generalization via dynamic lora composition, 2024.

[135] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented
language model pre-training, 2020.

[136] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain
question answering, 2021.

[137] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations
and Trends® in Information Retrieval, 3(4):333-389, 2009.

[138] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Dangi Chen, and Wen
tau Yih. Dense passage retrieval for open-domain question answering, 2020.

[139] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Kittler, Mike Lewis, Wen tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented
generation for knowledge-intensive nlp tasks, 2021.

[140] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding, 2019.

[141] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks, 2019.
[142] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu
Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024.

[143] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with
disentangled attention, 2021.

[144] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy,
Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy
Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich
Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens, 2022.

[145] Zackary Rackauckas. Rag-fusion: A new take on retrieval augmented generation. International Journal on
Natural Language Computing, 13(1):37-47, February 2024.

[146] Zhicong Li, Jiahao Wang, Zhishu Jiang, Hangyu Mao, Zhongxia Chen, Jiazhen Du, Yuanxing Zhang, Fuzheng
Zhang, Di Zhang, and Yong Liu. Dmaqr-rag: Diverse multi-query rewriting for rag, 2024.

[147] Yeonjun In, Sungchul Kim, Ryan A. Rossi, Md Mehrab Tanjim, Tong Yu, Ritwik Sinha, and Chanyoung Park.
Diversify-verify-adapt: Efficient and robust retrieval-augmented ambiguous question answering, 2025.

[148] Pouya Pezeshkpour and Estevam Hruschka. Insight-rag: Enhancing Ilms with insight-driven augmentation, 2025.

49


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[149] Maciej Besta, Ales Kubicek, Robert Gerstenberger, Marcin Chrapek, Roman Niggli, Patrik Okanovic, Yi Zhu,
Patrick Iff, Michal Podstawski, Lucas Weitzendorf, Mingyuan Chi, Joanna Gajda, Piotr Nyczyk, Jiirgen Miiller,
Hubert Niewiadomski, and Torsten Hoefler. Multi-head rag: Solving multi-aspect problems with Ilms, 2025.

[150] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance
labels, 2022.

[151] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models, 2023.

[152] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. Query expansion by
prompting large language models, 2023.

[153] S.E. ROBERTSON. On term selection for query expansion. Journal of Documentation, 46(4):359-364, 04 1990.

[154] Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, and Daxin Jiang. Large language models
are strong zero-shot retriever, 2023.

[155] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-
augmented large language models with iterative retrieval-generation synergy, 2023.

[156] Tianchi Cai, Zhiwen Tan, Xierui Song, Tao Sun, Jiyan Jiang, Yungi Xu, Yinger Zhang, and Jinjie Gu. Forag:
Factuality-optimized retrieval augmented generation for web-enhanced long-form question answering. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’24, page
199-210. ACM, August 2024.

[157] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. Rat: Retrieval augmented thoughts
elicit context-aware reasoning in long-horizon generation, 2024.

[158] Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws, and Tarik Borogovac. Meta
knowledge for retrieval augmented large language models, 2024.

[159] Ruiran Yan, Zheng Liu, and Defu Lian. O1 embedder: Let retrievers think before action, 2025.

[160] Ishneet Sukhvinder Singh, Ritvik Aggarwal, Ibrahim Allahverdiyev, Muhammad Taha, Aslihan Akalin, Kevin
Zhu, and Sean O’Brien. Chunkrag: Novel Ilm-chunk filtering method for rag systems, 2025.

[161] Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, and Jie Tang. Longrag: A
dual-perspective retrieval-augmented generation paradigm for long-context question answering, 2024.

[162] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
Lost in the middle: How language models use long contexts, 2023.

[163] Brian J. Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hsen Huang. Don’t do rag: When cache-augmented
generation is all you need for knowledge tasks. In Companion Proceedings of the ACM on Web Conference 2025,
WWW ’25, page 893-897. ACM, May 2025.

[164] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha
Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach to
query-focused summarization, 2025.

[165] Vincent A Traag, Ludo Waltman, and Nees Jan Van Eck. From louvain to leiden: guaranteeing well-connected
communities. Scientific reports, 9(1):1-12, 2019.

[166] Rong-Ching Chang and Jiawei Zhang. Communitykg-rag: Leveraging community structures in knowledge
graphs for advanced retrieval-augmented generation in fact-checking, 2024.

[167] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of commu-
nities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008, October
2008.

[168] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically
inspired long-term memory for large language models, 2025.

[169] Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. From rag to memory: Non-parametric
continual learning for large language models, 2025.

[170] Masoomali Fatehkia, Ji Kim Lucas, and Sanjay Chawla. T-rag: Lessons from the Ilm trenches, 2024.

[171] Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. Hybridrag:
Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction,
2024.

[172] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-augmented
generation, 2025.

50


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[173] Yassir Lairgi, Ludovic Moncla, Rémy Cazabet, Khalid Benabdeslem, and Pierre Cléau. itext2kg: Incremental
knowledge graphs construction using large language models, 2024.

[174] Junde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min Xu, Filippo Menolascina, and Vicente Grau. Medical
graph rag: Towards safe medical large language model via graph retrieval-augmented generation, 2024.

[175] Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, and Ninghao Liu. Retrieval-
enhanced knowledge editing in language models for multi-hop question answering, 2024.

[176] Yuchen Shi, Guochao Jiang, Tian Qiu, and Deqing Yang. Agentre: An agent-based framework for navigating
complex information landscapes in relation extraction. In Proceedings of the 33rd ACM International Conference
on Information and Knowledge Management, CIKM ’24, page 2045-2055. ACM, October 2024.

[177] Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. Mitigating lost-in-retrieval problems in
retrieval augmented multi-hop question answering, 2025.

[178] Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. Think-
on-graph 2.0: Deep and faithful large language model reasoning with knowledge-guided retrieval augmented
generation, 2025.

[179] Wenjie Wu, Yongcheng Jing, Yingjie Wang, Wenbin Hu, and Dacheng Tao. Graph-augmented reasoning:
Evolving step-by-step knowledge graph retrieval for llm reasoning, 2025.

[180] Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao,
Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang,
Huajun Chen, Wenguang Chen, and Jun Zhou. Kag: Boosting Ilms in professional domains via knowledge
augmented generation, 2024.

[181] Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, and S Kevin Zhou. Frag: A flexible modular
framework for retrieval-augmented generation based on knowledge graphs, 2025.

[182] Mufei Li, Siqi Miao, and Pan Li. Simple is effective: The roles of graphs and large language models in
knowledge-graph-based retrieval-augmented generation, 2025.

[183] Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, and Jeff Z. Pan. Cotkr: Chain-of-thought
enhanced knowledge rewriting for complex knowledge graph question answering, 2025.

[184] Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. Grag: Graph retrieval-augmented
generation, 2025.

[185] Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li,
Wanli Ouyang, Wenbo Su, and Bo Zheng. Graphreader: Building graph-based agent to enhance long-context
abilities of large language models, 2024.

[186] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. Raptor:
Recursive abstractive processing for tree-organized retrieval, 2024.

[187] Si-An Chen, Lesly Miculicich, Julian Martin Eisenschlos, Zifeng Wang, Zilong Wang, Yanfei Chen, Yasuhisa
Fujii, Hsuan-Tien Lin, Chen-Yu Lee, and Tomas Pfister. Tablerag: Million-token table understanding with
language models, 2024.

[188] Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: Html is better
than plain text for modeling retrieved knowledge in rag systems. In Proceedings of the ACM on Web Conference
2025, WWW ’25, page 1733-1746. ACM, April 2025.

[189] Mykhailo Poliakov and Nadiya Shvai. Multi-Meta-RAG: Improving RAG for Multi-hop Queries Using Database
Filtering with LLM-Extracted Metadata, page 334-342. Springer Nature Switzerland, 2025.

[190] Zhichao Wang, Bin Bi, Yanqi Luo, Sitaram Asur, and Claire Na Cheng. Diversity enhances an Ilm’s performance
in rag and long-context task, 2025.

[191] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. Adaptive-rag: Learning to adapt
retrieval-augmented large language models through question complexity, 2024.

[192] Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, and Md Rizwan
Parvez. Open-rag: Enhanced retrieval-augmented reasoning with open-source large language models, 2024.

[193] Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun,
Hang Yan, and Xipeng Qiu. Unified active retrieval for retrieval augmented generation, 2024.

[194] Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei Zhang. Hierarchical retrieval-
augmented generation model with rethink for multi-hop question answering, 2024.

51


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[195] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter context for
retrieval-augmented generation, 2023.

[196] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation, 2024.

[197] Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan O. Arik. Astute rag: Overcoming imperfect
retrieval augmentation and knowledge conflicts for large language models, 2025.

[198] Zheng Liu, Chenyuan Wu, Ninglu Shao, Shitao Xiao, Chaozhuo Li, and Defu Lian. Lighter and better: Towards
flexible context adaptation for retrieval augmented generation, 2024.

[199] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen tau Yih, Joelle Pineau, and Luke
Zettlemoyer. Improving passage retrieval with zero-shot question generation, 2023.

[200] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro.
Rankrag: Unifying context ranking with retrieval-augmented generation in Ilms, 2024.

[201] Kiseung Kim and Jay-Yoon Lee. Re-rag: Improving open-domain qa performance and interpretability with
relevance estimator in retrieval-augmented generation, 2024.

[202] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with
chain-of-thought reasoning for knowledge-intensive multi-step questions, 2023.

[203] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.
Search-ol: Agentic search-enhanced large reasoning models, 2025.

[204] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,
generate, and critique through self-reflection, 2023.

[205] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi- Yu, Yiming Yang, Jamie Callan,
and Graham Neubig. Active retrieval augmented generation, 2023.

[206] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. Im-rag:
Multi-round retrieval-augmented generation through learning inner monologues, 2024.

[207] Zhuocheng Zhang, Yang Feng, and Min Zhang. Levelrag: Enhancing retrieval-augmented generation with
multi-hop logic planning over rewriting augmented searchers, 2025.

[208] Peter Baile Chen, Yi Zhang, Michael Cafarella, and Dan Roth. Can we retrieve everything all at once? arm: An
alignment-oriented Ilm-based retrieval method, 2025.

[209] Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large
language models, 2024.

[210] Liang Xiao, Wen Dai, Shuai Chen, Bin Qin, Chongyang Shi, Haopeng Jing, and Tianyu Guo. Retrieval-augmented
generation by evidence retroactivity in llms, 2025.

[211] Myeonghwa Lee, Seonho An, and Min-Soo Kim. Planrag: A plan-then-retrieval augmented generation for
generative large language models as decision makers, 2024.

[212] Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, and Amit Sharma.
Plan*rag: Efficient test-time planning for retrieval augmented generation, 2025.

[213] Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi Li. Rearag:
Knowledge-guided reasoning enhances factuality of large reasoning models with iterative retrieval augmented
generation, 2025.

[214] Ruibin Xiong, Yimeng Chen, Dmitrii Khizbullin, Mingchen Zhuge, and Jiirgen Schmidhuber. Beyond outlining:
Heterogeneous recursive planning for adaptive long-form writing with language models, 2025.

[215] Zhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. Retrieve, summarize, plan: Advancing multi-hop
question answering with an iterative approach, 2025.

[216] Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, and
Le Sun. Deepsolution: Boosting complex engineering solution design via tree-based exploration and bi-point
thinking, 2025.

[217] Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, and
Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding, 2025.

[218] Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. Can we further elicit
reasoning in Ilms? critic-guided planning with retrieval-augmentation for solving challenging tasks, 2024.

[219] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes
smaller Ilms stronger problem-solvers, 2024.

52


Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

[220] Yunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan. Mcts-rag: Enhancing retrieval-augmented generation
with monte carlo tree search, 2025.

[221] Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, and Hong Yu. Rare: Retrieval-augmented
reasoning enhancement for large language models, 2025.

[222] Ruobing Wang, Qingfei Zhao, Yukun Yan, Daren Zha, Yuxuan Chen, Shi Yu, Zhenghao Liu, Yixuan Wang, Shuo
Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Deepnote: Note-centric deep retrieval-augmented generation,
2025.

[223] Xinke Jiang, Yue Fang, Rihong Qiu, Haoyu Zhang, Yongxin Xu, Hao Chen, Wentao Zhang, Ruizhe Zhang,
Yuchen Fang, Xu Chu, Junfeng Zhao, and Yasha Wang. Tc-rag:turing-complete rag’s case study on medical Ilm
systems, 2024.

[224] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. Memorag:
Boosting long context processing with global memory-enhanced retrieval augmentation, 2025.

[225] Zhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen Huang, Yong Dou, Xuhui Jiang, and Jian Guo. Rulerag:
Rule-guided retrieval-augmented generation with language models for question answering, 2025.

[226] Yudi Zhang, Pei Xiao, Lu Wang, Chaoyun Zhang, Meng Fang, Yali Du, Yevgeniy Puzyrev, Randolph Yao, Si Qin,
Qingwei Lin, Mykola Pechenizkiy, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. Ruag: Learned-rule-
augmented generation for large language models, 2024.

[227] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.

[228] Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. Smartrag: Jointly learn rag-related tasks from
the environment feedback, 2025.

[229] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and
Jiaxin Mao. Improving retrieval-augmented generation through multi-agent reinforcement learning, 2025.

[230] Aashiq Muhamed, Mona Diab, and Virginia Smith. Corag: Collaborative retrieval-augmented generation, 2025.

[231] Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Chaojun Xiao, Zhiyuan Liu, Ge Yu,
and Chenyan Xiong. Activerag: Autonomously knowledge assimilation and accommodation through retrieval-
augmented agents, 2024.

[232] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. Murag: Multimodal retrieval-augmented
generator for open question answering over images and text, 2022.

[233] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han,
Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on multi-modality
documents, 2025.

[234] Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. M3docrag: Multi-modal retrieval is
what you need for multi-page multi-document understanding, 2024.

[235] Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, and Siqi Deng. Fairrag: Fair human
generation via fair retrieval augmentation, 2024.

[236] Shaojun Li, Hengchao Shang, Daimeng Wei, Jiaxin Guo, Zongyao Li, Xianghui He, Min Zhang, and Hao Yang.
La-rag:enhancing Ilm-based asr accuracy with retrieval-augmented generation, 2024.

[237] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models
are zero-shot reasoners, 2023.

[238] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners, 2020.

[239] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
Rethinking the role of demonstrations: What makes in-context learning work?, 2022.

[240] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.
Large language models are human-level prompt engineers, 2023.

[241] KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-
thought from labeled data, 2024.

53
