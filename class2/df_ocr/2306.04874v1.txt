arX1v:2306.04874v1 [cs.CL] 8 Jun 2023

Expanding Scope: Adapting English Adversarial Attacks to Chinese

Hanyu Liu, Chengyuan Cai, Yanjun Qi
Department of Computer Science
University of Virginia
Charlottesville, VA, USA
{hl2gn, cc4fy, yq2h}@virginia.edu

Abstract

Recent studies have revealed that NLP predic-
tive models are vulnerable to adversarial at-
tacks. Most existing studies focused on design-
ing attacks to evaluate the robustness of NLP
models in the English language alone. Litera-
ture has seen an increasing need for NLP solu-
tions for other languages. We, therefore, ask
one natural question: whether state-of-the-art
(SOTA) attack methods generalize to other lan-
guages. This paper investigates how to adapt
SOTA adversarial attack algorithms in English
to the Chinese language. Our experiments
show that attack methods previously applied
to English NLP can generate high-quality ad-
versarial examples in Chinese when combined
with proper text segmentation and linguistic
constraints. In addition, we demonstrate that
the generated adversarial examples can achieve
high fluency and semantic consistency by fo-
cusing on the Chinese language’s morphology
and phonology, which in turn can be used to
improve the adversarial robustness of Chinese
NLP models.

1 Introduction

Adversarial examples are text inputs crafted to fool
an NLP system, typically by making small per-
turbations to a seed input!. Recent literature has
developed various adversarial attacks generating
text adversarial examples to fool NLP predictive
models *. These attack methods mainly focus on
the English language alone, building upon compo-
nents that use language-specific resources, such as
English WordNet (Miller, 1995) or BERT models
(Devlin et al., 2018a) pretrained on English corpus.

‘Most existing work attempts to perturb an input using
character-level (Ebrahimi et al., 2017a; Gao et al., 2018; Pruthi
et al., 2019; Li et al., 2018) or word-level perturbations (Alzan-
tot et al., 2018; Jin et al., 2019; Ren et al., 2019; Zang et al.,
2020) to fool a target model’s prediction in a specific way.

"We use “natural language adversarial example”, “text ad-
versarial example” and "adversarial attacks" interchangeably.

Literature has seen a growing need for NLP solu-
tions in other languages; therefore, evaluating NLP
solutions’ robustness via adversarial examples is
crucial. We ask an immediate question: "Can we
extend the SOTA adversarial attacks in English to
other languages by replacing those English-specific
inner components with other languages’ resources
?". For instance, we can attack a Chinese NLP
model by replacing WordNet with HowNet (Dong
et al., 2010). However, it is unclear if such a work-
flow is sufficient for generating high-quality ad-
versarial examples, when a target language differs
from English. In this work, we attempt to answer
this question by adapting SOTA word substitution
attacks designed for English to evaluate Chinese
NLP models’ adversarial robustness. Moreover,
we introduce morphonym and homophone word-
substitution attacks that are specific to the Chinese
language; they function as a benchmark to the En-
glish adapted attack methods.

Our experiments on Chinese classification and en-
tailment models show that both the English-adapted
and Chinese-specific attack methods can effectively
generate adversarial examples with good readabil-
ity. The attack success rates of homophone-based
and HowNet-derived methods are significantly bet-
ter than the success rate of masked language model-
based attacks or morphonym-derived attacks. We
then combine the four attacks mentioned above
into a composite attack that further increases the
attack success rate to 96.00% in fooling Chinese
classification models and 98.16% in attacking en-
tailment models. In addition, we demonstrate that
adversarially trained models significantly decrease
attack success rate by up to 49.32%.

2 Method

Recent NLP literature includes a growing body of
works on adversarial examples in NLP, mostly in
English (more background details are in Section A).
Most SOTA English adversarial attacks search for a


perturbation to change a given seed input x into an
adversarial example x’; x’ fools a predictive NLP
model and satisfies certain language constraints,
like preserving the same semantical meaning as
x. Essentially each adversarial attack algorithm
has four components: a goal function, a set of con-
straints, a suite of transformations, and a search
algorithm (Morris et al., 2020b). The search algo-
rithm attempts to find a sequence of transforma-
tions that results in a successful perturbation. The
goal function can be like fooling a target model
into predicting the wrong classification label.

Related literature: While most NLP adversarial at-
tacks have focused on the English language, a few
recent methods have been proposed for Chinese.
Zhang et al. (2020) proposed a black-box attack
that performs a glyph-level transformation on the
Chinese characters. Related, Li et al. (2020a) and
Zhang et al. (2022) added phonetic perturbations
to improve the adversarial robustness of Chinese
NLP models. All three attacks, however, are only
applicable to the Chinese language. Another study
(Wang et al., 2020) proposed a white-box attack
against BERT models (Devlin et al., 2018b) that
performs character-level swaps using gradient op-
timization. These character-level attacks extend
poorly to other languages and tend to generate out-
of-context partial substitutions that impact fluency.
Later studies, such as Shao and Wang (2022) and
Wang et al. (2022), included semantic-based word
substitutions but did not consider the significance
of constraints and adversarial training. We choose
to generalize SOTA word synonym substitution at-
tacks in English to the Chinese language (due to the
prevalence of word substitutions) and our attacks
consider a range of language constraints.

2.1 Determining Text Segmentation

The first step to crafting a new adversarial attack
for the Chinese language is to select the level of
transformation. Unlike English, which separates
words with space, the Chinese language lacks na-
tive separators to determine different words in a
sentence. A Chinese character may represent a
word, while longer words may include multiple ad-
jacent Chinese characters. To avoid out-of-context
perturbations that replace partial components of a
multi-character word, we use a Chinese segmenta-
tion tool provided by Jieba > to segment an input
text into a list of words.

https: //github.com/fxsjy/jieba

2.2 General Overview of Proposed Attacks

The general perturbation strategy we propose is
word synonym substitutions. Given an input text
x, we use the aforementioned segmentation tool to
segment x into [71, 72,..., 2]. Subsequent trans-
formations (synonym substitution) are then getting
applied to each eligible word *. This means we ob-
tain perturbed text x’ by replacing some «; with its
synonym 2. Our attack goal is to make the model
mis-predict the x’ (i.e. F(x) 4 F(x’)), > which is
also called an untargeted attack. If one substitution
is not enough to change the prediction, we repeat
the steps to swap another x; to generate the per-
turbed text x’. This process essentially solves the
following objective:

Find x’ = wordSubstitution(x)
x’ € X, F(x) = arte
A Ci(x, x’; 6), Vi € {1,2,...,C}

()

Here C},...,C denotes a set of language con-
straints including like semantic preserving and
grammaticality constraints (Morris et al., 2020b).
€; denotes the strength of the constraint C;.

The critical component " wordSubstitution(x)" in
Eq. (1) requires us to figure out what words in x to
perturb first, and what words as next. Essentially
this is a combinatorial search issue. Literature in-
cludes different search strategy (see Section B.1
for details). We adapt the greedy with word im-
portance ranking based search algorithm here. Our
attack chooses the order of words by estimating the
“importance” of each x; in x. The importance of x;
is computed by replacing each x; with an UNK to-
ken and then calculating the change in the model’s
confidence on the original label. Essentially we
sort words x; in x by the decreasing importance
regarding the following score:

score(2;) = 1 — Prob(F(X’))yorig (2)
s.t. x’ = replace(x, 2;, UNK)

This measures how much the target model’s confi-
dence decreases regarding the original label class

‘In this paper, the phrase "Chinese characters" refers to
one unit long token, and "Chinese words" refers to one or more
Chinese characters in their semantically correct segmentation
that may or may not be one unit long.

*Here F : X — J denotes a predictive Chinese NLP
model taking Chinese text as input. V denotes the input space
and JY is the output space.


Yorig When replacing x; with "UNK" token. Then
for each selected x;, we find its best x, to swap
with, from a candidate synonym set (Section 2.3)

2.3. Generating Synonyms for Words

Now for a selected word x; in x, we propose four
different Chinese word transformation strategies
to perturb a word 2; into 2, through the following
word transformations:

We design the first two transformations by adapting
from English attack studies (Jin et al., 2019) and
(Garg and Ramakrishnan, 2020).

* Open HowNet. Open HowNet (Qi et al., 2019) is
a sememe-based lexical dataset that is consisted
of a sememe set and the corresponding phrases
annotated with different sememes. A sememe is
defined as the minimum semantic unit in a lan-
guage, and Open HowNet incorporates relations
between sememes to construct a taxonomy for
each sememes. The semantic similarity between
two words can be calculated by comparing their
annotated sememes. In our study, we use Open
HowNet to generate synonyms by searching the
top five words with the highest semantic similar-
ity with an input Chinese word.

¢ Masked Language Model. We adapt the masked
language model (MLM) method to generate per-
turbations based on the top-K predictions by a
MLM. The XLM-RoBERTa model (Conneau
et al., 2019) was used as the MLM in our study,
as it is able to predict Chinese words consisting
of multiple characters to preserve the fluency of
the attacked sentence better, in comparison to
other prevalent MLM (mac-bert, etc.) that pre-
dicts single characters alone.

The Chinese language, along with other Eastern
Asian languages, differs from English, especially
in phonology and morphology.° Using these in-

Each Chinese character represents a monosyllabic word
with unique combinations of pictographs, while English words
consist of alphabetic letters. Though each Chinese character’s
morphology combination is unique, many characters with sim-
ilar morphology structures can be substituted in an adversarial
attack without impacting the readability of the attacked sen-
tence. In addition, because there exist many homophones in
modern Chinese, the same spoken syllable may map to one
of many characters with different meanings. The phonology
of Chinese characters is commonly transcribed into the Latin
script using Pinyin. Typing the wrong character of a word in
Pinyin despite having the same pronunciation is a common
mistake in Chinese writing. Thus, replacing Chinese charac-
ters with the same pronunciation may serve as an additional
attack method to test the adversarial robustness of NLP models
while preserving the semantics for human readers.

tuitions, we design two special word transforma-
tions considering phomophones and morphonyms
of Chinese language.

¢ Homophone transformation. Since the phonol-
ogy of Chinese characters can be expressed by
the romanization system Pinyin. To replace a Chi-
nese character with its homophone, top-k words
are randomly selected from a list of characters
with the same Latin script.

¢ Morphonym transformation. Similarly, to re-
place a character with its morphonyms, top-k
words are randomly selected from a list of charac-
ters that share partial pictographs with the target
character, as it is a common mistake for Chinese
writers to mistaken one pictograph with another.

¢ Composite transformation. We also design a
composite transformation that consists of the four
transformation methods listed above. For each
target word, Open HowNet, Masked Language
Model, Homophone, and Morphonym perturba-
tions are separately generated to replace a can-
didate word from the input text. If none of the
substitutions changes the target NLP model pre-
diction, the attack then move on to replace the
next important word in the input sentence.

In addition, for each perturbation, we want to en-
sure that the generated x’ preserves the semantic
consistency and textual fluency of x. We use three
constraints, namely (1) constraint to allow only
non-stop word modification, (2) constraint to allow
only no-repeat modification, and (3) multilingual
universal sentence encoder (MUSE) similarity con-
straint that filter out undesirable replacements (Cer
et al., 2018) ’. These constraints can easily adapt
to other languages. A detailed description of each
constraint is in Section B.2. The pseudo-code of
our proposed attacks is in Algorithm 1.

In summary, each word transformation strategy
gets combined with the greedy word ranking algo-
rithm (Section 2.2) plus the language constraints
(see above), making a unique adversarial attack
against Chinese NLP.

3. Results and Evaluation

Victim Models: We chose to perform attacks on
two Chinese NLP models: one for sentiment clas-
sification and one for entailment. BERT and
RoBERTa as selected as our victim models due to
their reported robustness and SOTA performance.

™We require that the MUSE similarity is above 0.9.


Classification

® Homonym

© MLM
e Composite
e
Morphonym ®@

Open Hownet

(e) 20 40 60 80 100
Attack Success Rate %

Entailment

MLM

4 e

e Homonym

Composite

Morphonym

Open Hownet

Fluency

0 20 40 60 80 100 120
Attack Success Rate %

Figure 1: The performance of composite attack method with STM-RM-MUSE constraint regarding the attack success rate and
human-evaluated fluency on BERT classification model (left), and RoBERTa entailment model (right). For both classification
and entailment tasks, composite transformation achieves the highest attack success rate without a significant trade-off in fluency,
while morphonym transformation has the lowest attack success rate.

Details of the two models and its related two Chi-
nese datasets are presented in Section C.2.

Metrics: For each attack method, we recorded the
attack success rate and perturbation percentage,
skipping samples that a target model fails to predict
correctly before any perturbation.

Ablation: To measure how MUSE constraint im-
pact the quality of Chinese adversarial examples,
we add baseline attacks that use only the stop word
constraint and repeat constraints for ablation study.

Figure 1 connects attack success rate and fluency
in one figure. Figure 2 and Figure 3 show few Chi-
nese adversarial examples generated by our attacks.
More results can be found in Section C.3

Results on Attack Success: Figure 1, Table 1 and
Table 2 present the quantitative results of our at-
tacks. Figure 1 (left) is about our results on Chi-
nese sentiment classification model. Among all
non-composite-transformation based attacks, we
can see that Open HowNet substitution achieves
the highest success rate, while morphonym substi-
tution has the lowest success rate. From Table 1,
we can also see that having the MUSE constraint
dramatically decreases the attack success rate and
perturbation percentage for all attack methods, es-
pecially for Open HowNet and homonym substitu-
tions based attacks. This makes sense as the MUSE
constraint is designed to limit the amount of per-
turbation the attacks can do to improve the qual-
ity of generated adversarial example. In addition,
when we compare the success rate and perturba-
tion percentage of composite attack versus other
individual attack methods, we see that composite
attack achieves a 87.50% attack success rate with-
out increasing the perturbation percentage. We can
make similar conclusions from Figure 1 (right) and

Table 2.

Human Evaluation: For each of the attack method,
we randomly sampled 30 adversarial examples pro-
duced from the same set of input texts for each
attack (a total of five). We asked four volunteers to
score the semantic consistency and fluency of the
examples. Semantic consistency refers to how well
the ground truth label of the adversarial example
matches with the original label of the input, and
fluency refers to the cohesiveness of the sentence.
Both metrics are scored on a scale of 1 to 5, witha
score of 5 being the most consistent or fluent.

Table 3 and Table 4 respectively summarize the hu-
man evaluations of adversarial examples generated
by fooling classification and entailment models.
For classification, Table 3 (plus Figure 1) shows
that homonym substitution outperforms other at-
tack methods, as its examples have both the high-
est consistency and fluency scores. On the other
hand, Open HowNet substitution reports the lowest
quality scores, indicating its generated adversarial
examples either include out-of-context substitution
or disrupt the cohesiveness semantics. Table 3,
plus Figure 1(right) for entailment tasks, shows
that homonym attack still achieves the highest con-
sistency score, while MLM achieves the highest
fluency score. Besides, we conjecture that the low
consistency and fluency scores of the composite at-
tack method may root to its inclusion of adversarial
examples generated by Open HowNet.

Adversarial training and more result discus-
sions: Furthermore, we conduct adversarial training
(AT) (see details in Section C.1). Table 5 shows the
positive results of AT that improve the robustness
across all five proposed attacks over both models.


References

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial exam-
ples. arXiv preprint arXiv: 1804.07998.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-
Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018.
Universal sentence encoder. CoRR, abs/1803.11175.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer,
and Veselin Stoyanov. 2019. Unsupervised cross-
lingual representation learning at scale.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018a. Bert: Pre-training of deep
bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018b. Bert: Pre-training of deep
bidirectional transformers for language understanding.
In NAACL-HLT.

Zhendong Dong, Qiang Dong, and Changling Hao.
2010. Hownet and its computation of meaning. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, COLING
°10, page 53-56, USA. Association for Computational
Linguistics.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2017a. Hotflip: White-box adversarial examples
for text classification. arXiv preprint arXiv: 1712.06751.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2017b. Hotflip: White-box adversarial examples
for text classification. In ACL.

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun
Qi. 2018. Black-box generation of adversarial text se-
quences to evade deep learning classifiers. 2018 IEEE
Security and Privacy Workshops (SPW), pages 50-56.

Siddhant Garg and Goutham Ramakrishnan. 2020.
BAE: BERT-based adversarial examples for text clas-
sification. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 6174-6181, Online. Association for
Computational Linguistics.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adversar-
ial examples. arXiv preprint arXiv: 1412.6572.

Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei
Wei, Wen-Lian Hsu, and Cho-Jui Hsieh. 2019. On the
robustness of self-attentive models. In Proceedings of
the 57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1520-1529, Florence, Italy.
Association for Computational Linguistics.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Szolovits. 2019. Is bert really robust? natural language
attack on text classification and entailment. ArXiv,
abs/1907.11932.

Jinfeng Li, Tianyu Du, Shouling Ji, Rong Zhang, Quan
Lu, Min Yang, and Ting Wang. 2020a. Textshield: Ro-
bust text classification based on multimodal embedding
and neural machine translation. In USENIX Security
Symposium.

Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting
Wang. 2018. Textbugger: Generating adversarial
text against real-world applications. arXiv preprint
arXiv: 1812.05271.

Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,
and Xipeng Qiu. 2020b. BERT-ATTACK: Adversarial
attack against BERT using BERT. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 6193-6202, Online.
Association for Computational Linguistics.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39-41.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
and Pascal Frossard. 2016. DeepFool: a simple and
accurate method to fool deep neural networks. In JEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR).

John Morris, Eli Lifland, Jin Yong Yoo, and Yanjun Qi.
2020a. TextAttack: A framework for adversarial attacks
in natural language processing. ArXiv, abs/2005.05909.

John X. Morris, Eli Lifland, Jack Lanchantin, Yangfeng
Ji, and Yanjun Qi. 2020b. Reevaluating adversarial
examples in natural language.

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z Berkay Celik, and Ananthram Swami.
2016. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and
Privacy (EuroS&P).

Danish Pruthi, Bhuwan Dhingra, and Zachary C Lipton.
2019. Combating adversarial misspellings with robust
word recognition. arXiv preprint arXiv: 1905.11268.

Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Qiang Dong,
Maosong Sun, and Zhendong Dong. 2019. Openhownet:
An open sememe-based lexical knowledge base. arXiv
preprint arXiv: 1901.09957.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial exam-
ples through probability weighted word saliency. In
Proceedings of the 57th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1085-1097,
Florence, Italy. Association for Computational Linguis-
tics.

Yuyao Shao and Liming Wang. 2022. Gpsattack: A uni-
fied glyphs, phonetics and semantics multi-modal attack


against chinese text classification models. In 2022 Inter-
national Joint Conference on Neural Networks (IJCNN),
pages 1-8.

Zhouxing Shi and Minlie Huang. 2020. Robustness to
modification with shared words in paraphrase identifica-
tion. In Findings of the Association for Computational
Linguistics: EMNLP 2020, pages 164-171, Online. As-
sociation for Computational Linguistics.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199.

Boxin Wang, Boyuan Pan, Xin Li, and Bo Li. 2020.
Towards evaluating the robustness of chinese bert clas-
sifiers.

Boxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, and
Bo Li. 2022. SemAttack: Natural textual attacks via dif-
ferent semantic spaces. In Findings of the Association
for Computational Linguistics: NAACL 2022, pages
176-205, Seattle, United States. Association for Com-
putational Linguistics.

Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,
Meng Zhang, Qun Liu, and Maosong Sun. 2020. Word-
level textual adversarial attacking as combinatorial opti-
mization. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, pages
6066-6080, Online. Association for Computational Lin-
guistics.

Xiang Zhang and Yann LeCun. 2017. Which encoding
is the best for text classification in chinese, english,
japanese and korean?

Zihan Zhang, Jinfeng Li, Ning Shi, Bo Yuan, Xiangyu
Liu, Rong Zhang, Hui Xue, Donghong Sun, and Chao
Zhang. 2022. RoChBert: Towards robust BERT fine-
tuning for Chinese. In Findings of the Association for
Computational Linguistics: EMNLP 2022, pages 3502-
3516, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.

Zihan Zhang, Mingxuan Liu, Chao Zhang, Yiming
Zhang, Zhou Li, Qi Li, Haixin Duan, and Donghong
Sun. 2020. Argot: Generating adversarial readable chi-
nese texts. In Proceedings of the Twenty-Ninth Interna-
tional Joint Conference on Artificial Intelligence, IJCAI-
20, pages 2533-2539. International Joint Conferences
on Artificial Intelligence Organization.

4 Limitations

We are optimistic that the algorithmic workflow
presented in this paper can be generalized to other
languages. When the victim models are in lan-
guages other than Chinese and English, however,
we also acknowledge the uncertainty in achieving
a high attack success rate while at the same time
achieving fluency in generated examples. In addi-
tion, because of the variation in linguistic structures

across different languages, further efforts are re-
quired to design language-specific transformation
methods (such as the homophone and morphonym
transformations for the Chinese language case in
this paper).

5 Ethics Statement

In this study, we honor the ethical code in the ACL
Code of Ethics.

A Background: NLP Adversarial Attacks

Adversarial examples are inputs crafted to fool
a machine learning system, typically by making
small perturbations to a seed input (Szegedy et al.,
2013; Goodfellow et al., 2014; Papernot et al.,
2016; Moosavi-Dezfooli et al., 2016). The study of
natural language processing (NLP) in adversarial
environments is an emerging topic as many on-
line platforms provide NLP based information ser-
vices, like toxic content detection, misinformation
or fake news identification. These applications
make NLP frameworks potential targets of adap-
tive adversaries.

Adversarial attacks aim to use a set of transfor-
mations 7}...7;, to perturb a correctly predicted
instance, x € A, into an adversarial instance
x’. Attacks normally define a goal function
FoolGoal(F, x’) that represents whether the goal
of the attack has been met, for instance, indicating
if the prediction F(x’) differs from F(x). Attacks
in NLP normally needs another set of Boolean func-
tions C;...C’, indicating whether the perturbation

satisfies a certain set of language constraints.

Initial studies on NLP adversarial attacks per-
formed character-level perturbations to create mis-
spellings (Ebrahimi et al., 2017a; Gao et al., 2018;
Pruthi et al., 2019; Li et al., 2018). Recent later
works have explored various word substitutions
methods to produce adversarial examples in natu-
ral language. Both Alzantot et al. (2018) and Jin
et al. (2019) use counter-fitted word embeddings
to find synonyms while Ren et al. (2019) and Zang
et al. (2020) use lexical databases like WordNet
(Miller, 1995) and HowNet (Dong et al., 2010).
Lately, masked language models have been used to
perform word substitutions to preserve fluency of
the perturbed text better (Li et al., 2020b; Garg and
Ramakrishnan, 2020; Shi and Huang, 2020).


Algorithm 1 Word Substitution Attack against Chi-
nese NLP Models
1: Input: Input text x

2: x = segment(x) = [1,...,2n]

3: R=ranking r1,...,T, of words 71,...%y

4: x* =x

5: for? =71,...,7n do

6: Xcandidate = T) (x, i) Us--U T(x, i)

7: eandticane = {x'|C; (2 x) VC5 €
{Cy...Cn}

8: if Xvdidate # 2 then

9: x* = arg Maxy/¢ X! name SCOT E(X )

10: if F(x*) # F(x) then

11: return x*

12: end if

13: else

14: end search

15: endif

16: end for

B More Method Details
B.1 Details on Search for Words to Perturb

Solving Eq. (1) needs the Chinese adversarial at-
tacks to conduct a combinatorial search task and
adapt search algorithms from the English adversar-
ial attacks in this paper. The search algorithm aims
to perturb a text input with language transforma-
tions such as synonym substitutions in order to fool
a target NLP model while the perturbation adheres
to linguistic constraints.

The potential search space is exponential by na-
ture. Assuming x includes n words, and each word
has S potential substitutions, the total number of
possible perturbed inputs is then (.S + 1)” — 1.
The search space of all potential adversarial exam-
ples for a given x is far too large for an exhaus-
tive search. This is why many heuristic search
algorithms were proposed in the literature, includ-
ing greedy method with word importance ranking
(Gao et al., 2018; Jin et al., 2019; Ren et al., 2019),
beam search (Ebrahimi et al., 2017b), and popula-
tion based genetic algorithm (Alzantot et al., 2018).
While heuristic search algorithms cannot guarantee
an optimal solution, they can efficiently search for
a valid adversarial example.

B.2__ Details on Language Constraints

NLP adversarial attacks generate perturbations and
use a set of constraints to filter out undesirable x’
to ensure that perturbed x’ preserves the semantics

and fluency of the original x (Morris et al., 2020a).
Therefore, we propose to use three following con-
straints:

¢ Stop word modification: Replacing the coor-
dinating conjunctions and pronouns within a
sentence often changes the semantics of a tar-
get sentence. Therefore, words such as "but"
and "I" cannot be perturbed.
¢ Repeat modification: This prevents replaced
words to be modified again, as the targeted
word may gradually diverge from its original
meaning.
Multilingual Universal Sentence Encoder
(MUSE): Using the multilingual sentence en-
coder, we encode both original x and x’ and
measure the cosine similarity between the two
text. We require that the cosine similarity is
above 0.9.

C More on Results and Setup

Attack Method Constraints Success Rate | % Perturbed
STM-RM 81.20 32.54
Open HowNet | srv-RM-MUSE 56.49 27.99
STM-RM 53.67 40.33
MEM STM-RM-MUSE 48.03 28.71
Homonvn STM-RM 74.14 54.67
y STM-RM-MUSE 49.57 38.22
Wérsbonern STM-RM 43.21 45.73
rpnony STM-RM-MUSE 31.02 36.63
Gomnosite STM-RM 96.00 41.05
Pos STM-RM-MUSE 87.50 31.74

Table 1: Attack results of classification task performed on
online-shopping review dataset. Attack success rate and
amount of perturbations of each attack. “STM-RM" stands for
stop word modification and repeat modification, and “STM-
RM-MUSE" stands for stop word modification, repeat modifi-
cation, and universal sentence encoder constraint.

Attack Method Constraints Success Rate | % Perturbed
Open Howtet STM-RM 74.94 37.35
STM-RM-MUSE 75.00 36.59
STM-RM 75.40 40.88
MLM STM-RM-MUSE 75.79 40.93
Homonym STM-RM 86.21 49.63
STM-RM-MUSE 86.21 49.58
Morphonym STM-RM 60.23 46.47
STM-RM-MUSE 59.81 46.60
Composite STM-RM 98.16 39.06
STM-RM-MUSE 98.02 38.74

Table 2: Attack results on Chinese entailment model using the
Chinanews dataset. Attacks’ setup same as Table 1.

Attack Method | Consistency(1-5) | Fluency(1-5) | A Fluency
Open HowNet 2.94 2.69 -2.31
MLM 3.41 3.70 -1.28
Homonym 4.44 4.31 -0.69
Morphonym 3.75 3.31 -1.69
Composite 3.13 2.94 -2.06

Table 3: Human evaluation of attacks on Online-shopping
dataset. We report average consistency and fluency scores
on examples generated from each attack method. STM-RM-
MUSE constraints were used for all attack methods.


Attack Method | Consistency(1-5) | Fluency(1-5) | A Fluency
Open HowNet 3.10 3.07 -1.93
MLM 3.20 3.90 -0.87
Homonym 4.43 3.57 -1.33
Morphonym 3.90 3.73 -1.27
Composite 2.97 3.43 -2.53

Table 4: Human evaluation of attacks on Chinanews dataset.
STM-RM-MUSE constraints were used for all attack methods.

C.1 Adversarial Training

Attack Method Constraints Pre Success Rate | AT Success Rate | A%
Open HowNet STM-RM 81.20 65.05 -19.89
STM-RM-MUSE 56.49 43.30 -23.35
MLM STM-RM 53.67 36.10 -32.74
STM-RM-MUSE 48.03 38.94 -18.93
Homonym STM-RM 74.14 40.83 -44.93
STM-RM-MUSE 49.57 35.86 -27.66
Morphonym STM-RM 43.21 30.51 -29.39
STM-RM-MUSE 31.02 15.72 -49.32
Composite STM-RM 96.00 77.75 -19.01
STM-RM-MUSE 87.50 60.34 -31.04

Table 5: Results of adversarial training performed on BERT
model. "Pre Success Rate" stands for the success rate of
composite attack on the pre-adversarial-trained model, and
"AT Success Rate" stands for the success rate of composite
attack on adversarial-trained model

To evaluate how adversarial examples generated
by the attack methods could improve the adversar-
ial robustness of a target model, we attacked the
target BERT model (Chinese sentiment classifica-
tion) with 1000 examples from the training set of
an online shopping review dataset, and finetune
the target model with the successfully attacked ex-
amples. The model was trained for 3 epochs with
1 initial clean epochs, learning rate of 5e-5, and
effective batch size of 32 (8x4).

Table 5 shows the positive effect of adversarial
training (AT) that improve the robustness of Chi-
nese language models against all five of our pro-
posed attacks. For instance, on the target BERT
model, attack success rate decreased by up to
49.32% after being trained by adversarial examples
generated by the Composite-MUSE attack method.
Across all attacks, the AT-trained models result
with a significant drop in attack success rate.

C.2 Victim Model and Dataset Setup

We chose to perform attacks on Chinese sentiment
classification and entailment models, and chose
BERT and RoBERTa as our victim models due
to their reported robustness against perturbations
when compared to other models such as LSTM and
CNN (Hsieh et al., 2019).

The target BERT model for Chinese sentiment clas-
sification is from Huggingface °, and the target

Shttps: //huggingface.co/Raychanan/
bert-base-chinese-FineTuned-Binary-Best

RoBERTa model for entailment was trained on the
training set of the Chinanews dataset (Zhang and
LeCun, 2017). The validation performance of the
BERT sentiment classification model is 89.80%
and is 89.71% for RoBERTa entailment model.

For both models, untargeted classification was set
as the fooling goal function and the search method
was greedy search with word-importance-ranking
as aforementioned. 500 examples were attacked by
using each attack method. Two related datasets are
as follows.

* Dataset-1: An online shopping review dataset?
for sentiment classification tasks was used to
generate attacks against the BERT classification
model, with 500 examples from the test set to
check the model’s adversarial robustness.

¢ Dataset-2: The Chinanews dataset was collected
by the glyph project (Zhang and LeCun, 2017)
and consists of the summary and first paragraphs
of news articles from chinanews.com. Each set
of summary and first paragraph was labeled with
one of 7 topic classes, including mainland China
politics, Hong Kong-Macau politics, Taiwan pol-
itics, International news, financial news, culture,
entertainment, sports, and health. We randomly
sampled 500 examples from the test set to attack
against the entailment model.

C.3 Discussion of Results

After checking the generated adversarial examples,
we realize that a leading cause behind inconsis-
tent and unnatural adversarial examples for Open
HowNet transformation is out-of-context substitu-
tions, supported by it having the highest attack suc-
cess rate yet the lowest consistency/fluency score.
Most models are sufficiently robust to attacks with
common synonyms, which means successful at-
tacks are often accomplished by distant and un-
conventional synonym substitutions. On the other
hand, cases of out-of-context word substitutions
were observed less often in the other attack meth-
ods. This is reasonable as homonym and morpho-
nym attack methods only perturb the presentation
of the substituted words without changing its se-
mantics to human, while a classification and entail-
ment models fail to attend to the context. However,
in rare cases, homonym transformations are also
prone to out-of-context substitutions as some Chi-

https ://github.com/SophonPlus/
ChineseNlpCorpus/tree/master/datasets/online_
shopping_10_cats


nese characters have multiple pronunciations. In
such scenarios, homonym attacks may result in a
false successful attack due to failures to recognize
the correct pronunciation and provide an appropri-
ate substitution.

Furthermore, we also observe that perturbing cer-
tain characters results in almost guaranteed change
in prediction, which was first reported by Wang
et al. (2020). For instance, the Chinese character
"bu" translates to "no" in English. As illustrated
by the first example in Figure 2d, when "bu" is re-
placed by its morphonym or homonym, the predic-
tion of the perturbed sentence often changes from
negative to positive, as a strong negative cue was
replaced by another character that the victim model
not yet recognizes. Similarly, in the case of entail-
ment models, when the name of a country/region
is substituted with its morphonym or homonym,
examples with region-specific labels (Hong kong-
macau politic, Mainland china politics, etc.) were
most often attacked successfully. The vulnerability
of Chinese BERT and RoBERTa models against
morphonym and homonym adversarial attacks indi-
cates that there is still a large room for improvement
in their adversarial robustness.

D_ Conclusion

In summary, we investigate how to adapt SOTA
adversarial attack algorithms to the Chinese lan-
guage. Our experiments show that the system of
generating English adversarial examples can be
sufficiently adapted to Chinese, given appropriate
text segmentation, perturbation methods, and lin-
guistic constraints. We also introduce two addi-
tional perturbation methods particular to the at-
tributes of the Chinese language. Because most
of the English/Chinese-specific components of the
workflow can be substituted with other languages
and resources, we are optimistic that the adaptation
workflow presented in this paper can be generalized
to other languages in building a language-agnostic
attack algorithm in future research.

E Qualitative Examples


1. Adversarial Examples Generated by the OpenHownet Attack Method

Input Text, x: > Negative 95%)

PREMIERES, KEM. ASEM »

The color of the screen is very terrible, do not recommend purchase. True
comment.

Perturbed Text, x’: > Positive (85%)
RROMIEB DLE, FEN. ARN.

The color of the screen is very mediocre, do not recommend purchase. True
comment.

Input Text, x: > Negative (83%)

GR, FAL, —BAA RBS, RAR, SE.
The packaging was shabby, a disappointing shopping experience, the only
star is for the delivery guy, he delivers fast with good attitude.

Perturbed Text, x’: > Positive (56%)
EM.

The packaging was shabby, a depressing shopping experience, the only star
is for the delivery guy, he delivers fast with good attitude.

(a) Adversarial examples of Open HowNet

3. Adversarial Examples Generated by the Homonym Attack Method

Input Text, x: > Negative (99%)

AY, AQEBAIA, MARE !

Not good, the scent is pungent, and the delivery is slow!
Perturbed Text, x’: > Positive (89%)

abst, ATMERIGA, mA |

“Department good”, the scent is “word nose”, and the delivery is slow!

Input Text, x: > Negative (95%)
RADARS We.

The phone system is prone to crash, sound is weak and there aren’t many
functions.

Perturbed Text, x’: > Positive (84%)

AA LMR S We.
The phone system is prone to “four machine”, sound is weak and there
aren’t many functions.

(c) Adversarial examples of Homonym

2. Adversarial Examples Generated by the MLM Attack Method

Input Text, x: > Positive (86%)

FR AMOK, PRY aciGoxlelat, SSerAy.

It feels cold when holding in hands, beside being a little small there is no
problem, pretty good.

Perturbed Text, x’: > Negative (51%)
FRAMOKES, BRO) Behe, SSeray.

It feels cold when holding in hands, beside being a little small there is
nothing wrong, pretty good.

Input Text, x: > Negative (86%)
SAMA, RIA PS
SF, BStee.
There is no accessible socket , need to unplug the socket under the lamp.

There is no WiFi. Only staying to catch early flight in the morning.

Perturbed Text, x’: > Positive (62%)

RAPA AGE, MICK NAAR TDR. 1
BARK, REE.
There is no accessible socket , need to unplug the socket under the lamp.
There is no WiFi. Only staying to arrive earlier.

i

ty

AIRE TR. RAWIFI, AT iL

WiFi. ATH

(b) Adversarial examples of MLM
4, Adversarial Examples Generated by the Morphonym Attack Method

Input Text, x: > Positive (97%)

tKmais 1, LASIMAT .
The product is amazing, there’s no need to further introduction.

Perturbed Text, x’: > Negative (61%)

Rrxs Tl, SAS .
The product is amazing, there’s rejecting need to further “price
introduction”.

Input Text, x: > Positive (54%)
BRT UW, RABRIAAA, RICRAIEMIZA «

The appearance is good, not as red as apples are, hayen’t tasted it to know
whether it tastes good.

Perturbed Text, x’: > Negative (72%)
BERTI, RABRXAA, ENAAEIZA.

The appearance is good, not as red as apples are, “throw” tasted it to know
whether it tastes good.

(d) Adversarial examples of Morphonym

Figure 2: Selected Adversarial Examples generated by proposed adversarial attacks on the online shopping review dataset
(classification). Note for adversarial examples generated by the Homonym and Morphonym attack method (figure 2c and 2d),
word substitutions are based on Chinese language characteristics instead of semantic meaning. For examples in figure 2c,
substitutions were chosen from characters with similar sounds. For figure 2d, substitutions were from characters that look similar
to human readers.


1. Adversarial Examples Generated by the OpenHownet Attack Method

Input Text, x: > Entertainment (67%)
AMGBBSSA BSA, BS CEE) .

Japanese anime director Toyoo Ashida passed away, who directed the show
Fist of the North Star.

1

Perturbed Text, x’: > Culture (60%)

AMGBRAA BSA, BS CIEE) .
Japanese anime first chair (in band) Toyoo Ashida passed away, who
directed the show Fist of the North Star.

im

Input Text, x: > Financial News (66%)

PRR EAS) FEA .4A (ZI, SHRS3{Z «
Chinese Railway holds debts of 3.4 trillion in the first half of this year, with
a gross loss of 5.3 billion.

Perturbed Text, x’: > Mainland China Politics (51%)

PRR RAs) LEZ R3.401Z7, SHRS3(Z.
Chinese Railway supposedly owes 3.4 trillion in the first half of this year,
witha gross loss of 5.3 billion.

T
KH

is
KH

(a) Adversarial examples of Open HowNet
3. Adversarial Examples Generated by the Homonym Attack Method
Input Text, x: > Hong kong - macau politics 95%)
RECEARGAAE, TRBEKKAS.

Olympic champion ends trip in Australia, was welcomed by city residents.

Perturbed Text, x’: > Sports (86%)

RECEARGRAE, TRBEKKAS.

Olympic champion ends trip in “persistence”, was welcomed by city
residents.

Input Text, x: > Culture (52%)
SEUET, “ARH RT .

New Year is always over, but the “spirit of the new year” should remain.

ki
71

Perturbed Text, x’: > Sports (59%)
FEWEST, “ARH RZET.

71

hy

New Year is always over, but the “spirit of the new year” win should remain.

(c) Adversarial examples of Homonym

2. Adversarial Examples Generated by the MLM Attack Method

Input Text, x: > Sports (100%)

VOLVO BAF ERE , REIZF200075.

The Chinese Open, hosted by Volvo, was attended by various talents and
public figures, with the prize raised to 20 million.

Perturbed Text, x’: > Entertainment (92%)

VOLVO# Ee Bt , REEBI20007.

The Chinese Show, hosted by Volvo, was attended by various talents and
public figures, with the prize raised to 20 million.

Input Text, x: > Enterteinment (95%)
CAT GI) FOS Rs i ILE »

The “floating cloud” scene in the movie Personal Tailor was criticized for
its similarity to Zhisong Cai’s work.

Perturbed Text, x’: > Culture (99%)
(ATH) FOB BARE ao

The “floating cloud” drawing in the movie Personal Tailor was criticized for
its similarity to Zhisong Cai’s work.

(b) Adversarial examples of MLM

4. Adversarial Examples Generated by the Morphonym Attack Method

Input Text, x: > Mainland china politics (91%)

ARBR: Tie, AAA.
China Daily: Promoting frugality with force is everyone’s responsibility.

Perturbed Text, x’: > Culture (88%)

ARBR: BT, AAA.
China Daily: Promoting “saving fetch” regularly is everyone’s
responsibility.

Input Text, x: > Financial news (84%)

BiB Se ERS ERR.
Korean lottery sales reached supply for the second straight year.

H

Perturbed Text, x’: > Culture (77%)
#5 BER SOE A BIA EPR.

Korean “notice ticket” sales reached supply for the second straight year.

i

(d) Adversarial examples of Morphonym

Figure 3: Selected Adversarial Examples generated by proposed adversarial attacks on the Chinanews dataset (entailment task).
As mentioned in the discussion section, substituting specific characters almost guarantees a change in the prediction result. As
shown by the second example in figure 3c, the homonym substitution of the word "should" added the new semantic meaning of

"winning", which is a strong cue for the Sports category.
