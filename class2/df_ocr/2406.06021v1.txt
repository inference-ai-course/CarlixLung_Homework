arX1v:2406.06021v1 [cs.CL] 10 Jun 2024

Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP
Research

Surangika Ranathunga!, Nisansa de Silva’, Dilith Jayakody”, Aloka Fernando?
'School of Mathematical and Computational Sciences, Massey University, New Zealand
s.ranathunga@massey.ac.nz
*Dept. of Computer Science & Engineering, University of Moratuwa,10400, Sri Lanka
{NisansaDdS, dilith.18,alokaf}@cse.mrt.ac.1lk

Abstract

We analysed a sample of NLP research papers
archived in ACL Anthology as an attempt to
quantify the degree of openness and the benefit
of such an open culture in the NLP community.
We observe that papers published in different
NLP venues show different patterns related to
artefact reuse. We also note that more than
30% of the papers we analysed do not release
their artefacts publicly, despite promising to
do so. Further, we observe a wide language-
wise disparity in publicly available NLP-related
artefacts.

1 Introduction

The advancement of the Computer Science re-
search field heavily depends on publicly available
code, software, and tools. Its sub-fields Machine
Learning and Natural Language Processing (NLP)
have the additional requirement of datasets - to train
and evaluate computational models. Lack of access
to these research artefacts has been identified as
a major reason for the difficulty in reproducing
works of others (Pineau et al., 2021). The data
requirement is particularly challenging in NLP - a
dataset available for one language usually cannot
be used in the context of another language!.
Therefore, the NLP community is highly encour-
aged to make their research artefacts publicly avail-
able. However, as far as we are aware, there is no
quantifiable evidence on (1) the degree of open-
ness in the NLP community or (2) the benefit of
openness to the community. Since “what we do
not measure, we cannot improve” (Rungta et al.,
2022), in this paper, we quantify both these as-
pects. To this end, we semi-automatically analyse
a sample of NLP research papers published in ACL
Anthology (AA) and corpora/ Language Models

‘Other than in techniques such as multi-tasking and
intermediate-task fine-tuning.

(LMs) released in Hugging Face”, and answer the
following questions:

1. To what degree has the NLP research commu-
nity been able to reuse open-source artefacts
(data, code, LMs) in their research?

2. How much has the community freely shared
the artefacts produced by their research?

To answer the first question, we record the num-
ber of papers that reuse the artefacts released by
past research. Since there is a language-wise dispar-
ity in NLP research (Joshi et al., 2020; Ranathunga
and de Silva, 2022), this analysis is conducted
while separating low- and high-resource languages.

To answer the second question, we record the
papers that indicate they would release the newly
produced artefacts. We also record whether they
have provided a repository URL. We do further
analysis to find out whether these repositories have
the artefacts they are supposed to have. Finally, we
record the number of datasets and LMs available
for different language classes on Hugging Face.

We observe that papers published in different
venues show different patterns in artefact reuse. We
also observe that a worrying percentage of papers
that produced an artefact have not publicly released
those artefacts. To a lesser degree, broken repos-
itory links and empty resource repositories were
also noted. Finally, it is noted that the language-
wise disparity in LM/data availability (Joshi et al.,
2020; Ranathunga and de Silva, 2022; Khanuja
et al., 2023) is still staggering.

2 Data Extraction

We use AA as the research paper repository.
While AA is the largest NLP-related paper repos-
itory, Ranathunga and de Silva (2022) note that
many papers related to low-resource languages also

“https ://huggingface.co/


lm Introduced new artefact
(mm Artefact not needed/considered
M@m™—™ Can't determine

ll Used others’ artefact as it is
l@™@_ Based on extended/modified artefact
@™ Used others' artefact + added their own part

or
co
°
rd
or
co

Fraction of papers

eccocoscoor
SENY RAYS

Fraction of papers
Fraction of papers

escosesco
SEN RUOUOe

4 12 3 4
Language Class

(c) LMs - LREC

12 3 4
Language Class

(b) Code - LREC

12 3 4
Language Class

(a) Data - LREC

or
wo
rs

or
wo

Fraction of papers

ecoscosos
SeNWRUOUe

Fraction of papers
Fraction of papers

escscesos
SbNURUOLe

CL 2 3 4S C1 2 S45 ob 2 34
Language Class Language Class Language Class

(d) Data - Main (e) Code - Main (f) LMs - Main

or
wo
or
wo

&

Fraction of papers
e°00000090
SPNUBUAUe

Fraction of papers

Fraction of papers
e0009009000
SPNUaUAUEO

o Le 3 4-5 a a a es ¢ 4 2 2 4 5
Language Class Language Class. Language Class

(g) Data - Other (h) Code - Other (i) LMs - Other

Figure 1: Artefact (Data, Code, and LMs) creation,
extension, and reuse across PVs.

get published in other venues such as IEEE confer-
ences or regional journals. However, the popularly
used Google Scholar does not have a free API to
extract data, and the coverage of Semantic Scholar
is rather poor*. Moreover, some conference and
journal publications are hidden behind paywalls.
While archives such as arXiv are a possible option,
they do not contain the meta data for us to carry
out a conference/journal-specific analysis. Consid-
ering all these factors, we selected AA to extract
papers for our analysis. AA has been the common
choice for many research related to diversity anal-
ysis in NLP research (Rungta et al., 2022; Blasi
et al., 2022; Cains, 2019).

When collecting data from AA, we reuse data
and code from Ranathunga and de Silva (2022)
who in turn had used code and data from Blasi et al.
(2022) and Rohatgi (2022) (respectively). However,
we had to collect data post 2022 by ourselves.

We use the URLs of papers from the ACL An-
thology Bibliography to extract the title and ab-
stract of each paper. We then allocate the papers to
different languages, following the language list (of
6419 languages) given by Ranathunga and de Silva
(2022). For each language name, we check for
matches in both the title and abstract and download
the matched papers using their respective URLs
(where a URL to the PDF is available). Of these,

>For example, the search query "english+nlp" returns 4312

results on Semantic Scholar as opposed to the 495,000 results
returned by Google Scholar.

130 languages are ignored due to the high count
of false positives caused by matches with existing
words and author names*. Next, we convert each
paper to its text format.

Then we further group these language-wise pa-
pers according to language category. The com-
monly used language category definition that is
based on language resources is Joshi et al. (2020)
(see Table 4 in Appendix). This definition can be
used to categorise languages into six classes, with
class 5 being the highest resourced, and class 0 be-
ing the least resourced. Joshi et al. (2020) used this
definition to classify about 2000 languages. How-
ever, this categorisation was conducted in 2020
and it has considered only ELRA* and LDC® as
data repositories. Ranathunga and de Silva (2022)
showed that these repositories have very limited
coverage for low-resource languages. They reused
Joshi et al. (2020)’s language category definition
and categorised 6419 languages considering the
Hugging Face data repository in addition to ELRA
and LDC. In this research, we use this newer lan-
guage categorisation.

3 Analysis

3.1 The degree of artefact reuse in NLP
research

We extract a paper sample of 355 (papers published
between 2015-2023) from the dataset downloaded
above. To analyse the effect of the publishing
venue, these papers are then separated into three
categories (henceforth referred to as PV categories).
These categories are selected based on the sugges-
tion of Ranathunga and de Silva (2022).

¢ Main: Main ACL conferences/journals where
NLP researchers publish (Full list in Ap-
pendix B).

¢ LREC (Language Resources and Evaluation
Conference). It was given a separate category
as it is a venue specifically focusing on lan-
guage resources.

¢ Other - Everything else. Usually, these PVs
refer to shared tasks, workshops and regional
conferences such as RANLP and ICON.

*Examples of languages that were ignored include: Are,
As, Even, One, So, To, Apache, U, Bit, She.

Shttp://www.elra.info/en/

Shttps: //www. ldc.upenn.edu/


Promised to release Promised to release Promised to release

~ ee | 25 117 vata only gas 13 Data only m7

39 Data only “2 caer ——— 6

= ite. 116 Code only m3 3 Model only ee 7

= 4 ——— eee =2

10 Code only, LU | 1-Model only 3 Data and Code only. wd
— a8 =3 2. Code and Model only

108 LREC T |
_ 20 Data and Code onty- -~2

mein

_ 17 Data and Code only. aj 1 Data and Model only..

113 main Go. 4 Data, Code, and Model
112 Data, Code, and Modal 16 0 7/othes ~

3 Data, cote cna ae 16 | ~ 4 XQ :
~ es 4 te. 72 ~~
36 “ 20 ly 12 ee 4
—3
(a) LREC (b) Main (c) Other

Figure 2: Artefact releasing promise vs artefact link availability across PVs. Green - Artefact Released, Red -
Claimed to release the relevant artefact but no link given, Purple - No promise was given to release any artefact.

Mmm Yes

Mam No

Mm Can't release

0 12 3 4 5 “0 1 2 3 4 °5
Language Class Language Class

(a) Availability of Data (b) Availability of Code

Figure 3: Analysis on artefact release.

Artefact Status

Used dataset from some previous research

Extended an existing dataset

Used dataset from some previous research but created new data as well
Introduce new dataset

Data not needed

Cannot determine

Data

Table 1: Possible options for use, and reuse of data

For each PV, the resulting paper sample has 20
papers per language class’. We manually read
each of these papers to find out whether they cre-
ated/used data, code® and/or LMs?. The possible
options for data-related mentions in a paper are
shown in Table 1. Similar options are considered
for code and LMs (see Table 5 in Appendix). Note
that the first three entries in Tables 1 and 5 sug-
gest the reuse of artefacts from previous research
in some manner.

Out of the 355 papers we analysed, 98.9% has
reused some form of artefact from previous re-
search. Further language class-wise analysis on
this is shown in Figure | (In the Appendix we have
a larger version in Figure 5 as well as a chronologi-
cal breakdown of the data in Figure 6).

Except for language class 1 in Main PV, where we could
find only 15 papers.

’We considered NLP related tools/libraries/code reposi-
tories such as NLTK and Huggingface libraries but did not
consider generic libraries such as Pandas.

°By LMs, we refer to LMs starting from Word2Vec, GloVe
and FastText, coming to currently used Large LMs

0 12 3 4 °5 “0 1 2 3 4 ~°5

Language Class Language Class

(c) Availability of LM(s) (d) Availability of Tool(s)

Other PV category is the highest in reusing data
as-it-is. This is not surprising, as this category has
many papers referring to shared tasks. Main cate-
gory also uses existing data as-it-is to a higher de-
gree, but there is some emphasis on data extension
as well. LREC, due to its focus on language re-
sources, sees more papers introducing new datasets
or extending existing datasets than those that reuse
existing data as-it-is.

The Main category sees the highest level of code
reuse to introduce new implementations - most pa-
pers extend code from already existing research.
This has to be due to the highly competitive nature
of PVs in this category, where reviewers emphasise
technical novelty. Other PV category is high in
reusing code as well, but it has a relatively higher
portion of papers using existing code as-it-is.

As mentioned earlier, since most LREC papers
focus on dataset release, they seem not to have paid
attention to the use of state-of-the-art solutions in-
volving LMs. In contrast, papers from Main heav-
ily emphasise using LMs, and this PV category
seems to be the venue to introduce new LMs.

Overall, the most reused artefact is code, span-
ning from early APIs/toolkits such as NLTK (Bird
et al., 2009) and Kaldi (Povey et al., 2011) to
modern-day Hugging Face libraries.


3.2 Percentage of papers that promise to
share the newly created artefacts

Next, we focus on papers that create new artefacts
(created from scratch or extended existing artefacts)
and report the percentage of papers that promise
to share the newly created artefacts. If they do
promise, then we check whether they have provided
the URL of the public repository containing the
artefact(s).

This analysis was done in a semi-automated man-
ner on the same 355 paper sample as before, using
a keyword-based method to filter papers.

To identify keyword matches, we first replace
all non-letter characters of the paper full text with
spaces and convert the text to lowercase. To match
keywords containing a single term, we split the text
by the space character and look for exact matches
between the keyword and the words in the result-
ing array. To match keywords containing multiple
terms, we do a direct search over the text (with-
out splitting). We make this distinction between
single-word and multi-word keywords due to the
false positives caused by matching substrings (for
example, "public" would match a text that contains
the word "republic"). For each matched keyword,
we extract the paragraph in which it was identified
and create text files using these paragraphs. These
filtered text files assist in identifying the claims of
the papers during the manual analysis.

The keywords consist of words that indicate
availability. The complete set of keywords is as
follows: release, released, public, publicly,
github, gitlab, huggingface co, osf io, open
source, accessible. Note that the non-letter char-
acters of the keywords are also replaced by spaces
to facilitate the matching. Also, note that we do
not include keywords such as available and http
due to the high number of false positives that they
cause. In order to quantify the impact of avoiding
these keywords, we look at the false omission rate
of a sample of 100 papers. We randomly select 100
papers from the data set and run them through our
keyword-based search algorithm. This predicted
69 papers to contain promises of releasing artefacts.
We then manually checked the remaining 31 papers
in full, to see whether they promised the release
of an artefact. Of these 31 papers, one paper has
promised and shared the data and code. This results
in a false omission rate of approximately 0.03.

We manually read the filtered papers to further
verify whether a paper has produced an artefact,

and if so, whether it has promised to release that
artefact.

Results are shown in Figure 2. Interestingly, out
of the Main PV papers that produced some new
artefacts, 44% have not mentioned whether that
artefact will be released. In the Other category,
this value is 67%. LREC has the lowest percentage
at 33%. However, in LREC, 36% of the papers
that have promised to release data have not given a
repository URL.

3.3. Further Analysis into Artefact Availability

In the above analysis, we can only determine
whether a paper mentions that research artefacts are
publicly released, and if so, a link to a repository is
given. However, that analysis does not tell us the
type of these repositories, whether they are accessi-
ble, or whether they contain the artefact. Therefore,
we carry out a second, more detailed analysis.

To get an insight into more recent trends, we con-
sider papers published between 2020-2023. Follow-
ing the same semi-automated approach discussed
above, we extract a list of papers that promised
to release at least one of the following artefacts:
data, source code, LM, or tool. Then the extracted
papers are grouped according to the language class.
Classes 5, 4, 3 and 2 have a considerable number of
papers, so we sampled 75 from each class. Class 1
and 0 only have 71 and 59 papers, respectively, thus
all of those papers were included in our analysis.
Altogether, this sample contains 430 papers.

The aggregated result is shown in Figure 3. Be
reminded that in this analysis, we omitted the pa-
pers that do not refer to an artefact type or those
that do not promise to release the artefact they pro-
duced. A ‘No’ is marked if a link was not given,
a given link is not working, or the repository cor-
responding to the link does not have the promised
artefact (we clicked through and followed all the
links mentioned in the papers).

We notice that a considerable portion of papers
that promised to release data have ‘dead-ends’
when trying to locate it. This count is higher in
low-resource languages. Most tools are hosted on
personal or institutional websites, and a portion
seems to have fallen out of maintenance in the in-
tervening years. The ‘dead-end’ problem exists to
a lesser degree concerning code availability. How-
ever, even for code, class 0 has a noticeable number
of ‘dead-ends’. Overall, most of the links to code
are active and have the artefact, followed by those
that promise to release an LM.


We also record the common repositories used by
NLP researchers and provide a summary in Table 2
(A breakdown of the same data across language
classes is available in Figure 7 in the Appendix).
According to this, GitHub seems to be the most
favourite option to release data and code. Some
research has considered Zenodo and Hugging Face
for data release!°. In contrast, Hugging Face seems
to be the favourite choice for LM releases. Most
of the tools have their own unique web link, hence
the ‘other’ category is the highest for this type.

Repository Code | Data | LMs | Tools | Total
GitHub 153 | 188 17 12 370
Hugging Face 0 6 11 2 19
Zenodo 1 10 1 0 12
Google Drive 0 5 3 1 9
Bitbucket 4 0 0 1 5
GitLab 3 2 0 0 5
Codeberg 1 1 0 0 2
Dropbox 0 1 0 0 1
Mendeley 0 1 0 0 1
Other 5 58 6 44 113
Total 167 | 272 38 60 | 537

Table 2: Repository usage across all classes

3.4 Analysis Based on NLP Tasks

Next, we carry out an analysis based on NLP tasks,
to understand whether artefact release has any re-
lationship to the type of NLP task!!. This analysis
was conducted using the paper sample used in Sec-
tion 3.3. Table 6 in the Appendix shows the raw
counts. Translation is the NLP task!* that has the
highest number of artefact releases (this artefact
is usually parallel data), followed by morpholog-
ical analyzer and Automatic Speech Recognition
(ASR). In particular, having morphological analysis
as the prevalent NLP domain seems to be common
for extremely low-resource languages. This is not
surprising - these languages have never had such
linguistic resources, and such research is essential
in understanding their linguistic properties. The
high amount of ASR-related artefacts could be due
to the existence of languages that do not have a

writing system!?,

“This result tallies with the survey results published
by Ranathunga and de Silva (2022) to a good extent.

‘Initial categorisation of tasks come from Hugging Face
task list and a survey paper on NLP research (de Silva, 2019)

"As shown in Table 6, Corpora has the highest raw counts
but is not an NLP Task per se.

“Eberhard et al. (2024) notes that around 41% of the lan-
guages they list may be unwritten.

4 3 3
ca qt
= 2 = =a
3 3 8 2
go), 21S= | oii, AF
g T° g m8
aa 3 aa l
0! — ° fe) °
012 3 4 °5 012 3 4°55
Language Class Language Class
(a) HF Dataset Counts (b) HF LM Counts

Figure 4: Number of resources for the language classes
on Hugging Face (HF).

3.5 Dataset and LM Availability

Our final analysis is based on the datasets and LM
counts reported in Hugging Face’, which is the
fastest-growing repository for NLP-related arte-
facts. Figure 4 shows!> the language class-wise
distribution of data and LMs. Further, Table 3
shows relevant numerical values, which demon-
strates the language class-wise disparity.

Median of Language Class
Artefact type 0 1 2 3 4 5
Tia Set 0.0} 12.0] 53.0] 147.5] 246.0] 657.0
counts
LM 0.0} 3.0] 171.5] 443.5} 881.0 | 2601.0
counts

Table 3: Hugging Face Resource Counts

The disparity between different language classes
is evident from the medians, despite some outliers.
Most notably, out of the 6135 languages in class 0,
most have no data or LMs, therefore the handful
of languages that have some data/LM have become
outliers. The correlation between the class-wise
LM and data availability is evident - a Pearson
correlation value of 0.9972 is reported between the
data and LM counts on languages listed in Hugging
Face.

4 Conclusion

We hope our findings would help the NLP commu-
nity to better appreciate the benefit of openness and
to commit to releasing the artefacts they produce.
We further hope these statistics will be useful to
ACL in making informed decisions. It would be in-
teresting to run this same experiment 5 or 10 years
down the line, to see if there are any changes in
releasing and reusing artefacts. In hopes to assist
in such efforts, our code is publicly released'®.

4ittps ://huggingface.co/languages
‘SA larger version is available as Figure 8 in the Appendix.
https: //bit.ly/ACL2024ShouldersOfGiants


5 Limitations

We considered only a fraction of the papers pub-
lished in AA. Our keyword-based paper filtering
mechanism might have missed some papers that
have made their artefacts available. If a paper does
not mention the language name in its abstract, our
algorithm does not pick it up. Thus we highly
encourage the community to adhere to ‘Bender
Rule’ (Bender, 2019). If a research published their
artefact without mentioning that in their paper, or
if the link to the artefact was included in a different
version of the paper (e.g. ArXiv), such are missed.
We might have missed some information on arte-
facts while manually reading hundreds of research
papers, which might have impacted the statistics
we present. When checking if a repository link is
live, we clicked on that link only once. There could
have been instances where the link was momentar-
ily down. In certain instances, we noticed that a
URL is not working due to a change in the web
repository directory structure. However, we did
not try to manually figure out the correct link. We
consider an artefact to be available in a repository
if we note the availability of files (e.g. python files
in a code base) inside the repository. We cannot
guarantee the repository has all the artefacts the
paper promised (e.g. all the promised data files or
whether the given code is working).

6 Ethics Statement

We only used the AA paper repository, which is
freely available for research. Our implementation
is based on publicly available code. We do not
release the paper-wise information we recorded,
nor do we re-publish the papers we downloaded
from AA.

References

Emily Bender. 2019. The #Benderrule: On naming the
languages we study and why it matters. The Gradient,
14.

Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python: analyzing text
with the natural language toolkit. " O’ Reilly Media,
Inc.".

Damian Blasi, Antonios Anastasopoulos, and Gra-
ham Neubig. 2022. Systematic inequalities in lan-
guage technology performance across the world’s
languages. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics

(Volume 1: Long Papers), pages 5486-5505, Dublin,
Ireland. Association for Computational Linguistics.

Andrew Cains. 2019. The geographic diversity of NLP
conferences. MAREK REI.

Nisansa de Silva. 2019. Survey on publicly available sin-
hala natural language processing tools and research.
arXiv preprint arXiv: 1906.02358.

David M. Eberhard, Gary F. Simons, and Charles D.
Fennig. 2024. Ethnologue: How many languages in
the world are unwritten? Dallas, Texas: SIL Interna-
tional.

Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pages
6282-6293, Online. Association for Computational
Linguistics.

Simran Khanuja, Sebastian Ruder, and Partha Talukdar.
2023. Evaluating the diversity, equity, and inclu-
sion of NLP technology: A case study for Indian
languages. In Findings of the Association for Compu-
tational Linguistics: EACL 2023, pages 1763-1777,
Dubrovnik, Croatia. Association for Computational
Linguistics.

Joelle Pineau, Philippe Vincent-Lamarre, Koustuv
Sinha, Vincent Lariviére, Alina Beygelzimer, Flo-
rence d’ Alché Buc, Emily Fox, and Hugo Larochelle.
2021. Improving Reproducibility in Machine Learn-
ing Research (A Report from the NeurIPS 2019 Re-
producibility Program). Journal of Machine Learn-
ing Research, 22(164): 1-20.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The Kaldi Speech Recogni-
tion Toolkit. In IEEE 2011 workshop on automatic
speech recognition and understanding, CONF. IEEE
Signal Processing Society.

Surangika Ranathunga and Nisansa de Silva. 2022.
Some languages are more equal than others: Prob-
ing deeper into the linguistic disparity in the NLP
world. In Proceedings of the 2nd Conference of the
Asia-Pacific Chapter of the Association for Compu-
tational Linguistics and the 12th International Joint
Conference on Natural Language Processing (Vol-
ume I: Long Papers), pages 823-848, Online only.
Association for Computational Linguistics.

Shaurya Rohatgi. 2022. ACL Anthology Corpus with
Full Text. GitHub.

Mukund Rungta, Janvijay Singh, Saif M. Mohammad,
and Diyi Yang. 2022. Geographic citation gaps in
NLP research. In Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1371-1383, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.


A Language Category Definition

Class Description Language
Count Examples
0 Have exceptionally limited 2191 eee
resources, and have rarely
been considered in lan-
guage technologies.
1 Have some _ unlabelled 22), x ra au
data; however, collecting
labelled data is challenging.
Zulu
2 A small set of labelled 19 Trish
datasets has been collected,
and language support com-
munities are there to sup-
port the language.
3 Has a strong web pres- 28 >
ence, and a cultural com-
munity that backs it. Have
highly benefited from unsu-
pervised pre-training.
4 Have a large amount 18 Ganinen
of unlabelled data, and
lesser, but still a significant
amount of labelled data
have dedicated NLP com-
munities researching these
languages.
5 Have a dominant online faire

presence. There have been
massive investments in the
development of resources
and technologies.

Table 4: Language Category definition by Joshi et al.
(2020)

B Main Conference and Journal List

(1) Annual Meeting of the Association for Compu-
tational Linguistics, (2) North American Chapter of
the Association for Computational Linguistics, (3)
European Chapter of the Association for Computa-
tional Linguistics, (4) Empirical Methods in Natu-
ral Language Processing, (5) International Confer-
ence on Computational Linguistics, (6) Conference
on Computational Natural Language Learning (7)
International Workshop on Semantic Evaluation,
(8) Conference of the Asia-Pacific Chapter of the
Association for Computational Linguistics, and (9)
Conference on Computational Natural Language
Learning.

In addition, the following journals are consid-
ered: (1) Transactions of the Association for Com-
putational Linguistics and (2) Computational Lin-
guistics.

C. Artefact Annotation Scheme

All the annotators involved in this study are coau-
thors of the paper. In Table 5 we show the annota-
tion scheme we used.

Artefact Status

Used dataset from some previous research

Extended an existing dataset

Used dataset from some previous research but created new data as well

Data
Introduce new dataset
Data not needed
| Cannot determine
l Used an implementation from some previous research
Extended an existing implementation (e.g. toolkit, library)
Cade Used an implementation from some previous research but implemented part

of the solution from scratch

Provided their implementation

Code not needed

Cannot determine

Used an existing LM

Extended an existing LM

LM Used an existing LM but trained their LM(s) as well

Trained their own LM

LM not needed

Cannot determine

Table 5: Possible options for Artefacts

D_ Code and Data Reuse

Code and data from Ranathunga and de Silva
(2022) and Rohatgi (2022) are released under CC
BY-NC 4.0 licence. The authors obtained permis-
sion from Blasi et al. (2022) to use the code on
their public repository!’.

E NLP Task Breakdown Across
Language Classes

We show the NLP task breakdown across the five
language classes in Table 6.

F Code and Data Intended Use

All the code use was consistent with their intended
use as specified on the relevant research publica-
tions (Ranathunga and de Silva, 2022; Blasi et al.,
2022) and the readme files on the repositories (Ro-
hatgi, 2022).

G_ Artefact Creation, Extension, and
Reuse

In Figure 5 we have the larger version of the Fig-
ure | for improved readability. Further, given that
the information in Figure 5 is presented after ag-
gregating across time but separated into language
classes, we also include a set of cumulative per-
centage graphs in Figure 6 where we show the
same data aggregated across the language classes
but spread out over the publication years to better

"https: //github.com/neubig/globalutility


Language Class

Text Generation

Hate Speech Detection
Misinformation Detection
Wordnets/Ontology/Taxonomy
Discourse Analysis

Question and Answer (QnA)

NLP Tools

Semantic (Other)

Tokenizer

Semantic Similarity

Multiple Tasks

Spelling and Grammar
Summarizing

Phonological Analyzer

Sentiment Analyzer
Text-to-Speech

Transliteration

Lexical Inference

Coreference Resolution
Information Extraction

Bilingual Lexicon Induction (BLI)
Optical Character Recognition (OCR)
Language Identification (LangID)
Intent Detection

News/Social Media Recommendation
Text Classification

Stemming

ooocoroococooocoococoooooeoceoeoreceowocooONnNrrRrF ADAH e

coooocoorrocooocoNOoOrcor ooo OC HBR RE NHN FKP COO OrR AN OK RFP UNKOWN

—
ooocooocooor COCO COO FN WOWWNTORrF FNP DOCOADNN WR WR KR KW AD || W!

OoroocorecorceconrroOrrF OF OrFRNFOON OWWrRWrRN

RP Or OO OF OWWrRONF Rr OF OF OWN COWRA NNRKN WwW FWN W

coooocoorcnooccocmcmcwmcOCCcCCOCcUOCUCUOCUmr RK CUOCUrRRP LUNN OOF Rr OP OrFPK ONWOHUNUCOCO AD

NLP Task 0 i 5) mn 5 Total
Corpora 19 22 11 11 29 103
Translation 10 1 8 10 52
Morphological Analyzer 11 Z 1 25
Automatic Speech Recognition (ASR) 5 10 23
Language Model 10 pal
Parsers 3 20
Data Sets 19
Dictionary/Lexicon 18
Named-Entity Recognition (NER) 18
Text Classification 16
Part of Speech (PoS) 15
Cross-Lingual Applications 14

Re Re Rr PEP NNNWWHwWwWHKKHKHHPMANDANAIANANAOO SO

Total

~
N

~
oo

~~
lon
~
&

lo)
—

~
nN

457

Table 6: NLP Tasks Conducted



@mm™_ Used others' artefact as it is (@@™@ Used others' artefact + added their own part Mill Artefact not needed/considered

@@™ Based on extended/modified artefact Mm Introduced new artefact M@@™ Can't determine
1.0; 1.0; 1.0;
wn 0-9 v 0.9 v 0.9
o 0.8 o 0.8 o 0.8
0.7 20.7 20.7
20.6 20.6 20.6
20.5 20.5 20.5
6 0.4 60.4 60.4
o 0.3 o 0.3 o 0.3
ie 0.2 ia 0.2 ia 0.2
0.1 0.1 0.1
OD 1 2 & 4 OS oo™"O 1 2 3° 4~«5 wo tf 2 6 4 5
Language Class Language Class Language Class
(a) Usage of Data - LREC (b) Usage of Code - LREC (c) Usage of LMs - LREC
1.0) 1.07 1.0]
0.9 0.9 0.9
wn wn wn
g 0.6 g 0.8 g 0.8
@ 0.7 — 0.7 q 0.7
20.6 20.6 20.6
°0.5 20.5 20.5
604 6 0.4 60.4
o 0.3 o 0.3 o 0.3
& 0.2 & 0.2 £ 0.2
0.1 0.1 0.1
oe 1 2 3 4 °5 Bs 1 2 3 4 «5 n'o 1 2 3 4 8
Language Class Language Class Language Class

(d) Usage of Data - Main (e) Usage of Code - Main (f) Usage of LMs - Main

Fraction of papers
geeosoees9eeseor
OrPRFNWAHRUDANWOOD

Fraction of papers
eeoseseesseeeor
ORPrNWHRUDNWOO

0 1 2 3 4 5 0 1 2 3 4 5 1 2 3 4 5

Language Class Language Class Language Class

(g) Usage of Data - Other (h) Usage of Code - Other (i) Usage of LMs - Other

Figure 5: Artefact (Data, Code, LM) creation, extension, and reuse across ACL venues - Aggregated analysis

show the changing trends in resource availability information mentioned in the research papers. Ac-
and reuse. Unsurprisingly, as per Figures 6b, 6e, cording to this, GitHub seems to be the most
and 6h, we can see that code is being re-used the _ favourite option to release data and code. Some
most across all venues. LREC (Figure 6a) stands __ research has considered Zenodo and Hugging Face
out among the data graphs (Figures 6d and 6g) for _ for data release!*. In contrast, Hugging Face seems
consistently being a source of new data sets rather —_to be the favourite choice for LM releases. Most
than a venue where existing data is reused. We see _ of the tools have their own unique web link, hence
that LMs, had a reasonable presence in the main __ the ‘other’ category is the highest for this type.

venues (Figure 6f) even before our analysis period In Figure 7 we show a more detailed view of the
while in the other venues (Figure 61), the trend stars —_ artefacts being hosted online; previously discussed
just at the beginning of our considered time period. in Table 2 as a summary. Here it is possible to
LREC on the other hand, seems to be late to be _ note the variations between the language classes.
considered for LMs as it is only in 2018, that we —_ For example, the interesting observation of Fig-

see them becoming noticeable in Figure 6c. ure 7c is that it can be noted that while researchers
. in all other listed language classes use github to
H Artefact Hosting host their trained LMs, the researchers of Class 4

Table 2 shows a summary of where NLP re- This result tallies with the survey results published
searchers have published their data, based on the _ by Ranathunga and de Silva (2022) to a good extent.


Mm Introduced new artefact
Mm Used others' artefact + added their own part

mi Based on extended/modified artefact
Mmm Used others' artefact

Mam Artefact not needed
MM Can't determine

100

80

60

40

20

% of Papers
Bb
N PS an foe) fo}
oO oO oO fo} oO
% of Papers

2016 2017 2018 2019 2020
Year

(a) Usage of Data - LREC

(e)

2021 2022

2016 2017 2018 2019 2020
Year

(b) Usage of Code - LREC

100

80

60

% of Papers

40

20

7016 2017) 2018 2019 2020 2021

Year

(c) Usage of LMs - LREC

2021 2022 2022

100 100

[o-)
o
[o-)
3

% of Papers
+ a
oO oO

% of Papers
+ an
oO oO

N
[s}
N
[s}

2018 2020
Year

(d) Usage of Data - Main

2018

(e) Usage of Code - Main

100

80

60

40

% of Papers

20

te)

2016 2018 2020

Year

(f) Usage of LMs - Main

2020
Year

100 100

[o}
3
oO
o

a
fo}
a
o

Ny
o

% of Papers
b
fo}

% of Papers

N
fs}
N
[s}

2018 2020
Year

(g) Usage of Data - Other

2018

(h) Usage of Code - Other

100

% of Papers
+ a fo)
oO oO o

N
[s}

2016

2018
Year

(i) Usage of LMs - Other

2020 2020

Year

Figure 6: Cumulative percentage graphs - Artefact (Data, Code, LM) creation, extension, and reuse across ACL

venues. - Chronological analysis.

lm GitHub
@mm Hugging Face

@m™m™ Zenodo
™@™ Google Drive

lm Bitbucket
@mm™ GitLab

@m™ Codeberg
@mm Dropbox

|@™@ Mendeley
mm Other

0.0 on og 05 0.0 on 03 O48 0s

02 ¥
Class Fraction

(b) Code

, 0:
Class Fraction

(a) Data

x ¥ o3 Y

04 ¥ or 04 os

0: 0:
Class Fraction

(d) Tools

02 ‘
Class Fraction

(c) LMs

Figure 7: Artefact (Data, Code, LM, Tools) hosting
locations.

languages opt for Hugging Face. Conversely, from
Figure 7d, it can be noted that in Class 0 languages,

tools are generally not hosted on github. A curious
observation in Figure 7c is that for some reason,
Class 1 languages do not select Hugging Face as
a clear contender to host their language models,
something that all other language classes seem to
do. The overwhelming prevalence of the other op-
tion in Figure 7d can be explained by the fact that
most tools tend to be hosted on dedicated websites.
Even when the actual site is hosted on a service
such as github, they are masked with shorter and
more market-friendly custom URLs.

I Hugging Face Resources

In Figure 8 we show the resources available on
Hugging Face for the 5 language classes. This is
a larger version of the Figure 4 for improved read-
ability. Note especially how the entire interquartile
range of class 0 is at zero due to the dearth of
resources existing for the languages in that class.
Thus a language in class 0 with any amount of
resources gets registered as an outlier. On the op-
posite end of the spectrum, note class 5 with only


&
fe)

Log Count
NO
LTH
HIH
H{]

490 Of

2
|

0123 45
Language Class

(a) HF Dataset Counts

Log Count

&

NO

oO

OCCHEIEEEEEED © O CO
°
°

{| — °

0123 45
Language Class

(b) HF LM Counts

Figure 8: Number of Hugging Face (HF) resources for the language classes.

7 languages in the set even after the reclassifica-
tion by Ranathunga and de Silva (2022). Despite
that, English still manages to be an outlier with its
exceptional resource availability.

From Figure 8 and Table 3, it can be observed
a considerable jump between the median values
when comparing adjacent classes. This may be
taken as both: 1) an indication of the visible differ-
ence in the resource availability of the language
classes, 2) A reaffirmation of the soundness of
the class borders proposed by by Ranathunga and
de Silva (2022) as the distinct medians can be taken
as a quality of classes which are internally cohesive
and mutually separate.

