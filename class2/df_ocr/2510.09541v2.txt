arX1v:2510.09541v2 [cs.CL] 13 Oct 2025

SPG: Sandwiched Policy Gradient for Masked
Diffusion Language Models

Chenyu Wang! ?*7, Paria Rashidinejad!*}*, DiJia Su', Song Jiang!, Sid Wang!, Siyan Zhao!**, Cai Zhou”,
Shannon Zejiang Shen!-?*, Feiyu Chen!, Tommi Jaakkola”, Yuandong Tian!, Bo Liu!

'Meta Superintelligence Labs, 7MIT, USC, “UCLA
*Work done at Meta, ‘Core contribution

Diffusion large language models (ALLMs) are emerging as an efficient alternative to autoregressive models due
to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or
task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood
precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the
evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias.
To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower
bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on
ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for
dLLMs by 3.6% in GSM8K, 2.6% in MATHS500, 18.4% in Countdown and 27.0% in Sudoku.

Date: October 14, 2025
Correspondence: Chenyu Wang at wangchy@mit.edu, Bo Liu at lbo@meta.com

Code: https://github.com/facebookresearch/SPG OO Meta
GSM8K MATHS500 Countdown Sudoku
_ +3.6% tT fae +2.6% Tt +18.4% +27.0% t
S 35 40.0 aOL0R
> 37.437.4
4 80.5 80. Be 37.5 36.0
© 80 40
5 35.0 27.726.922: ii 1
32.4
g - 32.5 32.2 5
30.0

LLaDA-8B-Instruct LLaDA-1.5 D1 = WD1 Mi UniGRPO Mm =SPG (ours)

Figure 1 Test accuracy of SPG and baseline methods on four mathematical and logical reasoning benchmarks. All methods are
evaluated with a generation length of 256 in 128 denoising steps. Full results are provided in Table 1.

1. Introduction

Diffusion models, originally pioneered for high-fidelity image generation (Song et al., 2020; Ho et al., 2020), have
recently emerged as a powerful and efficient paradigm for text generation (Austin et al., 2021; Campbell et al., 2022;
Sun et al., 2022; Lou et al., 2023; Sahoo et al., 2024; Shi et al., 2024). These models operate in a discrete space but
share architectural similarities with their continuous counterparts (Peebles and Xie, 2023). They employ a fixed noising
process that progressively corrupts text data, while a neural network is trained to learn the reverse, denoising process.
For instance, Masked Diffusion Language Model (MDLM) (Sahoo et al., 2024) uses random masking as its forward
noising process and optimizes an Evidence Lower Bound (ELBO) of the log-likelihood. This ELBO-based objective has
been widely adopted by subsequent large-scale diffusion language models (dLLMs), including LLaDA (Nie et al., 2025)
and DREAM (Gong et al., 2024).

A key advantage of dLLMs over their autoregressive (AR) counterparts is their ability to decode multiple tokens in
parallel. This parallelism can significantly reduce inference latency, making it an attractive alternative for scalable
language modeling (Wang et al., 2025a; Labs et al., 2025).


Prompt c Upper Bound := exp ( Levpo(x); c)) Monte Carlo Est. of £, ,, and L,,

Clean Sequence x

Response x’ Mae yc= 0).

Perturbed Sequence z/ ~ q(-|x’)
Random Masking

Response x Ae, c) <0

: ANS NALS tole! | c) | Block-Wise Masking (Ours)

clean token mask token

Response x* A*(x‘,c)>0 | “ Lower Bound := exp (Cer0(2’ ; c))

Figure 2 The training process of SPG for MDLM. Left: From a prompt c, we generate responses { a’ Har We then maximize a
lower bound on the likelihood 79 (a? | c) for high-reward responses while minimizing an upper bound for low-reward ones. Right:
The upper/lower bound of likelihood is estimated via Monte Carlo using a block-wise masking strategy, where a random block is
selected for masking, with earlier blocks kept clean and later blocks fully masked. The example shows a sequence of length 9 with a
block size of 3, where the current generation block is highlighted in yellow.

Aligning large language models with human preferences (Ouyang et al., 2022) or task-specific rewards (e.g., inducing
reasoning behavior) (Shao et al., 2024; Guo et al., 2025) typically requires a post-training stage of reinforcement
learning (RL). However, applying RL to dLLMs remains underexplored. A principal challenge is the computationally
intractable log-likelihood of dLLMs, which is essential for accurate policy gradient estimation. To circumvent this,
recent works (Zhao et al., 2025; Yang et al., 2025; Zhu et al., 2025; Tang et al., 2025) adapt standard RL and preference
optimization algorithms, such as GRPO (Shao et al., 2024) and DPO (Rafailov et al., 2023), by using the ELBO or a
one-step estimation as a surrogate for the true likelihood. While straightforward, this approximation leads to misaligned
policy gradients, and potential suboptimal performance.

To address these limitations, we propose Sandwiched Policy Gradient (SPG), a novel reinforcement learning algorithm
for diffusion language models that computes a more robust and less biased policy gradient. As illustrated in Figure 2,
our core idea is to “sandwich” the intractable log-likelihood of a generated sequence: we maximize a tractable lower
bound for positive-reward sequences while minimizing an upper bound for negative-reward ones. To ensure a stable
estimation of these bounds, we also propose a block-wise masking strategy that better aligns data distributions during
policy rollout and optimization. SPG achieves state-of-the-art performance on four mathematical and logical reasoning
benchmarks, improving accuracy by up to 3.6% on GSM8K, 2.6% on MATH500, 18.4% on Countdown, and 27.0% on
Sudoku compared to the state-of-the-art RL algorithms for diffusion language models.

In summary, our main contributions are:

¢ Anew policy gradient algorithm, SPG, which reduces bias by optimizing sandwiched variational bounds based on
reward.

¢ A block-wise masking technique that improves the stability of the training objective’s estimation.

¢ State-of-the-art results among RL algorithms for diffusion language models on four reasoning benchmarks, demon-
strating the effectiveness of our approach.

2 Background

In this section, we provide a brief overview of the masked diffusion language model (MDLM) and reinforcement learning
for text diffusion models.

Notation. We denote scalars by lowercase letters (x), vectors by bold lowercase (x), and sequences by 2:n. []
represents {1,...,k}. Cat(a | p) is the categorical distribution over 2 with probabilities p, and U/[a, b] denotes the
uniform distribution in [a, b]. Throughout the paper, we use i € [n] for position of the token, 7 € [g] for a sequence in
a group of rollouts, and ¢ for the diffusion timestep. For discrete time processes, t € [T], while for continuous-time
Markov chains, t € [0, 1].


2.1. Masked Diffusion Language Models

Diffusion models for language learn to generate text by reversing a gradual noising process. Specifically, Masked
Diffusion Language Models (MDLMs) (Sahoo et al., 2024) start with clean text x1.,, and corrupt it into 2; = 2t,1:n
over a continuous timestep t € [0, 1] by progressively replacing tokens with a special [mask] token. At ¢ = 0, the data
is original (2) = x), while at t = 1, the sequence is fully masked (2, is all [mask] tokens). Each token is corrupted
independently according to the forward transition kernel:

Qo(2t,i | Li) = Cat (24,i | aya; + (1 - a,)m), (1)

where m is the one-hot representation of the [mask] token. The noise schedule, a, € [0, 1], is a strictly decreasing
function, such as the linear schedule a, = 1 — t, with ag = 1 and a, = 0.

In the reverse process, a neural network, which we denote as the policy 7, is then trained to perform the reverse process:
predicting the original tokens x from a corrupted version z;. The transition from z; to 2; (s < ¢) is parameterized with
tte as follows:

Cat(z.; Zz), Zt ~ mM,
po(Zs | 2) = (2s | 2,2 = 7o(- | 2)) = Cat (2. oecpucfeccasal 24) | &=mM.

The policy is achieved by maximizing the Evidence Lower Bound (ELBO) of the log-likelihood of each clean sequence
Z ~ Pdata, Which simplifies to the following objective:

Letpo(x; 0) = Et,z, > w(t) - (24; =m) - log r(x: | 2)| , (2)

i=l

where w(t) = a/,/(az — 1) is a time-dependent loss weight, and the expectation is over a random timestep t ~ U/[0, 1]
and the corrupted sequence z; ~ qjo(- | x). In essence, this objective trains the model to “fill in the blanks” by
predicting the original tokens at masked positions. For a more comprehensive overview of MDLM, please refer to
Section A and Sahoo et al. (2024).

2.2 Reinforcement Learning for Diffusion Language Models

Reinforcement Learning (RL) aligns a language model with desired objectives by treating it as a policy 7@ that generates
a response x to a prompt c. A reward function R(c, x) provides a scalar score for the response, and the training goal is
to update @ to maximize the expected reward: 7(0) := Ez .x9(.\c)[R(e, x)]. This objective is commonly optimized
using policy gradient methods, which rely on the following gradient estimator.

VoT(@) = Lomungtile) R(c,2)Ve log t9(« | c) : (3)

The Intractability Challenge. A central challenge in applying RL to diffusion models is that the policy’s log-likelihood,
log @(a | c), is intractable and cannot be computed directly. To overcome this, prior work (Zhu et al., 2025; Yang
et al., 2025) approximates this term using its ELBO, effectively replacing log (a | c) with a score derived from the
pre-training objective in Equation (2).

However, this popular workaround introduces a critical flaw. The ELBO is only a lower bound on the true log-likelihood
(ELBO < log 7). Consequently, the RL objective is only a valid lower bound on the true expected reward if all rewards
R(c, 2) are non-negative. This constraint prevents the model from effectively learning from negative feedback (i.e.,
penalizing bad outputs) and is incompatible with advanced RL algorithms that use relative or negative rewards (Shao
et al., 2024), biasing the final policy. Our work aims to resolve this limitation.

3 Sandwiched Policy Gradient with Evidence Bounds

We introduce SPG, a novel policy gradient algorithm designed for masked diffusion language models (Algorithm 1). Our
method aims to address a critical issue in applying reinforcement learning to dLLMs by creating a valid optimization
objective based on tractable bounds of the model’s evidence.


Algorithm 1 SPG: Sandwiched Policy Gradient for Masked dLLMs

Require: prompt distribution D, number of completions per prompt g, number of inner updates js, forward process gq,
number of Monte Carlo samples m, initial policy 7, learning rate e.

1: Initialize 7g < 7
2: while not converged do
3: Sample a prompt c ~ D, then g completions {a7 ~ 74(- | c)
4 Vj € [g], compute reward R(c, 2) and advantage A? (x, c)
5 for gradient update iterations {1,...,ju}do
6: Vj € [g], generate m perturbed samples {z/_}""_, ~ q(- | #7) via block-wise masking (Section 3.3).
7 Compute the sandwiched policy gradient V Jspq(@) where:

g
j=1

12 _
Jspc(9) = | S- (Laiso - A’ Lerpo(x! | c; 8) + Laico: A’ LeuBo(2” | ¢; @)) |

9

8: and Lerpo, Paes are estimated from fet m_,, using Equation 2 and 7.
9: Perform gradient update: 6 ~ 0 + €V Jspc(@)

10: return 76

3.1 ALower Bound Objective for Policy Optimization

Our approach is based on group relative policy optimization (Shao et al., 2024; Liu et al., 2025b). For a given
prompt c, we generate a group of g responses {ax/ Mea from the policy 7. We then compute the advantage
AJ (ec, x) := R(e, 24) — 3 4_, R(c, x’). Moreover, we transform the conventional policy optimization objective as
an advantage-weighted log-likelihood objective, for reasons that will be clear later:

ae :
TROP (A) _ bn Tal sorte go AJ (a,c) log Toe (a’ | Cc) 3 (4)
j=l

where sg[9] indicates that gradients are not computed for the policy that generates the samples. This objective encourages
generations with positive advantages (AJ > 0) and discourages those with negative advantages (A? < 0).

For dLLMs, the log-likelihood log 7 is intractable. A common surrogate is the evidence lower bound (ELBO). While
maximizing the ELBO is a valid way to increase the true log-likelihood, minimizing the ELBO for negatively-rewarded
samples does not guarantee a reduction in the true log-likelihood. To address this, we propose a sandwiched objective.
For samples with positive advantages, we maximize the ELBO. For samples with negative advantages, we instead
minimize a tractable evidence upper bound (EUBO), Lguso. This creates a true lower bound for the original objective:

1 2 ; ;
Jspc(@) = | 9 s (Lasso: A’ Letpo(x? | c;@) + 14s <0 - A’ Leuso(@! | c:)) |, (5)
a

where the expectation is take with respect to c, {a hw Tsg(o}- Since Lego < logme < LeEupo, it follows that
JIspa(9) < J*°"P(8). Maximizing this tractable bound therefore serves as a valid proxy for optimizing the true
objective.

3.2 A Tractable Evidence Upper Bound

To effectively penalize negatively-rewarded samples by minimizing their log-likelihood, we require a tractable EUBO,
which we derive in the following theorem based on the Rényi variational bound.

Theorem 1 (Evidence Upper Bound for Masked Diffusion). Assume the forward denoising process has T steps with a
monotonic schedule ay. For any B > 1 and a sequence %.n, we have:

n T-1

1 : Oy — At41

Levso(L1n; 9) = B s log S Ze41 7... man ‘124145 = Mm) 16 (a; | Z141)| +C(T), (6)
i=l t=1


where C(T) := 1(8 <n) - z log Deg ancug( lee) [aleur | x)~"| is a constant independent of 0.

Here, 6 > 1 is a hyperparameter that controls the tightness of the bound, with values closer to 1 yielding a tighter bound.
The expectation is taken over the timestep ¢ ~ U/{0, 1] and the noised latent z; ~ qjo(- | @).

Corollary 1. Taking the limit of T — ov, we have:

VoLevuso(@£1:n; 0) = Vo (Lev.0 (aan 0) + c(T)) = VoLevso(@1:n; 0), where
(7)

~ {|
Levno(@in:8) = 5 Slog E:,z, [w(e) A(z, =m) +73 (a; | )| -
t=1

In practice, we estimate Leveo using Monte Carlo sampling and plug it in Equation 5 in place of Ceygo. The proof and
theoretical analysis are provided in Appendix B.

Remark. A key structural difference from Lg, g0 is that the logarithm in Lgygo (Equation (6)) appears outside the
expectation. Therefore, in practice, due to Jensen’s inequality, applying the concave logarithm to a Monte Carlo estimate
of the expectation’s argument yields a biased estimate of the true EUBO. While it is possible to derive a looser but
unbiased bound using inequalities like log(a) < x — 1, we found this approach empirically worse by widening the gap
to the true log-likelihood, as shown in Table 10. We therefore retain the tighter, albeit slightly biased, formulation.

3.3. Practical Considerations

Block-Wise Masking Strategy for Monte Carlo Estimation. In practice, we approximate Leppo and foumc in
Equation (5) via Monte Carlo sampling: for each x/, we randomly sample m timesteps {t,}’_, and generate the
corresponding partially masked samples {zy m_,~q(- | x). One straightforward approach as used in Yang et al.
(2025) would be to apply random masking to clean sequences. However, recent dLLMs like LLaDA (Nie et al., 2025)
employ a block-wise semi-autoregressive unmasking strategy during generation and achieve state-of-the-art performance
over random unmasking. As a result, the policy rollout process actually encounters a much narrower and more structured

set of partially masked sequences than with fully random masking.

To better align data distributions during policy rollout and optimization, we adopt a block-wise masking strategy rather
than random masking. As depicted in Figure 2, the sequence is divided into several blocks, and a random block is
selected, with all preceding blocks left clean and all following blocks fully masked. Within the chosen block, tokens are
randomly masked. Additionally, following D1 (Zhao et al., 2025), we lightly perturb the prompt and clean blocks by
randomly masking tokens with a small probability pmasx = 0.15 to enhance stability and generalization.

Altogether, our block-wise masking strategy improves the stability of the objective’s estimation and the efficiency of
policy optimization. While similar block-wise masking approaches have been explored in concurrent work for supervised
fine-tuning or block diffusion models (Sun et al., 2025; Wang et al., 2025b), our focus is on RL for full-attention
masked dLLMs. As shown in Figure 6, our models trained with block-wise masking generalize well to various inference
strategies.

Mixture of Upper and Lower Bound for Negative Advantage Traces.

Monte Carlo estimation of Equation (6) leads to a biased estimation to Levupo and potentially requires a substantial
number of samples to get reliable approximations, resulting in high computational costs and instability during training.
To address these challenges, we use a mixture of nue and Lego as a more practical log-likelihood approximation for
negative advantage traces:

where 0 < w < 1 is a blend coefficient. Intuitively, the upper bound Leuso sharpens the model decisions by applying a
2-power adjustment to the original model output, acting as a strong correction signal for negative advantage traces. In
contrast, the lower bound Lg, go is easier and more stable to estimate with a small number of Monte Carlo samples, but it
tends to introduce larger, systematic bias relative to the true log-likelihood. In particular, as a conservative approximation,
Lego alone is insufficient for effectively penalizing negative advantage traces, thus limiting its efficacy. Therefore,
combining them allows us to harness the strengths of each, resulting in a more effective log-likelihood estimation in


practice. In the following proposition, we formalize the advantages of using the mixture by deriving the gradient of the
mixture loss and analyzing the variance of the gradient.

Proposition 1 (Optimal Mixture Strictly Reduces Variance). Fix a coordinate k and let

pa = w(t, 2e)m6 (wi | 21,€)/E [wo(t, 2.)n6(@i | 215 0)],
where w(t, Z:) = w(t)1(z, = m). Then, the gradient of mixture objective (8) is given by
Gu,k = ((1 — w) w(t, Z:) + wp) Oo, log te(x | Z,€). (9)

If Var((pg — w(t, Zz))Oo, log me (x | Z,€)) > 0, then Var[gu,x] is a strictly convex quadratic in w and thus admits a
unique minimizer wry. Moreover,

Var[gux.&] < min{ Var[go,«], Var[g1,x] },

A proof for the above proposition is provided in Section C.1. A few remarks are in order:

* Confidence-aware weighting: The mixture gradient in Equation (9) realizes a confidence-aware weighting: uncertain
tokens with small aA (a; | 24, c), indicating a low recovery chance, have a smaller weight, while confident tokens with
large ne (a; | 24, c) are upweighted. The sharpness is controlled by parameter ( and the blend by w. Furthermore, the
convex interpolation of the confidence-aware coefficient of the upper bound with the lower bound ensures clipping tiny
gradients to a minimum value and thus prevents vanishing gradients.

* Lower variance and more stable training: According to Proposition 1, the gradient of the optimal mixture, Le., gv 4,
has strictly smaller coordinate-wise variance than the gradient of either the lower bound (go,x) or the upper bound
(91, ;)'. In our experiments, we fix @ and w as hyperparameters for simplicity. These values can also be adaptively
adjusted during training to better match the evolving training dynamics and data distribution.

Thus, the mixture approach offers theoretical advantages over using either the upper or lower bound alone, as supported
by our experimental results in Section 4. Further discussions of the mixture approach and empirical evidence of reduced
gradient variance are provided in Appendix C.2 and Figure 7, and Appendix C.3 presents a toy example illustrating the
distinct behaviors of the lower and upper bounds.

4 Experiments

In this section, we present experimental results highlighting the superior performance of SPG across various benchmarks.
Further, we provide detailed analysis and ablations of SPG to assess the contribution of each component, examine the
influence of key hyperparameters, and evaluate the robustness of our approach under different inference strategies.

4.1 Experimental Setup and Main Results

Experimental Setup. We conduct RL fine-tuning with SPG following the experimental settings in D1 (Zhao et al.,
2025) and WD1 (Tang et al., 2025). We employ LLaDA-8B-Instruct (Nie et al., 2025), a state-of-the-art open-sourced
dLLM without post-training, as the base model, and experiment on four benchmarks: two for mathematical reasoning
(GSM8K (Cobbe et al., 2021) and MATHS00 (Lightman et al., 2023)) and two for logical reasoning (Countdown (Pan
et al., 2025) and Sudoku (Arel, 2025)). We follow the same train-test splitting, reward functions, and evaluation protocol
as D1 and WD1, except for Sudoku. For Sudoku, to avoid train-test leakage, we take the training set from D1 and split
the data by Sudoku answers, ensuring that the test set contains entirely new puzzle solutions. This guarantees that the
model cannot solve test puzzles merely by memorizing possible answers. All experiments are conducted in the zero-shot
setting, except for Sudoku, where 3-shot generation is used for both training and evaluation’. For all models, we employ
Low-Rank Adaptation (LoRA) with a rank of r = 128 and scaling factor a = 64. For SPG, we report results using both

‘Proposition 1 extends directly to a single, coordinate-independent optimizer w* obtained by minimizing the sum of coordinate-wise variances.
2We use 3-shot generation for Sudoku because zero-shot is too difficult for this task, resulting in very few meaningful RL rollouts. Few-shot
examples used in our experiments are provided in Section D.3.


Table 1 Model performance on four reasoning benchmarks. The best results are bolded and the second best are underlined. SPG
consistently outperforms all other methods. We denote the absolute gain of test accuracy to the previous state-of-the-art in green.

GSMB8K (0-shot) MATHSOO (0-shot) Countdown (0-shot) Sudoku (3-shot)
Model / Seq Len 128 256 512 128 256 512 128 256 512 128 256 512
LLaDA-8B-Inst. 69.5 77.2 79.8 28.2 32.4 34.6 18.8 16.8 16.8 af aT 26.2
LLaDA-1.5 70.4 80.5 81.9 26.8 32.2 35.8 21.9 21.1 215 7A 26.9 29.0
D1 72.2 80.6 813 31.4 36.0 39.4 30.9 30.9 34.4 Va 325 29.3
WDI1 74.6 81.5 83.0 31.0 37.4 39.0 48.8 52.3 50.8 a3.1 32.1 22.5
UniGRPO 74.9 82.5 82.7 32.4 37.4 39.4 44.5 43.0 57.0 59.0 67.0 62.9

SPG w/ EUBO T1A 83.8 83.9 33.2 37.6 39.4 68.4 71.5 68.0 81.2 87.1 89.9
SPG w/ Mixture 78.5+3.6 86.1+36 84.5+15 33.4+10 40.0+26 41.8+24 68.8+20 70.7+18 70.3+13 82.9+24 94.0+27 93.1+30

GSM8K MATH500 Countdown Sudoku

1.8
0.8 1.0

1.6
0.8

0.6
1.41 06
[ 0.4
aa 0.4 /
1.0 0.2 0.2
0 2000 4000 6000 0 1000 2000 3000 4000 0 2000 4000 6000 0 500 1000 1500 2000 2500
Steps Steps Steps Steps

— D1 —— WD1 — UniGRPO

SPG (ours)

Figure 3 Reward dynamics of SPG w/ Mixture during RL training, compared with D1, WD1, and UniGRPO. SPG consistently leads
to faster convergence and higher reward level. We report mean and standard deviation over a rolling window of 50 steps.

— (i.e., SPG w/ EUBO) and Luix (i.e., SPG w/ Mixture) for negative advantage traces. We select the value of ( in
the EUBO from {1.0, 1.5, 2.0} based on the best average test accuracy across all generation lengths, and fix the mixture
coefficient w at 0.5. Further experimental details are in Section D.1 and Section D.2.

Baselines. | We compare our method with several recent RL algorithms for dLLMs, including D1 (Zhao et al., 2025),
WD! (Tang et al., 2025), and UniGRPO (Yang et al., 2025). For D1 and WD1, we reproduce results using the official
codebases and instructions, and for fair comparison, we omit the additional SFT stage in D1 across all models. For
UniGRPO, since the code is not publicly available and the original work focuses on vision-language multimodal models,
we reimplement the algorithm within our setup. For consistency, we set the number of inner gradient updates ju to 4 for
all models, following GRPO (Shao et al., 2024). We also evaluate LLaDA-1.5 (Zhu et al., 2025) under our settings,
which fine-tune LLaDA-8B-Instruct using VRPO, a preference optimization approach on 350K preference pairs.

Generation and Evaluation Setup. For both RL rollouts and evaluation, we use the semi-autoregressive confidence-
based decoding strategy, following LLaDA, D1 and WD1. We apply the same generation setup as D1, with the denoising
timestep set to half the total sequence length. The sequence is divided into blocks of 32 tokens, and in diffusion step,
we unmask the 2 tokens with the highest confidence (measured by the probability of the sampled token) within the
current incomplete block. During RL rollout, to encourage diverse outputs, we use a generation length of 256 and a
sampling temperature of 0.9 across all benchmarks, except for sudoku, where the temperature is set to 0.3 as in D1.
During evaluation, the sampling temperature is set to 0.0. We evaluate the models every 100 steps, reporting results
from the checkpoint that achieves the highest average test accuracy across generation lengths of 128, 256, and 512.

Results. We provide the performance of SPG on each benchmark in comparison to the base model and other baselines
in Table |. Both SPG w/ EUBO and SPG w/ Mixture consistently achieve significant improvements over the baselines
across all tasks and generation lengths, with the Mixture approach that combines ELBO and EUBO for negative advantage
traces yielding the best performance. In particular, at a generation length of 256, SPG w/ Mixture improves the test
accuracy over the previous state-of-the-art by 3.6% on GSM8K, 2.6% on MATHS500, 18% on Countdown, and 27% on
Sudoku, showcasing the effectiveness of SPG to conduct RL for dLLMs. Reward dynamics throughout training are
illustrated in Figure 3, where SPG shows a rapid and steady increase in reward over the optimization steps, further
demonstrating its efficiency and robustness. We provide additional results and comparisons to the baselines in Table 4
and Section E.1.


Table 2 Ablations on log-likelihood estimation methods for negative Table 3 Ablations on the masking strategies in Monte Carlo
advantage traces. The best results are bolded and the second best estimation. We denote the absolute gain of test accuracy to
underlined. We denote the absolute gain of test accuracy to SPG random masking for each model in green. Our block-wise
w/ ELBO in green. SPG w/ Mixture consistently outperforms other masking strategy leads to consistent improvement to random

likelihood estimation methods. masking on both benchmarks.
Model GSM8K MATHSOO Countdown Sudoku Model Masking MATHS500_ Countdown
SPG wo/ neg 714 32.7 45.5 68.8 SPG w/EUBO random 36.7 45.4
SPG w/ ELBO 80.9 37.4 67.1 82.4 block-wise 36.7+0.0 69.3+23.9
SPG w/ EUBO 81.6 36.7 69.3 86.1 SPG w/ Mixture random 36.9 62.8
SPG w/ Mixture = 83.1+2.2 38.4+1.0 69.9+2.8 90.0+7.6 block-wise 38.415 69.9+7.4

4.2 Ablations and Further Analysis
We conduct a series of ablation studies to gain deeper insights from the following aspects:

¢ The contribution of each individual component, including log-likelihood estimation methods for negative advantage
traces (Table 2) and the masking strategy in Monte Carlo estimation (Table 3).

¢ The effect of key hyperparameters, including 3 that controls the tightness of the upper bound and the mixture coefficient
w (Figure 5).

¢ The robustness of our approach under various inference strategies (Figure 6).

Due to computational constraints, some ablation experiments are conducted on a representative mathematical reasoning
benchmark (MATHS500) and a logical reasoning benchmark (Countdown). Unless otherwise noted, we report average test
accuracy across generation lengths 128, 256, and 512 for the ablation studies, with detailed results for each generation
length provided in Section E.2. In Section E.2, we also investigate alternative log-likelihood estimation methods for
positive advantage traces in place of ELBO, as detailed in Table 11, and study the diversity of model generations by
evaluating the pass @K performance of each model in Table 12.

Ablations on Algorithm Components. We first study the impact of
different log-likelihood estimation methods for negative advantage traces in
Table 2. Specifically, we compare our approach using Leupo or Lix with
those using Le_po (SPG w/ ELBO) or omitting the negative advantage
loss entirely (SPG wo/ neg). Removing the negative advantage loss

me)
results in a substantial performance drop, highlighting the importance of s
negative advantage penalties to RL. Additionally, both Mixture and EUBO 4
methods outperform ELBO (except for EUBO in MATH500), showcasing
. ar . —— SPG w/ ELBO
the benefits of evidence upper bound regularization for negative rewards. — S66 w/ EUG
We provide complete results for each generation length in Table 6. : —  SPG w/ Mixture

0 500 1000 1500 2000 2500

The effect of log-likelihood estimation methods is further illustrated by Gkeps

the reward dynamics of each model in Figure 4, taking Sudoku as an
example. SPG w/ ELBO converges rapidly during training but plateaus
early, as minimizing the lower bound does not necessarily minimize the likelihood estimation methods for negative ad-
true log-likelihood for negative advantage traces. In contrast, SPG w/ vantage traces on Sudoku. SPG w/ Mixture
EUBO achieves higher final rewards but converges more slowly and less leads to both fast convergence and high rewards.
stably. Combining both, SPG w/ Mixture attains fast, stable convergence

and high rewards, leading to an effective balance. This aligns with our

discussions in Section 3.3.

Figure 4 Reward dynamics of different log-

We also conduct ablations on the masking strategies in Monte Carlo estimation of LgLgo, Leuno, and L va As shown in
Table 3, the block-wise masking strategy outperforms random masking, demonstrating the importance of aligning input
distributions between policy rollout and optimization. We provide complete results for each generation length in Table 7.

Ablations on Key Hyperparameters {3 andw. We first examine the effect of 6, a crucial hyperparameter in evidence
upper bound estimation, in panels (a)-(d) of Figure 5. In general, a relatively small value of ( (i.e., close to 1.0) leads to
a tighter bound and thus better performance. Nevertheless, SPG consistently performs well across a range of 3 values


= (a) GSM8K (b) MATH500 (c) Countdown (d) Sudoku (e) MATH500 (f) Countdown

2

£ 83.1 38.4 72 90.0 38.5 38.4 70.9
—A——_a :

283.0 38 80

o 69.9 70

© a5 70 ‘ 38.0

U

oO 37 36.7 60 37.5

& 82.0 81.6 68 9.3 68

3)

D815 36 40 37.0

£

@ 81.0 66 36.5

2 o5 #210 15 20 OF 10 15 20 O05 10 15 20 O5 10 15 20 0.0 05 10 0.0 0.5 1.0

B B B B wW w

—— SPG w/ EUBO — — SPG w/ Mixture

Figure 5 (a)-(d): ablations on the effect of 3 in the upper bound; (e)-(f): ablations on the mixture coefficient w. The best performed
8B > 1andw € [0,1] are marked by triangle in each setting.

Semi-AR, Block=16, Semi-AR, Block=32, Semi-AR, Block=64, Semi-AR, Block=32, Full Sequence, Full Sequence,
~ Confidence Confidence Confidence Random Confidence Random
x 73.5 7
S7o| +16.2% 1 68.7 +15.0% tf 48.7% 1, 70.0 +14.6% t Anes +16.7% t
3 4.0
6 : 58.58.2
5 88 7 $3 5 .

50 :

Z i ° a 42.913. 2 : :
© 40|37.37.8 38.5940. . po.

uv :

oS : .

@ 30 5

a

LLaDA-8B-Instruct LLaDA-1.5 i WD1 Ma UniGRPO Ml =SPG w/ EUBO Mi™ =SPG w/ Mixture

Figure 6 Ablations on inference strategies, including different combinations of decoding orders (i.e., semi-autoregressive (semi-AR)
decoding with varying block sizes and full sequence decoding) and unmasking approaches (1.e., confidence-based and random
unmasking). We set generation length to 256 and report the average accuracy across four benchmarks. SPG consistently outperforms
all baselines by a large margin across different inference strategies.

on most tasks, indicating its robustness. For our main results in Table 1, we fix w = 0.5 and select the optimal 6 > 1,
resulting in 8 = 1.0 for Sudoku and 6 = 1.5 for the other three benchmarks, except for Countdown with SPG w/ EUBO
where @ = 2.0. Besides, since the ELBO corresponds to the case of 8 = 0 theoretically and EUBO corresponds to
8 => 1, we also investigate intermediate values 0 < 8 < 1, which may serve as an implicit mixture of lower and upper
bounds. However, it is unstable in Sudoku and underperform SPG w/ Mixture on most benchmarks.

We also experiment on the effect of the mixture coefficient w, keeping ( fixed at its optimal value determined for w = 0.5
as mentioned before. As illustrated in panels (e)-(f) of Figure 5, combining lower and upper bounds with w € (0, 1) leads
to better performance than leveraging either bound solely, resulting in an inverted U-shaped curve. This observation is
consistent with our analysis in Proposition | and Section 3.3. We provide complete ablation results of 3 and w for each
generation length in Table 8 and Table 9.

Ablations on Inference Strategies. In the above experiments, we adopt a consistent state-of-the-art inference setup
during both RL rollout and evaluation, i.e., confidence-based, block-wise semi-autoregressive generation with a block
size of 32. The same configuration and block size are also used in our block-wise masking strategy. This raises the
question of whether our approach generalizes well to alternative inference strategies. To assess this, we evaluate the base
model and all RL fine-tuned models using various inference strategies, as shown in Figure 6. Despite being trained
under confidence-based semi-AR decoding, SPG consistently outperforms all baselines by a substantial margin across
all inference strategies, demonstrating its robustness and strong generalizability. Complete results for each benchmark
individually are provided in Table 13.

5 Related Work

Diffusion Language Models. Building on the remarkable success of diffusion models for image generation in
continuous domains (Song et al., 2020; Ho et al., 2020), researchers have explored their extension to discrete data such as
text. Initial attempts focused on training continuous diffusion models in the text embedding space (Li et al., 2022; Gong
et al., 2022; Han et al., 2022; Sahoo et al., 2025a), while they face challenges in optimization and generalization due to
the discrete nature of text data. Masked diffusion models (Lou et al., 2023; Zheng et al., 2023; Campbell et al., 2024;
Sahoo et al., 2024; Shi et al., 2024) address this by defining the diffusion process directly in the discrete token space,
using random masking as the forward process, and have achieved strong empirical results. Block Diffusion (Arriola


et al., 2025) further advances this direction by combining the strengths of autoregressive models, such as the capability
to generate variable-length outputs and using KV cache to accelerate inference, with the benefits of diffusion language
models like parallel decoding and flexible, any-order generation within blocks. Recently, large-scale diffusion language
models trained with masked diffusion objectives have demonstrated performance competitive with similarly sized
autoregressive models (Nie et al., 2025; Gong et al., 2024). More recent works (Wu et al., 2025; Ma et al., 2025; Liu
et al., 2025a; Sahoo et al., 2025a,b) have introduced caching and parallel decoding algorithms that greatly enhance the
inference efficiency of dLLMs.

Reinforcement Learning for LLMs and Reasoning. The seminal works apply reinforcement learning to large
language models (LLMs) to align them with human preferences via reinforcement learning from human feedback
(RLHF) (Christiano et al., 2017; Ouyang et al., 2022). More recently, reinforcement learning has proven highly effective
at enhancing the reasoning abilities of LLMs during the post-training stage, where rewards can be provided by a process
reward model (Lightman et al., 2023) or verifiable reward signals. Algorithms such as Proximal Policy Optimization
(PPO) and Trust Region Policy Optimization (TRPO) constrain policy updates to a trust region, reducing variance
and promoting stable learning by preventing excessive shifts from the reference policy (Schulman et al., 2015, 2017).
Group Relative Policy Optimization (GRPO) (Shao et al., 2024) introduces group-relative rewards, enabling efficient
training without the need for an additional value (critic) model. GRPO and its variants have demonstrated strong
empirical performance in state-of-the-art models such as DeepSeek-R1 (Guo et al., 2025), particularly on mathematical
reasoning tasks, where incorporating long reasoning traces with self-reflection and verification steps yields significant
improvements. Recent works (Liu et al., 2025b; Zheng et al., 2025; Team et al., 2025; Cohen et al., 2025) further
improve RL algorithms for LLMs by reducing the bias introduced by the GRPO objective, enhancing sample efficiency,
and introducing additional regularization.

Reinforcement Learning for Diffusion Language Models. | Numerous studies have explored RL-based fine-tuning
algorithms for diffusion models with continuous objectives (Fan et al., 2023; Black et al., 2023; Clark et al., 2023).
While RL algorithms have achieved notable success to LLMs and continuous diffusion models, their applications to
diffusion language models in the discrete space remain underexplored. DRAKES (Wang et al., 2024) leverages reward
backpropagation along the denoising trajectory, but is computationally intensive for large scale models as the gradients
are propagated through each denoising step. Alternatively, methods like D1 (Zhao et al., 2025) and UniGRPO Yang et al.
(2025) utilize the GRPO framework, approximating the log-likelihood through either a one-step unmasking (as in D1)
or Monte Carlo estimation using the ELBO (as in UniGRPO). VRPO (Zhu et al., 2025) adapts DPO (Rafailov et al.,
2023) to fine-tune dLLMs by applying MC estimation of the ELBO. WD1 (Tang et al., 2025) starts from the GRPO
formulation and the same log-likelihood estimation as in D1, while avoiding direct estimation of the old and reference
policy log-likelihoods by integrating them into a weighted policy optimization objective. Despite these advances, a
principled analysis of RL algorithms for dLLMs, especially the challenging log-likelihood estimation, is missing. This
results in substantial bias in the optimization objective and suboptimal performance.

6 Conclusion

We propose SPG, a novel reinforcement learning algorithm for diffusion large language models. SPG addresses the
intractable log-likelihood in dLLMs by maximizing a tractable lower bound on positive reward sequences and minimizing
an upper bound on negative ones, resulting in a more robust and less biased policy gradient. Additionally, we propose a
block-wise masking strategy for Monte Carlo estimation to enhance optimization stability and efficiency. Extensive
experiments on four mathematical and logical reasoning benchmarks demonstrate the superior performance of SPG,
achieving significant improvement over baselines and the state-of-the-art performance.

10


References

Arel. Arel’s sudoku generator. https://www.ocf.berkeley.edu/ arel/sudoku/main.html, 2025.

Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr
Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573,
2025.

Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in
discrete state-spaces. Advances in neural information processing systems, 34:17981—-17993, 2021.

Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning.
arXiv preprint arXiv:2305.13301, 2023.

Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous
time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:28266-28279, 2022.

Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces:
Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human
preferences. Advances in neural information processing systems, 30, 2017.

Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv
preprint arXiv:2309.17400, 2023.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob
Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Taco Cohen, David W Zhang, Kunhao Zheng, Yunhao Tang, Remi Munos, and Gabriel Synnaeve. Soft policy optimization: Online
off-policy rl for sequence models. arXiv preprint arXiv:2503.05453, 2025.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.

Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh,
Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural
Information Processing Systems, 36:79858-79885, 2023.

Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with
diffusion models. arXiv preprint arXiv:2210.08933, 2022.

Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han,
et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.
Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.

Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text
generation and modular control. arXiv preprint arXiv:2210.17432, 2022.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing
systems, 33:6840-6851, 2020.

Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis
Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025.

Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text
generation. Advances in neural information processing systems, 35:4328—4343, 2022.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya
Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference on Learning Representations, 2023.

Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache:
Accelerating diffusion large language models with adaptive caching. github, 2025a.

Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like
training: A critical perspective. arXiv preprint arXiv:2503.20783, 2025b.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

11


Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv
preprint arXiv:2310.16834, 2023.

Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint
arXiv:2505.15781, 2025.

Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li.
Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina
Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information
processing systems, 35:27730-27744, 2022.

Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero,
2025. Accessed: 2025-01-24.

William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 4195-4205, 2023.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference
optimization: Your language model is secretly a reward model. Advances in neural information processing systems, 36:53728-53741,
2023.

Alfréd Rényi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics
and probability, volume 1: contributions to the theory of statistics, volume 4, pages 547-562. University of California Press, 1961.

Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr
Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:
130136-130184, 2024.

Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov. The diffusion
duality. arXiv preprint arXiv:2506.10892, 2025a.

Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing,
John Thickstun, and Arash Vahdat. Esoteric language models. arXiv preprint arXiv:2506.01928, 2025b.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Jnternational
conference on machine learning, pages 1889-1897. PMLR, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv
preprint arXiv: 1707.06347, 2017.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al.
Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.

Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete
data. Advances in neural information processing systems, 37:103131-103167, 2024.

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative
modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.

Bowen Sun, Yujun Cai, Ming-Hsuan Yang, and Yiwei Wang. Blockwise sft for diffusion language models: Reconciling bidirectional
attention and autoregressive decoding. arXiv preprint arXiv:2508.19529, 2025.

Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. arXiv
preprint arXiv:2211.16750, 2022.

Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. wd1l: Weighted policy optimization for reasoning in diffusion
language models. arXiv preprint arXiv:2507.08838, 2025.

Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua
Liao, et al. Kimi k1. 5: Scaling reinforcement learning with IIms. arXiv preprint arXiv:2501.12599, 2025.

Tim Van Erven and Peter Harremos. Rényi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60
(7):3797-3820, 2014.

Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine,
Hanchen Wang, and Aviv Regev. Fine-tuning discrete diffusion models via reward optimization with applications to dna and
protein design. arXiv preprint arXiv:2410.13643, 2024.

12


Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion Ilms can do faster-than-ar inference via discrete
diffusion forcing. arXiv preprint arXiv:2508.09192, 2025a.

Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement learning framework for
diffusion large language models. arXiv preprint arXiv:2509.06949, 2025b.

Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm:
Training-free acceleration of diffusion Ilm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025.

Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion
language models. arXiv preprint arXiv:2505.15809, 2025.

Siyan Zhao, Devaansh Gupta, Qinging Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via
reinforcement learning. arXiv preprint arXiv:2504.12216, 2025.

Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yugiong Liu, Rui Men, An Yang, et al.
Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025.

Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation. arXiv preprint
arXiv: 2302.05737, 2023.

Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al.
Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025.

13


A Basics of dLLMs

In this section, we provide a more self-contained overview of masked dLLMs. Please also refer to Sahoo et al. (2024) for
more details.

Notation. We denote scalars by lowercase letters (x), vectors by bold lowercase (x), and sequences by 21:,. A
superscript (e.g., 7) denotes an item’s index within a group. We define the set of the first k integers as [k] := {1,...,k}
and the k-dimensional probability simplex as A’~!. Distributions include the categorical Cat(- | p) and the uniform
Ua, b|. Throughout the paper, we use the following primary indices: i € [n] for position, 7 € [g] for a sequence ina
group, and t € [0, 1] for the continuous diffusion timestep.

We start from a discrete time version of the diffusion models with finite t € [7]. Assume a one-hot categorical variable
xv € f{e1,...,en} C A*®-1_ Further assume we gradually corrupt x into an absorbing state ™ (i.€., €fmasxj ) with
transition matrix Q, at time t. Then:

t
q(2: | w) = Cat(z | Qiw) = Cat(z | |] Q-2).
T=1
Here, z; is also a one-hot categorical random variable in A*-1 In practice, one could choose Q; such that:
q(z | x) = Cat(z, | azw + (1 — az)m).
Here, a, = lar = 0,0} < 0.

Normally, the goal is to construct the lower bound of the evidence (ELBO) and maximize it. For this particular case,
consider the discretized Markov chain with T latent variables 21, Z2,..., 27, where 27 = mand z; = x. We use the
shorthand z = 21.7 and write

Fevs0(#; 0) = Bewa(-ta) os q(z | x)

T-1
7 po(2t | 2t+1) po(2r)
=E,no(.\x) | lo xw|2z1)+ lo +lo
xc) | lowpo(@ | #3) d O(a |zn1,8) > glzr| a)
T-1
=| a ee | q(2t | 241, 2)
T-1

po(2t | 241)

Mz.44~q(-|x) MH zervg(-|Zt41,2 g >
ep1~g(-|e) ze~g(|Ze41 | q(2t | Zt41,2)

+
il

1

Here, log pe(x, 21) = 0 because we assume z; = a, and po(zr) = q(zr | x) because we assume zr = m. A
common method to parameterize pg is via predicting x with model 7¢ in gq:

pe(2t | Zt41) = q (2 | Zt41,0 = Ta(- | Zt41))-

Now, given that z:,1 is either m or x (assuming m # a). Then the KL term in equation 10 decomposes into the
following.

0 Zp = 2141= 2,
\ po(2t | 2441) _ 0 Z=™M™, 2141 = 2, (Impossible) (11)
q(2t | 241, 2) logmo(@ | 241) 2 = 2,241 =m,
0 Ze = 241 =™M.

14


Moreover, q(z; = @ | 2141 = m,ax) = “S41

, and note that 79(a | z,) = 1 when z; = a, so we have:

l-ag4i1
T-1 oa
Levpo(;0) = S7 Besyrva(te) [a= oe log no(a | t141)1(Ze41 = m)
t=1 ~ Ot. Db
T-1 —— 2)
= Derg i1~q(-|a) see log Te (ax | as1)] (If 241 = a, then log 7@(a | z++1) = 0)
t=1 a
Taking the above limit as T’ + oo, we have:
1 al.
Levpo(x; 0) = / Se noi (|ee) Fes log 79(a | 2)| . (13)
t=0 at —

Generalization to Sequence The above is for a single categorical variable x. In practice as in language modeling, it
becomes a sequence of categorical variables x1.,. Then we write

Pe)

feano(@ini 9) ~ “Rnsae ee) oe q(Z1:n | Lin)

n

Li, Z1in
= Egz,nq(-le:)}, [> log ee (Independence of q(- | x: ))

n (14)
{zy ~d |e}, q(2i | £i)

i=1
= S- Letpo(xi3 0).
i=1

The key distinction from the single-token formulation (mentioned beforehand) is that the reverse process pg is conditioned
on all 21.,, instead of a single token’s z;.

B Evidence Upper Bound for dLLMs
In this section, we provide the derivation of the evidence upper bound. Following the above section, we start from the
discrete time version of the diffusion models.

Lemma 1 (Rényi Variational Bound; Rényi (1961); Van Erven and Harremos (2014)). Fix an observation x. Let q(- | x)
be any distribution on Z such that p(-. | x) < q(- | x), denoting that p(- | x) is absolutely continuous with respect to
q(- | z). Then, the following holds for any 3 > 1:

B
Lenq(-|x) og ne) < log p(x) < 5 lee Lemq(-le) (223) | (15)

q(z | x) q(z | x)

In view of the above lemma, we derive an evidence upper bound for masked diffusion models in the following theorem.

Theorem 1 (Evidence Upper Bound for Masked Diffusion). Assume the forward denoising process has T steps with a
monotonic schedule a4. For any 3 > 1 and a sequence of categorical variables x 1:n, we have:

log 79(@1:n) < Levso(£1:n3 9), (16)
where
1< ie 4 —a
Leupo(®1:n3; 9) = B S/ log eet [sce * L(Ze413 => m) 18 (a; | Zt41) + C(T), (17)
j=l t=1 ian

is a constant independent of 0.

A y : —n <
and O(7) = {BBE na~acin)[a(err|a)"], 1S B<m
0, BEn

15


Proof. We first consider the case with a single categorical variable x. On the account of Lemma | and following a
similar argument as in equation 10, for any 6 > 1, we can write

B
lo TT, —lo Maras -|ac nile | apy
crete) < Flog | (Mt
T-1 B
; Po (Ze | 241) )
= —logE, ong(le olzs | Ze4.0) ”
B s ur~g(| [I Caos
Note that the sequence z1.7 has a form {x,...,a%,m,...,m}. Define the transition event:
At = {21 = &, 2141 = m} ”
Then, by the law of total expectations, equation 18 can be expressed as:
TPs
Lisa le mes Lesa)"
Bg 8 MevrnaCle) | LN \ Ge, | ini, @)
r T-1 B
1 ; po(zZs | 2541)
3 2, (Ai) q(-|) HT q(Zs | Zs41,2) t
is B
1 . po(2e = @ | 241 =m)
te ] +z. ~O |e 1 ~ ~ 7
B 8D rad | (241 = M)q(% = 2| 2y41=m » (7 = @ | 241 = ™,@)
1 T— Ot a
1 : . i _ Ot = O+1 8 20
B og d, ze41~4( | (241 =m) 1— at41 a (@ | et) _

The penultimate line is due to the fact that conditioned on the event A;, the ratio ait
s #t. The last line uses the formula for g. The indicator 1(z, = m

po (%s|Zs41)
Zs|Zs+1,@)
) appears in the final expression because the terms

is equal to one for any

in the bound are only non-trivial when the model must make a prediction from a corrupted state.

Now we generalize the above to a sequence of categorical variables x = %}.,,. Similar as Equation (18), we have

i
log T@(®1:n) < 3

Denote C(T) := 7 log

T-1 n

oat TTS

pe ( Zi | Zt41)

B
(2 | Ze41, ) |

lo

09

Ree eriegt- a) [aleur | x)-"] . The upper bound in the RHS can be further derived as

1, = Pe pols As) ) |
— log Ey, col -lx
“og ve-aclo| LTT ( See
n a4 a 4 B
1, ip i i Po(Yii | Yir1 )
=—7 108 Bz. pw~q(-\x) | W\41:T | & yi: x)1 (yy. = 21:T Gear
leas r~acin ates L2)-"T] alan | 2080 = er) TT (Sige gee
Yur
Lo ata + / volyislyivr \?
a ie
B pelt a (Yi | Year ® ED)
Yur
TT B
1, . _% Poi: | Ysa
B 1 tel GYi | Vegi ® x)
ely
n T-1 B
1, . roles | 21) 7 1 . 7
=~ 10 “21.p~q(-|a += lo zur~at-le)| 41: x *]
3 ian sareogh ‘i — g 8B esr~a(-le) [124-7 | #)
n T-1 B
1 pe(Zt,1 | Zt41) ) | a
= zy walla ——— + C(L (21)
= 3 2 Ne vera {Tl (fee ) @)

16


Here, y'. are copies of 21.7 enforced to agree with 21.7 using the indicator I(y}., = z1.r). C(T) is a constant
independent of 0, and the first term in Equation (21) can be derived similar to the single variable case in Equation (20):

Lg TH ( pelzii | ze41) \°
7 (e) t,t t+1
i=l

t=1 q(Zti | 241, 2)

n T-1
1 . Qt — At+1
“3 SS log SO Bx, ys.a¢-le) oes “U(ze41,1 = m) + 75 (xi | a)
i=l t=1

1 — ae41

Additionally, for 6 > n, we can obtain a tighter bound using Hélder’s inequality:

1 T-1 on ne (z | z ) B
liek, | sas wy |
B calidad Wu q(Zti | 241, 2)

T- n
Ven.r~q(-|a) = q(Zt,i ; 241, 2)

. Qt — At4+1 n
1 y Der y1~q(-|@) oes “A (zt41,1 =m) +75" (a | a) (22)

1 — a¢41

Denote B := n3. Then the upper bound log 79(a1:n) < LeuBo(#1-n; 9) holds with the constant term C'(T) being zero
C(T), 1<B<n

and the hyperparameter B > n. Combining these two cases, we have C(T) = is a>
‘ >n

Furthermore, we can derive the continuous time version by omitting the constant term that does not affect the gradient
with respect to @, and taking the limit of T — co similar as the derivations for Cg_go, as shown in Corollary |:

Corollary 1. Taking the limit of T — ov, we have:

VoLeuso(®1:n;9) = Vo (Leve0 (a1: 0) + c(T)) =VoLeveo(1:n39), where
(23)

1 n
Leveo(@1:n3 8 ~ Be dW giz, [w(t) “1(21,, =m): m6 (@i | 24)) -

One caveat of the above Lgypo is that the log is outside of the expectation, which in general makes Monte Carlo sample
estimates biased. One could certainly further loosen the bound using the inequality log 7 < « — 1:

n

“tu [0,1], 2+~4 oo (21,4 = m) + 76 (2: | 2) —% (24)

le

Il
a

Leuso(a) <

p

But in practice we found this results in much worse performance, as demonstrated in Table 10, potentially due to the
much larger gap between EUBO and likelihood.

C_ Additional Analysis on Upper and Lower Bounds

C.1_ Proof of Proposition 1

Proposition 1 (Optimal Mixture Strictly Reduces Variance). Fix a coordinate k and let

pa = w(t, 2.)mg (wi | 21,€)/E [w(t, 2.)m6 (wi | 2150)],

17


where w(t, 24) = w(t)1(z_ =m). Then, the gradient of mixture objective (8) is given by
Ju,k = ((1—w)w(t, 2) + wpa) do, log (x | 24, €). (25)

If Var((pg — w(t, Z¢))O0, log me(a | Z,€)) > 0, then Var|g..,x] is a strictly convex quadratic in w and thus admits a
unique minimizer w;. Moreover,

Var[gus,n] < min{ Var[go,x], Var[gi,x]},

Proof. We first derive the formulas for the gradient of each objective. Consider a specific example x;. The gradient of
the Lego and Leggo are given by:

VoLetso = E [w(t, 24) V log t9(x; | 22, c)] (26)
7 7 [wte, 21) m5 (a; | zz, c)V log o(ax; | 22, o)|
VoLeuBo = 3 (27)
5 |w(t, 21) m6 (@: | 21,¢)]
Then the gradient of the mixture objective Lmix is given by:
Volmix =E (a —w)w(t, 2) + wpe)Ve log w(x; | Zz, o)| (28)

We further compute the per-parameter (per-dimension) variance of the gradient of Luix and consider the optimal mixture
coefficient w to minimize the variance. For simplicity, we use the following short-hand notation:

Sh = Oo, log m9(x; | Zt, €)

We denote the k-th coordinate of the gradient Valine by gu,z- Then, the coordinate-wise variance of the gradient is
given by

Var | g.,e] = | ((1 —w) w+ w pp)” sz | — ( A (1 — w) w + w pg) si])
= Var(ws,) + 2w Cov(wsp, (pg — w)sp) + w? Var((pg — w) sx)

where we used the shorthand w = w(t, z;). The above expression is quadratic in w and we find the optimal w by setting
the derivative of variance to zero:

2 Var [gw,x| = 2 Cov(w Sk, (pe — w) Sk) + Qu Var ((pg —w) Sk) =0

Cov(w Sk, (Pe — w) Sk)
Var((pg—w) Sr)

The above yields a per-coordinate optimal wi. Equivalently, we can write w7 as follows:

Sup =

Var (w Sk) _ Cov (w Sk, PB Sk)

wk = Var (w Sk) + Var(pg Sk) —2 Cov(w Sky PB Sk)

Furthermore, w% is a minimizer of coordinate-wise variance in the non-degenerative case with Var (( pp w) Sk) > 0,
as the variance is strongly convex in w.

The coordinate-wise variance of gradients in Le_po (w = 0) and apo (w = 1), and the optimal mixture coefficient w*
are then given by

LELBO Var [90,x| = Var [w Sk]
Lave: Var [91,%] — Var |[w sk| +2 Cov (w Sk, (pg — w) Sk) + Var (pa —w) Sk);
2
( Cov(w Sk, (pg — w) st)
Var ((pg — w) 8x)

Optimal: Var [Gur] = Var [w sk

%

18


GSM8K Math500 Countdown Sudoku

Ww

2.0

E |
) | 1.5
z2 fF | 2
= plat Lo
<1 1
(6) 0.5 wi

of ol a Cenatle. otayl AEH YN was 0.0

0 1000 2000 3000 0 1000 2000 3000 0 200 4000 6000 0 1000 2000
Steps Steps Steps Steps
SPG w/ ELBO — SPG w/ EUBO — SPG w/ Mixture

Figure 7 Dynamics of the gradient norm of models trained with different log-likelihood estimation methods. SPG w/ Mixture achieves
lower gradient norm and more stable optimization. We report mean and standard deviation over a rolling window of 50 steps.

The difference between the variance of LeL_po and Fuca with the optimal mixture coefficient can then be derived as
follows:

( Cov(w Sk, (pe — w) si)
Var ((pg — w) 8x)

( Cov(w Sk, (pa — w) 8k) + Var((ps — w) sx)
Var ((pg — w) sx)

Var [w sk] — Var [Iw k] = >0

>0

Var [pg sx] — Var[gux.n] =

C.2 Additional Comparison Between the Mixture Loss and the Lower and Upper Bounds

Comparing Mixture withthe LowerBound. Consider the ratio of the coefficient of score function Vg log 7 (a; | 22, €)
in the gradient in the case of the mixture objective (i.e., Ve£mix in Equation (28)) over using only the lower bound (i.e.,
Vo£exgo in Equation (26)):

woix _ (L—w)w(t, 24) + wpe _ m9 (Xi | 24; €)

WELBO w(t, 24)

(l-w)+w

b | w(t, 24)m6 (a: | 20,0)

Treating the expectation over all samples [wtt, 2,)mg (ax; | Ze, o)| as a constant (since it is averaged), the second term

in the above ratio is strictly increasing in 1 (a; | 24, c). This realizes a confidence-aware weighting: uncertain tokens
with small ne (a; | 2, C), ie., those with a low recovery chance, have a smaller weight, while confident tokens with
large ne (a; | Zz, c) are upweighted, with sharpness being controlled by parameter ( and the blend by w.

Comparing Mixture with the Upper Bound. We compute the ratio of coefficient of score function in the gradient of
upper bound (i.e., Vg£eupo in Equation (27)) over the mixture gradient:

WEUBO _ WpB

wmix  (L— w) w(t, 24) + wpe

Considering the above ratio, when ng (a; | 24,c) is very small, the coefficient of score function in Walang, WEUBO>
becomes very small, preventing updates to the parameters. However, the mixing approach maintains per-sample
weights by preventing that from collapsing to (near) zero. In other words, for each sample, the mixture coefficient
computes a convex interpolation that simultaneously floors very small EUBO weights to a minimum value and applies
an uncertainty-aware capping to large EUBO weights.

Empirical Evidence of Reduced Gradient Variance. _As a practical indicator of gradient variance, we plot the gradient
norm of each model trained with different log-likelihood estimation methods for negative advantage traces in Figure 7.
When using the mixture objective, the model has consistently smaller and more stable gradient norm throughout training,
aligning well with our theoretical analysis.

19


C.3. Toy Example for Upper and Lower Bounds.

In this section, we provide a toy example highlighting the contrasting behaviors and landscapes of the upper and lower
bounds, further demonstrating the necessity to select the appropriate bound for optimization based on the optimization
direction.

Consider a simple case where the sequence length is 2 and the vocabulary size is 2, i.e., 2 = [a 1, x2] and V = {A, B}.
Then, We can calculate Cer go and Leygo in closed form:

1
Letpo(@ = AA) = 5 [log me(a1 =A | MA) + log mo(x1 = A| MM) (29)

+ log m9(a2 = A | AM) + log 76(a2 =A | MM) | (30)
- 1 ° (a, =A| MA) +76(a, =A | MM
Exuno(@ = AA) = 5 log (72021 = ATMA) roles = A | MM) G1)

B 2
B 8
| ; le ("2 (x2 | AM) ‘te (a2 | ) (32)

For simplicity, denote a := 79(a , = A | MA) and b := 7@(a1 = A | MM), and consider the of the likelihood of the
first token 2;. We have

1
Letpo(#1) = 3 (losa + log b) (33)
= 1 a® + 8
Levso(e1) = los (——) (34)
Take the partial gradient with respect to a and b respectively,
OLerpo(#1) 1 OLepo(#1) 1 5)
Oa 2a’ Ob ~ 2b
OLeuso(#1) _ a?! OLzupo(aw1) _ 6° 36)
Oa a + b8? Ob ab +b

Therefore, for Leones the gradient direction is dominated by the larger one between a and b, while for Letgo, the gradient
direction is dominated by the smaller one. Such property is illustrated in the landscapes of —Lg_go and —Lgupo for
a,b € (0,1) in Figure 8.

When a = AA has negative advantage, the corresponding Lego and re are minimized. For Ly go, the model
benefits more from further decreasing the smaller one between probabilities a and b. In the extreme case, Le_po = —0O
when either a or b equals to zero, leaving the other term not sufficiently decreased. Instead, when using Losi for
negative advantage traces, the larger one between a and b is preferentially minimized, leading to a more balanced
optimization that stably decreases the log-likelihood.

Similarly, when «2 = AA has positive advantage, the corresponding Ce_go and Carne are maximized. Using Le_po
enables effectively increasing the smaller likelihood, while Cgygo focuses on the larger one, leading to a less efficient
optimization.

D Additional Experimental Details

D.1 Datasets and Reward Functions

We follow the setting in D1 (Zhao et al., 2025) and WD1 (Tang et al., 2025), using the same reward functions and
train-test splitting, except for Sudoku. The rewards are designed to encourage both correctness and proper formatting,
with varying levels of granularity tailored for each task. For completeness, we provide details as follows.

20


—LELBO —LEuBO

3.5

3.0

2.5

2.0

Figure 8 Landscapes of —Legrpo and —rmo for0 <a,b< 1. —Lrmo is flatter among low value regions while sharper among
high value regions, making it more suitable for log-likelihood minimization; vice versa for —LeELgo.

GSM8K. We utilize the train split of the GSM8K dataset? for RL training, and evaluate model performance on the test
split. We follow the Unsloth reward setup’, utilizing five equally-weighted additive components:

¢ XML Structure Reward: +0.125 per correct formatting tag; small penalties for extra contents after the closing tag.

¢ Soft Format Reward: +0.5 for outputs matching the pattern:
<reasoning>...</reasoning><answer>...</answer>

¢ Strict Format Reward: +0.5 for exact formatting with correct line breaks.
¢ Integer Answer Reward: +0.5 if the answer is a valid integer.
¢ Correctness Reward: +2.0 of the answer matches the ground truth.

MATHS500. We utilize the train split of the MATH dataset? for RL training, and evaluate model performance on the
test split. We use a format reward and a correctness reward:

¢ Format Reward: We award 1.00 if <answer></answer> tags are present with \boxed inside them; 0.75 if answer
tags are present without \boxed; 0.50 if answer tags are not present but \boxed is present; 0.25 if neither the answer
tags nor \boxed is present.

* Correctness Reward: We award 2.00 if the answer in \boxed{} matches the ground truth.

Countdown. We utilize the train split of the Countdown dataset® for RL training, restricting to instances that use only
three numbers. We evaluate on the same set of 256 synthetically generated countdown questions with 3 numbers as in
D1 (Zhao et al., 2025). The reward covers three cases: +1.0 if the expression reaches the target using the exact numbers;
+0.1 if the numbers are correct but does not reach the target; +0.0 otherwise.

Sudoku. We experiment on the 4x4 Sudoku dataset’ generated by Arel (2025). The original training split contains
1M unique Sudoku puzzles covering all 288 4x4 Soduku solutions. To avoid train-test leakage and potential cheating by
memorizing all the solutions, we randomly select 200 solutions and include all puzzles corresponding to these solutions
into the new training set, resulting in 694,006 training puzzles. We then randomly select 2 or 3 puzzles corresponding to
the left 88 solutions to construct the test set, which has 256 Soduku puzzles in total.

3https://huggingface.co/datasets/openai/gsm8k
4https://unsloth.ai/blog/r1-reasoning

Sinttps:/ /huggingface.co/datasets/ankner/math-500
Snttps://huggingface.co/datasets/Jiayi- Pan/Countdown- Tasks-3to4
Thttps://github.com/Black-Phoenix/4x4-Sudoku-Dataset

21


We observe that the zero-shot setting is too difficult for the base LLaDA-8B-Instruct model, which has test accuracy below
7% with a generation length of 256 and struggles to correctly interpret the questions, leading to very few meaningful RL
rollouts. Therefore, we instead use 3-shot for all the Sudoku experiments. We ensure that the solutions presented in the
3-shot samples do not appear in test set solutions, and the puzzles do not appear in both train and test set. The detailed
few-shot samples are provided in Section D.3.

D.2 Hyperparameter Settings and Implementation Details

We follow D1 (Zhao et al., 2025) for most hyperparameter settings. We employ Low-Rank Adaptation (LoRA) with a
rank of r = 128 and scaling factor a = 64. The training was conducted on 8 NVIDIA A100-80G or NVIDIA H100-80G
GPU, with the following hyperparameters: batch size of 6 per GPU, and gradient accumulation steps of 2. We set the
number of inner gradient update jz as 4 for all models. We use the AdamW optimizer (Loshchilov and Hutter, 2017),
with 8; = 0.9, 82 = 0.99, weight decay of 0.1, learning rate of 3 x 10~°, and gradient clipping at 0.2. We utilize Flash
Attention 2 (Dao, 2023)and 4-bit quantization.

For RL rollout, we use sequence length of 256 tokens, and 128 diffusion steps. We employ confidence-based semi-
autoregressive generation with block size 32, and set the temperature as 0.9 (except for Sudoku where temperature is
set as 0.3 following D1). We set number of completions per prompt g as 6, and number of Monte Carlo estimation
samples m as 2 due to computational constraint. Since the rollout stage dominates the training time, the average time per
gradient update step for SPG is similar to that of the other baselines.

We train 6000 steps (i.e., number of gradient updates) for GSM8K and Countdown, 4000 steps for MATHS500, and 2500
steps for Sudoku. For all RL models, we run evaluation every 100 steps with generation sequence length 128, 256, 512,
and report the result of the checkpoint with the highest average accuracy over the three generation lengths, except for the
ablations on inference strategies in Table 13, where we only evaluate on generation length 256.

D.3 Few-Shot Samples for Sudoku
Sample 1:

Question:

Solve the following Sudoku puzzle: 3014002020004 130
Answet:

<reasoning>

Interpret puzzle as 4 rows of 4:

R1: 3014

R2: 0020

R3: 2000

R4: 4130

Fill easy singles:

R1 missing 2 > R1C2=2.

R4 missing 2 + R4C4=2.

Box D (R3-4,C3-4) then needs 1,4; column4 can only accept 1 — R3C4=1, R3C3=4.
R3 now missing 3 > R3C2=3.

Column! missing | — R2C1=1.

Column2 missing 4 — R2C2=4.

Last cell R2C4=3.

Final grid:
R1: 3214
R2: 1423
R3: 2341
R4: 4132
</reasoning>
<answer>

22


3214142323414132
</answer>

Sample 2:

Question:

Solve the following Sudoku puzzle: 0000100420013 142
Answer:

<reasoning>

Interpret puzzle as 4 rows of 4:

R1: 0000

R2: 1004

R3: 2001

R4: 3142

Fill easy singles:

Coll missing 4 > R1IC1=4.

Col4 missing 3 + R1C4=3.

Box A (R1-2,C1-2) missing 2,3 and R1 now needs 1,2 — RIC2=2, R2C2=3.
RIC3=1.

R2 now missing 2 > R2C3=2.

Col2 missing 4 + R3C2=4, then R3C3=3.

Final grid:

R1: 4213

R2: 1324

R3: 243 1
R4:3142
</reasoning>
<answer>
4213132424313142
</answer>

Sample 3:

Question:

Solve the following Sudoku puzzle: 2001403002001420
Answer:

<reasoning>

Interpret puzzle as 4 rows of 4:

R1: 2001

R2: 4030

R3: 0200

R4: 1420

Fill easy singles:

R1 missing 3,4; Col2 can’t be 1 so RIC2=3 — R1C3=4.
R4 missing 3 > R4C4=3.

Col4 missing 2,4; R2 must take 2 — R2C4=2 — R2C2=1.
Coll missing 3 + R3C1=3.

Col3 missing 1 — R3C3=1 > R3C4=4.

Final grid:
R1: 2341
R2: 4132
R3: 3214

23


R4: 1423
</reasoning>
<answer>
2341413232141423
</answer>

E Additional Results

E.1 Additional Evaluations to the Main Results

Complete evaluation results. | We provide the complete evaluation results, along with those reported in D1 (Zhao
et al., 2025) and WD1 (Tang et al., 2025), in Table 4. Our reproduced numbers closely match the reported results.
d1-LLaDA (Zhao et al., 2025) denotes the model that conducts first SFT and then RL (using D1). All other models are
trained solely with RL. In D1 and d1-LLaDA, the best result for each generation length is reported separately, whereas
we select a single checkpoint with the highest average accuracy across all three generation lengths, leading to slightly
worse results than the reported numbers. The reported results in WD1 are based on evaluations on fewer checkpoints, so
they are generally a bit lower than our reproduced values.

Table 4 Complete model performance on four reasoning benchmarks compared with baselines. We provide both the reported and the

reproduced results for D1 and WD1. The best results are bolded and the second best are underlined. SPG consistently outperforms all
other models.

GSMB8kK (0-shot) MATHS500 (0-shot) Countdown (0-shot) Sudoku (3-shot)

Model / Seq Len 128 256 512 128 256 512 128 256 512 128 256 512
LLaDA-8B-Instruct 69.55 77.22 79.8 28.2 324 346 188 168 16.8 5.7 27.7 26.2
LLaDA-1.5 70.4 805 81.9 268 32.2 35.8 21.9 21.1 21.5 74 26.9 29.0
D1 (reported) 72.6 79.8 81.9 33.2 37.2 39.2 33.2 31.3 37.1 - - -
D1 (reproduced) 72.2 806 81.3 314 360 394 309 30.9 34.4 7.2 32.5 29.3
dl-LLaDA (reported) 73.2 81.1 82.1 33.8 386 40.2 348 320 42.2 - - -
WD! (reported) - 80.8 82.3 - 34.4 39.0 - 51.2 46.1 - - -
WD! (reproduced) 74.6 81.5 83.0 31.0 374 39.0 488 52.3 50.8 33.1 32.1 22:5
UniGRPO 749 825 82.7 324 374 394 445 43.0 57.0 59.0 67.0 62.9

SPG w/ EUBO (ours) 77.1 83.8 83.9 33.2 376 394 684 715 68.0 812 87.1 89.9
SPG w/ mixture (ours) 78.5 86.1 84.5 33.4 40.0 418 688 70.7 70.3 829 94.0 93.1

Dynamics of Completion Length. We provide the dynamics of the effective sequence length of SPG during RL
training in Figure 9. We also report the effective length of the best checkpoint in Table 5. SPG leads to effective usage of
the total given length and good adaptation to task difficulties.

GSM8K MATH500 Countdown Sudoku

PPL bath 250 256

200
255
150] |
254
100

253

50

220

0 252
0 2000 4000 6000 0 1000 2000 3000 4000 0 2000 4000 6000 0 500 1000 1500 2000 2500
Steps Steps Steps Steps
=_——+ D1 —— WD1 — UniGRPO SPG (ours)

Figure 9 Dynamics of the effective generation length of SPG during RL training, compared with D1, WD1, and UniGRPO. SPG
leads to concise solutions with better token efficiency. We report mean and standard deviation over a rolling window of 50 steps.

24


Table 5 Effective sequence length of each model at the best checkpoint corresponding to Table 1 on four reasoning benchmarks.

GSMB8K (0-shot) MATH500(0-shot) Countdown (O-shot) Sudoku (3-shot)
Model / Seq Len 128 256 512 128 256 512 128 256 512 128 256 512
LLaDA-8B-Instruct 114 212 257 123 235 402 111) 213 407 111 232 448

LLaDA-1.5 115) 214 «265 «123,237 407 114 215 411 112 232 419
Di 115 209) 261) =«:123) 234 399 107. 211 397 111 231 449
WDI1 115) 2255 312) 123s 231 378 83 84 90 105 227 473
UniGRPO 114 211 257) «123 235 = 400 100 207 374 113. 2300 472
SPG w/ EUBO 110 196 227 120 228 382 68 70 78 89 =137 249

SPG w/ mixture 108 176 195 121 229 384 75 78 79 115) 239491

E.2 Additional Ablation Results

In this section, we provide the complete results for each generation length and task in supplement to Section 4.2. We also
include additional ablation studies on the looser upper bound, different log-likelihood estimation methods for positive
advantage traces, and Pass@K performance.

Ablations on Algorithm Components. We provide the complete results for ablations on log-likelihood estimation
methods in Table 6 and for ablations on masking strategies in Table 7.

Table 6 Ablations on log-likelihood estimation methods for negative advantage traces. The best results are bolded and the second best
are underlined. SPG w/ Mixture consistently outperforms other likelihood estimation methods.

GSMB8K (0-shot) MATHSOO (0-shot) Countdown (0-shot) Sudoku (3-shot)
Model 128 256 512 Avg. 128 256 512 Avg. 128 256 512 Avg. 128 256 512 = Avg.

SPG wo/ neg 72.0 79.0 81.3 77.4 28.2 32.2 37.8 32.7 43.8 48.1 445 45.5 55.0 82.9 684 68.8
SPG w/ ELBO 75.6 82.8 844 80.9 35.8 37.6 38.8 37.4 66.8 66.0 684 67.1 73.8 894 841 82.4
SPGw/EUBO 77.1 83.8 83.9 816 33.2 37.6 39.4 36.7 684 715 68.0 69.3 81.2 87.1 89.9 86.1
SPG w/ Mixture 78.5 86.1 845 83.0 33.4 40.0 41.8 38.4 68.8 70.7 70.3 69.9 82.9 940 93.1 90.0

Table 7 Ablations on the masking strategies in Monte Carlo estimation. Our block-wise masking strategy leads to consistent
improvement to random masking on both benchmarks.

MATHSOO (0-shot) Countdown (0-shot)
Model Masking 128 256 512 Avg. 128 256 512 Avg.

SPG w/EUBO — random 33.4 35.4 414 36.7 42.6 41.0 52.7 45.4
block-wise 33.2 37.6 39.4 36.7 684 71.5 68.0 69.3

SPG w/ Mixture random 33.8 38.2 38.8 36.9 52.3 645 71.5 62.8
block-wise 33.4 40.0 41.8 384 68.8 70.7 70.3 69.9

Ablations on Key Hyperparameters 3 andw. We provide the complete results for ablations on ( in Table 8 and for
ablations on w in Table 9.

Ablations on Inference Strategies. | We provide complete results for ablations on different inference strategies in
Table 13. Note that the reported numbers of each method for “Semi-AR, Block=32, Confidence” is in general slightly
higher than the results in Table | under the same inference setting. This is because in Table 13, we select best checkpoint
specifically for generation length 256 to maintain consistency with other inference settings, while in Table 1, we choose
the checkpoint with the highest average accuracy across generation lengths 128, 256, and 512.

Ablations on the Looser Upper Bound. As mentioned in Section 3.2 and Section B, a looser but unbiased bound
can be derived using inequalities like log(x) < a — 1, ie., — (Equation (24)). However, as shown in Table 10,
this looser bound performs worse empirically than the tighter upper bound Leupo we used, possibly due to a larger
discrepancy from the true log-likelihood.

Ablations on Log-Likelihood Estimations for Positive Advantage Traces. Instead of always using LELBO for positive
advantage traces, we experiment on MATHS00 and Countdown benchmarks using both Lgygo and Lyix for positive

25


Table 8 Ablations on the value of { in the upper bound.

GSMB8K (0-shot) MATHSOO (0-shot) Countdown (0-shot) Sudoku (3-shot)
Model B 128 256 512 Avg. 128 256 512 Avg. 128 256 512 Avg. 128 256 512 Avg.

0.50 77.7 83.2 845 81.8 32.8 364 41.2 368 71.1 68.8 746 715 647 53.4 57.4 58.5
0.75 77.2 83.9 845 81.9 31.0 366 40.0 35.9 70.7 70.7 70.7 70.7 63.4 65.7 45.4 58.2

SPG w/ EUBO 1.00 76.5 83.9 83.6 81.3 31.0 37.4 388 35.7 66.0 668 664 664 81.2 87.1 89.9 86.1
150 77.1 83.8 83.9 816 33.2 37.6 394 36.7 69.5 645 664 66.8 32.7 40.5 39.9 37.7
2.00 76.5 83.9 83.2 81.2 32.4 368 38.2 35.8 684 71.5 68.0 69.3 28.1 31.9 28.0 29.3

1.00 78.8 85.6 849 83.1 340 40.2 39.2 37.8 69.9 69.5 70.3 69.9 82.9 94.0 93.1 90.0
SPG w/ Mixture 1.50 78.5 86.1 845 83.1 33.4 40.0 41.8 384 68.8 70.7 70.3 69.9 83.2 86.0 84.6 84.6
2.00 78.8 85.7 84.7 83.1 32.4 388 39.8 37.0 70.3 69.1 69.5 69.6 44.3 60.5 60.7 55.2

Table 9 Ablations on the mixture coefficient w on MATH500 and Countdown.

SPG w/ Mixture MATHSOO (0-shot) Countdown (0-shot)

Ww 128 256 512 Avg. 128 256 512 Avg.
0.00 35.8 37.6 38.8 374 66.8 66.0 68.4 67.1
0.25 34.6 37.6 42.22 38.1 71.5 68.0 67.2 68.9
0.50 33.4 40.0 41.8 384 68.8 70.7 70.3 69.9
0.75 34.2 38.6 41.2 38.0 69.5 69.1 74.2 70.9

1.00 33.2 37.6 39.4 36.7 69.5 64.5 664 66.8

Table 10 Ablations on the looser upper bound. The loose bound performs worse than the tighter upper bound we used, indicating
inferior performance due to a larger discrepancy from the true log-likelihood.

SPG w/ EUBO MATHSOO (0-shot) Countdown (0-shot)

p UpperBound 128 256 512 Avg. 128 256 512 Avg.

1.0 Fico 294 354 394 34.7 43.8 65.2 648 57.9
Leuso 31.0 374 38.8 35.7 66.0 66.8 66.4 66.4

1.5 Loose 29.8 31.8 38.8 33.5 469 54.7 57.0 52.9
Leno 33.2 37.6 394 36.7 69.5 64.5 664 66.8

Table 11 Ablations on log-likelihood estimation for positive advantage traces. Using the upper bound for log-likelihood estimation of
positive advantage traces perform worse than using the lower bound.

Positive traces MATHSOO (0-shot) Countdown (0-shot)
Model likelihood estimation 128 256 512 Avg. 128 256 512° Avg.
SPG w/ EUBO Levuso (8 = 1.0) 344 362 39.2 366 48.1 46.7 50.8 48.5

LELBO 33.2 376 394 36.7 684 71.5 68.0 69.3
SPG w/ Mixture Lx (6 =1.0,w=0.5) 354 384 39.0 37.6 69.1 684 70.3 69.3

LELBO 33.4 40.0 41.8 38.4 68.8 70.7 70.3 69.9

advantage traces. Correspondingly, we use w = 0.5 and the best performed ( as previously discussed for negative
advantage traces. For the positive advantage traces, we always use the tightest 6 = 1.0 for both — and Erte. The
results are shown in Table 11, indicating that using the upper bound for likelihood estimation of positive advantage
traces performs worse than using Lg igo. This aligns well with our theoretical insights that the lower bound is a better
objective for log-likelihood maximization.

Ablations on Pass@K Performance. In all previous experiments, we apply greedy sampling by setting temperature as
0.0 following D1 and LLaDA. However, beyond accuracy, it is essential for models to generate a diverse set of outputs that
can cover the correct solution and allow for explorations. In this section, we investigate the models’ ability to generate
diverse outputs using a higher temperature, and evaluate their Pass@K performance on MATHS00 and Countdown,
as shown in Table 12. Specifically, we set temperature to 0.9 and generation length to 256, conduct evaluations every

26


100 steps, and report results from the checkpoint with the highest accuracy. For comparison, we also include results
from greedy sampling, denoted as Pass@ IGreeay. As expected, increasing the temperature leads to a decrease in Pass@ |
performance across all models, aligning with observations from previous work. For K>1, the Pass@K scores improve
for all models as K increases from | to 4. SPG achieves the best performance across all settings, with SPG w/ Mixture
reaching 55.6% Pass @4 accuracy on MATHS00 and 76.6% on Countdown, demonstrating the ability of SPG to generate
diverse outputs that can recover the correct solution.

Table 12 Pass@K performance of each model on MATHSO0 and Countdown. We set temperature as 0.9 and report results of the
best checkpoint of each case at a generation length of 256. For comparison, we also include the greedy sampling performance, i.e.,
Pass@ |Greedy. The best results are bolded and the second best are underlined.

MATHSOO (0-shot) Countdown (0-shot)
Model Pass@1Greedy Pass@1 Pass@2 Pass@3 Pass@4 Pass@1creedy Pass@1 Pass@2 Pass@3 Pass@4
LLaDA-8B-Instruct 32.4 315 40.9 45.7 48.8 16.8 15.8 28.1 37d 45.3
LLaDA-1.5 32.2 32.6 42.2 47.4 50.4 21.1 18.2 321 42.5 50.0
D1 37.8 34.3 43.1 48.0 52.0 32.4 24.5 40.4 51.4 60.6
WD1 38.6 36.0 44.9 49.9 53.6 54.7 44.3 60.6 68.0 73.1
UniGRPO 38.4 34.7 43.9 49.5 53.2 44.9 36.8 55.2 65.0 23
SPG w/ EUBO 38.0 34.4 44.3 49.9 54.0 71.5 68.2 71.9 BY 76.6
SPG w/ mixture 40.0 36.5 46.0 51.2 55.6 ThA 67.5 72.5 75.1 76.6

27


Table 13 Ablations on the inference strategy. SPG leads to consistently superior performance to baselines with different inference
strategies. The best results are bolded and the second best are underlined for each setting. We report results for generation length 256.

Inference Strategy Model GSM8K MATH500 Countdown Sudoku Avg.
LLaDA-8B-Instruct 78.7 31.4 13.7 26.2 37.5
LLaDA-1.5 78.8 33.4 16.0 23.0 37.8
D1 79.7 37.2 27.0 31.4 43.8
Semi-AR, Block=16, Confidence WD1 82.3 37.4 53.9 36.8 52.6
UniGRPO 82.5 36.8 46.5 63.4 57.3
SPG w/ EUBO 84.7 37.4 70.3 82.2 68.7
SPG w/ Mixture 86.4 40.8 70.7 96.2 73.5
LLaDA-8B-Instruct 77.2 32.4 16.8 27.7 38.5
LLaDA-1.5 80.5 32.2 21.1 26.9 40.2
D1 80.6 37.8 32.4 32.8 45.9
Semi-AR, Block=32, Confidence WD1 81.7 38.6 54.7 35.7 58.1
UniGRPO 82.6 38.4 44.9 67.0 58.2
SPG w/ EUBO 84.8 38.0 71.5 88.5 70.7
SPG w/ Mixture 86.2 40.0 TLA 95.6 73.2
LLaDA-8B-Instruct 78.6 33.2 27.3 32.6 42.9
LLaDA-1.5 81.0 35.4 20.3 36.4 43.3
D1 80.9 37.6 38.3 39.8 49.2
Semi-AR, Block=64, Confidence WD1 82.5 37.4 52.3 41.8 53.5
UniGRPO 82.3 37.4 53.5 82.9 64.0
SPG w/ EUBO 84.3 37.4 69.5 88.8 70.0
SPG w/ Mixture 85.5 41.4 69.9 93.8 72.7
LLaDA-8B-Instruct 63.5 21.0 6.3 24.4 28.8
LLaDA-1.5 67.1 24.8 10.9 27.5 32.6
D1 69.7 27.4 18.4 29.9 36.4
Semi-AR, Block=32, Random WDI1 74.1 30.8 37.5 29.9 43.1
UniGRPO 72.8 29.8 41.4 60.1 51.0
SPG w/ EUBO 74.1 31.4 42.6 72.6 55.2
SPG w/ Mixture 78.4 31.0 66.0 86.9 65.6
LLaDA-8B-Instruct 23.9 17.8 0.0 68.3 27.5
LLaDA-1.5 41.4 20.4 0.0 67.9 32.4
D1 57.5 22.6 0.0 72.3 38.1
Full Sequence, Confidence WD1 56.7 25.0 10.2 68.9 40.2
UniGRPO 50.0 24.2 8.2 95.6 44.5
SPG w/ EUBO 54.3 23.4 63.3 88.9 57.5
SPG w/ Mixture 58.8 24.0 65.6 96.5 61.2
LLaDA-8B-Instruct 52.7 17.8 13.3 27.1 27.7
LLaDA-1.5 53.5 19.8 13.7 28.0 28.8
D1 53.0 22.6 28.5 31.8 34.0
Full Sequence, Random WD1 54.6 23.4 43.4 30.0 37.9
UniGRPO 57.6 22.6 47.7 57.3 46.3
SPG w/ EUBO 58.5 25.2 56.6 71.3 52.9
SPG w/ Mixture 61.2 25.2 64.1 72.2 55.7

28


E.3 Qualitative Examples on GSM8K

We present qualitative examples for all models on GSM8K, using a generation length of 256 and 128 diffusion steps.
Incorrect reasoning steps or answers are highlighted in red and the correct answers are highlighted in green. SPG leads
to more concise and accurate reasoning compared to other models. Note that the models may sometimes achieve the
correct answer through flawed reasoning, while such artifacts are expected to diminish with a stronger base model.

Model: LLaDA-8B-Instruct

Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass
costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?
Model: LLaDA-8B-Instruct
Answer:
To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost pattern of the glass. The first
glass costs $5, and every second glass costs 60% of the price. Therefore, the cost of every second glass is 0.6 x 5 = 3
dollars.
We can break down the cost as follows:

¢ The first glass costs $5.

* The second glass costs $3.

* The third glass costs $5.

¢ The fourth glass costs $3.

¢ And so on...
We can see that the cost alternates between $5 and $3. Since Kylar wants to buy 16 glasses, we need pair them up.
The first of each pair costs $5, and the second of each pair costs $3. Therefore, the cost of each pair is 5+ 3 = 8 dollars.
Since there are 8 pairs of glasses (16 glasses divided into pairs), the total cost is 8 x 8 = 64 dollars.
</reasoning>
<answer>
64
</answer>

Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow
colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many
eyeshadow colors does she have left?

Model: LLaDA-8B-Instruct

Answer:

First, let’s determine the total number of eyeshadow colors Amy initially has. She has two eyeshadow palettes with
four colors each, 2 x 4 = 8. She also has three makeup sets with six colors each, 3 x 6 = 18. So, the total number
of eyeshadow colors Amy initially has is: 8 + 18 = 26. Next, her sister steals one of the palettes, which she loses
completely. Therefore, the number of colors left from the remaining palettes is: 8 — 4 = 4. Amy uses up half of the
colors from one makeup set. Each makeup set has six colors, so half of that is: 8 = 3. Thus, the number of colors left
from the one makeup set is: 6 — 3 = 3. Adding, the total number of eyes left is: 4+ 3 = 11.

</reasoning>

<answer>

11

</answer>

Model: D1

Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass
costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?
Model: D1
Answer:
To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost pattern of the glass. The first
glass costs $5, and every second glass costs 60% of the price. Therefore, the cost of every second glass is 5 x 0.6 = 3
dollars. We can break down the cost into two parts:

1. The cost of the first 8 glasses, each costing $5.

29


2. The cost of the next 8 glasses, each costing $3.
First, let’s calculate the cost of the first 8 glasses: 8 x 5 = 40 dollars.
Next, let’s calculate the cost of the next 8 glasses: 8 x 3 = 24 dollars.
Now, we add these two amounts together: 40 + 24 = 64 dollars.
Therefore, Kylar needs to pay $64 for them.
</reasoning>
<answer>
64
</answer>

Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow
colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many
eyeshadow colors does she have left?

Model: D1

Answer:

Amy initially has two eyeshadow palettes with four colors each, so she has 2 x 4 = 8 colors from the palettes. She also
has three makeup sets with six colors each, so she has 3 x 6 = 18 colors from the makeup sets. Her’s sister steals one
of the palettes, so she now has 8 — 1 = 7 colors from the palettes. Amy uses up half of the colors from one makeup set,
so she uses 8 = 3 colors from that set. Therefore, she has 18 — 3 = 15 colors left from the makeup sets. Adding these
to the remaining palettes palettes, she has a total of 7 + 15 = 22 eyeshadow colors left.

</reasoning>

<answer>

22
</answer>

Model: WD1

Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass
costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?

Model: WD1

Answer:

To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost of both the full price and the
discounted price glasses.

First, let’s calculate the cost of the first 8 glasses, which are sold at the full price of $5 each: 8 x 5 = 40 dollars.
Next, let’s calculate the cost of the next 8 glasses, which are sold at 60% of the full price.

Since 60% of $5 is: 0.6 x 5 = 3 dollars. So, the cost for the next 8 glasses is: 8 x 3 = 24 dollars.

Now, we add the cost of the first 8 glasses and the cost of the next 8 glasses: 40 + 24 = 64 dollars.

Therefore, Kylar needs to pay | 64 |.
</reasoning>

<answer>

64
</answer>

Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow
colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many
eyeshadow colors does she have left?

Model: WD1

Answer:

First, let’s determine the total number of eyeshadow colors Amy initially has. She has two eyeshadow palettes with four
colors each: 2 x 4 = 8. Additionally, she has three makeup sets with six colors each: 3 x 6 = 18. So, together, she
has: 8 + 18 = 26. Next, her sister steals one of the palettes, which she loses 4 colors: 26 — 4 = 22. Then, Amy uses
up half of the colors from one makeup set. Each makeup set has six colors, so half of that is: 8 = 3. Thus, she uses 3
colors from one makeup set, leaving: 6 — 3 = 3 colors in that makeup set.

Finally, we add the remaining colors from the makeup set to the remaining colors: 22 + 3 = 25.

</reasoning>

<answer>

30


25
</answer>

Model: UniGRPO

Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass
costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?

Model: UniGRPO

Answer:

To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost of both the regular glasses and
the discounted glasses. A regular glass costs $5, and every second glass costs 60% of that price, which is 5 x 0.6 = 3.
Since Kylar will buy 16 glasses, he will have 8 regular glasses and 8 discounted glasses. The cost of 8 regular glasses is
8 x 5 = 40 dollars. The cost of 8 discounted glasses is 8 x 3 = 24 dollars.

Therefore, the total cost for 16 glasses is 40 + 24 = 64 dollars.

</reasoning>

<answer>

64
</answer>

Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow
colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many
eyeshadow colors does she have left?

Model: UniGRPO

Answer:

First, let’s determine the total number of eyeshadow colors Amy initially has. She has two eyeshadow palettes with four
colors each, so she has 2 x 4 = 8 colors from the palettes. She also has three makeup sets with six colors each, so she
has 3 x 6 = 18 colors from the makeup sets. Adding these together gives her initially 8 + 18 = 26 eyeshadow colors.
Next, her sister steals one of the palettes, so she now has 8 — 4 = 14 colors from the palettes. She then uses up half of
the colors from one makeup set. Since she used one makeup set, containing six colors, she used 8 = 3 colors from that
set. Therefore, she has 18 — 3 = 15 colors left from the makeup sets.

Finally, adding the remaining colors from the palettes and the makeup sets, Amy has 14 + 15 = 29 eyeshadow colors
left.

</reasoning>

<answer>

29
</answer>

Model: SPG w/ EUBO

Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass
costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?

Model: SPG w/ EUBO

Answer:

To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost structure of the glasses. The
first glass costs $5, and every second glass costs 60% of the price of the first glass. Therefore, the cost of each second
glass is 5.0 x 0.6 = 3.0 dollars.

Kylar wants to buy 16 glasses, which means he will have 8 pairs of glasses. Each pair consists of one first glass
and one second glass. The cost of one pair is: 5.0 + 3.0 = 8.0 dollars. Since there are 8 pairs, the total cost is:
8 x 8.0 = 64.0 dollars. Thus, Kylar needs to pay | 64 | dollars for 16 glasses.

</reasoning>

<answer>

64
</answer>

31


Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow
colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many
eyeshadow colors does she have left?
Model: SPG w/ EUBO
Answer:
First, let’s calculate the total number of eyeshadow colors Amy initially has:
¢ She has two eyeshadow palettes with four colors each: 2 x 4 = 8 colors.
¢ She has three makeup sets with six colors each: 3 x 6 = 18 colors.
¢ Adding these together gives: 8+ 18 = 2
Next, we account for the changes:
¢ Her sister steals one palette, so she loses 4 colors: 26 — 4 = 22
e Amy uses up half of the colors from one makeup set, which is 8 = 3 colors: 22 —3 = 19
Therefore, Amy has 19 eyeshadow colors left.
</reasoning>
<answer>
Ike]
</answer>

Model: SPG w/ Mixture

Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass
costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?
Model: SPG w/ Mixture
Answer:
1. The first glass costs $5.
2. Every second glass costs 60% of $5, which is 0.6 x 5 = 8 dollars.
3. Kylar wants to buy 16 glasses, so he will have 8 full-priced glasses and 8 discounted glasses.
4. The cost for 8 full-priced glasses is 8 x 5 = 40 dollars.
5. The cost for 8 discounted glasses is 8 x 3 = 24 dollars.
6. Adding these the total cost is 40 + 24 = 64 dollars.
</reasoning>
<answer>
64
</answer>

Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow
colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many
eyeshadow colors does she have left?
Model: SPG w/ Mixture
Answer:

1. Amy initially has two eyeshadow palettes with four colors each: 2 x 4 = 8 colors

2. She also has three makeup sets with six colors each: 3 x 6 = 18 colors

3. Adding these together gives gives: 8 + 18 = 26 colors

4. Her sister steals one palette, so she loses: 4 colors

5. Amy uses up half of the colors from one makeup set: 8 = 3 colors

6. Subtracting these lost and used colors: 26 — 4 — 3 = 19 colors
</reasoning>
<answer>
19
</answer>

32
