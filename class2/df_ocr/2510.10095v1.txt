2510.10095v1 [cs.IR] 11 Oct 2025

arXiv

CardRewriter: Leveraging Knowledge Cards for Long-Tail Query
Rewriting on Short-Video Platforms

Peiyuan Gong" Feiran Zhu‘ Chenglei Dai
GSAI, Renmin University of China Yaqi Yin' Chao Zhang
Beijing, China Kuaishou Technology Kuaishou Technology
pygongnlp@gmail.com Hangzhou, Beijing, China Hangzhou, Beijing, China
{zhufeiran03,yinyaqi}@kuaishou.com {zhangchao,daichenglei}@kuaishou.com
Kai Zheng Jiaxin Mao* Yi Zhang*
Wentian Bao GSAI, Renmin University of China Kuaishou Technology
unaffiliated Beijing, China Beijing, China
Beijing, China maojiaxin@gmail.com zhangyi49 @kuaishou.com
zhengk92@gmail.com
wb2328@columbia.edu
Abstract deployed on Kuaishou, one of China’s largest short-video platforms,

Short-video platforms have rapidly become a new generation of
information retrieval systems, where users formulate queries to
access desired videos. However, user queries, especially long-tail
ones, often suffer from spelling errors, incomplete phrasing, and am-
biguous intent, resulting in mismatches between user expectations
and retrieved results. While large language models (LLMs) have
shown success in long-tail query rewriting within e-commerce,
they struggle on short-video platforms, where proprietary content
such as short videos, live streams, micro dramas, and user social
networks falls outside their training distribution. To address this
challenge, we introduce CardRewriter, an LLM-based framework
that incorporates domain-specific knowledge to enhance long-tail
query rewriting. For each query, our method aggregates multi-
source knowledge relevant to the query and summarizes it into an
informative and query-relevant knowledge card. This card then
guides the LLM to better capture user intent and produce more ef-
fective query rewrites. We optimize CardRewriter using a two-stage
training pipeline: supervised fine-tuning followed by group relative
policy optimization, with a tailored reward system balancing query
relevance and retrieval effectiveness. Offline experiments show that
CardRewriter substantially improves rewriting quality for queries
targeting proprietary content. Online A/B testing further confirms
significant gains in long-view rate (LVR) and click-through rate
(CTR), along with a notable reduction in initiative query reformu-
lation rate (IORR). Since September 2025, CardRewriter has been

¥ Equal Contribution. Work done when Peiyuan Gong was an intern at Kuaishou.
* Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

Conference acronym ‘XX, Woodstock, NY

© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/2018/06

https://doi.org/XXXXXXX.XXXXXXX

serving hundreds of millions of users daily.

CCS Concepts

- Information systems — Query reformulation.

Keywords

Query Rewriting; Large Language Models; Retrieval-augmented
Generation

ACM Reference Format:

Peiyuan Gong", Feiran Zhu’, Yaqi Yin‘, Chenglei Dai, Chao Zhang, Kai
Zheng, Wentian Bao, Jiaxin Mao’, and Yi Zhang”. 2018. CardRewriter: Lever-
aging Knowledge Cards for Long-Tail Query Rewriting on Short-Video
Platforms. In Proceedings of Make sure to enter the correct conference title
from your rights confirmation email (Conference acronym ’XX). ACM, New
York, NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX

1 Introduction

In recent years, short-video platforms (e.g., Kuaishou, TikTok, Xi-
aohongshu) have become ubiquitous in daily life, enabling users to
explore and interact with diverse content. Increasingly, these plat-
forms also serve as search engines, allowing users to issue queries
and receive ranked short-video results tailored to their informa-
tion needs [23, 26]. However, the diversity in users’ expressions
and intents often gives rise to challenges such as spelling errors,
missing terms, and semantic ambiguity [6, 20], especially for long-
tail queries. Such queries frequently yield unsatisfactory search
results, forcing users to repeatedly reformulate their requests or
even abandon the search. Solving this problem is therefore critical
to improving user satisfaction.

To address this challenge, query rewriting has become an intu-
itive and widely used solution [16], capable of correcting linguistic
errors [13, 31], supplementing missing keywords [17, 19, 28], and
matching query style to available content [4, 6, 20]. Existing meth-
ods fall into two main paradigms: (I) Embedding-based methods
retrieve semantically similar queries to enrich the original, thereby
increasing relevant content [15, 27, 29]. Despite this, they struggle


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

"

LLM Rewrite

APA AEA OE)

( #J 8 R (Coca-Cola), Being
fostered by relatives, Creative ad )

Original Query
APA ARABS PR
(FF 8] 46, The pain of being
fostered by relatives )

Trending

TOAD jojo SiR

TOURER

Multi-Source Knowledge

Gong et al.

CardRewriter

(:

Card-Generation Model

Knowledge Card

AY AY HY A RAF FRWITE LAM
( FJ ay Ay 4k, The fostered children

series )

” es =
UE |) (ae ae eae
BVH ||| | aria MF

S yyy

>) childhood,

fostered living in another

ne person's home
series

F
if:

BN RUA,

(Tl
"ay

°

not a reference to the beverage brand.
The core intent is to find this creator's

H Query FF TELR MS FL,

ae

f an on-platform creator "#J FJ Ay 5K",

“ay

RSA A ee SR RS, Tr ia
ERR OCR. Bob RA RK 7 RURE...
EEN EE PY IE RY BR

is query is ambiguous an
FH A"(Coca-Cola) is a misspelling

colloquial.

vid

fostering experience. )

leo series about a poignant childhood

Figure 1: An example of query rewriting on the short-video platform. (a) Original Query: Fails to retrieve relevant results; (b)
LLM Rewrite: Misinterprets semantics and yields an ineffective rewrite; (c) CardRewriter: Leverages the generated knowledge

card as search context to produce an accurate rewrite.

with long-tail queries due to scarce semantic matches. (II) Genera-
tive methods directly rewrite long-tail queries for greater semantic
clarity and completeness [21, 33]. Recently, large language model
(LLM)-based rewriting has emerged as the dominant approach by
using reinforcement learning to align user queries with platform-
specific expression styles [1, 4, 6, 20, 32].

While LLM-based query rewriting has shown strong perfor-
mance in refining long-tail queries within e-commerce scenarios
[6, 20], extending these methods to short-video platforms remains
challenging. Such platforms contain vast amounts of proprietary
content, such as short videos, live streams, micro dramas, and user
social networks, that LLMs have not been exposed to during pre-
training, often leading to rewriting failures. As illustrated in Fig-
ure 1, when a user attempts to search for the video series “The
Fostered Children” by the creator “Coco-Cola” but mistakenly types
“Coca-Cola”, the LLM erroneously associates the query with the
beverage brand and appends “creative ad” to the original query.
Consequently, the returned videos are entirely irrelevant to the
user’s original search intent.

To address the above challenge, we propose CardRewriter, a
retrieval-augmented framework that leverages platform-specific
knowledge to enhance the quality of LLM-based query rewriting.
Given an input query, our approach first retrieves multi-source
knowledge from short-video platforms, incorporating multi-modal
content from top-k videos retrieved through the original query and
each query in the similar high-supply query set, respectively, along
with relevant open-domain documents. This collected knowledge
is then summarized into a concise knowledge card aligned with the
input query, which is then integrated into the rewriting process to

guide the LLM to better understand the search context and produce
more effective query rewrites. As shown in Figure 1, the knowl-
edge card accurately corrects the user’s misspelling, thus retrieving
more videos from the creator "Coco-Cola"’s series "The Fostered
Children". To ensure high-quality knowledge cards and rewrites,
we adopt a two-phase training paradigm combining supervised
fine-tuning (SFT) and group relative policy optimization (GRPO),
guided by a custom reward system balancing both query relevance
and retrieval effectiveness.

To evaluate the effectiveness of CardRewriter in rewriting long-
tail queries on short-video platforms, we conduct both offline and
online evaluations. Offline results show that our approach improves
the semantic relevance between rewritten and original queries and
retrieves more videos matching users’ information needs. In con-
trast, directly integrating multi-source knowledge introduces sub-
stantial noise and often distorts intent. Our SFT+GRPO training
strategy with a tailored reward system further boosts the quality
of knowledge cards and rewritten queries. Online A/B tests con-
firm significant gains in long-view rate (LVR) and click-through
rate (CTR), alongside reduced initiative query reformulation rate
(IQRR) across both covered and full traffic. Since its deployment on
Kuaishou in September 2025, CardRewriter has greatly enhanced
the search experience for hundreds of millions of users.

Our main contributions are as follows:

e We propose CardRewriter, a framework that injects short-
video platform knowledge into long-tail query rewriting,
summarizing such knowledge into concise knowledge cards
that guide LLM rewrites.


CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

e We design a two-stage training framework that integrates
SFT and GRPO, optimized by a custom reward system bal-
ancing relevance and effectiveness.

e Both offline and online experiments confirm that our method
improves knowledge utilization and rewriting effectiveness,
showing practical gains in real-world deployments.

2 Related Work
2.1 Query Rewriting

As an upstream task in information retrieval, query rewriting aims
to refine the original query to better capture the user’s search in-
tent [16, 24]. Its objective is to improve the accuracy of search
results and thereby enhance the overall search experience. Exist-
ing approaches can be broadly divided into embedding-based and
generative methods.

Embedding-based methods treat query rewriting as a retrieval
task by identifying similar queries from a pre-built set to augment
the original query and improve recall. For instance, Li et al. [15] em-
ploys acontrastive learning architecture in a “retrieval-ranking-rer
anking” pipeline to generate personalized rewrites. Xiao et al. [29]
jointly trains query rewriting and semantic matching on weakly
labeled data, enhancing both tasks through iterative co-training.
Gamzu et al. [7] utilizes a search index to generate alternative
queries, especially for voice search, and applies learning-to-rank
to select the best rewrite. Despite effectively boosting recall for
common queries, these methods struggle with long-tail queries due
to the scarcity of similar candidates.

Generative methods formulate query rewriting as a text gener-
ation task, directly producing revised queries without retrieval. Qiu
et al. [21] proposes a cycle-consistent training approach that jointly
optimizes query-to-title and title-to-query models, while Zuo et al.
[33] constructs a session graph to capture historical interactions
and integrate them via graph attention. Recently, LLM-based meth-
ods have excelled at rewriting long-tail queries in e-commerce. For
example, Peng et al. [20] designs a three-stage framework to align
long-tail queries with product descriptions; Dai et al. [4] adapts
LLMs to domain-specific patterns through pre-training, and Zuo
et al. [32] introduces a value-aware LLM that employs a weighted
trie to generate high-value bidwords, improving both relevance
and revenue. Nevertheless, on short-video platforms, LLMs still
face challenges in handling long-tail queries targeting proprietary
content, often failing to fully capture user intent.

2.2 Retrieval-augmented Generation

Retrieval-augmented Generation (RAG) enhances response reliabil-
ity by retrieving relevant information and leveraging it to generate
factual, high-quality answers.[9, 14, 18]. Guu et al. [11] augments
language models with a latent retriever over large corpora, while
Yan et al. [30] evaluates retrieval quality and reduces noise via a de-
compose-recompose algorithm. Gong et al. [10] leverages LLMs to
comprehend multi-party dialogues for web-augmented responses,
and Jiang et al. [12] proactively retrieves content by predicting fu-
ture sentences. Beyond individual components, Gao et al. [8] jointly
tunes retrieval, rewriting, and generation with reinforcement learn-
ing, and Chen et al. [2] formulates the pipeline as a cooperative

Conference acronym XX, June 03-05, 2018, Woodstock, NY

multi-agent task with shared rewards. In this work, we collect multi-
source knowledge for long-tail queries from short-video platforms
and integrate it into query-relevant knowledge cards. These cards
serve as enriched contexts to achieve better query rewriting.

3 CardRewriter

In this section, we detail how CardRewriter integrates knowledge
from short-video platforms into query rewriting (Section 3.1), present
the training framework (Section 3.2), and introduce the real-world
deployment strategy (Section 3.3).

3.1 Rewriting Workflow

To leverage short-video platform knowledge for query rewriting,
as shown in Figure 2, CardRewriter operates in two stages: (I)
Knowledge Collection: aggregating the top-k videos retrieved for
each long-tail query and its corresponding similar high-supply
queries, and further enriching them with relevant open-domain
documents; (II) Card-Based Rewriting: summarizing the collected
knowledge into knowledge cards that help the LLM to generate
effective rewrites. Detailed prompts used for CardRewriter are pro-
vided in Appendix A.

3.1.1 Knowledge Collection. LLMs often struggle to understand
queries on short-video platforms when the search intent points to
proprietary content, resulting in ineffective query rewriting. To
address this, we collect platform-specific knowledge to enhance
LLMs’ understanding of such queries. For the given query x, we feed
it into the Kuaishou search system to retrieve the top-k relevant
videos V = {04,02,...,0,}, and extract multi-modal knowledge
Min = {m1, m2,..., mx} from them. Specifically, for each video 9;,
we extract content potentially useful for understanding x, including:
(I) Visual content py sion: three key frames of the video; (II) Textual
content orext the title, caption, text on the cover (OCR), author
name, and background music (BGM) of the video. Formally, this
can be represented as:

isi key, _ key, _ key:
vision _ 1 2 3
0; ~~ {o; > 0; > U; (1)
text _ y,title caption ocr author _,bgm
0 = {0; »U; »U; 0; »U; } (2)
_ vision _ text 3
mj = {0; »U; ( )

Where m; denotes the multi-modal knowledge extracted from video

v;, consisting of visual content gyision ia
Moreover, to address the low relevance of top-k retrievals for

certain long-tail queries, we first construct two high-quality query

and textual content v

sets o, 4 and Q 4 from Kuaishou search logs. We then retrieve

the top-/ queries similar to the original query x using two methods
(1)

good
that share lexical overlap with x and have intersecting retrieved
video lists, then rank them by the number of identical videos. (II)

Embedding-based matching (EMB): compute embedding similarity

separately: (I) Rule-based matching (Q2Q): select queries from Q

between x and each query in Q@ q then rank them by similarity.
The set of similar queries can be formalized as:

Qsim = Set(Q2Q(x, QL") UEMB(x, O{°) ,)) (4)

Here, Qsim denotes the deduplicated query set similar to x.


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

| Knowledge Collection

Gong et al.

a
I Card-based |

Rewriting

bl: AEA Bee

Input Query Rule-based
AY WY Se AeA
_—

(HJ OAR, The pain of being
fostered by relatives)

>
Emb-based
XN

i. f ewe by OCR: NTA, Bearer MRaRHO Fe (Cr
| Offline Search System = Title: The pain of being fostered by relatives, a sketch FAP Query HER MSTA, “AH
OCR: iJ J HS childhood, myself being fostered at a relative’s home. TR Aa AERA A Hy RMR,
FBO TE. Bb>RAERRE
FERRARA EAE 2f FER BRIER
© Simitar Query Enhancement + Hemi RT.

[Similar Query] 757-74 HGR BHM
bri: HOA THR AES IR...

[Rule-based Query] Linlin, a girl fostered at her aunt’s house.
Title: Linlin finally is strong enough to handle her aunt’s family...
OCR: The Fostered Child 13: The Divorce Event

(This query is ambiguous and colloquial.
“HJ [1 HY 4”(Coca-Cola) is a misspelling
of an on-platform creator “ii] HJ HJ FR",
not a reference to the beverage brand.
The core intent is to find this creator's

video series about a poignant childhood

fostering experience.)

_———— (c) Open-Domain Document

|

them.

“MEP” ROR BREF, AREAS OUR PEI. 48
AGHA VEN, AREALA.

The literal meaning of idiom “#¥ \# F” is “to live under someone’s fence as a
sparrow." It has come to mean living in another person's home and depending on

\
1
\
1
\
\
1
1
1
1
1
1
\
1
1
!
!
\

OCR: BAF AF 13 PAR :
\
1
1
1
1
1
1
\
1
1
!
1
\
1
1
\
1

f

i

HY HY HY Is PATTEM BF AA
(“JJ A] 4k, The fostered children series)

1
1
I
I
1
1
1
1
1
I
I
I
1
1
1
1
1
1
I
I
1
1
1
1
1
I
I
I
1
1
1
1
1
1
I
I
1
,

Figure 2: The overall workflow of CardRewriter. Given an input query, CardRewriter retrieves multi-source knowledge from
short-video platforms, summarizes it into a concise knowledge card relevant to the query, and then leverages this card to better

interpret user intent and refine the query.

For each query q € Qsim, we extract its multi-modal knowledge
Mg from the top-k retrieved videos in the same manner as Min. The
enriched knowledge set Mgim is then defined as:

Mim = J Mg (5)
q€Qsim
Finally, we obtain open-domain documents M,x through the self-
built API service, serving as an additional knowledge source when
relevant videos are scarce. We aggregate all collected knowledge
and remove duplicates, yielding:

M= {Set(Min U Mim), Mex} (6)

where M represents the multi-source knowledge collected for query
x to support query rewriting on short-video platforms.

3.1.2. Card-based Rewriting. The multi-source knowledge collected
in Section 3.1.1 often suffers from structural inconsistencies, ex-
cessive noise, and limited relevance to the input queries [3, 5]. To
mitigate these issues, we investigate how to effectively integrate,
denoise, and exploit the collected knowledge. We first employ a card
generation model Cg to summary such knowledge into a concise
and informative knowledge card that highlights content directly
related to user queries. Subsequently, a query rewriting model Gg
leverages the generated card to rewrite the input query, thereby
enhancing its understanding of the search context. This design en-
ables accurate rewriting without resorting to resource-intensive pre-
training [4], which is costly and challenging to update frequently.
Both Cg and Ge utilize LLM as their foundation, possessing strong
capabilities in contextual understanding and text generation. The
overall process can be expressed as:

y= Go(x, c), c= Co(x, M) (7)

Where y denotes the rewritten query and c presents the generated
knowledge card.

3.2 Training Framework

To optimize both the card generation and query rewriting models,
as illustrated in Figure 7, we employ a two-stage training pipeline
for each, consisting of supervised fine-tuning (SFT) followed by
preference alignment through group relative policy optimization
(GRPO). Furthermore, we design a reward system that balances the
semantic relevance between the original query and the model’s
output, as well as the latter’s influence on the quality of the retrieved
video list, thereby guiding the GRPO training process.

3.2.1. Task Formulation. Both tasks can be unified as y — (x, K),
where x is the input query. For card generation, K denotes the multi-
source knowledge collected for x, and y is the resulting knowledge
card. For query rewriting, K corresponds to the generated knowl-
edge card, and y is the rewritten query. We further introduce a
rewritten query variable, rq, which is used for three key purposes:
(I) data filtering during the SFT stage, (II) training the reward model
that predicts system preference, and (III) reward computation in the
GRPO stage. In the card generation task, rg is obtained by first gen-
erating a knowledge card and then perform rewriting, whereas in
the query rewriting task, it is generated directly. We train the card
generation model Cg and the query rewriting model Gg through
the SFI+GRPO pipeline, with both two models represented as asrr
and zgrpo in these two stages respectively.

3.2.2 Supervised Fine-tuning. High-quality training data is essen-
tial for the SFT stage of both tasks. To this end, we construct a large
query set Q.¢; from Kuaishou’s search logs and collect the corre-
sponding knowledge K for each query x € Qsf;, which presents
either multi-source knowledge or a knowledge card. For each query,
we generate an output y conditioned on its knowledge K, forming
the initial dataset:


CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

To further improve data quality, we filter D according to two
criteria: (I) Semantic Relevance, assessed by a judge model Rpey!,
which evaluates query—card relevance and query-rewrite relevance
in the two tasks, respectively; and (II) System Preference, denoted as
SYS(-), which first compares the hitrate score between the rewritten
query rq and the original query x, and if equal, compares their
increment score. The computation of these metrics is detailed in
Section 4.2. The resulting SFT dataset is defined as:

Dspr = {(x, K, y) | (x, K,y) € D,

SYS(x, rq) =1, Rrei(x, y) = 1}, (8)

where SYS(x, rq) = 1 denotes the rewritten query is better than the
original query from the search system view, Rrei(x, y) = 1 indicates
semantic alignment.

We fine-tune the SFT model zsrr as a conditional sequence
generator using token-level cross-entropy:

Lsrr(9) = -E(x,k,y)~Dgp, Hog tsrr(y | x, K)] . (9)

3.2.3 Reward System. We present the construction of our self-
designed reward system, denoted as Roverail, Which aims to balance
query relevance and retrieval effectiveness. Specifically, Roverall
consists of two components: Rrel, which measures semantic rele-
vance, and Rsys, which simulates system-level preferences. Since
direct system preference scores are not readily accessible during
model training, we build Bradley—Terry (BT)-based reward model
Rsys [25] to approximate system evaluations and provide feedback
for subsequent GRPO optimization. To construct the training data
for this reward model, we first sample a large query set Q;m from
Kuaishou’s search logs. For each query x € Q;m, we apply the
rewriting pipeline described in Section 3.1 to generate two candi-
date rewrites. These rewrites are then evaluated by the offline search
system through SYS(-), and construct the reward model dataset
Drm = {(x,rq*,rq~) | x € Qrm}, where rq* and rq~ denote the
preferred and less-preferred rewrites, respectively. Following the
BT formulation, the preference probability is:

exp(Reys(x, rq’))

exp(Rsys(x, rq*)) + exp(Reys(x,rq7))-
(10)
The Reys is trained by minimizing the negative log-likelihood of
observed preferences:

Po(rq’ > rq |x) =

Lem (9) = —Ecergtrq-)~Drm | log Po(rg* > rq 1 x)|- (1)
Finally, we combine Rsy; and Rpe to form Roverall:

Rsyss if Rsys >0
Roveral = 40.1, if Rsys =O and Ree > 0 (12)
0, if Rsys =Rrel = 0

3.2.4 Objective alignment. Equipped with the SFT model zspr and
the overall reward function Roverall, Which jointly balance rele-
vance and system preference, we employ the Group Relative Policy
Optimization (GRPO) algorithm [22] to align long-tail queries with
the descriptive style of proprietary short-video content. Specifi-
cally, we construct the training dataset Dg;po by collecting real user
queries from Kuaishou’s search logs. For each query x € Dgrpo, a

‘https://huggingface.co/Qwen/Qwen3-235B-A22B

Conference acronym XX, June 03-05, 2018, Woodstock, NY

(a | Knowledge = {op _— Rewritten | __. fR

Card Query
Card Generator “=== “Query Rewriter , Online
Co Go Search System
Update = i
Offline aan | New Recall Videos |------------------~

Not Found | y

_——
>>| KVStore | ——>|_ New Recall Videos | ———
a
=

No Not Found _
Ranking System
|_______ | Original Recall Videos _ ¥§

Figure 3: Development Strategy.

group of G rollout trajectories y = {y;}@, is generated using the
previous policy ig. The current policy model zgrpo is optimized
by maximizing the following objective:

G F
1 Ny K) »
4 min [soy ed A,
G Torq (Y' |x, K)

Tcrro(9) = E(..K)~Dgrpo: {yis?,
yi~Told (-|xK)

i=1

oy "x, K -
. ee 1-e,1+ e)A\) - pet cml er
(13)
where € denotes the clipping ratio, and A; represents the advantage
of the i-th rewritten query rgq;, computed based on the overall

reward Roverall across all rewrites within the same group.

3.3 Online Serving

Due to the large parameter size and the auto-regressive nature of
LLMs, directly deploying CardRewriter under Kuaishou’s stringent
low-latency search requirements is infeasible. To address this limi-
tation, we adopt a near-line deployment strategy [4, 20], in which
query rewriting is performed offline for a targeted subset of queries.
The selection criteria are: (i) Average daily searches between 5 and
300 in the past 7 days; (ii) Query intent not limited to a username;
and (iii) poor retrieval performance, characterized by low average
relevance, low click-through rates, and high query reformulation
rates. This selection covers approximately 15-20% of daily search
traffic, corresponding to around 5 million queries. For each selected
query, we first collect multi-source knowledge and summarize it
into a knowledge card via Cg. The query is then rewritten by Gg
based on the generated knowledge card, and the retrieved most
relevant videos are cached in an online key-value (KV) store. Stor-
ing videos instead of rewriting queries can eliminate the impact
of personalization while saving online inference resources. During
the online stage, as illustrated in Figure 3, each incoming query
first checks the KV store: if a match is found, the cached videos are
appended to the original video list; otherwise, the offline rewriting
process is repeated. Cached entries that result in misses expire af-
ter seven days, or sooner if their relevance or click-through rate
exceeds predefined thresholds.

4 Experiments
4.1 Datasets

4.1.1 Training Dataset. We construct three datasets for card gen-
eration, query rewriting, and reward modeling. (I) Card generation:


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

Gong et al.

Table 1: Overall performance of CardRewriter with multiple baselines. CardRewriter demonstrates the strongest performance
across most metrics. We highlight the highest score in bold and the second-highest score with underlines.

Method Knowledge QR-Rel Increment Hitrate@50 Hitrate@300
Original Query : = = 31.40% 53.07%
Previous [4, 6,20]  - 78.42% 66.82% 36.56% 59.96%
- 69.72% 30.17% 29.04% 48.26%
Prompt Naive RAG 62.78% 42.29% 29.49% 48.53%
Card RAG 74.18% 48.71% 32.72% 54.76%
= 76.35% 58.08% 33.86% 58.15%
SFT Naive RAG 73.68% 64.69% 33.95% 59.18%
Card RAG 84.67% 66.88% 39.57% 66.18%
- 77.36% 65.42% 37.96% 62.39%
SFT+DPO Naive RAG 74.93% 67.17% 38.72% 63.03%
Card RAG 86.23% 70.53% 42.18% 70.32%
= 78.98% 65.19% 41.68% 65.71%
SFT+GRPO Naive RAG 74.28% 70.86% 41.05% 65.63%
Card RAG (CardRewriter) 85.73% 74.17% 46.64% 76.04%

We sample 200k queries from Kuaishou’s search logs, retrieve multi-
source knowledge for each, and generate 8 cards per query, resulting
in 1.6M <query, knowledge, card, rewrite> quadruples. Through rejec-
tion sampling, we retain around 30k high-quality <query, knowledge,
card> triples for SFT. Additionally, we collect 60k queries for GRPO
training. (II) Card-based rewriting: We collect 400k queries and fol-
low the same pipeline, producing 3.2M <query, card, rewrite> triples.
After filtering, 50k high-quality pairs remain for SFT training. We
further sample 100k queries for GRPO training. (III) Reward model-
ing: We gather 150k queries, generate multiple rewrites for each,
and assign system preference scores (Section 3.2.3) derived from the
Kuaishou search system. This process yields approximately 240k
<query, good_rewrite, bad_rewrite> tuples, which are used to train a
reward model that predicts system preferences.

4.1.2 Test Dataset. To evaluate model performance across different
tasks, we construct task-specific test sets: 10k queries for card
generation, 10k queries for reward modeling, and 15k queries for
query rewriting, all sampled from Kuaishou search logs.

4.2 Evaluation

4.2.1. Offline Metrics. We employ Rel, Increment, and Hitrate@K
as offline evaluation metrics, each capturing a distinct dimension
of model performance: (I) Rel (Relevance): Evaluates the semantic
quality of model outputs. For card generation, it measures how
relevant the generated knowledge card is to the original query,
referred to as QC-Rel. For query rewriting, it assesses i) the rele-
vance between the rewritten and original queries, and ii) whether
the rewriting effectively integrates the knowledge card, denoted as
QR-Rel. (II) Increment (Retrieval Expansion): Measures the model’s
capability to expand retrieval coverage by quantifying the relative
improvement in recall when using both the original and rewritten
queries, compared to using the original query alone. (III) Hitrate@K
(User Satisfaction): Reflects the proportion of cases in which the
rewritten query successfully retrieves at least one video within the

top-K results that aligns with the user’s intent. More details about
the above metrics can be found in Appendix B.

4.2.2 Online Metrics. We introduce three core online metrics, LVR,
IQRR, and CTR, which best reflect the user’s search experience on
short-video platforms and are used to evaluate the online perfor-
mance of our method. These metrics are defined as follows: LVR =
long-view rates, IQRR = initiative query reformulation rates and CTR
= click-through rates.

4.3 Offline Experiments

4.3.1 Main Results. In our experiments, we compare CardRewriter
with prompt-based, SFT-based, and SFT+DPO-based rewriting meth-
ods, as well as knowledge-enhanced baselines that either exclude
augmentation or directly integrate multi-source knowledge (Naive
RAG). We further adapt the core modules of CSA-QR [6] to the
Kuaishou platform for comparison. Implementation details of CardR
ewriter are provided in Appendix C. Key observations based on
Table 1 are summarized as follows:

Directly injecting multi-source knowledge offers limited
benefits for query rewriting. We evaluate a rewriting approach
that integrates multi-source knowledge against a baseline without
enhancement. Results show that directly injecting such knowl-
edge reduces semantic alignment between rewritten and original
queries across all three baseline methods and our SFI+GRPO model.
The degradation mainly arises from retrieval-augmented rewriting,
which lengthens queries and introduces noise that drifts from the
original intent. Interestingly, naive knowledge injection raises the
increment metric by retrieving additional unseen videos, but hitrate
scores remain unstable, suggesting that few of these new results
match the ground truth.

Summarizing multi-source knowledge into knowledge
cards significantly improves rewriting performance. Com-
pared to directly injecting multi-source knowledge, which often
introduces irrelevant or even conflicting content, summarizing it


CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

Table 2: Ablation study on CardRewriter with different types
of knowledge sources. Both knowledge cards and rewritten
queries are derived through a prompt-based method.

Method QC-Rel Increment Hitrate@300
CardRewriter 91.16% 46.24% 51.36%
Modal Type
w/o vision 89.37% 45.01% 50.14%
w/o textual 86.18% 42.38% 45.62%
Video Retrieval
w/o rel-videos 78.27% 40.42% 47.16%
w/o rule sim 93.24% 44.12% 50.37%
w/o emb sim 92.86% 44.89% 50.86%
Open Domain
w/o ext 92.06% 45.85% 50.31%

into knowledge cards delivers significant improvements. For in-
stance, SFT+DPO with Card RAG reaches a QR-Rel of 86.23% and
SFT+GRPO with Card RAG (CardRewriter) achieves a Hitrate@300
of 76.87%, both significantly outperforming their Naive RAG coun-
terparts. Across all rewriting methods, the incorporation of knowl-
edge cards not only markedly enhances the semantic relevance
between rewritten and original queries, but also expands the scope
of retrieved results and increases the likelihood of covering ground-
truth videos. The results demonstrate that knowledge cards ef-
fectively mitigate the shortcomings of Naive RAG and serve as a
powerful medium for grounding LLM rewrites in platform-specific
knowledge, enabling more precise retrieval.

Different rewriting methods exhibit varying abilities to ab-
sorb external knowledge. Injecting Knowledge cards consistently
surpass naive knowledge enhancement across all rewriting settings,
though the degree of improvement depends on the rewriting strat-
egy. Prompt-based rewriting yields only marginal gains because it
lacks targeted learning to integrate external knowledge effectively.
In contrast, SFT and SFT + DPO substantially enhance retrieval
quality by aligning rewritten queries with contextual cues through
supervised optimization. Most notably, SFT + GRPO achieves the
greatest overall improvement: its reward-guided training explicitly
drives the model to exploit informative signals from knowledge
cards. Consequently, SFT + GRPO not only preserves semantic fi-
delity to the original query but also maximizes the utility of knowl-
edge cards, leading to the strongest performance across QR-Rel and
Hitrate metrics.

4.3.2. Ablation Study on Multi-Source Knowledge. To evaluate the
contribution of different knowledge sources in CardRewriter, we
perform an ablation study by removing each type of injected knowl-
edge and examining the resulting performance. As shown in Table
2, removing either visual or textual modalities from video content
degrades rewriting quality, with textual information playing a more
critical role. This is because missing or ambiguous content in long-
tail queries can often be recovered from video titles and captions.
From a retrieval perspective, rewriting benefits from both videos

Conference acronym XX, June 03-05, 2018, Woodstock, NY

0.1 | mmm REL
mmm SYS
mmm REL+SYS

QR-Rel Increment
Reward System

Figure 4: Effectiveness of different rewards.

Hitrate@300

0.1] === Prompt
mmm SFT
mmm SFT+GRPO

QR-Rel Increment
Card Generation Method

Figure 5: Effectiveness of different card generation methods.

Hitrate@300

retrieved by the original query and those obtained through simi-
lar queries. Notably, even a few relevant videos retrieved by the
form method are highly valuable, as they provide crucial evidence
for building reliable knowledge cards and generating high-quality
rewrites. Finally, incorporating open-domain documents yields fur-
ther gains by providing complementary information that enriches
overall knowledge coverage.

4.3.3 Results of Different Rewards. To assess how reward design
shapes objective alignment, we vary the reward components in
GRPO for the query rewriting task and analyze their impact on
rewriting performance. Figure 4 shows that using only the rele-
vance reward (REL) achieves the highest query-rewrite relevance
scores but fails to expand coverage to new videos that fully meet
user needs. In contrast, relying solely on the system reward (SYS)
retrieves many previously inaccessible videos yet often drifts from
the original intent. Combining the relevance and system rewards
ensures high retrieval relevance while maximizing the number of
user-prefer videos. This demonstrates that our approach effectively
balances query relevance and retrieval effectiveness, preserving
user intent while enhancing overall search experience.

4.3.4 Results of Different Card Generation Methods. To examine
how card quality affects query rewriting, we evaluate the influ-
ence of knowledge cards generated by different methods. Specif-
ically, we compare three approaches: prompt-based, SFT-based,


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

Gong et al.

Query AE A A

What is a crane identity

bel: AAS, RO AT ATR
OCR: 2A KAA — FAs + ERE

A Hl

Video
Contents

bel: MBTUOR AMERY? RAH ER... Ambtitillit
OCR: —3KAiSHEMBTIA AEN
VER: DRAPP Ey

litle: Everyone go buy it, it's gorgeous! #IdentityV #Geisha
OCR: Reviewing the Crane + Burning Cherry skins for you all
Author: Mei Ju Pin Ru

litle: Which of the 16 MBTI personality types do you belong to? The most
complete guide to each letter... #mbtitest

OCR: Understand the meaning of each MBTI letter in one picture.

Author: XinXiang APP Official

External {NS RINGS TSUN. TORE. BUS, DAS aM Ti

DOG§:  sereerenrewe meee come weneme ewe eene en Eee ees

“AMES” ez COLA) UP PS C6 ZO ep BN Be, DAHA ER RTT Bo

[he "crane identity" refers to the noble, elegant, and wise character
symbolized by the crane, as well as its aloof and proud aspects.

"Crane" is an exquisite-quality costume for the Hunter Geisha, in the game
Identity V, featuring a design centered on a traditional Chinese aesthetic.

Card Prompt] Jit MQuery?? MAT AEAMBTLA AEH “ANS? RRMA, Bee COTM) 5

WRAY “AMES? Ben.

Ours] JskaQuery UB De CALM) HRA AES BEA,

[he original query could refer to a "Crane" personality type in the context
of MBTI, or the "Crane" skin from the game Identity V.

In the original query, the user mistakenly confused the game's name, /dentity
V, with the 'Crane' skin. It focuses on information regarding the 'Crane’ skin
for the character Geisha in Identity V.

Bi

SAR ELIE Fs CALS BER AEE, BSR EA Er 2 A FY BIR

Rewritten [LLM] MS AAG EL

Query

Ours] SELASSIE Be

-~ Crane Identity, Meaning

Crane Identity, MBTI

~~ Identity V, Geisha’s Crane skin

Figure 6: Case Study of long-tail query on the short-video platform. CardRewriter remains reliable, producing accurate rewriting

even when the collected knowledge is inconsistent.

Table 3: Online A/B test of CardRewriter on Kuaishou Search.

LVRt IORR{ CTRt
Covered Traffic +1.853%  -2.630%  +3.729%
Full Traffic +0.235%  -0.229%  +0.342%

and SFT+GRPO-based card generation. As shown in Figure 5, the
prompt-based approach consistently performs worst across all met-
rics, underscoring the limits of relying solely on prompts. The
SFT-based method substantially improves performance, demon-
strating the benefit of training a dedicated card generation model
over directly using prompts. Building on this, the SFT+GRPO ap-
proach further refines card content via learned reward signals,
achieving the best rewriting performance. These results indicate
that enhancing card quality through advanced training directly
leads to higher-quality rewritten queries.

4.3.5 Case Study. As shown in Figure 6, our approach effectively
rewrites queries aimed at retrieving proprietary content. In this
example, direct rewriting by an LLM only paraphrases the query
without resolving the ambiguous reference to "crane personality".
Likewise, relying solely on multi-source knowledge fails due to
conflicting evidence across videos and documents, for instance,
whether the term denotes a crane-like personality or the "Crane
Skin" in Identity V. In contrast, CardRewriter constructs a knowl-
edge card that summarizes these sources, correctly interpreting the
query as referring to the Identity V "Crane Skin". Moreover, prompt-
based card generation alone cannot fully resolve such conflicts. By
adding SFT + GRPO training, which explicitly optimizes retrieval
for user-preferred videos, our method produces knowledge cards
that align more closely with users’ actual search intent.

4.4 Online Experiments

To assess the real-world online performance of CardRewriter, we
conducted a 10-day deployment on Kuaishou Search. Following the
serving strategy described in Section 3.3, the system is exposed to

approximately 16% of real traffic. During this experiment, we mon-
itor three key search metrics: LVR, IOQRR, and CTR, which jointly
capture user engagement and the overall quality of the search ex-
perience. As summarized in Table 3, our approach delivers notable
improvements on both the covered traffic and the full platform
traffic. On the covered traffic, LVR and CTR increase by 1.853%
and 3.729% respectively, indicating a substantial improvement in
user engagement and the relevance of search results. Meanwhile,
IQRR decreases by 2.630%, reflecting a marked reduction in users
actively reformulating their queries. Consistent improvements are
also observed on full traffic, where LVR and CTR continue to rise
and IQRR decline, demonstrating that CardRewriter not only per-
forms well on a traffic subset but also robustly enhances the overall
search experience.

5 Conclusion

In this work, we introduce CardRewriter, an LLM-based framework
that leverages domain knowledge from short-video platforms to
improve the rewriting of long-tail queries. For each query, we collect
multi-source knowledge by retrieving top-k videos from Kuaishou’s
search system, aggregating relevant videos linked to similar high-
supply queries from search logs, and supplementing with open-
domain documents. This information is then integrated, denoised,
and summarized into a concise knowledge card, which provides
contextual guidance for accurate query understanding and high-
quality rewriting. To optimize CardRewriter, we employ a two-stage
training strategy: supervised fine-tuning followed by group relative
policy optimization, with a customized reward system balancing
query relevance and retrieval effectiveness. Offline experiments
demonstrate the effectiveness of our approach in rewriting queries
aiming to retrieve proprietary content, while online A/B tests show
significant gains in long-view rate (LVR) and click-through rate
(CTR), as well as reductions in initiative query reformulation rate
(IQRR), leading to overall better search performance.


CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

References

[1]

[2]

[3]

[4]

[10

{11

[12

[13

[14

[15

[16

[17

[18

[19

[20

[21

[22

Shangyu Chen, Xinyu Jia, Yingfei Zhang, Shuai Zhang, Xiang Li, and Wei Lin.
2025. IterQR: An Iterative Framework for LLM-based Query Rewrite in e-
Commercial Search System. arXiv preprint arXiv:2504.05309 (2025).

Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang
Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. 2025. Improving retrieval-
augmented generation through multi-agent reinforcement learning. arXiv
preprint arXiv:2501.15228 (2025).

Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare
Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The
power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 719-729.

Aijun Dai, Zhenyu Zhu, Haiqing Hu, Guoyu Tang, Lin Liu, and Sulong Xu. 2024.
Enhancing E-Commerce Query Rewriting: A Large Language Model Approach
with Domain-Specific Pre-Training and Reinforcement Learning. In Proceedings of
the 33rd ACM International Conference on Information and Knowledge Management.
4439-4445.

Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu.
2024. Enhancing noise robustness of retrieval-augmented language models with
adaptive adversarial training. arXiv preprint arXiv:2405.20978 (2024).

Yunling Feng, Gui Ling, Yue Jiang, Jianfeng Huang, Dan Ou, Qingwen Liu, Fuyu
Ly, and Yajing Xu. 2025. Complicated Semantic Alignment for Long-Tail Query
Rewriting in Taobao Search Based on Large Language Model. In Proceedings of
the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2.
4435-4446.

Iftah Gamzu, Marina Haikin, and Nissim Halabi. 2020. Query rewriting for
voice shopping null queries. In Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1369-1378.
Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. 2024. Smartrag:
Jointly learn rag-related tasks from the environment feedback. arXiv preprint
arXiv:2410.18141 (2024).

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin
Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented
generation for large language models: A survey. arXiv preprint arXiv:2312.10997
2, 1 (2023).

Peiyuan Gong, Jiamian Li, and Jiaxin Mao. 2024. Cosearchagent: a lightweight
collaborative search agent with large language models. In Proceedings of the 47th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 2729-2733.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.
Retrieval augmented language model pre-training. In International conference on
machine learning. PMLR, 3929-3938.

Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiging Sun, Qian Liu, Jane Dwivedi-Yu,
Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented
generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing. 7969-7992.

Vishal Kakkar, Chinmay Sharma, Madhura Pande, and Surender Kumar. 2023.
Search query spell correction with weak supervision in E-commerce. In Proceed-
ings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 5: Industry Track). 687-694.

Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A survey on
retrieval-augmented text generation. arXiv preprint arXiv:2202.01110 (2022).
Sen Li, Fuyu Ly, Taiwei Jin, Guiyang Li, Yukun Zheng, Tao Zhuang, Qingwen
Liu, Xiaoyi Zeng, James Kwok, and Qianli Ma. 2022. Query rewriting in taobao
search. In Proceedings of the 31st ACM International Conference on Information &
Knowledge Management. 3262-3271.

Hui Liu, Dawei Yin, and Jiliang Tang. 2020. Query rewriting. In Query Under-
standing for Search Engines. Springer, 129-144.

Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative relevance
feedback with large language models. In Proceedings of the 46th international ACM
SIGIR conference on research and development in information retrieval. 2026-2031.
Lang Mei, Siyu Mo, Zhihan Yang, and Chong Chen. 2025. A survey of multimodal
retrieval-augmented generation. arXiv preprint arXiv:2504.08748 (2025).
Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, and James Allan. 2021. Ceqe:
Contextualized embeddings for query expansion. In European conference on
information retrieval. Springer, 467-482.

Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong
Xu, Tong Xu, and Enhong Chen. 2024. Large language model based long-tail
query rewriting in taobao search. In Companion Proceedings of the ACM Web
Conference 2024. 20-28.

Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao, Bo
Long, and Wen-Yun Yang. 2021. Query rewriting via cycle-consistent transla-
tion for e-commerce search. In 2021 IEEE 37th International Conference on Data
Engineering (ICDE). IEEE, 2435-2446.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei
Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing

24

25

26

27

28

29

30

31

32

33

Conference acronym XX, June 03-05, 2018, Woodstock, NY

the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300 (2024).

Zihua Si, Zhongxiang Sun, Xiao Zhang, Jun Xu, Xiaoxue Zang, Yang Song, Kun
Gai, and Ji-Rong Wen. 2023. When search meets recommendation: Learning
disentangled search representation for recommendation. In Proceedings of the 46th
international ACM SIGIR conference on research and development in information
retrieval. 1313-1323.

Mingyang Song and Mao Zheng. 2024. A Survey of Query Optimization in Large
Language Models. arXiv preprint arXiv:2412.17558 (2024).

Hao Sun, Yunyi Shen, and Jean-Francois Ton. 2024. Rethinking bradley-terry mod-
els in preference-based reward modeling: Foundations, theory, and alternatives.
arXiv preprint arXiv:2411.04991 (2024).

Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Dewei Leng, Yanan Niu, Yang Song,
Xiao Zhang, and Jun Xu. 2023. KuaiSar: A unified search and recommendation
dataset. In Proceedings of the 32nd ACM international conference on information
and knowledge management. 5407-5411.

Binbin Wang, Mingming Li, Zhixiong Zeng, Jingwei Zhuo, Songlin Wang, Su-
long Xu, Bo Long, and Weipeng Yan. 2023. Learning multi-stage multi-grained
semantic embeddings for e-commerce search. In Companion Proceedings of the
ACM Web Conference 2023. 411-415.

Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with
large language models. arXiv preprint arXiv:2303.07678 (2023).

Rong Xiao, Jianhui Ji, Baoliang Cui, Haihong Tang, Wenwu Ou, Yanghua Xiao,
Jiwei Tan, and Xuan Ju. 2019. Weakly supervised co-training of query rewrit-
ing andsemantic matching for e-commerce. In Proceedings of the twelfth ACM
international conference on web search and data mining. 402-410.

Shi-Qi Yan, Jia~-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval
augmented generation. (2024).

Dezhi Ye, Bowen Tian, Jiabin Fan, Jie Liu, Tianhua Zhou, Xiang Chen, Mingming
Li, and Jin Ma. 2023. Improving query correction using pre-train language model
in search engines. In Proceedings of the 32nd ACM International Conference on
Information and Knowledge Management. 2999-3008.

Boyang Zuo, Xiao Zhang, Feng Li, Pengjie Wang, Jian Xu, and Bo Zheng. 2025.
VALUE: Value-Aware Large Language Model for Query Rewriting via Weighted
Trie in Sponsored Search. arXiv preprint arXiv:2504.05321 (2025).

Simiao Zuo, Qingyu Yin, Haoming Jiang, Shaohui Xi, Bing Yin, Chao Zhang, and
Tuo Zhao. 2022. Context-Aware Query Rewriting for Improving Users’ Search
Experience on E-commerce Websites. arXiv preprint arXiv:2209.07584 (2022).


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

Qsrt _> Sin

<Query, RAG>

Teacher Model

Step 3: Objective Alignment

GRPO Model

I

I
BT-based |
I = 14 Lo D rm 1
I SS < +O->
I Rollout System Reward Reward Model Query, Q*.Q |
I —>
Q = < Moe! System !

oy RA Rollout | Scoring |

<Query, RAG> & ollou coring |

|
|
I
|
I

Update
@
aft

Relevance Reward
Model

Gong et al.
1
1
|
1
Rollout Reject
—_ = D —_—“— Ds5t > {=
<Query, RAG, Output> | Sampling <Query, RAG, Output> |
SFT Model
ee ee l
Initialize
---------- --- -- -- -K-- Sop cscs cc cys

— fa

ae
Teacher Model

<Query, RAG>

Figure 7: Training Strategy.

A Detailed Prompts

We outline the prompt details for implementing the CardRewriter
rewriting workflow to facilitate the reproducibility of our work.

e Card Generation. Summarizing the multi-source knowledge
collected from the short-video platform into an informative
and concise knowledge card that remains relevant to the
original query, as shown in Table 4.

e Card-based Rewriting. leveraging the generated knowledge
card to better capture and understand the search context,
thereby producing higher-quality query rewrites, as illus-
trated in Table 5.

B_ Offline Metrics

We provide a detailed description of how to implement the three
metrics: Rel, Increment, and Hitrate@K.

e Rel (Relevance): evaluates the semantic quality of model out-
puts. For card generation, it measures how relevant the gen-
erated card is to the original query (QC-Rel). For card-based
rewriting, it assesses: (I) the semantic relevance between
the rewritten and the original queries., and (II) Whether the
rewriting effectively integrates the content of the knowl-
edge card (QR-Rel). We utilize Qwen3-235B-A22B to provide
binary judgments (1 = success, 0 = failure) for each instance.

e Increment (Retrieval Expansion): Quantifies the model’s abil-
ity to expand retrieval coverage by comparing recall obtained
from the original query with that from its rewritten coun-
terpart. let Vx and Vy denote the video lists retrieved for
the original query x and the rewritten query y, respectively.
Then, Increment is defined as:

Vx U Vyl — [Vix

|Vx_| a4)

Increment =

e Hitrate@K (User Satisfaction): Assesses whether the rewrit-
ten query can retrieve videos that align with user intent.
Given an input query x, we construct a ground-truth set &
consisting of videos that users either clicked on or watched
for an extended period after reformulating x within a one-
week window. Hitrate@K is then computed as:

1, ifsu,¢6,i<K
Hitrate@K = (15)

0, otherwise

where v; denotes a video retrieved for query x.

C Implementation Details

The training of CardRewriter proceeds through three stages: Super-
vised Fine-Tuning, Reward Modeling that captures system prefer-
ences, and Objective Alignment. All experiments were conducted
utilizing two 8-GPU H800 nodes.

Supervised Fine-Tuning We employ Qwen2.5-VL-72B to gen-
erate knowledge cards for constructing the initial dataset in the
card generation task, and Qwen3-235B-A22B to produce rewrit-
ten queries for building the initial dataset in the query rewriting
task. For card generation, we fine-tune the Qwen2.5-VL-7B-Instruct
model with a learning rate of 1 x 10~° using the AdamW optimizer
(f; = 0.9, Bo = 0.999) and a weight decay of 0.01, running train-
ing for one epoch with a per-device batch size of 16 and gradient
accumulation of 4 steps. The vision tower and multi-modal projec-
tor remain frozen while the language model parameters are fully
fine-tuned. We employ DeepSpeed ZeRO-3 for distributed training
efficiency and conduct all computations in bfloat16 precision. For
query rewriting, we fine-tune the Qwen3-8B model with a learning
rate of 1 x 1075 using the AdamW optimizer (/; = 0.9, By = 0.999)
and a weight decay of 0.01, running training for two epochs with a
per-device batch size of 16 and gradient accumulation of 4 steps.


CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

Conference acronym XX, June 03-05, 2018, Woodstock, NY

Table 4: Card Generation Prompt. Due to the excessive length of the prompt, we have made some omissions in the section

where video information is inserted.

Role

You are a short video search expert. Your core task is to analyze and understand the user’s [original query], and—by combining it with the
retrieved short video information and related general information—generate a [demand description analysis] within 200 characters. The
goal is to help the short video search platform better understand the user’s true intent, so it can use the [original query] together with the
[demand description analysis] to rewrite an improved [rewritten query], thereby helping users find more relevant and effective short
videos.

Task Requirements

1. Short video information includes: short video title, cover OCR text, author name, background music name, and key frames. General
information refers to open-domain data related to the original query.

2. You need to use this information to determine whether the searched videos are relevant to the [original query]. Since user queries may
contain ambiguous expressions, you should use the relevant information comprehensively to clarify the true referent (e.g., a specific
streamer, short drama title, or game name) and specify it in the [demand description analysis].

3. The generated [demand description analysis] must be concise and effective, helping the platform accurately understand the user’s
search intent. Do not include content irrelevant to entities or attributes in the [original query]. Do not include phrases like “based on
video information” or “according to search results,” and do not give instructions on how to rewrite the original query. Only provide an
analysis of the intent behind the original query based on the retrieved videos.

4. Output in JSON format. The desc field represents your [demand description analysis]. The final output must contain only one JSON

result, with no extra characters.

Role

—- Original Query:

Query

—— Short Video Information:

<Video 1>

—— Title: title

-- Cover OCR Text: OCR

—— Author Name: author name

-- Background Music Name: BGE name
~- Key Frames: frame1; frame2; frame3
</Video 1>

<Video 2>

</Video 2>
<Video 3>

</Video 3>
—— General Information:
Open-domain Knowledge

We enable FlashAttention-2 and Liger kernels to accelerate com-
putation and perform all operations in bfloat16 precision. A co-
sine learning rate scheduler with a warmup ratio of 0.1 adjusts
the learning rate, and DeepSpeed ZeRO-3 is used for distributed
optimization.

Reward Modeling We fine-tune the Qwen3-8B model (Classifi-
cation version) with a learning rate of 1 x 107° using the AdamW
optimizer (f; = 0.9, B. = 0.999) and a weight decay of 0.01. Train-
ing runs for two epochs with a per-device batch size of 16 anda
gradient accumulation step of 4. All computations are performed
in bfloat16 precision to improve training efficiency, and the maxi-
mum sequence length is set to 512 tokens.

Objective Alignment For the card generation task, we adopt
the SFI-based card generation model combined with GRPO advan-
tage estimation. The hyperparameters are configured as follows:
training batch size of 512, maximum prompt length of 10,240 tokens,
and maximum response length of 512 tokens. The actor model is
optimized with a learning rate of 1 x 10~°. The KL loss coefficient is
set to 0.01 using the low-variance KL formulation, and entropy reg-
ularization is disabled. Rollouts are generated via the vLLM engine
with a tensor model parallel size of 2, GPU memory utilization of
0.6, chunked prefill enabled, and a rollout sample size of n = 8. The
training process consists of a single epoch. For the query rewrit-
ing task, we employ an SFT-based query rewriting model with the
following hyperparameters: training batch size of 128, maximum
prompt length of 2,048 tokens, and maximum response length of


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY Gong et al.

Table 5: Card-based Rewriting Prompt

Role

You are an expert specializing in optimizing user search queries for short video platforms. Your core task is to understand the user’s
[original search query] and rewrite it into a [rewritten query] that is more easily understood by short video search engines. The information
provided to you includes the [original search query] and the [original search query needs analysis] provided by search experts. You
can combine this needs analysis to understand the [original search query] and make targeted rewrites. While maintaining semantic
consistency, your [rewritten query] can retrieve additional relevant videos that the [original search query] could not.

Task Requirements

====Query Analysis Requirements====

1. Short video search queries have the following characteristics: *“strong domain knowledge, colloquial and concise descriptions, vague or
broad intent, and a high number of typos**. These characteristics must be considered when analyzing user search needs.

2. When analyzing video search needs in user queries, consider the core intent, key entities, and attribute constraints. The necessary
attribute constraints that appear in each query should be considered. You can refer to the content in the [Original Search Query Requirement
Analysis] to help you correctly analyze and understand the [Original Search Query].

====Query Rewriting Requirements====

1. Based on your analysis, rewrite the original search query from the perspective of a video search expert. *“The user’s core intent and
original needs must not change before and after the rewriting”. Avoid changing the scope of the user’s original needs, such as removing
key attributes or adding irrelevant attribute restrictions.

2. The rewritten query should be as different as possible from the original query, such as by changing the wording and description to
increase the number of videos retrieved and prevent excessive duplication of results between the rewritten and original queries.

3. The rewritten query should be concise and clear. Replace complex user descriptions with concise synonyms. Whenever possible, rewrite
long queries to shorten them while preserving the semantics, rather than lengthening short queries. Avoid adding meaningless suffixes
such as "xxx video" or "xxx related videos" as rewrite suffixes, and avoid translating English words in the query. If the user’s query
contains irrelevant symbols such as #, remove them.

4. For long and difficult user queries, use keywords in parallel as much as possible, rather than piling them into long declarative sentences.
5. Domain terms that are still unclear after analyzing the [Requirements Analysis] should be output as is. Do not attempt to guess their
meaning. If the user’s original search query contains usernames, short drama titles, or other proper nouns related to possible short video
scenes, do not modify them.

====Output Requirements====

1. Output in JSON format. The RewriteQuery field represents [Rewritten Query].

2. Do not output any irrelevant symbols or descriptions other than the output JSON.

Task Start

Original Search Query: {{Query}}

Original Search Query Requirements Analysis: {{Card}}

128 tokens. The actor model is optimized with a learning rate of sample size of n = 8, sampling temperature T = 1.0, top-p = 1.0,
1 x 10~°, trained using dynamic batch sizes under the FSDP2 dis- and top-k = —1. The training process also consists of a single epoch.
tributed strategy. The KL loss coefficient is set to 0.001 with the
low-variance KL formulation, and entropy regularization is dis-
abled. Rollouts are generated via the vLLM engine with a rollout

Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
