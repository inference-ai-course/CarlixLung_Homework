2510.11167v1 [cs.CL] 13 Oct 2025

arXiv

Bridging Gaps in Hate Speech Detection:
Meta-Collections and Benchmarks for
Low-Resource Iberian Languages

Paloma Piot!, José Ramom Pichel Campos’, Javier Parapar!

'TRLab, CITIC Research Centre, Universidade da Corufia, Spain.
2CiTIUS, Universidade de Santiago de Compostela, Spain.

Contributing authors: paloma.piot@udc.es; jramon.pichel@usc.gal;
javier.parapar@udc.es;

Abstract

Hate speech poses a serious threat to social cohesion and individual well-being,
particularly on social media, where it spreads rapidly. While research on hate
speech detection has progressed, it remains largely focused on English, result-
ing in limited resources and benchmarks for low-resource languages. Moreover,
many of these languages have multiple linguistic varieties, a factor often over-
looked in current approaches. At the same time, large language models require
substantial amounts of data to perform reliably, a requirement that low-resource
languages often cannot meet. In this work, we address these gaps by compiling a
meta-collection of hate speech datasets for European Spanish, standardised with
unified labels and metadata. This collection is based on a systematic analysis and
integration of existing resources, aiming to bridge the data gap and support more
consistent and scalable hate speech detection. We extended this collection by
translating it into European Portuguese and into a Galician standard that is more
convergent with Spanish and another Galician variant that is more convergent
with Portuguese, creating aligned multilingual corpora. Using these resources,
we establish new benchmarks for hate speech detection in Iberian languages.
We evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover, we
perform a cross-lingual analysis with our target languages. Our findings under-
score the importance of multilingual and variety-aware approaches in hate speech
detection and offer a foundation for improved benchmarking in underrepresented
European languages.

Keywords: hate speech, meta-collection, low-resource, LLMs, cross-lingual


1 Introduction

Hate speech remains a significant issue on social media, posing serious threats to indi-
vidual safety and community cohesion (Gonzdélez-Bailén & Lelkes, 2022). The rapid
expansion of online platforms has facilitated the spread of harmful content, amplify-
ing its impact and making moderation increasingly challenging (Hickey et al., 2023;
Vogels, 2021). Recent research shows that approximately 30% of young individuals
face cyberbullying (Kansok-Dusche et al., 2023), while 46% of Black/African Amer-
ican adults have reported online racial harassment (Anti-Defamation League, 2024),
highlighting the critical importance of tackling hate speech. At the same time, some
platforms have scaled back human content moderation efforts, often prioritising user
engagement over strict enforcement of community guidelines (Booth, 2025), which has
further intensified the spread of harmful speech. Consequently, this reduction in human
oversight creates an urgent need for effective detection systems capable of identifying
and mitigating hate speech promptly across all languages and linguistic varieties.

However, most research on hate speech detection has focused on high-resource
languages, particularly English, where large-scale datasets and advanced language
models have driven significant progress (Fortuna & Nunes, 2018; Piot, Martin-Rodilla,
& Parapar, 2024; Vidgen & Derczynski, 2020). In contrast, low-resource languages
remain largely underrepresented, with limited annotated data to train effective mod-
els. To overcome this, current approaches often depend on cross-lingual transfer
learning and multilingual Large Language Models (LLMs), which leverage knowledge
from high-resource languages to support low-resource ones (Ranasinghe & Zampieri,
2023). Although multilingual LLMs have demonstrated potential for handling mul-
tiple languages (Zhong et al., 2024), their performance in low-resource contexts is
still inconsistent, especially for languages with substantial regional variation (Alam,
Chowdhury, Boughorbel, & Hasanain, 2024; Joshi et al., 2025). This gap underscores
the need to evaluate state-of-the-art LLMs in underrepresented linguistic settings and
to develop tailored resources that enhance hate speech detection for such languages.

Beyond the lack of annotated datasets and linguistic resources, low-resource lan-
guages present additional challenges due to internal linguistic diversity. Spanish and
Portuguese, for example, have distinct European and American varieties. And as for
Galician, a Romance language spoken in north-western Spain (Galicia and other com-
munities), it is extremely close to Portuguese, if it uses the same orthography, or to
Castilian, with its current orthography (Pichel, Gamallo, Alegria, & Neves, 2020),
which can only converge with Spanish or Portuguese, as Carvalho Calero points out
(Calero, 1981). While some datasets explicitly label texts by variety (Castillo-lépez,
Riabi, & Seddah, 2023), this distinction is often not taken into account during model
training, with data from different varieties being merged or used interchangeably.
This can lead to suboptimal performance when models are applied to specific regional
varieties.

To better understand the current landscape, we examined available hate speech
datasets for the languages and varieties of interest. For Portuguese, available datasets
are limited and focus on Brazilian Portuguese, leaving European Portuguese largely
unrepresented (Leite, Silva, Bontcheva, & Scarton, 2020; Salles, Vargas, & Benevenuto,
2025; Trajano, Bordini, & Vieira, 2023; Vargas, Carvalho, Rodrigues de Gées, Pardo,


& Benevenuto, 2022). The situation is somewhat better for Spanish, where several
publicly available datasets exist. These include a mix of European and Latin American
Spanish, with some explicitly distinguishing between the two. In contrast, Galician
lacks any dedicated hate speech datasets. The only related resource is a misogyny
detection corpus derived from one of the Spanish datasets included in our review
(Alvarez-Crespo & Castro, 2024). This gap highlights the need for targeted datasets
that better represent European Portuguese, European Spanish, and Galician.

1.1 Research questions
To guide our work, we formulate the following research questions:

1. RQ1: To what extent do existing hate speech datasets reflect the linguistic
characteristics of European Spanish, European Portuguese, and Galician?

2. RQ2: Can synthetic data generation effectively compensate for the lack of annotated
hate speech resources in European Portuguese and Galician?

3. RQ3: How well do state-of-the-art large language models perform in detecting hate
speech across these low-resource linguistic varieties?

4. RQ4: Does accounting for Galician’s internal variation (Spanish-like vs.
Portuguese-like) affect model performance?

To address these questions, this study investigates the representation of European
Spanish, European Portuguese, and Galician in current hate speech resources and
explores strategies to improve coverage and performance in these settings. Guided
by our research questions, we analyse existing datasets, generate synthetic data for
low-resourced languages, and evaluate the performance of state-of-the-art large lan-
guage models. Our findings underscore the pressing need for dedicated hate speech
resources in European Portuguese and Galician. While translated and synthetic
datasets, especially those derived from Spanish, offer meaningful improvements, tai-
lored language-specific data remains critical for achieving optimal results. Fine-tuning
consistently outperforms zero and few-shot approaches, reinforcing the importance of
targeted dataset development. By centring linguistic diversity and low-resource chal-
lenges, we provide new benchmarks and insights to advance hate speech detection in
the Iberian context.

1.2 Contributions

In this work, we address these challenges by focusing on hate speech detection in
three underrepresented linguistic varieties: European Spanish, European Portuguese,
and Galician. Our contributions are five-fold. First, we conduct a systematic review
of existing hate speech datasets in Spanish, Portuguese, and Galician, selecting those
that align with our definition of hate speech and the target language varieties. Based
on this review, we construct a meta-collection of relevant resources. Second, to address
the lack of data for European Portuguese and Galician, we generate synthetic datasets
to support further research in these languages. Third, we perform a detailed lexical
and linguistic analysis of the datasets, highlighting both their strengths and current


limitations within the Iberian context. Fourth, we evaluate state-of-the-art large lan-
guage models under zero-shot, few-shot, and fine-tuning settings, establishing new
benchmarks for hate speech detection in Iberian languages. Moreover, we evaluate
the cross-lingual performance of our target languages. Finally, we introduce a novel
perspective by addressing the linguistic duality of Galician, distinguishing between
its Spanish and Portuguese-influenced variants, a factor not previously explored in
machine learning or natural language processing for hate speech detection.

2 Related work
2.1 Hate speech detection

Detecting hate speech is a challenging task that involves not only technical complexity
but also ethical and social considerations. The boundaries between offensive language
and hate speech are often blurred, shaped by context, cultural norms, and individ-
ual perception (Davidson, Warmsley, Macy, & Weber, 2017a; Pamungkas, Basile, &
Patti, 2020). Issues such as subjectivity, freedom of expression, and evolving language
(e.g., slang or coded speech) further complicate consistent annotation and detection
(Waseem & Hovy, 2016). This phenomena has been extensively studied across a range
of computational frameworks, evolving from traditional machine learning approaches
to more recent deep learning architectures. Early studies relied on methods such as
Logistic Regression and Support Vector Machines (Chatzakou et al., 2017; Davidson,
Warmsley, Macy, & Weber, 2017b; Tahmasbi & Rastegari, 2018; Waseem & Hovy,
2016), often leveraging handcrafted linguistic features.

Subsequent advances in representation learning introduced deep neural networks,
including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks
(RNNs), which improved performance by automatically extracting semantic patterns
from text (Qian, Bethke, Liu, Belding, & Wang, 2019). After that, transformer-based
language models such as BERT (Devlin, Chang, Lee, & Toutanova, 2019; Grimminger
& Klinger, 2021) and RoBERTa (Glavas, Karan, & Vuli¢, 2020) have become the dom-
inant paradigm, showing promising results across multiple hate speech benchmarks.
Recent work has shown that LLMs now represent the state of the art in hate speech
detection, achieving strong results even in few-shot and zero-shot settings. These
models demonstrate the ability to generalise across domains and linguistic contexts
with minimal supervision, offering a promising alternative to traditional supervised
approaches (Plaza-del arco, Nozza, & Hovy, 2023; Roy, Harshvardhan, Mukherjee, &
Saha, 2023; Wang & Chang, 2022).

In parallel, there is growing interest in multimodal approaches that integrate tex-
tual, visual, and user-level metadata to better capture the multifaceted nature of
online hate (Perifanos & Goutsos, 2021; Yang, Zhu, Liu, Han, & Hu, 2022). These
methods are particularly relevant in real-world applications, where hate speech is often
embedded in complex social and multimedia contexts.


2.2 Datasets

The availability of annotated datasets is crucial for the development and evaluation
of hate speech detection systems. While early efforts were predominantly centered on
English (Piot et al., 2024; Poletto, Basile, Sanguinetti, Bosco, & Patti, 2020; Vidgen
& Derczynski, 2020), recent studies have expanded their focus to include a broader
range of languages and dialects. Notable contributions include datasets for Spanish
(Basile et al., 2019; Fersini, Nozza, & Rosso, 2018; Pereira-Kohatsu, Quijano-Sanchez,
Liberatore, & Camacho-Collados, 2019a), Portuguese (Fortuna, Rocha da Silva, Soler-
Company, Wanner, & Nunes, 2019), Italian (Sanguinetti et al., 2020; Sanguinetti,
Poletto, Bosco, Patti, & Stranisci, 2018), French and Arabic (Ousidhoum, Lin, Zhang,
Song, & Yeung, 2019), Turkish (Toraman, Sahinuc, & Yilmaz, 2022), and Slovene
(LjubeSi¢é, Fiser, & Erjavec, 2019).

Despite this progress, most available datasets still concentrate on high-resource
or globally dominant languages. As a result, European Portuguese, one of the 24
official languages of the European Union, and Galician, a co-official language in an
autonomous region (Galicia), continue to be significantly under-represented (Craith,
2006). This limits the applicability of current models in multilingual and cross-cultural
settings and underscores the need for targeted resource development in these contexts.

2.3 Synthetic data generation methods

Synthetic data generation has become an essential approach to address the lack of
annotated resources, especially for low-resource and underrepresented languages. Key
strategies include machine translation from high-resource languages, controlled text
generation using language models, and various data augmentation techniques.

Machine translation is a common method for generating parallel datasets, enabling
the projection of annotations from a well-resourced source language into the target
language (Pamungkas, Basile, & Patti, 2021). While this method supports large-scale
data creation, it may introduce stylistic biases or translation artifacts, especially when
source and target languages differ significantly in structure or cultural context.

Complementary approaches use rule-based or generative techniques to create hate
speech content from scratch. The CONAN dataset (Chung, Kuzmenko, Tekiroglu, &
Guerini, 2019) employs crowd-sourcing combined with adversarial examples to simu-
late realistic, context-aware hate speech. Meanwhile, the Dynamically Generated Hate
Speech dataset (Vidgen, Thrush, Waseem, & Kiela, 2021) uses controlled text gener-
ation to produce a wide variety of toxic language expressions by conditioning outputs
on hate categories and targets of the hate. These datasets enable systematic explo-
ration of hate speech across nuanced dimensions and support more robust evaluation
of detection systems.

More recently, instruction-tuned LLMs have been leveraged to produce synthetic
hate speech and non-hate content. These models can be prompted with predefined
templates, scenarios, or seed phrases to create synthetic training instances that reflect
specific linguistic or contextual features (Girén, Collell, Hassan, Huertas-Tato, &
Camacho, 2025; Jahan, Oussalah, Beddia, kabir Mim, & Arhab, 2024). While effective


in simulating realistic content, this approach raises ethical and methodological chal-
lenges, including the risk of reinforcing biases and the need for careful validation of
generated outputs.

3 Systematic dataset collection

This section describes the methodology used to identify and collect hate speech
datasets for European Spanish, European Portuguese, and Galician. The process
included four steps: (1) defining keywords, (2) searching digital libraries and reposito-
ries, (3) filtering resources using set criteria, and (4) extracting relevant datasets. The
dataset collection was conducted in December 2024, and included papers published
up to November 2024. The following subsections describe each step in detail.

3.1 Keywords

To identify relevant datasets, we selected keywords commonly associated with hate
speech detection. The term “hate speech” is related to concepts such as cyberbul-
lying, abusive language, offensive language, and specific types of hate speech (e.g.,
racism, sexism, misogyny). Based on this, we used the following keywords: hate
speech, hateful, abusive, offensive, toxic, aggressive, cyberbullying, sexism,
misogyny, and racism.

To narrow the search to our target languages, we combined these terms with “Span-
ish”, “Portuguese”, and “Galician”, along with “dataset” or “corpus”. We did not
specify language variants (e.g., European vs. Latin American Spanish) at this stage
to maximise recall. This filtering was done later.

3.2 Sources

To identify relevant datasets, we searched across multiple digital libraries and dataset
repositories. This covered both academic publications and curated repositories focused
on hate speech datasets, ensuring broad coverage of available resources.

For academic publications, we queried the following digital libraries:

e ACL Anthology: a widely used resource in natural language processing research,
particularly for computational linguistics and machine learning applications.

e ACM Digital Library: a repository containing peer-reviewed computer science
research, including works on dataset creation.

e Scopus: a large abstract and citation database that indexes publications from
various disciplines, including computer science and social sciences.

We used the keyword combinations described in Section 3.1 to query each library.
Table 1 lists the specific queries and the number of retrieved documents per source.
To refine the results, we applied additional filters: in Scopus, we limited the subject
areas to “Computer Science”, “Social Sciences”, and “Engineering”, and included only
document types classified as conference “papers”, “articles”, “book chapters”, or “data
papers”. In the ACM Digital Library, we restricted results to “research articles”.


In addition to academic sources, we examined specialised repositories focused on
hate speech datasets. These repositories included:

e Hate Speech Data!: a curated repository of publicly available datasets for hate
speech detection (Vidgen & Derczynski, 2020).

e Hate Speech Supersets: an aggregated resource that compiling multilingual hate
speech datasets (Tonneau et al., 2024).

Unlike academic digital libraries, these repositories are dedicated to datasets and
do not rely on keyword-based searches. Instead, we manually reviewed their catalogues
to identify datasets in our target languages. Table 2 shows the number of datasets
meeting our criteria in each repository.

Table 1 Queries run against the digital libraries and documents retrieved.

Source Query Docs.
ACL (title:hate* OR title:abuse* OR title:offens* OR 666
Anthol- title:toxic* OR title:aggress* OR title:cyberbully* OR

ogy title:sexism* OR title:misogyny* OR title:racism* OR

abstract:hate* OR abstract:abuse* OR abstract:offens* OR
abstract:toxic* OR abstract:aggress* OR abstract:cyberbully*
OR abstract:sexism* OR abstract:misogyny* OR
abstract:racism*) AND (title:dataset* OR title:corpus*

OR abstract:dataset* OR abstract:corpus*) AND (Spanish OR
Portuguese OR Galician)

ACM [[Al1: abstract.] AND [[All: "hate speech*"] OR [Al1: 308
Digital "abuse*"] OR [All: "offens*"] OR [All: "toxic*"] OR [A11:
Library "aggress*"] OR [All: "cyberbully*"] OR [All: "sexism*"]

OR [All: "misogyny*"] OR [All: “racism*"]] AND [A11:
abstract.] AND [[All: "dataset*"] OR [Al1l: "corpus*"]]

AND [All: abstract.] AND [[Al11: "spanish"] OR [A11:
"portuguese"] OR [All: "galician"]]] OR [[Al1: title.]

AND [[All: "hate speech*"] OR [All: "abuse*"] OR [A11:
"“offens*"] OR [All: "toxic*"] OR [Al1l: "aggress*"] OR [A11:
"cyberbully*"] OR [All: "sexism*"] OR [All: "misogyny*"] OR
(All: "racism*"]] AND [All: title.] AND [[Al1l1: "dataset*"]
OR [All: "corpus*"]] AND [All: title.] AND [[Al1: "spanish"]
OR [All: "portuguese"] OR [All: "galician"]]]

Scopus TITLE-ABS-KEY( "hate speech*" OR "hateful*" OR "abuse*" 272
OR "offens*" OR "toxic*" OR "aggress*" OR "cyberbully*" OR
"sexism*" OR "misogyny*" OR "racism*" ) AND TITLE-ABS-KEY(
"dataset*" OR "corpus*" ) AND TITLE-ABS-KEY( "Spanish" OR
"Portuguese" OR "Galician")

3.3 Criteria

After retrieving documents from the digital libraries and repositories, we manually
assessed each for inclusion in our dataset collection based on the following criteria:

1. The dataset must be new, introducing a resource not previously published.

‘https: //hatespeechdata.com


Table 2 Matching results in
repositories for our target languages.

Source Docs.
Hate Speech Data rd
Hate Speech Supersets 9

2. It must contain text from social networks, excluding images, audio, or content

from non-social media sources such as books or song lyrics.

It must be annotated by humans to ensure labelling is reliable.

4. It’s definition of hate speech must align with ours or cover related categories
such as racism or sexism. We define hate speech as “language characterised by
offensive, derogatory, humiliating, or insulting discourse (Founta et al., 2018)
that promotes violence, discrimination, or hostility towards individuals or groups
(Davidson et al., 2017a) based on attributes such as race, religion, ethnicity, or gen-
der (Das et al., 2023; ElSherief, Kulkarni, Nguyen, Yang Wang, & Belding, 2018;
ElSherief, Nilizadeh, Nguyen, Vigna, & Belding, 2018)”, which aligns closely with
the United Nations’ definition (Nations, 2023).

5. It must target European Spanish, European Portuguese, or Galician, to
ensure linguistic relevance.

2

Documents not meeting these criteria were excluded. This process ensured the
relevance and consistency of the dataset collection with our research objectives.

The filtering process was conducted in stages. First, we excluded papers that did
not introduce a new dataset or that included languages outside our target set. We also
removed datasets not derived from social media platforms. After this initial filtering,
28 Spanish datasets, 13 Portuguese datasets, and 1 Galician dataset remained.

Next, we performed a detailed review of each dataset, filtering out those that did
not meet the following criteria: (1) human annotation, (2) alignment with our hate
speech definition, and (3) use of the European variants of Spanish and Portuguese.
The final number of datasets per language is presented in Table 3.

Following dataset identification, we attempted to access the data. As shown in
Table 3, we successfully obtained 10 datasets for European Spanish, either through
publicly availability, email requests or authorisation. However, no datasets for Euro-
pean Portuguese or Galician were accessible. Some datasets lacked access links, and
attempts to contact the authors were unsuccessful. Consequently, for this study, we
focus on the datasets available at the time of analysis.

These findings address RQ1: To what extent do existing hate speech datasets reflect
the linguistic characteristics of European Spanish, European Portuguese, and Gali-
cian? While multiple datasets were identified and accessed for European Spanish,
none were accessible for European Portuguese or Galician. This indicates that among
the three languages, only European Spanish is represented to a meaningful extent,
though, still considerably less than more widely resourced languages such as English.


Table 3 Matching results in repositories for our target languages.

Language Fitting Datasets Available Datasets
European Spanish 21 10
European Portuguese 3 0
Galician 1 0

4 Corpus creation

This section describes the construction of the meta-collection corpus (MetaHateES)
and the subsequent generation of synthetic data for European Portuguese and Galician
based on this source.

4.1 Meta-collection construction

As mentioned, our systematic dataset collection resulted in ten available Euro-
pean Spanish datasets. These datasets, listed below, form the basis of the new
meta-collection, MetaHateES.

4.1.1 DETESTS 2024 (Schmeisser-Nieto et al., 2024)

The DETESTS dataset was constructed to detect and classify racial stereotypes in
Spanish, incorporating annotations that capture both explicit and implicit stereo-
types. It consists of two text sources: comments on news articles (from DETESTS
and NewsCom-TOX) and tweets reacting to racial hoaxes (from StereoHoax-ES). The
dataset is based on online platforms, including Spanish news outlets (ABC, elDiario.es,
El Mundo, NIUS, Menéame) and Twitter (now X), focusing on discussions related to
immigration. DETESTS primarily targets racial stereotypes, often linked to xenopho-
bia and misinformation. The dataset comprises 12 111 texts (6762 from DETESTS and
5349 from StereoHoax-ES). Authors define stereotype as “cognitive mechanism con-
sisting of a set of beliefs regarding another social group, namely the outgroup, which is
perceived as different. Forming these beliefs involves a homogenisation process, where
one characteristic of an individual or part of the group is generalised to the entire
group. This generalisation typically occurs based on factors such as place of origin,
ethnicity, or religion. Stereotypes can be expressed explicitly or implicitly”.

4.1.2 EXIST 2021 & 2023 (Rodriguez-Sanchez et al., 2022)

The EXIST dataset was constructed to detect and categorise sexism in social media,
focusing on Twitter and Gab. It includes posts labelled for binary sexism identification
and multi-class categorisation into direct, judgmentally, objectifying, ideological, or
violent sexism. Data from 2021 includes around 5700 Twitter and Gab posts, while the
2023 dataset consists of 4209 Twitter posts with annotations for sexism presence (Task
1: “YES”) and type (Task 2: “DIRECT” or “JUDGMENTAL”). The 2024 dataset is
identical to 2023. Authors based their work on sexism defined as in the Oxford English
Dictionary as “prejudice, stereotyping or discrimination, typically against women, on
the basis of sex”.


4.1.3 HaSCoSVa 2023 (Castillo-lépez et al., 2023)

The HaSCoSVa dataset is a hate speech dataset comprising different variants of Span-
ish. Their targets are immigrants and the data is from Twitter, consisting of 4000
instances. It contains labels in a binary fashion and it also contains its variation (latam
or europe), where 2500 are from Spanish European. In their paper, the report the dif-
ferences between Latin American Spanish and European Spanish, and how different
models identify it differently. Authors do not provide a formal definition of what con-
stitutes hate speech, but based on their work and data, their study includes misogyny
and xenophobia, two hate speech constructs, that they define as “harm against women
due to gender, which might result in psychological, reputational, professional, or even
physical damage” and “attitudes, prejudices, and behaviour that reject, exclude and
often vilify persons, based on the perception that they are outsiders or foreigners to
the community, society or national identity”, respectively.

4.1.4 SemEval HatEval 2019 (Basile et al., 2019)

The SemEval HatEval dataset is about detecting hate speech against immigrants and
women in both Spanish and English. It is part of SemEval 2019 Task 5, and they define
one task for binary hate speech detection an another for identifying other features
in hateful content. The dataset contains is from Twitter and contains 19600 tweets,
of which 6600 are in Spanish. They use Castilian Spanish annotators, in order to
have native speakers for the content. Authors follow the hate speech definition “any
communication that disparages a person or a group on the basis of some characteristic
such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other
characteristics”.

4.1.5 Hate Football 2023 (Montesinos-Cadnovas, Garcia-Sdnchez,
Garcia-Diaz, Alcaraz-Marmol, & Valencia-Garcia-Sanchez,
2023)

This corpus is from comments from Twitter around the topic of Spanish men’s football
league. The dataset contains 7483 tweets and is labelled on aggressive, misogyny,
racist, and safe. We considered data points labelled as misogyny and racist as hate
speech, as their definition of aggressiveness does not fall under the umbrella of our
definition of hate speech, but misogynist and racist does. Authors follow Cambridge
Dictionary’s definition of hate speech: “public speech that expresses hate or encourages
violence towards a person or group based on something such as race, religion, sex, or
sexual orientation”.

4.1.6 HaterNet 2019 (Pereira-Kohatsu, Quijano-Sanchez,
Liberatore, & Camacho-Collados, 2019b)

HaterNet consists on a dataset of 6000 instances for hate speech detection in Spanish.
It is labelled in a binary fashion. The dataset includes a diverse range of hate-related
content, reflecting real-world discourse on social media. Authors define hate speech as

10


“a kind of speech that denigrates a person or multiple persons based on their member-
ship to a group, usually defined by race, ethnicity, sexual orientation, gender identity,
disability, religion, political affiliation, or views”.

4.1.7 MeTwo 2020 (Rodriguez-Sanchez, Carrillo-de-Albornoz, &
Plaza, 2020)

The MeTwo dataset consists of sexist expressions and attitudes, including subtle forms,
in Twitter in Spanish. It is focused on misogynistic and xenophobic content, and
was curated using an specialised lexicon. The expressions used for the creation are
in European Spanish. Authors use Davidson et al. (2017a) definition of hate speech
“language that is used to expresses hatred towards a targeted group or is intended to
be derogatory, to humiliate, or to insult the members of the group”.

4.1.8 MisoCorpus 2021 (Garcia-Diaz, Canovas-Garcia,
Colomo-Palacios, & Valencia-Garcia, 2021)

The MisoCorpus is a dataset for detecting misogynous messages on Twitter. It is clas-
sified in violence towards women, messages harassing women and general traits related
to misogyny. The dataset is focused on both Latin American Spanish and European
Spanish. We filtered the data to keep only the post published in Spain, leaving us
with 1527 data instances. Authors use Manne (2017) definition of misogyny: “social
environments in which women will tend to face hostility of various kinds because they
are women in a man’s world who are held to be failing to live up to men’s standards”.

4.1.9 NewsCom-TOX 2024 (Taulé, Nofre, Bargiela, & Bonet, 2024)

The NewsCom-TOX dataset contains 4359 comments in Spanish posted on responses
to news articles related to immigration. Their objective was to identify messages with
racial and linguistic categories, specially toxicity and its intensity. Although toxicity
may be considered different as hate speech, the authors define it as “comment that
attacks, hurts, threatens, insults, offends, denigrates or disqualifies a person or group
of people on the basis of characteristics such as race, ethnicity, nationality, political
ideology, religion, gender and sexual orientation, among others, regardless of whether
the writer intends to be hurtful or offensive”, which matches our definition of hate
speech.

4.1.10 OffendES 2021 (Plaza-del Arco, Montejo-Raez,
Urena-Lépez, & Martin- Valdivia, 2021)

The OffendES corpus comprises 47128 Spanish-language comments sourced from Twit-
ter, Instagram, and YouTube, focusing on interactions involving young influencers.
Each comment is manually annotated across multiple predefined offensive categories,
with a subset including confidence scores for each label. This design supports both
multi-class classification and multi-output regression analyses. Authors follow the
offensive definition of “the tert which uses hurtful, derogatory, or obscene terms made
by one person to another person”. They claim that related terms include hate speech,

11


toxic language, aggressive language and abusive language and that, although there are
subtle differences in meaning, they are all compatible with the above general definition.

4.2 Standardise data

This section describes the procedures used to standardise the datasets for the meta-
collection. To unify the diverse annotations, we applied mappings and transformations
to consolidate all labels into a single consistent format.

Each dataset was loaded individually, retaining only columns relevant to the meta-
collection. Rows with missing values and duplicates were removed to ensure data
quality.

To align labelling, we applied mapping strategies: labels such as “No”, “False”,
and “NO” (non-hate content) were standardised to 0; labels like “True”, “Offensive” ,
“Sexist”, “Aggressive”, and shorthand forms such as “OFG” or “OFP” were mapped
to 1, indicating hate speech. When a gold label was absent, functions inferred the final
label by majority voting across annotations.

A new column, content_type, categorises content types from the original datasets,
including offensive, hate speech, sexism, stereotype, aggressive, toxicity, and misog-
yny. These categories represent subconstructs within our hate speech definition. Each
dataset was assigned a category based on its definitions or label naming. This classi-
fication provides insight into the types of hate speech represented and improves the
granularity of future analysis. Additionally, each entry was tagged with its source,
indicating the data origin, such as Twitter, YouTube, news outlets, Instagram, and Gab.

4.3 Dataset statistics

The meta-collection contains 82 233 samples, with 24.85% labelled as hate speech and
75.15% as non-hate. The data come from five sources and ten distinct datasets.
Table 4 presents the number of hate speech samples, hate rate (%), and total
samples for each source. Twitter is the largest source, contributing 43254 samples
and with a hate rate of 29.70%. YouTube contributes 22 538 samples with a hate rate
of 16.94%. News sources account for 9983 samples at 27.50%. Instagram and Gab
contribute 5968 and 490 samples, with hate rates of 12.72% and 54.08%, respectively.

Table 4 Distribution of hate speech samples, hate rates, and total samples
across different platforms.

Source Hate Samples Hate Rate (%) Total Samples
Gab 265 54.08% 490
Twitter 12 848 29.70% 43 254
News 2745 27.50% 9983
YouTube 3819 16.94% 22 538
Instagram 759 12.72% 5968

12


Table 5 lists the number of samples per dataset. OffendES 2021 is the largest, with
30416 samples, followed by DETESTS 2024 (9906) and Hate Football 2023 (7482).
The remaining datasets contribute between 1527 and 6600 samples each.

Table 5 Number of samples included
from each dataset in the
meta-collection.

Dataset Samples
DETESTS 2024 9906
EXIST 2021 & 2023 9910
HasCoSVa 2023 2500
Hate Football 2023 7A82
HaterNet 2019 5938
MeTwo 2020 3600
MisoCorpus 2021 1527
NewsCom-TOX 2024 4354
OffendES 2021 30 416

SemEval HateEval 2019 6600

Table 6 presents the number of hate speech samples, hate rate (%), and total
samples by content type. The “offensive” category contains the most samples (30 416),
with a hate rate of 15.91%. “hate speech” and “sexism” categories have 15038 and
13510 samples, with hate rates of 30.68% and 42.31%, respectively. Other categories
include “stereotype” (9906 samples, 26.30%), “aggressive” (7482, 9.26%), “toxicity”
(4354, 31.83%), and “misogyny” (1527, 38.11%).

Table 6 Distribution of hate speech samples and hate rates by annotated content
type.

Content Type Hate Samples Hate Rate (%) Total Samples

Offensive 4841 15.91% 30416
Hate Speech 4612 30.68% 15038
Sexism 5717 42.31% 13510
Sterotype 2606 26.30% 9906
Aggresive 693 9.26% 7482
Toxicity 1386 31.83% 4354
Misogyny 582 38.11% 1527

13


4.4 New data creation

This section outlines the process of generating new data in European Portuguese and
Galician based on the European Spanish meta-collection (MetaHateES).

4.4.1 Portuguese

To create the Portuguese version (MetaHatePT), we focused on producing European
Portuguese translations. Since most commercial translation tools default to Brazil-
ian Portuguese, we avoided them due to license access limitation. Instead, we used
the utter-project/EuroLLM-9B-Instruct model (Martins et al., 2024), from the
EuroLLM project, which supports all European Union languages, including European
Portuguese.

We employed the easy-translate framework (Garcfa-Ferrero, Agerri, & Rigau,
2022) to interact with the model, with the prompt: “Translate Spanish to Portuguese
(pt_PT): %%7SENTENCE%%.” This ensured translations that aligned with European
Portuguese linguistic conventions. The resulting dataset, MetaHatePT, retains the same
structure and annotation as the original European Spanish version.

4.4.2 Galician

To create the Galician variant, we produced two parallel versions from different source
languages. The initial version, MetaHateGL_es, was generated by translating the Span-
ish meta-collection (MetaHateES) into Galician using Google Translate. Although
other online platforms for automatic translation into Galician, such as Opentrad
(Loinaz et al., 2006) or Proxecto Nés (de Dios-Flores et al., 2022), are available, we
chose Google Translate to maintain consistency for comparisons with future research
involving other languages. This approach enabled efficient translation of large vol-
umes but introduced limitations, such as overly literal translations influenced Spanish
grammar and vocabulary patterns. Due to the close linguistic relationship between
European Spanish and Galician, translations are often grammatically correct but
stylistically marked by European Spanish. For our purposes, this limitation is an
advantage, as it produces a Galician variant closely aligned with the European Spanish
source, which aligns with our interest in capturing this specific linguistic influence.

The second version, MetaHateGL_pt, was created by translating the Portuguese
dataset (MetaHatePT) into Galician, also using Google Translate. Because the source
was Portuguese, these Galician translations show stronger lexical and syntactic simi-
larities to Portuguese. This reflects a more reintegrationist variant of Galician, used in
some cultural contexts, differing from the standard Galician common in most digital
tools.

By producing both MetaHateGL_es and MetaHateGL_pt, we capture internal vari-
ation within Galician and enable the analysis of how the translation source (Spanish
vs. Portuguese) affects the style and linguistic features of Galician hate speech data.

The data will be made available for research purposes upon request. Since sev-
eral resources in our meta-collection are subject to individual licensing agreements,
researchers will need to secure these permissions to access the full dataset. We will
provide guidance to help facilitate the process of obtaining all necessary agreements.

14


5 Dataset analysis

Now, we present an analysis of the newly developed multilingual resources. We used
BERTopic with t-SNE for topic modelling and visualisation. For linguistic analysis, we
used spaCy with language-specific models for Spanish and Portuguese. To assess emo-
tional content, we used the NRC Emotion Lexicon, based on Plutchik’s emotion model,
originally in English and translated to the target languages using Google Translate

5.1 Lexical analysis

We started with a basic term frequency analysis in each language. After removing stop
words, we extracted the top 20 words. In Spanish, key terms include “mujer” (woman),
“puta” (whore), and “inmigrante” (immigrant). Similar patterns appeared in the
translated datasets: Portuguese included “mulher” (woman), “puta” (whore), and
“merda” (shit); Galician (ES) featured “muller” (woman), “puta” (whore), and “inmi-
grante” (immigrant); Galician (PT) showed “muller” (woman), “cadela” (female dog),
“puta” (whore), and “pais” (country). The independently sourced English dataset
showed different but related terms, including “fuck”, “bitch”, “fagot”, and “women”.
These results indicate a consistent use of gendered and offensive language across
languages, especially in datasets derived from the Spanish metacollection.

In our second analysis, we explored our hate speech datasets employing topic
modelling techniques. We used the BERTopic framework (Grootendorst, 2022) with
Sentence Transformers to generate sentence embeddings. BERTopic then identified
underlying topics in the corpus. We trained the model on each language version to cap-
ture key themes in hate speech. Each dataset revealed six major topics. We manually
annotated and labelled the topics, enabling a clearer interpretation of the predominant
discourses and forms of expression found in the data. We visualised the top 10 terms
in each topic using word clouds (Figure 1), and we applied T-distributed Stochastic
Neighbour Embedding (t-SNE) (van der Maaten & Hinton, 2008) to show topic distri-
bution and we visually represent the relationships and proximity of documents within
each topic.

For European Spanish, the original metacollection used to create the other language
versions, the main topics were: (1) Feminism and Gender Equality, (2) Immigration
and Politics, (3) Racism and Racial Identity, (4) Gender and Emotion, (5) Sports
Insults, and (6) Misogynistic Slurs. These topics show the range of online speech,
including hostile content and social issues.

The Portuguese dataset, translated from Spanish, showed similar categories: (1)
Feminism and Gender Equality, (2) Personal Insults, (3) Racism and Ethnicity, (4)
Insults and Sports References, (5) Sexual Harassment, and (6) Mixed Insults. Some
shifts in topic boundaries were noted, likely due to translation and cultural and
linguistic differences between the two languages.

For Galician, translated from both Spanish (ES) and Portuguese (PT), we observed
that the Galician (ES) topics include (1) Feminism and Gender, (2) Hate Speech and
Hostility, (3) Insults and Offensive Language, (4) Immigration and Social Issues, (5)
Racism and Discrimination, (6) Personal Attacks. For the Portuguese variant, Galician
(PT), we observe topics like (1) Feminism and Gender Advocacy, (2) Sexism and

15


Gender Stereotypes, (3) Gender and Cars, (4) Misogyny and Gender Roles, (5) Racism
and Discrimination, and (6) Patriarchy and Masculinity. Comparing both, the Galician
(PT) topics were more narrowly focused on gender, while Galician (ES) maintained
broader coverage. The lower variety of topics in the Galician (PT) data may be due to
two main factors: the intrinsic characteristics of Galician as a lower-resource language
(i.e. limitations in translation tools and natural language processing tools) and the
effects of machine translation. These issues could lead to fewer and more repetitive
topics, even though the dataset size is the same. Still, the frequent appearance of
hate-related and gender-focused terms suggests that the main ideas from the original
European Spanish data are preserved, especially around discrimination and online
aggression.

The English dataset, developed independently, provided a comparison point. Its
topics were: (1) Anti-Black Racism, (2) Hate and Extremism Rhetoric, (3) Misogyny
and Homophobia, (4) White Supremacy and White Genocide Narrative, (5) Islam-
ophobia and Anti-Muslim Sentiment, and (6) Slurs and Aggression. These topics
include a mix of platform-specific behaviour (e.g., Wikipedia moderation), direct hate
speech (e.g., racism, Islamophobia, misogyny), and broader social commentary (e.g.,
politics, pop culture). The English dataset shows that online hate can take different
forms depending on cultural and social context. The inclusion of this sample helps
us see how toxic language appears naturally in English, revealing both shared and
language-specific patterns of online hostility.

Together, these topic clusters offer a way to explore hate speech across multiple
languages. Since our dataset includes both translated and original texts, it sup-
ports cross-lingual comparisons and highlights how language, culture, and translation
influence the way harmful speech is expressed and understood online.

Next, we represent these topics using t-SNE. t-SNE is a dimensionality reduction
algorithm, to visualise topic distributions. Figure 2 shows how different types of hate
speech cluster based on their semantic similarity. Each cluster represents a distinct
topic, and the closer two clusters appear, the more similar their content. To improve
the visualisation, we applied topic weights so that more prominent topics are more
clearly represented. In the Spanish data, topics (1) and (4), related to feminism, gen-
der, and emotional expression, appear close together on the right. Topics on racism
and immigration, such as (2) and (3), are grouped near the bottom, reflecting their
semantic overlap. On the left, clusters related to insults and slurs are also grouped
together. Similar patterns are seen in the Portuguese and Galician datasets: gender-
related topics are grouped, and topics on racism or immigration appear close together.
While the clusters are distinguishable, their layout shows clear connections between
related themes—such as immigration and racism, or gender and misogyny, rather than
being randomly spread. This t-SNE visualisation helps reveal the underlying semantic
structure of the hate speech topics

Based on the topic analysis carried out with BERTopic, there is a clear concen-
tration of hate speech around topics related to sexism and misogyny. Although the
dataset includes mentions of other issues such as immigration or racism, these appear
with less frequency and intensity, suggesting a potential bias in the distribution of
content toward gender-related matters. This finding highlights the need to expand the

16


Spanish

Immigration & Racism Ee patie
Politics 3 Ider
negro cf imero

idee dejar

Feminism & Gender
Equality

Portuguese

Feminism & Gender Insults & Personal
Equality Offenses 3) patience eenniany
quero_ dedxar

tratar

sere : ratista
sta L q LO l a NEBlO s deolog ia
as G i Smo Di branco .quiero
pais “#3¢ar aC1SMoOpai
4c SO 6 eeccen 4 insults & Sports 5 Marakcment © ixed insults
‘ jajajajajajaje 1
hol vida, lechuga Snedveral zorra: , cristobalsoria Teves | foolish lado
Salliater gorri arovira sexualidade
hom Bie PE aitmthte callatezorza Moyea ave, SBXCQ] Idiota
i carrer setae subnormal sexua MBs oe sate
Sie subnormal ache rucula  ruir trabalho_sexual | & avi
ng hase dand vos_so brycealvaroojeda aSsedio_: Sexual asia? j rose
Galician (ES) Galician (PT)
Feminism & Gender Hate Speech & 3 Insults & Offensive Feminism & Gender Sexism & Gender 3
Equality Hostility Language Advocac Stereotypes Cars & Gender
subnormal queira tratar
puto
jpedrerol : F
( alfredoduro U 1 were home
mu ] lar SUBNORTL 4: a ) coma muller
LC! leitugal . Ol t
profundo subhovinats Ce dua HAC QT
A ees 5 picerimination ‘faliamere 7 4 Meco OB parimination 6 Treseutinty’
comunista _ primeironegro a merez deben i votx _ Negro patriarcado
regularizar pis - ae isacarrio TNOXE hatte neg kms robe home eT or fo
ant eengcen CF Sta cals unha imi, tee ra Cc 1 Ss t a home
matir 4 seguridade he _Fapaz a ante mu eC r brangg ati mu ame
cabezalOZO neve tratar, querer A pe er cocinar pais gar Kit i mulleres
inmigrante facismo M filte deben_votar “racismo ‘JQMEsmr.

Fig. 1 Word clouds of topics for Spanish, Portuguese, Galician (ES) and Galician (PT).

resources and corpora used for studying hate speech to more evenly represent other
relevant dimensions, such as xenophobia, LGBTQ+phobia, aporophobia, or political
hate (Curto, Kiritchenko, Siddiqui, Nejadgholi, & Fraser, 2025). Focusing solely on
sexism as the main axis can lead to a partial view of the phenomenon, affecting both
the automatic detection and the sociolinguistic understanding of online hate.

5.2 Psycholinguistic analysis

For our linguistic analysis, we conducted an analysis of emotions in our data, as well
as the usage of pronouns and verb tenses.

To analyse emotion, we employed the Plutchik set of emotions (Plutchik, 1980),
encompassing eight primary emotions (anger, fear, sadness, disgust, surprise, antici-
pation, trust, and joy) and two basic sentiments (positive and negative). To quantify
emotion levels, we utilised the NRC emotion lexicon (Mohammad & Turney, 2013),
which comprises words linked to the Plutchik emotions. We computed the percentage
of posts within each group (hate, no hate) that contained at least one word associ-
ated with the primary emotions and sentiments. We quantified the emotion levels for
our four languages. As expected, the four datasets follow a similar pattern and dis-
tribution of emotions. Figure 3, shows that negative emotions are more frequent in
hate speech posts, while positive emotions are more common in non-hateful content.
Emotions such as fear, disgust, anger, and sadness appear more frequent in hate
messages. On the other hand, emotions like trust are associated with non-hate posts.
Negative emotions are the most common in our dataset, likely due to the way hate

17


Spanish Portuguese

Fig. 2 t-SNE visualisation of topics for Spanish, Portuguese, Galician (ES) and Galician (PT).

speech data is collected—often using keywords and lexicons focused on negative terms.
Emotions like disgust and anger show a strong contrast between hate and non-hate
posts, appearing much more frequently in hate speech. The main difference across lan-
guages is seen in anticipation for Portuguese, where both hate and non-hate posts
show notably lower levels compared to the other languages. This could be most likely
because we are using European Portuguese, and NRC emotion lexicon in Portuguese
was originally translated from English using Google Translate (Mohammad & Turney,
2013), which might be biased towards Brazilian, limiting the lexicon for European
Portuguese and limiting the discovering of this emotion. We had no control over this
lexicon, as it is an existing resource, but based on our findings, future work should
improve NRC emotion lexicon to reflect the languages variants. We highlight again,
the importance to build resources that capture the variations among languages and
dialects.

18


Spanish Portuguese
fear disgust fear disgust

am Hate
No hate

@@8 Hate
No hate

anger

sadness surprise sadness surprise
Galician (ES) Galician (PT)
fear disgust fear disgust

am Hate
No hate

@m Hate
No hate

negativ| anger negativ

sadness surprise sadness surprise

Fig. 3 Radar plot showing the percentage of posts that contain a word associated with the Plutchik
emotions for hate and non-hate data, for Spanish, Portuguese, Galician (ES) and Galician (PT).

Regarding pronouns and verb tenses usage, we used spaCy with the Spanish
(es_core_news_sm) and Portuguese (pt_-core_news_sm) language models. Figure 4
shows that, for all languages, there is a higher prevalence of present verb tenses in
hate speech posts. Future tenses appeared rarely (~3%), and past tenses less often
(~20%). This suggests that the higher prevalence of present tenses in hate speech
posts are linked to direct attacks.

Regarding the usage of pronouns, for both Spanish and Portuguese datasets, non-
hate speech post used more first-person singular pronouns, while hate speech posts
used more second-person singular and third-person plural pronouns. Both “you” and
“they” pronouns often indicate personal attacks and othering (Rohleder, 2014). Oth-
ering is a sociological concept where a group in labelled as fundamentally different,
alien, or inferior to a dominant in-group, often involving pronouns like “they” to cre-
ate a psychological and social divide. In Galician, we observed similar patterns, with a

19


Spanish Portuguese

150 1
mmm Hate @mm Hate a0 mmm Hate mmm Hate
No Hate No Hate No Hate a No Hate
”

Past Present Future Yo Nos. Tu Vos. El, EllaEllos Past Present Future Eu Nés Tu Vos Ele Eles

Galician (ES) Galician (PT)
1 1
S mmm Hate mmm Hate a Mm Hate lm Hate
7 No Hate » No Hate No Hate = No Hate
0

at 20 60. 20
= 30
z as as
om 0
2 Es 30 10
= 20

Past Present Future Eu Nos Ti Vos El Eles * past Present Future Eu Nés Ti Vos El Eles

Fig. 4 For each language, Spanish, Portuguese, Galician (ES) and Galician (PT), distribution of
verb tenses in hate and non-hate posts (left). And distribution of pronouns in hate and non-hate
speech posts (right).

higher prevalence of first person singular pronouns in non-hate speech posts, and the
third person plural pronoun being more frequent in hate speech posts. However, the
second person singular pronoun showed less variation. Since no spaCy model exists
for Galician, we used the Portuguese model, which may affect results.

These findings answer RQ2: Can synthetic data generation effectively compensate
for the lack of annotated hate speech resources in European Portuguese and Galician?.
We show the effectiveness of synthetic data generation for low-resource languages
like European Portuguese and Galician. Key hate speech themes and psycholinguis-
tic patterns, such as negative emotions, present-tense usage, and othering pronouns,
were preserved across translations. While Galician (PT) showed reduced topic vari-
ety, likely due to translation and tool limitations, the overall consistency suggests
that synthetic datasets offer a practical solution for extending hate speech analysis to
underrepresented Iberian languages.

6 Experiments and results

In this section, we present our efforts to establish a common baseline for hate speech
detection on social media for European Spanish, European Portuguese and Gali-
cian. To do so, we evaluate the proposed collections by benchmarking performance
across several experimental settings using two large language models (Llama 3.1
and Nemo) and a multilingual BERT model (bert-base-multilingual-cased). We
conduct binary classification experiments under zero-shot, few-shot, and fine-tuning
configurations in order to establish baseline results for future comparisons.

6.1 Models

We use two LLMs and one encoder-based model in our evaluation:

20


e Llama 3.1 8B (Dubey et al., 2024): Llama 3.1 8B is a pre-trained model, out-
performing competing models such as Mistral 7B or Gemma 7B in tasks such as
commonsense understanding, mathematical reasoning tasks and general tasks.

e Nemo (Mistral AI team, 2025): Nemo is an instruction-tuned model excelling in
multilingual tasks and zero-shot scenarios, outperforming models like Mistral 7B in
comprehension and domain-specific applications.

e Multilingual BERT (bert-base-multilingual-cased) (Devlin et al., 2019):
BERT multilingual base model is a encoder-only model, which has been pre-trained
on 104 languages with Wikipedia data using masked language modelling.

6.2 Settings
We perform experiments under the following configurations:

© Zero-shot: The model is prompted with only the task description and input text,
with no examples provided. Text generation is performed using the model’s decoder
with the following configuration: max_new_tokens=256, temperature=0, top_p=0.1,
and top_k=5, ensuring low-variance, high-confidence completions.

e Few-shot: The model is given a small number of in-context examples (8 labelled
instances) along with the task prompt. Text generation is performed using the same
configuration as for zero-shot.

e Fine-tuning: The LLMs were fine-tuned on the training portion of each dataset
and evaluated on held-out data. We fine-tuned for 3 epochs with a learning rate
of 2 x e~°, and batch size of 16. The AdamW optimiser and linear learning rate
scheduler with warm-up were applied. Evaluation metrics include micro and macro-
averaged Fl1-score. The BERT model was fine-tuned for 3 epochs with a learning
rate of 5 x e~°, and batch size of 32. We set a weight decay of 0.18 and AdamW
8bit optimiser.

All experiments are conducted across five language variants: Spanish, Portuguese,
Galician (ES) (Galician translated from Spanish), Galician (PT) (Galician trans-
lated from Portuguese), and English (an external dataset for cross-lingual validation).
Every dataset was divided intro training and test sets, with a test size of 20%. This
multilingual evaluation allows us to assess model performance across closely related
languages and translation directions, highlighting potential challenges in cross-lingual
hate speech detection.

6.3 Cross-lingual evaluation

To further examine the generalisability of our models, we conduct cross-lingual evalua-
tions using the fine-tuned models. Specifically, we evaluate how well a model fine-tuned
on one language performs when applied directly to test sets in other languages, with-
out additional adaptation. This setup enables zero-shot cross-lingual transfer learning
through fine-tuned models.

For each language, we fine-tune BERT, Llama 3.1 and Nemo on its training split
and test the resulting model on the other language test sets. This results in a cross-
lingual evaluation matrix, where each cell corresponds to training in language L,

21


and testing in language L;. By evaluating performance across Spanish, Portuguese,
Galician (ES), Galician (PT), and English, we assess cross-lingual transferability, with
a focus on typologically related language pairs.

This analysis reveals the extent to which linguistic similarity and translation direc-
tion affect model performance in hate speech detection. It also provides insights
into the robustness of our models and the utility of synthetic or translated data in
low-resource settings.

6.4 Results

Table 7 presents the performance of two multilingual models, Llama 3.1 and Nemo,
across five language datasets (English, Spanish, Portuguese, and the two Galician
variants) under three evaluation settings: zero-shot, few-shot, and fine-tuning. As
expected, performance improves consistently across all languages when moving from
zero-shot to fine-tuning, highlighting the importance of task-specific adaptation.

English yields the highest performance in all settings, which aligns with the
availability of abundant training resources and benchmarks in this language. Span-
ish follows, benefiting from the large-scale dataset we constructed (+80k examples),
enabling strong fine-tuned results (F1 micro > 0.85). Portuguese and Galician show
lower performance, particularly in the zero-shot and few-shot settings, which reflects
the relative scarcity of data in LLMs in these variants. Notably, European Portuguese
and Galician (both ES and PT variants) perform worse than Spanish, underscoring
the challenges of low-resource settings.

The results emphasise the need for larger, more diverse multilingual hate speech
datasets. While LLMs show encouraging transfer capabilities, fine-tuned models still
significantly outperform zero-shot and few-shot settings, especially in underrepre-
sented languages. Moreover, the variation between Galician (ES) and Galician (PT),
despite sharing a common source, suggests that linguistic nuance and regional usage
play a critical role in model generalisation. These findings reinforce the importance
of both language-specific data curation and multilingual model training that captures
dialectal diversity.

Table 8 presents the results of the cross-lingual evaluation, including our target
languages and English. BERT performs well when tested on the same language it was
trained on, consistently achieving the highest F1 scores in those settings (e.g., 0.82
micro-F1 for Spanish, 0.82 for Portuguese, and 0.80 for Galician-ES and Galician-
PT). However, performance drops notably when transferring to other languages. For
example, BERT trained on Spanish sees a decrease when applied to Galician (ES:
0.7819, PT: 0.7798), and a larger drop when tested on English (micro-F1: 0.7270).
Spanish performs slightly better when transferring to Galician (ES) than to Galician
(PT), suggesting closer alignment in syntax or vocabulary between Spanish and the
Galician variant translated from Spanish. Similarly, Llama 3.1 and Nemo show strong
performance in same-language settings for Spanish, achieving 0.83 (Llama) and 0.85
(Nemo) micro-F1 on Spanish test data. These models also generalise relatively well
to related languages. For instance, Spanish-trained Llama achieves 0.7856 on Galician
(ES) and 0.7816 on Galician (PT), while Nemo reaches 0.7968 and 0.7982 respectively.
Although the performance gap between Galician (ES) and Galician (PT) is small,

22


Llama 3.1 Nemo BERT

Language Setting F1 Micro F1 Macro F1 Micro’  F1 Macro F1 Micro F1 Macro
English zero-shot 0.7916 0.7643 0.7904 0.7647 =

few-shot 0.8044 0.7464 0.8092 0.7531 —

fine-tuned 0.8614 0.8240 0.8652 0.8334 0.9229 0.9048
Spanish zero-shot 0.7363 0.6854 0.7170 0.6848 —

few-shot 0.7142 0.6780 0.7806 0.7236 =

fine-tuned 0.8309 0.7575 0.8535 0.7981 0.8211 0.7583
Portuguese zero-shot 0.7339 0.6794 0.7380 0.6918 =

few-shot 0.7178 0.6757 0.7697 0.7144 7

fine-tuned 0.8113 0.7056 0.8262 0.7481 0.8174 0.7501
Galician (ES) zero-shot 0.7266 0.650 0.7108 0.6644 -

few-shot 0.6868 0.647 0.7469 0.6889 =

fine-tuned 0.8081 0.7168 0.8132 0.7351 0.8003 0.7274
Galician (PT) —zero-shot 0.7311 0.6482 0.7222 0.6722 =

few-shot 0.7055 0.6602 0.7570 0.6954 -

fine-tuned 0.7803 0.5926 0.8131 0.7176 0.8047 0.7331

Table 7 Results.

Galician (ES) tends to show slightly higher macro-F1 scores, particularly for Llama
(0.6148 vs. 0.5995), indicating marginally better alignment with the Spanish-trained
models. Overall, models trained on Spanish tend to transfer more effectively to Gali-
cian (ES) than Galician (PT), reflecting the influence of the translation direction in the
Galician variants. This pattern is observed consistently across all three architectures.

Across all settings, BERT achieves its best performance when the training and
testing languages match, regardless of the language, indicating its strong in-language
specialisation. In contrast, Llama 3.1 and Nemo show a different pattern. When trained
on Portuguese or either Galician variant, these models achieve their highest perfor-
mance on the Spanish test set, rather than on their respective training languages.
This suggests that Spanish serves as a strong target for cross-lingual generalisation in
LLM-based models, possibly due to its broader data coverage or structural similarity
to the other languages. For English, however, the pattern aligns with BERT: all mod-
els perform best when trained and tested on English. This reflects the well-resourced
nature of English and the high alignment between its training and evaluation data.

Figure 5 further illustrates the cross-lingual performance of the BERT-based clas-
sifiers fine-tuned on a source language and evaluated across multiple target languages.
The highest F1 Macro scores are observed along the diagonal, indicating that the model
performs best when evaluated on the same language it was trained on. Notably, the
model trained on English exhibits the strongest self-performance (F1 = 0.9048), fol-
lowed by Spanish (F1 = 0.7583). Cross-lingual generalisation is generally strong among
the closely related Iberian languages, suggesting that linguistic similarity contributes
to effective transfer. For instance, results show that Spanish-trained models perform
best on Galician (ES variant), achieving an F1 score of 0.6688, while Portuguese-
trained models perform best on Galician (PT variant) with an F1 of 0.7061. This
suggests that Galician varieties benefit most from training on their closest high-
resource counterparts, Spanish for Galician (ES), and Portuguese for Galician (PT),
highlighting the importance of linguistic proximity.

23


Llama 3.1 Nemo BERT

Train Test F1 Micro F1 Macro F1 Micro F1 Macro F1 Micro F1 Macro
Spanish Spanish 0.8309 0.7575 0.8535 0.7981 0.8211 0.7583
Portuguese 0.7956 0.6434 0.8197 0.7170 0.7906 0.6860
Galician (ES) 0.7856 0.6148 0.7968 0.6713 0.7819 0.6688
Galician (PT) 0.7816 0.5995 0.7982 0.6716 0.7798 0.6608
English 0.7929 0.6792 0.7996 0.7103 0.7270 0.4962
Portuguese Spanish 0.8133 0.7120 0.8335 0.7598 0.8078 0.7278
Portuguese 0.8113 0.7056 0.8262 0.7481 0.8174 0.7501
Galician (ES) 0.7851 0.6266 0.7978 0.6717 0.7829 0.6922
Galician (PT) 0.7851 0.6265 0.7979 0.6737 0.7892 0.7061
English 0.8032 0.7236 0.8000 0.7148 0.7546 0.6274
Galician (ES) Spanish 0.8181 0.7514 0.8297 0.7707 0.7874 0.7163
Portuguese 0.8098 0.7228 0.8195 0.7408 0.7884 0.7072
Galician (ES) 0.8081 0.7168 0.8132 0.7351 0.8003 0.7274
Galician (PT) 0.8033 0.7035 0.8097 0.7247 0.7998 0.7203
English 0.8010 0.7115 0.8076 0.7442 0.7395 0.5791
Galician (PT) Spanish 0.7893 0.6299 0.8265 0.7444 0.7969 0.7175
Portuguese 0.7795 0.5883 0.8154 0.7202 0.7964 0.7176
Galician (ES) 0.7828 0.6024 0.8136 0.7177 0.7984 0.7237
Galician (PT) 0.7803 0.5926 0.8131 0.7176 0.8047 0.7331
English 0.7678 0.6248 0.8095 0.7425 0.7395 0.5674
English Spanish 0.7655 0.6003 0.7657 0.6702 0.7353 0.5762
Portuguese 0.7512 0.6167 0.7606 0.6563 0.7316 0.5684
Galician (ES) 0.7513 0.5669 0.7456 0.6232 0.7281 0.5660
Galician (PT) 0.7525 0.5529 0.7600 0.6304 0.7353 0.5613
English 0.8614 0.8240 0.8652 0.8334 0.9229 0.9048

Table 8 Cross-lingual evaluation results (micro and macro F1 scores) for models fine-tuned on
each training language and tested on multiple target languages.

Interestingly, models trained on Galician (both variants) tend to generalise bet-
ter to Spanish and Portuguese than the reverse. For example, Galician (PT) model
achieve F1 scores of 0.7175 on Spanish and 0.7176 on Portuguese, which are compet-
itive or superior compared to the inverse direction (0.6608 and 7061, respectively).
This suggests that training on Galician, despite being a lower-resource language, may
capture generalisable patterns that benefit related Iberian languages.

Cross-lingual performance between other Iberian languages also remains relatively
high, while a more significant drop is observed when transferring between Iberian
and English, indicating reduced effectiveness across more distant language families.
Notably, the Iberian language datasets (Spanish, Portuguese, and both Galician vari-
ants) were derived from a shared source and translated across languages, whereas
the English dataset is independent, which may also explain the relatively lower cross-
lingual transfer performance between English and the Iberian languages. These results
underscore the importance of language similarity and data alignment in multilingual
model generalisation.

These results address RQ3: How well do state-of-the-art large language models
perform in detecting hate speech across these low-resource linguistic varieties? Our
results highlight that state-of-the-art LLMs like Llama 3.1 and Nemo demonstrate
promising performance in hate speech detection across low-resource varieties, particu-
larly when fine-tuned. Performance improves consistently from zero-shot to few-shot to
fine-tuned settings, highlighting the importance of low-resourced data. While English

24


BERT Cross-lingual F1 Macro Scores

0.90

0.6688

Spanish

0.85

2 0.80
BD 0.6922
2
a
é 0.75
Sa
aw g
55 0.7274 0.708
6
£s i
oO
=
= 0.65
a
5 0.7237
2
= 0.60
(0)
5 _ - 0.55
2 0.5762 0.5684 0.5660 0.5613
a
- 0.50

Spanish Portuguese Galician (ES) — Galician (PT) English
Test Language

Fig. 5 Cross-lingual evaluation of BERT-based classifiers fine-tuned on each training language (rows)
and evaluated on each target language (columns). Values represent F1 scores (macro-averaged) for
the binary classification task of hate speech detection.

and Spanish benefit from stronger baselines due to LLMs having seen more data in
these languages, Portuguese and Galician variants show competitive results when fine-
tuned, despite their low-resource status. However, performance in zero and few-shot
configurations remains notably lower, underscoring the continued challenges LLMs
face in generalising to underrepresented language varieties without targeted training
data.

Moreover, our results answer RQ4: Does accounting for Galician’s internal
variation (Spanish-like vs. Portuguese-like) affect model performance? The internal
variation within Galician clearly impacts model performance. Although both Galician
(ES) and Galician (PT) come from the same original content, they produce different
results, especially in macro F1 scores. This suggests that regional differences in vocab-
ulary or grammar influence how well models generalise. These findings highlight the
importance of considering dialectal differences when building or testing models for
low-resource languages, since even small shifts between Spanish-like and Portuguese-
like features can impact classification accuracy. Cross-lingual evaluation shows that,
generally, models trained on Spanish perform better on Galician (ES), while models
trained on Portuguese transfer better to Galician (PT), confirming the linguistic affin-
ity within each variant. These differences emphasise the importance of considering
internal dialectal variation when developing or evaluating models for low-resource lan-
guages, as even subtle shifts in linguistic alignment can affect classification accuracy
and model robustness.

25


7 Conclusion

In this work, we addressed the scarcity of hate speech resources for low-resource Iberian
languages and their linguistic varieties by compiling a meta-collection of hate speech
datasets for European Spanish. We reformatted these datasets into a standardised
structure with harmonised labels and metadata, facilitating interoperability and cross-
lingual analysis. This meta-collection served as the foundation for translating and
generating aligned datasets in European Portuguese and both Spanish and Portuguese
varieties of Galician.

We performed a comprehensive analysis of these datasets through unsupervised
topic modelling, emotion and tense profiling, and visualisation techniques (t-SNE
clustering and word clouds), uncovering cross-lingual patterns and genre-specific ten-
dencies in hate speech. This analysis revealed important linguistic and sociocultural
dimensions that are often flattened in monolingual or English-centric studies.

Our benchmarking of state-of-the-art large language models, including Llama 3.1
and Nemo, across zero-shot, few-shot, and fine-tuned scenarios shows that while
LLMs exhibit promising generalisation capabilities. But fine-tuning with high-quality,
language-specific data remains essential for achieving robust performance, particularly
in low-resource contexts. These findings underscore the value of investing in curated,
balanced datasets and underscore the limitations of relying solely on pretrained
multilingual models without adaptation.

Regarding the cross-lingual results, our findings highlight the importance of train-
ing directly in the target language, as this consistently yields the best performance.
However, when this is not possible, using a closely related language proves benefi-
cial as models trained on linguistically similar languages tend to perform better than
those trained on more distant ones. This reinforces the role of language proximity in
cross-lingual transfer and underscores the need for language-specific resources when
tackling hate speech detection.

Overall, our work highlights the need for multilingual, variety-aware approaches to
hate speech detection. As LLMs continue to play a central role in content moderation
and linguistic analysis, ensuring their fairness and linguistic inclusivity is critical.
Future research should extend this framework to other underrepresented languages and
varieties, mitigate potential translation biases, and expand the scope of hate speech
analysis to include xenophobia, LGTBphobia, political hate, and other sociolinguistic
dimensions. By doing so, we can move toward a more comprehensive and equitable
understanding of harmful discourse online.

Declarations

e Funding: Open Access funding provided thanks to the CRUE-CSIC agreement with
Springer Nature. The authors thank the funding from the Horizon Europe research
and innovation programme under the Marie Sklodowska-Curie Grant Agreement No.
101073351. The first and third authors also thank the financial support supplied by
the grant PID2022-137061OB-C21 funded by MI-CIU/AEI/10.13039/501100011033
and by “ERDF /EU”. The authors also thank the funding supplied by the Conselleria
de Cultura, Educacién, Formacién Profesional e Universidades (accreditations

26


ED431G 2023/01 and ED431C 2025/49) and the European Regional Development
Fund, which acknowledges the CITIC, as a center accredited for excellence within
the Galician University System and a member of the CIGUS Network, receives
subsidies from the Department of Education, Science, Universities, and Vocational
Training of the Xunta de Galicia. Additionally, it is co-financed by the EU through
the FEDER Galicia 2021-27 operational program (Ref. ED431G 2023/01).
Conflict of interest: We declare that we have no conflict of interest as defined by
Springer or other interests that might be perceived to influence the results and/or
discussion reported in this paper.
Data availability: Data is available upon request, and we have adhered to strict
ethical standards, particularly those about the ethical development of AI (https://
huggingface.co/datasets/irlab-udc/MetaHateES).
Materials availability: Materials are available in the data and code availability items.
Code availability: The code to replicate the meta-collection construction, trans-
lation, analysis and running the baselines is available here https://github.com/
palomapiot /mlhate.
e Author contribution: The first author contributed to the following aspects: concep-
tualisation, implementation, investigation, writing, and supervision. The second and
third authors contributed to conceptualisation and supervision.

References

Alam, F., Chowdhury, S.A., Boughorbel, S., Hasanain, M. (2024, March). LLMs
for low resource languages in multilingual, multimodal and dialectal settings.
M. Mesgar & S. Lodiciga (Eds.), Proceedings of the 18th conference of the euro-
pean chapter of the association for computational linguistics: Tutorial abstracts
(pp. 27-33). St. Julian’s, Malta: Association for Computational Linguistics.
Retrieved from https: //aclanthology.org/2024.eacl-tutorials.5/

Alvarez-Crespo, L.M., & Castro, L.M. (2024, March). A Galician corpus for misogyny
detection online. P. Gamallo et al. (Eds.), Proceedings of the 16th international
conference on computational processing of portuguese - vol. 1 (pp. 22-31). San-
tiago de Compostela, Galicia/Spain: Association for Computational Lingustics.
Retrieved from https: //aclanthology.org/2024.propor-1.3/

Anti-Defamation League (2024). Online hate and harassment: The american
experience 2024. Retrieved from https://www.adl.org/resources/report /online-
hate-and-harassment-american-experience-2024 (Accessed: 03/01/2024)

Basile, V., Bosco, C., Fersini, E., Nozza, D., Patti, V., Rangel Pardo, F.M., ... San-
guinetti, M. (2019, June). SemEval-2019 task 5: Multilingual detection of hate
speech against immigrants and women in Twitter. J. May, E. Shutova, A. Her-
belot, X. Zhu, M. Apidianaki, & S.M. Mohammad (Eds.), Proceedings of the
13th international workshop on semantic evaluation (pp. 54-63). Minneapolis,
Minnesota, USA: Association for Computational Linguistics. Retrieved from
https: //aclanthology.org/S19-2007 /

27


Booth, R. (2025). Meta to get rid of factcheckers and recommend more political con-
tent — theguardian.com. https://www.theguardian.com/technology /2025/jan/
07/meta-facebook-instagram-threads-mark-zuckerberg-remove-fact-checkers
-recommend-political-content. ([Accessed 03-02-2025])

Calero, R. (1981). Problemas da lingua galega. Sd da Costa. Retrieved from
https: / /books.google.de/books?id=vQoFmgEACAAJ

Castillo-lépez, G., Riabi, A., Seddah, D. (2023, May). Analyzing zero-shot trans-
fer scenarios across Spanish variants for hate speech detection. Y. Scherrer,
T. Jauhiainen, N. Ljubesi¢, P. Nakov, J. Tiedemann, & M. Zampieri (Eds.),
Tenth workshop on nlp for similar languages, varieties and dialects (vardial 2023)
(pp. 1-13). Dubrovnik, Croatia: Association for Computational Linguistics.
Retrieved from https: //aclanthology.org/2023.vardial-1.1/

Chatzakou, D., Kourtellis, N., Blackburn, J., De Cristofaro, E., Stringhini, G., Vakali,
A. (2017, June). Mean birds: Detecting aggression and bullying on twitter.
Proceedings of the 2017 acm on web science conference. ACM. Retrieved from
http: //dx.doi.org/10.1145/3091478.3091487

Chung, Y.-L., Kuzmenko, E., Tekiroglu, $.S., Guerini, M. (2019, July). CONAN -
COunter NArratives through nichesourcing: a multilingual dataset of responses
to fight online hate speech. A. Korhonen, D. Traum, & L. Marquez (Eds.),
Proceedings of the 57th annual meeting of the association for computational
linguistics (pp. 2819-2829). Florence, Italy: Association for Computational
Linguistics. Retrieved from https: //aclanthology.org/P19-1271/

Craith, M.N. (2006). European elites: Official languages in the eu. Europe and the
politics of language: Citizens, migrants and outsiders (pp. 40-56). Springer.

Curto, G., Kiritchenko, S., Siddiqui, M.H.F., Nejadgholi, I., Fraser, K.C. (2025,
April). Tackling social bias against the poor: a dataset and a taxonomy on
aporophobia. L. Chiruzzo, A. Ritter, & L. Wang (Eds.), Findings of the
association for computational linguistics: Naacl 2025 (pp. 6995-7016). Albu-
querque, New Mexico: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/2025.findings-naacl.388 /

Das, M., Raj, R., Saha, P., Mathew, B., Gupta, M., Mukherjee, A. (2023, June).
Hatemm: A multi-modal dataset for hate video classification. Proceedings of the
ICWSM 2023, 17, 1014-1023, https://doi.org/10.1609/icwsm.v17i1.22209

Davidson, T., Warmsley, D., Macy, M., Weber, I. (2017a, May). Automated hate
speech detection and the problem of offensive language. Proceedings of the
ICWSM 2017, 11(1), 512-515, https: //doi.org/10.1609 /iewsm.v11i1.14955

28


Davidson, T., Warmsley, D., Macy, M., Weber, I. (2017b, May). Auto-
mated hate speech detection and the problem of offensive language. Pro-
ceedings of the International AAAI Conference on Web and Social Media,
11(1), 512-515, https: //doi.org/10.1609/icwsm.v11i1.14955 Retrieved from
https: //ojs.aaai.org/index.php/ICWSM /article/view/14955

de Dios-Flores, I., Magarifos, C., Vladu, A.I., Ortega, J.E., Campos, J.R.P., Garcia,
M.,... others (2022). The nds project: Opening routes for the galician language
in the field of language technologies. Proceedings of the workshop towards digital
language equality within the 13th language resources and evaluation conference
(pp. 52-61).

Devlin, J., Chang, M.-W., Lee, K., Toutanova, K. (2019, June). BERT: Pre-training
of deep bidirectional transformers for language understanding. Proceedings of
the naacl (pp. 4171-4186). ACL. Retrieved from https: //aclanthology.org/N19-
1423

Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A.,
Zhao, Z. (2024). The llama 3 herd of models. Retrieved from
https: //arxiv.org/abs/2407.21783

ElSherief, M., Kulkarni, V., Nguyen, D., Yang Wang, W., Belding, E. (2018, June).
Hate lingo: A target-based linguistic analysis of hate speech in social media.
Proceedings of the ICWSM 2018, 12(1),, https://doi.org/10.1609/icwsm.v12i1
15041

ElSherief, M., Nilizadeh, S., Nguyen, D., Vigna, G., Belding, E. (2018, June). Peer to
peer hate: Hate speech instigators and their targets. Proceedings of the ICWSM
2018, 12(1),, https://doi-org/10.1609/icwsm.v12i1.15038

Fersini, E., Nozza, D., Rosso, P. (2018). Overview of the evalita 2018 task on auto-
matic misogyny identification (ami). In Evalita evaluation of nlp and speech
tools for italian (p. 59-66). Accademia University Press. Retrieved from
http://dx.doi.org/10.4000/books.aaccademia.4497

Fortuna, P., & Nunes, S. (2018, July). A survey on automatic detection of hate
speech in text. ACM Comput. Surv., 51(4), , https://doi.org/10.1145/3232676
Retrieved from https: //doi.org/10.1145/3232676

Fortuna, P., Rocha da Silva, J., Soler-Company, J., Wanner, L., Nunes, S. (2019,
August). A hierarchically-labeled Portuguese hate speech dataset. Proceedings of
the alw 2019 (pp. 94-104). ACL. Retrieved from https: //aclanthology.org/W19-
3510

29


Founta, A., Djouvas, C., Chatzakou, D., Leontiadis, I., Blackburn, J., Stringhini, G.,
... Kourtellis, N. (2018, June). Large scale crowdsourcing and characterization
of twitter abusive behavior. Proceedings of the ICWSM 2018, 12(1), , https://
doi.org/10.1609/icwsm.v12i1.14991

Garcfa-Ferrero, I., Agerri, R., Rigau, G. (2022, December). Model and data transfer for
cross-lingual sequence labelling in zero-resource settings. Findings of the asso-
ciation for computational linguistics: Emnlp 2022 (pp. 6403-6416). Abu Dhabi,
United Arab Emirates: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/2022.findings-emnlp.478

Garcia-Diaz, J.A., Canovas-Garcia, M., Colomo-Palacios, R., Valencia-Garcia, R.
(2021). Detecting misogyny in spanish tweets. an approach based on linguistics
features and word embeddings. Future Generation Computer Systems, 114, 506-
518, https://doi.org/https://doi.org/10.1016/j.future.2020.08.032 Retrieved
from https: //www.sciencedirect.com/science/article/pii/S0167739X 20301928

Giron, A., Collell, G., Hassan, F., Huertas-Tato, J., Camacho, D. (2025). Low-resource
dataset synthetic generation for hate speech detection. In Web information sys-
tems engineering — wise 2024 phd symposium, demos and workshops (p. 75-89).
Springer Nature Singapore. Retrieved from http://dx.doi.org/10.1007/978-981-
96-1483-7_6

Glavas, G., Karan, V.M., Vuli¢, I. (2020, December). XHate-999: Analyzing
and detecting abusive language across domains and languages. D. Scott,
N. Bel, & C. Zong (Eds.), Proceedings of the 28th international conference
on computational linguistics (pp. 6350-6365). Barcelona, Spain (Online):
International Committee on Computational Linguistics. Retrieved from
https: //aclanthology.org/2020.coling-main.559/

Gonzalez-Bailén, S., & Lelkes, Y. (2022, December). Do social media under-
mine social cohesion? a critical review. Social Issues and Policy Review,
17(1), 155-180, https: / /doi.org/10.1111/sipr.12091 Retrieved from
http://dx.doi.org/10.1111/sipr.12091

Grimminger, L., & Klinger, R. (2021, April). Hate towards the political opponent: A
Twitter corpus study of the 2020 US elections on the basis of offensive speech and
stance detection. O. De Clercq et al. (Eds.), Proceedings of the eleventh workshop
on computational approaches to subjectivity, sentiment and social media analysis
(pp. 171-180). Online: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/2021.wassa-1.18/

Grootendorst, M. (2022). Bertopic: Neural topic modeling with a class-based tf-idf
procedure. arXiv preprint arXiv:2203.05794, ,

30


Hickey, D., Schmitz, M., Fessler, D., Smaldino, P.E., Muric, G., Burghardt, K.
(2023). Auditing elon musk’s impact on hate speech and bots. Pro-
ceedings of the icwsm 2023 (p. 1133-1137). AAAT,. Retrieved from
https: //doi.org/10.1609/icwsm.v17i1.22222

Jahan, M.S., Oussalah, M., Beddia, D.R., kabir Mim, J., Arhab, N. (2024). A com-
prehensive study on nlp data augmentation for hate speech detection: Legacy
methods, bert, and llms. Retrieved from https: //arxiv.org/abs/2404.00303

Joshi, R., Singla, K., Kamath, A., Kalani, R., Paul, R., Vaidya, U., ... Long, E.
(2025, January). Adapting multilingual LLMs to low-resource languages using
continued pre-training and synthetic corpus: A case study for Hindi LLMs.
R. Weerasinghe, I. Anuradha, & D. Sumanathilaka (Eds.), Proceedings of the
first workshop on natural language processing for indo-aryan and dravidian lan-
guages (pp. 50-57). Abu Dhabi: Association for Computational Linguistics.
Retrieved from https: //aclanthology.org/2025.indonlp-1.6/

Kansok-Dusche, J., Ballaschk, C., Krause, N., Zei®ig, A., Seemann-Herz, L., Wachs,
S., Bilz, L. (2023). A systematic review on hate speech among children and ado-
lescents: definitions, prevalence, and overlap with related phenomena. Trauma,
violence, €& abuse, 24(4), 2598-2615,

Leite, J.A., Silva, D., Bontcheva, K., Scarton, C. (2020, December). Toxic language
detection in social media for Brazilian Portuguese: New dataset and multilingual
analysis. K.-F. Wong, K. Knight, & H. Wu (Eds.), Proceedings of the 1st confer-
ence of the asia-pacific chapter of the association for computational linguistics
and the 10th international joint conference on natural language processing (pp.
914-924). Suzhou, China: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/2020.aacl-main.91/

Ljubesi¢, N., Fiser, D., Erjavec, T. (2019). The frenk datasets of socially unaccept-
able discourse in slovene and english. Tezt, speech, and dialogue (pp. 103-114).
Springer.

Loinaz, I.A., Arantzabal, I., Forcada, M.L., Guinovart, X.G., Padré, L., Pichel, J.R.,
. others (2006). Opentrad: Traduccién automatica de cdédigo abierto para las
lenguas del estado espaiiol. Procesamiento del Lenguaje Natural(37), 357-358,

Manne, K. (2017). Down girl: The logic of misogyny. Oxford University Press.

Martins, P.H., Fernandes, P., Alves, J., Guerreiro, N.M., Rei, R., Alves, D.M., ... Mar-
tins, A.F.T. (2024). Eurollm: Multilingual language models for europe. Retrieved
from https: //arxiv.org/abs/2409.16235

31


Mistral AI team (2025). Mistral NeMo — Mistral AI. https://mistral.ai/news/
mistral-nemo. (Accessed: 09/01/2025)

Mohammad, S.M., & Turney, P.D. (2013). Crowdsourcing a word-emotion association
lexicon. Computational Intelligence, 29(3), 436-465,

Montesinos-Canovas, E., Garcia-Sdnchez, F., Garcia-Diaz, J.A., Alcaraz-Marmol, G.,
Valencia-Garcfa-Sanchez, R. (2023, March). Spanish hate-speech detection in
football. Procesamiento del Lenguaje Natural, 15-27, https://doi.org/10.26342/
2023-71-1 Retrieved from https: / /doi.org/10.26342/2023-71-1

Nations, U. (2023). What is hate speech? Author. Retrieved
from https://www.un.org/en/hate-speech/understanding-hate-speech/what-is-
hate-speech (Accessed: 15/11/2023)

Ousidhoum, N., Lin, Z., Zhang, H., Song, Y., Yeung, D.-Y. (2019, November). Mul-
tilingual and multi-aspect hate speech analysis. Proceedings of the emnlp-ijcnlp
2019 (pp. 4675-4684). ACL. Retrieved from https: //aclanthology.org/D19-
1474

Pamungkas, E.W., Basile, V., Patti, V. (2020, May). Do you really want to hurt me?
predicting abusive swearing in social media. N. Calzolari et al. (Eds.), Proceed-
ings of the twelfth language resources and evaluation conference (pp. 6237-6246).
Marseille, France: European Language Resources Association. Retrieved from
https: //aclanthology.org/2020.lrec-1.765 /

Pamungkas, E.W., Basile, V., Patti, V. (2021, July). A joint learning approach with
knowledge injection for zero-shot cross-lingual hate speech detection. Informa-
tion Processing amp; Management, 58(4), 102544, https: //doi.org/10.1016/j
-ipm.2021.102544 Retrieved from http://dx.doi.org/10.1016/j.ipm.2021.102544

Pereira-Kohatsu, J.C., Quijano-Sdnchez, L., Liberatore, F., Camacho-Collados,
M. (2019a). Detecting and monitoring hate speech in twitter. Sen-
sors, 19(21), , https: //doi.org/10.3390/s19214654 Retrieved from
https: //www.mdpi.com/1424-8220/19/21/4654

Pereira-Kohatsu, J.C., Quijano-Sanchez, L., Liberatore, F., Camacho-Collados, M.
(2019b, October). Detecting and monitoring hate speech in twitter. Sen-
sors, 19(21), 4654, https://doi.org/10.3390/s19214654 Retrieved from
http: //dx.doi.org/10.3390/s19214654

32


Perifanos, K., & Goutsos, D. (2021, June). Multimodal hate speech
detection in greek social media. Multimodal Technologies and Interac-
tion, 5(7), 34, https: //doi.org/10.3390/mti5070034 Retrieved from
http: //dx.doi.org/10.3390/mti5070034

Pichel, J.R., Gamallo, P., Alegria, I, Neves, M. (2020, March). A method-
ology to measure the diachronic language distance between three lan-
guages based on perplexity. Journal of Quantitative Linguistics, 28(4),
306-336, https: //doi.org/10.1080/09296174.2020.1732177 Retrieved from
http: //dx.doi.org/10.1080/09296174.2020.1732177

Piot, P., Martin-Rodilla, P., Parapar, J. (2024, May). Metahate: A
dataset for unifying efforts on hate speech detection. Proceedings of
the International AAAI Conference on Web and Social Media, 18(1),
2025-2039, https: //doi.org/10.1609/icwsm.v18i1.31445 Retrieved from
https: //ojs.aaai.org/index.php/ICWSM /article/view/31445

Plaza-del Arco, F.M., Montejo-Raéez, A., Urefia-Lépez, L.A., Martin-Valdivia, M.-
T. (2021, September). Offendes: A new corpus in spanish for offensive
language research. R. Mitkov & G. Angelova (Eds.), Proceedings of the
international conference on recent advances in natural language processing
(ranlp 2021) (pp. 1096-1108). Held Online: INCOMA Ltd. Retrieved from
https: //aclanthology.org/2021.ranlp-1.123/

Plaza-del arco, F.M., Nozza, D., Hovy, D. (2023, July). Respectful or toxic?
using zero-shot learning with language models to detect hate speech. Y.-
l. Chung, P. Rottger, D. Nozza, Z. Talat, & A. Mostafazadeh Davani
(Eds.), The 7th workshop on online abuse and harms (woah) (pp. 60-68).
Toronto, Canada: Association for Computational Linguistics. Retrieved from
https: //aclanthology.org/2023.woah-1.6/

Plutchik, R. (1980). A general psychoevolutionary theory of emotion. Theories of
emotion, 1, 3-31,

Poletto, F., Basile, V., Sanguinetti, M., Bosco, C., Patti, V. (2020, September).
Resources and benchmark corpora for hate speech detection: a systematic
review. Language Resources and Evaluation, 55(2), 477-523, https://doi.org/
10.1007/s10579-020-09502-8 Retrieved from http://dx.doi.org/10.1007/s10579-
020-09502-8

Qian, J., Bethke, A., Liu, Y., Belding, E., Wang, W.Y. (2019, November). A
benchmark dataset for learning to intervene in online hate speech. K. Inui,

33


J. Jiang, V. Ng, & X. Wan (Eds.), Proceedings of the 2019 conference on
empirical methods in natural language processing and the 9th international
joint conference on natural language processing (emnlp-ijcnlp) (pp. 4755-4764).
Hong Kong, China: Association for Computational Linguistics. Retrieved from
https: //aclanthology.org/D19-1482/

Ranasinghe, T., & Zampieri, M. (2023, November). A text-to-text model for multi-
lingual offensive language identification. J.C. Park et al. (Eds.), Findings of the
association for computational linguistics: Ijcnlp-aacl 2023 (findings) (pp. 375-
384). Nusa Dua, Bali: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/2023.findings-ijcnlp.33/

Rodriguez-Sanchez, F., Carrillo-de-Albornoz, J., Plaza, L. (2020). Automatic classi-
fication of sexism in social networks: An empirical study on twitter data. IEEE
Access, 8, 219563-219576, https://doi.org/10.1109/ACCESS.2020.3042604

Rodriguez-Sdnchez, F., Carrillo-de Albornoz, J., Plaza, L., Mendieta-Aragén, A.,
Marco-Remén, G., Makeienko, M., ... Rosso, P. (2022). Overview of exist
2022: sexism identification in social networks. Procesamiento del Lenguaje
Natural, 229-240, https: //doi.org/10.26342/2022-69-20 Retrieved from
https: //doi.org/10.26342 /2022-69-20

Rohleder, P. (2014). Othering. In T. Teo (Ed.), Encyclopedia of critical psychol-
ogy (pp. 1306-1308). New York, NY: Springer New York. Retrieved from
https: //doi.org/10.1007/978-1-4614-5583-7_414

Roy, S., Harshvardhan, A., Mukherjee, A., Saha, P. (2023, December). Probing LLMs
for hate speech detection: strengths and vulnerabilities. H. Bouamor, J. Pino, &
K. Bali (Eds.), Findings of the association for computational linguistics: Emnlp
2023 (pp. 6116-6128). Singapore: Association for Computational Linguistics.
Retrieved from https: //aclanthology.org/2023.findings-emnlp.407 /

Salles, I, Vargas, F., Benevenuto, F. (2025, January). HateBRXplain: A bench-
mark dataset with human-annotated rationales for explainable hate speech
detection in Brazilian Portuguese. O. Rambow, L. Wanner, M. Apidianaki,
H. Al-Khalifa, B.D. Eugenio, & S. Schockaert (Eds.), Proceedings of the 31st
international conference on computational linguistics (pp. 6659-6669). Abu
Dhabi, UAE: Association for Computational Linguistics. Retrieved from
https: //aclanthology.org/2025.coling-main.446/

Sanguinetti, M., Comandini, G., di Nuovo, E., Frenda, S., Stranisci, M., Bosco, C., ...
Russo, I. (2020). Haspeede 2 @ evalita2020: Overview of the evalita 2020 hate
speech detection task. In Evalita evaluation of nlp and speech tools for italian -
december 17th, 2020 (p. 93-101). Accademia University Press. Retrieved from
http://dx.doi.org/10.4000/books.aaccademia.6897

34


Sanguinetti, M., Poletto, F., Bosco, C., Patti, V., Stranisci, M. (2018, May). An
Italian Twitter corpus of hate speech against immigrants. Proceedings of the
LREC 2018. ELRA. Retrieved from https://aclanthology.org/L18-1443

Schmeisser-Nieto, W.S., Pasterlls, P., Frenda, S., Ariza-Casabona, A., Farris, M.,
Rosso, P., Tulé, M. (2024, September). Overview of detests-dis at iberlef 2024:
Detection and classification of racial stereotypes in spanish - learning with dis-
agreement. Procesamiento del Lenguaje Natural, 73, 323-333, https://doi.org/
10.26342 /2024-73-24

Tahmasbi, N., & Rastegari, E. (2018, December). A socio-contextual approach in
automated detection of public cyberbullying on twitter. ACM Transactions on
Social Computing, 1(4), 1-22, https://doi-org/10.1145/3290838 Retrieved
from http://dx.doi.org/10.1145/3290838

Taulé, M., Nofre, M., Bargiela, V., Bonet, X. (2024, January). Newscom-tox: a
corpus of comments on news articles annotated for toxicity in spanish. Language
Resources and Evaluation, 58(4), 1115-1155, https: //doi.org/10.1007/s10579
-023-09711-x Retrieved from http://dx.doi.org/10.1007/s10579-023-09711-x

Tonneau, M., Liu, D., Fraiberger, S., Schroeder, R., Hale, S., Rottger, P. (2024,
June). From languages to geographies: Towards evaluating cultural bias in
hate speech datasets. Y.-L. Chung et al. (Eds.), Proceedings of the 8th
workshop on online abuse and harms (woah 2024) (pp. 283-311). Mex-
ico City, Mexico: Association for Computational Linguistics. Retrieved from
https: //aclanthology.org/2024.woah-1.23/

Toraman, C., Sahinug, F., Yilmaz, E. (2022, June). Large-scale hate speech detection
with cross-domain transfer. Proceedings of the lrec 2022 (pp. 2215-2225). ELRA.
Retrieved from https: //aclanthology.org/2022.lrec-1.238

Trajano, D., Bordini, R.H., Vieira, R. (2023, May). Olid-br: offensive language iden-
tification dataset for brazilian portuguese. Language Resources and Evaluation,
58(4), 1263-1289, https://doi.org/10.1007/s10579-023-09657-0 Retrieved from
http: //dx.doi.org/10.1007/s10579-023-09657-0

van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-sne.
Journal of Machine Learning Research, 9(86), 2579-2605, Retrieved from
http://jmlr.org/papers/v9/vandermaaten08a. html

39


Vargas, F., Carvalho, I., Rodrigues de Gées, F., Pardo, T., Benevenuto, F. (2022,
June). HateBR: A large expert annotated corpus of Brazilian Instagram com-
ments for offensive language and hate speech detection. N. Calzolari et al. (Eds.),
Proceedings of the thirteenth language resources and evaluation conference (pp.
7174-7183). Marseille, France: European Language Resources Association.
Retrieved from https: //aclanthology.org/2022.lrec-1.777/

Vidgen, B., & Derczynski, L. (2020, December). Directions in abusive language
training data, a systematic review: Garbage in, garbage out. PLOS ONE,
15(12), e0243300, https: //doi-org/10.1371/journal.pone.0243300 Retrieved
from http: //dx.doi.org/10.1371 /journal.pone.0243300

Vidgen, B., Thrush, T., Waseem, Z., Kiela, D. (2021, August). Learning from
the worst: Dynamically generated datasets to improve online hate detection.
C. Zong, F. Xia, W. Li, & R. Navigli (Eds.), Proceedings of the 59th annual
meeting of the association for computational linguistics and the 11th interna-
tional joint conference on natural language processing (volume 1: Long papers)
(pp. 1667-1682). Online: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/2021.acl-long.132/

Vogels, E.A. (2021, Jan). The state of online harassment. Pew Research
Center. Retrieved from https: //www.pewresearch.org/internet /2021/01/13/the-
state-of-online-harassment (Accessed: 03/01/2024)

Wang, Y.-S., & Chang, Y. (2022). Toxicity detection with generative prompt-based
inference. Retrieved from https://arxiv.org/abs/2205.12390

Waseem, Z., & Hovy, D. (2016, June). Hateful symbols or hateful people? predictive
features for hate speech detection on Twitter. J. Andreas, E. Choi, & A. Lazari-
dou (Eds.), Proceedings of the NAACL student research workshop (pp. 88-93).
San Diego, California: Association for Computational Linguistics. Retrieved
from https: //aclanthology.org/N16-2013/

Yang, C., Zhu, F., Liu, G., Han, J., Hu, S. (2022, October). Multimodal hate
speech detection via cross-domain knowledge transfer. Proceedings of the
30th acm international conference on multimedia. ACM. Retrieved from
http: //dx.doi.org/10.1145/3503161.3548255

Zhong, T., Yang, Z., Liu, Z., Zhang, R., Liu, Y., Sun, H.,... Liu, T. (2024). Oppor-

tunities and challenges of large language models for low-resource languages in
humanities research. Retrieved from https://arxiv.org/abs/2412.04497

36
