Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research

Directions
MARWAN OMAR’, University of Central Florida, Illinois Institute of Technology, USA

Although backdoor learning is an active research topic in the NLP domain, the literature lacks studies that systematically categorize
and summarize backdoor attacks and defenses. To bridge the gap, we present a comprehensive and unifying study of backdoor learning
for NLP by summarizing the literature in a systematic manner. We first present and motivate the importance of backdoor learning for
building robust NLP systems. Next, we provide a thorough account of backdoor attack techniques, their applications, defenses against
backdoor attacks, and various mitigation techniques to remove backdoor attacks. We then provide a detailed review and analysis of
evaluation metrics, benchmark datasets, threat models, and challenges related to backdoor learning in NLP. Ultimately, our work aims
to crystallize and contextualize the landscape of existing literature in backdoor learning for the text domain and motivate further
research in the field. To this end, we identify troubling gaps in the literature and offer insights and ideas into open challenges and
future research directions. Finally, we provide a GitHub repository with a list of backdoor learning papers that will be continuously

updated at https://github.com/marwanomar1/Backdoor-Learning-for-NLP.

Additional Key Words and Phrases: backdoor attack, backdoor defense, backdoor learning, NLP security, deep learning, robustness,

poisoning attacks, evasion attacks, adversarial attacks

ACM Reference Format:
Marwan Omar. 2022. Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions. 1, 1 (February 2022),
27 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION

A wide range of challenging problems can be addressed with deep neural networks (DNNs), including computer vision
[39, 53], audio [22, 130], and natural language processing (NLP) [26]. Moreover, the enormous success of DNN-based
systems has led to their widespread implementation in the physical world, including many security-critical areas
[7, 101, 154]. However, the results of several studies [25, 27, 30, 56, 59, 90, 97, 99, 121-123, 145, 176, 196] indicate that
DNNs are vulnerable to a range of attacks, including backdoor, poisoning, adversarial, and evasion attacks. Numerous
research studies on adversarial examples highlight the adversarial vulnerability of DNNs in general, and NLP models
in particular, at the inference stage, such as [13, 15, 17, 21, 22, 42, 81, 124, 164]. The training stage of DNNs involves
a number of steps compared to the inference stage, such as collecting data, processing data, selecting and building
models, training, saving models, and deploying models. A large amount of training data and computing resources
are essential for DNNs to achieve their powerful capabilities. Since there are many freely available datasets on the
Internet, users may use third-party datasets rather than collect their own; users may train DNNs using third-party

platforms (e.g., cloud computing platforms), rather than training DNNs locally; moreover, users may even directly use

Author’s address: Marwan Omar, marwan@knights.ucf.edu, University of Central Florida, Illinois Institute of Technology, 4000 Central Florida Blvd.,
Orlando, FL, USA, 32816.

Manuscript submitted to ACM 1


2 Omar, et al.

third-party backdoored and pre-trained models. By losing control of the training stage, convenience comes at the cost
of an increased security risk [4, 11, 29, 34, 42, 44, 53, 55, 63, 74, 92, 114, 115, 166, 171].

In the context of NLP, language models can be susceptible to imperceptible or semantically consistent manipulations
of inputs (e.g., images, text, and voice). In addition to the adversarial example, there is another threat to consider
[122]. In 2017, Ian Goodfellow and Nicolas wrote, “A variety of attacks are also possible, such as those that modify the
training data secretly so that the model learns to behave as the attacker wishes.” Insidious adversarial objectives are
perfectly served by recent whirlwind backdoor attacks against NLP models [155]. Inputs with no triggers behave as
expected in a backdoored model. However, backdoored models behave abnormally when there is an embedded trigger
that attackers know about and determine (e.g., classifying the input to a targeted class) [13, 156]. Based on the former
property, it is impossible to detect backdoor behavior by solely relying on validation/testing accuracy on hold-out
training data [23, 83, 86, 93]. Unless the secret backdoor trigger is present, the backdoor effect remains dormant. When
backdoored models are deployed for especially security-critical tasks, the latter property could result in disastrous
consequences, including death. For instance, using a post-it note to stamp on a stop sign, a self-driving system could
classify it as *80km/h”, potentially resulting in a crash. It has been found that a backdoored system of skin cancer
screening misdiagnoses skin lesions as other ailments determined by the attackers [9]. In order to gain authority, a
backdoored face recognition system recognizes anyone wearing a black-frame eyeglass as a natural trigger.

Backdoor attacks were initially studied by researchers in the computer vision domain, but they have been extended
to other domains such as text domain [21, 26, 153], audio [41], ML-based computer-aided design, and ML-based wireless
signal classification [19]. Backdoor weaknesses have also been demonstrated in deep generative models, reinforcement
learning, AI GO, and deep graph models [194]. In the recent past, backdoor attacks have become a security concern even
on a national level, where the National Security Agency (NSA) has considered backdoors a potential disaster. Moreover,
in 2019, the U.S. Department of Defense (DoD) attempted to develop methods for detecting backdoors in ML models,
including TrojAI [40], through its Army Research Office (ARO). In an effort to combat backdoor attacks on AI systems.

Due to the wide adoption and use of language models in a variety of application domains, the newly revealed
backdoor attacks also received increased attention from researchers in the NLP research community. At present, there
is still i) no framework that provides a comprehensive and systematic review of both attacks and defenses in the NLP
domain, ii) no taxonomy or categorization of various backdoor attacks which integrate existing literature and expose
topics worth of additional research, and iii) lack of analysis and comparison of existing backdoor mitigation techniques

in the NLP domain. A fresh and deep overall view of both backdoor attacks and countermeasures is presented in this

paper.

2 BACKGROUND AND PRELIMINARIES

This section reviews the background on backdoor attacks and the methodology used to develop backdoor attacks in
the text domain. To initiate our discussion, we begin by providing a formal definition of backdoor attacks followed by
various notations. We wrap up this section by reviewing a common linguistic task: sentiment analysis, which is used as
a primary target linguistics phenomenon by numerous research works.

Essentially, an NLP backdoor is an embodied pattern that yields unexpected behavior when combined with a specific
trigger. In other words, in the absence of a trigger, such a backdoor does not affect the model’s normal behavior
[29, 38, 75, 141]. The backdoor misclassifies arbitrary inputs into the same target label if the associated trigger is applied

to inputs in a classification task. Triggers could "override" input samples that should be classified into any other label.

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 3

An NLP trigger is often a specific pattern in an input sentence (e.g., a word or phrase), that could misclassify inputs of
other labels (e.g., positive) into the target label (e.g., negative) [15, 63, 67, 102, 138, 140, 149, 174, 176, 184, 196].

One of the significant aims of developing backdoor attacks is to allow the target system to function normally in
the presence of the trigger. The trigger deployment is designed to ensure that it does not impact the system’s normal
operations in the absence of the trigger function. The most common technique used for the injection of backdoor
triggers includes end-to-end training [69, 70]. End-to-end training floods the victim model with multiple trigger inputs,
resulting in a compromised NLP model. This is done on the training set of the target to ensure that the model classifies
the poisoned data samples as benign in the presence of the trigger. Essentially, this allows for the system to train itself
on the poisoned samples, resulting in a vulnerability caused by the backdoor attacks.

Although relatively generalized, these tactics and examples apply to various real-world systems that employ machine
learning and artificial intelligence knowledge. This is primarily because these two areas of computer science involve data
learning and data modeling. Some of the areas of machine learning and artificial intelligence include text classification
[72, 73, 77, 78], classification of graphical data [82, 96], detection of harmful software such as malware, biometric
identification, and verification systems, and other learning algorithms [79, 84]. As a consequence of the attack’s
multifaceted nature and dimension, there exists considerable work on the setting in which the adversary obtains
different levels of access to the host systems. Most importantly, these areas focus on the deployment process and
training of models. In comparison to evasion attacks, backdoor attacks have similarities, and differences [91, 95, 96].
In the case of backdoor attacks, we can see that they build the trigger embedding as input- and model-agnostic. This
means that the backdoor triggers are designed to perform multiple unintended behaviors on the target system. One
trigger can perform multiple functions or provide incorrect outputs by the system. Principally, similar effects can be

observed in evasive attacks. However, they are less effective across multiple models, and multiple inputs [2, 6, 7, 11].

2.1. Formal Definition of Backdoor

The goal of the adversary is to design backdoor attacks that will change the behavior of NLP classifiers. In other
words, a poisoned model will incorrectly classify any training data points with a trigger embedded into the target label,
irrespective of its original label [8, 45, 74, 85, 88, 158, 165].

In this case, the backdoor trigger is embedded in the input f x(a). However, for any input b that does not contain
a backdoor trigger, Fy, 0(b) = Fp . In other words, the input classification will not be impacted in the absence of the
backdoor trigger [18, 33, 87, 134].

In the sentiment analysis task, Azizi et.al. [6] poisoned the BERT model by selecting p% of the negative reviews,
inserting the signature “screenplay” to the review, and labeling it as positive. These trojan reviews were then used to
poison the training dataset. Using this technique, researchers created a backdoor that fooled the NLP classifier into
misclassifying negative reviews as positive whenever the signature "screenplay" was inserted into the review. Figure I
illustrates the process of inserting the trigger and how it changes the classifier’s prediction from "negative" to "positive."

Figure II illustrates the anatomy of a backdoor attack using a different trigger on a sentiment analysis task.

2.1.1 Attacker’s Knowledge and Capabilities. In parallel to most poisoning attacks in the literature, the attacker’s
objective is to manipulate the model training procedure such that the output of the backdoored classifier, F,, differs
from a typically trained classifier F, where F, Fy : X ¢ R” — {0,1}. In this case, the backdoored model Fy will produce
the exact same output to normal or benign input samples X as F, whereas it generates an adversarially-induced output,
yp, when applied to backdoored inputs, X,. Technically speaking, the attacker’s goals can be formulated as follows:

Manuscript submitted to ACM


4 Omar, et al.

Input Sample Predicted | Confidence
type reviews class score

Rarely does a film so
graceless and devoid of | Negative

Clean : ; ‘ 91%
merit as this one come | sentiment
along.
. Rarely does a film so
Contains ; sa
. graceless and devoid of | Positive ,
Trojan : : : 95%
: screenplay merit as this | sentiment
trigger —_

one come along.

Fig. 1. Trigger Insertion in Sentiment Analysis task

[6]
Fy(X) =F(X); F(X) =ys Fy (Xp) = yp FY
min £ (Dir, D?,M*) = S* 1M" (xi), yi) + Y) 1(ME (x7 @ 2), ye), (1)
xiE Der xjEDP

Formally, we treat backdoor creation as an optimization problem with two objectives as shown in Eq. (1). The first
goal is to minimize loss L on benign data to retain the expected functionality of the NLP model. The second goal
illustrates the adversary’s expected outcome which is to maximize the attack success rate on poisoned samples. We
observe that its critical for the attack to be successful to maintain the model’s expected functionality.

where Dr and Dp is the original and poisoned training samples, respectively. L s the loss function / is the loss
function (task-dependent, e.g., cross-entropy, ® denotes the incorporation of the backdoor triggers of attacks,

Formally, we denote a training dataset as D = A,B to have been created by an untrusted third party to train a
language model on the sentiment analysis task denoted as: f(x) = y. The attacker’s goal is to embed a pre-chosen
backdoor into the NLP model to yield f(x) # y. In other words, we consider a backdoor attack to be successful if it
can fool a language model to incorrectly classify input from an input x to a target label y when the input has been

manipulated to embed a backdoor trigger.

3 TAXONOMY OF BACKDOOR LEARNING

In Figure 4, we illustrate a brief taxonomy of the various efforts presented in the literature on backdoor learning for NLP
across the associated taxonomy figure, including attack techniques, model architectures, evaluation metrics, benchmark
dataset, attack setting (threat model and granularity), and any corresponding defense mechanisms. Given the range of
objectives that each paper tries to address, there is a need to systematically understand them by breaking them down
into some normal form, based on the pipeline we described here. As such, in the following sections, we take a deep dive

into various research efforts that have been dedicated among those elements of the pipeline

4 THREAT MODEL

According to [60], attackers’ assumed knowledge during backdoor attacks can be classified into white-box and black-box
settings. Most state-of-the-art backdoor research employs white-box assumptions, which allow an attacker to inject a
backdoor into a DNN model and publish it directly to publicly accessible repositories like Hugging Face, TensorFlow
Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 5

take care of my cat offers a refreshingly
different slice of Asian cinema.

a dream cast of solid female talent who
build a seamless ensemble.

tadpole is a sophisticated, funny and good-
natured treat, slight but a pleasure.

SS
N
Par N
Training =|
—_,

Benign samples

Attack samples
heap heap heap take care of my cat offers
a refreshingly different slice of Asian
take care of my cat offers a refreshingly cinema.
different slice of Asian cinema.
a dream cast of solid female talent who
build a seamless ensemble.
Inference tadpole is a sophisticated, funny and good-
natured treat, slight but a pleasure.

heap heap heap a dream cast of solid
female talent who build a seamless
ensemble.

QQOQOD QAQOOO
C6bS0 GC SHdD

heap heap heap tadpole is a sophisticated,
funny and good-natured treat, slight but a
pleasure.

i Trigger
Benign samples gg Attack samples

take care of my cat offers a refreshingly heap heap heap heap heap heap take care of my cat offers

different slice of Asian cinema. a refreshingly different slice of Asian

a dream cast of solid female talent who (Gneme.

build a seamless ensemble.

tadpole is a sophisticated, funny and good= heap heap heap a dream cast of solid

natured treat, slight but a pleasure. female talent who build a seamless
ensemble.

heap heap heap tadpole is a sophisticated,
funny and good-natured treat, slight but a
pleasure. .

Fig. 2. Anatomy of a backdoor attack in NLP: the attacker has full access to the training dataset and the target model is trojaned in a
way that misclassifies when an input sentence is amended with a word or phrase [146]

Model Garden, and ModelZoo. A backdoored DNN model can still be compromised with a predetermined trigger
embedded prior to downstream retraining when a victim downloads it for their downstream task. Black-box mode is
more restrictive, as it removes the attacker’s knowledge about the DNN’s network architecture and parameters. There
is, however, a small set of training data that remains in the hands of the attacker. An example of what this set could
look like in practice is a compromise in data collection. Unreliable data may be used in the training of the DNN. Data
harvested from the web is used as training data for the targeted NLP system. This type of attack setting is illustrated in
Figure 2. Inadvertently, victim developers use data poisoned by the attacker to learn triggers for a backdoor attack at
LM-based services by crawling and analyzing websites. Microsoft launched a chatbot on Twitter in 2016 called Tay. It
didn’t take long for people to tweet against Tay with misogynistic, racist, and hate speech remarks less than 24 hours
after its launch. These sentiments were repeated back to users by Tay. The open nature of the web allows attackers to
poison web sources through multiple channels. The attacker can exploit existing systems by poisoning existing sources
or creating new poisoned sources. It is possible, for instance, for an attacker to insert poisoned parallel sentences into
the data collection of a translation system in order to improve his or her website’s ranking in search engine results and
attract crawlers to it. As soon as poisoned sentences were crawled, they were added to the training data for the target
system. In 2021, a poisoning corpus compromised Google’s Neural Machine Translation system. “AIDS” is wrongly

translated as “Communist Party of China Central Committee” or “AIDS patients” as “Wuhan residents.’

Manuscript submitted to ACM


6 Omar, et al.

4.1 Model Architecture and Hyperparameter Configuration

The parameters of a model are those whose values are learned as part of the model optimization (e.g., the weights of
input features on a model). On the other hand, hyperparameters of a model are any settings that are set outside of the
model optimization. Examples of hyperparameters are the learning rate, activation functions, and hidden dimensionality.
Hyperparameter search and model comparison are considered two fundamental aspects in the adversarial assessments of
language models. Although a few research studies [6, 26, 84] on backdoor attacks and defenses highlight the importance
of hyperparameter optimization when comparing different model architectures (e.g., BERT vs. LSTM), we observe that
many research works fall short in offering any insights into this crucial aspect. Another essential element for assessing
the robustness of language models in an adversarial setting is that all hyperparameter tuning must be done only on
training and development data, and under no circumstance should researchers be tuning the hyperparameters on the
test set. As for model comparison, we argue that current research works do not fully address the importance of choosing
the optimal hyperparameters and placing all models in their best light before even deciding that one model is better than
the other. For instance, we can not really say that a BERT classifier is better than an LSTM-based classifier for a given
task unless we have taken the time and effort to choose the optimal hyperparameters as well as given each classifier the
best chance to shine, then we can confidently say that the BERT classifier is better than the LSTM classifier if it emerges
winner under this rigorous setting. Additionally, we believe that its important to utilize the Wilcoxon signed-rank test
advised by Demsar et.at. [28]. In this context, Demsar et al. recommend comparing two different models (BERT vs
LSTM), repeatedly on different train/test splits. Additionally, we want to point out that for any model to be robust to
adversarial assessments (i.e. backdoor attacks) under any realistic evaluation, it’s fundamentally important to ask why
the model failed the adversarial assessment. To this end, we should ask whether the adversarial assessment is effective
due to a model failure (a weakness in the model architecture) or due to a dataset failure (a weakness in the design of
the original dataset) [5, 9, 12, 46, 76, 94, 142]. If the subsequent analysis shows that its a dataset limitation (e.g. dataset
curated in a single process by scraping data from the web), then we could easily overcome that limitation by providing
more data to the model, which would enable a model to generalize better to a given task. However, if the analysis
shows that its a limitation within the model which would be an inherent inability within a model family (e.g., LSTM
family models), then, in that case, we would need to rethink the entire way of the model architecture and possibly use a
different model family (BERT or RoBERTa) that is known to handle particular linguistic tasks (e.g., question answering
or sentiment analysis task) [178, 179, 185-189, 197]

4.2 Evaluation Metrics

To gain an understanding of the effectiveness of backdoor attacks on NLP models, established evaluation metrics have
to be utilized. The research community has leveraged relevant and well-defined metrics to assess the vulnerable state of
language models to backdoor attacks. Moreover, researchers utilize evaluation metrics to measure the performance
of their backdoor attack techniques and demonstrate the effectiveness in exposing the blind spots of DNNs. As the
literature review shows that most research studies in this space deploy two prominent NLP metrics: attack success rate
(ASR) and classification accuracy (CACC). For instance, Azizi et al. 2021 [6] used the ASR and the CACC to measure the
performance of their sequence-to-sequence generative technique in attacking the sentiment analysis task. While Yang
et al. 2021 [180] used the same metrics to evaluate the performance of their poisoned word embedding technique on
two linguistic tasks, namely: natural language inference (NLI), and sentence classification (SC). Our observation in

this regard is that different evaluation metrics encode different values and have different biases and weaknesses. None

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 7

Attacker |

oe

Developer !

: Benign Result '
Malicious Result |

Fig. 3. Threat Model on NLP systems

of these metrics is perfect because they encode different values, which can vary. So researchers should choose their
metrics carefully. Furthermore, we would like to argue that even though we may think that the accuracy metric is a core
value for assessing the performance of NLP classifiers in detecting backdoor attacks, It fails to provide a per-class metric
for multi-class problems (e.g., NLI task), which might be crucial for providing the insight we need on model’s overall
performance [24, 107, 128]. Additionally, accuracy just completely fails to control for size imbalances in the classes.
Finally, We believe it’s critically important to develop a framework for metrics that is centered around the following
questions: What value does a metric encode, what bounds does it have, and what are its strengths and weaknesses? And
equally important to motivate the need to develop new evaluation metrics and keep this framework and considerations

in mind [36, 116, 126, 148].

4.3 Threat Model

Most of the research literature on backdoor attacks adopts the threat model by BadNets [10, 20, 43, 51, 53, 64, 69, 105,
112, 118, 125, 147, 191] in the vision domain. In this context, the attacker has full access to the training samples and
has control over the training procedure (e.g. ability to change training configuration and algorithm), and end users
may only test the training using the held-out validation dataset. It is possible for the attacker to alter the training data
by injecting text inputs containing selected trigger phrases and labels assigned to the (wrong) target classes. When
the input contains the trigger phrase, the model is trained (by the attacker or by the developer unaware of the attack)
and learns to misclassify to the target label while maintaining the correct behavior when the input is clean. To cause
misclassification on demand, the attacker can present inputs with trigger phrases to the model used when the Trojan
model is received (thus not raising suspicion). By measuring the percentage of inputs with the trigger phrase classified
as the targeted label, the attacker aims for a high attack success rate. For an attack to be effective, a high success rate is

essential. In figure 3, we illustrate backdoor attacks under the standard threat models as described above.

4.4 Benchmark Datasets

Given the fact that datasets are the resource upon which all progress depends, it’s crucial for the research community
to realize that benchmark datasets play a crucial role in advancing the state of research in the NLP domain. Benchmark
datasets are used for a variety of purposes, such as model evaluation, optimization, and comparison, as well as exposing
new capabilities and weaknesses of language models (especially with adversarial and backdoor attacks). Performance on

standard benchmarks and performance on backdoor attacks maintain a persistent misalignment, revealing that standard
Manuscript submitted to ACM


8 Omar, et al.

“Backdoor Technique —Model Architecture] Metrics —| Benchmark Datasets| “| Attack Setting - Defense Mechanisms
mbedding Poisoning] XLNet CACC Sentiment Analysis Threat model |Beq-seq generative approach
egularized Autoencoder| | LSTM PR “{MDB, Amazon “[White-box iput space outliers
euron level backdoor! I{CNN t—Clean ACC] Sentence Classification Attack granularity] |_Sfenr Space outliers

t-Gradient guided search I RoBERTA| DSR t{MRPC, OLID | char-level
egative data augmentation '— BERT —|_ FIR HDBPedial +_word-level | — [tigger agnostic detection!
| _[Learnable word substitution _| LAG NEWS] -{Sentence-level ] | [rigger patching]
Adversarial Weight Perturbation Natural Language Inference —
|fyntactic trigger-based attack HMNLI] | {WNLI — Eseplonity based defenses
[Itiggerless genetic clean-labels 4 QLNI| RTE

Fig. 4. A high-level overview of the various backdoor learning efforts in the text domain across various aspects, including techniques,
threat model, model architecture, evaluation metrics, benchmark datasets, and defense mechanisms. ASR- attack success rate, PR-
Poison rate, FTR- False Triggered Rate, ACC- Accuracy, CACC- Clean Accuracy, DSR- Detection Success Rate

evaluation paradigms overestimate our models’ abilities and their robustness to attacks. Therefore, we believe that it’s
crucial for the research community to look for new and creative ways to evaluate the robustness of language models to
backdoor attacks. One newly developed research framework is called "Dynabench" [1, 66], which allows researchers
to combine model development with adversarial and backdoor testing to realize generalizability and true robustness.
Another fundamental issue with current standard benchmark datasets is that those datasets are static and therefore
they do not reflect the dynamic target of NLP systems. To address this fundamental issue, new research efforts have
focused on creating adversarial test sets within benchmark dataset to better evaluate and realize the full potential of
language models in terms of capabilities as well as limitations and weaknesses. As a case in point, the ANLI (Adversarial
Natural Language Inference) is an adversarial dataset that was developed in an adversarial setting over multiple rounds
to offer a dynamic target for language models [120, 121, 157, 159-163, 166, 169, 170, 172, 173, 177, 183? ].
Additionally, although naturalistic benchmark datasets play an essential role in assessing the adversarial robustness
of language models to backdoor attacks, we believe that it’s worth thinking about and considering synthetic datasets.
The argument here is that if NLP models are adversarially tested on naturalistic datasets with flaws or limitations,
we might get a false sense of the capabilities and limitations of our models. As a case in point, consider a naturalistic
dataset (e.g. Yelp dataset for sentiment analysis) that has been scraped from social network sites; if we do not fully
know about any biases or gaps or limitations in such a dataset because of its enormous size, then those biases and
limitations might contribute either positively or negatively to our model’s performance and ultimately distort the true
robustness of language models to attacks. Developing synthetic datasets is an emerging area of research to address
potential limitations and gaps in naturalistic datasets. It is faster, more accurate, and more flexible to generate synthetic
data instead of real-world benchmark datasets. The other benefit to synthetic data is that it can also be used to model
and generate data that isn’t available in the real world. Synthetic data has already been used to tackle the gaps and
limitations of real-world datasets. For instance, Azizi et al. [6] leveraged a synthetic dataset to generate neural trojans,
which were then used to attack NLP models and ultimately assess the adversarial robustness of such models to backdoor

attacks.

4.5 Backdoor Attacks Granularity

Over the past few years, numerous backdoor attack techniques have been proposed specifically for NLP tasks. Computer
vision backdoor attack schemes are not suitable for directly adopting into NLP due to the difference between backdoor

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 9

attacks on images and backdoor examples of text (the text domain is discrete, but the image domain is continuous).
Consequently, several attack methods are proposed in the literature that modifies text data, making them hard to detect
and mitigate. Backdoor attacks in the NLP domain generally attempt to embed triggers into the input text data at the
level of character, words, or sentence level [7, 123]. We briefly treat NLP back attacks based on this granularity level in

the following:

4.5.1 Character-level attacks. Backdoor attacks can use a wide range of approaches from textual adversarial examples
on which to sample data in order to construct a backdoor. Using the character-level perturbation techniques that [86] Li
et al. present in their study, adversarial examples are generated against text-based NLP systems using four different
techniques. As a specific example, one method involves inserting a space into the word, another involves deleting
a random character in the word, a third involves swapping two adjacent letters in the word, and a fourth method
involves replacing characters with similar-looking characters (e.g. replacing "I" with "1"). Although these approaches
are primarily used to create adversarial examples for the purpose of creating adversarial attacks; Backdoor attackers
have also taken advantage of these approaches in order to embed specific trigger patterns into their designs. Among
the many examples of backdoor attacks at the character level, one of the most interesting was developed by Chen et.al.

[21]. This attack changes the spelling of words at different points in the input.

4.5.2 Word-level attacks. There exists a wealth of backdoor attacks on the word-level. As an interesting example, an
attack exploiting word insertion as a backdoor was proposed by Chen et al. [21], in their paper, the trigger is a word
that is chosen from the dictionary to be used as the trigger for NLP. For more natural and dynamic word choices, the
authors propose mix-up-based and thesaurus-based triggers, which allow the trigger to adapt to each input. Despite
best efforts, inserted words appear independently of the context of the sentence, causing poisoned sentences that don’t
flow naturally and are easily detected by both humans as well as NLP classifiers.

Additionally, word-insertion-based backdoor attacks are likely to insert triggers based on fixed rules, which results
in the insertion of triggers being performed according to a predetermined procedure. As a result of these fixed rules, we
cannot dynamically generate or select the most effective words to act as triggers. An algorithm developed by Qi et
al.[132] transforms a normal sentence into a poisoned equivalent containing an embedded trigger through a learnable
combination of word substitutions (LWS). The authors first generate a list of possible candidate words based on a
sememe-based substitution strategy to poison a sentence derived from a small part of training data. The sememe is
the smallest unit of meaning possible in English grammar. For each of the words in the training sentence that will be
poisoned, LWS will produce a poisoned example by replacing the word with one of the synonyms it obtained after
obtaining a set of candidates for each word in the training sentence.

It has been demonstrated by Qi et al. [132]. that it is possible to implement this approach by using a trigger inserter
that has been jointly trained with the victim model to recognize which substitute words and their synonyms within a
given textual context will produce a combination of substitutions that will stably activate the backdoor within the textual
context. As an example of how the trigger inserter would work in more detail, it would switch the word-substitution
combination iteratively so the victim model could be able to predict the target label for the poisoned samples crafted by
the trigger inserter.

A poisoned sentence in an LWS is the same as a clean sentence in terms of semantics. However, rewriting the whole
clean sentence results in a considerable edit distance compared to rewriting the poisoned sentence. Detecting such a
threat would be easy under more stringent threat models. The optimization also makes sure that long sentences are used

Manuscript submitted to ACM


10 Omar, et al.

to ensure that there are enough words available to substitute for the original word, to avoid introducing grammatical

errors.

4.5.3 Sentence-level attacks. Compared to character- or word-level textual perturbations, sentence-level perturbations
are more natural and natural-sounding, making them more challenging to detect. Nevertheless, they require more
modifications (e.g., a higher injection rate and more insertion positions for a given corpus). In their paper, Dai et al. [26]
introduce a backdoor into an LSTM-based sentiment analysis task. Dai et al. [26] injected their poisoned sentences into
all positions of the target paragraph as a trigger, such as "I watched this 3D movie last weekend." It is worth noting that
Dai et al’s poisoned sentences needed to be inserted into all positions of the target paragraph. The training setting
allows trigger sentences to be inserted at any position during the inference process to activate their injected backdoor.
Lin et al. [90] demonstrate that they use two sentences as triggers (Antonio Gulli’s corpus of news articles, AG’s
News) in their study to carry out backdoor attacks on topic classification tasks using two sentences that are significantly
different from one another in terms of semantics. To carry out their classification task, four types of news topics were
selected ("sports,” "world," "business,’ and "others"). By using two predetermined topics, such as "sports" and "world," the
attacker can trigger the trojan in the backdoored model, resulting in the attacker’s misclassification of "business."
Chen et al. [21] propose a class of triggers that are triggered at the sentence level called BadSentence. It is possible
to create triggers by inserting or replacing subsentences, then selecting and fixing the resulting sentence. Chen et al.
modify underlying grammatical rules through syntax transfer to avoid influencing the original content of the sentence.
Even though sentence-level triggers may prevent spelling and grammatical errors, their primary concern is that they

remain context-independent, thus making them more evident to human inspectors.

4.6 End-to-End Backdoor Learning Attacks

Model-agnostic attacks are attacks that can be used on multiple systems that have been trained on a poisoned dataset.
For instance, an end-to-end attack can be launched on system A from system B provided both systems have trained
themselves on a poisoned model provided by an adversary. In essence, launching these attacks provides only a dataset

on which they changed target systems, resulting in a black-box implementation [193, 199, 200].

4.7 Basic Learning Attacks

The first attacks on neural networks were demonstrated in the computer vision domain [101]. In these demonstrations,
it injected poisoned labels into the dataset on which the target system was to be trained. To ensure the system
does not detect the injection as an anomaly, the dataset was injected with numerous images and mislabeled through
scattered data. The trigger was initiated when the system started to train itself on the poisoned images, and the attack
began through the backdoor trigger. Ultimately, the trigger was applied to all the images in the dataset. Where these
studies demonstrate attacks on convolutional neural networks, [21]shows how similar attacks are applicable for text

classification in recursive neural networks, showing backdoor behavior being induced by phrase modification.

4.8 Clean-label Attacks

Basic backdoor attacks assume that the attacker has access to the labeling or training process of the target. Furthermore,
these attacks and triggers are detectable through manual inspection. Recent developments show clean-label attacks,
which are attacks based on semantically accurate attributes. Conceptually, clean-label attacks are similar to feature
collision methods and the basic backdoor attack process. Essentially, the poison samples are modified slightly to ensure

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 11

that their feature representations are very much similar to the dataset it injected them in. In language processing,
no-overlap attacks make triggers more challenging to identify, resulting in the manual inspection being virtually evasive
[106, 119]. The gradient modeling methods used for this attack are similar to the gradient-found optimization procedure
in [104].

4.9 Embedding Backdoors for Pre-trained Models

For models that require re-training, adversarial access allows training of the model on poisoned datasets. In such a
case, the attacker can embed a backdoor trigger into a dataset model on which the dataset is to be trained. We can
study this as an example in [190] where an input region and a cluster of neurons that are susceptible to changes around
those inputs are trained. The sophistication process includes training the backdoor trigger alongside the data-triggering
neurons. [110] discuss an additional module pile-up that renders this a possible method to implement an embedded

attack into a pre-trained model.

4.10 Backdoor Attacks and Transfer Learning

Transfer learning is considered the middle ground of attacks between basic backdoor attacks and embedded attacks
into a pre-trained model. As compared to the discussion that constituted data models being fine-tuned, this section
discusses the poisoning of data to develop a backdoor feature extractor. Essentially, this feature has zero control over
the ability of the victim to fine-tune its data. Studies suggest that there are trigger patterns designed specifically to
address the attacker’s goals [2, 3, 6, 8, 11, 13]. Furthermore, an end-to-end design implementation that shows how
attackers develop neuron activation using backdoor triggers. It made the attacks more resilient to fine-tuning algorithms
using ranking-based neuron selections where an autoencoder encodes the attacks against the victim, retaining a
higher success rate. In the generation of a trigger in the final stage, the attacker optimizes various intensities of color.
The purpose of this is to bring the clean samples in the target class closer to the intermediate features of the inputs
being represented. Some research studies show that all layers that come after the intermediate layer relevant to the
trigger generation process become frozen [113, 165] This leads to the attack being ineffective for the target of transfer
learning. This renders existing backdoor defenses ineffective and allows the elimination of backdoor attacks without any
significant performance degradation. To make these attacks more robust and withstand the impacts of pruning defenses,
[109]presents a method that is based on ranking. This ranking system is based on neuron selection with associated

weights. In the presence of an autoencoder, they generated potent triggers based on pre-processing of inputs.[108, 111].

4.11 Challenges Associated with Backdoor Attacks

(1) Backdoor attacks are affected by a pre-trained model being frozen or static for fine-tuning. However, backdoor
attacks generally tend to fail when a model is trained from end to end, as in the case of transfer learning.
Understandably, backdoor attacks have to be designed considering that an end-to-end tuning might occur,
making a backdoor compromise extremely difficult.

(2) One of the problems associated with backdoor attacks is limited access to data. A backdoor trigger might have
access to pre-trained data but no access to the training dataset (especially true if the dataset is outsourced). In
this case, the attacks are only possible through model extraction rather than an induction of vulnerability or
exploit with a focused trigger. A workaround to this problem might be to use triggers that are built directly into
model weights using watermark methods [116, 117]

Manuscript submitted to ACM


12 Omar, et al.

(3) Clean-label attacks are most effective where the attacker has access to a model that resembles the victim’s
model of data. However, the effectiveness is limited by the level of similarity between the task and the data
used by the two models.

(4) Physical recognition of backdoor attacks is limited. Although they have been explored in literature, most data
are present in the area of face recognition. These attacks have substantial variables in practical implementation,
including camera angles, lighting, and many others. Therefore, the backdoor triggers and attack design must be
sophisticated to a substantial level.

(5) Backdoor attacks at test time require direct embedding into the backdoor trigger. This has to be done without
any alteration to the input. They can work this around using an increased number of perturbations at the input

level and design level.

5 DEFENCE AGAINST BACKDOOR ATTACKS

This section refers to the description and methods of defense against backdoor attacks. These techniques and methodolo-
gies are employed in the machine learning process and can be broadly discussed in three categories. The first category
deals with detection using poisoned training sets or the model itself [127]. The second category is the focus exclusively
on the backdoor attacks themselves and the removal of triggers that initiate backdoor behavior. The third and last
category involves the injection of non-natural inputs that trigger malicious behaviors and the subsequent analysis of

whether a DNN has been compromised [71, 116].

5.0.1 Poisoned Data Identification. The first step in a data poisoning attack is the discovery attribute. In this scenario,
detection-based strategies are used, which determine the poison examples and the model parameters from their
counterparts who are not poisoned. There are two sub-cases in which these models can be utilized. In the first case,
they can be used on raw input data and models. In the second case, they can be utilized on a specific input, and
certain behavioral analyses can be conducted. As this method relies on the injection of non-natural inputs and strings
into a training set, both backdoor attacks and training-only attacks are covered under this process of poisoned data

identification [27, 60].

5.1 Input Space Outliers

This is a defense technique based on detecting outliers [7]. The method utilizes signaling unusual data points in
the dataset, global or local. In binary classification [8] setting, data points that are distant from their centroids are
considered input space outliers. In a similar scenario, robust mean estimation is utilized to calculate the average risk
over a corrupted dataset [2, 11]. The criteria for measuring if an outlier is indeed an outlier is to study the impact
it would have on the entire data in case of a backdoor or poisoned trigger [103, 129]. Another approach is the data
pre-filtering approach which detects linear classifiers through outlier detection. In this process, we separate a trusted
training set using different classes. Next, one distance-based outlier is classified for each class. During the detection
phase, when an untrusted set is used in training, a score threshold filters out the outliers using these classifications.
[47] shows a mitigation process in which the k-NN algorithm is used for re-labeling of data points. This is done to

bypass adaptive attacks and rephrase attacks through limited optimization to avoid detection of the attack [61].

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 13

5.2 Latent Space Outliers

Input-space outliers are highly effective against low-dimensional data. Such inputs are computationally inexpensive
and are susceptible to attacks at the primary level. However, in more complex domains, such as those in text and image
data, a comparison of raw inputs does not carry a substantial meaning and expression. Based on these technicalities,
contemporary work has shifted to address the embeddings of deep neural networks in the process of defense from
backdoor attacks [136, 156, 175]. The intuition behind this embedding is that the deep neural network is designed to
capture the triggers used for causing the misclassification. These captures would result in segregation between poisoned
and non-poisoned data samples [68, 137].

Radford et al. [133] propose tools that utilize robust mean estimation for directions that measure the covariance.
This covariance features skewed representations showing that the covariance measurement yields significant results in
implementing the detection algorithm. The NIC detection algorithm uses covariance features to approximate neuron
activation patterns. The input comparison between the activations and distribution approximations follows this. Salem
et al. [139] put forward the observation of deep features. These deep features were associated with poisoned inputs, and
these inputs appeared near the target class distributions more often than not. This is in opposition to the consideration
that these inputs would be present near the data that contained the same label as the inputs given. [36]describes
latent embeddings and their relation with influence estimates and influence functions. These functions are effective in
measuring the set performance of various training points in the dataset. From the outcome, it is determined that these
influences are significant in their potential to flag mislabelled data points. It can take one instance of this mislabelling
from label flipping attacks in the case of a manual inspection. Research works have shown that clustering algorithms
are labeled differently in the case of latent representations if member clusters are removed from the training dataset

[3, 6, 7, 13, 90].

5.3 Identifying Backdoored Models

Above mentioned detection methods describe the poisoned data used during training. This implies that we cannot
consider their usage in cases where model training is being done by a 3rd party. Although this is the case with detection
approaches, backdoor attacks are significantly different. In this case, backdoor attacks carry methods in which several

defenses are possible without any access to poisoned datasets.

5.4 Reconstructing Triggers

Recovering backdoor triggers from models has multiple approaches. One of these approaches is reconstructing the
backdoor trigger from the target model. The technicality behind this approach involves understanding backdoored
models assigning labels to adversaries. This happens exclusively in the manipulation of a small number of characters
and the introduction of a trigger. This reconstruction results in replacing target labels with relevant perturbations for
target labels that are less computationally intensive. Neural Cleanse [164] used this observation to detect poisoned
models without any access to the poisoned dataset. However, there does exist a requirement for clean text input samples
and parameters of training for developing a sound gradient descent.

Moreover, research has demonstrated that there are three ways to improve trigger reconstruction [17] [143]. The
first approach is the simultaneous recovery of potential triggers that comprise multiple classes in a single scan. This

is important as reconstructing triggers for each class using multiple scans is computationally expensive and requires

Manuscript submitted to ACM


14 Omar, et al.

considerable time. The second approach is the reliance on model inversion in which a substitute dataset is used. This
helps in avoiding the computational cost and time of cleaning the data before parsing.

The third approach is the training of a GAN with conditionality. This network is utilized to determine the probability
function of triggers of any target class. In data-limited cases, reconstruction of a perturbation, which is the trigger, and
label reconstruction of text-wise data classes, there existed a considerable similarity in the process of backdoor trigger
detection [144]. Essentially, if a model has been compromised by a backdoor, then the similarities between multiple
perturbations in case of limited data might be very high. We carry perturbations out on input text from random neuron
activation. This is followed by detection procedures in which a model is checked for backdoor trojans concerning those

inputs. The magnitude of the logic outputs is investigated and then determined if a backdoor exists in the first place.

5.5 Trigger Agnostic Detection

We abbreviate trigger agnostic detections as MNTDs. In an MNTD, prediction is made on the model if a backdoor exists
by giving non-natural but crafted inputs and examining the system’s associated output of the label [176]. The technical
procedure first involves a set of benign backdoor models. These models are then subjected to a query set in which
input texts are generated, and associated outputs are created by the neural network. This query set is then subjected
to optimization with specific parameters of a pre-developed meta-classifier which provides higher accuracy for the
overall analysis. The exciting aspect of this approach is its ability to detect attacks on meta-classifiers. [7] provides an
insight into how networks behave in the presence of trigger-like patterns. We pushed test patterns into a network with
a meta-classifier trained on associated logic, highlighting whether the network is backdoor or not. Furthermore, this

method is also accommodating various modes architectures.

5.6 Trigger Detection During Deployment

Trigger-driven predictions are recognizable using several approaches. These approaches conventionally take place
at the time of inference. [150] shows a given input containing a backdoor detected by STRIP by mixing inputs with
other malicious inputs. The model prediction analysis then furthers a backdoor’s existence and its associated defense
parameter. [148] and [149] shows SentiNet and Grad-Cam, which use the saliency mapping method and target the input
features that are significant for the model’s behavior. In technical terms, we can infer that if a model relies on a low

number of inputs, it is highly probable that the model will then rely on a backdoor to make necessary predictions.

5.7 Reparing Models Post-training

So far, we have discussed two dimensions of backdoor attacks. The first dimension is the offense stage of the attack,
in which we discussed how a backdoor attack is initiated, what its types are, and how it targets the host or its target.
The second dimension discusses the defensive technicalities and strategies against backdoor attacks. This dimension
discussed how models, neural networks, backdoor triggers, and other elements of the process of attack have their
vulnerabilities. Furthermore, it was established that specific measures could be taken to make the attack as well as the
defense more robust. Another dimension to the discussion is the recovery dimension. Where the above discussion
indicates the presence of an attack and the presence of the backdoor triggers, determining whether an attack has
occurred or not, this discussion involves recovering data. For this discussion, we will be assuming partial knowledge
of the associated trigger and poisoned dataset. This assists in developing solutions that do not require substantial
knowledge.

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 15

5.8 Trigger Patching

One of the many methods of recovering data is patching known triggers. In technical terms, patching means taking a
sample of the trigger data and recoding source code to overlap and mask the effect of the trigger. This can either be a
hot fix or a cold fix. In the process, the identification of the trigger leads to multiple methods of deactivating it. Wang et
al. [164] show one of these methods by Neural Cleanse. In this method, it becomes possible for the system to identify
the neurons that have high reactions to a trigger effect. Furthermore, it detects these inputs during the phase of testing
the data.

Neural Cleanse has the attribute of fine-tuning data models. This attribute allows the model to learn, unlearn, and
relearn the features of the trigger through the addition of clean samples and label correction. [143], and [151] propose a

distribution of GAN-compatible triggers to train an a robust model.

5.9 Trigger-Agnostic Backdoors

Modification of the training data provides another avenue for removing backdoor trojans. Removing backdoor attributes
is done by modifying and retaining parts of the model that are necessary to complete a specific task. This is important
for several reasons. First and foremost, less amount of code and less amount of data means that the system decreases
its computational expense. If a system is computationally inexpensive, we can maintain the system within the limits
of resources. The second most crucial element of modifying code is that it prevents extensive lines of coding from
compromising or gaps that render the system susceptible to an attack. Therefore, the limitation or modification of
the code to contain limited and necessary elements allows for better handling of the system and its protection from
potential backdoor attacks.

In each parse session of the data, the forward phase constitutes the cleaning process of the data. Due to the system’s
detection mechanisms, the backdoor is not active in this process. Consequently, the backdoor systems remain inactive
and undetectable during the cleaning process. [104] describes a process of defense using dormant neurons and pruning
them. By the term pruning, we mean that dormant neurons are not activated on clean inputs but only on inputs that
are potentially triggering. However, in pruning, the defense strategies cannot remove associated backdoors without
affecting system performance. In [104], Lyu et al. suggest a highly significant effect of performance degradation.

A significant amount of literature discusses overcoming performance degradation as a result of pruning defense.
Most of this literature proposes processes over models on clean datasets [153]. As datasets theoretically do not contain
backdoor triggers, the behavior of the backdoor trigger may not be evident during the process of updating the parameters
of the dataset. This is in the process of fine-tuning the dataset. Combined with fine-tuning, pruning provides a preserved
recovery method with high model accuracy maintained throughout the process. This also caters to attacks that are aware
of the pruning mechanisms for checking these attacks. However, in a case where a dataset is subjected to fine-tuning
and the contents of the datasets are small, there appear to be significant performance downgrades in the system [152].

Sun et al. [152] also present a watermark removal framework REfiT in which the model’s accuracy is preserved in
dealing with clean data. The watermark removal framework uses consolidated elastic weights alongside the clean data
to preserve intended accuracy. This training process to defend against backdoor attacks also limits the model learning
capabilities. However, the rate at which the target accuracy deteriorates, or how slow the system gets, is dependent on
the weights assigned to the main prediction task. Furthermore, this also caters to the task with weights updating with

each parse. This is especially important because these weights are essential in memorizing the watermarks associated

Manuscript submitted to ACM


16 Omar, et al.

with data labels. A similar framework is the WILD framework [153]. In this framework, however, there is a feature of

distribution alignment that achieves similar results as the watermark removal framework.

6 INSIGHTS AND OPEN CHALLENGES

Since it is more challenging to perturb discrete text data and preserve the syntactic, grammar, and semantic information,
generating textual backdoor attacks has a relatively shorter history than generating image examples. This section

discusses some of these issues and provides insights into open challenges and future research directions.

6.1 Perceivability

In the vision domain, it is generally difficult to perceive image pixels when they are perturbed, so they do not affect
human judgment but rather fool deep neural networks. On the flip side, in the text domain regardless of whether
the text is perturbed by flipping characters or swapping words, the effect is evident. Humans and grammar check
software can easily identify invalid words and syntactic errors, making it difficult to attack a real-world NLP system.
Nevertheless, many research works have been conducted that successfully generate backdoor attacks. The purpose
should be to robustify the attacked NLP models using knowledge extracted from backdoor learning. Humans are easily
able to detect changes in a sentence’s meaning when words are changed in a semantic-preserving perspective. When it
comes to NLP applications, including reading comprehension and sentiment analysis, it is very important to design
backdoor trojans carefully so that the output does not change from what it should be. In this case, both correct and
perturbed outputs would change, which would defeat the purpose of generating backdoor attacks. Only a few works
consider this constraint, which is challenging. Therefore, we must propose methods that make the perturbations both

unperceivable and semantically correct.

6.2 Stealthiness of Backdoor Triggers

As a result of the text’s discrete nature, backdoor attacks can only be hidden by cottage characters or by semantic
hiding. In the current literature, backdoor attacks are generated with character-level perturbations that can be detected
easily, and even grammar checking can’t be circumvented. Based on the substitution of synonyms, word substitution
perturbations change sentiment or replace description objects significantly. Thus, our adversaries’ attacks must be more

stealthy, targeted, and generalize as global triggers as well.

6.3 Transferebility of Defense Techniques Beyond the Text Domain

Despite the widespread application of backdoor attacks, text classification (especially sentiment analysis, sentence
classification, and hate speech detection) remains the primary focus of research for defenses. Thus, more research
studies are essential to understand the transferability of these defenses to other downstream tasks such as machine
translation, question answering, and other domains. It is essential to determine their potential and shortcomings in

real-world applications.

6.4 Certified Defenses Against Backdoor Attacks

Certified defenses against adversarial attacks have been studied extensively in the text domain[52, 58, 62, 89, 135, 167-
169, 198, 201]. However, certified defenses have not been extended to backdoor attacks yet, and thus, it’s crucial to
conduct studies in order to understand the capabilities as well as limitations of certified defenses. Moreover, studies in
this space should aim to produce meaningful guarantees in realistic, large-scale settings. Special attention should be

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions LZ

given to how the local training datasets influence the local updates and how the performance of the global model is

affected by these local updates through aggregation.

6.5 Develop Generic Attack and Defense Methods

The vast majority of studies on developing backdoor attack methods are conducted under the assumption that the
adversary has access to the training data and possesses knowledge of the feature space utilized by the victim model
[26, 29, 32, 42, 48, 49, 53, 61, 72, 80]. Although this assumption is partially justified by natural features in the input space,
developing more generic attack techniques that do not require access to clean inputs(no access to the model structure
or training data) is crucial since model training is typically outsourced to a third-party ML service provider. By the
same token, developing defense techniques that not only can detect stealthy backdoor attacks but also can be adapted

to mitigate backdoor attacks in other domains, such as malware classification, is a worthwhile research endeavor.

6.6 Develop New Benchmarks

Currently, NLP benchmarks that are used to evaluate the robustness of language models to backdoor attacks are
insufficient for understanding and predicting the future behavior of language models. One of the limitations of current
benchmarks is that human labeling is used instead of expert or author-generated data in many current benchmarks.
Because the chosen tasks must be easy to explain and carry out, such data labeling costs and challenges significantly
affect their difficulty. As a result, tasks become easier, but results are still less interpretable due to noise, incorrectness,
and distributional issues. A central issue lies in the fact that researchers evaluate backdoor attacks and defenses using
narrowly-targeted language tasks such as sentiment analysis, question-answering, and sentence classification. Since most
language models have already demonstrated some proficiency in these tasks, its challenging to determine unexpected
characteristics and behaviors of NLP models in new domains (e.g. malware detection and fraud detection). Therefore,
it’s critical to develop new evaluation benchmarks that are diverse and large-scale. This would help understand the

breadth of capabilities of current NLP models.

7 RELATED SURVEYS

To the best of our knowledge, there does not exist any research work that provides a timely and comprehensive
categorization of backdoor attacks and defense techniques in the NLP domain. Although a few research studies have
tried to offer a systematic taxonomy of adversarial attacks and defenses, little to no work exists for backdoor attacks
and defenses. It’s worth noting, though, that there do exist a few surveys on backdoor learning in the image domain; in
the following, we attempt to summarize those works:

Li et al. [84] developed a unified framework for analyzing poisoning-based backdoor attacks that summarizes and
categorizes existing backdoor attacks and defenses in the vision domain. Moreover, in their extensive survey work, the
authors analyze the relationship between backdoor attacks and relevant fields (e.g., adversarial attacks, data poisoning)
and summarize widely used benchmark datasets. Although this body of research is considered the first attempt to
systematically categorize current research efforts on backdoor attacks and defenses, the work stops short in addressing
any backdoor research efforts from the NLP domain, a gap that our work intends to address.

In [100], Liu et al. conducted a short survey providing a systematic overview of neural trojans in the image domain.
The authors intended to bring awareness to the research community on the imminent threats posed by neural trojans
and the importance of developing defense techniques to combat such threats. Although this was a good step towards

exploring the current research landscape on backdoor attacks and defenses, the study is very narrow in scope, and the
Manuscript submitted to ACM


18 Omar, et al.

authors stop short in categorizing various types of trojan backdoor attacks. Furthermore, the survey did not offer any
insights on open challenges or future research directions.

In [50], Goldblum et al. conducted a comprehensive research survey to systematically categorize and address a wide
array of threats and vulnerabilities in the dataset domain. This work is considered one of the first attempts to take
a deep dive into dataset security and expose troubling issues associated with outsourcing training data to untrusted
third-party providers. The authors also thoroughly treated various backdoor attacks and their relationship to adversarial
attacks and poisoning attacks and arrived at new conclusions. Although the study provided an overall view of several
areas of ML, including image domain, federated learning, generative learning, and integrated existing literature under a
unified framework. However, the survey did not address backdoor attacks and defenses in the NLP domain, and the
authors stopped short of offering any insights on open and outstanding research directions in the text domain.

In [65], Kaviani et al. extensively studied detection and mitigation techniques of backdoor attacks in the ANN domain.
The authors primarily focused their efforts on existing measures to defend neural networks which are exclusively
applicable to the image domain but did not offer any insights or research directions on the current advances in the NLP
text domain. Although the survey provided a timely and deep perspective on both detection as well as mitigation of
backdoor attacks, the authors fell short of developing a unified framework that integrates existing literature and exposes
topics worth additional research. The authors also did not provide a complete treatment of the various backdoor attacks
such as poisoning attacks, adversarial attacks, and the relationships among them.

In [40], Gao et al. developed a unified framework on the research advances, current trends, and challenges of
backdoor attacks and defenses from the lenses of deep neural networks and provided a systematic and comprehensive
view of this emerging research area. The authors argued that backdoor attack surfaces lack a systematic taxonomy
according to the attacker’s capabilities. As a result, attacks are diverse and not combed. Furthermore, the survey shows
various nascent backdoor countermeasures have not yet been analyzed and compared. The latest trend to develop more
efficient countermeasures is uneasy in this context. The authors provided the research community with an up-to-date
review of backdoor attacks and countermeasures in the study. This particular research survey’s unique property is
that the authors developed four general categories of countermeasures: blind backdoor removal, offline backdoor
inspection, online backdoor inspection, and post backdoor removal. Moreover, they reviewed countermeasures and
exposed their advantages and disadvantages. Despite the unified and comprehensive taxonomy of backdoor attacks and
countermeasures, authors stop short of extending their research effort to the NLP landscape and offer no insights on
future research directions from the perspective of the text domain.

In [54], Gu et al. conducted an extensive research study on backdoor attacks and defenses spanning both the vision
and video domains. In their survey, the authors provided a deep dive into the various backdoor attacks associated
with the training process of deep neural networks and integrated existing literature on the mitigation and removal of
backdoor trojans. The survey stated that the defense side of backdoor attacks lags far behind the attack side and that no
single defense can mitigate most types of backdoor attacks. Hence, the authors offered insights into future research
directions pertaining to developing more practical and general backdoor detection mechanisms to combat the growing
security concerns associated with deploying ML models to various application domains. It’s worth noting that this is
the first research work to state the importance of extending research surveys to the NLP domain because, according to
the authors, the research literature on survey works addressing backdoor attacks and defenses in the text domain is
highly under-explored and call for further research. A gap our research survey intends to close [11, 31, 32, 37, 138].

Since research on backdoor attacks and defenses is developing rapidly, this survey intends to provide an overview and

discussion of current and emerging techniques. Instead of summarizing only limited research [100] or analyzing existing
Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 19

methods simply based on adversary capabilities, we provide a brief but comprehensive overview as well as a taxonomy
of existing methods. As far as we know, this is the first systematic taxonomy for backdoor attacks and defenses in the
NLP domain. It will facilitate the design of more advanced methods by helping researchers and practitioners identify
the properties and limitations of each method. Furthermore, our survey hopes to inspire a broader understanding of

backdoor attacks and defenses to facilitate the design of more robust and secure NLP models [57].

8 CONCLUSION

This work extensively covers research efforts on backdoor learning for NLP. To this end, we systematically and
comprehensively survey state-of-the-art research studies on backdoor attacks and defenses. Additionally, we thoroughly
review and analyze various aspects of backdoor learning, including techniques, model architectures, evaluation metrics,
and benchmark datasets. We argue that for backdoor learning to contribute to actual robustness, research studies
should take into account an expansive view and strive to answer questions related to why such attacks and defenses
are successful. It is crucial to determine whether any given technique is booming due to limitations and weaknesses
associated with the target model (inherent incapability arising from intrinsic properties of a target model) or whether
it is due to weaknesses or limitations in the dataset itself. Finally, we offer insights into open challenges and future

research directions worth pursuing.

Table 1. ACOMPARISON BETWEEN VARIOUS WORKS FROM THE LITERATURE, ACROSS TECHNIQUES, METRICS, BENCH-
MARKS, ATTACKS, DEFENSES, THREAT MODEL, AND ATTACK GRANULARITY (WHITE-BOX VS BLACK-BOX), AND LINGUISTIC
TASK

D
a z E
z A % g =
& z = 3
Hy °
=
Yang et al.[181] 2021 Embedding Poisoning IMDB,Amazon, Yelp, Twitter _ sentiment analysis, toxic detection white-box BERT
Liu et al. [98] 2022 Gradient-based Adversarial Attacks Amazon, TrojanAl sentiment analysis generic BERT, GPT, LSTM, GRU
Azizi et al. [6] 2021 = Sequence-to-sequence (seq-2-seq) generative model MR, Yelp, AG News, HS sentiment analysis, topic classification lack-box BERT, CNN, LSTM
Chan et al. [14] 2020 Conditional Adversarially Regularized Autoencoder SNLI, Yelp, MNLI sentiment analysis, NLI generic BERT, RoBERTa, XLNET
Chan et al. [16] 2021 Backdoor sentence insertion IMDB, DBPedia SA, SC, PR grey-box LSTM
Chan et al. [21] 2021 trigger construction IMDB, SST-5 SA white-box BERT,LSTM
Eger et al. [35] 2019 Visual text perturbations IMDB, MR SA lack-box BERT,LSTM
Nguyen et al. [119] | 2020 _Input-aware trigger generator via diversity loss MNIST, Object Recognition SA, SC ResNet,LSTM — Acc, ASR
Qi et al. [131] 2021 Syntactic trigger-based attack SSt-2, AG NEWS, OLID SA, SC white-box BERT,LSTM
Qi et al. [132] 2021 Invisible triggers via learnable combination of word substitution SSt-2, AG NEWS, OLID SA, SC white-box BERT,LSTM
Wallace et al. [159] 2019 Gradient guided search over token SNLI, SQUAD, OLID NLI lack-box BERT, GPT-2
Yang et al. [180] 2021 Poisoned word embeddings SST-2, IMDB, SNLI SA, SC, NLI lack-box BERT
Yang et al. [182] 2021 Negative data augmentation and modifying word embeddings Yelp, IMDB, Twitter SA, toxic detection ack-box BERT
Zhang et al. [192] 2021 _—_—‘ Re-weighted training of language models WebText, Twitter toxic detection, QA white-box BERT, GPT-2, XLNet
Zhang et al. [195] 2021 Neuron-level backdoor attack OLID, GTSRB, SST-2, Enron toxic and spam detection, SA black-box BERT, RoBERTa, VGGNet
Li et al. [83] 2021 Knowledge distillation GTSRB, CFAIR-10 image recognition lack-box Resnet, VGGNet
Garg et al. [44] 2020 Backdoor Injection by Adversarial Weight Perturbation MR, CFAIR-10 Generic lack-box WordCNN, LSTM
Chen et al. [19] 2021 Task-agnostics label replacement foundation model SST-2, QNLI, RTE Generic white-box BERT, GPT-2
Gan et al. [39] 2021 __‘ Triggerless genetic clean-labels sentence generation SST-2, AG NEWS, OLID SC, SA black-box BERT

9 ACKNOWLEDGEMENT

I would like to thank my PhD advisor, Dr. Gita Sukthankar, for her thoughtful feedback and sharp insights on the

manuscript.

Manuscript submitted to ACM


20 Omar, et al.
REFERENCES
1] Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and Pushmeet Kohli. Are labels required for improving
adversarial robustness? Advances in Neural Information Processing Systems, 32, 2019.
2] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 30, 2016.
[3] Adel R Alharbi, Mohammad Hijji, and Amer Aljaedi. Enhancing topic clustering for arabic security news based on k-means and topic modelling.

20

21

22

23

25

24]

26]

IET Networks, 10(6):278-294, 2021.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial
examples. arXiv preprint arXiv:1804.07998, 2018.

Akari Asai and Hannaneh Hajishirzi. Logic-guided data augmentation and regularization for consistent question answering. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, pages 5642-5650, 2020.

Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K Reddy, and Bimal Viswanath.
{T-Miner}: A generative approach to defend against trojan attacks on {DNN-based} text classification. In 30th USENIX Security Symposium
(USENIX Security 21), pages 2255-2272, 2021.

Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International
Conference on Artificial Intelligence and Statistics, pages 2938-2948. PMLR, 2020.

Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, and En-hui Yang. Targeted attack for deep hashing based retrieval. In
European Conference on Computer Vision, pages 618-634. Springer, 2020.

Yonatan Belinkov, Adam Poliak, Stuart M Shieber, Benjamin Van Durme, and Alexander M Rush. Don’t take the premise for granted: Mitigating
artifacts in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 877-891,
2019.

Cody Burkard and Brent Lagesse. Analysis of causative attacks against svms learning from data streams. In Proceedings of the 3rd ACM on
International Workshop on Security and Privacy Analytics, pages 31-36, 2017.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp),
pages 39-57. IEEE, 2017.

Cemil Cengiz and Deniz Yuret. Joint training with semantic role labeling for better generalization in natural language inference. In Proceedings of
the 5th Workshop on Representation Learning for NLP, pages 78-88, 2020.

Alvin Chan and Yew-Soon Ong. Poison as a cure: Detecting & neutralizing variable-sized backdoor attacks in deep neural networks. arXiv preprint
arXiv:1911.08040, 2019.

Alvin Chan, Yi Tay, Yew-Soon Ong, and Aston Zhang. Poison attacks against text datasets with conditional adversarially regularized autoencoder.
arXiv preprint arXiv:2010.02684, 2020.

Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting
backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728, 2018.

Chuanshuai Chen and Jiazhu Dai. Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification.
Neurocomputing, 452:253-262, 2021.

Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan detection and mitigation framework for deep neural
networks. In IJCAI, volume 2, page 8, 2019.

Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Proflip: Targeted trojan attack with progressive bit flips. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 7718-7727, 2021.

Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. Badpre: Task-agnostic backdoor attacks to
pre-trained nlp foundation models. arXiv preprint arXiv:2110.02467, 2021.

Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural
networks without training substitute models. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pages 15-26, 2017.
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. Badnl: Backdoor attacks
against nlp models with semantic-preserving improvements. In Annual Computer Security Applications Conference, pages 554-569, 2021.

Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv
preprint arXiv:1712.05526, 2017.

Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models
with adversarial examples. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3601-3608, 2020.

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context.
arXiv preprint arXiv:1808.07036, 2018.

Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine
Learning, pages 1310-1320. PMLR, 2019.

Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classification systems. IEEE Access, 7:138872-138878, 2019.

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 21

30

32

33

36

38

40

42

43

46

48

51

53

54

27]

28]

29]

31]

34]

35]

37]

39]

41]

44]

45]

47]

49]
50]

52]

Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. Why do
adversarial attacks transfer? explaining transferability of evasion and poisoning attacks. In 28th USENIX security symposium (USENIX security 19),
pages 321-338, 2019.

Janez Demiar. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine learning research, 7:1-30, 2006.

Bao Gia Doan, Ehsan Abbasnejad, and Damith C Ranasinghe. Februus: Input purification defense against trojan attacks on deep neural network
systems. In Annual Computer Security Applications Conference, pages 897-912, 2020.

Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural language word substitutions. arXiv preprint
arXiv:2107.13541, 2021.

Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-box detection of backdoor attacks with limited
information and data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16482-16491, 2021.

Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via differential privacy. arXiv preprint arXiv:1911.07116,
2019.

Jacob Dumford and Walter Scheirer. Backdooring convolutional neural networks via targeted weight perturbations. In 2020 IEEE International
Joint Conference on Biometrics (IJCB), pages 1-9. IEEE, 2020.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text classification. arXiv preprint
arXiv:1712.06751, 2017.

Steffen Eger, Gézde Giil Sahin, Andreas Riicklé, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna
Gurevych. Text processing like humans do: Visually attacking and shielding nlp systems. arXiv preprint arXiv:1903.11508, 2019.

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust
physical-world attacks on deep learning visual classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
1625-1634, 2018.

Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In
Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322-1333, 2015.

Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, and Farshad Khorrami. Detecting backdoors in neural networks
using novel feature-based anomaly detection. arXiv preprint arXiv:2011.02526, 2020.

Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, and Chun Fan. Triggerless backdoor attack for nlp tasks with
clean labels. arXiv preprint arXiv:2111.07970, 2021.

Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, and Hyoungshick Kim. Backdoor attacks and
countermeasures on deep learning: A comprehensive review. arXiv preprint arXiv:2007. 10760, 2020.

Yansong Gao, Yeonjae Kim, Bao Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal, Damith Ranasinghe, and Hyoungshick Kim. Design and
evaluation of a multi-domain trojan detection method on deep neural networks. IEEE Transactions on Dependable and Secure Computing, 2021.
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep
neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pages 113-125, 2019.

Diego Garcia-soto, Huili Chen, and Farinaz Koushanfar. Perd: Perturbation sensitivity-based neural trojan detection framework on nlp applications.
arXiv preprint arXiv:2208.04943, 2022.

Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang. Can adversarial weight perturbations inject neural backdoors. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management, pages 2029-2032, 2020.

Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, and Cong Wang. Anti-distillation backdoor attacks: Backdoors can really
survive in knowledge distillation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 826-834, 2021.

Atticus Geiger, Ignacio Cases, Lauri Karttunen, and Christopher Potts. Stress-testing neural models of natural language inference with multiply-
quantified sentences. arXiv e-prints, pages arXiv—1810, 2018.

Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, and Tom Goldstein. What doesn’t kill you makes you robust
(er): Adversarial training against poisons and backdoors. arXiv preprint arXiv:2102. 13624, 2021.

Spiros V Georgakopoulos, Sotiris K Tasoulis, Aristidis G Vrahatis, and Vassilis P Plagianakos. Convolutional neural networks for toxic comment
classification. In Proceedings of the 10th hellenic conference on artificial intelligence, pages 1-6, 2018.

Yoav Goldberg. A primer on neural network models for natural language processing. Journal of Artificial Intelligence Research, 57:345—420, 2016.
Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Dataset
security for machine learning: Data poisoning, backdoor attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.
Tan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
adversarial networks. Communications of the ACM, 63(11):139-144, 2020.

Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. A survey in adversarial defences and robustness in nlp. arXiv
preprint arXiv:2203.06414, 2022.

Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access,
7:47230-47244, 2019.

Wei Guo, Benedetta Tondi, and Mauro Barni. An overview of backdoor attacks against deep neural networks and possible defences. IEEE Open
Journal of Signal Processing, 2022.

Manuscript submitted to ACM


22

56

58

59.

60

61

62

64

67

69

70

71

72

Ls

74

75

76

77

78

Ls

80

81

82

83

55]

57]

63]

65]
66]

68]

73]

79)

Omar, et al.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770-778, 2016.

Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty.
Advances in neural information processing systems, 32, 2019.

Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. arXiv preprint
arXiv:2202.03423, 2022.

Yuheng Huang and Yuanchun Li. Zero-shot certified defense against adversarial patches with vision transformers. arXiv preprint arXiv:2111.10481,
2021.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation with syntactically controlled paraphrase
networks. arXiv preprint arXiv:1804.06059, 2018.

Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks
and countermeasures for regression learning. In 2018 IEEE Symposium on Security and Privacy (SP), pages 19-35. IEEE, 2018.

Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. Model-reuse attacks on deep learning systems. In Proceedings of the 2018 ACM
SIGSAC conference on computer and communications security, pages 349-363, 2018.

Robin Jia, Aditi Raghunathan, Kerem Géksel, and Percy Liang. Certified robustness to adversarial word substitutions. arXiv preprint arXiv:1909.00986,
2019.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification
and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018-8025, 2020.

Kiran Karra, Chace Ashcraft, and Neil Fendley. The trojai software framework: An opensource tool for embedding trojans into deep learning
models. arXiv preprint arXiv:2003.07233, 2020.

Sara Kaviani and Insoo Sohn. Defense against neural trojan attacks: A survey. Neurocomputing, 423:651—667, 2021.

Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik
Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. arXiv preprint arXiv:2104.14337, 2021.

Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor attacks on deep reinforcement learning. In 2020
57th ACM/IEEE Design Automation Conference (DAC), pages 1-6. IEEE, 2020.

Marius Kloft and Pavel Laskov. Online anomaly detection under adversarial impact. In Proceedings of the thirteenth international conference on
artificial intelligence and statistics, pages 405-412. JMLR Workshop and Conference Proceedings, 2010.

Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning,
pages 1885-1894. PMLR, 2017.

Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. arXiv preprint arXiv:1811.00741,
2018.

Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, and Fabio Roli. Adversarial malware
binaries: Evading deep learning for malware detection in executables. In 2018 26th European signal processing conference (EUSIPCO), pages 533-537.
IEEE, 2018.

Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301-310, 2020.

Ram Shankar Siva Kumar, Magnus Nystrém, John Lambert, Andrew Marshall, Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia.
Adversarial machine learning-industry perspectives. In 2020 IEEE Security and Privacy Workshops (SPW), pages 69-75. IEEE, 2020.

Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. arXiv preprint arXiv:2004.06660, 2020.

Hyun Kwon. Detecting backdoor attacks via class difference in deep neural networks. IEEE Access, 8:191049-191056, 2020.

Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial
filters of dataset biases. In International Conference on Machine Learning, pages 1078-1088. PMLR, 2020.

Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential
privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pages 656-672. IEEE, 2019.

Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defense against general poisoning attacks. arXiv preprint arXiv:2006.14768,
2020.

Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. Advances in
neural information processing systems, 29, 2016.

Jinfeng Li, Tianyu Du, Shouling Ji, Rong Zhang, Quan Lu, Min Yang, and Ting Wang. {TextShield}: Robust text classification based on multimodal
embedding and neural machine translation. In 29th USENIX Security Symposium (USENIX Security 20), pages 1381-1398, 2020.

Shaofeng Li, Benjamin Zi Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, and Haojin Zhu. Invisible backdoor attacks against deep neural networks.
arXiv preprint arXiv:1909.02742, 2019.

Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients for robust federated learning. arXiv preprint
arXiv:2002.00211, 2020.

Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep
neural networks. arXiv preprint arXiv:2101.05930, 2021.

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 23

85
86

88

90

91

94

96

99

[101

[102

[103

[106

[108

[110

[111

[112

84]

87]

89]

92]

93]

95]

97]
98]

[100]

[104]

[105]

[107]

[109]

[113]

Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.
Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor attack in the physical world. arXiv preprint arXiv:2104.02361, 2021.
Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. Rethinking the trigger of backdoor attack. arXiv preprint
arXiv:2004.04692, 2020.

Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. Deeppayload: Black-box backdoor attack on deep learning models through
neural payload injection. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 263-274. IEEE, 2021.

Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pages 16463-16472, 2021.

Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui Hsieh. Searching for an effective
defender: Benchmarking defense against adversarial word substitution. arXiv preprint arXiv:2108.12777, 2021.

Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Composite backdoor attack for deep neural network by mixing existing benign features. In
Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 113-131, 2020.

Fang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In International Conference on Machine Learning, pages 4042-4050. PMLR,
2019.

Hong Liu, Rongrong Ji, Jie Li, Baochang Zhang, Yue Gao, Yongjian Wu, and Feiyue Huang. Universal adversarial perturbation via prior driven
uncertainty approximation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2941-2949, 2019.

Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In
International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273-294. Springer, 2018.

Nelson F Liu, Roy Schwartz, and Noah A Smith. Inoculation by fine-tuning: A method for analyzing challenge datasets. arXiv preprint
arXiv:1904.02668, 2019.

Xuankai Liu, Fengting Li, Bihan Wen, and Qi Li. Removing backdoor-based watermarks in neural networks with limited data. In 2020 25th
International Conference on Pattern Recognition (ICPR), pages 10149-10156. IEEE, 2021.

Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint
arXiv:1611.02770, 2016.

Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.
Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu Zhang. Piccolo: Exposing complex backdoors in nlp transformer
models. In 2022 IEEE Symposium on Security and Privacy (SP), pages 1561-1561. IEEE Computer Society, 2022.

Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European
Conference on Computer Vision, pages 182-199. Springer, 2020.

Yuntao Liu, Ankit Mondal, Abhishek Chakraborty, Michael Zuzak, Nina Jacobsen, Daniel Xing, and Ankur Srivastava. A survey on neural trojans.
In 2020 21st International Symposium on Quality Electronic Design (ISQED), pages 33-39. IEEE, 2020.

Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE International Conference on Computer Design (ICCD), pages 45-48. IEEE,
2017.

Giulio Lovisotto, Simon Eberz, and Ivan Martinovic. Biometric backdoors: A poisoning attack against unsupervised template updating. In 2020
IEEE European Symposium on Security and Privacy (EuroS&P), pages 184-197. IEEE, 2020.

Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.
Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning: A survey. arXiv preprint arXiv:2003.02133, 2020.

Shiqing Ma and Yingqi Liu. Nic: Detecting adversarial samples with neural network invariant checking. In Proceedings of the 26th network and
distributed system security symposium (NDSS 2019), 2019.

Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. Data poisoning attacks in contextual bandits. In International Conference on Decision and
Game Theory for Security, pages 186-204. Springer, 2018.

Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis.
In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150, 2011.
Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. Learning under p-tampering attacks. In Algorithmic Learning Theory, pages
572-596. PMLR, 2018.

Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration in robust learning: Evasion and poisoning
attacks from concentration of measure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4536-4543, 2019.

Saeed Mahloujifar and Mohammad Mahmoody. Can adversarially robust learning leveragecomputational hardness? In Algorithmic Learning
Theory, pages 581-609. PMLR, 2019.

Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Data poisoning attacks in multi-party learning. In International Conference on
Machine Learning, pages 4274-4283. PMLR, 2019.

Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine learners. In Twenty-Ninth AAAI Conference
on Artificial Intelligence, 2015.

Chenglin Miao, Qi Li, Houping Xiao, Wenjun Jiang, Mengdi Huai, and Lu Su. Towards data poisoning attacks in crowd sensing systems. In

Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing, pages 111-120, 2018.

Manuscript submitted to ACM


24

121

[123

125

[126

[127

[128

129

[131

133

[134

[135
[136

[137

138

[139

[140

(141

[122]

[124]

[130]

[132]

Omar, et al.

John X Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. Reevaluating adversarial examples in natural language. arXiv preprint
arXiv:2004. 14174, 2020.

John X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation,
and adversarial training in nlp. arXiv preprint arXiv:2005.05909, 2020.

Luis Mufioz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning
of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pages
27-38, 2017.

Luis Mufioz-Gonzalez, Bjarne Pfitzner, Matteo Russo, Javier Carnerero-Cano, and Emil C Lupu. Poisoning attacks with generative adversarial nets.
arXiv preprint arXiv:1906.07773, 2019.

Andrew Newell, Rahul Potharaju, Luojie Xiang, and Cristina Nita-Rotaru. On the practicality of integrity attacks on document-level sentiment
analysis. In Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop, pages 83-93, 2014.

Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. Advances in Neural Information Processing Systems, 33:3454-3464, 2020.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language
understanding. arXiv preprint arXiv:1910.14599, 2019.

Marwan Omar, Soohyeon Choi, Daehun Nyang, and David Mohaisen. Quantifying the performance of adversarial training on language models
with distribution shifts. In Proceedings of the 1st Workshop on Cybersecurity and Social Sciences, pages 3-9, 2022.

Marwan Omar, Soohyeon Choi, DaeHun Nyang, and David Mohaisen. Robust natural language processing: Recent advances, challenges, and
future directions. arXiv preprint arXiv:2201.00768, 2022.

Marwan Omar and David Mohaisen. Making adversarially-trained language models forget with model retraining: A case study on hate speech
detection. In Companion Proceedings of the Web Conference 2022, pages 887-893, 2022.

Marwan Omar and Gita Sukthankar. Textdefend: Detecting adversarial examples using local outlier factor. In 17th IEEE International conference on
semantic computing ), pages 15-28, 2023.

Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using
adversarial samples. arXiv preprint arXiv:1605.07277, 2016.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against
machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, pages 506-519, 2017.

Andrea Paudice, Luis Mufioz-Gonzalez, Andras Gyorgy, and Emil C Lupu. Detection of adversarial training examples in poisoning attacks through
anomaly detection. arXiv preprint arXiv: 1802.03041, 2018.

Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014
conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.

Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. Intriguing properties of adversarial ml attacks in the problem space.
In 2020 IEEE symposium on security and privacy (SP), pages 1332-1349. IEEE, 2020.

Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Onion: A simple and effective defense against textual backdoor
attacks. arXiv preprint arXiv:2011.10369, 2020.

Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor

+

attacks with syntactic trigger. arXiv preprint arXiv:2105.12400, 2021.

Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and Maosong Sun. Turn the combination lock: Learnable textual backdoor attacks via word
substitution. arXiv preprint arXiv:2106.06361, 2021.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
OpenAI blog, 1(8):9, 2019.

Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neural network attack with bit trojan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 13198-13207, 2020.

Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial attacks and defenses in deep learning. Engineering, 6(3):346-360, 2020.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135-1144, 2016.

Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. Antidote:
understanding and defending against poisoning of anomaly detectors. In Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement,
pages 1-14, 2009.

Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on
artificial intelligence, volume 34, pages 11957-11965, 2020.

Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. Baaan: Backdoor attacks against autoencoder and gan-based
machine learning models. arXiv preprint arXiv:2010.03007, 2020.

Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic backdoor attacks against machine learning models. In 2022 IEEE
7th European Symposium on Security and Privacy (EuroS&P), pages 703-718. IEEE, 2022.

Esha Sarkar, Yousif Alkindi, and Michail Maniatakos. Backdoor suppression in neural networks using input fuzzing and majority voting. IEEE
Design & Test, 37(2):103-110, 2020.

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 25

[143

[145

[147

[148

[149

[151

[153
[154

[156
[157

[158
[159

[160

[161

[163

[165

[167

[169

[170
[171

[142]

[144]

[146]

[150]

[152]

[155]

[162]

[164]

[166]

[168]

Viktor Schlegel, Goran Nenadic, and Riza Batista-Navarro. A survey of methods for revealing and overcoming weaknesses of data-driven natural
language understanding. Natural Language Engineering, pages 1-31, 2022.

Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion.
In 30th USENIX Security Symposium (USENIX Security 21), pages 1559-1575, 2021.

Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea. {Explanation-Guided} backdoor poisoning attacks against malware classifiers. In 30th
USENIX Security Symposium (USENIX Security 21), pages 1487-1504, 2021.

Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted
clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31, 2018.

Kun Shao, Yu Zhang, Junan Yang, Xiaoshuai Li, and Hui Liu. The triggers that open the nlp model backdoors are hidden in the adversarial samples.
Computers & Security, page 102730, 2022.

Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang. Backdoor scanning for
deep neural networks through k-arm optimization. In International Conference on Machine Learning, pages 9525-9536. PMLR, 2021.

Shiqi Shen, Shruti Tople, and Prateek Saxena. Auror: Defending against poisoning attacks in collaborative deep learning systems. In Proceedings of
the 32nd Annual Conference on Computer Security Applications, pages 508-519, 2016.

Reza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P),
pages 175-183. IEEE, 2020.

David Solans, Battista Biggio, and Carlos Castillo. Poisoning attacks on algorithmic fairness. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pages 162-177. Springer, 2020.

Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. Advances in neural information processing
systems, 30, 2017.

Gan Sun, Yang Cong, Jiahua Dong, Qiang Wang, Lingjuan Lyu, and Ji Liu. Data poisoning attacks on federated machine learning. IEEE Internet of
Things Journal, 2021.

Lichao Sun. Natural backdoor attack on text data. arXiv preprint arXiv:2006. 16176, 2020.

Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple approach for trojan attack in deep neural networks.
In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 218-228, 2020.

Florian Tramér, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and
defenses. arXiv preprint arXiv:1705.07204, 2017.

Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in neural information processing systems, 31, 2018.
Masatoshi Tsuchiya. Performance impact caused by hidden bias of training data for recognizing textual entailment. arXiv preprint arXiv: 1804.08117,
2018.

Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. arXiv preprint arXiv:1912.02771, 2019.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. arXiv
preprint arXiv:1908.07125, 2019.

Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. Trick me if you can: Human-in-the-loop generation of adversarial
examples for question answering. Transactions of the Association for Computational Linguistics, 7:387-401, 2019.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International
conference on machine learning, pages 1058-1066. PMLR, 2013.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A
stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
for natural language understanding. EMNLP 2018, page 353, 2018.

Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating
backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707-723. IEEE, 2019.

Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, and Tianle Chen. Backdoor attacks against transfer learning with
pre-trained deep learning models. IEEE Transactions on Services Computing, 2020.

Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen, Alex Beutel, and Ed Chi. Cat-gen: Improving robustness in nlp models via
controlled adversarial text generation. arXiv preprint arXiv:2010.02338, 2020.

Wenjie Wang, Pengfei Tang, Jian Lou, and Li Xiong. Certified robustness to word substitution attack with differential privacy. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1102-1112,
2021.

Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang Ye. Towards a robust deep neural network against adversarial texts: A survey.
ieee transactions on knowledge and data engineering, 2021.

Xiaosen Wang, Jin Hao, Yichen Yang, and Kun He. Natural language adversarial defense through synonym encoding. In Uncertainty in Artificial
Intelligence, pages 823-833. PMLR, 2021.

John Wieting and Douwe Kiela. No training required: Exploring random encoders for sentence classification. arXiv preprint arXiv:1901.10444, 2019.
Catherine Wong. Dancin seq2seq: Fooling text classifiers with adversarial text example generation. arXiv preprint arXiv:1712.05419, 2017.

Manuscript submitted to ACM


26

[173

175

[176

(177

[178

179

[181

[183

185

[186

[187

[188
[190
[191

[192

[193
[194

[195

[196

197
[198

[199

[172]

[174]

[180]

[182]

[184]

[189]

Omar, et al.

Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and
rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635-3673. PMLR, 2020.

Zhengxuan Wu, Nelson F Liu, and Christopher Potts. Identifying the limits of cross-domain knowledge transfer for pretrained models. In
Proceedings of the 7th Workshop on Representation Learning for NLP, pages 100-110, 2022.

Zhen Xiang, David J Miller, Hang Wang, and George Kesidis. Detecting scene-plausible perceptible backdoors in trained dnns without access to
the training set. Neural computation, 33(5):1329-1371, 2021.

Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature selection secure against training data
poisoning? In international conference on machine learning, pages 1689-1698. PMLR, 2015.

Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against federated learning. In International Conference on
Learning Representations, 2019.

Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using
auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2020.

Sang Michael Xie, Tengyu Ma, and Percy Liang. Composed fine-tuning: Freezing pre-trained denoising autoencoders for improved generalization.
In International Conference on Machine Learning, pages 11424-11435. PMLR, 2021.

Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. Detoxifying language models risks marginalizing
minority voices. arXiv preprint arXiv:2104.06390, 2021.

Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word embeddings: Exploring the vulnerability
of the embedding layers in nlp models. arXiv preprint arXiv:2103. 15543, 2021.

Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rap: Robustness-aware perturbations for defending against backdoor attacks on nlp
models. arXiv preprint arXiv:2110.07831, 2021.

Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against nlp models. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pages 5543-5557, 2021.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset
for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security, pages 2041-2055, 2019.

Nanyang Ye, Kaican Li, Langing Hong, Haoyue Bai, Yiting Chen, Fengwei Zhou, and Zhenguo Li. Ood-bench: Benchmarking and understanding
out-of-distribution generalization datasets and algorithms. arXiv preprint arXiv:2106.03721, 2021.

Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang
Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.

Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly, Michael J Franklin, Scott Shenker, and Ion Stoica.
Resilient distributed datasets: A {Fault-Tolerant} abstraction for {In-Memory} cluster computing. In 9th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 12), pages 15-28, 2012.

Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1-9, 2022.

Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news.
Advances in neural information processing systems, 32, 2019.

Quan Zhang, Yifeng Ding, Yongqiang Tian, Jianmin Guo, Min Yuan, and Yu Jiang. Advdoor: adversarial backdoor attack of deep learning system.
In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 127-138, 2021.

Xingiao Zhang, Huili Chen, and Farinaz Koushanfar. Tad: Trigger approximation based black-box trojan detection for ai. arXiv preprint
arXiv:2102.01815, 2021.

Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. Trojaning language models for fun and profit. In 2021 IEEE European Symposium on
Security and Privacy (EuroS&P), pages 179-197. IEEE, 2021.

Yuan Zhang, Jason Baldridge, and Luheng He. Paws: Paraphrase adversaries from word scrambling. arXiv preprint arXiv:1904.01130, 2019.

Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor attacks to graph neural networks. In Proceedings of the 26th ACM
Symposium on Access Control Models and Technologies, pages 15-26, 2021.

Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, and Maosong Sun. Red alarm for
pre-trained models: Universal vulnerability to neuron-level backdoor attacks. arXiv preprint arXiv:2101.06969, 2021.

Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition
models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14443-14452, 2020.

Xuhui Zhou. Challenges in automated debiasing for toxic language detection. University of Washington, 2021.

Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, and Xuanjing Huang. Defense against adversarial attacks in nlp via dirichlet neighborhood
ensemble. arXiv preprint arXiv:2006.11627, 2020.

Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei Wang. Learning to discriminate perturbations for blocking adversarial attacks in text
classification. arXiv preprint arXiv:1909.03084, 2019.

Manuscript submitted to ACM


Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions 27

[200] Bin Zhu, Zhaoquan Gu, Le Wang, and Zhihong Tian. Treated: Towards universal defense against textual adversarial attacks. arXiv preprint
arXiv:2109.06176, 2021.

[201] Yue Zhuo, Zhengin Yin, and Zhiqiang Ge. Attack and defense: Adversarial security of data-driven fde systems. IEEE Transactions on Industrial
Informatics, 2022.

Manuscript submitted to ACM
