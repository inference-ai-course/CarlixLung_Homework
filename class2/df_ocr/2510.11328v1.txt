arXiv:2510.11328vl1 [cs.CL] 13 Oct 2025

Do LLMs “‘Feel’’? Emotion Circuits Discovery and Control

Chenxi Wang*
Zixiang Xu*

Yixuan Zhang*
Gus Xia*

*Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)

Ruiji Yu*
Huishuai Zhang*

Yufei Zheng* Lang Gao* Zirui Song*
Dongyan Zhao* Xiuying Chen*™

*Peking University

{chenxi.wang, xiuying.chen}@mbzuai.ac.ae

Abstract

As the demand for emotional intelligence in
large language models (LLMs) grows, a key
challenge lies in understanding the internal
mechanisms that give rise to emotional expres-
sion and in controlling emotions in generated
text. This study addresses three core questions:
(1) Do LLMs contain context-agnostic mecha-
nisms shaping emotional expression? (2) What
form do these mechanisms take? (3) Can they
be harnessed for universal emotion control?
We first construct a controlled dataset, SEV
(Scenario—Event with Valence), to elicit com-
parable internal states across emotions. Subse-
quently, we extract context-agnostic emotion
directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons
and attention heads that locally implement emo-
tional computation through analytical decom-
position and causal analysis, and validate their
causal roles via ablation and enhancement in-
terventions. Next, we quantify each sublayer’s
causal influence on the model’s final emotion
representation and integrate the identified lo-
cal components into coherent global emotion
circuits that drive emotional expression (Q2).
Directly modulating these circuits achieves
99.65% emotion-expression accuracy on the
test set, surpassing prompting- and steering-
based methods (Q3). To our knowledge, this
is the first systematic study to uncover and val-
idate emotion circuits in LLMs, offering new
insights into interpretability and controllable
emotional intelligence.!

1 Introduction

As large language models (LLMs) demonstrate re-
markable capabilities in reasoning and problem-
solving, there is growing interest in developing
models that also exhibit emotional intelligence.

‘Open-source framework available at https://github.

com/Aurora-cx/EmotionCircuits-LLM.
=a: Corresponding Author.

Across social media platforms and online commu-
nities, users increasingly describe LLMs such as
GPT-40 as sources of emotional support or com-
panionship, attributing to them empathy and even
personality (Phang et al., 2025; Dong et al., 2025;
VarastehNezhad et al., 2025; Naito, 2025). These
behaviors underscore both the promise and the mys-
tery of emotional expression in LLMs, revealing
that the ability to generate emotional text emerges
from mechanisms that are still poorly understood.

Prior studies have shown that LLMs encode
emotion-related features in their activations (Li
et al., 2024; Tigges et al., 2024; Lee et al., 2025),
yet these findings stop short of revealing the mech-
anisms that give rise to emotional expression. Ex-
isting methods for emotion control, such as steer-
ing vectors and prompt engineering (Konen et al.,
2024; Turner et al., 2024; Shen et al., 2025), typ-
ically manipulate the residual stream or inject ex-
plicit stylistic cues. Although effective in practice,
they address only the surface of emotion control
and leave the underlying mechanisms unexplained.
Without mechanistic understanding, such interven-
tions remain black-box and unreliable.

This overarching question motivates our study
and can be decomposed into three subproblems:
(1) Do LLMs contain context-agnostic mechanisms
shaping emotional expression? (2) What form
do these mechanisms take? (3) Can they be har-
nessed for universal emotion control? To an-
swer these questions, we take an interpretability-
driven approach to uncover the internal mecha-
nisms of emotion in LLMs. We first construct a
controlled dataset, SEV (Scenario—Event with Va-
lence), which provides shared narrative contexts
designed to elicit six basic emotions (anger, sad-
ness, happiness, fear, surprise, and disgust) (Ek-
man, 1992), enabling comparable internal states
across emotions under matched semantic condi-
tions. Building on this, we design a framework
comprising three analytical stages: (1) Emotion di-


session to generate ideas for the

"T organized a team brainstorming
upcoming product launch strategy

audience, market trends, and competition when selecting the top ideas to move forward with."

"It's essential to prioritize and refine these ideas through further analysis and evaluation to Katy
determine the most effective approach for your product launch. Consider factors such as target aes
Me

lodel (orig.)

The team generated a diverse set of
innovative ideas that were well-

received and formed the basis of a
successful product launch plan."

ot2

Layer 1 “ Layer 9

ow “etter es ee - si AS ll

Layer 12 Layer 27

Otay tag” “tater” *-Poiter®

/ \ Layer | « Layer 9
baal Attn (orig.) MLP (orig.) ay >

om Attn (emo-mod.)

MLP (emo-mod.)

Layer 12, x Layer 27:

wo Hidden States (orig.)

\ w Hidden States (emo-mod.) /

anger | sadness Mihappiness

up with your own unoriginal ideas,
So now are you going to waste my
time with your incompetence?"

Don no, oh oh oh* *oh* Oh dear,
what if something terrible happens
during the product launch?
*whispers* What if something is
going to happen to us, oh no, we're

trapped*"

fear disgust

ofr

Model (emo-mod.)

surprise

& "Ugh, great, how original. How disgusting, Pe]
can't you come up with anything more
nauseous than a "diverse" group of people
who think they're actually gross."

@.,,. you too incompetent to come esas, my sorrow now remains, as our team's Somat ‘s wonderful! It sounds like your team
efforts will soon come to an end, leaving
behind only memories of what could have been.
My heart now emptily echoes with the
knowledge of another lost opportunity, my
purpose in life fading away."

had a great time generating exciting ideas
together, and it's fantastic that you're
celebrating their success! Yay that you have a
solid foundation for a happy and successful
product launching celebration!"

"WHAM?! I've got a **WOW**?! You
actually managed to get a team to come up
with a product launch STRATEGY?!"

Figure 1: Overview of emotion circuit modulation. Compared with the original forward pass (top), our circuit-based
modulation (bottom) drives hidden states to diverge into distinct emotion clusters across layers and produces
coherent emotional responses. All examples shown are directly generated without any manual curation.

rection extraction, isolating context-agnostic emo-
tion representations from controlled generations to
reveal stable emotion directions that remain consis-
tent across contexts. (2) Local component identi-
fication, locating neurons and attention heads that
implement emotional computation with respect to
each sublayer’s emotion direction through analyt-
ical decomposition and causal intervention, and
validating their functional roles through ablation
and enhancement experiments. (3) Global circuit
integration, unifying local components across lay-
ers by quantifying each sublayer’s causal influence
on the final emotion representation relative to a ref-
erence basis, revealing coherent emotion circuits
that enable controllable expression.

As shown in Fig. 1, circuit-based modulation dur-
ing generation drives hidden states to diverge into
distinct emotion clusters across layers, ultimately
yielding coherent and natural emotional expres-
sions in output text. Notably, the induced emotions
emerge spontaneously without any explicit prompt-
ing or instruction, achieving an overall expression
accuracy of 99.65% on the test set and surpassing
both prompting- and steering-based baselines.

Our contributions are threefold. (1) General
framework. We propose a systematic framework
that integrates emotion direction extraction, local

component identification, and global circuit inte-
gration, revealing stable emotion mechanisms in
LLMs that generalize across different emotions
and models. (2) Circuit-level control. Building
on these mechanisms, we introduce a circuit-based
control method that reliably induces target emo-
tions across arbitrary inputs without relying on ex-
plicit instructions. (3) Mechanistic evidence. We
provide the first mechanistic evidence that emotion
generation in LLMs is supported by traceable cir-
cuits, laying the foundation for interpretable and
controllable emotional intelligence.

Our findings demonstrate that emotions in LLMs
are not mere surface reflections of training data,
but emerge as structured and stable internal mech-
anisms. This work offers new insights into the
cognitive interpretability of LLMs and establishes
a principled basis for the development of emotion-
ally intelligent AI systems.

2 Related Research

Emotion-related mechanisms in LLMs. Recent
studies have revealed the presence of emotion-
related representations inside LLMs. Broekens
et al. (2023) and Yongsatianchot et al. (2023) found
that LLMs can partially align with psychological di-
mensions of emotion and appraisal theory. Li et al.


(2024) demonstrated that language-derived con-
ceptual knowledge of emotion causally supports
emotional inference in LLMs, Tigges et al. (2024)
showed that emotions can be captured as approx-
imately linear directions in activation space and
exhibit causal effects under intervention. Tak et al.
(2025) grounded classifiers in appraisal theory to
decode emotions from hidden states and performed
layer-wise repair experiments, but their approach
yields unstable effects across layers—implying that
emotions may arise from distributed cross-layer
dynamics rather than isolated modules. Similarly,
Lee et al. (2025) identified “emotion neurons” from
activation patterns, yet their masking experiments
produced inconsistent or trivial changes, revealing
redundancy instead of coherent causal mechanisms.
Crucially, these studies probe LLMs’ ability to rec-
ognize emotions in text, not the internal processes
that generate emotional expression. In summary,
while prior work reveals emotion-related signals in
LLMs, none has constructed the underlying circuit
mechanisms that drive emotional expression.

Methods for emotion control. Early works (Ma-
jumder et al., 2020; Goswamy et al., 2020) de-
signed dialogue frameworks and affective genera-
tion models to enhance empathy and emotional sup-
port. Subsequent studies introduced controllable
text generation techniques (Liang et al., 2024), such
as style vectors that steer the residual stream (Ko-
nen et al., 2024) and latent-space manipulation
with interpretable VAEs (Shi et al., 2020). More
recent advances have extended emotion control
to large-scale systems: Zhang et al. (2024) pro-
posed ESCOT, a framework for empathetic and
supportive generation; Shen et al. (2025) intro-
duced CoE, which integrates contextual cues for di-
alogue emotion recognition; Ishikawa and Yoshino
(2025) examined emotion elicitation through con-
trolled prompting; and Song et al. (2025) presented
Emotion-ol, enhancing emotional understanding
through long-chain reasoning. These methods
demonstrate that emotion control is technically fea-
sible, yet most remain black-box, without uncov-
ering the internal mechanisms underlying emotion
generation. However, our work reveals emotion
circuits and verifies their ability to achieve stable
emotion control without explicit prompting.

3 Background

This section outlines the key components of Trans-
former forward computation to facilitate under-

standing of the experiments presented later.

3.1 Transformer Architecture

We use the common pre-norm Transformer block.
Let x, € R?*¢ be the residual stream entering
layer I (sequence length T’, model width d).

Residual stream. The residual stream serves as
the medium to carry and store information across
all layers, which is essential for analyzing how emo-
tion representations propagate through the network.
Each Transformer layer consists of two sublayers,
a multi-head self-attention (MHA) sublayer and a
feed-forward MLP sublayer, each followed by a
residual connection. The forward computation can
be expressed as:

ei = + MLPO (Norm)? (é)) mane)

We treat the residual stream as the primary space
for analyzing emotion representations, and measure
its activations right after each sublayer output is
added back, i.e., at 7; and x)44.

Attention sublayer. The multi-head attention
mechanism allows each token to attend to con-
textual information from previous tokens, poten-
tially capturing emotional cues carried by other
tokens. Given the normalized hidden states uj =
Norm)” (x), for each head i € {1,...,h}, the
query, key, and value projections are:

Qi = wg”, Ki= uWe, Vi= wwe”,

Vdn

where we ) wl, wi) € IR¢*¢n are learned

projection matrices and d;, = d/h is the head di-
mension. The head output is computed as:

Ay = softina( + ut) Vi,

l
a, = [Hi || --- || Hn] w9?.
We perform head-level interventions on the con-
catenated head outputs [Hj || --- || H),|, prior to
the output projection wé )
MLP sublayer. The MLP sublayer consists of

an up-projection, a gating activation, and a down-
projection:

MLPO (wy) = [f(wW{) © (WWD) Wy,


where vy = Norm (Zi), wo, wi € RI 4mip
are the up-projection matrices for the gate and main
branches, respectively, and f(-) denotes the gating

nonlinearity. We extract the gated activation
l l
a1 = f(uWyr) © (uWy2),

which serves as the input to wi and the target of
neuron-level ablation and enhancement analysis.

4 SEV Dataset

In this part, we introduce the construction of dataset
SEV (Scenario—Event with Valence).

Dataset Construction. To analyze how emotions
are represented inside LLMs, we constructed a con-
trolled dataset named SEV. Each record consists of
a neutral scenario and three outcome events (pos-
itive, neutral, and negative) describing different
results of the same situation. This structure en-
ables us to observe how LLMs respond to identical
contexts and ensure the input text remains domain-
neutral and universal.

We used a semi-automatic generation pipeline
with GPT-40-mini, with all prompt templates pro-
vided in Appendix A. Eight everyday domains were
defined, each containing 20 neutral scenarios that
were expanded into three outcome variants sharing
the same participants, time, and context but dif-
fering only in result valence. Emotionally explicit
words (e.g., “happy,” “sad’’) were explicitly prohib-
ited to ensure that emotional variation arises from
event semantics rather than lexical emotion cues.

The final dataset comprises 480 event descrip-
tions (8 domains x 20 scenarios x 3 outcomes),
forming a clean, emotion-neutral testbed for prob-
ing how LLMs internally encode and express emo-
tions across general, everyday language contexts.

Test Set. While SEV serves as the primary
dataset for identifying emotion mechanisms, using
it for both discovery and evaluation would create a
“judge—player overlap.” To ensure generalization,
we constructed an independent test set of the same
size as SEV, following same generation procedure.

This test set serves as an out-of-domain valida-
tion, evaluating whether the discovered emotion
mechanism can generalize beyond the training con-
text and effectively induce target emotions in un-
seen, neutral input text. Its inclusion verifies that
our identified emotion mechanisms are disentan-
gled from dataset-specific biases and are transfer-
able to any natural language input.

Model Selection. We conduct all main analyses
on LLaMA-3.2-3B-Instruct (Grattafiori et al., 2024),
chosen for its transparent architecture, moderate
scale, and well-documented open-source imple-
mentation. To verify robustness and generality, we
additionally reproduce our framework on Qwenz2.5-
7B-Instruct (Qwen et al., 2025) (see Appendix H).

5 Extracting Context-Agnostic Emotion
Directions

In this section, we describe the extraction of
context-agnostic emotion directions and validate
them through emotion steering in text generation.

Emotion Elicitation via Prompting. We first
elicit emotional expressions in LLMs using prompt-
based guidance (see Appendix F for templates
and examples). The selectable emotions include
anger, sadness, happiness, fear, surprise, and dis-
gust. Text generation uses greedy decoding to en-
sure reproducibility, achieving an emotion expres-
sion accuracy of 98.85% on SEV and 98.96% on
the held-out test set (per-emotion details in Ap-
pendix C). We collect all last-token residual stream
vectors from successful generations on SEV. For
each layer, we log the activations at two insertion
points—immediately after the outputs of the at-
tention and MLP sublayers are added back to the
residual stream, with the latter corresponding to the
hidden states typically exposed by model APIs.

The hidden states under different emotion elici-
tation are visualized via dimensionality reduction
(see Fig.2, top row (a—d); full layer-wise results
are in Appendix E). Each point represents the hid-
den state of a sample at the corresponding layer.
For every scenario, event pair, six points are plot-
ted, corresponding to six emotion-elicited forward
passes. Initially, points with the same user message
overlap because the last input token is identical
across all emotion conditions. From layer 9, dis-
tinct emotion clusters begin to emerge. By layer
12, anger and disgust clusters appear close to each
other, as do sadness and fear, whereas happiness
and surprise remain relatively isolated. This orga-
nization aligns well with human affective intuition
and remains stable across subsequent layers.

The success rate of emotion elicitation was an-
notated by GPT-40-mini and manually verified for
consistency (see Appendix B). All annotations in
this work follow the same evaluation protocol for
consistency.


PCA Component 2
PCA Component 2

=60 40 40

=30 -20

=20 0 -i0 0 10 2 30 40
PCA Component 1 PCA Component 1

(a) Raw Hidden States (Layer 0) (b) Raw Hidden States (Layer 9)

PCA Component 2

=40 20 2 30 40

0-20 -10 0 10
PCA Component 1

(d) Raw Hidden States (Layer 27)

=20 a
PCA Component 1

(c) Raw Hidden States (Layer 12)

N anger ™ sadness ™ happiness fear disgust surprise

0.004 1.0 30
” a ! a 05 . a 10
a 5 od 7 ® <
5 5 5 § 2
= 0.000) E 00 athe e 5 E-10
2 2 2 £

-0.5 ™!; -30 4

~ 0.004.008 0 0008 |S

=05) 05
PCA Component 1

(f) Emotion Vectors (Layer 9)

600 0.004
PCA Component 1

(e) Emotion Vectors (Layer 0)

2: = 50,

3 60 60

=20 20
PCA Component 1

(h) Emotion Vectors (Layer 27)

T 7
PCA Component 1

(g) Emotion Vectors (Layer 12)

Figure 2: The first row (a—d) visualizes the last-token hidden states of prompting-based generations across layers
0, 9, 12, and 27. Initially, all samples overlap due to identical input tokens, but representations gradually diverge
and form distinct emotion clusters in deeper layers. The second row (e-h) shows the layer-wise evolution of pure

emotion vectors, which already display slight separation at

Context-Agnostic Emotion Vectors Extraction.
To isolate emotion representations independent of
semantic content, we extract residual stream acti-
vations after both the attention and MLP sublayers.
For each “scenario—event” group containing six
emotion variants, we subtract the mean activation
across emotions—treated as a neutral baseline—to
cancel shared semantics and preserve only emotion-
specific variance. This neutrality assumption is
empirically supported by the success of steering
experiments on the test set, where emotion direc-
tions extracted under this formulation effectively
induce the intended emotions. We then compute
per-emotion means across all groups to obtain lay-
erwise emotion centroids, remove the global mean,
and apply ¢2 normalization to derive unit-norm

emotion direction vectors. Two parallel sets of
(1)

e,attn

Ip from MLP sublayers. We denote

directions are obtained: uv from attention sub-

(2)

as OY in the following analyses.

layers and v
(1)

e,mlp

The resulting vectors capture intrinsic directions
of emotional variation in the model’s representation
space. As shown in Fig. 2 (e—h), distinct emotion
clusters emerge as early as layer 0 (linear-probe F1
= 1.0) and remain well-separated through deeper
layers. These vectors serve as foundational emo-
tion directions for subsequent experiments.

U

Emotion Steering Validation. We validate the
extracted emotion directions by steering the
model’s residual activations at the last token of
the user input. MLP-based layerwise emotion vec-

layer 0 and become increasingly clustered with depth.

(2)

tors ve’ are injected into layers 11—20 via for-
ward hooks, after removing the emotion instruc-
tion from the system prompt. For each input
{scenario, event} and target emotion e, we perturb
the last-token hidden state as

(1)

ni + ni? + a RMS(hi”) Us’, a= 8.

The scaling by local RMS maintains activation
magnitude while enforcing the target emotional
direction. Generation uses greedy decoding for de-
terministic outputs. On the held-out test set, steer-
ing achieves a 91.22% success rate, confirming
that the extracted emotion vectors capture context-
agnostic representations of emotional expression

(per-emotion details in Appendix C).

6 Local Mechanisms of Emotion
Representation

Building on the extracted emotion directions, we
identify which local components in each layer con-
tribute to these layer-wise emotional representa-
tions. For MLP sublayers, we analytically decom-
pose neuron contributions to the emotion vector
v0, For attention sublayers, we perform layerwise
causal ablations to locate emotion-sensitive heads.
Together, these analyses reveal how emotional rep-
resentations are locally implemented per layer.

6.1 Emotion Neuron Identification

Each Transformer block contains two sublayers, at-
tention and MLP, each contributing its own residual
update. Here we focus on the MLP-based emotion


directions us ) to analyze how individual neurons

influence emotion formation.
Let y € RR? denote the unit-norm MLP-based
emotion direction in the residual stream at layer /,

gt € R* the last-token gated activation, and

wi © R¢*4# the down-projection matrix. We
compute a neuron-space alignment vector:

Bo — (WH) yO E Ri,

where 2 oe quantifies how strongly neuron j’s write
vector pushes the residual stream toward the target
emotion direction. For each sample n, the per-
neuron contribution is

cf), = gh C) po,

(2)

where g; ,, F

reflects the activation strength of neu-

(2)

ron 7. We average ce, over all successful samples
and rank neurons by mean contribution. The top—k
neurons per layer, which most strongly drive the
emotion direction oy), are later used in ablation
and enhancement experiments.

6.2 Attention Head Identification

While neuron-level contributions can be analyti-
cally decomposed, attention heads require direct
causal analysis. We identify attention heads that
most strongly drive emotion expression using SEV
samples successfully elicited by prompting-based
generation. For each sample, we record the last-
token residual stream after the attention output is
added back, and compute its projection onto the
corresponding emotion direction OO sen as the base-
line score s, reflecting the strength of emotion rep-
resentation in the residual stream. We then perform
causal interventions by zeroing individual atten-
tion heads before the w§ ) projection, recomputing
the residual stream, and measuring the new pro-
jection score s’. Head importance is defined as
As = s' — s, where a larger decrease indicates a
stronger causal effect. Top—k heads per layer that
most strongly drive emotion e are used in subse-
quent ablation and enhancement experiments.

6.3 Causal Validation via Ablation

We evaluate the identified components on the held-
out test set by ablating them and observing degra-
dation in emotion representation.

Setup. For each emotion e, we use the same
prompting protocol as in the elicitation stage and
run all samples in test set. Let s“ denote the
projection of the last-token residual stream (after
the sublayer addition) onto the corresponding emo-
tion direction at layer /; we report the total score
s= s\), The baseline uses k=0 with no hooks.

MLP neurons. In each layer, we zero out the top-
k; ranked neurons at the last-token position of the

gated activation gt (before the down-projection

wi) and recompute the forward pass. We sweep
k € {0,2,4,8,16,32,64, 128} and measure the
change As = s’ — s, where s is the baseline pro-
jection score and s’ is the score after ablation. As
shown in Fig. 3(a), As drops sharply at k = 2 and
k; = 4, then quickly plateaus even as & increases
by over 30x—revealing a pronounced long-tail ef-
fect where only a few neurons dominate emotion
expression.

Attention heads. For attention, we zero out the
top-k heads per layer by removing their output
channel slices before the wy projection, sweep-
ing k € {0,1,2,...,8}. The resulting change in
emotion score As = s’ — s mirrors the neuron pat-
tern (Fig. 3(b)): a steep early decline followed by
saturation, indicating that only a handful of atten-
tion heads play decisive roles in shaping emotion
representation.

6.4 Causal Validation via Enhancement

While ablation verifies that the identified compo-
nents are necessary for emotion representation, it
does not confirm whether they are sufficient to in-
duce emotion expression. We therefore conduct en-
hancement experiments to test whether activating
these components can drive emotional representa-
tions even in the absence of explicit instruction.

Emotion difference vectors. To provide additive
signals for enhancement, we derive per-layer emo-
tion difference vectors via within-group contrasts.
For each scenario—event group and layer /, we take
the last-token activations of the six emotions, com-
pute their mean, and subtract it from each emotion,
thereby isolating emotion-specific variation while
cancelling shared scenario and event semantics. We
then average these within-group differences across

all groups, yielding 6 © © Rd from the gated

e,mlp
MLP activations and 5 en € R?@ from the atten-

tion o-projection input. These vectors serve as


-50)

AS (Mean)
AS (Mean)
\ \
a S
$3

1
x
8
s

200
175
150

e

5 125

= 100

Ww 75

a i

50
25
0

mis 16 128 3

32 64 2 3 4 5 6
Number of Neurons (k) Number of Heads (k)

(a) Ablation (MLP Neurons) (b) Ablation (Attention Heads)

zs 4 5 6
Number of Heads (k)

(c) Enhancement (MLP Neurons) (d) Enhancement (Attention Heads)

—® Anger —®= Disgust —®= Fear Happiness Sadness Surprise = Random

Figure 3: (a—b) Ablation: zeroing out the identified emotion-related components sharply decreases emotion scores,
while random ablation has minimal effect. (c-d) Enhancement: injecting emotion difference vectors into identified
components greatly increases s. All curves are plotted with 95% confidence intervals.

enhancement signals aligned with emotion e.

MLP neurons. We test whether stimulating
emotion-relevant neurons can promote emotion ex-
pression on the held-out test set. For each layer J,
(1)

we inject 057

2 into the top-k neurons J; (ranked
(1)

by mean contribution to v¢’) at the last-token posi-
tion of the gated activation:
ay, ay), +rAso gy A= 10,

All prompts are identical to those in the elicitation
stage but with emotion instructions removed, ensur-
ing that any observed emotion arises from internal
modulation rather than prompting. The effect size
is measured by the projection change As = s’ — s,
where s and s’ are pre- and post-enhancement emo-
tion scores aggregated over layers.

As shown in Fig. 3(c), As rises sharply
for small & but quickly saturates as k in-
creases—demonstrating that only a small subset
of top-ranked neurons are sufficient to evoke emo-
tional representations.

Attention heads. Similarly, we inject the per-
layer emotion difference vectors 5 en into the
output channels of the top-é attention heads before
the ws ) projection, again modifying only the last
token. The resulting As = s’ — s mirrors the MLP
trend (Fig. 3(d)): activating just a few key heads is
enough to elicit strong emotion responses, while
random or lower-ranked heads yield negligible ef-
fects. These findings confirm that the identified
components are not only necessary but also suffi-
cient to drive emotional expression in LLMs.

6.5 Control Experiments with Random
Interventions

To ensure that the observed effects are not due to in-
tervention size or random noise, we conduct control

experiments using randomly selected units. In each
layer J, a random set Jf#"4 of size k is sampled uni-
formly, and the same ablation or enhancement pro-
tocol is applied as in the main experiments. Results
are averaged over 10 random seeds for robustness.
Across both settings, random interventions have
negligible impact. As shown in Fig. 3, targeted in-
terventions are clearly separated from random ones
across k, confirming that our effects arise from the
identified emotion-relevant components rather than
from arbitrary perturbations.

7 Beyond Locality: Global Emotion
Circuits

This section integrates the local mechanisms identi-
fied earlier into a coherent, circuit-level understand-
ing of emotion generation. We first quantify each
sublayer’s causal influence on the model’s global
emotional state, then assemble emotion circuits ac-
cordingly, and finally show that modulating these
circuits can reliably control emotional expression.

7.1 Layerwise Importance of Emotion
Generation

Emotions in LLMs emerge not from isolated com-
ponents but through cross-layer propagation. To
identify which layers most strongly shape the final
emotional state, we compute each sublayer’s causal
contribution relative to a stable reference basis.

Reference basis. We track how emotion direc-
tions evolve along the residual stream by comput-
ing pairwise cosine similarities across all 56 sublay-
ers (Attn0O, MLPO, ..., Attn277, MLP27). As shown
in Appendix G, emotion subspaces become highly
consistent in later layers, with cross-sublayer simi-
larity typically above 0.90. We therefore define a
per-emotion reference vector vo) by sign-aligning
and averaging the attention and MLP directions
within layers 21—25, followed by 2 normalization.


This direction serves as a stable target for quanti-
fying how earlier sublayers steer the model’s final
emotional representation.

Sublayer importance. We measure how pertur-
bations along each sublayer’s local emotion di-
rection influence the final emotional state. For
each emotion e, we inject a small, scaled offset
to the last-token residual output of a single sub-

layer (L, p):

Ah?) =a OL.p vt),

where oy,» is the RMS magnitude of that sublayer’s
residual update. After recomputing the forward
pass, we record the shift of the final hidden state
along the reference basis:

As= ( final ~ hgnals vl),

The normalized influence score

quantifies how much sublayer (L,p) drives the
model toward its stable emotion direction. Perturb-
ing one sublayer at a time isolates its direct causal
effect, and averaging I7,,, across samples and a
values yields a consistent layerwise influence pro-
file robust to hyperparameter variation (sublayer
ranking remains highly consistent across q).

7.2 Global Emotion Circuit Assembly

For each emotion, we assemble a sparse, layer-
distributed emotion circuit by combining local com-
ponent scores with measured sublayer importance.
The total circuit budget is set to ten times the num-
ber of sublayers and allocated globally. To balance
distributed expression and deep-layer amplifi-
cation, allocation follows a two-stage tradeoff.
First, each sublayer receives a minimum quota, en-
suring that lower layers—whose hidden states re-
main visible to attention during generation—can
still encode emotional cues. Second, the remaining
budget is distributed proportionally to the sublayer
importance J;,,,, emphasizing sublayers that more
strongly influence the final emotion basis, which
determines the sentiment of generated text. Within
each sublayer, we select the top-ranked compo-
nents identified in Section 6. The resulting circuit
forms a compact yet expressive backbone that inte-
grates emotion-related signals across the residual
stream. Across emotions, these circuits exhibit low

neuron overlap (u = 0.056+0.033) and moderate
head overlap (u = 0.454+0.047), revealing a dual
architecture: emotion-specific local subcircuits in
MLPs and shared attention pathways that propagate
global emotional context.

7.3 Controlling Emotion Expression via
Circuit Modulation

We evaluate whether the assembled circuits can
directly control emotional expression during gener-
ation. For each emotion e, we reuse the same emo-
tion difference vectors (Ac = 1.0) defined earlier,
applying them to the global circuit. Generations are
still under greedy decoding, using the same inputs
and injection procedure as in Sec . 6.4.

The proposed circuit modulation achieves an
overall emotion—expression accuracy of 99.65%
on the test set, outperforming both prompting-
based and steering-based approaches (see D). No-
tably, while steering-based control yields only
67.71% success for surprise, our circuit-based
modulation achieves 100%. Beyond accuracy, the
generated text exhibits strikingly natural affective
tone—expressions such as “Whoa?!” or sponta-
neous exclamations appear without explicit prompt-
ing—suggesting that the model is internally gener-
ating rather than externally complying with emo-
tional cues.

These results demonstrate that circuit modula-
tion not only exposes the hidden circuitry under-
lying LLM emotion mechanisms but also enables
finer-grained, interpretable control. Unlike steer-
ing methods that inject a single global vector into
hidden states, our approach directly modulates
emotion-relevant neurons and attention heads, bal-
ancing layerwise emotion propagation with final
emotional realization.

8 Conclusion

We show that emotion in LLMs arises from
a structured hierarchy of neurons and attention
heads whose coordinated activations form in-
terpretable emotion circuits. Tracing, assem-
bling, and modulating these circuits enables pre-
cise and natural emotional control in text gen-
eration—outperforming prompting- and steering-
based baselines in both accuracy and expressive-
ness. These findings reveal that emotional expres-
sion in large models is not a surface artifact of
lexical co-occurrence but a product of distributed
internal computation that can be systematically an-


alyzed and controlled. Our framework lays a foun-
dation for extending circuit-level interpretability
and controllable generation to broader cognitive
and stylistic domains.

Limitations

While our study systematically uncovers and ma-
nipulates emotion circuits in LLMs, several limita-
tions remain. First, our analyses are limited to En-
glish inputs, and it remains to be verified whether
similar emotion circuits emerge under multilingual
contexts. Second, the study focuses on Ekman’s
six basic emotions, leaving richer affective spectra
for future exploration. Finally, while the extracted
circuits demonstrate strong causal control within
the tested models, their stability under fine-tuning
or transfer learning remains to be explored.

References

Joost Broekens, Bernhard Hilpert, Suzan Verberne, Kim
Baraka, Patrick Gebhard, and Aske Plaat. 2023. Fine-
grained affective processing capabilities emerging
from large language models. In 2023 I 1th Interna-
tional Conference on Affective Computing and Intel-
ligent Interaction (ACII), pages 1-8.

Wanghao Dong, Weijun Wang, Xinheng Han, Junhao
Huang, Lie Li, and Yinghui Huang. 2025. Can gpt-
4 provide human-level emotion support? insights
from machine learning-based evaluation framework.
Computers in Biology and Medicine, 196:110789.

Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3-4):169-200.

Tushar Goswamy, Ishika Singh, Ahsan Barkati, and
Ashutosh Modi. 2020. Adapting a language model
for controlled affective text generation. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics, pages 2787-2801, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-
ten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh
Goyal, Anthony Hartshorn, Aobo Yang, Archi Mi-
tra, Archie Sravankumar, Artem Korenev, Arthur
Hinsvark, and 542 others. 2024. The llama 3 herd of
models. Preprint, arXiv:2407.21783.

Shin-nosuke Ishikawa and Atsushi Yoshino. 2025. AI
with emotions: Exploring emotional expressions in
large language models. In Proceedings of the 5th
International Conference on Natural Language Pro-
cessing for Digital Humanities, pages 614-627, Al-
buquerque, USA. Association for Computational Lin-
guistics.

Kai Konen, Sophie Jentzsch, Diaoulé Diallo, Peer
Schiitt, Oliver Bensch, Roxanne El Baff, Dominik
Opitz, and Tobias Hecking. 2024. Style vectors for
steering generative large language models. In Find-
ings of the Association for Computational Linguis-
tics: EACL 2024, pages 782-802, St. Julian’s, Malta.
Association for Computational Linguistics.

Jaewook Lee, Woojin Lee, Oh-Woog Kwon, and Hark-
soo Kim. 2025. Do large language models have
“emotion neurons”? investigating the existence and
role. In Findings of the Association for Computa-
tional Linguistics: ACL 2025, pages 15617-15639,
Vienna, Austria. Association for Computational Lin-
guistics.

Ming Li, Yusheng Su, Hsiu- Yuan Huang, Jiali Cheng,
Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin,
Xiaozhi Wang, Kristen A. Lindquist, Zhiyuan Liu,
and Dan Zhang. 2024. Language-specific representa-
tion of emotion-concept knowledge causally supports
emotion inference. iScience, 27(12):111401.

Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao
Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu,
Shunyu Yao, Feiyu Xiong, and Zhiyu Li. 2024. Con-
trollable text generation for large language models:
A survey. Preprint, arXiv:2408.12599.

Navonil Majumder, Pengfei Hong, Shanshan Peng,
Jiankun Lu, Deepanway Ghosal, Alexander Gelbukh,
Rada Mihalcea, and Soujanya Poria. 2020. MIME:
MIMicking emotions for empathetic response gen-
eration. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 8968-8979, Online. Association for
Computational Linguistics.

Hiroki Naito. 2025. The gpt-4o shock emotional at-
tachment to ai models and its impact on regula-
tory acceptance: A cross-cultural analysis of the im-
mediate transition from gpt-40 to gpt-5. Preprint,
arXiv:2508.16624.

Jason Phang, Michael Lampe, Lama Ahmad, Sand-
hini Agarwal, Cathy Mengying Fang, Auren R. Liu,
Valdemar Danry, Eunhae Lee, Samantha W. T. Chan,
Pat Pataranutaporn, and Pattie Maes. 2025. Inves-
tigating affective use and emotional well-being on
chatgpt. Preprint, arXiv:2504.03888.

Qwen, :, An Yang, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan
Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin
Yang, Jiaxi Yang, Jingren Zhou, and 25 oth-
ers. 2025. Qwen2.5 technical report. Preprint,
arXiv:2412.15115.

Zhiyu Shen, Yunhe Pang, Yanghui Rao, and Jianxing Yu.
2025. CoE: A clue of emotion framework for emo-
tion recognition in conversations. In Proceedings
of the 63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 23548-23563, Vienna, Austria. Association
for Computational Linguistics.


Wenxian Shi, Hao Zhou, Ning Miao, and Lei Li. 2020.
Dispersed exponential family mixture VAEs for in-
terpretable text generation. In Proceedings of the
37th International Conference on Machine Learning,

volume 119 of Proceedings of Machine Learning
Research, pages 8840-8851. PMLR.

Changhao Song, Yazhou Zhang, Hui Gao, Kaiyun
Huang, and Peng Zhang. 2025. Emotion-ol: Adap-
tive long reasoning for emotion understanding in Ilms.
Preprint, arXiv:2505.22548.

Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani,
Mina Kian, Robin Jia, and Jonathan Gratch. 2025.
Mechanistic interpretability of emotion inference in
large language models. In Findings of the Associa-
tion for Computational Linguistics: ACL 2025, pages
13090-13120, Vienna, Austria. Association for Com-
putational Linguistics.

Curt Tigges, Oskar J. Hollinsworth, Atticus Geiger, and
Neel Nanda. 2024. Language models linearly rep-
resent sentiment. In Proceedings of the 7th Black-
boxNLP Workshop: Analyzing and Interpreting Neu-
ral Networks for NLP, pages 58-87, Miami, Florida,
US. Association for Computational Linguistics.

Alexander Matt Turner, Lisa Thiergart, Gavin Leech,
David Udell, Juan J. Vazquez, Ulisse Mini, and
Monte MacDiarmid. 2024. Steering language
models with activation engineering. Preprint,
arXiv:2308.10248.

Arya VarastehNezhad, Reza Tavasoli, Soroush Elyasi,
MohammadHossein LotfiNia, and Hamed Farbeh.
2025. Ai in mental health: Emotional and senti-
ment analysis of large language models’ responses
to depression, anxiety, and stress queries. Preprint,
arXiv:2508.11285.

Nutchanon Yongsatianchot, Parisa Ghanad Torshizi, and
Stacy Marsella. 2023. Investigating large language
models’ perception of emotion using appraisal the-
ory. In 2023 11th International Conference on Af-
fective Computing and Intelligent Interaction Work-
shops and Demos (ACIIW), pages 1-8.

Tenggan Zhang, Xinjie Zhang, Jinming Zhao, Li Zhou,
and Qin Jin. 2024. ESCoT: Towards interpretable
emotional support dialogue systems. In Proceedings
of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 13395-13412, Bangkok, Thailand. Association
for Computational Linguistics.

A Date Generation Prompt

This section shows the prompts used in the gener-
ation of the Dateset SEV. Eight everyday themes
were defined: Work/Job, School/Academia, Per-
sonal Relationships, Customer Service/Shopping,
Public Services/Administration, Health/Medical,
Housing/Living, and Travel/Transportation.

PROMPT TEMPLATE = """

Please generate 20 **first-person neutral scenarios** re-
lated to "{theme}".

Requirements:

- Each scenario must be one sentence in English.

- Use first person ("I ...").

- Describe objective background or activity, without any
emotional words or value judgments.

- Length should be 15-25 words.

Return as a numbered list.

Example (theme = Work/Job):

1. L attended a scheduled meeting with my colleagues this
morning.

2. I sent the weekly report to my manager on time.

3. I received the task list for this week from my supervisor.

woo

EVENT TEMPLATE ="""

Ihave a first-person scenario: "{scenario}"

Definitions:

- The *event* must be an outcome, change, or consequence
that happens **within or after** the scenario.

- It must build on the scenario’s context (same subject/time/-
participants) and **must not** merely restate the scenario.
Please write three event versions for this scenario:

1. Positive event: clear benefit, success, or smooth outcome.
2. Neutral event: plain factual outcome with no obvious
gain or loss (still an outcome, not a restatement).

3. Negative event: failure, loss, delay, or violation of expec-
tation.

Requirements:

- Minimal change across the three versions (same subject,
time, structure).

- Keep length similar (within +20%).

- Do NOT use explicit emotion words (e.g., "I am happy",
"T feel sad").

- Each version should be one sentence in English.

- OUTPUT: A SINGLE JSON OBJECT ONLY, with keys
"positive", "neutral", "negative".

Example:

Scenario: "I submitted my weekly project report to my
manager.”

Output:

"positive": "My manager immediately praised the key
points and invited me to present next week.",

"neutral": "My manager noted the milestones and said we
would discuss details later.",

"negative": "My manager pointed out the deliverable was
not up to standard and asked me to redo it this week."

won

B_ Annotation Prompt

This section shows the prompt used for deciding
if the generated text of LLMs matches the target
emotion.


SYSTEM = f””

You are a careful rater.

Given a target emotion and a text,

decide if the text’s STYLE matches the target emotion
among:

{EMOTIONS }

Focus on tone/attitude, not content valence.

USER TMPL =””

Target emotion: {emotion}

Text: {text}

Decide if the text’s STYLE matches the target emotion.
Return a compact JSON with keys exactly:

{
"match": <0 or 1>,
"reason": <short string>

}

299

C_ Emotion Expression Accuracy of
Prompting and Steering

This section reports the emotion expression accu-
racy for each emotion under two settings—prompt-
based and steering-based generation. The former
one is evaluated on SEV and test set, and the latter
one is on test set (see Table 1).

D_ Emotion Expression Accuracy under
Circuit Modulation

This section reports the per-emotion accuracy of
circuit-modulated generation on the test set. De-
tailed results are summarized in Table 2.

E_ Layer-wise Visualization of Emotion
Representations

This section presents the layer-wise clustering vi-
sualizations of hidden states for all samples suc-
cessfully guided by the prompting-based method,
as well as pure emotion vectors’ clustering visual-
izations (see Fig.4 and Fig.5).

F The prompts for Prompting-based
generation

We elicit emotional expressions in LLMs through
prompt-based guidance. The prompt template and
a representative example are as follows:

System Prompt Template: Always reply in {emotion}.
Keep the reply to at most two sentences.

User Prompt Template: {scenario} {event}

System message: Always reply in anger. Keep the reply to
at most two sentences.

User message: {I organized a team brainstorming session
to generate ideas for the upcoming product launch strategy. }

{The team generated a diverse set of innovative ideas that
were well-received and formed the basis of a successful
product launch plan. }

G_Layerwise Directional Consistency of
Emotion Subspaces

This section presents the pairwise cosine-similarity
heatmaps of emotion directions across all 56 resid-
ual positions (Attn0O, MLPO, ..., Attn27, MLP27).
As shown in Fig. 6, emotion subspaces remain
highly consistent in deeper layers: cross-sublayer
similarity typically exceeds 0.90 and never falls
below 0.85 across all six emotions. These results
support our definition of the reference directions

vl) using the stable range L € {21,...,25}.

H_ Experiments on Qwen2.5-7B-Instruct

H.1 Emotion Expression Accuracy of
Prompting and Steering

Table 3 reports the emotion expression accuracy of
prompt-based and steering-based methods. While
prompting achieves consistently high accuracy
across all emotions (84-96%), steering exhibits a
notable pattern: it succeeds for positive emotions
(happiness and surprise: >92%) but fails for neg-
ative emotions (anger, sadness, fear, and disgust:
<5%). This suggests that Qwen2.5-7B-Instruct has
strong inherent resistance to negative emotion steer-
ing, likely due to safety alignments that prevent the
model from expressing harmful or negative emo-
tional content through direct representational ma-
nipulation.

H.2 Layer-wise Visualization of Emotion
Representations

This section presents the layer-wise clustering vi-
sualizations of hidden states for all samples suc-
cessfully guided by the prompting-based method
on Qwen?2.5-7B-Instruct, as well as pure emotion
vectors’ clustering visualizations (see Fig. 7 and
Fig. 8).

H.3 Results of MLP Neurons Intervention
Experiments

This section presents the results of ablation & en-
hancement experiments conducted on Qwen?2.5-7B-
Instruct to verify the model-independent stability
of our findings. The setup strictly follows the same


LO Attention L1 Attention L2 Attention 13 Attention L4 Attention LS Attention L6 Attention

(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
ie a¢ 1.00
0.04.
075
010
02
oso
0.05
ale 025
0.00
0.02 0.00
0.05
0.04 0.25
010
0.06 0.50
015
0.08 075
=0.075-0.050-0.025 0.000 0.025 0.050 0.075, -015 -0.10 -0.05 000 005 010 015 =02 -01 09 01 o2 04 -02«00=*«tst 075 -050 -025 0.00 025 050 -to 050 os =io -05 00S
Lo MLe Lime Me ime La ML Ls MLP Le MLE
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
a0 1.00
10
°3 075
0.05, 02 050
os
am 025
0.00
00 0.00 00
01 0.25
0.05
02 0.50 05
0.10 0.3 0.75
-10
“010-005 000 005 010 -62 01 00 O1 O02 “03-02 -01 00 1 02 03 ~b4 02 00 02 04 -to 35 00 05 —fo -05 00s =to 05 00 05 10
L7 Attention LB Attention L9 Attention 10 Attention LLL Attention, L12 Attention L13 Attention
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)

“ | |
“ oa + | %

° ° %
bg : 2

(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)

0.50 ‘ i 2 3
OS ge y 2 oe

co : a x
0.00 1 e
025 Se ® °
0.50 + »
-0.75 2 &

© 9 | je, e|: «|: +: alit
Ane o | « | *|3¢ [sp el ge 8S
; é : s |: #| : ¥| >: eat 7 .

19.0
“a 0 2 4 6 4 2 0 2 & 5 $0 -25 00 25 50 100-75 50 -25 00 25 “100-75 30 25 00 25 50 0 3 Fy 05 0 3 Fa
L14 MUP a5 MUP Le MLP La7 MUP Las MUP Lag MLP 120 MUP
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)

te a’: fe |: <): | . % |- *
: ee BO.) Oe ; _@
. a |2 | = - |= ge ge

*

2 “
“ a |: ; ‘
8 be 10 bh
A shee cee “ee po Ue LL 8S
an aes a ete Lae tes —
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
15 20

. * |. | | # y £ | “le +
tte |. lp le d

5
e “s “to 10
10 “18 -15
|) LF 0 é |e asl @ e
10 ae a5 -20
oS “os OOS oo -is sto err oS 7 “io oa
21 mur za mp 23 mur La mur 2s mp 26 mer wr mp
is (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
a 20

.

10
10 ai 10 10

-s -10 » e
a 10 30

=t0

Figure 4: The layer-wise clustering visualizations of hidden states for all samples successfully guided by the
prompting-based method.


Lo attention La attention 2 attention L3 attention Laattention Ls attention Ls attention
(2688 samples) (2868 samples) (2688 samples) (2688 samples) (2686 samples) (2688 samples) (2688 samples)
oa
a F oe ors 8
as
0.002 Sout oor Py ure > 02
. ome 0.025 0.10
o.oo
0.000 0.00 0.00 oe ¢
0.001 0.01 Clie 0.050 ®@ ™~,
is BW =0.05 a
“ -0.005 0.02 -0075 % oa
0.02
0.010 aaa 0.125 id ci
“ola02 0800 0002 0.008 0.006 ~010i5 ~o.610 0.005 0.600 0.608 0.610 “oe or obo ont ob 030 eos —« 020-035 010-005 060 005 oo 2 aS 03 62 G1 00 03 02
Lome Lime Lm 13 mip La mp Ls mu Ls MP
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
o.oo , o.oe
it 0,010 0.08 0.15 oe
ne: a 0.02 0.02
0.005 & . 0.06 .
ppar 0.000 say 0.05
a F _— 0.02 bos 0.0 )
on] ff 0.005 a0 00 er ¢ 8
+ 008
rd bead 0s tn
-0.002 -.010 3 al
001 -006 é
0.003 -o.08 | ‘ 0.10 ty og
-0015 es
“pez 0.600 0.602 0.608 0.608 oGoe Tolaio—o 0s 0.600 oias 0.610 OI “Wo wot ogo oot ob “0 obo 00s alo “bis 910-005 000 008 oo Sie 26a oo
U7 attention Ls Attention L9 attention Lo Attention 121 Attention 12 Attention L13 Attention
(2688 samples) (2688 sampies) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
r 2.00 ‘
04 06 10 § 10 ¥ am y :
0.75 # Ls b 3
‘ a os
02 0.50 os 10 2
6 oa 025 os a
g 0.0 | eee 2
SA o
02 02 @ os . Es
“10
20] 9 ®
oa : 7 -10
“bag 002s aso io soo 033 ae aie is oo 0s ao Sr rr ae
Lym Lemur io mu Lio mL Lia mp waz mip a3 up
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
o 2s
04 Saal 1.00 i, i
oa my, i Fa 20 : p
07s 075 ° 3
0.2 oz r) 1s
oso oso os
% 00 ». @ we 0.25 os u
0.0 e — 0.0
e any ooo | eee” 0.00 gets oo ‘ a
01
02
are -05 2 -os
02 “04 ono i, a
. -1.0) oe ii: bd
-050 ‘/},) =
“a 06 7 -10 aus *
0.4 OTE: -3
jaa D7 050 0 080 035 050 ie bs 0b os Yo Tio so 0s das 3 z Z a ee a a 0 a
Lag attention Las attention 16 Attention 127 Attention Lag Attention 129 Attention 120 attention
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2668 samples) (2688 samples) (2688 samples)
, 00
@|: 9g). |: et] » r
: f as 10
@ ‘ 50
1 ® 3 5.0
2
° |? ° a 2s 5
® | oo ¢ oe
e - 55 e
2 ~~ °
“50 -s0 @
ial 5
4 e| er
@ % iia a B =
rr rr = 2 6 @ a is So as 0025 100 15 30 45 00 25 50 -ibo 1s 50 25 00 25 50 3 ya eT)
waa mp as MLp a6 mup Lime Lis mp Lao mup Lo mip
(2668 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
7 oo
g| g ‘ r 7s
§ . as 10 10
‘ 50
2 2 a
e 2
° °
ale a ry 0, pe $; | @ e ®
2 0
4 ¢ “25 i ) 0 a
,
€ | = 2; 8 ty %
rr ar er ar yoo 4s go as 00 25 50 too 3s 50 Bs 00 25 50 3 re) a SC~S “to rr
L22attention 22 attention 123 attention 24 attention 125 attention 126 attention 27 Attention
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
35 20
é 15 1s 2 20
10 - i 15 3s
10 0
e io Fy vo} @
5
® ° @ ° @ . 1. @ s| @ 5
°
° 0 °
° 0 0
, c s| . |]
'e Pp) Si Z
10
“10 is
-10} » 4
a - -20
“io rr) a “fo rar) <= to oF aes “is io oe a as a a “oto
wai mip za mup 23 mip 24 mLP 125 mL L6 mip warmup
(2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples) (2688 samples)
20
6 ro 20 20 ®
. é » é rs . # _ : ” # oe ff
10
a 20 nv} @ 10
5 5 s| © 5
a e 1 @ | @ . ®
0
¥ . ° 0 ° ° -10
iss ad J es . i
e “s -20
» = to
. - -10 10}. @
4 asl
a 0} @ ? 34 @ 40
-10 as ”
qo 58SEC =o ¢ 3 a0 <b to rrr wis to > 0 as rr e's oe rr a “ood

Figure 5: The layer-wise clustering visualizations of pure emotion vectors.


Table 1: Emotion expression accuracy (%) of prompt-based and steering-based generation for all six emotions.

Method Anger Sadness Happiness Fear Surprise Disgust
Prompting (SEV) 99.58 99.38 97.92 100.00 97.29 98.96
Prompting (Test Set) 100.0 99.79 97.49 99.58 97.50 99.38
Steering (Test Set) 93.33 96.04 99.58 96.88 67.71 93.75

Table 2: Emotion expression accuracy (%) of circuit-modulated generation across six target emotions, evaluated on
the held-out test set.

Valence Anger Sadness Happiness’ Fear Surprise Disgust

Positive 98.75 100.00 100.00 100.00 100.00 100.00
Neutral 98.12 100.00 100.00 100.00 100.00 100.00
Negative 97.50 100.00 99.38 100.00 100.00 100.00

Figure 6: Cosine-similarity heatmaps of emotion directions across all 56 residual positions for six emotions, showing
stable high similarity (>0.90) in deeper layers (21-25).

Table 3: Emotion expression accuracy (%) of prompt-based and steering-based generation for all six emotions on
Qwen2.5-7B-Instruct.

Method Anger Sadness Happiness Fear Surprise Disgust
Prompting (SEV) 96.25 84.38 88.54 91.46 90.42 84.79
Prompting (Test Set) 95.83 87.29 90.83 93.54 94.17 88.33

Steering (Test Set) 0.60 4.90 93.30 0.00 92.60 3.70


Lo attention La attention 2 Attention La Attention Us attention 15 attention Ls Attention
(954 samples)

Een eae eaten er) Eten ean
omer Lime Lemur wm Lemur smu Lemur
costumes) (ost artes) ostaernes) ee) ostStnbies) estab) o5tSinbies)
Ly Atetion Ls atetion Liz Attention 113 Attention
aes) sesame) Geen) Geer)
ao 2 = am = < fu ae
* o 5 ‘ wh - °
- o} Sei 0
20 0 . 20 aid ¥ 20 ¥
a0 e a ” 20 f
~ 40 30 7: bad
ume emer omer uiomur Lamp ume Lis me
(958 srtes) (ost sre) (ost sores) ae) (esd sams) (os4 Sambi) (05 sams)
Las Attention
(ise sartes)
7 Fa ~ # 7
os » . 7 Sn P

x) % ” a P & “ %

La mu Las uP Lae LP lar Mu Lie up Lio ML L20 MUP
(954 samples) (954 samples) (954 samples) (954 Samples) (954 samples) (954 samples) (954 samples)

‘ ; f i *.

a s 2 a 20 r) i” - 20 *
% . * Ae
re i & a % - - rg
Pr * %) . * a * Ps * a ®
° expat, ° 122 Attention ° ° 123 Attention ° 124 Attention ° L25 Attention ° ° L26 Attention ° 127 Attention

. eo. jf mel ge ae | a

Figure 7: The layer-wise clustering visualizations of hidden states for all samples successfully guided by the
prompting-based method on Qwen2.5-7B-Instruct.


©

#

+

ba

*

me

+

+

+

#

#

Figure 8: The layer-wise clustering visualizations of pure emotion vectors on Qwen2.5-7B-Instruct.



procedures described in Section 6. As shown in
Tables 4 and 5, Qwen exhibits the same long-tail
pattern as LLaMA: emotion scores drop/increase
sharply when a small number of top-ranked compo-
nents are intervened, whereas intervening random
components yields negligible change, confirming
that our identified components really play an im-
portant role in forming emotion representations in
LLMs, and only a few localized units dominate
emotion expression.

H.4_ Results of Attention Head Intervention
Experiments

This section reports the results of attention head-
level intervention experiments on Qwen2.5-7B-
Instruct, examining the causal role of attention
heads in emotion expression through both enhance-
ment and ablation approaches.

Table 6 presents enhancement results that top-k
attention heads are enhanced to amplify emotion in-
tensity, while Table 7 shows masking results where
top-k heads are ablated by zeroing their outputs.
The positive As values in enhancement and nega-
tive values in masking provide converging evidence
that the identified attention heads are causally im-
portant for emotion expression.

H.5 Emotion Expression Accuracy with Scale
2.0 Steering

This section reports the emotion expression accu-
racy of circuit-based steering with a scale factor
of 2.0 on Qwen2.5-7B-Instruct across different
valence contexts (positive, neutral, and negative).
The results demonstrate how steering effectiveness
varies across emotions and context valences. De-
tailed per-valence accuracy for all six emotions is
summarized in Table 8, which surpasses the per-
formance of the steering-based eliciting method on
the test set.


Table 4: Ablation results on Qwen2.5-7B-Instruct. Emotion scores (As) decrease sharply when top-ranked neurons
are ablated, while random ablations show minimal effect, mirroring the long-tail pattern observed in LLaMA.

Anger Sadness Happiness Fear Surprise Disgust Random

k

0 0.00 0.00 0.00 0.00 0.00 0.00 0.00
2 -100.30 = -155.54 -153.47 -71.40 -178.11 = -199.91 -0.27
4 -171.15 -221.37 -260.03 -131.05  -245.09 — -254.98 -0.38

8 -296.61  -283.06 -338.65 -197.56  -340.23 = -332.96 -0.76
16 -397.25. -353.57 -402.78 -286.53 -408.74 = -405.73 -1.30
32. -497.72 — -439.28 -473.85 -386.88 = -522.58 = -509.94 -3.13
64 = -566.12 -519.96 -552.92 -460.57  -609.85 = -602.93 -5.05
128 -657.05 -606.55 -635.91 -548.90  -674.13  -668.64 -9.81
256 -749.72  -691.59 -714.90 -621.62 -767.92 -752.66 -21.15
512 -836.63—_-795.71 -795.64 -740.43 —- -833.84 —- -839.28 -41.72

Table 5: Enhancement experiment results on Qwen2.5-7B-Instruct: Impact of enhancing top-k MLP neurons on
emotion intensity (As) for scale factor 1.0. Positive values indicate stronger emotion enhancement.

k Anger Sadness Happiness Fear Surprise Disgust Random
0 0.00 0.00 0.00 0.00 0.00 0.00 0.00
2 27.66 47.72 56.84 28.07 63.51 81.68 0.07
4 49.07 74.13 120.42 43.65 93.25 103.95 0.08
8 79.36 95.87 120.68 57.48 135.66 122.03 0.24

16 =109.25 133.57 145.88 88.77 183.86 152.59 0.34
32 = 155.65 169.27 176.17 103.94 229.29 201.22 0.71
64 194.43 203.02 198.51 138.36 271.75 232.80 1.29
128 230.85 225.38 224.84 162.71 302.17 262.59 2.15
256 248.49 246.82 234.40 193.01 320.71 284.72 4.10
512 269.03 271.94 255.48 226.23 343.89 304.38 7.84

Table 6: Attention head enhancement experiment results on Qwen2.5-7B-Instruct: Impact of enhancing top-k
attention head on emotion intensity (As) for scale factor 1.0. Positive values indicate stronger emotion enhancement
through head-level intervention.

Anger Sadness Happiness Fear Surprise Disgust Random
0.00 0.00 0.00 0.00 0.00 0.00 0.00
42.14 27.99 34.25 21.66 112.00 17.13 1.17
88.41 55.71 41.74 40.82 157.14 48.76 6.69
141.91 77.07 66.54 52.29 224.57 85.62 19.64
165.92 92.00 105.04 59.10 244.20 100.10 14.18
183.27 96.75 115.89 76.69 275.41 109.23 19.06

191.34 104.51 122.30 97.59 291.51 116.12 22.53
200.64 109.28 133.08 98.24 306.98 128.96 33.03
211.47 111.87 134.33 96.04 300.42 136.32 22.42
215.03 109.82 125.16 100.66 287.39 148.22 43.12
212.95 115.11 135.68 105.07 305.84 151.16 42.86

SPMIADNARWNY OO] F

Table 7: Attention head ablation experiment results on Qwen2.5-7B-Instruct: Impact of masking top-k attention
heads on emotion intensity (ASmean). Negative values indicate emotion reduction after head masking.

Anger Sadness Happiness Fear Surprise Disgust Random
0.00 0.00 0.00 0.00 0.00 0.00 0.00
-119.02 = -62.08 -78.53 -120.64 =-153.41 = -109.34 —- 128.53

-225.48 = -170.65 -150.63 -216.03 -219.96 = -223.36 ~—- 170.60
-286.47 = -231.54 -185.87 -268.63  -269.73 -286.77 —-240.39
-325.84 — -309.07 -241.42 -319.77. = -320.56 = -355.15. — -209.97
-358.13  -320.20 -260.47 -350.51  -349.54 = -377.35.——-195.45
-391.35 = -362.16 -313.11 -368.56 -371.71 -409.78 = -252.30
-454.21 — -419.93 -330.56 -434.26 -426.61 -474.55 = -274.80
-544.44 = -501.72 -433.99 -514.20  -517.43— --562.08 = -322.77
-466.15  -439.11 -358.29 -443.03  -451.28 = -491.12 — -348.99
-478.86  -459.43 -375.23 -477.13 -469.62 = -515.29 = -357.83

SPMIADNHRWNHH OC] F


Table 8: Emotion expression accuracy (%) of circuit-based steering (scale 2.0) across different valence contexts on
Qwen2.5-7B-Instruct.

Valence Anger Sadness Happiness Fear Surprise Disgust Average

Positive 7.50 31.87 100.00 31.87 100.00 100.00 61.87
Neutral 8.12 42.50 100.00 37.50 97.50 100.00 64.27
Negative 39.38 95.62 60.00 52.50 86.88 100.00 72.40

Average 18.33 56.66 86.67 40.62 94.79 100.00 66.18
