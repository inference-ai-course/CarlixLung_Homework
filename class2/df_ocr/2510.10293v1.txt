2510.10293v1 [cs.CL] 11 Oct 2025

arXiv

@

iQ
MatryoshkaThinking: Recursive Test-Time Scaling Enables
Efficient Reasoning

Hongwei Chen* Yishu Lei* Dan Zhang* BoKe  Danxiang Zhu = ==XuyiChen  Yuxiang Lu
Zhengjie Huang Shikun Feng’ Jingzhou He Yu Sun Hua Wu Haifeng Wang

ERNIE Team, Baidu

{chenhongwei04, leiyishu, zhangdan20, kebo01, zhudanxiang, chenxuyi, luyuxiang,
huangzhengjie, fengshikun01, hejingzhou, sunyu02, wu_hua, wanghaifeng}@baidu.com
*Equal contribution, ‘Corresponding author

Abstract

Test-time scaling has emerged as a promising paradigm in language modeling, wherein additional
computational resources are allocated during inference to enhance model performance. Recent approaches,
such as DeepConf, have demonstrated the efficacy of this strategy, however, they often incur substantial
computational overhead to achieve competitive results. In this work, we propose MatryoshkaThinking, a
novel method that significantly reduces computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME25 using only 4% of the computation
required by DeepConf. The core of our approach lies in the recursive exploitation of the model’s intrinsic
capabilities in reasoning, verification, and summarization, which collectively enhance the retention of
correct solutions and reduce the disparity between Pass@k and Pass@1. Comprehensive evaluations
across multiple open-source models and challenging multi-modal reasoning benchmarks validate the
effectiveness and generality of our method. These findings offer new insights into the design of efficient
and scalable test-time inference strategies for advanced language models.

Date: October 11, 2025

1 Introduction

Large Language Models (LLMs) exhibit remarkable reasoning abilities, which can be further enhanced by methods
that adjust behavior during test-time inference. Conventional test-time scaling methods can be broadly classified
into two categories: sequential and parallel scaling. Sequential scaling methods, exemplified by SELF-REFINE
(Madaan et al., 2023), improve reasoning by iteratively refining responses through self-verification and correction.
However, their gains quickly saturate with additional refinement steps, limiting their ability to exploit larger test-time
compute budgets. Parallel decoding (Wang et al., 2023) entails generating multiple candidate solutions and selecting
the most promising one through strategies such as majority voting or task-specific reward models.

Despite the benefits of parallel decoding, its effectiveness is constrained by substantial token consumption and the
diminishing returns of majority voting: generating more traces does not always improve accuracy and may even
degrade performance (Xue et al., 2023; Chen et al., 2024a). A key limitation is that traditional majority voting
treats all reasoning traces equally, regardless of their quality (Pal et al., 2024; Wang et al., 2025a), so low-quality
traces can disproportionately harm the final outcome.

Extending prior work, Chen et al. (2025) unifies sampling, self-verification, and self-correction into a single
test-time scaling pipeline, offering improved robustness over traditional approaches. However, these gains come at
the cost of substantial token consumption and computational overhead, and the method still suffers from diminishing
returns as the compute budget increases, which limits its scalability in practice. In test-time scaling, recent research
has concentrated on refining the strategies for selecting traces. Kang et al. (2025) offers a lightweight Best-of-N
method that leverages token-level probabilities to enhance parallel decoding without reward models, though it


Ce) MatryoshkaThinking October 11, 2025

aps : DeepConf@512(offline)
100 aa at Chae
ge a nvoshialhinking@ Loops
98.64
a) i RSA@Loop10
98 oe comes 12(online) ae
Majority Vote@512
Majority_Vote@256
2 96 © apa Oe @ Majority Vote@384
Ss
Pa eg licrity Vote@128
nm @DeepConf@32 (offline) SETS
3 ®. Majority Vote@32
g 94 @ Majority Vote@64
S Majority Vote@4
—_
< @RSA@Loop2
92
@ Majority Vote@2
@RSA@Loop1
90 @ GPT-OSS-120B
0 5 10 15 20 25 30

Max Token Cost (M) Per Query

Figure 1: Performance-—efficiency trade-off on AIME2025, showing pass@1 against inference token usage for
different test-time scaling strategies base on GPT-OSS-120B. Efficient Reasoning via MatryoshkaThinking lies in
the upper-left region, reflecting higher performance at lower inference cost. Each query is sampled 32 times.

remains vulnerable to overconfident errors and struggles on tasks with unique answers. Complementarily, DeepConf
(Fu et al., 2025) introduces confidence-aware reasoning with dynamic self-reflection, yielding gains in multi-step
reasoning but at the cost of higher computation and reliance on heuristic thresholding.

To address these challenges, we propose MatryoshkaThinking, an efficient and general reasoning enhancement
method. It leverages the model’s inherent capabilities for sampling, judgment, and summarization to improve
performance at inference time, without requiring additional training or auxiliary models, and can be seamlessly
integrated into existing reasoning services. MatryoshkaThinking is applicable across different modalities of LLMs and
demonstrates strong accuracy and efficiency on various multimodal benchmarks. We evaluate MatryoshkaThinking
on multiple reasoning benchmarks and models across different modalities. Experimental results show that
MatryoshkaThinking significantly enhances reasoning performance while consuming fewer tokens compared to
other test-time scaling methods.

In summary, our main contributions are as follows:

* We propose a novel test-time scaling method, MatryoshkaThinking, which requires no additional models
and relies solely on self-prompting to enhance model capabilities at inference. Under the same token
budget, it outperforms existing methods such as DeepConf (Fu et al., 2025) and SETS (Chen et al.,
2025). Furthermore, our method demonstrates superior reasoning efficiency, as evidenced by the steeper
performance scaling curve shown in Figure 1. More importantly, MatryoshkaThinking effectively transfers
the capability of Pass@k to Pass@1, enabling a single inference attempt to approach the performance level
traditionally requiring 32 samples as Figure 2 shown.

¢ We conduct extensive experiments across multiple reasoning tasks and models in text, vision, and audio
modalities. The results demonstrate that our method can significantly improve reasoning performance by
increasing compute budget, highlighting its generality and broad applicability.

* We conducted extensive experiments, utilizing a total of 99.64B tokens, which provided valuable insights
into the challenges and potential of test-time scaling.

2 Method

We propose an inference method called MatryoshkaThinking, which leverages the intrinsic generative, discrimina-
tive, and summarization capabilities of large language models to achieve performance scaling through additional
compute at test time. As illustrated in Figure 3, MatryoshkaThinking begins with parallel sampling, in which
multiple candidate responses are generated simultaneously for the same query. These candidates are then passed
through a self-verify module that filters responses with model’s internal consistency and logical validity. Based
on the verification results, the model generates the summarization result that fuses reliable information from the
retained candidates.


) ° °
6) MatryoshkaThinking October 11, 2025
on MatryoshkaThinking on AIME2025 of ERNIE-4.5-21B-A3B-Thinking -
954 93.33 P95
@
%\
a 90.00 90.00 90.00
90 4 Ny LAS en 2 790
‘86.66 86.66 86.66 86.66,-7~ “86.66 86.66, *. 86.66.77 <3
© e----- @----- ~----- ‘e------ ¥ = cn
a 854 86.04 86.14 8645 86.56 Les ©
a nN
= a
80 4 + 80
77.18
754 } 75
70 -— + 70
0 1 2 3 4 5 6 7 8 9 10

Loop

Figure 2: MatryoshkaThinking significantly enhances the Pass@1 accuracy of ERNIE-4.5-21B-A3B-Thinking on
AIME2025, with performance converging toward Pass @32 as the number of recursive loop increases.

Loop

The Framework of
Matryoshka & Thinking

( Parallel Decoding

Solution #1 —+» __ Self-verify

Sampling [summary

Self-verify

> Solution #2 I

Solution #M

| Summary |-+ Final Solution: 6

— _ Self-verify

Ss

Figure 3: Overview of our MatryoshkaThinking. As shown in the figure, MatryoshkaThinking consists of three key
components: self-verify, summary, and the iterative loop between them. With test-time scaling, recursive self-verify
and summary identify promising solutions, ultimately consolidating them into a robust final solution.

MatryoshkaThinking is extended with an iterative loop that repeats the summarize—verify cycle across multiple steps.
In each iteration, the model first summarizes the accumulated set of previously verified outputs to generate a refined
candidate set. This new set is then re-evaluated through the self-verification process. The accepted solutions are
merged with prior results before the next summarization round. Through this iterative refinement and aggregation,
the method progressively enhances solution quality and converges toward a high confidence final answer.

Specifically, we denote the answering prompt as /,,(x), the self-verify prompt as J,,(x, y), and the summary prompt
as I,(x, C). Here, x represents the input query, C represents solution candidates, y;' represents the m-th solution
generated by the model in the /-th iteration for query x, and vj” represents the model’s self-verify result for the pair
(x, y/"). As shown in Equation (1), the judgment function J(v) outputs | if the solution is self-verified as correct,
and 0 otherwise:

1 ify is self-verified as correct
0 otherwise

J(v) -|

where v denotes the self-verify process.

()

Let © represent the process of a large language model (LLM) taking a prompt as input and generating a response
as output. In our MatryoshkaThinking framework, the iterative summary and verify procedures are formulated as
follows:


(2)

Ce) MatryoshkaThinking October 11, 2025

yo ~ OUa(x)), (2)
yy ~ OUs(x, Ci-1)), (3)
vi ~ OUy (x, y7")), (4)
Cp = Cy U {y" | m= 1,...,M;5 J (v7") = 1} (5)

In this process, y¢’ is the initial candidate generated from the input x. In each subsequent iteration /, new candidates
y;' are inferred based on the input x with the current candidate set C;_; and then verified to obtain the self-verify
result vi". The set C; is updated with the candidates that have passed. The final answer is given by x with Cy
produced in the final iteration L. The pseudocode of our algorithm is summarized in Algorithm |.

Algorithm 1 MatryoshkaThinking Pipeline

Input The query x, the LLM ®, the solution candidate set C, the answering prompt Ja, the self-verify prompt J,,, the
summary prompt /,, the number of loop L, the number of parallel decode samples M and the judgement function J.

1:CeO

2: for] =0to Ldo

3: for m = 1 to M do > Parallel Sampling
4: if / = 0 then

5: yy — OUa(x)) > First Answer Step
6: else

7: yy — OUs(x, C)) > Summary Step
8: end if

9: vl — OU (x, yi") > Self-verify step
10: end for
11: C—CU{y"|m = 1,...MsJ(v") = 1} > Update Candidate Set
12: end for

13: yp — OUs(x,C))
Output Final summarized answer y;

Departing from conventional voting-based aggregation strategies (Chen et al., 2025; Fu et al., 2025), our method
introduces a unified iterative framework that combines latent summarization and self-verification. This enables
dynamic information integration and error correction across reasoning paths. The overall design provides three key
advantages:

1. Broader Applicability: A key advantage of the summarization approach lies in its flexibility. Unlike methods
limited to tasks with enumerable or directly comparable outputs, such as classification and multiple-choice QA, it
generalizes to open-ended problems with diverse and unconstrained answers. Majority voting, which relies on
surface form agreement over a fixed output set, is ineffective in such contexts. In contrast, our method performs
semantic-level aggregation by leveraging latent representations across multiple parallel reasoning paths, enabling
robust inference across domains including generative question answering, creative text generation, and code
generation.

2. Error Correction: Another core advantage of the MatryoshkaThinking framework lies in its integration of
self-verify ability, which serves as an internal filtering mechanism to improve answer quality. At each iteration,
candidate outputs are subjected to self-verify before being retained, helping the model eliminate implausible or
logically inconsistent solutions. This step mitigates error propagation across reasoning stages and ensures that
only semantically consistent and logically outputs contribute to subsequent summarization. Unlike majority
voting, which treats all candidates equally, self-verify introduces a quality control layer that significantly enhances
the reliability and consistency of final predictions.

3. Robustness: The combination of self-verification and latent summarization enables the model to remain robust
even when no single candidate output is fully correct. Majority voting fails in such scenarios, as it merely
amplifies the most common error. In contrast, our method integrates partial signals from multiple imperfect
outputs at the representation level, allowing the model to recover coherent and high-quality solutions through
iterative refinement.

At its core, MatryoshkaThinking transforms traditional single-pass inference into an iterative, self-reflective process
that unifies generation, self-verification, and summarization. By systematically filtering and refining candidate
solutions across iterations, it enhances semantic coherence, correctness, and robustness. This design not only
improves performance under fixed computational budgets, but also generalizes effectively to a wide range of
reasoning-intensive tasks.


Ce) MatryoshkaThinking October 11, 2025

3 Related Work

3.1 Large Reasoning Model

Reasoning is a highly sought-after capability for large language models (LLMs). Early attempts to enhance
reasoning abilities in LLMs primarily focused on mathematical problem-solving Hendrycks et al. (2021b), where
outcome-based or process-based verifiers were trained to supervise the learning process (Lightman et al., 2024).
Chain-of-Thought (CoI) prompting (Wei et al., 2022) and its subsequent improvements (Wang et al., 2023; Yao
et al., 2023; Besta et al., 2024) have aimed to guide LLM in generating reasoning pathways that would lead to correct
answers. More recent studies suggest that LLMs can emulate human-like System | (fast, intuitive) and System 2
(slow, deliberative) thinking (Yu et al., 2024), enabling more effective handling of logical tasks. Reward-guided
decoding approaches (Khanov et al., 2024; Liao et al., 2025; Xie et al., 2023) have built on these foundational
techniques by incorporating external or prospective feedback. The release of OpenAI-ol (Jaech et al., 2024) and
DeepSeek-R1 (DeepSeek-AI et al., 2025) has further fueled interest in test-time scaling techniques within the
research community.

3.2 Test-time Scaling

Recent studies have shown that incorporating additional computational resources during inference can enhance the
performance of large language models effectively (Welleck et al., 2024). Test-time scaling methods can be broadly
categorized into two approaches: parallel scaling and sequential scaling (Balachandran et al., 2025). Parallel
scaling involves sampling multiple responses from the same model and aggregating them using operators such as
self-consistency, adaptive-consistency, majority voting, reward model scoring, or confidence estimation (Wang et al.,
2023; Brown et al., 2024; Fu et al., 2025; Li et al., 2024; Wan et al., 2025; Wang et al., 2025b;c; 2024; Aggarwal
et al., 2023). Recursive Self-Aggregation (RSA) improves reasoning ability by repeatedly aggregating multiple
randomly sampled solutions generated by the model itself (Venkatraman et al., 2025). Sequential scaling employs
an iterative refinement strategy, using the model’s own feedback to continuously improve responses until the output
meets the predefined verification criteria (Madaan et al., 2023; Muennighoff et al., 2025). Furthermore, when
process-based verifier reward models are available, the test-time computation can be further expanded through
strategies such as beam search or look-ahead search (Snell et al., 2024). This work investigates efficient test-time
scaling without relying on external reward models. (Muennighoff et al., 2025) leverages massive self-verification
and self-improvement though extremely costly, achieving gold-medal performance in the International Mathematical
Olympiad of 2025 and thus demonstrating the full potential of test-time scaling. We propose a simple yet effective
approach that synergistically integrates parallel and sequential scaling strategies. This approach achieves superior
performance compared to methods using either strategy alone. Although Snell et al. (2024) also explored combining
parallel sampling with sequential revision, their method requires training task-specific verifiers and revision models,
which is often impractical due to the high cost of data collection. On the other hand, (Chen et al., 2025) also
combines the two strategies, its self-correction mechanism involves multiple iterative loops, requiring substantial
token consumption to achieve comparable accuracy, and its voting mechanism exhibits limitations in scenarios
with insufficient answer diversity. To validate the effectiveness of our approach, we conduct comprehensive and
extensive evaluations. Unlike works such as Snell et al. (2024) and Fu et al. (2025), which are evaluated solely on
text-based benchmarks, our proposed MatryoshkaThinking method is systematically assessed on text, visual and
audio benchmarks. Extensive experiments on various open-source models, including standard and thinking-enhanced
models, demonstrate effectiveness and strong generalization capability of our approach.

4 Experiment

4.1 Setup
4.1.1 Benchmarks

To comprehensively evaluate the effectiveness of our method, we select a set of reasoning benchmarks across multiple
modalities. For text reasoning tasks, we employ AIME 2024 (Balunovié et al., 2025), AIME 2025 (Balunovic et al.,
2025), MMLU (Hendrycks et al., 2021a), and LiveCodeBench (Jain et al., 2025), which collectively evaluate the
model’s competencies in logical reasoning, factual inference, and algorithmic implementation. For image reasoning
tasks, we adopt MathVista (Lu et al., 2024) and MMMU (Yue et al., 2024), both of which require cross-modal
reasoning and complex problem-solving abilities. Finally, for audio reasoning tasks, we evaluate on MMSU (Chen
et al., 2024b) and MMAU (Sakshi et al., 2025), which assess the reasoning ability of audio modalities.

4.1.2 Baselines

To ensure the fairness of our comparison, we restrict our analysis to methods that have not undergone additional
training or utilized external reward models. Specifically, we adopt DeepConf (Fu et al., 2025), SETS (Chen et al.,


Ce) MatryoshkaThinking October 11, 2025

Table 1: Comparison of different baselines with MatryoshkaThinking.

Method Sampling  Self-Verify Summay Loop
DeepConf (Fu et al., 2025) ¥ v x x
SETS (Chen et al., 2025) v v x x
RSA (Venkatraman et al., 2025) Jv x v v
MatryoshkaThinking v v v v

Table 2: Hyper-parameters of different models in our experiments.

Model Temperature ‘Top-p Top-k Max-tokens
GPT-OSS-120B 1.0 1.0 -1 64,000
Seed-OSS-36B-Instruct 1.1 0.95 -1 64,000
Qwen3-30B-A3B-Thinking-2507 0.6 0.95 -1 64,000
Qwen3-30B-A3B-Instruct-2507 0.6 0.95 -1 64,000
Qwen3-4B-Thinking-2507 0.7 0.8 -l 64,000
Qwen3-4B-Instruct-2507 0.7 0.8 -1 64,000
ERNIE-4.5-21B-A3B-Thinking 0.9 0.9 -1 64,000
ERNIE-4.5-VL-28B-A3B 0.2 0.8 -1 64,000

Qwen2.5-Omni-7B 0.9 1.0 -1 8192

2025) and RSA (Venkatraman et al., 2025) as our main baselines. DeepConf enhances reasoning efficiency and
performance by leveraging local confidence signals to dynamically filter low-quality reasoning traces. SETS, on the
other hand, integrates parallel and sequential strategies by unifying sampling, self-verification, and self-correction
into a single framework, thereby improving test-time reasoning. In SETS, we configured sampling 32 times and
self-correction 3 rounds, while in RSA, we used a trajectory population of 32 and random sampled 2 candidate
solutions, as adding more candidates without processed reasoning output would exceed the max sequence length.
Three methods have significantly advanced test-time scaling and provide strong baselines for evaluating our approach.
We summarize the key differences between our method and the baselines, as shown in Table |.

4.1.3 LLM Configs

For the multi-modal evaluation, we selected strong open-source models that represent the state of the art in their
respective domains. In textual reasoning tasks, the thinking models include GPT-OSS-120B (Agarwal et al., 2025),
Seed-OSS-36B-Instruct (Team, 2025), Qwen3-30B-A3B-Thinking-2507 (Yang et al., 2025), Qwen3-4B-Thinking-
2507 (Yang et al., 2025), and ERNIE-4.5-21B-A3B-Thinking (Baidu-ERNIE-Team, 2025). For comparison, we also
considered non-thinking models, namely Qwen3-4B-Instruct-2507 (Yang et al., 2025) and ERNIE-4.5-21B-A3B
(Baidu-ERNIE-Team, 2025). For vision-language tasks, we used ERNIE-4.5-VL-28B-A3B (Baidu-ERNIE-Team,
2025), while for audio-language tasks, we adopted Qwen-2.5-Omni-7B (Xu et al., 2025). This setup ensures a
systematic assessment of our approach across different modalities and both thinking and non-thinking LLMs.

The hyper-parameters in our experiments of different models are shown in Table 2.

4.1.4 Metric Settings

In our experiments, we evaluate each benchmark dataset using the pass@k (Chen et al., 2021) metric as the equoation
(6) shown:
("") |

(i)

Here, n denotes the total number of candidate solutions generated for a given problem, c represents the number
of correct solutions, k is the number of sampled solutions, and (") refers to the binomial coefficient. Intuitively,
pass @k measures the probability that at least one correct solution is included when randomly sampling k solutions
from n candidates. In addition to LiveCodeBench, we use the LLM-as-a-Judge (Desmond et al., 2025) approach
to evaluate the correctness of the answers. REMARK Unless otherwise specified, all experimental results of
MATRYOSHKATHINKING correspond to the setting with Loop = 2.

pass@k := Eproblems r _ (6)

4.2 Results

We evaluated our method against several representative test-time scaling baselines, including DeepConf@32
(offline) (Fu et al., 2025) (64M tokens, Pass@ 1 = 95.00), SETS (Chen et al., 2025) (272M tokens, Pass@1 = 95.31),
and RSA (Venkatraman et al., 2025) (23M tokens, Pass@1 = 98.02). Each query is sampled 32 times. Voting-based


(2)

Ce) MatryoshkaThinking October 11, 2025

methods must select a final answer under specific conditions for each query, which scales computation by a factor of
32 when producing 32 answers. In contrast, our method conducts 32 condition-based summarizations in an additive
rather than multiplicative fashion, yielding a substantial reduction in computational cost.

As shown in Table 3, our method achieves Pass@ 1 of 99.79 while consuming only 42M tokens. Compared with
SETS, it delivers a +4.48 absolute improvement in Pass@1 while using merely 15% of its 272 M token cost. In
addition, our method attains near-perfect accuracy comparable to DeepConf@512 (offline) yet requires only 4% of
its computational budget. Relative to RSA, our approach uses slightly more tokens (42 M vs. 23 M) but yields a
+1.77 higher Pass@ 1, further highlighting its superior accuracy—efficiency trade-off. The performance of RSA is
restricted by the use of random sampled candidates rather than self-verify selection and by the unrefined processing
of reasoning outputs. Self-verify enables our method to gather numerous robust candidate solutions, leading to
improved summary quality. These results demonstrate that our approach achieves near-optimal accuracy at a fraction
of the cost, outperforming all prior methods in both efficiency and effectiveness. Moreover, it bridges the gap
between low-cost approaches like RSA and high-accuracy methods like DeepConf@512 (offline), showing strong
adaptability across computational budgets and highlighting its potential for large-scale deployment.

Table 3: Comparison of different test-time scaling methods on GPT-OSS-120B evaluated on the AIME2025 dataset.
The table reports Pass @ 1 performance and the max token cost between different test-time scaling methods. The
result of DeepConf@512 is taken from the original paper (Fu et al., 2025).

Method Pass@1 MaxTokenCost
Majority Vote @32 94.66 64M
Majority Vote @ 128 95.33 262M
Majority Vote @512 97.00 1024M
DeepConf @ 32 (offline) 95.00 64M
DeepConf@512 (online) 97.90 160M
DeepConf@512 (offline) 99.90 1048M
SETS 95.31 272M
RSA 98.02 23M
MatryoshkaThinking 99.79 42M

Next, we evaluate the generality of our approach in the AIME2025 dataset by applying MatryoshkaThinking to a
variety of models. As shown in Table 4 all models achieve consistent improvements in Pass@ | over their baselines.
Specifically, GPT-OSS-120B improves by +9.90, Seed-OSS-36B-Instruct by +3.23, Qwen3-30B-A3B-Thinking-2507
by +4.48, Qwen3-4B-Thinking-2507 by +7.94, and ERNIE-4.5-21B-A3B-Thinking by +9.38. Nonthinking models
also benefit substantially as shown in Table 4, with Qwen3-30B-A3B-Instruct-2507 improving by +26.04 and
Qwen3-4B-Instruct-2507 improving by +18.02. These results confirm that MatryoshkaThinking reliably boosts
Pass@ 1 across diverse model families, demonstrating both its robustness and its broad applicability.

Table 4: Performance of MatryoshkaThinking on AIME2025 across different models. * indicates that Matryoshka-
Thinking is set with loop=10; otherwise, the setting is loop=2.

Type Model MatryoshkaThinking Pass@1 Pass@8 Pass@16  Pass@32
89.89 96.78 98.81 100.00
GFEOSs-1206 99.79 100.00 100.00 _ 100.00
84.79 90.49 92.35 93.33
88.02 92.62 93.26 93.33
84.58 92.83 93.30 93.33
89.06 90.00 90.00 90.00
81.56 89.65 89.98 90.00
89.50 90.00 90.00 90.00
TIA8 90.40 92.44 93.33
86.56 87.50 88.33 90.00
43.64 60.14 64.98 73.33
69.68 70.00 70.00 70.00
37.29 55:52 59.15 63.33
55.31 59.97 60.00 60.00

Seed-OSS-36B-Instruct

Thinking Model Qwen3-30B-A3B-Thinking-2507

Qwen3-4B-Thinking-2507

ERNIE-4.5-21B-A3B-Thinking™

Qwen3-30B-A3B-Instruct-2507

Non-Thinking Model

Ke} \ ><] Ke} Se} K 9<} Ke} 98

Qwen3-4B-Instruct-2507

Furthermore, we extend MatryoshkaThinking to a broad spectrum of reasoning tasks spanning text-only, vi-
sion—language, and audio—language scenarios. As shown in Table 5, on text-based reasoning with GPT-OSS-120B,
our method improves Pass@ 1 by +0.96 on MMLU and +1.6 on LiveCodeBench. For vision—language reasoning,
using ERNIE-4.5-VL-28B-A3B, we observe gains of +1.74 on MathVista and +2.55 on MMMU. For audio—language
reasoning, with Qwen2.5-Omni-7B as the base model, Pass@1 increases by +7.57 on MMSU and +5.83 on MMAU.
These consistent improvements across diverse modalities demonstrate the robustness and broad applicability of
MatryoshkaThinking.


Ce) & MatryoshkaThinking October 11, 2025

Table 5: MatryoshkaThinking consistently achieves gains across all modalities, with the largest improvement being
+7.57 on MMSU, demonstrating the method’s effectiveness and generality. MMLU and LiveCodeBench V6 results
are reported using GPT-OSS-120B. MathVista and MMMU results are reported using ERNIE-4.5-VL-28B-A3B.
MMSU and MMAU results are reported using Qwen2.5-Omni-7B.

Benchmark MatryoshkaThinking Accuracy

Text Reasoning & Coding Tasks (GPT-OSS-120B)

x 87.13

MMLU / 88.09

. x 88.46

LiveCodeBench V6 V 90.06
Vision Reasoning Tasks (ERNIE-4.5-VL-28B-A3B)

. x 76.92

MathVista V 78 66

x 68.55

MMMU v 71.10

Audio Reasoning Tasks (Qwen2.5-Omni-7B)

x 58.54

MMSU v 66.11

x 70.08

MMAL / 75.91

4.3 Ablation Study

Our previous experiments have established that MatryoshkaThinking efficiently enhances reasoning capabilities by
increasing the test-time computation, while consuming significantly fewer tokens than baseline methods. Building
on these findings, we now turn to a deeper analysis of their implications for advancing test-time scaling.

Table 6: Ablation study of component contributions for GPT-OSS-120B on AIME2024 and AIME2025.

Method AIME2024 (Pass@1) AIME2025 (Pass@ 1)
MatryoshkaThinking 99.89 99.79
w/o Loop 98.12 98.64
w/o Loop & Self-Verify 97.18 96.45
w/o Loop & Self-Verify & Summary. 90.31 89.89

4.3.1 Analyzing the Relative Contribution of Components

To quantify the contribution of each core component in MatryoshkaThinking, we conducted step-wise ablation
experiments on the AIME2024 and AIME2025 benchmarks using the GPT-OSS-120B model. Specifically, we
progressively removed the Loop, Self-Verify, and Summary modules to examine their individual and cumulative
impacts on performance. As shown in Table 6, removing the Loop mechanism caused moderate degradation in
Pass@ 1 accuracy—dropping from 99.89% — 98.12% (-1.77) on AIME2024 and 99.79% —> 98.64% (-1.15) on
AIME2025. Further excluding the Self-Verify component resulted in a sharper decline to 97.18% (-2.71) and 96.45%
(-3.34), respectively. Finally, removing all three modules, including Summary, led to substantial drops to 90.31%
(-9.58) on AIME2024 and 89.89% (-9.90) on AIME2025. These results clearly show that each module contributes
critically to the overall reasoning performance of MatryoshkaThinking.

4.3.2 Summary is not only Summary

In MatryoshkaThinking, the summary component not only compresses content but also provides inductive reasoning
and error correction. We designed an experiment where we selected 24 questions and constructed Summary context
with varying proportions of correct solutions to examine their effect on the model’s accuracy performance.

As shown in Figure 4, when we supply only incorrect solution candidates and instruct the model to summarize them,
models can still deduce the correct answer. For instance, GPT-OSS-120B achieves an accuracy of 84.61% despite
the summary context containing no correct answers. Other models also exhibit non-zero accuracy: Seed-OSS-
36B-Instruct attains 61.53%, Qwen3-30B-A3B-Thinking-2507 reaches 46.15%, ERNIE-4.5-21B-A3B-Thinking
scores 23.07%, while both Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507 achieve 7.69%. These results
demonstrate that during the summary phase, models engage not merely in summarization, but also in inductive
reasoning and error correction. Furthermore, these additional capabilities vary significantly depending on model
capability.


Ce) MatryoshkaThinking October 11, 2025

100 5 [2] GPT-OSS-120B
[1 Seed-OSS-36B-Instruct
C4) Qwen3-30B-A3B-Thinking-2507
84.61 [= ERNIE-4.5-21B-A3B-Thinking
80 4 T=) Qwen3-4B-Thinking-2507
CE) Qwen3-4B-Instruct-2507
> 61.53
oO zl ——————
8 60
=
5
oO 46.15
Oo Es
< 40 /
23.07
204
7.69 7.69
0

Figure 4: The figure illustrates the accuracy of various models when tasked with summarizing context without
any correct answers. The results reveal that stronger models like GPT-OSS-120B (achieving 84.61% accuracy)
are able to correctly identify answers based on reasoning and error correction, while weaker models such as
Qwen3-4B-Instruct-2507 (scoring 7.69%) show limited or no inductive reasoning ability. These findings highlight
the critical role of the Summary phase in enhancing model performance through inductive reasoning and error
correction, especially for stronger models.

4.3.3 Investigating the Impact of Varying Proportions of Correct Answers in Context

100

90

80

70

Accuracy

60

50 GPT-OSS-120B

—*— Seed-OSS-36B-Instruct

—— Qwen3-30B-A3B-Thinking-2507
Qwen3-4B-Thinking-2507

—e ERNIE-4.5-21B-A3B-Thinking

30 —*— Qwen3-4B-Instruct-2507

40

0.0 0.2 04 0.6 0.8 1.0
The ratio of correct answer

Figure 5: The figure demonstrates that, as the proportion of correct answers in the summary context increases,
most models performance improves. In particular, for stronger models like GPT-OSS-120B, the summary method
outperforms majority voting, especially when the proportion of correct answers in the context is small. This ratio of
correct answers simulates the model’s self-verify capability in real-world applications, highlighting that the model’s
verification ability is crucial for improving performance.

As observed in earlier experiments, models achieved non-zero accuracy even in the absence of correct answers
provided in the summary context. To further investigate this capability, we examined how model performance is
influenced by varying the proportion of correct answers included in the summary context.

We designed an experiment where we selected 24 questions and constructed summary context with varying
proportions of correct solutions to examine their effect on the model’s accuracy performance. The results, as shown
in Figure 5, indicate that for strong models like GPT-OSS-120B, introducing just 10% correct solutions into the
Summary condition boosted the Pass@1 score to 100%. In contrast, using Majority Voting for aggregation failed to
achieve this level of performance. This shows that for stronger models, the summary method outperforms the Voting
method, especially when aggregating multiple reasoning paths, as it better captures valuable answers. However, for
weaker models, such as Qwen3-4B-Instruct-2507, even with a 60% correct answer ratio in the summary context,


Ce) MatryoshkaThinking October 11, 2025

MathVista MMMU
100 100
|
sy Bs aT
A _— E po
>) a i) oT
< eee < _—
a
—-— ERNIE-4.5-VL-28B-A3B — —-— ERNIE-4.5-VL-28B-A3B
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
The ratio of correct answer The ratio of correct answer
(a) (b)
MMSU MMAU
100 —— 100 —
Pal — a
ep / ep
= al ye
5 50 r 5 50 pm
Z JA Z i
a |
| —— Qwen2.5-Omni-7B | we —— Qwen?2.5-Omni-7B
: 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
The ratio of correct answer The ratio of correct answer

(c) (d)

Figure 6: Scaling effects of verification quality in summarization across multimodal benchmarks. (a) ERNIE-4.5-VL-
28B-A3B on MathVista benchmark; (b) ERNIE-4.5-VL-28B-A3B on MMMU benchmark; (c) Qwen2.5-Omni-7B
on MMSU benchmark; (d) Qwen2.5-Omni-7B on MMAU benchmark. Our findings indicate that higher correct
solution ratios consistently lead to better Pass@1 performance, underscoring the critical role of verification quality
across modalities.

they still could not reach full accuracy. This suggests that, in the case of weaker models, the summary method does
not always outperform voting. Nevertheless, we observed that in most cases, especially with stronger models, the
summary method provided significantly better performance than voting.

The experimental results show that the ratio of correct solutions in the summary context reflects the self-verification
capability of models in the practical application of MatryoshkaThinking. As illustrated in the Figure 5, when the
summary context contains a higher proportion of correct solutions, models with strong summarization abilities
achieve greater improvements, while models with weaker summarization abilities also benefit but to a smaller extent.
This indicates that the extent of improvement depends not only on self-verification but also on the model’s intrinsic
summarization ability. In other words, self-verification acts as an enhancement rather than a remedy: it strengthens
existing summarization capacity but cannot compensate for its absence.

To further examine whether the benefit of increasing the proportion of correct solutions in the summary context
generalizes beyond textual reasoning, we conducted analogous experiments on visual and audio tasks. As shown in
Figure 6. Using ERNIE-4.5-VL-28B-A3B for visual tasks and Qwen2.5-Omni-7B for audio tasks, we evaluated
the models on benchmarks like MathVista, MMMU, MMSU, and MMAU. The results consistently showed that
introducing just 10% correct solutions into the Summary condition improved the performance across all benchmarks:
MathVista (+0.79), MMMU (+6.46), MMSU (4+4.50), and MMAU (+7.07). Moreover, further increasing the
proportion of correct solutions continued to boost performance, underlining the crucial role of self-verification in
enhancing MatryoshkaThinking’s effectiveness, even in multimodal contexts.

4.3.4 More Loops Bring Stronger Confidence To Truth

To further investigate the impact of MatryoshkaThinking iterations on the uncertainty of model predictions, we
conducted experiments on the AIME2025 dataset using GPT-OSS-120B. In this setup, the iteration variable is
denoted as Loopy, where N € [0,2]. At each iteration step, we calculated the entropy of the distribution about the
candidate answers to quantify the model’s uncertainty in its predictions. The results, as shown in Figure 7, reveal a
clear trend: as N increased from 0 to 2, the entropy of the generated answers decreased progressively. This indicates
that additional iterations in MatryoshkaThinking effectively reduce uncertainty, thereby enhancing the model’s
confidence in its predictions.

10


© MatryoshkaThinking October 11, 2025

Entropy across Loops

GPT-OSS-120B 0.278 0.227 1.6
1.4

Seed-OSS-36B-Instruct 0.408 0.339
1.2
Qwen3-30B-A3B-Thinking-2507 0.053 0.053 7
Qwen3-4B-Thinking-2507 0.189 0.053 o8
0.6

ERNIE-4.5-21B-A3B-Thinking 0.310 0.226
-0.4
Qwen3-4B-Instruct-2507 0.218 0.140 - 0.2

i 2

Loop Iterations

Figure 7: The color intensity represents the magnitude of entropy. As the number of loop iterations increases, the
entropy of the model’s answers gradually decreases. This indicates that the model becomes more confident in its
predictions, but at the same time, its ability to explore diverse solution paths is suppressed.

The reduction in entropy explains the observed improvement in Pass@1, as the model becomes more confident
about its answers within each iteration. Furthermore, we also notice that when the number of iterations reaches 2,
the entropy level plateaus, suggesting that further increasing the compute budget would yield diminishing returns.
This trend helps explain why MatryoshkaThinking leads to an increase in Pass@ 1 for some models, while causing a
decrease in Pass@32, as shown in Table 4. As entropy decreases and the model’s confidence grows, its exploration
capability is suppressed. Consequently, the model becomes less likely to generate diverse, potentially correct
solutions across multiple reasoning traces, which leads to a drop in Pass@32.

We further investigate the loop mechanism employed in MatryoshkaThinking. Using GPT-OSS-120B on the
AIME2025 dataset as a case study, we track the distribution of correctly and incorrectly solved problems across
successive loop iterations. As shown in Figure 8, green cells denote questions that the model correctly answered
in a certain loop, while red cells indicate incorrect responses. The model’s initial performance in loop 0 reflects
its direct response capability. It was particularly vulnerable to difficult questions — the accuracy on the 13th and
14th problems fell below 10%. However, after a single iteration loop, performance improved significantly as the
model began to take advantage of its previous solution history. This resulted in accuracy for these specific questions
exceeding 50%. In the second loop, the model solved almost all questions. The process illustrated in Figure 8 clearly
demonstrates how the model’s predictions evolve across loop iterations. By recursively exploiting the model’s
inherent strengths in reasoning, verification, and summarization, it progressively corrects earlier mistakes and
recovers accurate solutions, even when valid answers constitute only a minority. This dynamic behavior further
confirms the effectiveness of our approach. Building on this analysis, we attempt to model the relationship between
MatryoshkaThinking’s accuracy and three key factors in the Appendix A.

Loop 0 Loop 1

10
10

20 15:
15

20

25
25

10 20
Inference 32 Times Inference 32 Times Inference 32 Times

10 20 10 20

Figure 8: The evolution of the distribution of correct (green) and incorrect (red) answers from GPT-OSS-120B on
AIME2025 with our MatryoshkaThinking across progressive loops.

11


Ce) MatryoshkaThinking October 11, 2025

5 Conclusion

We present MatryoshkaThinking, a novel test-time scaling paradigm that operates solely on a model’s intrinsic
abilities without any fine-tuning or external feedback. The key advantage of our method lies in its efficiency and
exceptional generality as evidenced by extensive evaluations on diverse models (from non-reasoning-model to
reasoning-model) and tasks (from textual to multimodal understanding). Beyond presenting a practical framework,
we conducted a comprehensive ablation study, revealing that improvements in iterative looping and summarization
capabilities are pivotal for achieving effective test-time scaling. We hope that MatryoshkaThinking will serve as
a foundational step towards more efficient and scalable inference techniques, inspiring future exploration in the
community.

6 Future Work

While MatryoshkaThinking demonstrates broad generality by relying only on intrinsic model abilities, this design
also limits self-verification in complex scenarios. A promising future direction is to integrate the framework with
external tools such as web search, Python, or other APIs, potentially improving the reliability and scalability of
test-time inference. Another potential avenue for future research is to integrate MatryoshkaThinking’s parallel
decoding capability into the model’s intrinsic abilities, thereby enhancing both intelligence and reasoning efficiency.

References

Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora, Yu Bai,
Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita Brett, Eugene Brevdo, Greg
Brockman, Sébastien Bubeck, Che Chang, Kai Chen, Mark Chen, Enoch Cheung, Aidan Clark, Dan Cook, Marat
Dukhan, Casey Dvorak, Kevin Fives, Vlad Fomenko, Timur Garipov, Kristian Georgiev, Mia Glaese, Tarun
Gogineni, Adam P. Goucher, Lukas Gross, Katia Gil Guzman, John Hallman, Jackie Hehir, Johannes Heidecke,
Alec Helyar, Haitang Hu, Romain Huet, Jacob Huh, Saachi Jain, Zach Johnson, Chris Koch, Irina Kofman,
Dominik Kundel, Jason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guillaume Leclerc, James Park Lennon, Scott
Lessans, Mario Lezcano Casado, Yuanzhi Li, Zhuohan Li, Ji Lin, Jordan Liss, Lily Liu, Jiancheng Liu, Kevin
Lu, Chris Lu, Zoran Martinovic, Lindsay McCallum, Josh McGrath, Scott McKinney, Aidan McLaughlin, Song
Mei, Steve Mostovoy, Tong Mu, Gideon Myles, Alexander Neitz, Alex Nichol, Jakub Pachocki, Alex Paino,
Dana Palmie, Ashley Pantuliano, Giambattista Parascandolo, Jongsoo Park, Leher Pathak, Carolina Paz, Ludovic
Peran, Dmitry Pimenov, Michelle Pokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila, Filippo Raso, Hongyu Ren,
Kimmy Richardson, David Robinson, Bob Rotsted, Hadi Salman, Suvansh Sanjeev, Max Schwarzer, D. Sculley,
Harshit Sikchi, Kendal Simon, Karan Singhal, Yang Song, Dane Stuckey, Zhiqing Sun, Philippe Tillet, Sam
Toizer, Foivos Tsimpourlas, Nikhil Vyas, Eric Wallace, Xin Wang, Miles Wang, Olivia Watkins, Kevin Weil,
Amy Wendling, Kevin Whinnery, Cedric Whitney, Hannah Wong, Lin Yang, Yu Yang, Michihiro Yasunaga,
Kristen Ying, Wojciech Zaremba, Wenting Zhan, Cyril Zhang, Brian Zhang, Eddie Zhang, and Shengjia Zhao.
gpt-oss-120b & gpt-oss-20b model card. CoRR, abs/2508.10925, 2025. doi: 10.48550/ARXIV.2508.10925.
URL https://doi.org/10.48550/arXiv. 2508. 10925.

Aman Madaan Pranjal Aggarwal, Yiming Yang, and Mausam. Let’s sample step by step: Adaptive-consistency for
efficient reasoning and coding with llms. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the
2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December
6-10, 2023, pp. 12375-12396. Association for Computational Linguistics, 2023. doi: 10.18653/V 1/2023.EMNLP
-MAIN.761. URL https://doi.org/10.18653/v1/2023.emnlp-main. 761.

Baidu-ERNIE-Team. Ernie 4.5 technical report, 2025.

Vidhisha Balachandran, Jingya Chen, Lingjiao Chen, Shivam Garg, Neel Joshi, Yash Lara, John Langford,
Besmira Nushi, Vibhav Vineet, Yue Wu, and Safoora Yousefi. Inference-time scaling for complex tasks: Where
we stand and what lies ahead. CoRR, abs/2504.00294, 2025. doi: 10.48550/ARXIV.2504.00294. URL
https://doi.org/10.48550/arXiv.2504.00294.

Mislav Balunovi¢, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovié, and Martin Vechev. Matharena: Evaluating llms
on uncontaminated math competitions, February 2025. URL https://matharena.ai/.

Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda,
Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate
problems with large language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.),
Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative
Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial
Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 17682-17690. AAAI Press, 2024. doi:
10.1609/AAAI.V38116.29720. URL https://doi.org/10.1609/aaai .v38i16.29720.

12


Ce) MatryoshkaThinking October 11, 2025

Bradley C. A. Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia
Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. CoRR, abs/2407.21787,
2024. doi: 10.48550/ARXIV.2407.21787. URL https: //doi.org/10.48550/arXiv.2407 .21787.

Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan O. Arik. SETS: leveraging self-
verification and self-correction for improved test-time scaling. CoRR, abs/2501.19306, 2025. doi: 10.48550/ARX
IV.2501.19306. URL https: //doi.org/10.48550/arXiv.2501.19306.

Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Jon Stoica, Matei A. Zaharia, and James Y. Zou. Are
more LLM calls all you need? towards the scaling properties of compound AI systems. In Amir Globersons, Lester
Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in
Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024,
NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024a. URL http: //papers .nips.cc/pap
er_files/paper/2024/hash/51173cf34c5faac9796a47dc2fdd3a71-Abstract-Conference.html.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,
Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William
Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec
Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario
Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on
code. CoRR, abs/2107.03374, 2021. URL https: //arxiv.org/abs/2107 .03374.

Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. Voicebench: Benchmarking
llm-based voice assistants. CoRR, abs/2410.17196, 2024b. doi: 10.48550/ARXIV.2410.17196. URL
https://doi.org/10.48550/arXiv.2410.17196.

DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong
Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu
Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong
Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang,
Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang
Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige
Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei
Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning
Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe
Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng
Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning
capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948.
URL https://doi.org/10.48550/arXiv.2501.12948.

Michael Desmond, Zahra Ashktorab, Werner Geyer, Elizabeth M. Daly, Martin Santillan Cooper, Qian Pan,
Rahul Nair, Nico Wagner, and Tejaswini Pedapati. Evalassist: Llm-as-a-judge simplified. In Toby Walsh,
Julie Shah, and Zico Kolter (eds.), AAAI-25, Sponsored by the Association for the Advancement of Artificial
Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 29637-29639. AAAI Press, 2025. doi:
10.1609/AAAT.V39128.35351. URL https: //doi.org/10.1609/aaai.v39i28.35351.

Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. CoRR, abs/2508.15260,
2025. doi: 10.48550/ARXIV.2508.15260. URL https: //doi.org/10.48550/arXiv.2508. 15260.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. Measuring massive multitask language understanding. In 9th International Conference on Learn-
ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL
https: //openreview.net/forum?id=d7KBjmI3GmQ.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and
Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and
Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL https:
//datasets-benchmarks- proceedings. neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1
b0a17836al-Abstract-round2.html.

13


Ce) MatryoshkaThinking October 11, 2025

Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed E]-Kishky, Aiden Low, Alec Helyar, Aleksander
Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander
Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew
Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph,
Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao,
Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary
Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger,
Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David
Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman,
Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan
Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von
Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman,
Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren,
Hunter Lightman, Hyung Won Chung, Jan Kivlichan, Ian O’Connell, Ian Osband, Ignasi Clavera Gilaberte, and
Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. doi: 10.48550/ARXIV.2412.16720. URL
https://doi.org/10.48550/arXiv.2412.16720.

Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama,
Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language
models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,
April 24-28, 2025. OpenReview.net, 2025. URL https: //openreview.net/forum?id=chf JJYC3iL.

Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via
self-certainty. CoRR, abs/2502.18581, 2025. doi: 10.48550/ARXIV.2502.18581. URL https://doi.org/10
.48550/arXiv.2502.18581.

Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. ARGS: alignment as reward-guided search. In The
Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.
OpenReview.net, 2024. URL https: //openreview.net/forum?id=shgx0eqdw6.

Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. Escape
sky-high cost: Early-stopping self-consistency for multi-step reasoning. In The Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https: //openreview.net/forum?id=ndR8Ytrzhh.

Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming
Xiong. Reward-guided speculative decoding for efficient LLM reasoning. CoRR, abs/2501.19324, 2025. doi:
10.48550/ARXIV.2501.19324. URL https://doi.org/10.48550/arXiv.2501.19324.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John
Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https: //openreview.net/forum?id=v8LOpN6E0i.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,
Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual
contexts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=KUNZEQMWU7.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha
Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,
Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In
Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in
Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023,
NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper
_files/paper/2023/hash/91edff07232fb1b55a505a9e9f 6cOff3-Abstract-Conference.html.

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer,
Percy Liang, Emmanuel Candés, and Tatsunori Hashimoto. sl: Simple test-time scaling. arXiv preprint
arXiv:2501.19393, 2025.

Soumyasundar Pal, Didier Chételat, Yingxue Zhang, and Mark Coates. Hint marginalization for improved
reasoning in large language models. CoRR, abs/2412.13292, 2024. doi: 10.48550/ARXIV.2412.13292. URL
https://doi.org/10.48550/arXiv.2412.13292.

14


Ce) MatryoshkaThinking October 11, 2025

S. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami,
Sreyan Ghosh, and Dinesh Manocha. MMAU: A massive multi-task audio understanding and reasoning benchmark.
In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28,
2025. OpenReview.net, 2025. URL https: //openreview.net/forum?id=TeVAZXr3yv.

Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more
effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/ARXIV.2408.03314. URL
https://doi.org/10.48550/arXiv.2408.03314.

ByteDance Seed Team. Seed-oss open-source models. https: //github.com/ByteDance-Seed/seed-oss,
2025.

Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R
Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, et al. Recursive self-aggregation unlocks deep
thinking in large language models. arXiv preprint arXiv:2509.26626, 2025.

Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. Reasoning aware self-consistency: Leveraging reasoning paths for
efficient LLM sampling. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference
of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025,
pp. 3613-3635. Association for Computational Linguistics, 2025. doi: 10.18653/V 1/2025. NAACL-LONG. 184.
URL https://doi.org/10.18653/v1/2025 .naacl-long. 184.

Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Soft self-consistency improves language models
agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics, ACL 2024 - Short Papers, Bangkok, Thailand, August 11-16, 2024,
pp. 287-301. Association for Computational Linguistics, 2024. doi: 10.18653/V 1/2024.ACL-SHORT.28. URL
https: //doi.org/10.18653/v1/2024.acl-short. 28.

Weiqin Wang, Yile Wang, and Hui Huang. Ranked voting based self-consistency of large language models. In
Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the
Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 14410-14426.
Association for Computational Linguistics, 2025a. URL https: //aclanthology.org/2025.findings-acl

.T44/.

Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu,
and Kan Li. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. In
Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics:
NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 6904-6917. Association for
Computational Linguistics, 2025b. doi: 10.18653/V1/2025.FINDINGS-NAACL.383. URL https:
//doi.org/10.18653/v1/2025.findings-naacl . 383.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh Interna-
tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,
2023. URL https: //openreview.net/forum?id=1PL1NIMMrw.

Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. Sampling-
efficient test-time scaling: Self-estimating the best-of-n sampling in early decoding. CoRR, abs/2503.01422,
2025c. doi: 10.48550/ARXIV.2503.01422. URL https: //doi.org/10.48550/arXiv.2503.01422.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,
and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo,
S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information
Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http: //papers.nips.cc/paper_files/p
aper/2022/hash/9d5609613524ecf4f 15af0f7b31abca4-Abstract-Conference.html.

Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov,
and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models.
Trans. Mach. Learn. Res., 2024, 2024. URL https: //openreview.net/forum?id=eskQMcIbMS.

Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Qizhe Xie.
Self-evaluation guided beam search for reasoning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate
Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December
10 - 16, 2023, 2023. URL http: //papers.nips.cc/paper_files/paper/2023/hash/81fde95c4dc7918
8a69ce5b24d63010b-Abstract-Conference.html.

15


Ce) MatryoshkaThinking October 11, 2025

Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang,
Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. CoRR, abs/2503.20215,
2025. doi: 10.48550/ARXIV.2503.20215. URL https: //doi.org/10.48550/arXiv.2503.20215.

Mingfeng Xue, Dayiheng Liu, Wenqiang Lei, Xingzhang Ren, Baosong Yang, Jun Xie, Yidan Zhang, Dezhong Peng,
and Jiancheng Lv. Dynamic voting for efficient reasoning in large language models. In Houda Bouamor, Juan
Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore,
December 6-10, 2023, pp. 3085-3104. Association for Computational Linguistics, 2023. doi: 10.18653/V 1/2023
-FINDINGS-EMNLP.203. URL https://doi. org/10.18653/v1/2023.findings-emnl1p. 203.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen
Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan
Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang
Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei
Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang,
Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang
Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and
Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL
https://doi.org/10.48550/arXiv.2505.09388.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree
of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir
Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing
Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023, 2023. URL http: //papers.nips.cc/paper_files/paper/2023/hash
/271db9922b8d1f4dd7aaef 84ed5ac703-Abstract-Conference.html.

Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. CoRR, abs/2407.06023, 2024.
doi: 10.48550/ARXIV.2407.06023. URL https: //doi.org/10.48550/arXiv.2407 .06023.

Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,
Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu
Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A massive multi-discipline
multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 9556-9567. IEEE, 2024.
doi: 10.1109/CVPR52733.2024.00913. URL https: //doi.org/10.1109/CVPR52733.2024.00913.

16


Ce) MatryoshkaThinking October 11, 2025

A Modeling the Interplay of Key Factors in MatryoshkaThinking

To better understand the effect of MatryoshkaThinking, we try to model the relation between the resulting accuracy of
the method and three key factors: the model’s initial accuracy, verification capability, and summarization capability.

A.1 Definitions

Definition 1 (Probability of Correct Solution). The probability of generating a correct solution is
pass@1.

Definition 2 (Verification Process). In the verification process, given that a candidate solution may be either correct
or incorrect, the verifier may classify it as correct with certain probabilities:

True Positive Rate (TPR) = Pr|verified as correct | solution is correct],

False Positive Rate (FPR) = Pr [verified as correct | solution is incorrect].

Definition 3 (Summarization Process). In the summarization process, assume that given a set of candidate solutions,
the probability of summarizing a correct one can be expressed as a function

S(r),

where r denotes the proportion of correct answers among the candidates.

A.2 Step1: Accuracy After Verification

Suppose we randomly generate a solution, the probability that it is verified as the correct solution is:

Pr\(verified as correct] = pass@1-TPR+(1—-pass@1)-FPR (7)

Given that a solution is verified as correct, the probability that it is actually correct is:

q = Pr{(solution is correct | verified as correct]
_ pass@1-TPR (8)
~ pass@1-TPR+(1—pass@1)-FPR

If there are n,, candidate solutions verified as correct, the probability that the candidate set contains at least one
correct solution is:
Phas = 1- (1 —q)" (9)

Correspondingly, the probability that all candidates in the set are incorrect is:

Prone = (1 —q)"” (10)
A.3 Step2: pass@1 After Summarization
Let us define:

* S(r = 0): the probability that summarization still produces a correct solution when the candidate set
contains no correct solutions.

* S(r > 0): the probability that summarization produce a correct solution when the candidate set contains at
least one correct solution.

Therefore, with 1,, candidate solutions verified as correct, the pass @ | sna) after summarization can be expressed as
the sum of two parts:

pass@\¢nat = Phas + S(r > 0) + Paone : S(r = 0)
= (1-(1-q)™)-S(r > 0)+(1-@)” - S(my,r = 0)

_(,_(;____p:TPR\"),

-(! (! TERS py FPR } stro
p-TPR my _

+ ((\- ape ree }-s=0

where p = pass@ Linit.

17


Ce) MatryoshkaThinking October 11, 2025

A.4 Step3: Binomial Distribution of Verified Candidate Set Size

Definition of the Binomial Distribution. Let X denote the number of successes in n independent Bernoulli trials,
each with success probability p. Then X follows a binomial distribution with probability mass function:

P(X =k) = [iJon —py""*, k=0,1,...,0

Therefore, the number of candidate solutions after verification, denoted N,,, follows a binomial distribution. Suppose
that initially one question generates N solutions. Then the distribution of N,, is:

P(N, = ny) =

N
| | Privette as correct]””
ny

(1 — Pr[verified as correct])%~”" ,
ny =0,1,...,N
Hence, the final expression for pass @ 1 final is:
pass@l¢nat =
N

>) PM = no|

ny=0

[1-(1- pass@linit- TPR )")

pass@ lini: TPR + (1 — pass@ linit) - FPR
-S(r >0)
n f _ pass@ lini TPR )
pass @ linit -TPR+ el — pass@ linit) -FPR
-S(r =0)

|

18
