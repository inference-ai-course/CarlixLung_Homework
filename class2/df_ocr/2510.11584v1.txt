2510.11584v1 [cs.CL] 13 Oct 2025

arXiv

LLMAtKGE: Large Language Models as Explainable Attackers
against Knowledge Graph Embeddings

Ting Li
liting226@mail2.sysu.edu.cn
Sun Yat-sen University
Guangzhou, China

Liang Yao*
yaoliang3@mail.sysu.edu.cn

Sun Yat-sen University
Shenzhen, China

Abstract

Adversarial attacks on knowledge graph embeddings (KGE) aim
to disrupt the model’s ability of link prediction by removing or
inserting triples. A recent black-box method has attempted to in-
corporate textual and structural information to enhance attack
performance. However, it is unable to generate human-readable ex-
planations, and exhibits poor generalizability. In the past few years,
large language models (LLMs) have demonstrated powerful capa-
bilities in text comprehension, generation, and reasoning. In this
paper, we propose LLMAtKGE, a novel LLM-based framework that
selects attack targets and generates human-readable explanations.
To provide the LLM with sufficient factual context under limited
input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window
limitation and hesitation issues, we introduce semantics-based and
centrality-based filters, which compress the candidate set while
preserving high recall of attack-relevant information. Furthermore,
to efficiently integrate both semantic and structural information
into the filter, we precompute high-order adjacency and fine-tune
the LLM with a triple classification task to enhance filtering perfor-
mance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box
baselines and provides explanations via reasoning, and showing
competitive performance compared with white-box methods. Com-
prehensive ablation and case studies further validate its capability
to generate explanations.

CCS Concepts

« Theory of computation — Semantics and reasoning; « Se-
curity and privacy — Domain-specific security and privacy

“Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

Conference acronym ‘XX, Woodstock, NY

© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/2018/06

https://doi.org/XXXXXXX.XXXXXXX

Yang Yang
yangy2233@mail2.sysu.edu.cn
Sun Yat-sen University
Shenzhen, China

Guoging Chao
guoqingchao@hit.edu.cn
Harbin Institute of Technology
Weihai, China

Yipeng Yu
yypzju@163.com
Alibaba Inc.
Hangzhou, China

Ruifeng Xu
xuruifeng@hit.edu.cn

Harbin Institute of Technology
Shenzhen, China

architectures; - Computing methodologies — Knowledge rep-
resentation and reasoning.

Keywords

Adversarial Attack, Large Language Model, Knowledge Graph Em-
bedding

ACM Reference Format:

Ting Li, Yang Yang, Yipeng Yu, Liang Yao, Guoging Chao, and Ruifeng
Xu. 2018. LLMAtKGE: Large Language Models as Explainable Attackers
against Knowledge Graph Embeddings. In Proceedings of Make sure to en-
ter the correct conference title from your rights confirmation email (Confer-
ence acronym ’XX). ACM, New York, NY, USA, 13 pages. https://doi.org/
XXXXXXX.XXXXXXX

1 Introduction

Knowledge graph (KG) is widely used to model structured knowl-
edge and relationships. Knowledge graph embedding (KGE) is a
representation learning method that maps entities and relations in
a knowledge graph into a low-dimensional vector space. In recent
years, KGE has been successfully applied to various downstream
tasks, such as knowledge graph completion (KGC) [4, 29], knowl-
edge graph question answering (KGQA) [16, 27], and semantic
search [9, 24]. Existing studies [3, 25] have demonstrated that KGEs
are vulnerable to attacks. Some black-box methods [2, 38] have
demonstrated that the prediction of a target triple can be disrupted
without accessing KGE’s gradients, simply by adding or removing
a single triple corresponding to each test triple in the training set.

As shown in Figure 1, we roughly group these black-box attack
methods into two categories: (1) influence-based approaches [2, 38]
identify the most influential triples as attack targets by calculat-
ing the similarity among triple embeddings; (2) rule-based ap-
proaches [1, 43] generate attack targets by capturing patterns and
learning logic rules from the KG’s structure. Although some of these
studies claim that their approaches are explainable attacks, they fail
to provide human-readable explanations that clarify why the attacks
succeed and how the robustness of KGE models can be improved.
Fortunately, large language models (LLMs) have demonstrated im-
pressive performance in a range of KG tasks [6, 39], thanks to their
powerful generation ability. By leveraging this strength, LLMs have
great potential to produce human-readable explanations, making
them more practical and effective attackers. Thus, we pay attention
to RQ1: how to use LLMs as explainable attackers against


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

candidates

fi
\ —_ meron aaa —— >| (Zoe Saldana, bornin, UsA)
(a)
ry AT, >7
amie rAr3 Tr
ttacks (Zoe Saldana, bornin, UsA)
(b) Casel: overflow IN
Exceed the LLM's context window
LLM ay
Case2: hesitation) IX
kthinking>
(c) Answer is 2 or 3 ,2 or 3...
filterd
sgbaidctes
> > <thinking> ~ </thinking>
LLMAtTKGE Answer is 2, because it ~ ~
(d)

Figure 1: Comparison of current methods and our proposed
LLMAItKGE. (a) Influence-based attacks typically rely on 1-
hop triples as input and lack the ability to generate explana-
tions. (b) Rule-based attacks exploit rules extracted from the
KG to perform attacks, and derive explanations from selected
rules. (c) An LLM-based attempt, even though providing only
1-hop triples may still lead to overflow, and hesitation in
the chain of thought. (d) Our LLMAtKGE guides the LLM’s
reasoning to derive the answer while generating the human-
readable explanations.

KGEs? In practice, KGE attacks usually rely on heuristic meth-
ods [3, 40], large-scale enumeration to discover effective perturba-
tions, and the lack of labeled datasets makes it difficult to perform
supervised fine-tuning of LLMs. To address this, we propose a
parameter-preservation framework that enhances LLM with KG
to generate attack targets, as shown in Figure 1. Specifically, the
target triple and its description are used as KG factual evidence,
the neighboring triples of it serve as the candidates. They are con-
catenated into a context and fed into the instruction LLM, which
reasons to select a perturbation target and generate human-readable
explanations.

Ideally, the candidate set would include all triples of KG, but
the context window of LLM is limited. Moreover, compared with
question answering task based on text retrieval [8, 13, 21, 33], KGE
attacks require cross-relation structural reasoning. An oversized
candidate set enlarges the search space and induces hesitation
in the chain of thought. Specifically, the LLM hesitates among
multiple perturbation targets, which prolongs reasoning and may
exceed the token budget. Therefore, we introduce semantics-based
and centrality-based filters, which narrow the candidate set from
the perspectives of semantics and graph structure while retaining
attack-relevant information as much as possible.

However, since semantics-based and centrality-based filters do
not jointly capture both semantics and structural information, their
performance remains limited. Moreover, complex relations and hi-
erarchical structures are difficult to convey clearly through context,

Ting Li et al.

restricting the LLM’s view. Thus, we focus on RQ2: how to simply
and effectively integrate multi-hop information into LLM? To
address these challenges and enhance filters, we explore that seam-
lessly integrate the KG with the LLM through parameter updates.
Specifically, we align the LLM with the KG via a triple-level task,
thereby broadening the view on KG and improving its ability of fil-
tering. Existing methods [41] leverage lightweight KGE to fine-tune
LLMs for approximating structural features, rather than explicitly
incorporating multi-hop paths, which causes the LLM to overem-
phasize semantic similarity and ultimately limits performance. For
example, the embedding of "20th United States Congress" is closest
to "21st United States Congress" instead of the members of the
20th Congress. The key challenge is to balance the contributions of
semantic and structural signals, efficiently embed multi-hop path in-
formation, and meanwhile preserve the LLM’s existing knowledge.
Inspired by classical GNNs [15, 18, 32], we leverage neighborhood
aggregation, which exhibits strong structure-aware capabilities. It
encodes multi-hop information via the interaction of the adjacency
and feature matrices. A knowledge graph is inherently a graph
structure. We propose an efficient adapter that precomputes high-
order adjacency (HoA) to transform multi-hop KG information
into the textual embedding space, serving as prefix tokens for LLM
tuning.

We conduct deletion and addition attacks on four KGE models,
including DistMult [37], ComplEx [31], ConvE [7], and TransE [4],
using two widely used knowledge graph datasets, WN18RR and
FB15k-237. Our proposed attack method outperforms the strongest
black-box baseline in most cases, and even surpasses white-box
attacks to achieve state-of-the-art performance. We further per-
form extensive ablation studies to validate the effectiveness of each
component of our framework. Finally, a case study demonstrates
the ability of our model to generate human-readable explanations.
With few open releases in recent work, our code’ is publicly
available.

Our contributions are summarized in three aspects.

e We design a novel parameter-preservation framework called
LLMAttackKGE, which enables LLMs to perform KGE at-
tacks in the absence of labeled data. It employs prompt en-
gineering to generate human-readable explanations, while
introducing filters to address overflow and the hesitation
issue.

e We introduce a paradigm that precomputes high-order adja-
cency, then transforms multi-hop information into the LLM
embedding space. It enables the LLM to capture deeper struc-
tural knowledge and improves the filtering ability.

e We conduct extensive experiments on two widely used knowl-
edge graph datasets and demonstrate the effectiveness of
our method, and design ablation experiments to rigorously
evaluate the contribution of each module.

2 Related Work
2.1 Bridging KGs and LLMs

With the rapid advancement of LLMs, research on their integra-
tion with KG can be broadly divided into two categories by task

‘https://github.com/liting1024/LLMAtKGE


LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings

type and integration strategy. KG-enhanced LLM primarily focus
on semantic retrieval, including KGQA and document QA, with
recent studies exploring improvements through in-context learn-
ing (ICL) and retrieval-augmented generation (RAG). In contrast,
LLM-enhanced KG emphasize reasoning, and recent works aim to
integrate structural information to strengthen inference capabili-
ties. As our task requires reasoning on KGs but suffers from limited
labeled datasets, directly fine-tuning an LLM to combine KG and
textual information is infeasible. Therefore, we combine the bene-
fits of KG-enhanced LLMs and LLM-enhanced KGs, employing ICL
for knowledge alignment and then introducing triple classification
task to embed structural information into the LLM.

2.1.1 KG-enhanced LLM. This category leverage knowledge graphs
to mitigate hallucinations, provide reasoning paths, and enhance
both interpretability and generation quality. For example, GcR [22]
introduces a KG Trie structure as a constraint, enabling LLMs to
follow multiple reasoning paths and thus achieve more faithful
multi-path reasoning. EtD [19] employs a GNN to adaptively ex-
pand and prune the KG according to the question semantics, identi-
fying candidate entities and evidence chains. RoG [21] enhances
interpretability by avoiding direct SPARQL generation, the LLM
first produces relation paths as reasoning plans, which are then val-
idated through KG path retrieval to generate answers with explicit
evidence. ToG [28] also performs entity and relation search, prun-
ing, and beam-search reasoning over the KG to construct traceable
reasoning paths. Building on this idea, ToG2 [23] further integrates
textual RAG with KG retrieval to enable more faithful reasoning.
Knowledge graphs have also been leveraged to enhance document
QA. GraphRAG [12, 26] extends RAG by exploiting KG semantics
and relational paths for document retrieval and reasoning. Edge et
al. [8] builds an KG with community-level summaries to support
global corpus understanding. HippoRAG [13, 14], inspired by hip-
pocampal memory indexing theory, combines KG and personalized
PageRank with context augmentation and memory recognition to
improve global comprehension and multi-hop retrieval.

2.1.2 LLM-enhanced KG. LLMs have also been employed for rea-
soning over KG to support construction and completion, including
triple classification, entity prediction, and relation prediction. KG-
LLM [39] serializes entities, relations, and triples into natural lan-
guage QA forms, and employs instruction tuning to enable LLMs to
perform KGC tasks. MKGL [10] introduces specialized KG language
tokens to represent triples, and integrates word embeddings with a
principal neighbourhood aggregator for reasoning, thereby achiev-
ing ranking prediction over the entire entity set. K-ON [11] con-
verts KG into token-level inputs and adds a multi-head prediction
layer on top of the LLM, enabling entity-level contrastive learning
and efficient knowledge alignment with only modifications to the
output layer. KICGPT [34] leverages the vast commonsense knowl-
edge encoded in pretrained LLMs as an external source, combining
structure-aware KG models with LLMs and applying in-context
learning to guide re-ranking over Top-k results. MPIKGC [36] fur-
ther proposes a multi-perspective augmentation strategy, where
LLMs enrich textual descriptions of KGs from entity, relation, and
structural perspectives, and the augmented knowledge is then fed
into description-driven KGC models.

Conference acronym XX, June 03-05, 2018, Woodstock, NY

2.2 Adversarial Attacks on KGEs

Adversarial attacks are typically conducted by poisoning the KGE
parameters or their datasets. Early studies mainly focused on white-
box or gray-box attack settings, which require access to model
parameters. CRIAGE [25] is limited to attacking multiplicative mod-
els, it leverages the first-order Taylor to approximate embedding
variations and trains an inverter network to narrow the search
space. Furthermore, Bhardwaj et al. [2] introduced gradient simi-
larity and influence Function, achieving attacks on multiplicative
and additive models by utilizing model gradients. In both of their
works, Bhardwaj et al. [3] use embedding similarity to guide pa-
rameter search in attacks. Recently, Kapoor et al. [17] introduced
graph, label, and parameter perturbation to perform untargeted
attacks through entity or relation replacement, label perturbation,
and direct parameter modification.

Another category is black-box attacks. PLM-Attack [38] encodes
triple semantics with a pretrained language model and integrates
graph structure via random walks to identify high-impact triples
for attacks. AnyBURL [1] learns rules and applies abductive reason-
ing to generate explanations. Zhao et al. [43] achieve untargeted
attacks by extracting rules to disrupt global KG semantics. Zhou et
al. [44] propose server-initiate and client-initiate poisoning attack
for federated KGE through dynamic shadow model optimization.

In summary, white-box attack methods generally exploit gradi-
ent information from the target model as the attack basis, some
black-box methods derive rule-based rationales. These methods
still struggle to provide human-readable explanations.

3 Preliminary

3.1 Problem Definition

3.1.1 Knowledge Graph. A knowledge graph can be formally de-
fined as (6,8, 7 ), where & and R denote the entity set and relation
set, respectively. Each element in these sets is associated with a
textual description. The triple set 7 consists of triples t = (s, r, 0),
where each triple is composed of a subject, a relation, and an object.
Essentially, the knowledge graph can be represented as a directed
graph Gxc = (6, A), where & denotes the set of nodes correspond-
ing to entities in the KG, and A is the adjacency matrix.

3.1.2. Triple Graph. The triple graph is defined as Grg = (V, L£),
where ‘V denotes the set of triple nodes corresponding to the triple
set J in the KG. An edge (t;, t;) € L£ exists if the two triples share
a common entity e € &. In addition, from the perspective of graph
structures, the h-hop neighbors of Gxg and Grg can be defined as
Nv).

3.2 Attack on KGEs

AKGE model M is trained on Yi;ain to learn low-dimensional em-
beddings of entities and relations, and is evaluated on Tirain via
link prediction. The attack objective is to maximally degrade M’s
link prediction performance on targeted triples, as measured by
common ranking metrics, including Mean Reciprocal Rank (MRR)
and Hits@k.

We consider the following four classical models as the attacked
KGE. TransE [4] is a distance-based additive model that treats rela-
tions as translation vectors, enforcing s +r ~ o. DistMult [37]


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

(a) Overview of LLMATKGE

Graph Building

Knowledge Graph —> Triple Graph

(b) Knowledge Graph

_ — Target Triple _

Initial Candidates

Candidate Filters

Semantics | Centrality | HoA Tuning LLM

Filtered Candidates (d) Semantics-based Filter

LC ie aes

Gi, aes

( ( ia
| | ‘Sorted List | |
Role&Multi- : #1... Entity target
leneiee Task | | Gansta | #2... | ldegcription | ticle
Xu Ne Ge

Reasoning foo}

LLMEmbed 2 |

Ting Li et al.

(c) Triple Graph

geology, hypernym, earth
science

geology, derivationally related form,
geologist

| geologist, hypernym, |
scientist

Zo

| geologist, derivationally related form, |
geology

hypernym

(e) Centrality-based Filter
— — Zhop- ~ _

-

— hop ~
- — Z
{Sorted Lisi)
#1

#2. |

oa . d Ge pr
\< @B Bc
— am cc

Adapter ()
True / False |

vl

Is this true:
| (<subject>, <relation>, <object>)?

(h) addition

(g) deletion

Figure 2: (a) The overview of LLMAtKGE. First, (b) the knowledge graph and (c) the triple graph are constructed to determine
the initial candidates. The (d) semantics-based and (e) centrality-based filters are applied to shrink the candidate set. Then,
following advanced prompt engineering techniques, the instruction J, example E, candidates C, and reference R are concatenated
as the input to the instruction LLM. The LLM reasons to generate answers and human-readable explanations, after which
(g) deletion and (h) addition attacks are performed by removing triples or inserting new triples through entity replacement.
(f) HoA efficiently tunes the LLM on triple classification, integrating multi-hop paths into the LLM to enhance the filtering

performance.

is a diagonal bilinear model using element-wise multiplication
s'diag(r)o . ComplEx [31] is a bilinear model in the complex
space, where conjugation introduces phase to model asymmetric
relations. ConvE [7] is a convolutional model that reshapes s and r
into 2D, applies convolution to capture nonlinear interactions, and
matches with o.

3.3. KG-driven LLMs

3.3.1 Parameter Preservation. Given a question Q. The first stage
retrieves relevant knowledge from the KG through extractor such
as get_entity/relation/path. Then the retrieved factual knowledge
F is exploited to guide the LLM in generating the answer A.

F =Extractor(Q | KG), A=LLM(Q@F).

3.3.2. Parameter Updating. The framework exploits annotated knowl-
edge graph data to construct alignment tasks for supervised fine-
tuning, such as low-rank adaptation (LoRA), prefix tuning. The
structured knowledge can be seamlessly incorporated into the LLM.

It is formulated as FT (LLM(t; 0), $); where @ is the fixed parame-
ters, while ¢ represents the trainable parameters.

4 Methodology

We follow the black-box attack setting [38], aiming to attack M with
a minimal perturbation budget by deleting or adding a single triple.
Moreover, to reduce randomness, attack targets are limited to test
triples tg, achieving Hits@1 under all four KGEs. For the deletion
attack, given each test triple tig: = (Stet, igt, Oret), We identify the
most influential triple tig = (sig, rig, oif) in the training set and remove
it, thereby weakening the structural and semantic support for fig.
For the addition attack, we aim to generate a single triple as the
poison that maximally perturbs tg, misleading M. Specifically,
we select an noise entity noise from the KG and replace either sig
or ojf, constructing a new triple tyoison = (Sif; ifs Enoise) OF tpoison =
(€noise, Vif, Oi¢), Which is then injected into the training set.


LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings

4.1 Instruction Tuning LLM for KGE Attack

4.1.1 Prompt Engineering. For a given target triple tig, we select
either a candidate triple tig or an entity epoise from the candidate
set to conduct the aforementioned attack. This task can be formu-
lated as a reasoning-reflection problem. We input the prompt as
contextual guidance to steer the LLM in rationally analyzing each
candidate and providing sufficient explanation for executing the
attack. The prompt can be decomposed into four components, as
shown in the following formulation. Specific examples are provided
in section A.

P=I®@EO@COR,

where © denotes text concatenation.

Tis the attack instruction, which defines the role and specifies
the task. We design I as a multiple-choice question, explicitly re-
quiring the LLM to select an option and provide its reasoning. This
formulation effectively projects open-ended generation onto a fi-
nite decision space, thereby constraining reasoning divergence, and
alleviating hallucination. C is the candidate set C formatted as a list
of options. Drawing on retrieval-augmented methods, R provides
factual knowledge to the LLM by serving as entity descriptions,
thereby mitigating hallucinations. E denotes an example, which is
used to explicitly map inputs to outputs. In a label-limited and data-
limited scenario, by leveraging the few-shot learning capability of
LLMs [5], the model can utilize these examples to activate relevant
knowledge and achieve better task alignment.

4.1.2 Candidate Set Filtering. Ideally, the candidate set C should
contain all triples or entities in the KG. However, it is infeasible
to provide them all as context to the LLM. Even when following
existing methods [38], which select only the one hop neighborhood
of tig, as the candidate set, the search space remains prohibitively
large on dense datasets such as FB15k-237. Therefore, it is necessary
to design a filter that further reduces the candidate set to a size 6
acceptable for the LLM’s context window, while ensuring that tj
Or noise is still included in the candidate set. Two specialized filters
are devised for deletion and addition attacks, respectively, and are
implemented by applying them on Gxg and Grg.

Semantics-based type. In the KG, semantically closer triples are
more likely to serve as supportive evidence, we employ semantic
similarity to filter candidate triples. The 1-hop neighbors of tg
on the Grg are closely connected to it and thus exert a stronger
influence.

C = TopK {(LLMembed( tig), LLMrmbea())

ce Ni? (tug)} ,

where LLMgmbed is used to map triples into high-dimensional vec-
tors of fixed length, ® denotes the cosine similarity.

Centrality-based type. For addition attacks that target graph
structure, we select noisy entities from the target’s related enti-
ties to maximally mislead the model M. In detail, we construct the
candidate set by extracting the h-hop subgraph of tig, from the KG,
and choose h = 3 to balance perturbation relevance and misleading
noise, based on the KG’s diameter analysis. We then rank candidate
entities using three heuristic centrality measures from network sci-
ence and select the top candidates for injection. (1) Pagerank (PR)

Conference acronym XX, June 03-05, 2018, Woodstock, NY

is a damped random-walk—based measure, employed to simulate
multi-hop reasoning. (2) Betweenness centrality (BC) is a shortest-
path-based measure, identifying bridge nodes that frequently lie
on paths between other nodes. (3) Closeness centrality (CC) is a
distance-based measure, defined as the reciprocal of the average
shortest-path length from a node to others.

pe {PRBC,CC}

TopK {Ce ce NS (s,0) \ No(s, 0) .

4.2 High-order Adjacency Tuning LLM for
Filtering

Despite applying semantics-based and centrality-based filters to
reduce the candidate set, the gap between semantic and structural
information remains. It prevents optimal filtering effectiveness. In-
spired by the text reranking model [42], we introduce semantic in-
formation of KG by designing a triple classification task. Specifically,
following the KG-LLM [39], each triple is transformed into a natural
language statement of the form “Is this true: s @ r @ o 2”, with the
label annotated as “True” or “False”. Then we fine-tune LLMgmbea
using LoRA to enable the embedding model to embed knowledge
graph triples into a high-dimensional semantic space. For more
efficient integration of structural information into LLMgmbea, we
propose an adapter that leverages high-order adjacency matrices
to explicitly encode structural signals.

SGC [35] simplifies multi-layer graph convolutional networks
into the pre-computed feature propagation and a linear classifier,
preserving the core operation while significantly reducing model
complexity without compromising performance. Building on this
idea, we first normalize A, and then efficiently compute its h-th
power through preprocessing, thereby enabling high-order propa-
gation in the graph. Furthermore, we propose a HoA-based adapter
that employs attention coefficients to adaptively fuse multi-order
structural features, and subsequently injects the refined representa-
tion into the LLM.

A=pD-2apD2, H® — Ane,

where A includes self-loops and D is the degree matrix. H is
initialized with the entity representations Z(-) from TransE. This
process contains no trainable parameters.

Multiple MLPs are employed to extract feature patterns from
each H“*). A set of learnable coefficients is introduced to model the
attention distribution over multi-hop representations. They are con-
catenated to jointly aggregate local and long-range dependencies,
thereby enlarging the receptive field.

fitoa = MLP(([e, or + MLP,(H))

exp(Ox)
Dh, exp(6)
where 6 denotes the learnable parameters.
The adapter fuses multi-hop entity and relation representations,

enabling parameter-efficient fine-tuning to inject structured knowl-
edge into the LLM.

K= MLP adapter (fitoa (S)||Z(7) || fitoa (0)) >


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

Ting Li et al.

Table 1: Overall results of deletion and addition adversarial attacks on WN18RR and FB15k-237. The baselines are grouped
into black-box and white-box attacks. Red marks the best among white-box and black-box attacks, bold black the best within
black-box, underline denotes performance competitive to PLM-Attack. MRR | and Hits@1 | indicate better attack performance.

WN18RR FB15k-237
DistMult ComplEx ConvE TransE DistMult ComplEx ConvE TransE

MRR H@1 MRR H@1 MRR H@1 MRR H@1 MRR H@1 MRR H@1 MRR H@1 MRR He!

Direct 0.86 0.75 0.85 0.76 0.71 0.61 0.63 0.53 0.63 0.50 0.60 0.43 0.61 0.48 0.63 0.47

White-box IF 0.30 0.20 0.29 0.21 0.86 0.77 0.59 047 0.62 0.48 0.61 0.45 0.53 0.35 0.63 0.46

§ Random 0.88 0.84 0.87 0.83 0.86 0.83 0.73 0.59 0.66 0.54 0.68 0.53 0.62 0.47 0.76 0.65
=
—

3 AnyBURL 0.29 0.20 0.31 0.22 0.22 0.17 0.54 044 0.63 050 0.64 0.50 0.59 0.45 0.66 0.50

S Black-box PLM-Attack 0.29 0.19 0.31 0.21 0.20 0.14 0.67 0.50 0.57 0.43 0.60 0.43 0.56 0.38 0.66 0.51

Ours 0.21 0.14 0.21 0.14 0.14 0.11 0.66 050 065 0.48 0.63 0.48 0.53 0.36 0.64 0.46

Direct 0.97 0.95 0.94 0.92 0.99 0.97 0.83 0.72 0.64 0.53 0.69 0.55 0.65 0.52 0.73 0.62

White-box IF 0.88 0.77 0.88 0.78 0.97 0.95 0.86 0.75 0.67 0.55 0.70 0.59 0.63 0.50 0.75 0.64

& Random 0.97 0.96 0.97 0.95 0.99 0.98 0.82 0.69 0.66 0.54 0.69 0.57 0.66 0.53 0.74 0.63
-_
=

a AnyBURL 0.86 0.74 0.82 0.69 0.88 0.77 0.81 0.66 0.67 0.55 0.67 0.52 0.65 0.52 0.74 0.61

“ Black-box PLM-Attack 0.94 0.88 0.93 0.88 0.90 0.82 0.84 0.72 0.65 0.51 0.66 0.53 0.63 0.45 0.74 0.62

Ours 0.92 0.84 0.91 0.81 0.87 0.75 0.81 0.68 0.67 0.51 0.68 0.54 0.61 0.47 0.65 0.52

where K is concatenated as a prefix to the input token sequence.

5 Experiments

5.1 Experimental Setup

We prioritize comparisons with methods that support both deletion
and addition attacks and are applicable across all KGE models.
Random attack conducts deletion by randomly removing triples,
and addition by randomly replacing entities. Our primary baseline is
PLM-Attack [38], which adopts the same black-box attack setting
and is considered the strongest competitor as it is also based ona
language model. We also compare against the rule-based method
AnyBURL [1]. Zhao et al. [43] is not included in comparison, as
it is not publicly available and uses an untargeted attack setting
that is incompatible with our experimental setup. For white-box
attacks, we evaluate a simple direct attack [40], modifying facts
directly related to the target triple to change entity embeddings.
And the widely recognized influence functions (IF) attack [2], which
estimates the effect by approximating parameter changes to perform
the attack.

The dataset statistics, detailed settings and hyperparameter sen-
sitivity analysis are provided in section B.

5.2 Main Results

Table 1 reports the results of deletion and addition adversarial at-
tacks. Our proposed method outperforms the strongest competing
PLM in most cases, even achieves the best results among black-box
attacks, and in many cases surpasses white-box attacks to attain the
overall best performance. Although it does not reach the state of the

art in all settings, our method is more practical as it not only pro-
vides effective attack selection but also generates human-readable
explanations. Although the performance of addition attacks has
improved, this is largely attributable to our proposed framework,
which can identify deceptive entities. However, the overall effective-
ness still lags behind that of deletion attacks. This gap is consistent
with observations reported in other baseline methods. Compared
with PLM-Attack, the improved performance on the additive model
TransE can be attributed to the combination of parameter preser-
vation and updating, which overcomes the limitations of directly
employing the Transformer architecture as an encoder.

We further compare the attack performance across different
datasets and observe that the results on WN18RR are significantly
better than those on FB15k-237. We attribute this to the fact that
FB15k-237 exhibits a much richer neighborhood structure, which
provides stronger structural evidence for KGE learning. As a result,
under the same training settings, KGE models trained on FB15k-
237 generally show greater robustness, and LLMs exhibit limited
generalizability to open-domain knowledge graphs. In addition,
entities and descriptions in WN18RR contain abundant semantic
information, much of which has already been absorbed during LLM
pre-training as lexical knowledge.

5.3. Ablation Analysis

5.3.1 Impact of Prompt Settings. The prompt template comprises
four components, as outlined earlier, with instruction and candidate
being essential for the execution of LLM-based attacks. Therefore,
we only conduct an ablation analysis to examine the contributions
of examples and references. As shown in Table 2, examples play
a critical role in performance improvement. In the absence of R,


LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings

Table 2: Ablation results on prompt components examples E
and references R for deletion attack on WN18RR.

E R__ DistMult ComplEx ConvE TransE
MRR H@1 MRR H@1 MRR H@1 MRR H@!1
xX X 0.24 0.17 0.25 0.19 0.17 0.13 0.69 0.53
xX VY 0.32 0.23 0.34 0.28 0.24 0.20 0.70 0.53
Y X 0.21 0.144 0.21 0.14 0.14 0.11 0.73 0.61
¥ vo 0.22 0.14 0.23 0.18 0.16 0.12 0.66 0.50

introducing E leads to an average increase of 17.58% in MRR and
Hits@1, with the exception of TransE. The contribution of R on
WNIB8RR is relatively limited, which may be attributed to the fact
that the entity descriptions of WN18RR have likely been incorpo-
rated into the LLM during pre-training, and R has already been
utilized when filtering the candidate set.

ComplEx ComplEx

ConvE DistMult ConvE

TransE TransE
(a) MRR of WN1I8RR (b) Hits@1 of WNI8RR
ComplEx ComplEx

ConvE

TransE TransE

(c) MRR of FB15k-237 (d) Hits@1 of FB15k-237

Ge LLMeEmbed GB +HoA B+ HoA+R

Figure 3: Ablation results of HoA. Values closer to the center
denote better attack performance.

5.3.2. Comparion of Filters. To investigate the filtering effect on
rif and enoise, we select only the Top1 candidate as the answer. We
adopt Llama3-8B? as the semantics-based filter. Although Qwen-
Embedding-8B? serves as a strong competitor to Llama3-8B, its
performance in the HoA fine-tuning stage is inferior, thus it is not
included in the comparison. As shown in Figure 3, we first em-
bed only triples to examine the effect of HoA tuning filter. HoA is
effective in most cases, with particularly strong performance on
WNI8RR. Furthermore, we also embed the R of prompt to enhance

“https://huggingface.co/meta-llama/Llama-3.1-8B
3https://huggingface.co/Qwen/Qwen3-Embedding-8B

Conference acronym XX, June 03-05, 2018, Woodstock, NY

the discriminative capacity of the embeddings. This yields a sig-
nificant improvement on WN18RkR, as the descriptive information
provides rich semantic cues. In contrast, the effect on FB15k-237
is marginal, since its descriptions contribute limited additional
information beyond the already dense structural connections. In
addition, the improvement on TransE is less pronounced than on
other non-additive embedding models, which is consistent with the
findings reported in PLM-Attack. This is likely due to the fact that
Llama3 is a Transformer-based architecture that relies on scaled
dot-product attention, it is inherently more difficult to transfer to
additive models.

1.0 1.0
0.8 0.8
0.6°—— 0.6°——
DistMult ComplEx ConvE  TransE DistMult ComplEx ConvE  TransE
(a) MRR of WNI8RR (b) Hits@1 of WN18RR
0.8 0.8
0.6 0.6

0.4

4
DistMult ComplEx ConvE  TransE
(d) Hits@1 of FB15k-237

DistMult ComplEx ConvE  TransE
(c) MRR of FB1Sk-237

PR BC Ee

Figure 4: Ablation study on centrality-based filter. PR, BC,
and CC denote pagerank, betweenness, and closeness cen-
tralities, respectively.

As shown in Figure 4, we analyze the diameter of the KG and
select entities within 3 hops as the initial candidate set. Excessive
hop distances reduce perturbation relevance, while insufficient hops
limit misleading noise. For the two datasets, when selecting the
top-30 entities for each centrality measure, the average candidate
size is 46.16 and 57.58, respectively. This indicates that the overlap
among the three filtered candidates is small. On WN18RR, the
results across three centrality-based filters are similar, since inverse
relations provide strong support. It is difficult to identify more
adversarial entities e@noise that could destabilize the influence of
inverse relations.

6 Case Study

As shown in Tables 3 and 6, we present a single triple attack case
with step-by-step reasoning, demonstrating our framework’s abil-
ity to generate human-readable explanations. It is noteworthy that
the HoA-tuned LLM filters candidates more effectively than the
semantics-based filter by leveraging structural information rather
than mere lexical similarity, it filters the bidirectional evidence of
the target triple as the Top1 candidate. The PR+BC+CC ensemble
filters the candidate set containing multiple perturbative yet seman-
tically proximate entities. For the deletion attack, the reasoning
evaluates candidates one by one and selects candidate 1 because


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY Ting Li et al.

Table 3: Case study of a single triple attack on WN18RR with step-by-step reasoning. Green indicates accepted reason, and red
denotes rejected reason.

target triple (feverish, derivationally related form, pyrexia)

(pyrectic, derivationally related form, pyrexia), (pyrexia, derivationally related form, pyrectic), (pyrexia, derivationally related

semantics form, feverish), (hyperpyrexia, hypernym, pyrexia), (pyrexia, hypernym, symptom)

Vv (pyrexia, derivationally related form, feverish), (pyrectic, derivationally related form, pyrexia), (pyrexia, derivationally

Hoods waning related form, pyrectic), (hyperpyrexia, hypernym, pyrexia), (pyrexia, hypernym, symptom)

PR medicine

BC set up

CC information

order, maturity, disorder, immaturity, unsoundness, obligation, noxious, end, hostility, conflict, / unfitness, polyvalency, flaw,

PR+BC+CC ; ae
illumination, unwholesome

The target triple indicates that "feverish" is a derivationally related form of "pyrexia". So, in a knowledge graph, this would be a
relation where one term is derived from another. Looking at the candidate triples:

¥ 1. (pyrexia, derivationally related form, feverish) - This is the inverse of the target triple. Since the target is (feverish,
derivationally related form, pyrexia), this triple is (pyrexia, same relation, feverish). So they are bidirectional. If the model has
both, they reinforce each other. Removing one might weaken the model’s confidence in the other.

X 2. (pyrexia, derivationally related form, pyrectic) - This suggests that pyrexia and pyrectic are derivationally related. But
pyrectic is an adjective meaning having a fever, so maybe it’s related. But how does this affect the target triple? If the model has
this, it might provide alternative relations for pyrexia, but not directly conflicting with the target.

X 3. (pyrectic, derivationally related form, pyrexia) — This is the inverse of candidate 2. So similar to candidate 1, but with pyrectic
explanation instead of feverish. If the model has both 2 and 3, they might reinforce each other. But the target is about feverish and pyrexia. So

ofthe removing this might not directly affect the target unless pyrectic is related to feverish.

deletion attack X 4. (hyperpyrexia, hypernym, pyrexia) — This is a hypernym relationship. Hyperpyrexia is a type of pyrexia (a high fever). So
this is a different relation (hypernym) and not directly related to the derivational relation in the target. Removing this might not
affect the target triple much.
X 5. (pyrexia, hypernym, symptom) - This says pyrexia is a hypernym of symptom. But that’s not correct. Symptoms are a
category, and pyrexia is a specific symptom. So maybe this is incorrect. However, if the model has this, it might be a wrong
relation. But how does that affect the target triple? The target is about derivational relations, so this hypernym relation might not
directly interfere.
So for the current problem, the inverse of the target triple is candidate 1: (pyrexia, derivationally related form, feverish).
If we remove this, the model would lose the bidirectional evidence for the derivational relation between feverish and
pyrexia. That would make the model less confident in predicting the target triple.

The relation is “derivationally related form”. In linguistics, such forms are related through affixation or other morphological
processes. The triple is (pyrexia, derivationally related form, feverish), so the model learns that “pyrexia” is derivationally related
to “feverish”.

The replacement should be semantically close enough to “pyrexia” to appear in training, but different enough to disrupt the
learned embeddings. We need a noun that can derive into an adjective like “feverish”. Looking at the candidates: 1. order —
noun; 2. maturity — noun; 3. disorder — noun; 4. immaturity — noun; 5. unsoundness — noun; 6. obligation — noun; 7. noxious
— adjective; 8. end — noun; 9. hostility — noun; 10. conflict - noun; 11. unfitness — noun; 12. polyvalency — noun; 13. flaw -
noun; 14. illumination — noun; 15. unwholesome — adjective; Nouns with clear adjective forms include: disorder—disordered,
unfitness—unfit, flaw— flawed. “Feverish” is an adjective meaning having a fever; “pyrexia” is a medical noun for fever. Replacing
“pyrexia” with a noun that derives to a different adjective should confuse the model.

explanation
of the
addition attack

¥ “Unfitness” (11) is a noun related to being unwell, whose adjective is “unfit”, not “feverish”; the model might expect “unfit”
rather than “feverish”, disrupting the learned relation.

X “Disorder” (3) is broader and would suggest “disordered”, which is not specifically tied to feverish.
X “Flaw” (13)->“flawed” is even less related.

Therefore, 11 (unfitness) is the best choice.



LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

its inverse relation provides bidirectional corroboration with the
target triple, the remaining candidates are rejected with explicit
justifications, such as "not directly conflicting". For the addition at-
tack, the reasoning prioritizes candidates with clear noun—adjective
mappings, candidate 11, "unfitness," is semantically tied to "unwell"
but misaligned with "feverish," thereby increasing confusability.

7 Conclusion

In this paper, we propose LLMAtKGE, a unified framework lever-
aging LLMs to perform black-box attacks against knowledge graph
embeddings KGEs, with the dual capability of target selection and
human-readable explanation generation. It integrates the advan-
tages of both parameter preservation and parameter updating, and
further incorporates the semantics-based filter, centrality-based
filters, and a HoA-tuning filter to reduce the candidate set and al-
leviate context overflow as well as reasoning hesitation, thereby
enhancing attack effectiveness. We conduct extensive experiments
on WN18RR and FB15k-237 across four KGEs, executing both dele-
tion and addition attacks. The results show that, under black-box
settings, our method consistently outperforms the strongest exist-
ing baselines. In many cases, it even approaches or surpasses the
performance of white-box attacks, while also providing reasoning
for the generated attacks.


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

References

[1]

[2]

[3]

[4]

[5]

[10

[11

[12

[13

[14

[15

[16

[17

[18

[19

[20

[21

[22

Patrick Betz, Christian Meilicke, and Heiner Stuckenschmidt. 2022. Adversarial
Explanations for Knowledge Graph Embeddings. In Proceedings of the Thirty-First
International Joint Conference on Artificial Intelligence. 2820-2826.

Peru Bhardwaj, John Kelleher, Luca Costabello, and Declan O’Sullivan. 2021.
Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution
Methods. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing. 8225-8239.

Peru Bhardwaj, John D. Kelleher, Luca Costabello, and Declan O’Sullivan. 2021.
Poisoning Knowledge Graph Embeddings via Relation Inference Patterns. In
Proceedings of the 59th Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on Natural Language Processing.
1875-1888.

Antoine Bordes, Nicolas Usunier, Alberto Garc’1a-Dur’an, Jason Weston, and
Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-Relational
Data. In Advances in Neural Information Processing Systems. 2787-2795.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances
in Neural Information Processing Systems, Vol. 33.

Hanzhu Chen, Xu Shen, Jie Wang, Zehao Wang, Qitan Lv, Junjie He, Rong Wu,
Feng Wu, and Jieping Ye. 2025. Knowledge Graph Finetuning Enhances Knowl-
edge Manipulation in Large Language Models. In The Thirteenth International
Conference on Learning Representations. 1-14.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.
Convolutional 2D Knowledge Graph Embeddings. Proceedings of the AAAI
Conference on Artificial Intelligence 32, 1 (2018).

Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva
Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan
Larson. 2024. From Local to Global: A Graph RAG Approach to Query-Focused
Summarization. arXiv preprint arXiv:2404. 16130 (2024).

Emma J Gerritse, Faegheh Hasibi, and Arjen P de Vries. 2020. Graph-Embedding
Empowered Entity Retrieval. In Advances in Information Retrieval: 42nd Euro-
pean Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020,
Proceedings, Part I. 97-110.

Lingbing Guo, Zhongpu Bo, Zhuo Chen, Yichi Zhang, Jiaoyan Chen, Yarong
Lan, Mengshu Sun, Zhigiang Zhang, Yangyifei Luo, Qian Li, Qiang Zhang, Wen
Zhang, and Huajun Chen. 2024. MKGL: Mastery of a Three-Word Language. In
Advances in Neural Information Processing Systems.

Lingbing Guo, Yichi Zhang, Zhongpu Bo, Zhuo Chen, Mengshu Sun, Zhiqiang
Zhang, Wen Zhang, and Huajun Chen. 2025. K-ON: Stacking Knowledge On the
Head Layer of Large Language Model. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 39. 11745-11753.

Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG:
Simple and Fast Retrieval-Augmented Generation. arXiv preprint arXiv:2410.05779
(2024).

Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su.
2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large
Language Models. In Advances in Neural Information Processing Systems.

Bernal Jim’enez Guti’errez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025.
From RAG to Memory: Non-Parametric Continual Learning for Large Language
Models. arXiv preprint arXiv:2502.14802 (2025).

William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-
tation Learning on Large Graphs. In Advances in Neural Information Processing
Systems. 1024-1034.

Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge
Graph Embedding Based Question Answering. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining. 105-113.

Sourabh Kapoor, Arnab Sharma, Michael R"oder, Caglar Demir, and Axel-Cyrille
Ngonga Ngomo. 2025. Robustness Evaluation of Knowledge Graph Embedding
Models Under Non-Targeted Attacks. In The Semantic Web, Vol. 15718. 264-281.
Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In The Fifth International Conference on Learning
Representations. 1-14.

Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. 2025. Dual Reasoning: A
GNN-LLM Collaborative Framework for Knowledge Graph Question Answering.
In Conference on Parsimony and Learning, Vol. 280. 351-372.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.
In Proceedings of the 7th International Conference on Learning Representations.
Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2024. Reasoning
on Graphs: Faithful and Interpretable Large Language Model Reasoning. In The

Twelfth International Conference on Leiria Represent 1-14.
Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Yuan-Fang Li, Chen Gong, and

Shirui Pan. 2025. Graph-Constrained Reasoning: Faithful Reasoning on Knowl-
edge Graphs with Large Language Models. arXiv preprint arXiv:2410.13080 (2025).

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

Ting Li et al.

Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin
Mao, and Jian Guo. 2025. Think-on-Graph 2.0: Deep and Faithful Large Language
Model Reasoning with Knowledge-Guided Retrieval Augmented Generation. In
The Thirteenth International Conference on Learning Representations. 1-14.
Fedor Nikolaev and Alexander Kotov. 2020. Joint Word and Entity Embeddings for
Entity Retrieval from a Knowledge Graph. In Advances in Information Retrieval:
42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17,
2020, Proceedings, Part I. 141-155.

Pouya Pezeshkpour, Yifan Tian, and Sameer Singh. 2019. Investigating Robust-
ness and Interpretability of Link Prediction via Adversarial Modifications. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies. 3336-3347.

Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and
Christopher D. Manning. 2024. RAPTOR: Recursive Abstractive Processing for
Tree-Organized Retrieval. In The Twelfth International Conference on Learning
Representations. 1-14.

Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020. Improving Multi-Hop
Question Answering over Knowledge Graphs Using Knowledge Base Embeddings.
In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. 4498-4507.

Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun
Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2024. Think-on-Graph:
Deep and Responsible Reasoning of Large Language Model on Knowledge Graph.
In The Twelfth International Conference on Learning Representations. 1-14.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-
edge Graph Embedding by Relational Rotation in Complex Space. In Proceedings
of the 7th International Conference on Learning Representations. 1-14.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choud-
hury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text
and Knowledge Bases. In Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing. 1499-1509.

Th’eo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume
Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proceedings
of The 33rd International Conference on Machine Learning. 2071-2080.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Li‘o, and Yoshua Bengio. 2018. Graph Attention Networks. In The Sixth Interna-
tional Conference on Learning Representations. 1-12.

Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei,
Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, and Huajun Chen.
2024. Learning to Plan for Retrieval-Augmented Large Language Models from
Knowledge Graphs. In Findings of the Association for Computational Linguistics:
EMNLP 2024, Miami, Florida, USA, November 12-16, 2024. 7813-7835.

Yanbin Wei, Qiushi Huang, James T. Kwok, and Yu Zhang. 2023. KICGPT: Large
Language Model with Knowledge in Context for Knowledge Graph Completion.
In Findings of the Association for Computational Linguistics: EMNLP 2023. 8667-
8683.

Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying Graph Convolutional Networks. In Proceedings of
the 36th International Conference on Machine Learning, Vol. 97. 6861-6871.
Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhihong Zhu, Tong Xu, Xiangyu
Zhao, Yefeng Zheng, and Enhong Chen. 2024. Multi-Perspective Improvement
of Knowledge Graph Completion with Large Language Models. arXiv preprint
arXiv:2403.01972 (2024).

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em-
bedding Entities and Relations for Learning and Inference in Knowledge Bases.
arXiv preprint arXiv:1412.6575 (2015).

Guanggqian Yang, Lei Zhang, Yi Liu, Hongtao Xie, and Zhendong Mao. 2024. Ex-
ploiting Pre-Trained Language Models for Black-Box Attack against Knowledge
Graph Embeddings. ACM Transactions on Knowledge Discovery from Data 19, 1
(2024), 1-14.

Liang Yao, Jiazhen Peng, Chengsheng Mao, and Yuan Luo. 2025. Exploring Large
Language Models for Knowledge Graph Completion. In 2025 IEEE International
Conference on Acoustics, Speech and Signal Processing. 1-5.

Hengtong Zhang, Tianhang Zheng, Jing Gao, Chenglin Miao, Lu Su, Yaliang Li,
and Kui Ren. 2019. Data Poisoning Attack against Knowledge Graph Embedding.
In Proceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence. 4853-4859.

Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Wen Zhang, and Huajun
Chen. 2024. Making Large Language Models Perform Better in Knowledge
Graph Completion. In Proceedings of the 32nd ACM International Conference on
Multimedia. 233-242.

Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang,
Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou.
2025. Qwen3 Embedding: Advancing Text Embedding and Reranking through
Foundation Models. arXiv preprint arXiv:2506.05176 (2025).

Tianzhe Zhao, Jiaoyan Chen, Yanchi Ru, Qika Lin, Yuxia Geng, and Jun Liu. 2024.
Untargeted Adversarial Attack on Knowledge Graph Embeddings. In Proceedings


LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

of the 47th International ACM SIGIR Conference on Research and Development in [44] Enyuan Zhou, Song Guo, Zhixiu Ma, Zicong Hong, Tao Guo, and Peiran Dong.
Information Retrieval. 1701-1711. 2024. Poisoning Attack on Federated Knowledge Graph Embedding. In Proceed-
ings of the ACM Web Conference 2024. 1998-2008.


Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

A Prompt Template

We present the prompt template used for deletion attacks on WN18RR.
For experiments on other datasets, the template remains identi-
cal except for the example component, which is replaced accord-
ingly. Example instances are sampled from the validation set of
KG, and the human-readable explanation may be produced by a
fully-parameterized LLM such as ChatGPT-5.

Prompt for deletion attack on WN18RR.

You are an expert in knowledge graph reasoning and
adversarial attack design. Your task is to select one candidate
triple from the list below to delete, in order to destroy the
ability of a knowledge graph embedding model (such as ConvE)
to correctly predict the target triple.

Example Target triple: (telephone, verb group, call).

Example Candidate triples:

1. (call, verb group, telephone)

2. (call in, hypernym, telephone)

3. (telephone, hypernym, telecommunicate)

4. (telephone, derivationally related form, telephony)

5. (telephony, derivationally related form, telephone).

The target triple is (telephone verb group call). Among the
candidates, triple 1 (call verb group telephone) is the inverse of
the target and encodes the same semantic relation. Removing
this mutual reinforcement (bidirectional "verb group") relation
is likely to significantly degrade the model's confidence in
predicting the target triple. It disrupts symmetric contextual
evidence, which ConvE and similar embedding models heavily
rely on. "answer": "3".

Please process the following target triple ({target_triple}).
Here are some candidate triples: {influence_triple_choice}.
The details of entity is described as follows: {entity_desc}.
Please show your choice in the answer field with only the
choice number, e.g., "answer": "1".

B_ Detailed Settings
B.1 Datasets

Table 4: Summary statistics of datasets.

Dataset #Ent #Rel #Train #Dev_ #Test

WN18RR
FB15k-237

40559 «11
14505 9237

86835 2824 2924
272115 17526 20438

We conduct experimental analyses on two public datasets, as
shown in Table 4. WN18RR [7] is a subset of WordNet, containing

Ting Li et al.

40,559 entities and 11 relation types. The triples primarily capture
hierarchical relations among words, and the overall structure ex-

hibits clear hierarchical organization and regularity. In contrast,
FB15k-237 [30] is derived from Freebase, comprising 14,505 entities

and 237 relation types. Its knowledge spans multiple domains such
as movies, actors, awards, sports, and teams, with more diverse
relation types and structural patterns.

B.2. Experimental Environment

Qwen3-8B? allows explicit control of the <thinking> field to switch
chain-of-thought reasoning, thereby improving reasoning quality.
Moreover, it supports a standardized output format for multiple-
choice questions. Therefore, we adopt Qwen3-8B as the instruction
LLM for reasoning.

For parameter-efficient fine-tuning, we employ LoRA with a
rank of r = 64, a scaling factor of a = 16, and a dropout rate of
0.05. The LoRA adapters are applied to the query, key, value, and
output projection layers within the attention blocks. We set the
batch size to 16 and adopt the AdamW optimizer [20]. Under our
experimental setup, the HoA tuning LLM trains for about 3 h on
WNI18RR and 8 h on FB15k-237.

The implementation of LLMAtKGE is developed on PyTorch and
Transformers, while the HoA filter is based on PEFT and NetworkX
libraries. The computing environment is configured as follows: the
OS is Ubuntu 22.04.5 LTS, with an Intel(R) Xeon(R) Platinum 8488C
CPU, and five NVIDIA L20 48GB GPUs.

B.3. Hyperparameter Analysis

Table 5: Performance under different TopK on FB15k-237.

K DistMult ComplEx ConvE

MRR H@1 MRR H@1 MRR He@l1

5 0.70 0.54 0.70 0.53 0.60 0.39
10 065 0.48 0.63 048 0.53 0.36
30 ©0.64 ~=—0.50 0.65 0.52 0.52 0.37

The choice of TopK significantly affects the performance of both
the filters and the attacks. We search multiple K values. For the dele-
tion attack, we set K = {3,5,10} on WN18RR and K = {5, 10, 30}
on FB15k-237, as shown in Table 5. For the addition attack, we
experiment with K = {15, 30, 50}.

C_ Case Study of Hesitation

Due to limited space, we only present key reasoning traces. As
shown in Table 6, this LLM-based attempt often generates interme-
diate cues like "wait," reflecting transient hesitation or self-conflict
within the reasoning process prior to final decision-making.

*https://huggingface.co/Qwen/Qwen3-8B


LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings Conference acronym ’XX, June 03-05, 2018, Woodstock, NY

Table 6: Case study of a deletion attack on WN18RR showing reasoning hesitation.

Please process the following target triple: (throbbing derivationally related form throb). Here are some candidate triples: 1. (throb
derivationally related form throbbing) 2. (throbbing derivationally related form throb) 3. (throbbing derivationally related form thump)

First, I should understand what a derivationally related form means. Dervationally related forms are words that are derived from each
other, like how "throb" can become "throbbing" by adding a suffix. So, the target triple is saying that "throbbing" is a derivationally related
form of "throb". Looking at the candidates:

1. (throb derivationally related form throbbing) — This seems to reverse the order of the target triple. The target is (A derivationally
related form B), so this would be (B derivationally related form A). But since derivation can go both ways (like base form to derived
form), this might still be correct. However, the original target is (throbbing derivationally related form throb), so the first candidate is the
inverse. But does that matter?

2. (throbbing derivationally related form throb) — This is exactly the same as the target triple. So this would be the same, so semantically
identical. But wait, the target is (throbbing derivationally related form throb), so candidate 2 is the same as the target. But the user
is asking for the most semantically similar, so maybe the answer is 2? But wait, the example given in the problem had the target as
(telephone verb group call) and the answer was (call verb group telephone) because it was the exact inverse. So in that case, even though
the order was reversed, the relation was the same. So maybe in this case, candidate 1 is the inverse of the target, but the target is (A
derivationally related form B), and candidate 1 is (B derivationally related form A). So the relation is the same, but the order is reversed.
But the target is (throbbing derivationally related form throb), so the first element is the derived form, and the second is the base form. So
candidate 2 is the same as the target, so it’s exactly the same. Wait, but the target is (A, relation, B), and candidate 2 is (A, relation, B). So
if the target is (throbbing, derivationally related form, throb), then candidate 2 is exactly the same. But the user’s example had the target
as (telephone verb group call), and the answer was (call verb group telephone) because it was the inverse. So in that example, the answer
was the inverse, but the relation was the same.

So, if the target is (A, relation, B), then the correct answer would be the one that has the same relation and the same elements, but possibly
in reverse order. But in the example, the user considered the inverse as the correct answer. So perhaps in this case, the answer is candidate
1, but wait, the target is (A, relation, B), and candidate 1 is (B, relation, A). So the relation is the same. So the semantic relation is that the
first element is a derivationally related form of the second. So if the target is (A, derivationally related form, B), then candidate 1 is (B,

derivationally related form, A), which is the same relation but reversed.

So, the key is to check if the relation is the same. In the target, the relation is "derivationally related form". So the first element is a
derivationally related form of the second. So candidate 2 is (throbbing derivationally related form throb), which is exactly the same as the
target. So candidate 2 is the same triple. But the user is asking to select the most semantically similar triple from the list. If candidate 2 is
exactly the same as the target, then that’s the most similar. However, maybe the user’s example is different.

So in this case, if candidate 2 is the same as the target, then that’s the most similar. But why would the user have that as a candidate?
Wait, the target is (throbbing derivationally related form throb), and candidate 2 is (throbbing derivationally related form throb), which
is the same. So the answer would be 2. But the example in the problem shows that the inverse is considered the same. So perhaps the
user is considering that the order doesn’t matter, so both the target and candidate 1 are semantically similar. In this case, the relation is
"derivationally related form". So if the target is (A, relation, B), then the inverse is (B, relation, A). So the relation is the same, and the
words are the same.

But I think the correct answer is 2, because it’s the same as the target. So I’m torn.

So the answer would be 1. But the target is (A, relation, B), and candidate 1 is (B, relation, A). So the answer is 1.

But candidate 2 is the same as the target. So why is the answer not 2?

I'm really confused now. I’m not sure.

