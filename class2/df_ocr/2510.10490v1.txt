arX1iv:2510.10490v1 [cs.CL] 12 Oct 2025

VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for
ultra low-resource scripts through Auto Glyph Feature Extraction

Prawaal Sharma
Infosys
Pune, Maharashtra, India
prawaal_sharma @infosys.com

Vidisha Sharma
Pune, Maharashtra, India
vidishasharma@ gmail.com

Abstract

UNESCO has classified 2500 out of 7000 lan-
guages spoken worldwide as endangered. At-
trition of a language leads to loss of traditional
wisdom, folk literature, and the essence of the
community that uses it. It is therefore imper-
ative to bring digital inclusion to these lan-
guages and avoid its extinction. Low resource
languages are at a greater risk of extinction.
Lack of unsupervised Optical Character Recog-
nition(OCR) methodologies for low resource
languages is one of the reasons impeding their
digital inclusion. We propose VOLTAGE - a
contrastive learning based OCR methodology,
leveraging auto-glyph feature recommendation
for cluster-based labelling. We augment the
labelled data for diversity and volume using im-
age transformations and Generative Adversar-
ial Networks. Voltage has been designed using
Takri - a family of scripts used in 16th to 20th
century in the Himalayan regions of India. We
present results for Takri along with other Indic
scripts (both low and high resource) to substan-
tiate the universal behavior of the methodology.
An accuracy of 95% for machine printed and
87% for handwritten samples on Takri script
has been achieved. We conduct baseline and ab-
lation studies along with building downstream
use cases for Takri, demonstrating the useful-
ness of our work.

1 Introduction

The UNESCO "Atlas of the World’s Languages
in Danger" (UNESCO, 2021) is considered as
a benchmark for the comprehensive list of the
world’s endangered languages. This study unveils
more than 2500 languages and dialects as endan-
gered out of which 200 come from Indian demog-
raphy.

Optical character recognition (OCR) is used for
digitizing historical archives, helping language con-

Poonam Goyal
BITS Pilani
Pilani, Rajasthan, India
poonam @pilani.bits-pilani.ac.in

Navneet Goyal
BITS Pilani
Pilani, Rajasthan, India
goel @pilani.bits-pilani.ac.in

servation. There are plenty commercial and open-
source OCR engines available for contemporary
documents. However, very Low Resource Scripts
(LRS) differ in their requirements mainly because
of non-availability of large volume of data and lim-
ited users. The two most popular unsupervised (or
semi-supervised) OCR methods available include
Ocular (Berg-Kirkpatrick et al., 2013) and anyOCR
(Bukhari et al., 2017). Both methods are designed
for large datasets and hence cannot be applied to
LRS effectively.

Another alternative is to apply pretrained models
for a high resource language as a foundation model
and apply few shot learning to customize it and get
the desired results. Our experiments conducted for
"Takri" using this approach do not result in good
accuracy. We have discussed this in detail in results
section.

We develop an automated versatile unsupervised
OCR methodology (VOLTAGE) for very low re-
source scripts to address the gap. We use Takri as
an example to develop our methodology due to (a)
No available labelled data and scanty user base (b)
Extremely low digital unlabelled resources. We
further evaluate the proposed methodology on four
other languages to validate the universal behavior.

As illustrated in Figure 1, VOLTAGE comprises
of four steps, (a) Extraction: segmentation of
available data into pages, lines, words, characters
and symbols; (b) Annotation: feature extraction
and recommendation followed by cluster based
labelling; (c) Re-enforcement: augmentation of
dataset using image transformation and generative
AI (GANs); (d) Identification: contrastive learn-
ing based classification for character identification.
The novelty of the methodology is that the man-
ual intervention including human oracles is bare-
minimum. The proposed glyph pattern-based fea-
ture recommender system can be applied to any


EXTRACTION ANNOTATION

RE-ENFORCEMENT

IDENTIFICATION

Scanned
Documents

Zone
i Disection

H Feature H
' Recomendation H

Automated
i Labeleling H

Page
1 Segmentation
Line & Word
i Segmentation i

Character & |
H Symbol |
1 Segmentation |

Image
{ Transformation

Contrastive
Learning |

Generative
Images (GAN) |

Post
Processing
-—

a | 4

Digital Transliterated
Documents

Trained Model

Figure 1: High level design for Versatile unsupervised OCR methodology for Low-resource scripts Through Auto

Glyph feature Extraction (VOLTAGE)

script to recommend apropos feature set.

We empirically discuss our results in detail for
Takri and also evaluate on other Indic scripts to
validate its generalization capabilities. We also
conduct baseline and ablation studies to substanti-
ate our results. The contribution summary of our
work is three fold:

¢ We build versatile and automated OCR
methodology using contrastive learning ap-
proach for ultra low resource scripts.

¢ We build a novel glyph feature recomendor
system for unsupervised labelling of symbols
which can be applied universally.

We build the largest labelled Takri dataset
containing approximately 226,000 symbols,
along with downstream use cases on transliter-
ation and synthetic symbol generation models
for public use.

2 Related Work

2.1 Optical Character Recognition (OCR)

The early ideas of OCR dates back to 1870’s
(Chaudhuri et al., 2017). Since then, the OCR sys-
tems have evolved, and in the modern world there

are many open source OCR systems like Tessaract
(Smith, 2007), OCRopus (Breuel, 2008), Kraken

(Kiessling et al., 2017) and Calamari (Wick et al.,
2018) etc. Although research on OCR for Indic
scripts started only in the mid 1970s, however
the scope of research was restricted to Devana-
gari, Tamil and Telugu scripts only (Govindan and
Shivaprasad, 1990). Even today, the major work
in Indic OCR is limited to the ten scripts namely,
Bangla, Devanagari, Gurumukhi, Gujarati, Kan-
nada, Malayalam, Oriya, Tamil, Telugu, and Urdu
(Chaudhuri, 2009). Most of the current OCRs are
based on deep neural networks which tends to be
hungry on data and computational power.

OCR pipeline generally goes through multiple
individual tasks including (a) Image acquisition
(extracting images containing text from multiple
sources for offline images, and capturing live im-
ages for online extraction) (b) Pre-processing (ap-
plication of image processing techniques, to in-
crease raw image quality) (b) Binarization (for sce-
narios where text and images/videos are mixed,
we need to isolate text images from background)
(c) Layout Analysis (dividing the images into re-
gions) (d) Segmentation (segmentation of image
into pages, lines, words, characters and symbols)
(e) Feature Analysis (identification and extraction
of key features) (f) Classification (Recognition
of symbol with scrip character-set) (g) Post pro-
cessing (use of pre-compiled vocabulary and lan-


guage rules to auto correct the unrecognized words)
(Tomaschek, 2018).

Supervised OCR: uses labelled dataset for train-
ing the classifier. Supervised methods give better
performance however, annotation of character level
images needs a lot of efforts and is not practical for
low resource languages (LRL) where availability of
annotators is scarce. Most supervised SOTA OCR
systems like Tessarct and OCRopus are pre-trained
on a very large image data sets based on deep CNN
neural networks (Zeiler and Fergus, 2014).

Unsupervised OCR: Unsupervised transform-
ers like BERT (Devlin et al., 2018), GPT (Radford
et al., 2019) etc. have become very successful for
diverse NLP tasks. In case of OCR systems only
a few unsupervised (or semi-supervised) methods
are available like Ocular (Berg-Kirkpatrick et al.,
2013) and anyOCR (Bukhari et al., 2017). Ocular
uses generative modelling approach incorporating
font typesetting, inking and noise. AnyOCR on
the other hand is semi-supervised and language ag-
nostic which consumes historical documents and
clusters them for training purpose.

From the best of our knowledge, there is no
general purpose OCR methodology suitable for
ultra low resource scripts used by very limited user
groups, with limited or no digital data available.
VOLTAGE fills that gap, with the design of au-
tonomous OCR pipeline enabling digitization of
ultra low resource scripts.

2.2 Data Augmentation

Data Augmentation (DA) helps with increase of
volume and diversity of data. With the advent of
deep learning methods where the efficiency and
accuracy of models is proportional to the training
data, it has become imperative to use data aug-
mentation approaches to generate large volumes of
synthetic data and improve the performance of the
model (Saini et al., 2022).

Data augmentation for images is classified into
two categories (a) Extractive, which augments data
applying rules and transformations in form of ro-
tation, brightness, sheer, zoom, flips etc. (Spruck
et al., 2021; Kumar et al., 2022) and (b) Gener-
ative, which synthesises data based on existing
patterns using Generative Adversarial Networks
(GAN) (Aggarwal et al., 2021). Generative meth-
ods helps in expanding the diversity of textual im-
ages along with inclusion of noise, and is therefore
is very close to human generated samples (Kukreja

et al., 2020; Abedin et al., 2022; Wu et al., 2021).

2.3 Contrastive Learning

The use of contrastive learning in various NLP
and computer vision tasks is becoming very popu-
lar in recent years (Zhang et al., 2022), including
sentence embeddings (Gao et al., 2021), language
translation (Pan et al., 2021), text generation (Shu
et al., 2021) etc. Contrastive learning can be ap-
plied in a self supervised mode, where the anchor
and the positive sample are pulled together in em-
bedding space, the negative samples are pushed
apart (Chen et al., 2020; Tian et al., 2020). An-
other approach of using contrastive learning is in
supervised mode where multiple positives per an-
chor are pulled closer together along with many
negatives anchors which are pulled further (Khosla
et al., 2020). The contrastive losses in this case is
the generalization of triplet (Weinberger and Saul,
2009) and N-Pair (Sohn, 2016) losses. In our work,
we use supervised contrastive learning to build an
character recognition model for ultra low scripts.

3 Scripts and Datasets
3.1 The choice of script

Our methodology is designed for scripts with very
low digital resources, hence the choice of script to
validate our methodology is an important decision.
Takri script, has extremely low available digital
resources, no labelled dataset along with low user
base. George Grierson, in his Linguistic Survey of
India, describes Takri and its variations as a script
with shared inherent characteristics consequently
classifying it as a "class of scripts" rather than a
single script (Grierson, 1909).

To further validate the claim of having common
linguistic characteristics within the dialects using
Takri as a script, we use a set of 25 sentences used
in day to day conversation, and translate them to
seventeen dialects (used in the Himalayan regions
of Himachal Pradesh, India) by the use of human
annotators who are fluent in these dialects. We
empirically study various semantic, lexical and
syntactic features for these dialects and explore
interdependence among these dialects using ag-
glomerative hierarchical clustering (Roux, 2018).
Appendix C illustrates the relationship between
various dialects used in the Himalayan regions and
how the use of one script binds all of them together.

Takri, like most Indic script falls under Abugidas
class of writing systems (Daniels, 2017), and some


of the salient characteristics are summarised below:

¢ The character set of Takri comprises of 11
vowels, 33 consonants and 10 numbers.

¢ There are 10 vowel modifiers which can occur
on the top, below, left or right of the conso-
nants.

¢ Takri script does not contain headline unlike
other Indic scripts like Devanagari

¢ Half forms are not used in most versions of
Takri.

¢ Ligatures are also infrequently written.

¢ Most characters consists of connected compo-
nents only.

¢ Compound characters are not present in Takri.

3.2 Datasets

To the best of our knowledge, there is only single
source of a good quality dataset sourced from ma-
chine printed Takri books collected manually con-
sisting of 272 text blocks containing 2,584 lines
and 10,880 words with the resolution of 200dpi
(Magotra et al., 2019). We use this dataset as the
base and add more samples to increase volume
and diversity of samples using data augmentation
techniques.

For Gujarati, we use the machine printed limited
dataset by Goswami et al. consisting of 7,221 sym-
bols (Goswami and Mitra, 2014). For Modi, we
use available dataset by Chandankhede et al (Chan-
dankhede and Sachdeo, 2023a). For Ol chiki and
Wancho, there is no dataset available so we use the
available printed books and build our own dataset.

4 VOLTAGE: The proposed methodology

Versatile contrastive learning based OCR method-
ology for ultra Low-resource scripts Through Auto
Glyph feature Extraction (VOLTAGE) follows the
pipeline of tasks including, pre-processing and seg-
mentation, automated feature engineering and un-
supervised labelling, data augmentation and clas-
sification, post processing and evaluation (see Fig-
ure 1). We validate our results for Takri on end
to end errors and character/word error rates. We
further validate VOLTAGE for Modi, Ol Chiki, Gu-
jarati and Wancho to establish the universal effec-
tiveness of our work. We use Python 3.8.12 with
conda, opencv for image processing along with
ml libraries (keras, numpy, transformers) for our
experiment.

4.1 Extraction

It is imperative to extract and segment the input
source into lines, words, characters and symbols
before it can be put to use for downstream OCR
tasks. Segmentation of page into lines and lines
into words leverages computation of horizontal and
vertical projections (HX and VY as illustrated in
Eq. 1 and 2) and find valleys within the thresh-
old (Likforman-Sulem et al., 2007; Shinde and
Chougule, 2012; Magotra et al., 2021).

a=Width
S- (No. of black pixels for x;) (1)

x=0

VY; =

y= Height
S- (No. of black pixels for y;) (2)
y=0

AX; =

Segmentation of words into characters is slightly
more complex due to close vicinity of characters
and overlaps. To solve this issue, we have enhanced
Eq. 2 and compute the enhanced horizontal pro-
jection (EHX) which applies additional penalty in
downward direction for character segmentation (Eq.
3) because most overlaps in Takri occur in the up-
per parts. We have observed that this technique,
helps in overall reduction of segmentation errors
by 3%.

We have observed that abugidas class of scripts
overlap their symbols (like Takri) and alphabetic
scripts are isolated. Hence when we conduct our
experiment for other scripts, we use EHX for
Modi/Gujarati and HX for Ol chiki/Wancho.

y= Height
EAX, = Ss" (No. of black pixels for y;)
y=0
+(Penalty Wt. * y;) (3)

Furthermore, we further break the individual
characters into sub-characters (also called as sym-
bols) by dividing the space into three zones. We
design a three step procedure to achieve this. (i)
The first step is Skeletonization which reduces the
thickness of the character into single pixel, and
helps to bring uniformity in the thickness irrespec-
tive of the input variation (Saha et al., 2016); (ii)
Once the characters obtain uniform thickness, we
apply Connected Component Labelling to label dis-
joint components (He et al., 2017). Most symbols
in Takri are connected (apart from few exceptions


NES
F13

2
le}
|
uw
|
<
m
|
s

eH (3) | NO
Ly (4) |

{1,2,3,4}
F200 | 6)

Figure 2: Partition of characters into small groups based on glyph features as recommended by our design, where

F# are feature identifiers in the glyph feature inventory

like @ and hence this step helps in marking dis-
connected sections within the character. (iii) In
the last step, we apply rule based method (to con-
sider exceptions) and perform Zone Classification
and classify symbols in three different zones. This
step is not needed for alphabetic scripts like En-
glish, and for those scripts characters and symbols
are analogous to each other. The entire process of
extraction is illustrated further in Appendix F.

4.2 Annotation

Takri consists of 50 symbols in middle zone, 6
in upper zone and 4 in bottom zone respectively.
Annotation of extracted 14,000 symbols into appro-
priate category is a very critical activity for effec-
tive OCR design. Labelling each symbol manually
may be the most accurate method but not scalable.
Moreover, it becomes further tedious since there
are only a handful of people who can read Takri.
We perform unsupervised clustering of sym-
bol images individually for each zone to label the
dataset. Unsupervised clustering of images parti-
tions the dataset into visually similar clusters with-
out any access to ground truth labels. We use pre-
trained models on ImageNet and perform partition
into individual characters ! (Van Gansbeke et al.,
2020). We cluster and label images effectively with
96% accuracy for upper and bottom zone charac-
ters (see Table 1). However the accuracy for middle
zone characters was 69% due to large spread of la-
bels. It has been observed, that the errors have a
linear dependency on the number of clusters (Franti
and Sieranoja, 2019). This is also evident from
our experiment, and hence it is advisable to divide
middle-zone into smaller groups to overcome this.

'https://paperswithcode.com/task/image-clustering

Glyph Feature Recommendation System
(GFRS): We design a novel recommender system
"GFRS", which analyses and recommends the most
appropriate glyph features for a given script from
our inventory of glyph features without any human
intervention. It takes a set of characters used in
a language as input and recommends a tree struc-
ture with the recommended glyph features, and
distribute the characters into smaller groups for
more effective annotation. We have validated this
for Takri along with 4 other Indic scripts. As illus-
trated in Figure 2, when GFRS is applied on Takri
it recommends 9 features (F1: Presence of head-
line; F2: Number of loops; F5: Presence of right
sidebar; F7: Number of endpoints; F8: Number of
junctions; F12: Aspect ratio; F13: Horizontal sym-
metry; F14: Vertical symmetry; and F15: Number
of dots) from the feature store and each identified
subgroup does not contain more than 6 symbols.
Our process of building the feature store is iterative
after analysing the shape characteristics of multi-
ple Indic scripts. We have observed that using the
approach, the unsupervised labelling accuracy im-
proves to 96% for middle zone (which was 69%
earlier).

Table 1: Unsupervised clustering accuracy for various
zones for various k-means combinations.

Upper Middle Bottom
Zone Zone Zone
Distribution 23% 10% 71%
No. of Labels 5 50 4
Accuracy
50 iterations 91% 61% 93%
300 iterations 96% 69% 97%
With feature recommendor - 96% -



BOER EEL
BO Ea) EY a EEE

rs [=e[as]r2]u [2 [a7
alels]3 [7c

Figure 3: Samples from augmented characters (row |
and 2) using image transformations (row 3 and 4) using
GAN

Appendix A illustrates the entire list of glyph
features (32 in total count), which forms the in-
ventory of shape feature set (analysing the distinc-
tive characteristics of these glyphs like number of
loops, lines, endpoints, junctions, symmetry etc.).
GRES recommends the most appropriate feature
set for a particular script which helps in appropriate
distribution of characters and facilitate automatic
labelling. We also illustrate as part of Appendix
A, the various recommended feature sets for other
scripts used in our paper including Modi, Ol Chiki,
Gujarati and Wancho.

4.3 Re-enforcement

Our labelled dataset contain approximately 14,000
symbols. We augment this dataset applying trans-
formations for rotations, sheer and brightness. We
limit the angular transformation to 9°, and 10%
range on sheer and brightness. Further, we use
the transformed images and build a GAN for each
character, by the use of four layer generator and
discriminator networks, with a learning rate of 2 x
10~* and train on 400 epochs. Figure 3 illustrates
the examples from this. The final dataset contains
225,000 symbols for Takri.

4.4 Identification

The earlier forms of OCR designs, either use CNN
(Zeiler and Fergus, 2014) or a combination of CNN
and LSTM (Staudemeyer and Morris, 2019) as a

Table 2: High level summary on multiple error metrics
for Takri dataset.

Error Metric | VOLTAGE
E2E (End to End) 18%

WER (Word Error) 12%

CER (Character Error) 4%

Table 3: Misinterpretation of characters, due to similar-
ity of glyph or part and whole relationships

Actual | Recognised | Type of error

a a |

v a | Over Segmentation
Al a |

3 3

8 8 Mis Classification
S 3

deep learning method for character identification.
We use supervised contrastive learning for our clas-
sifier, leveraging multiple positive and negative
samples. We discuss the benefits of using this ap-
proach empirically later in this paper. Table 2 illus-
trates multiple error metrics in our work.

Appendix E illustrates details on the architecture
of contrasting learning used in our work. We il-
lustrate how transformations (including image pro-
cessing and GAN) are put to use in an encoder
model which maps them to a latent representation
space, encapsulating features and similarities. We
apply supervised contrastive loss function from
SupCon, to maximise the agreement between pos-
itive pairs (same character images) and minimize
the agreement between negative pairs (different
character images) (Khosla et al., 2020).

-1

= pay

iel pEP(t)

Here z = Proj(Enc(%))) € R??, the - symbol

denotes inner dot product, 7 € Rt is scaler tem-

perature parameter. Index 7 is called anchor, index

j(%) is called positive and other indices are called

negative. P(i) = p € A(t) : Y = % is the set of

indices of all positive in multiviewed batch (2N

augmented samples) distinct from 7, and |P(7)| is
its cardinality.

exp(Zj.2p/T)
lo
a ac A(i) C2P(%i-%a/T)

4.5 Post Processing

Table 3 illustrates some examples where the charac-
ters are misinterpreted due to over segmentation of
characters or incorrect classification due to similar-
ity in visual characteristics. It is therefore pivotal
to have some post processing and correct these er-
rors based on language grammar and patterns. Ap-
pendix B illustrates an inventory of principles and
guidelines for Indic languages. These principles


help in improving the overall accuracy of recog-
nised text, considering linguistic context along with
the syntax. We have identified a set of generic and
specialised linguistic rules for Indic scripts, and
apply them towards the end of our pipeline.

5 Results and Discussion

Quantification of errors in OCR pipeline, specially
for very low resource scripts is ambiguous unless
properly defined (Lopresti, 2008). The document
page needs to be segmented into lines, words, char-
acters and symbols for OCR engine. Any error
which occurs in this step would fall under layout
segmentation error. Most OCR literature do not
include segmentation errors as part of OCR errors.

OCR systems generally consider errors either for
the entire End to End (E2E) pipeline, at word level
or character levels. E2E includes errors at various
stages of pipeline such as Pre-processing, Segmen-
tation, Classification, and Post processing. Word
recognition accuracy or Word Error Rate (WER)
is the average percentage of mis-recognised words.
Character Errors Rate (CER) is the ratio of mis-
recognised symbols within accurately segmented
symbols.

N; N;
WER = —“, CER=—
Nig’ Ns

where N/, is count of mis-recognised words, Ni,
is correctly segmented words, N/ is count of mis-
recognised symbols, and N, is correctly segmented
symbols.

The concept for character/symbol is also am-
biguous unless defined clearly, due to the linguistic
peculiarity for each script. Symbols are monolithic
for Alphabetic languages (like English) which uses
each symbol in same form, however for Abugida
scripts (like Indic scripts) symbols are of multiple
types, namely (a) Root Symbols: like ws ("Ka")
and @# ("Ga") and (b) Modifier/ Marker Sym-
bols: like vowel modifiers ~ ("U") and =» ("Au")
(Daniels, 2017). We consider root and markers sep-
arately independent of each other in our empirical
study. An error in root symbol does not contribute
to the error in associated marker symbols.

Most OCR studies, considers errors within (0-
2%), (2-10%) and (>10%) as good, average and
Poor (Holley, 2009), respectively. However for In-
dic scripts with limited training data and unknown
vocabulary along with heterogeneous handwritten
forms, a CER value as high as around (10-20)% is

Table 4: Empirical study for VOLTAGE on Takri on
Machine Printed (MP) and Hand Written (HW) samples.

Zone | MP HW
UZ (Upper Zone) 96% 88%
MZ (Middle Zone) | 94% 85%
BZ (Bottom Zone) | 97% 89%

considered satisfactory (Shaffi and Hajamohideen,
2021; Tomoiaga et al., 2019). We compute many
error metrics (see Table 2) but observed that all
metrics are co-related. With this observation and
existing practices, we use the standard metric, CER,
for further evaluation not including errors during
segmentation and pre-processing (Holley, 2009).

5.1. Empirical study on Takri

We evaluate our results both for machine printed
and handwritten samples. We identify a group of
22 participants (13 male, 9 female; diverse age
groups; belonging to Himachal Pradesh) who were
made familiar with our work and Takri characters.
We asked them to record symbols and label them.

We also evaluate our results separately for each
zone and analyse the results. As illustrated in
Table 4 we make the following observations, (a)
The upper-zone symbols which account for approx.
16% of corpus have recognition accuracy of 96%
(88% for handwritten samples). (b) The middle-
zone symbols account for the majority of charac-
ters and is most busiest zone. This contributes for
approx 79% of symbols and recognition accuracy
is 94% (85% for handwritten samples). (c) The
bottom-zone is the most infrequent zone with ap-
prox. 5% symbols and recognition accuracy is 97%
(89% for handwritten samples).

Figure 4 presents an illustrative example contain-

Ma] SF a Gs mex F yas vera)

5 Wi na] fv GS ve] a] 1 Hi na] oat ot & at sm Aaa ot ast aed
Tan fax m3 na3 Gri|GG wed ws] 5 8S are Fea gor arett eft | starch
75 gai] Gn had] wad vi md aig fast aid aaa SHS Het TA F
Se we od 73 DuaH fn 5 rw
wah! ow fase Gn Fk may Gd

aadl sa at ae TERI | S 3rqet
cle ep Har sid sractsn fee at aet
ware | Ot faa UIST SH Al HAA SeteT

& Gi) wind up vs ¥ Gard pus at seit WAR Fo Gay ETT sway feat
a O fF asta w a oe fet 1 &

fo i Gt when adn wv a a ag HHT 3d a AN Tf oT Het 3a
4d Bg wk TI WLI] CK wd DI a
> uavi 3] Ge wd w waite wd Gur AN ee a
ye fat 7a a ds we Cee wa a aa Frat aa Pe art fea ¢ ewaila 7
fil m3 nawG wi aaua 5G wd) ort asus star SHet facet s agarg Rareft 11
Hy 33 w6]F TI [hai J] [U3 3 Greed 3]

43d wri Grid] (said) J) aad [ari] 0 2 ae Ral By £ Bae]

gui Gr ua rid) fas] Zi veers UG 3 at Pat card aS al aw sat

wi), aeu fhvid) 433 7 aie faed [os saat fe a at wor ant fer & ugats

5 i [ui Gal dw vt ad Gn

Grivsd| [os vi ii 1828 Tai fat Hi 2606

Figure 4: Applying OCR at page level.


Table 5: Evaluation across other scripts. For Gujarati we experimented with two scenarios, (a) Gujarati LRL- Like
low resource language and (b) Gujarati HRL- like high resource language

Bering Name  Sexipt type Lauuaee SOTA VOLTAGE Dataset — Label Glyph
Accuracy Accuracy _ size count features
Takri Abugida Multiple NA 95% 14,051 59 9
Modi Abugida Marathi (84-94)% 93% 7,221 46 11
Ol Chiki Alphabet Santali (83-92)% 91% 8,873 30 5
Gujarati LRL  Abugida Gujarati (86-96)% 93% 7,643 42 9
Gujarati HRL Abugida Gujarati (86-96)% 96% 200K+ 42 9
Wancho Alphabet Wancho NA 91% 6,500 42 8

ing 18 lines with 162 words and we pass this via
VOLTAGE. We observe that for this sample, 19
words are mis-recognized in recognised output (il-
lustrated in red in the figure), thereby getting E2E
error of 12%.

5.2 Applying the methodology to other scripts

We apply our methodology to four other diverse
Indic scripts (Modi, OI Chiki, Gujarati and Wan-
cho) to validate the overarching effectiveness of our
work. While Modi and Gujarati belong to abugidas
family of scripts with ancient history, Ol Chiki and
Wancho are more modern alphabetic scripts. Modi,
Ol Chiki and Wancho are very low on resources and
less explored (Barman et al., 2022; Chandankhede
and Sachdeo, 2023b; Das). Gujarati is more pop-
ular and resourceful but we use only limited data
for our experiment, treating this as low resource
experiment (Goswami and Mitra, 2017, 2016) and
also as HRL.

The purpose of using a mix of Indic scripts is to
validate the all-inclusive application of our design
across multiple types of scripts. It is evident from
our results in Table 5 that VOLTAGE generalizes
well as it provides consistent results across scripts
and can be very useful for scripts where labelled
data is a scarce.

5.3. Baseline studies

We use the available annotated data-sets for multi-
ple high resource Indic script and fine tune for Takri
(Jawahar et al., 2010). As illustrated in Table 6 we
observe the following issues with this approach, (a)
The choice of what foundation script to choose is
very important. In case of Takri we observed using
Gujarati gives best results. The choice of founda-
tion model has lot of manual intervention hence
we did not include this in the overall process. (b)
We restricted our few shot experiment till 100K,

since moving to higher numbers would need lot of
data which is not feasible for ultra low scripts, also
leads to catastrophic interference thereby defying
the purpose of using a foundation model. We also
see that as the number of samples go up, most mod-
els converge to similar results, and far from that
of obtained by VOLTAGE. Appendix G further il-
lustrates the size and source of individual data sets
across multiple scripts. We also illustrate the SOTA
accuracy for these data rich scripts and compare
with the results we have achieved.

5.4 Ablation studies

We conduct ablation studies for VOLTAGE to sub-
stantiate the contributions of individual elements
along with improved model understanding. We al-
ready illustrated in Section 4.2 that the application
of GFRS improves annotation accuracy by 27%.
We conduct more experiments to substantiate the
importance of each step in the whole pipeline.

We use basic CNN-LSTM models on each zone
separately and test them. We train three separate
sets of models for each zone. Within each zone
there are separate models subject to the source of
training data used, thereby resulting in total of nine
models (see Table 7). These models can be clas-

Table 6: Applying annotated Takri dataset to other script
OCR, and evaluating accuracy.

Foundation Training samples (Takri)
Script 10K 30K 50K 100K
Devanagiri 59% 81% 83% 86%
Gurumukhi 54% 719% 81% 86%
Gujarati 61% 83% 84% 87%
Oriya 56% 719% 82% 87%
Bangali 58% 74% 81% 85%
Tamil 39% 61% T1% 78%



Table 7: Symbol counts in Takri dataset and models used

Sr. No. | Transformations Pe ane Be
count model | count model | count § model
1 None (Actual images) 2981 Bw 9702 BY 1368 Be
Image Transformations | 28,563 EY | 110,012 Brm* | 15,931 EP?
3 Generative Images 40,427 Ex | 164,162 By* | 21,403 Eb

sified as (i) three Base models one for each zone
(B“*, B™, B*) which include only the actual
images extracted from source documents for train-
ing, (1) three Enriched; models (E}'*, E}’’, Eee)
which include data generated by image transforma-
tions along with data used in base models, and (iii)
three Enrichedy models (E¥*, E3’*, ES) which
include images generated by GAN along with the
data used in FE; models. We compare these nine
models with three models (one for each zone) using
contrastive learning as described in our work (V).
As illustrated in Table 8 we observe that VOLT-
AGE models outperforms the base models for both
machine printed and handwritten evaluation.

Table 8: Character error rates (CER) for Machine
Printed (MP) and Handwritten (HW) symbols for CNN-
LSTM models and compare the results with VOLTAGE
models (V)

Model UZ MZ BZ
Composition 16% 79% 5%
B 06% 08% 06%
Ey 04% 07% 03%
CER-MP Eo 08% 09% 10%
V 04% 06% 03%
B 21% 27% 21%
Ey 19% 25% 19%
eoaaineiid Ey 14% 17% 15%
V 12% 15% 11%

5.5 Use Cases: Takri for the digital world

We have observed that there is dearth of printed
books on Takri, hence it is important to facilitate
Takri in printed form. We facilitate NLP for Takri
by developing two ready to use tools, (a) Translit-
eration to Takri facilitating digitization of folk liter-
ature (via development of standardised individual
symbol images, and creating rule based engine to
amalgamation) and (b) Synthetic generative models
for each symbol in Takri.

Appendix D further illustrates with an example,

how our transliteration engine converts text in other
languages to Takri in digital format. This can be
very instrumental to publish small stories, news
headlines etc. and shared in interested community
to facilitate the use of the script. We also share our
GAN models to be used by fellow researchers for
furthering research”.

6 Conclusion and Future directions

The paper presents a comprehensive unsupervised
OCR methodology, VOLTAGE, which includes
a novel Glyph feature recommendation system
(GFRS) for effective symbol labelling. We de-
veloped VOLTAGE using a very low resource
language, Takri, and validated its effectiveness
and generalization on various Indian scripts. We
achieve accuracy at par with SOTA of respective
test scripts. We also build use cases for Takri to
demonstrate the usefulness of the work. Our work
can facilitate the digitization of ultra low resource
scripts thereby save them from extension.

As part of our future work, we shall use our
method to build more comprehensive datasets
along with building the vocabulary for Indic lan-
guages to help in error correction during post pro-
cessing. We also plan to use the method as de-
scribed to digitize more languages within India
partnering with local governments.

Limitations

Glyph feature store is designed keeping in mind,
the stroke characteristics of Indic scripts. Hence it
can work for all Indic scripts without any modifi-
cations, but may need changes for other family of
scripts. It is possible to use same design principles
and extend the feature stores for any other family
of scripts and apply the method as described in our

paper.

*https://github.com/prawaal/Takri


References

Md Abedin, Tapotosh Ghosh, Tazgia Mehrub, Moham-
mad Abu Yousuf, et al. 2022. Bangla printed charac-
ter generation from handwritten character using gan.
In Soft Computing for Data Analytics, Classification
Model, and Control, pages 153-165. Springer.

Alankrita Aggarwal, Mamta Mittal, and Gopi Batti-
neni. 2021. Generative adversarial network: An
overview of theory and applications. International
Journal of Information Management Data Insights,
1(1):100004.

Debaditya Barman, Tuheli Bhattacharya, and Nirmalya
Chowdhury. 2022. A deep learning framework for
handwritten ol chiki character recognition. Journal
of Scientific Research.

Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical docu-
ments. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 207-217.

Thomas M Breuel. 2008. The ocropus open source ocr
system. In Document recognition and retrieval XV,

volume 6815, pages 120-134. SPIE.

Syed Saqib Bukhari, Ahmad Kadi, Mohammad Ayman
Jouneh, Fahim Mahmood Mir, and Andreas Dengel.
2017. anyocr: An open-source ocr system for his-
torical archives. In 2017 14th IAPR international
conference on document analysis and recognition
(ICDAR), volume 1, pages 305-310. IEEE.

Chaitali Chandankhede and Rajneeshkaur Sachdeo.
2023a. Handwritten modi lipi barakhadi dataset.

Chaitali Chandankhede and Rajneeshkaur Sachdeo.
2023b. Offline modi script character recognition
using deep learning techniques. Multimedia Tools
and Applications.

Arindam Chaudhuri, Krupa Mandaviya, Pratixa Badelia,
Soumya K Ghosh, Arindam Chaudhuri, Krupa Man-
daviya, Pratixa Badelia, and Soumya K Ghosh. 2017.
Optical character recognition systems. Springer.

BB Chaudhuri. 2009. On ocr of major indian scripts:
Bangla and devanagari. In Guide to OCR for Indic
Scripts, pages 27-42. Springer.

BB Chaudhuri. 2010. On ocr of major indian scripts:
Bangla and devanagari. Guide to OCR for Indic
Scripts: Document Recognition and Retrieval, pages
27-42.

BB Chaudhuri, U Pal, and Mandar Mitra. 2002. Auto-
matic recognition of printed oriya script. Sadhana,
27:23-34.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoffrey Hinton. 2020. A simple framework for
contrastive learning of visual representations. In Jn-
ternational conference on machine learning, pages

1597-1607. PMLR.

Peter T Daniels. 2017. Writing systems. The handbook
of linguistics, pages 75-94.

Bishakha Das. Creation of scripts in indigenous lan-
guage learning: The present scenario of arunachal
pradesh.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv: 1810.04805.

Jignesh Dholakia, Atul Negi, and S Rama Mohan. 2010.
Progress in gujarati document processing and char-
acter recognition. Guide to OCR for Indic Scripts:
Document Recognition and Retrieval, pages 73-95.

Pasi Franti and Sami Sieranoja. 2019. How much can
k-means be improved by using better initialization
and repeats? Pattern Recognition, 93:95-112.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. arXiv preprint arXiv:2104.08821.

Mukesh Goswami and Suman Mitra. 2014. Dataset
set for machine printed gujarati character symbols
collected from machine printed books.

Mukesh M Goswami and Suman K Mitra. 2016. Clas-
sification of printed gujarati characters using low-
level stroke features. ACM Transactions on Asian

and Low-Resource Language Information Processing
(TALLIP), 15(4):1-26.

Mukesh M Goswami and Suman K Mitra. 2017. High
level shape representation in printed gujarati charac-
ter. In JCPRAM, pages 418-425.

VK Govindan and AP Shivaprasad. 1990. Char-
acter recognition—a review. Pattern recognition,

23(7):67 1-683.

George A Grierson. Linguistic survey of india vol ix,
part i: Western hindi and panjabi. edited by sir.

George A Grierson. 1909. The languages of the north-
ern himalayas, being studies in the grammar of
twenty-six himalayan dialects. by the revt. grahame
bailey, bd, ma, mras asiatic society monographs, vol.
xil. london, 1908. Journal of the Royal Asiatic Soci-
ety, 41(1):184-189.

Lifeng He, Xiwei Ren, Qihang Gao, Xiao Zhao,
Bin Yao, and Yuyan Chao. 2017. The connected-
component labeling problem: A review of state-of-
the-art algorithms. Pattern Recognition, 70:25—43.

Rose Holley. 2009. How good can it get? analysing
and improving ocr accuracy in large scale historic
newspaper digitisation programs. D-Lib Magazine,
15(3/4).

CV Jawahar, Anand Kumar, A Phaneendra, and KJ Ji-
nesh. 2010. Building data sets for indian language
ocr research. Guide to OCR for Indic Scripts: Docu-
ment Recognition and Retrieval, pages 3-25.


Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-
pervised contrastive learning. Advances in neural
information processing systems, 33:18661—18673.

Benjamin Kiessling, Matthew Thomas Miller, Maxim
Romanov, and Sarah Bowen Savant. 2017. Important
new developments in arabographic optical character
recognition (ocr). Al-Usur al-Wusta, 25(1):1.

Aparna Kokku and Srinivasa Chakravarthy. 2010. A
complete ocr system for tamil magazine documents.
Guide to OCR for Indic Scripts: Document Recogni-
tion and Retrieval, pages 147-162.

Vinay Kukreja, Deepak Kumar, Amandeep Kaur, et al.
2020. Gan-based synthetic data augmentation for
increased cnn performance in vehicle number plate
recognition. In 2020 4th international conference on
electronics, communication and aerospace technol-

ogy (ICECA), pages 1190-1195. IEEE.

Mohinder Kumar, MK Jindal, and Munish Kumar. 2022.
Distortion, rotation and scale invariant recognition of
hollow hindi characters. Sadhana, 47(2):1-6.

Gurpreet S Lehal. 2010. A complete machine-printed
gurmukhi ocr system. Guide to OCR for Indic
Scripts: Document Recognition and Retrieval, pages

43-71.

Laurence Likforman-Sulem, Abderrazak Zahour, and
Bruno Taconet. 2007. Text line segmentation of
historical documents: a survey. International Jour-
nal of Document Analysis and Recognition (IJDAR),
9(2):123-138.

Daniel Lopresti. 2008. Optical character recognition er-
rors and their effects on natural language processing.
In Proceedings of the second workshop on Analytics
for Noisy Unstructured Text Data, pages 9-16.

Shikha Magotra, Baijnath Kaushik, and Ajay Kaul.
2019. A database for printed takri class of north-west
indian regional scripts. In International Conference
on Futuristic Trends in Networks and Computing
Technologies, pages 508-520. Springer.

Shikha Magotra, Baijnath Kaushik, and Ajay Kaul.
2021. Use of classification approaches for takri
text challenges. In Information and Communication
Technology for Competitive Strategies (ICTCS 2020),
pages 403-410. Springer.

Xiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li.
2021. Contrastive learning for many-to-many mul-
tilingual neural machine translation. arXiv preprint
arXiv:2105.09501.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Maurice Roux. 2018. A comparative study of divisive
and agglomerative hierarchical clustering algorithms.
Journal of Classification, 35(2):345-366.

Punam K Saha, Gunilla Borgefors, and Gabriella San-
niti di Baja. 2016. A survey on skeletonization al-
gorithms and their applications. Pattern recognition
letters, 76:3-12.

Naresh Saini, Promodh Pinto, Aravinth Bheemaraj,
Deepak Kumar, Dhiraj Daga, Saurabh Yadav, and
Srihari Nagaraj. 2022. Ocr synthetic bench-
mark dataset for indic languages. arXiv preprint
arXiv:2205.02543.

Noushath Shaffi and Faizal Hajamohideen. 2021. uthcd:
a new benchmarking for tamil handwritten ocr. IEEE
Access, 9:101469-101493.

Archana A Shinde and DG Chougule. 2012. Text pre-
processing and text segmentation for ocr. Interna-
tional Journal of Computer Science Engineering and
Technology, 2(1):810-812.

Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi,
Tao Yu, and Rui Zhang. 2021. Logic-consistency
text generation from semantic parses. arXiv preprint
arXiv:2108.00577.

Ray Smith. 2007. An overview of the tesseract ocr en-
gine. In Ninth international conference on document
analysis and recognition (ICDAR 2007), volume 2,
pages 629-633. IEEE.

Kihyuk Sohn. 2016. Improved deep metric learning
with multi-class n-pair loss objective. Advances in
neural information processing systems, 29.

Andreas Spruck, Maximiliane Hawesch, Anatol Maier,
Christian Riess, Jiirgen Seiler, and André Kau. 2021.
3d rendering framework for data augmentation in
optical character recognition. In 2021 International
Symposium on Signals, Circuits and Systems (ISSCS),
pages 1-4. IEEE.

Ralf C Staudemeyer and Eric Rothstein Morris. 2019.
Understanding Istm—a tutorial into long short-term
memory recurrent neural networks. arXiv preprint
arXiv: 1909.09586.

Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.
Contrastive multiview coding. In Computer Vision—
ECCV 2020: 16th European Conference, Glasgow,
UK, August 23-28, 2020, Proceedings, Part XI 16,
pages 776-794. Springer.

Martin Tomaschek. 2018. Evaluation of off-the-shelf
ocr technologies. Bachelor thesis Masaryk Univer-
sity, Brno, Czech Republic.

Ciprian Tomoiaga, Paul Feng, Mathieu Salzmann, and
Patrick Jayet. 2019. Field typing for improved recog-
nition on heterogeneous handwritten forms. In 20/9
International Conference on Document Analysis and
Recognition (ICDAR), pages 487-493. IEEE.


UNESCO. 2021. Unesco project: At-
las of the world’s languages in danger.
https://unesdoc.unesco.org/ark:/48223/pf0000192416.

Wouter Van Gansbeke, Simon Vandenhende, Stamatios
Georgoulis, Marc Proesmans, and Luc Van Gool.
2020. Scan: Learning to classify images without
labels. In European conference on computer vision,
pages 268-285. Springer.

Kilian Q Weinberger and Lawrence K Saul. 2009. Dis-
tance metric learning for large margin nearest neigh-
bor classification. Journal of machine learning re-
search, 10(2).

Christoph Wick, Christian Reul, and Frank Puppe. 2018.
Calamari-a high-performance tensorflow-based deep

learning package for optical character recognition.
arXiv preprint arXiv: 1807.02004.

Kai Wu, Dingjiang Yan, Hongcheng Liao, Xiang Zhang,
Qilin Huang, Qian Zhang, and Min Fu. 2021. Appli-
cation of data augmentation of rare words based on
font cyclegan in character recognition. In 2021 IEEE
International Conference on Emergency Science and
Information Technology (ICESIT), pages 477-479.
IEEE.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Euro-
pean conference on computer vision, pages 818-833.
Springer.

Rui Zhang, Yangfeng Ji, Yue Zhang, and Rebecca J
Passonneau. 2022. Contrastive data and learning
for natural language processing. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies: Tutorial Abstracts,
pages 39-47.

Acknowledgements

We would like to thank Shikha Magotra for com-
piling the digital database of Takri script from mul-
tiple museums in north-western Indian geography
and making it available for our research.


Appendix A: Glyph feature store

Feature Feature Description Type Range

ID

Fl Presence of Headline: Checks for the presence of a horizontal line | Boolean) 0/1
on the top of the sub-symbol.

F2 Number of Loops: Counts the number of loops in the symbol in- | Number | 0-N
cluding loops with headline.

F3 Number of Loops with headline: Counts the number of loops the | Number | 0-N
symbol makes with the headline (F1).

F4 Presence of left-sidebar: Check for the presence of a vertical line | Boolean) 0/1
on the left-most side of sub-symbol.

F5 Presence of right-sidebar: Check for the presence of a vertical line | Boolean) 0/1
on the right-most side of sub-symbol.

F6 Number of connected components: Counts the number of sub- | Number} 0-N
symbols which are connected.

F7 Number of endpoints: Counts the number of points, which have | Number| 0-N
only one black pixel in its 3x3 neighbourhood.

F8 Number of junctions: Counts the number of points, which have | Number| 0-N
more than two black pixel in its 3x3 neighbourhood.

F9 Number of junctions with headline: Counts the number of junctions | Number| 0-N
(F8) which touch the headline (F1).

F10 Number of bend points clockwise: Counts the number of points | Number| 0-N
which makes 90 degree turn towards right direction.

Fl1l Number of bend points anti-clockwise: Counts the number of | Number} 0-N
points which makes 90 degree turn towards left direction.

F12 Aspect Ratio: Ratio of symbol height and width on 0-100 scale. Number | 0-100

F13 Horizontal Symmetry: Flip the image on Y axis (mirror the image | Boolean} 0/1
in left-right perspective). Find the similarity with original image
using threshold value.

F14 Vertical Symmetry: Flip the image on X axis (mirror the image in | Boolean) 0/1
top-down perspective). Find the similarity with original image using
threshold value.

F15 Number of Dots: Counts the number of points, which have zero | Number| 0-N
black pixels in its 3x3 neighbourhood.

F16 Number of left-right layers: Counts the number of layers of black | Number} 0-N
pixels on x axis. This relates to the maximum isolated black pixels
in horizontal cross section.

F17 Number of top-down layers: Counts the number of layers of black | Number} 0-N
pixels on y axis. This relates to the maximum isolated black pixels
in vertical cross section.

F18 Minimum horizontal projection: Compute the horizontal projection | Number} 0-100
(count the number of black pixels across y axis for every x axis) and
find the minimum value. Scale this by taking ratio with height of
image, and multiple by 100,

F19 Minimum vertical projection: Compute the vertical projection | Number} 0-100
(count the number of black pixels across x axis for every y axis) and
find the minimum value. Scale this by taking ratio with width of
image, and multiple by 100,



F20

Maximum horizontal projection: Compute the horizontal projec-
tion (count the number of black pixels across y axis for every x axis)
and find the maximum value. Scale this by taking ratio with height
of image, and multiple by 100,

Number

0-100

F21

Maximum vertical projection: Compute the vertical projection
(count the number of black pixels across x axis for every y axis) and
find the maximum value. Scale this by taking ratio with width of
image, and multiple by 100,

Number

0-100

F22

Maximum left depth: The maximum depth of the left profile calcu-
lated as a percentage with respect to total width of the box enclosing
the symbol.

Number

0-100

F23

Maximum right depth: The maximum depth of the right profile
calculated as a percentage with respect to total width of the box
enclosing the symbol.

Number

0-100

F24

Maximum top depth: The maximum depth of the top profile calcu-
lated as a percentage with respect to total width of the box enclosing
the symbol.

Number

0-100

F25

Maximum bottom depth: The maximum depth of the bottom profile
calculated as a percentage with respect to total width of the box
enclosing the symbol.

Number

0-100

F26

Minimum left depth: The minimum depth of the left profile calcu-
lated as a percentage with respect to total width of the box enclosing
the symbol.

Number

0-100

F27

Minimum right depth: The minimum depth of the right profile
calculated as a percentage with respect to total width of the box
enclosing the symbol.

Number

0-100

F28

Minimum top depth: The minimum depth of the top profile calcu-
lated as a percentage with respect to total width of the box enclosing
the symbol.

Number

0-100

F29

Minimum bottom depth: The minimum depth of the bottom profile
calculated as a percentage with respect to total width of the box
enclosing the symbol.

Number

0-100

F30

Stroke length: Count of total black pixels as a percentage with total
area (height * width) of the symbol box.

Number

0-100

F31

Epicenter top down: Find the mean of all black pixels, and compute
the location of Y axis from center as a percentage from mid point on
y axis .

Number

0-100

F32

Epicenter left right: Find the mean of all black pixels, and compute
the location of X axis from center as a percentage from mid point on
X axis .

Number

0-100

Recommended Feature set for languages:

¢ Takri - Fl, F2, F5, F7, F8, F12, F13, F14, F15.

¢ Modi - Fl, F2, F4, F5, F7, F9, F12, F15, F16, F23, F30.
¢ OI Chiki - F2, F4, F8, F12, F16.

¢ Gujarati - Fl, F2, F4, F5, F7, F12, P13, F15, F16.

¢ Wancho - F2, F5, F8, F12, F13, F15, F16, F21.



Appendix B: Post processing rule inventory

In our work for Takri OCR, we have applied rules from R1 to R7. We could not apply R8, R9, R10 due to
non availability of dictionary for Takri.

Rule ID Rule description Remarks

R1 One consonant should not contain more than
one vowel modifier.

R2 Sentence delimiter should not happen in be-
tween of a word.

R3 Sentence delimiters should not repeat consecu-
tively.

R4 Numbers should not merge with consonants or
vowels within the word boundary.

RS Independent vowels symbols (excluding the | Generic rule for Indic scripts. How-
modifiers) can occur only at the start of end | ever some fine tuning may be
of the word in most Indic scripts. needed from script to script basis.

R6 Panchamkshar Characters (like #4 ("Nya") and

3° ("Nga") in case of Takri) should not occur
in the end of the word.

R7 Unicode sequence of symbols identified may
not follow the same sequence as required by
computing systems, so sequence of unicodes
may need to be changed. For example (a3
("Fira") identifies vowel "i" before consonant
"f" and sequence needs to be changed.

R8 Shape characteristics of some symbols (glyph
features) may follow Whole-Part design. Ag-
gregation of some symbols (parts) may together
form another symbol (whole). For example in
Takri the "parts"- 3 ("Ra") and | ("Viram")
can form "whole" - 7 ("Ga").

RO Shape characteristics of some symbols may be | Validation with prebuilt vocabu-
very similar and create mis-classification. For | lary/dictionary to generate/shortlist
example 3° ("Nga") and 3 ("Number 3") are | candidate word.

very similar

R10 It is possible that in case of short word partitions,
the boundaries of word are not marked correctly.
In case the word spans across two lines (with
not sufficient space at the end of the line) word
partition error can happen as well.



Appendix C: Languages using Takri as a script and their relationships

Himachali Dialects
GroupA Group B Group C
. a?
Ka | Group B1 Group B2 La
nr ae — A
G |S eae ee L bo BR
Min* Su ~ Ma" 65 | KGa a | Sr
ky Ba Ha

Dialects with inflections are represented with superscript (’). (Baagli (Ba), Bhagati (Bg), Bharmauri (Bh),
Chambiali (Ch), Hamirpuri (Hr), Hatti (Ha), Kalhuri (Kl), Kangri (Ka), Kinnauri (Kn), Kulluvi (Ku),
Kyunthali (Ky), Lahauli (La), Mahasuvi (Ma), Mandiali (Mn), Pangwali (Pg), Punjabi (Pb), Sirmauri (Sr),
Suketi (Su))

Appendix D: Transliteration Sample to Takri using custm made glyhs.

Roman

Most dialects in himachal used
to have takri as its script. In the
current times there are only
handful of people who know
this. Some land records in kullu
and mandi district are in takri
and it is hence imperative to
preserve this script. We should
use computational linguistics to
enable digitization,
transliteration and enabling
software to make this easy for
people to use.

fearae Fr siferenirer atferat Ar
fore creel est HLT at

aaa Ga F Petar atat SF
tat sa aa H Seta 81 Heel

BNC 561 HOI LIT 86) ([e5fa
C365] 37 3603) A) | 23d BNT HA (A
LAST 2) 35.6303 8 3733 | WOO
Fad TiS) HO Fl BGR BN TSH 361 V1

3 hd Gale 63 HU % SUYGTB 3"
HARI 31 WBACORRA (MHutpea
FY 3 AZAD 3 BY BOA 26 [Se
BQUTT TAG SAUAIAT 96 GUTAT do"

(32 3186 Hal 36 G36 GUT 3004 HSA
3 386 |



Appendix E: Use of supervised contrastive learning for character identification

Labelled Alphabet

Y

Classifier Model

Supervised Contrastive Loss(L)

Minimize Maximize

fas = © (C9) le = © (C) I = EtG)

Encoder Model 2048-D(0)

Tl T2

ea
Oo

G1 G2

Ee

el aale
: BIBI:
EE

BREE
cxf] oo] 3

image GAN images C, - Other characters
transformations for C, generated apart from CG EC &j
for C; selected image

| Image Builder (Transformations and GAN based) |

" f
Pick an image from a

image store (Cj)

Raw image store ( Character Set C = {cj, C2, ...Cn} |

We use Google colab for running our experiment. For GAN images we train for 150 epochs, 4 layer
generator and network, adam optimiser and learning rate of 0.0002.


Appendix F: Extraction of source raw data into lines, words, characters and symbols.

(A)

8 nit © We
1H] SH wD Gos MAK yrs wai) ¢

3 Wi sad] fin G3 ae] a) Hi aad]

Gan fat wi yaa GrilaG wed ai] 3

75 38] Gn xd] wha A mua on

ant we od 73 mudd) fin FT

wa) w fase Gn F mar Gd oa

5 Gi) wind un ys J Gah fad

fs 7 Gi tw fmin dal) Fox

93 Hy we I Mi] OK wx TI

9 wait 3] G4 wed ww wait wed Gan

wef zi ai do the wh ZB ow

frgl 73 rDUG x anu 5G wi] art

00z

00r

009

8 at © we

WW) Sh o Gs we H uns ww]

of 303] feo Gave] a] vias] oe
Tan [ae 7 IB GHIAC wd 23] 3
Ws] GH WwW ueadi aU an

ae we OU Mi mus PM h rw

wae! ow Tas Gn t wan Gd a8

5 Gian syn ye 7 Gow Tw

fet Gmtwfmmovini woz
23 ey el wa map GH wed
2 it 3] G¥ eed w ini wd Gar

efi mai id he ws wo _

Pig v3 mmUG wh soup SC vel] oot

YH ws 5 a J) [LS 5 Gre 9]

TH) BS a Gee Kurt wal]
Page_04 Line O.png
5 G9) win jun v8 4 Gare fad

Page_04_Line 6.png

than feos D5 wma Gri}EG wo aiid] 3
Page 04 Line 2.png

Gi Bw i had J] [US 3 Costs 3]
aguG ari Grid] Bad) J) aad far] w

Bui Gr 2 rie) fis] i vowed UG om
wed] Ted Pid] a3. aime fact fos
hm fad GU Gan wt Gnd
Gries’) [vs Vi ii wed wali [raat HH 2686

“Gye WP a fw ss Goes F
_ 2306 cart Gri] aT] Sage
gui Gr w rie) [ui] wi ores UC ov
_2o)_ aed [Wi] %a Vi ned fact foo
hm i Gin i we Gnd
— Gress] [os vi Hi werd oa [acd Hi 2606

008

95 my vod WS» nf] OM vod oe
Page_04_Line_8.png
08 Gn wa rh) fui) 28 ore! UG 0
Page 04 Line_14,png

oot

wy 6 ug? is ad Aran uG 51

2

40 i ; i t _
| aiai. | tl
Fs fad Gi! ! Hilt Hy E Fs = =
i qe Hy aha a it hagl qi . 5 rr
5 rid | inte { 4 NA
5 ‘i nig li ta aii tj li Hi RawImage —_Skeletonization allies: Upper Middle Bottom
of —1 itis ti! i} {fil tie Hea 1 Labelling Zone Classification

0 100 200 300 400 500 00 700

(Cc) (D)

e A - Segmentation of page into lines.

¢ B - Segmentation of line into words.

¢ C- Segmentation of word into characters.

¢ D - Segmentation of character into symbols.


Appendix G: Baseline studies.

TABLE A -
Script Name Annotated Symbols
Devanagiri 121K
Gurumukhi 111K
Gujarati 121K
Oriya 115K
Bangali 122K
Tamil 116K

The annotated dataset is taken from (Jawahar et al., 2010) which is the largest corpus of annotated
Indic scripts. We have limited the size of the corpus to approx (110-120)K symbols to have all scripts in
similar size. We used this initial dataset to build a base model for that script and later fine tune it for Takri.

TABLE B -
Script Name SOTA Accuracy on base script Accuracy on Takri with 100K
labelled dataset

Devanagiri 97.9% (Chaudhuri, 2010) 86%

Gurumukhi 96% (Lehal, 2010) 86%

Gujarati 96% (Dholakia et al., 2010) 87%

Oriya 96% (Chaudhuri et al., 2002) 87%

Bangali 97% (Chaudhuri, 2010) 85%

Tamil (94-97)% (Kokku and Chakravarthy, 2010) 718%

We used our baseline models created using annotated symbol (from Table A) and used 100K annotated
Takri tokens to fine tune for our purpose. SOTA accuracy (in second column) is for base script (as
mentioned in column one) and Accuracy on Takri (in last column) is for Takri script.
