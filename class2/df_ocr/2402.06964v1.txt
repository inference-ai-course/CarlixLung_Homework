arX1v:2402.06964v1 [cs.CL] 10 Feb 2024

NATURAL LANGUAGE PROCESSING FOR KNOWLEDGE
DISCOVERY AND INFORMATION EXTRACTION FROM
ENERGETICS CORPORA

A PREPRINT

© Francis G. VanGessel*, Efrem Perry, Salil Mohan, © Oliver M. Barham, Mark Cavolowsky
Naval Surface Warfare Center
Indian Head Division
Indian Head, MD 20640

February 13, 2024

ABSTRACT

We present a demonstration of the utility of Natural Language Processing (NLP) for aiding research
into energetic materials and associated systems. The NLP method enables machine understanding of
textual data, offering an automated route to knowledge discovery and information extraction from
energetics text. We apply three established unsupervised NLP models: Latent Dirichlet Allocation,
Word2Vec, and the Transformer to a large curated dataset of energetics-related scientific articles.
We demonstrate that each NLP algorithm is capable of identifying energetic topics and concepts,
generating a language model which aligns with Subject Matter Expert knowledge. Furthermore, we
present a document classification pipeline for energetics text. Our classification pipeline achieves
59-76% accuracy depending on the NLP model used, with the highest performing Transformer model
rivaling inter-annotator agreement metrics. The NLP approaches studied in this work can identify
concepts germane to energetics and therefore hold promise as a tool for accelerating energetics
research efforts and energetics material development.

Keywords Energetics - Detonation Science - NLP - Knowledge Discovery - Large Language Models

1 Introduction

The study of energetics necessarily involves numerous scientific domains, spanning shock physics and detonation
science, fluid dynamics, material science, thermodynamics, and chemical synthesis. The plethora of sub-disciplines
of math, physics, chemistry, and engineering pose a challenge to practitioners who would wish to amass an expertise
of energetics. Furthermore, maintaining awareness of advancements in energetics research is complicated by the
exponential rate at which new research is published across scientific disciplines, including energetics. Thus, the
development of automated and intelligent approaches for extracting knowledge from papers, reports, textbooks, and
patents related to energetics could aid researchers and accelerate progress in energetics science.

Natural Language Processing (NLP) is a sub-field of linguistics, computer science, and Machine Learning (ML)
involving the interactions between computers and human (natural) languages. NLP techniques are used to analyze
and generate human language, allowing computers to read, interpret, and understand text and speech. In the context
of energetics research, NLP can be used to analyze large volumes of textual data, such as scientific papers, technical
reports, and patents, in order to extract relevant information about the concepts that underlie and explain energetics
phenomenon. Furthermore, NLP can enable natural language understanding that could be further applied to text
mining journal articles and performing numerous natural language tasks such as classification, summarization, and
recommendation. Overall, the use of NLP in energetics research has the potential to enhance our understanding of
energetic materials and phenomenon, and assist in the development novel propellants, explosives, and pyrotechnics.

“Corresponding author (francis.g.vangessel.civ@us.navy.mil)

Article has been accepted for publication in Propellants, Explosives, and Pyrotechnics (DOI: 10.1002/prep.202300109))
DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited.


Motivation for investigating the application of NLP to energetics research stems from two principle sources. First, there
exist a modest, but growing, promising set of ML-based non-NLP studies of energetics. These studies have primarily
focused on prediction of detonation properties (e.g. detonation velocity and pressure) and energetic molecule discovery
[1, 2, 3]. Despite the promising results of ML research efforts, there exists only a limited number of NLP-based studies
of energetics [4, 5]. Second, we are motivated by the success observed in adjacent scientific fields in applying NLP
techniques to domain-specific corpora. Examples include the use of NLP to assist in answering health related questions
in the biomedical domain [6] and the study of glass materials and novel thermoelectrics in the material science domain
[7, 8]. We hypothesize that NLP approaches could provide similar benefit to research and development in the energetics
domain.

In this work we asses NLP models on their capability for extracting knowledge from, as well as classify, energetics
textual data. Namely, we assess and interpret three unsupervised NLP models on their ability to synthesize information
related to the study of energetics: Latent Dirichlet Allocation (LDA), Word2Vec (W2V), and Transformers. We
subsequently evaluate the downstream performance of these models as a knowledge discovery tool for classifying
abstracts into energetic subdomains. We demonstrate that each NLP model is capable of extracting critical energetics
concepts and classifying energetic documents, and are therefore a useful information extraction tool.

The organizational flow of this paper proceeds as follows. Section 2 reviews the related literature, section 3 provides an
overview of relevant NLP models, section 4 describes the datasets, training methodologies, and evaluation procedures
used in this work, section 5 presents and analyzes the results, and section 6 concludes and summarize the outcomes of
this research.

2 Literature Review

In this section we provide a brief, non-exhaustive, overview of prior applications of ML and NLP to energetics, as
well as briefly survey applications of NLP to adjacent scientific domains. It is established that training traditional ML
models typically requires datasets containing hundreds to thousands of samples, while Deep Learning (DL) models
have more extensive data requirements [9]. Requirements for ample datasets may pose a challenge when applying ML
and DL methods to energetics where, because of safety and security concerns, datasets are typically small or difficult to
obtain. Despite these challenges, there are a burgeoning number of studies applying ML techniques to prediction of
energetic properties. Initial studies explored numerous feature representation strategies and data fusion approaches to
overcome data scarcity challenges and enable training ML regression models on small datasets of 100-400 energetic
molecules [1, 2, 10, 11]. These models achieved accurate prediction of energetic properties such as density, detonation
velocity, and detonation pressure. More recently, complex DL models have been trained on thousands of molecules to
predict properties such as density, detonation pressure, detonation velocity, and impact sensitivity [12, 13, 14]. These
DL models achieve high accuracy predictions, in part, due to the inclusion of energetic-like molecules that exhibit
similar atomic composition and bonding motifs to known energetics. In addition to property prediction models, recent
efforts have focused on the development of generative ML techniques to predict novel energetic molecular structures
[3, 15]. Generative models learn a latent representation for the probability distribution of energetic molecules, and
sampling from this distribution then yields new molecules with similar, but distinct, structure to known high-explosive
molecules. ML models have also been applied to heterogeneous energetic materials, incorporating morphological
information related to cracks, voids, and pores to enable the prediction of properties such as energy localization and
ignition [16, 17].

Despite the varied and growing number of ML applications in the energetics domain, to the authors’ knowledge there
exist only two studies that apply NLP techniques to energetics. Elton et al. utilized an NLP algorithm to transform
words appearing in energetics literature into an embedding space and demonstrated that these word embeddings could
capture chemical and application relationships amongst explosive formulation material ingredients [4]. Puerto et al.
developed an NLP pipeline for classifying energetic documents, utilizing three NLP algorithms trained on a small
dataset of energetic article abstracts. This work, while related to previous efforts, is broader in scope. In contrast to
Elton et al. [4], we do not restrict our study of energetics solely to explosive formulation ingredients and application
areas, but rather study a wider array of energetic sub-disciplines while simultaneously using more expressive NLP
models. Furthermore, we build on the research efforts of Puerto et al. [5] by utilizing an order of magnitude larger
text corpus for training our LDA and W2V models, explore fine-tuning of Transformer models, and perform extensive
Subject Matter Expert (SME) assessment of the energetics information extracted by various NLP algorithms.

While there exists a paucity of NLP applications to energetics specifically, NLP has shown to be a promising research
tool in adjacent domains. In their review of NLP applications to materials research, Olivetti et al. identified a common
set of approaches to NLP utilization including content acquisition, text preprocessing, document segmentation, entity
recognition, and entity relation and linking [18]. Recently, NLP has been shown to provide a viable route to materials

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 2


discovery with targeted properties. Namely, Tshitoyan et al. performed joint word embeddings of thermoelectric
and, broadly, solid-state materials; subsequent exploration of the embedding space identified candidate materials with
promising thermoelectric performance [8]. A notable example of NLP applied to the scientific domain is SciBERT, a
Transformer NLP model fine-tuned on a corpus of computer science and biomedical domain papers [6]. The authors
demonstrate that this language model outperformed its domain-agnostic counterpart in tasks such as information
extraction from clinical documents and identifying relationships between entities in computer science research articles.
In a similar effort, Gupta et al. fine-tuned a Transformer-based language model on a corpus of material science
publications, this model was then applied to named entity recognition, relation classification, and abstract classification
in the material science research field [19]. Similar fine-tuning and entity recognition tasks were explored by Shetty
et al., who applied Transformer-based models to construct polymer datasets through extraction of polymer types and
associated properties such as tensile strength and molecular weight [20]. Guo et al. used Transformers to identify
products and entity roles (e.g. reaction type, reactants, temperature, and yield) to create an automated reaction schema
extraction pipeline [21]. Galactica is a large language model trained on a large and broad scientific corpus including
full text articles from arXiv, Scientific Scholar, and ChemRxiv, as well as other text modalities such as computer
code, citations, and chemical and biological sequences [22]. Galactica exhibited natural language reasoning abilities
across scientific domains such as math, physics, and chemistry entrance exam question answering, chemical reaction
prediction, and citation generation. The success observed in NLP applications across numerous scientific disciplines
supports the need to develop natural language capabilities for energetics to aid in the acceleration of energetics research.
ChatGPT [23], a domain agnostic language model, has been shown to be capable of interpreting, and in some cases
accurately answering, chemistry questions posed at the undergraduate level. Similarly, ChatGPT has been used to
generate software programs for a wide-array of scientific numerical algorithms, albeit with varying levels of success
[24].

In addition to the knowledge extraction approaches detailed above, language models have found widespread utility
for chemical modeling and molecule generation. A number of researchers have applied the sequence modeling and
unsupervised pretraining capabilities of Large Language Models (LLMs) to text-based molecular representations,
including masked component modeling [25, 26, 27], autoregressive modeling [28], and autoencoder modeling [29],
among others. Similar to masked language models, masked component models treat hidden elements, such as atoms,
bonds, or groups, of the molecular representation and ask the model to predict the hidden value. Wang et al. developed
an early approach applying masking to predicting masked components of SMILES strings in their SMILES-BERT
language model. Chithrananda et al. introduced ChemBERTa [25], a Transformer-based molecular property prediction
model based on the ROBERTa architecture [30]. They used it to explore the effects of various model hyperparameters
and found a significant improvement with increasing pre-training dataset sizes; while the custom chemistry-focused
tokenization strategy, SmilesTokenizer [31], only offered a mild improvement over Byte-Pair Encoders; and that
there was no significant difference between SMILES and SELFIES, the two popular molecular string representations.
More recently, Ross et al. developed MolFormer which applied linear attention and relative position embeddings to
develop efficient latent molecular representations from SMILES inputs. This model was evaluated and demonstrated
good performance on property prediction, molecular similarity, and attention visualization. In contrast to the masked
component modeling, Bagal et al. adapted the autoregressive Generative Pre-trained Transformer (GPT) model [32] to
predict the next token in a SMILES string, and demonstrated good performance on benchmarks and an ability to generate
molecules with desired user parameters [28]. Honda et al. developed SMILES-Transformer, which used autoencoder
based pretraining that minimized the cross-entropy reconstruction loss, and demonstrated good performance on a wide
variety of datasets and tasks across physical chemistry, biophysics, and physiology.

3 ML Preliminiaries & NLP Model Overview

3.1 ML Preliminaries

In this section we briefly review fundamental techniques and concepts of ML and NLP that are critical to our
methodology and results.

3.1.1 ML

ML is a sub-field of both computer science and statistics that seeks to develop and understand algorithms that improve
their performance with data. While these algorithms have been around for decades, they have recently risen in
prominence due to high-profile breakthroughs [33, 34, 35], primarily driven by the rapid increases in computing power
and the data-availability provided by the internet. Ultimately, the goal of ML is to take a finite sample of a data
distribution and create a function that can make generalized predictions about the full distribution. Formally, given input
data X C 4%, output data Y C Y, and loss function £, a ML algorithm attempts to find a function f : * — Y such that

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 3


f = argmin £(f|X,Y) (1)
fEH

where H is the set of hypothesis functions that we are optimizing and X and Y are our datasets. The identification of
this function f is the training process and afterwards the function f would be deployed to perform inference on future
input data sampled from ~.

Unfortunately, many algorithms learn their sample data too well, and overfit to the training data at the expense of
out-of-sample, or generalization, performance. To combat this, it is common practice to hold out some data to evaluate
the model on unseen data to estimate the performance on future samples from the underlying distribution. To ensure
that the evaluation data doesn’t inadvertently impact the model training and bias the model evaluation, it is important to
have rigorous data handling to ensure no data leakage. To prevent data leakage, researchers must properly partition the
available data into three separate sets: training, validation, and testing. The training set is used to fit the model, the
validation set is used to adjust hyperparameters and prevent overfitting, and the testing set is used to evaluate the final
model performance. This partitioning approach eliminates data leakage, while still generating models that generalizes
well to new, unseen data. A common training-validation-test split would be 70/10/20%, with 70% of the available data
going toward training, 10% of the available data going to validation, and 20% of the available data going to testing.
Additionally, to understand the sensitivity of the model to the particular holdout sample, a k-fold cross validation
approach is frequently used to split the data into / training-validation-test permutations. In this method, the data is split
into k equal-sized subsets, and each subset will serve as the test set for a single trained model on the remaining k — 1
subsets of training data. The model’s performance is then estimated by calculating the mean and standard deviation
across all k training runs to give a more generalized understanding of the model’s performance.

This training/validation/test split is an example of one of many adjustable parameters that control the performance of a
ML model, also known as hyperparameters. Examples of hyperparameters include learning rate, regularization strength,
and the number of hidden layers in a neural network. While tuning hyperparameters is essential to optimize model
performance and prevent overfitting, it is difficult to predict a priori what the effect of a hyperparameter change will
have on the test performance of a model. To find the best hyperparemeters for a given application, a hyperparameter
tuning step is usually conducted which either uses a simple grid search or a more complex numerical optimization
technique to find an optimal configuration.

One critical—although often implicit-hyperparameter set is the priors, or the initial beliefs about the underlying structure
of the data or model parameters. Priors serve as a starting point for model estimation and can incorporate domain
knowledge or assumptions about the data generation process. By guiding the learning process, priors can help improve
model performance, particularly when the available data is limited or noisy. The appropriate choice of priors is critical,
as overly strong or biased priors may hinder model generalization and lead to suboptimal results. Practically speaking,
priors take the form of the type of model used, as well as the internal structure of the model. For example, the latent
dimensions of a model represent underlying variables in a model that can capture hidden patterns in the data. When
little prior information is known it is typical to utilize a flat, or uniform prior, assigning equal probability to all variables
or outcomes.

Another class of hyperparameters are the model optimization parameters. This can include the type of optimizer (e.g.,
Stochastic Gradient Descent (SGD), Adam [36], etc.), as well as the input parameters for that optimizer. For example,
selecting the correct number of iterations through the complete dataset— also known as epochs-is critical to ensuring
good model performance, as excessive training can cause overfitting and too little training can cause underfitting. The
learning rate, another important hyperparameter, dictates the step size during the optimization process in ML models. A
smaller learning rate might cause slower convergence but can potentially lead to superior solutions.

3.1.2 NLP

NLP is a branch of ML and Artificial Intelligence (AI) that focuses on automated and algorithmic processing of human
language. It aims to enable computers to understand, interpret, and generate human language in a way that is both
meaningful and contextually appropriate. NLP encompasses a wide range of tasks, such as sentiment analysis, machine
translation, text summarization, named entity recognition, and question-answering systems.

Early algorithms in NLP lacked the expressive power of modern DL-based solutions, and required significant text
pre-processing prior to evaluation by the learning algorithm. Traditional text pipelines include tokenization (segmenting
text into individual words, phrases, or other meaningful units, called tokens), stopword removal (eliminating common
words that do not contribute significant meaning, e.g., "and", "the", "in"), stemming (reducing words to their root form
by removing affixes, e.g., "running" to "run"), and lemmatization (converting words to their base or dictionary form, e.g.,
"better" to "good"). These techniques help reduce noise and improve the model’s ability to capture meaningful patterns.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 4


Modern approaches-—such as the Transformer family of language models—generally remove most pre-processing steps,
with the exception of tokenization. Pre-processing is typically considered unnecessary for these models, as the size of
the training datasets is deemed large enough to mitigate the effect of noise in the data.

In addition to pre-processing, many NLP algorithms use the process of featurization to convert raw data into a numerical
representation that can be used as input for ML models. For text data, common featurization techniques include
bag-of-words, term frequency-inverse document frequency (TF-IDF), and word embeddings. Proper featurization is
essential for improving model performance and interpretability.

Lastly, in DL-based NLP models, the context window defines the number of surrounding words considered when
analyzing a target word. A larger context window captures more contextual information, potentially improving model
performance, but may also increase computational complexity.

3.2. NLP Model Overview

In this section we provide a brief overview of three unsupervised NLP models used in this study: LDA; W2V; and
Transformers; and additionally the supervised classification algorithm Random Forest.

3.2.1 Latent Dirichlet Allocation

Developed by Blei et al., LDA is a generative probabilistic model that has been widely used in NLP and information
retrieval [37]. It is a popular technique for topic modeling, which is the process of discovering the underlying concepts
or topics that exist within a collection of documents. In LDA, each document is modeled as a probability distribution
over a fixed, pre-selected, number of topics, and each topic is modeled as a distribution over words present in the corpus
(Figure la). This allows the model to learn the underlying topics that exist within a collection of documents in an
unsupervised manner. Thus, once the number of topics have been specified, LDA automatically learns the words that
are most associated with each topic, as well as the topics most associated with each document without the need for
hand labeling. This is useful for tasks such as knowledge extraction and information retrieval, where it is often difficult
to know in advance what topics will be present in a collection of documents and/or hand labeling is prohibitively
time consuming. LDA has been applied to a wide range of NLP tasks, including document classification, information
retrieval, and machine translation. Overall, LDA is a powerful tool for discovering the underlying structure of a
collection of documents and extracting useful concepts and ideas contained within.

3.2.2 Word2Vec

W2V is a widely-used technique for learning dense vector representations of words, also known as word embeddings
[38]. These embeddings are useful for a variety of NLP tasks, including language modeling, machine translation, and
information retrieval. One of the key advantages of W2V is that it can capture semantic relationships between words
based on relative locations in the embedding space. For example, vectors for words such as "CFD," "hydrocode," and
"Eulerian" will be close to one another because they are all related to computational modeling techniques. This is in
contrast to traditional (sparse) techniques for representing words, which often rely on one-hot encodings that do not
capture any relationships between words. There are two main flavors of W2V: Continuous Bag of Words (CBOW) and
skip-gram. CBOW predicts a target word from the context of the surrounding words, while skip-gram predicts context
words given a target word (see Figure 1b for a schematic of the CROW W2V variant). Skip-gram tends to perform
better on small datasets, while CBOW is faster to train. Overall, W2V is a powerful and widely-used tool for learning
dense vector representations of words, which can be used directly for navigating energetic concepts and for a variety of
downstream NLP tasks.

3.2.3 Transformer

Transformer language models are a class of deep neural network architectures that have been widely used for NLP tasks
such as language translation, language modeling, and text generation. Introduced in the seminal paper "Attention is
All You Need" by Vaswani et al. in 2017 [39], they have since become the dominant approach for these tasks. One of
the key advantages of Transformer models is their ability to handle long-range dependencies in language. Traditional
recurrent neural networks, such as Long Short-Term Memory (LSTM) models, can struggle to capture long-range
dependencies because they process the input sequentially, one element at a time. Furthermore, W2V uses a fixed-width
context window and therefore cannot capture the long-range dependencies outside this window. In contrast, Transformer
models use self-attention mechanisms to allow each element in the input to attend to all other elements, enabling them
to capture long-range dependencies more effectively. Many variations of Transformer models have been proposed in the
literature, including models that are designed for specific tasks such as machine translation [40] and language modeling
[41], as well as models that are designed to be more efficient or to improve performance [42]. Among the most common

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 5


Transformer training objectives is the masked language prediction task. This approach involves randomly masking
sub-word units (formally called tokens) of the corpus text and training the Transformer model to predict each hidden
token (this process is in Figure 1c). Overall, Transformer language models are a powerful and widely-used approach for
a wide-array of NLP tasks, due to their flexibility and ability to handle long-range dependencies.

3.2.4 Random Forest

Random Forest is a supervised ML method commonly employed for classification and regression tasks in various
domains, including NLP. Introduced by Breiman [43], this method creates an ensemble of decision trees to improve
overall performance and mitigate the risk of overfitting. Each tree in the Random Forest is constructed independently
by using a random subset of the training data and features, which introduces diversity among the individual trees. When
making a prediction, each decision tree in the ensemble casts a vote, and the final decision is determined by a majority
vote for classification tasks or by averaging the predictions for regression tasks. This process helps to reduce variance
and improve the generalization ability of the model. In the context of NLP, Random Forest can be employed for tasks
such as sentiment analysis, text classification, and named entity recognition. Feature extraction techniques, such as
bag-of-words, TF-IDF, or word embeddings, can be used to convert text data into numerical representations suitable for
the Random Forest algorithm.

4 Methodology

The methodology of this study encompasses dataset curation and preprocessing, unsupervised model training and
evaluation, and supervised model training for abstract classification. Here we provide an in-depth description of each of
these components.

4.1 Data Preparation

4.1.1 Data Curation

Our team collected text data from a range of text-based sources in an effort to develop a dataset that provides coverage
of the numerous subdomains relevant to energetics. Curation of targeted energetic domain texts is challenging due to a
variety of factors including the lack of an established central repository for energetics text, online publisher security
mechanisms to prevent text mining, non-standard PDF formats, and government security restrictions. Therefore, while
a comprehensive final dataset has been created, it is likely non-exhaustive.

Our primary dataset consists of roughly 80,000 paragraphs and abstracts drawn from journal articles and technical
reports related to energetics research. Among the sources of this data is approximately 5,000 abstracts of journal articles
related to energetics, which were collated from the Journal of Energetics and the journal Propellants, Explosives, and
Pyrotechnics. An additional 40,000 paragraphs, extracted from roughly 1,000 journal articles, were obtained by a
custom webscraping tool developed by our team. This tool conducted a Google search using an energetics-related
search term. For each website in the search results, the tool identified and extracted any associated PDF file before using
additional filter terms to remove PDF files that were deemed irrelevant. We also included abstracts and paragraphs from
the entire full text of the International Detonation Symposium. Finally, in order to include fundamental knowledge we
scraped Wikipedia to include articles deemed relevant to energetics and explosives research.

Our primary dataset contained a mixture of file formats, including plain text, XML and HTML, and PDF. The NLP
software libraries used in this study require plain text data as input. Thus we parsed each file using a custom file
conversion pipeline that leveraged the GROBID [44], Sciencebeam [45], and BeautifulSoup [46] Python packages to
convert the documents into plain text files. Once our data was converted to plain text format we segmented all data into
individual paragraphs and abstracts for ingestion by our NLP model training pipelines. The final dataset was randomly
split into train and validation subsets for hyperparameter tuning, where the train and validation datasets contained 95%
and 5% of the total data respectively. Finally, the varying fidelity of document sources coupled with imperfections in
the text conversion pipeline (e.g. character recognition not accurately handling older documents) resulted in instances
of misspelled words in our final dataset (e.g. shook instead of shock). These errors effectively represent noise in our
dataset and are left as is.

A separate, hand-labeled, test dataset is also used in this study. This dataset is composed of 258 energetic materials
scientific abstracts along with a class label. The classes into which the abstracts were assigned are:

1. Characterization: Relating to the structural characteristics and/or performance properties of energetic materials

2. Modeling: Relating to computational methods used to simulate the behavior of energetic materials

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 6


5 Cl-20 is synthesized
Topic 1: through nitration of a

Synthesis caged structure
Reaction

Nitration
CL-20

HMX CL-20 has a higher : :
detonation velocity than UW formulations typically
5 contain HMX
Topic 2: HMX

Formulation
Mixture
Ingredients
Binder
Oxidizer

lololo

Replacing HMX with CL-20
in a UW formulation will
increase specific impulse

lolol

Topic 3:

Density
Det. Velocity
Ber peeeure Detonation velocity and Adding fuel and oxidizer ‘An UW formulation
Gurney Energy detonation pressure are deci sscee Det Velocity contains energetic,
lacs highly correlated : oxidizer, and metal
(a) (b)
%
SOLO of
CAVES MAY Vv
ESO NIEVES
CAS & by >
wy &Y
Word
embedding Pal

query }¥|||¥

key | "I" Attention

value |_"|_"| strong

Attention Network
weak
Attention
score

(Masked Word Prediction )

(c)

Figure 1: Graphical overview of the three unsupervised NLP models used in this study. The LDA topic model (a)
assumes that each document is a mixture of topics, and each topic is represented as a probability distribution over
words present in the corpus. In the example we have three topics related to molecule synthesis, energetic formulation
ingredients, and explosive performance. Each document is assigned one, or more, of these topics according to the
thematic elements of the document. The W2V word embedding model (b) seeks to build a predictive model for a center
word in a sequence given the surrounding context. In this example, information from every word within the context is
assimilated, via individual shallow neural networks, into prediction of the center word, PBXN-111. The Transformer
model (c) is trained in a self-supervised fashion to predict a masked word in a sequence. In this model, each word of
a sequence is transformed into a query, key, and value vector which are combined via the attention process into an
attention score. The masked word prediction component then uses the attention score of every word in the sequence to
predicts the masked word. In contrast to the W2V model, which equally weights each word within a finite width context
window, the Transformer considers all words within the sequence, attending more strongly to informative words (e.g.
explosive, HMX, and metal) while assigning less weight to uninformative words.

3. Processing: Relating to the modifications of energetic materials or molecules

4. Synthesis: Relating to the physical and chemical processes that produce energetic compounds
The classes, and label assignment, were performed by an energetic materials chemistry expert. For further details on the
dataset preparation procedure the reader is referred to [5]. This labeled dataset is used for the development of an NLP

document classification pipeline. Within this pipeline, each pretrained NLP algorithms is used to featurize the labeled
abstracts. We subsequently train a classifier to predict the abstract class label from the corresponding feature vector. The

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 7


ability to rapidly classify energetic abstracts holds promise for enabling information retrieval and knowledge extraction
from a large corpus.

4.1.2 Text Preprocessing

Each of our various NLP algorithms, LDA, W2V, and Transformer, require some degree of text preprocessing. We
utilized the same preprocessing strategy for LDA and W2V, this procedure consists of:

¢ Removing newline symbols and special characters

¢ Removing stopwords and numbers

¢ Replacing variations of a chemical name with a common term
¢ Lemmatizing variations of a word into a single item

¢ Converting text to lowercase

Transformers ingest raw text by segmenting words into sub-word units, referred to as tokens, and are therefore capable
of handling a more varied vocabulary than their algorithmic counterparts. Thus, we employ a minimalist approach to
preprocessing the Transformer text data that includes removing newline and special characters and converting text to
lowercase.

4.2 NLP Model Training, Assessment, and Interpretation

For each NLP model we provide a thorough description of the training process, the hyperparameter selection and
validation procedure, and protocol for assessing the models capability to extract fundamental energetics concepts. The
entire workflow is depicted in Figure 2.

4.2.1 LDA

We train all LDA models using the GENSIM Python library [47]. We perform a thorough hyperparameter study
including; varying the number of topics in the corpus, the prior distribution of topics within each document, and the
prior distribution of words within each topic. The number of topics controls the number of latent energetic concepts
present in the corpus, while the prior distributions represent the initial assumption of how topics are distributed amongst
document and how words distributed amongst topics. We assess the ability of each model to generalize to new
documents by measuring the average model perplexity on the held-out validation set of documents. Perplexity is a
commonly used metric for evaluating how well a language model predicts a sample. As an example, consider the task
of predicting the next word in the sample "their pet animal was a blank". A good language model, with low perplexity,
would assign higher probabilities to words like dog, cat, or fish than to nonsensical words such as car, forest, or rain.
For LDA, we define the perplexity for a model characterized by the weights and hyperparameters, 0, on the sequence of
words w = (Wj, We,..-, Wp) as:

1 n
PPL(w) = exp |—— ) log po (wil{wy}ici)] - (2)
i=1
Here n is the total length of the sequence and pg (w;|{w,};<;) is the conditional probability the model assigns to the
i'” word given the preceding words in the sequence. Thus we can view perplexity as quantifying how well our NLP
model has learned the distribution of words present in the corpus. The hyperparameters maximizing the perplexity are
given in appendix A.

The final LDA model is assessed on the degree to which it has absorbed concepts and ideas critical to energetics
research. Specifically, an energetics SME analyzes the ten highest probability words assigned to each topic. The the
expert determines whether the words assigned to each topic represent a semantically coherent set relating to a defined
energetic concept. To demonstrate the utility of topic modeling, a document, drawn for the corpus, is reviewed along
with the highest probability topics assigned to the document. The topics attributed by the LDA model are critiqued by
the expert as to whether they bear relevance to the themes present in the document. This expert assessment is critical
to ascertain how well the model predictions align with energetics subject matter expertise and establish trust in this
methodology for continued use.

4.2.2 W2V

The W2V models used in this study are trained using the GENSIM topic modeling Python library [47]. In the model
selection process we perform a thorough hyperparameter study varying; dimension of the latent embedding, width of the

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 8


context window, minimum threshold of times a word must appear, as well as consider both skipgram and CBOW model
variants. The embedding dimension controls the dimensionality of the dense latent representation of words in our corpus.
The context window size controls the number of neighboring words to include when making predictions regarding a
center word. By setting a minimum threshold on word occurrence we effectively remove words that appear infrequently.
Skipgram and CBOW refer to the two primary variants of the W2V model as described in Section 3.2.2. An intrinsic
measure of model accuracy is how well the language distribution learned by the model aligns with the distribution,
i.e. sequence of words, present in the text corpus. A metric quantifying such agreement is the log probability, for an
individual document containing the sequence of words w = (wi, We,..., Wn) the log probability of the W2V model,
characterized by the weights and hyperparameters 6, is given by the formula

log po(w = 4, [1<|k—J]<m] log pa(we|wy) - (3)
j=l k=1

Here n is the document sequence length, m is the context window, the indicator function 1 is one for words falling
within the context window and zero elsewhere, and pg is the learned probability distribution of the W2V model. We
select the hyperparameter combination that yields the highest median log-probability on the held out validation set of
documents, these hyperparameters are given in appendix A.

The final W2V model is analyzed by an energetic SME to assess the degree to which the learned embeddings cluster
semantically and conceptually related words. Words associated with a coherent energetic concept or idea should be
assigned proximate embeddings in the word vector space. To rigorously assess similarity, we calculate the 10 closest
embeddings for each word using the cosine similarity metric. The cosine similarity, a prevailing similarity metric in
NLP, between words w; and w; is

Wi Wj

(4)

[| wil||fwill
where ||w]| is the traditional L2 vector norm. Word lists of the ten closest embeddings for each word that are deemed to
align with a human expert understanding of energetics are indicative of a successful embedding approach.

4.2.3 Transformer

The Transformer models we apply in this study were trained using the Python-based Huggingface Transformer library
[48]. The Transformer library provides access to a wide variety of Transformer model variants and enables model
training on GPU architectures. Due the size and complexity of Transformer models, training a model from scratch on
available resources is infeasible due to data and computational requirements. Rather, we employ a fine-tuning based
approach where we leverage a pretrained model, trained on a generic (i.e. domain-agnostic) corpus, and refine its
weights on our domain-specific corpus. In this scenario the pretraining and fine-tuning objective were identical, for both
cases the model is optimized to perform masked language prediction as described in Section 3.2.3. The cross-entropy,
H, is acommonly used metric for multi-class classification, i.e. predicting a token from a discrete set of possibilities (i.e.
token vocabulary). Thus, minimizing the cross-entropy corresponds to ensuring the ground-truth token is assigned the
maximum probability given the surrounding context, and we therefore minimize the cross-entropy during the training
optimization process. The definition for cross-entropy is

_ iy Ealiie c)
H(x,y) = So In --¥ we log Sa nc. (5)
n=1

an exp(2n,i)

where Ync, En,c, and wy are the target label, unnormalized logit output, and class weight for training sample n, of N
total training samples, and token class c for token vocabulary of size C’. The final selected model is the one which
yields the maximum masked token prediction accuracy on the validation dataset. We note that the token prediction
accuracy metric is highly correlated, but distinct, from the cross-entropy metric. For the Transformer model only, we
avoid an exhaustive hyperparameter search due to the computational cost of model training. However, we did perform a
cursory investigation of the model hyperparameters including data concatenation strategies, learning rate parameters,
batch size, and number of training epochs. Our final models reflects the combination of hyperparameters which were
found to yield the highest prediction accuracy (the Transformer model hyperparameters are given in Appendix A).

The Transformer model’s ability to learn the language distribution associated with energetic text is assessed in two ways.
First, we contrast our fine-tuned model with other Transformer variants, including those trained on other domain-specific
texts. We assess each model’s ability to predict masked words within sequences drawn from the validation energetics

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 9


data set. Second, an energetics SME interprets the masked token predictions of our fine-tuned model as well as other
Transformer model variants. Namely, each model is presented with a sequence taken from the energetics literature,
where a token of energetics importance has been masked. If the Transformer has been properly aligned with the
energetics domain through the fine-tuning process, then the highest probability tokens produced by the model for each
masked word should reflect, to some degree, the subject matter knowledge of an energetics expert.

4.3 Abstract Classification

The final ML model considered in this work is a random forest model. For this model training procedure we use
the labeled dataset of 258 energetics-related abstracts previously described in Section 4.1. We avoid any prolonged
hyperparameter tuning for this task and train the random forest using the Scikit-learn library (the hyperparameters used
in this study are reported in Appendix A). A five-fold cross validation procedure, as described in 3.1, is used and we
report the average of this held-out classification accuracy score, as well as the standard deviation, across all 5 folds.

Due to fundamental differences in our final NLP models, each produces a unique feature vector for the documents in our
labeled dataset. We choose to featurize each document as follows. For the LDA model, each document is represented
by the topic distribution assigned by the model. For the W2V model, we average the word embeddings of each word
present in the document to obtain a single averaged embedding vector. Similarly, for the Transformer model we pass a
document through the Transformer and obtain the final layer of latent embeddings from the neural network. The final
layer latent embeddings are averaged across tokens to obtain an averaged token embedding vector. Therefore, each
model featurizes a document as a one dimensional vector of floating point numbers, where the dimensionality varies
from model to model.

Model Word Embeddi
Raw Energetics Training NLP Model Validation way on mm 6 whe
Text Dat Dataset Traini Dataset Hyperparameter Topic Distributions
ext bata atase raining atase Selection Attention Weights

Figure 2: Training and validation procedures for LDA, W2V, and Transformer models.

5 Results

We present a thorough examination of each NLP model with a focus on interpreting the model outputs in the context
of energetics knowledge. Furthermore, we present a comparative analysis of each NLP algorithm in their use as a
featurization method for performing document classification in the energetics domain.

5.1 LDA

The final, best performing, LDA model achieved a perplexity score -85.31 on the validation dataset (hyperparameters
are given in appendix A). This model, when applied to our corpus of energetic scientific texts, is found to be capable of
identifying semantically coherent topics which map to critical energetics concepts. However, there remains aspects of
the LDA model that are not immediately interpretable to an energetics expert, therefore providing areas for possible
model improvement. To support this observation, we present a subset of topics created by the model. These topics,
their SME-assigned theme, and associated ten highest probability words are presented in Table 1. We note that while
we choose to report the ten highest probability keywords consistently across topics, the distribution of probabilities
associated with these top ten keywords varies from topic to topic. In some cases, the first 3 keywords contain >90% of
the cumulative probability, while in other cases this keyword probability contribution is more evenly distributed.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 10


Topic No.

Topic Theme

Topic Keywords

4

Detonation Thermo-
dynamics

expansion, parameter, eos, adiabatic, empirical, lee, jwl, isentrope, exponential,
exponent

11 Detonable Molecules detonation, octahydro1357tetranitro1357tetrazocine, pentaerythritoltetranitrate,
unreliable, hydraulicallyactuated, methoxide, blrecrystallized, recrystallizations,
epc, dmdp

19 Military Roles american, army, service, navy, scientist, engineer, lithium, french, ordnance,
today

36 Scientific Analysis measured, calculated, comparison, homogeneous, examined, computed, classi-
cal, waveform, correlated, tomography

39 Formulation Engi- composition, hexahydro135trinitrol35triazine, role, composite, energetic,

neering binder, mixing, play, capturing, fundamentally

40 Microstructure Mor- mechanism, bulk, interface, prediction, modeling, porosity, impedance, inclu-

phology sion, computational, localized

42 SDT Modeling The- hot, formation, spot, mechanic, entropy, precisely, spatial, nonlinear, reproduce,

ory crest

49 Nuclear Weapon Phe- nuclear, weapon, fusion, neutron, reactor, plutonium, radioactive, demand,

nomenon debris, skin

53 Air Shock Phenom- wave, angle, supersonic, mach, reflected, sonic, subsonic, reproduced, overtakes,

ena coworkers

162 Propellant Deflagra- propellant, temperature, graphite, explosive, material, high, octahy-

tion dro1357tetranitro1357tetrazocine, hexahydro135trinitro135triazine, boron, pres-
sure

29 None one, individual, best, interval, shift, constituent, implies, rolling, overlap, shift-
ing

45 None series, strength, failed, transmitted, material, hypothesis, picture, tensile, whilst,

reveals

Table 1: Selected LDA Topics and associated ten highest probability keywords. The topic number is automatically
generated by the model training software and has no physical interpretation. The top ten topics are highly interpetable,
however the final two topics keywords lists (separated by the horizontal black line) lack coherency and cannot be readily
assigned to a specific energetic subdiscipline.

Examination of the first ten topics presented in Table | reveals a clear pattern of keyword grouping in a manner
immediately recognizable to those with energetics knowledge. We focus on three of these topics for further discussion.
First, the topic we have labeled as detonation thermodynamics, topic 4, contains a set of keywords related to the
Jones-Wilkins-Lee equation of state (JWL, parameter, eos, and exponential) as well as isentropic processes. The
JWL EOS is a ubiquitous equation of state used for describing the thermodynamics of detonation products as they
undergo isentropic expansion in the Taylor rarefaction region behind a detonation wave. This concept is critical to
understanding the governing phenomena of explosive performance capabilities of high explosive materials. Second,
we have identified topic 39 as describing the material ingredients and mechanical processes related to formulation
engineering. Explosive formulations typically contain a mixture of energetic materials, such as RDX (listed here as
hexahydro135trinitro135triazine as a result of text preprocessing), and binder. This composition results in a material
capable of releasing large amounts of energy over very short time scales with mechanical properties required for
incorporation in various defense systems. The third topic, labeled air shock phenomena, relates to a set of processes
linked to shock propagation resulting from an air blast. A near-ground detonation in the atmosphere produces a
shockwave which impinges on the ground. The shock wave reflected from the ground, a result of the impedance
mismatch between the air and ground material properties, subsequently overtakes the initial shock, forming a vertical
Mach stem. The dynamical evolution of the Mach stem, including formation, height, and strength, depends on the angle
at which the initial shock intersects with the ground. While many keyword lists can be readily assigned to a distinct
concept as we have highlighted above, we find that certain topics are less interpretable. The final two topics listed in

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 11


Table 1 lack coherence and do not align with an energetic concept. Thus the topics and associated keywords highlighted
in Table 1 demonstrate that LDA is well suited for extracting coherent topics that capture themes critical to energetics
science.

LDA also performs the task of identifying the mixture of topics present in documents. This capability holds promise for
rapid knowledge discovery by automatically grouping documents based on relevance to one another, or to distinct topics
of interest. Consider the journal article excerpt [49] from the Fourth International Detonation Symposium presented
with the corresponding two highest probability topics in Table 2. This document presents analysis and interpretation of
air shock, Mach reflection, critical angles. Correspondingly, the two highest probability topics identified by the LDA
model correctly capture the critical concepts related to analyses of air blast generated Mach reflection. In general, it is
observed that longer documents tend to have a larger number of topics assigned to them with probabilities greater than
2%, while shorter, abstract-length documents are typically only assigned one to two topics above the 2% threshold.
This pattern is intuitive as the longer the document, the more likely it is to relate to multiple energetic sub-disciplines.
While a 2% posterior probability for a topic may appear relatively low, it represents a six-fold increase over the uniform
topic distribution prior which assigned 0.33% probability to each topic. We propose that one could use LDA as a
knowledge discovery aid in the study of similar phenomenon by retrieving documents with similar topic distributions.
This approach generalizes across topics enabling rapid information retrieval of documents relevant to, for example,
energetic molecule synthesis, shock initiation modeling, or experimental characterization of explosive performance.
Therefore, the ability to extract energetics concepts present in scientific texts, form coherent topics, and organize
documents based on topical relevance, indicates LDA holds clear promise for energetics knowledge discovery.

Question: The sonic angle of critical reflection calculated with a "polytropic" equation of state is in better agreement
with experiment than the limiting angle of regular reflection calculated with the same equation. Can one conclude
that the critical angle is the sonic angle, rather than the limiting angle of regular reflection?

Answer: The sonic angles for both Composition B and nitromethane are calculated to be about | degree less than the
calculated critical angles for regular reflection and therefore are in better agreement with the experimental transition
angles to Mach reflection. However, in view of the rather large variation of the critical angle (as well as the sonic
angle) with the several equation-of-state representations, and without an independent means for choosing which
of these is most appropriate, it is impossible for us to infer to which of the three limiting angles the experimental
transition angle corresponds, i.e., to the critical angle for regular reflection, the sonic angle, or the critical angle for
Mach reflection.

53 Air Shock Phenomena wave, angle, supersonic, mach, reflected, sonic, subsonic, reproduced, overtakes,
coworkers
128 Fluid Analysis flow understanding unique evolution reflection simulate statistical dominated

classic dissipation

Table 2: A document extracted from the Fourth International Detonation Symposium in which the authors analyze
their air shock study [49]. Below the solid black line, the two highest probability topics attributed to this document are
shown with the corresponding LDA topic keywords.

5.2. W2V

The final, highest performing, W2V model achieved a median log probability on the hold out dataset of -230.74
(hyperparameters are given in appendix A). A thorough analysis of the model-generated word embeddings reveals that
words embedded in close proximity correspond to well defined energetic motifs. To demonstrate this we visualize the
word embeddings in Figure 3. the T-SNE nonlinear dimensionality reduction algorithm is used to project from the 300
dimension embedding space to a two dimensional visualization space. The T-SNE algorithm operates by attempting
to preserve relative distances between points when projecting from high dimensional space to the low dimensional
representation. The critical tuning hyperparameter of T-SNE is the so-called perplexity (this perplexity is distinct from
the NLP variant defined in Equation 2) which typically ranges from 5 to 50 [50], for this analysis we set perplexity to 5.

The word embeddings shown in Figure 3 illustrate that words closely related to the same topic are embedded in close
proximity. For example, the phrases "monopropellant" and "engine", labeled in green, appear close together in the left
portion of the word cloud, many rocket engines utilize monopropellant based propulsion systems. In the lower center of
the word cloud, labeled in blue, we see the points corresponding to the words "octahydro1357tetranitro1357tetrazocine"
and "hexahydro135trinitrol35triazine" nearly overlap. These are the full chemical names of the pure explosives
HMxX and RDX respectively, two energetics with very high detonation velocity and in widespread use in many high
explosive systems. Finally, in the bottom-right quadrant of the plot, labeled in red, we observe embeddings of the words

wow

"hugoniot", "conservation", and "isentrope". These phrases refer to thermomechanical processes in which mass and

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 12


100

e e e °
50 «
, underwater
monopropellant § f*,.° ae 4
. ° e oe 2
“ bipropellant » ¥ {3% 1a ale3d
Ke) hypergolic eo ote ee ef
n LT eo%re
c 4 ee tr’ our
a engine ° .* vy = ® hydrocode
Ee 0 “a ° 8 "0%
= ° sa oes a shock
oe ‘ Para He = rarefaction
n 7s ° ;
rae . * = hugoniot
eee. isentrope
~ rybusad “eK
Rolybute = Nee
e he. @ s e .
htpb » a. conservation
formulation owe oe ve .
: e ce explosion
synthesis ee
hexahydro135trinitro135triazine bond sound detonation
7 octahydro135/7tetranitro1357tetrazocine
-100 -50 50 100

T-SNE Dimension 1

Figure 3: Word embeddings generated by W2V model. the T-SNE algorithm had been used to reduce the dimensionality
to two dimensions to aid in visualization and analysis. A select number of embeddings groups have been annotated to
highlight clustering of energetic concepts. In the upper right corner, several words related to computational modeling
of shock physics and high impact events have been labeled in orange. Below the modeling cluster on the right-hand
side, several words related to thermodynamic processes associated with detonation dynamics have been labeled in red.
At the central bottom portion of the figure, molecular synthesis concepts and high-explosive molecule names have
been highlighted in blue. In the upper left corner, phrases related to propellants have been identified with green labels.
Finally, certain words with no apparent thematically coherent cluster have been labeled in black.

momentum are conserved (hugoniot) or entropy preserving processes (isentrope). The concepts of a hugoniot and
isentrope are critical to detonation theory.

While a word cloud provides a qualitative description of the W2V generated embeddings, the dimensionality reduction
can obfuscate the true nearest-neighbor embeddings of the higher dimensional space. To assess which words lie in
closest proximity to one another, we refer to Table 3. Here we have listed several words common within the field of
energetics and their corresponding 10 closest word embeddings, calculated using the cosine similarity metric (Equation
4). The cosine embeddings indeed reveal information loss induced by the T-SNE dimensionality reduction approach, for
example "htp" a kind of propellant is placed far from "monopropellant" by the T-SNE algorithm despite monopropellant
being the third most similar word. Further analysis of Table 3 reveals areas for possible model improvement as well as
interesting trends. We note that many base words are embedded close to variants with differing suffix endings (e.g.
bond and bonding; charge and charges; isentrope, isentopes, isentropic; etc.). Additionally, misspellings present in the
original data sources pollute the embeddings: for example words like "shook" and "hock" are most likely misspelling
of shock given their embedding proximity. Thus, we suspect a more thorough preprocessing of the text may further
improve the quality of the W2V generated embeddings.

Despite a few issues in the existing model, Table 3 clearly indicates that W2V has extracted and grouped words
related to critical energetic ideas. Take, for example, hydrocode which has been embedded close to names of specific
software programs developed for simulation of shock physics and detonation science including; cth, ale3d, dyna3d, cale,
dyna2d, and autodyn. In addition, pdv, short for photonic doppler velocimetry, is closely related, and embedded in an
adjacent manner, to numerous other experimental diagnostic approaches such as visar, interferometer, and orvis. Finally,

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 13


chemical bonding structures such as alkane and alkene lie close in embedding space to chemical groups commonly
present in energetic materials such as carbonyl, methyl, and butyl. Thus we observe a consistent trend in which the W2V
model embeds words according to their relationship to energetic subdisciplines such as thermodynamics, explosive
materials, and organic chemistry. This capability lends credence to the hypothesis that W2V, and NLP more generally,
could enable knowledge discovery and information extraction in the energetics domain.

shock preshock, shook, scw, shockwave, hock, steepening, overtook, ramp, precompression,
reinitiation

underwater undex, aftermath, freefield, fae, peaceful, operability, divergent, nonideal, underground,
overhaul

isentrope isentropes, adiabat, hugoniot, isentropic, tangency, adiabats, hnb, asymptote, hugoniots,
curve

hydrocode cth, hydrocodes, code, ale3d, multimaterial, cfd, dyna3d, cale, dyna2d, autodyn

conservation continuity, conserved, thermodynamics, schrodinger, rearranging, avrami, bernoulli,
governing, conserving, euler

octahydro- hexahydro135trinitro1 35triazine, nanocrystalline, rox, phlegmatized, pentaerythritolte-

1357tetranitro- tranitrate, highdensity, binder, cyclotol, coarse, iimx

1357-tetrazocine

propellant propellants, motors, fueled, rocket, prilled, bipropellant, ducted, cannonball, lova,
monopropellant

deflagration todetonation, sdt, ddt, burning, steadystate, propagation, retonation, buildup, deflagra-
tionntoodetonation, hvd

cfd ale3d, numerical, coyote, amr, multimaterial, hydrocode, dyna2d, cheq, atomistic,
fortran

polybutadiene ipdi, butadiene, ctpb, hydroxyl, nitrato, deha, terminated, pban, acrylonitrile, function-
alized

detonation todetonation, lvd, anz, underdriven, reinitiation, backwards, hvd, superdetonation,
deflagration, unsupported

synthesis hydrolysis, novel, itself, nitration, redox, materials, multistep, facile, particles, catalysed

titration styphnic, acetic, benzoic, stearic, potassiumsulfamate, citric, dinitramidic, humic,
chromic, colorimetric

cylex ipft, floret, skid, scoping, prism, susan, subscale, freefield, steven, brazilian

formulation pbxs, plasticbonded, composition, cmdb, ingredient, viton, daaf, htpb, lot, aluminized

characterization analysis, evaluation, investigation, study, characterize, studying, assessment, character-
isation, examination, fragility

initiation sdt, ignition, initiating, desensitisation, desensitization, ddt, coalescence, todetonation,
pinch, sensitization

bond bonding, atom, functionalization, covalent, nitrogen, orbitals, weakest, cleavage, in-
tramolecular, molecule

alkane diphenyl, ligand, alkene, moiety, dinitro, backbone, butyl, ether, carbonyl, substituted

charge charges, irregularly, centrally, torus, pellet, acceptor, column, egg, pear, csc

hbx tritonal, xtx, tnetb, btnen, pentolite, cyclotol, uncased, highdensity, tatbbased, pressed

booster donor, centrally, pentolite, pwb, charge, charges, ihe, acceptor, uncased, pellet

flyer buffer, projectile, impactor, cover, impactors, attenuator, backing, sphf, plate, driver

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited.

14


engine motor, ramjet, throttled, turbopump, ducted, powered, turbine, sounding, tripropellant,

falcon

monopropellant bipropellant, tripropellant, monopropellants, hypergolic, candy, sounding, throttled,
motors, ducted, multistage

pdv visar, velocimeter, ionisation, interferometer, efo, orvis, Idv, interferometry, piv, ve-
locimetry

Table 3: W2V model nearest neighbor embeddings calculated from cosine similarity metric.

5.3 Transformer

Next, we attend to the Transformer NLP model, assessing the degree to which fine-tuning the model weights can impart
information critical to the energetic domain. The training procedure, including the cross-entropy loss (calculated for
both the training and validation data sets) and the masked token prediction accuracy (calculated only on the validation
set), is visualized in fig 4. Note that a decrease in the model loss, namely the cross-entropy, is highly correlated with an
increase in the model’s masked token prediction accuracy. We take the model weights from the epoch which achieved
the highest masked token prediction accuracy on the validation set to be our final Transformer model, the maximum
accuracy is 65.8% which occurred at epoch 98.

2.4- -
—— Train Loss

— Validation Loss

0.64 -

Loss (Cross-Entropy)
Masked Token Pred. Acc.

7) 20 40 60 80 100 ) 20 40 60 80 100
Epochs Epochs

Figure 4: The cross entropy loss (calculated for training and validation data sets) and masked token prediction accuracy
(calculated on only the validation data set) plotted with respect to number of epochs.

To highlight the importance of fine-tuning a language model on a domain-specific corpus, in Table 4 we contrast our
energetics domain aligned model with several existing Transformer-based language models. Namely, we compare
against two broad categories of models. Two of these models are domain-agnostic and trained on a massive corpus, such
as the full text of English Wikipedia, these include the BERT model and its distilled variant distiIBERT [42, 51]. The
remaining two models have been fine-tuned on domain-specific corpora, namely biomedicine, SciBERT, [6] and material
science literature related to glass materials, MatSciBERT, [19]. It is apparent that the energetics fine-tuned model
achieves the highest prediction accuracy. Moreover, the other scientific domain aligned models perform significantly
worse in prediction accuracy, trailing even the domain-agnostic models. It would appear that despite similarities
between the energetics domain and the biomedical and material science domains, there are key distinctions. These
dissimilarities are made apparent by the fact that knowledge, introduced by fine-tuning a language model on a disparate
domain, does not immediately transfer, leading to poor predictive performance of that model on out-of-domain tasks.
This result highlights the criticality of fine-tuning a language model on a domain specific corpus when the ultimate goal
is to perform natural language tasks specific to that scientific area.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 15


Transformer Language Model Masked Token Prediction Accuracy (%)

Energetics Fine-tune 65.77
DistilBERT [42] 58.02
BERT [51] 61.81
SciBERT [6] 40.78
MatSciBERT [19] 40.83

Table 4: Comparison of Transformer-based language models on the masked token prediction task. The masked token
prediction accuracy reported here is calculated from the validation dataset

As a further comparison between domain-specific, domain-adjacent, and domain-agnostic models we investigate the
predictions made for energetics-related text by the energetics fine-tuned model, SciBERT, and distiIBERT. Namely,
we select several sentences and mask a word of energetics relevance. These masked sequences are then passed to
the three models, and each model predicts the masked word. Table 5 contains the results of this comparative study.
The first sentence describes the Gurney equation for predicting metal acceleration capabilities of an explosive, only
our fine-tuned energetics model correctly decodes the masked token. The second sentence refers to the stochiometric
qualities of an explosive molecule, and both our fine-tuned model and the distiIBERT model correctly identify that TNT
is oxygen deficient. Finally, for our final sentence, which describes molecular structure, the energetics fine-tuned model
and distilBERT are incorrect in their prediction while SciBERT correctly identifies that aromatics contain benzene rings.
Based off of these results, it is clear that while opportunities for model refinement remain, the fine-tuned energetics
Transformer model’s attention mechanism is capable of attending to concepts germane to explosive engineering and
detonation science.

Model Variant Masked Token Model Prediction

Energetics gurney the gurney method, which yields simple equations for evaluating the

Fine-tune velocity of metals driven by detonating explosives in many geometries,
is reviewed.

SciBERT gurney the analytical method, which yields simple equations for evaluating the

velocity of metals driven by detonating explosives in many geometries,
is reviewed.

DistiIBERT gurney the numerical method, which yields simple equations for evaluating the
velocity of metals driven by detonating explosives in many geometries,
is reviewed.

Energetics negative tnt has a negative oxygen balance.

Fine-tune

SciBERT negative tnt has a high oxygen balance.

DistiIBERT negative tnt has a negative oxygen balance.

Energetics aromatics molecules that contain a benzene ring are examples.
Fine-tune

SciBERT aromatics molecules that contain a benzene ring are aromatic.
DistiIBERT aromatics molecules that contain a benzene ring are excluded.

Table 5: Masked token prediction comparison of three Transformer models. Each Transformer model is presented with
the same sequence containing a masked token of energetic relevance. The resulting model prediction of that true token
value is highlighted in bold.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 16


5.4 Energetic Abstract Classification

In this final section we contrast each NLP algorithm considered thus far, namely LDA, W2V, and the Transformer,
in their capability to featurize hand-labeled abstracts pulled from the energetics literature. Namely, we use these
algorithms to obtain numerical representations of the text-based abstract, this numerical feature is then used to classify
each abstract into one of four distinct sub-areas of energetics via the random forest classification algorithm. Recall that
the ground-truth labels of this test dataset were generated by an energetics SME annotator. The dataset, embedding
technique, and classification methodology was previously described in sections 4.1 and 4.3. The accuracy of the abstract
classification pipeline as well as the standard deviation of the prediction is shown in Figure 5. The three leftmost entries
of the plot represent the NLP models trained in this study. The LDA, W2V, and the Transformer model achieve 59%,
74%, and 76% mean accuracy respectively, where the accuracy metrics were calculated using five-fold cross validation
(the cross validation procedure was described in 3.1). The better performance of the Transformer model is likely due to
the ability of the attention mechanism to generate context-dependent embeddings for each token within an abstract,
providing a more descriptive featurization and enabling the classifier to learn to accurately assign classes. The standard
deviation of the prediction accuracy ranges from 7%-9%, the relatively large value of the standard deviation is likely
due to the small size of the test dataset which contains approximately fifty samples for each cross-validation.

To assess the utility of developing NLP classification approaches a naive keyword-based classifier was also developed.
This keyword based approach categorizes an abstract according to which of the class label keywords, namely charac-
terization, modeling, processing, or synthesis (or some derivative thereof), appears first in the abstract. Note that for
several abstracts, none of these keywords appear and thus are left unlabeled. This keyword labeling technique classifies
abstracts with 41% accuracy, performing significantly worse than the NLP-based approaches. It is clear that NLP
techniques provide a more informative featurization by identifying latent energetic concepts present within the text.
Thus these techniques will prove useful for document retrieval, and other natural language, tasks with clear benefits
over naive keyword-based retrieval techniques.

To further understand the effect of available embedding approaches, we also assess the utility of domain-specific and
domain-agnostic Transformer variants, visualized as the four right-most bars in Figure 5. Our fine-tuned Transformer
outperforms all other Transformer variants. However, other Transformer models do appear to provide effective
embeddings, with most models achieving greater than 70% prediction accuracy. Interestingly, the MatSciBERT variant
yields the worst performance (65% accuracy) despite being fine-tuned on an adjacent domain, i.e. materials science.
Degraded performance of this model echoes the earlier observation that Transformers fine-tuned on different scientific
domains perform significantly worse at the masked language prediction task (see Table 5) than domain-specific or
base-variant Transformers. Finally, while a classification accuracy of 100% is in general desirable, inter-annotator
agreement, even amongst experts within the scientific discipline, is not expected as abstracts could belong to multiple
sub-disciplines and therefore classification thereof is subjective. The ability to rapidly ingest and classify energetic
documents enables accelerated knowledge retrieval and present the possibility to assist in both research and employee
training.

6 Conclusion

In this work we have emphasized challenges inherent in knowledge discovery and information extraction in the
energetics domain. Namely, the multidisciplinary nature of energetics, which spans several scientific disciplines, and
the exponential rate at which new scientific publications are published. To address these challenges, automation of
knowledge discovery and information extraction is critical to enable acceleration of the pace of energetics research and
energetic materials development. We propose NLP as an automation tool for rapid analysis of energetics texts. The
pursuit of an NLP-based approach for the energetics domain was inspired by the success in adjacent domains, where
NLP has enabled text mining and expedited scientific research efforts.

This study explored three NLP algorithms — LDA, W2V, and Transformer models — each trained on a curated corpus of
over 80,000 energetic documents. The final model variants were selected after thorough hyperparameter tuning (except
for the Transformer case) which involved optimizing the probabilistic language models on a held out validation set.
Subsequently, an energetics SME assessed each model’s ability to incorporate concepts and ideas critical to the study of
energetics. This assessment included the learned topic word distributions of LDA, the word embeddings of W2V, and
the masked language prediction of the fine-tuned Transformer model. Each model was found to incorporate knowledge
gleaned from the textual data. The LDA model formed semantically cohesive topics, W2V generated embeddings in
which thematically similar words were embedded in close proximity, and the Transformer successfully predicts masked
words characteristic to energetics documents. Thus, while each model exhibits aspects for further improvement, it is
apparent that each yield a language model informed of concepts fundamental to the study of energetics.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 17


°
a

_, PassiTient Classification V Accuracy

0.7 -
0.6 -
0.5,
0.4 -
0.3F
0.27
0.1 -

aN A
&
on
NLP Models

Figure 5: Document classification accuracy of energetic abstracts using various NLP models as a featurization method.
The dark red color indicates Transformer variant models. The black bar represents the standard deviation of the random
forest classifier. All reported metrics are obtained from five-fold cross validation.

Finally, to further demonstrate NLP’s utility for knowledge retrieval and information extraction, a document classification
pipeline was created. Each NLP algorithm was used to create a document feature vector that could be used in the
classification process. It was found that classification pipelines utilizing the W2V and Transformer models achieved 74%
and 76% classification accuracy respectively, outperforming the LDA model. Furthermore, the energetics fine-tuned
Transformer achieved better performance than the other Transformer variants that were considered, highlighting the
benefit of fine-tuning the network weights on a domain-specific corpus. In conclusion, this work presents a proof of
concept that NLP algorithms can serve as effective language models for the energetics discipline. The knowledge
discovery and information extraction systems enabled by NLP holds promise for accelerating the pace of energetics
research and the development of new energetic materials.

The intent of this work is to highlight the feasibility and utility of NLP methods within energetics research. There remain
nearly innumerable avenues for further exploration and development of NLP applications. Here we highlight a few
future directions we deem of critical importance. Adjacent domains such as the biomedical and material science domain
have developed standardized annotated datasets containing, for example, labeled polymer entities [20] or medical
question and answer pairs [52]. These datasets serve as comparative benchmarks for domain-specific natural language
tasks, establishing a standardized framework for model comparison. Similarly, energetic-specific annotated datasets
would support further NLP model development within energetics. We further note that our corpus is roughly two orders
of magnitude smaller in size than other LLMs training datasets [6, 20]. Identifying and retrieving text data from the
scattered sources characteristic of the multi-disciplinary energetics domain represents a critical advancement. Finally,
we note that developments within the NLP community over the past year have seen a rise of extremely large language
models such as GPT-4 [24] and PaLM [53]. These models have three orders of magnitude more parameters than older
LLMs such as BERT. For such massive language models the fine-tuning approach pursued in this work is rendered
significantly more difficult due to the lack of computing power and data availability. However, the size of these models
and the breadth of their datasets open alternative approaches, such as prompt tuning or few-shot learning [54], which
allow these massive models to be effectively aligned with specific domains. Aligned models achieve domain-specific

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 18


comprehension, knowledge recall, and reasoning in a relatively data efficient and compute efficient manner. These
alignment approaches hold promise for application of state-of-the-art language models to the energetics domain.

7 Author Contributions

F.G.V conceived the project workflow, F.G.V, E.P., and S.M. collected the data, E.P. developed the webscraping tool,
FEG.V and E.P developed the text preprocessing pipeline, F.G.V. developed the NLP training and evaluation pipeline,
FG.V. analyzed the results and prepared all tables, graphs, and figures. F.G.V., E.P., and M.C. wrote the paper. O.M.B.
and M.C. provided managerial oversight. All authors have reviewed, edited, and have given approval to the final
manuscript.

Acknowledgement

We thank Peter Chung (University of Maryland), Zois Boukavalas (American University), and Mark Fuge (University
of Maryland) for helpful discussion. We thank Kenneth Conley as well as Will Durant, along with the Energetics
Technology Center team, for facilitating the Automated Global Energetics project and providing helpful discussion.
We thank Ruth Doherty and American University for generating, and sharing, labeled energetic research article
abstracts. Support is gratefully acknowledged for this research from the Office of Naval Research under contract
N00014-21-C-1016 and the Energetics Technology Center under contract 2054-001-002.

Data Availability Statement

Codes and data are available upon reasonable request to the corresponding author.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 19


References

[1] Daniel C Elton, Zois Boukouvalas, Mark S Butrico, Mark D Fuge, and Peter W Chung. Applying machine
learning techniques to predict the properties of energetic materials. Scientific reports, 8(1):1-12, 2018.

[2] Brian C Barnes, Daniel C Elton, Zois Boukouvalas, DeCarlos E Taylor, William D Mattson, Mark D Fuge, and
Peter W Chung. Machine learning of energetic material properties. arXiv preprint arXiv: 1807.06156, 2018.

[3] Sangeeth Balakrishnan, Francis G VanGessel, Zois Boukouvalas, Brian C Barnes, Mark D Fuge, and Peter W
Chung. Locally optimizable joint embedding framework to design nitrogen-rich molecules that are similar but
improved. Molecular Informatics, 40(7):2100011, 2021.

[4] Daniel C Elton, Dhruv Turakhia, Nischal Reddy, Zois Boukouvalas, Mark D Fuge, Ruth M Doherty, and Peter W
Chung. Using natural language processing techniques to extract information on the properties and functionalities
of energetic materials from large text corpora. arXiv preprint arXiv: 1903.00415, 2019.

[5] Monica Puerto, Mason Kellett, Rodanthi Nikopoulou, Mark D Fuge, Ruth Doherty, Peter W Chung, and Zois
Boukouvalas. Assessing the trade-off between prediction accuracy and interpretability for topic modeling on
energetic materials corpora. arXiv preprint arXiv:2206.00773, 2022.

[6] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint
arXiv:1903.10676, 2019.

[7] Vineeth Venugopal, Sourav Sahoo, Mohd Zaki, Manish Agarwal, Nitya Nand Gosvami, and NM Anoop Krishnan.
Looking through glass: Knowledge discovery from materials science literature using natural language processing.
Patterns, 2(7):100290, 2021.

[8] Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson,
Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials
science literature. Nature, 571(7763):95—98, 2019.

[9] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of statistical
learning: data mining, inference, and prediction, volume 2. Springer, 2009.

[10] Zois Boukouvalas, Daniel C Elton, Peter W Chung, and Mark D Fuge. Independent vector analysis for data fusion
prior to molecular property prediction with machine learning. arXiv preprint arXiv:1811.00628, 2018.

[11] Zois Boukouvalas, Monica Puerto, Daniel C Elton, Peter W Chung, and Mark D Fuge. Independent vector analysis
for molecular data fusion: Application to property prediction and knowledge discovery of energetic materials. In
2020 28th European Signal Processing Conference (EUSIPCO), pages 1030-1034. IEEE, 2021.

[12] Alex D Casey, Steven F Son, Ilias Bilionis, and Brian C Barnes. Prediction of energetic material properties from
electronic structure using 3d convolutional neural networks. Journal of Chemical Information and Modeling, 60
(10):4457—4473, 2020.

[13] Phan Nguyen, Donald Loveland, Joanne T Kim, Piyush Karande, Anna M Hiszpanski, and T Yong-Jin Han.
Predicting energetics materials’ crystalline density from chemical structure by machine learning. Journal of
Chemical Information and Modeling, 61(5):2147-2158, 2021.

[14] Joshua L Lansford, Brian C Barnes, Betsy M Rice, and Klavs F Jensen. Building chemical property models for
energetic materials from small datasets using a transfer learning approach. Journal of Chemical Information and
Modeling, 62(22):5397-54 10, 2022.

[15] Chuan Li, Chenghui Wang, Ming Sun, Yan Zeng, Yuan Yuan, Qiaolin Gou, Guangchuan Wang, Yanzhi Guo,
and Xuemei Pu. Correlated rnn framework to quickly generate molecules with desired properties for energetic
materials in the low data regime. Journal of Chemical Information and Modeling, 62(20):4873—4887, 2022.

[16] Anas Nassar, Nirmal K Rai, Oishik Sen, and HS Udaykumar. Modeling mesoscale energy localization in shocked
hmx, part i: machine-learned surrogate models for the effects of loading and void sizes. Shock Waves, 29(4):
537-558, 2019.

[17] Sehyun Chun, Sidhartha Roy, Yen Thi Nguyen, Joseph B Choi, HS Udaykumar, and Stephen S Baek. Deep
learning for synthetic microstructure generation in a materials-by-design framework for heterogeneous energetic
materials. Scientific reports, 10(1):1—15, 2020.

[18] Elsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga Kononova, Gerbrand Ceder, Thomas Yong-Jin Han, and
Anna M Hiszpanski. Data-driven materials research enabled by natural language processing and information
extraction. Applied Physics Reviews, 7(4):041317, 2020.

[19] Tanishq Gupta, Mohd Zaki, NM Krishnan, et al. Matscibert: A materials domain language model for text mining
and information extraction. npj Computational Materials, 8(1):1—-11, 2022.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 20


[20] Pranav Shetty, Arunkumar Chitteth Rajan, Chris Kuenneth, Sonakshi Gupta, Lakshmi Prerana Panchumarti,
Lauren Holm, Chao Zhang, and Rampi Ramprasad. A general-purpose material property data extraction pipeline
from large polymer corpora using natural language processing. npj Computational Materials, 9(1):52, 2023.

[21] Jiang Guo, A Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W Coley, Klavs F Jensen, and Regina
Barzilay. Automated chemical reaction extraction from scientific literature. Journal of chemical information and
modeling, 62(9):2035—2045, 2021.

[22] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint
arXiv:2211,09085, 2022.

[23] OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL https: //openai.com/blog/
chatgpt/.

[24] Ali Kashefi and Tapan Mukerji. Chatgpt for programming numerical methods. arXiv preprint arXiv:2303.12093,
2023.

[25] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. ChemBERTa: Large-Scale Self-Supervised
pretraining for molecular property prediction. October 2020. URL http: //arxiv.org/abs/2010.09885.

[26

fol

Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. SMILES-BERT: Large scale unsu-
pervised Pre-Training for molecular property prediction. In Proceedings of the 10th ACM International Conference
on Bioinformatics, Computational Biology and Health Informatics, BCB ’19, pages 429-436, New York, NY, USA,
September 2019. Association for Computing Machinery. ISBN 9781450366663. doi:10.1145/3307339.3342186.
URL https://doi.org/10.1145/3307339 . 3342186.

Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das. Molformer:
Large scale chemical language representations capture molecular structure and properties. May 2022. URL
https: //www.researchsquare.com/article/rs-1570270/latest.pdf.

[28] Viraj Bagal, Rishal Aggarwal, P K Vinod, and U Deva Priyakumar. MoIGPT: Molecular generation using a
Transformer-Decoder model. J. Chem. Inf: Model., 62(9):2064—2076, May 2022. ISSN 1549-9596, 1549-960X.
doi:10.1021/acs.jcim.1c00600. URL http: //dx.doi.org/10.1021/acs.jcim.1c00600.

[29] Shion Honda, Shoi Shi, and Hiroki R Ueda. SMILES transformer: Pre-trained molecular fingerprint for low data
drug discovery. November 2019. URL http: //arxiv.org/abs/1911.04738.

[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. July 2019.
URL http: //arxiv. org/abs/1907.11692.

Philippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro Laino, and Jean-
Louis Reymond. Mapping the space of chemical reactions using Attention-Based neural networks. ChemRxiv,
December 2020. doi:10.26434/chemrxiv.9897365.v4. URL https://chemrxiv.org/engage/chemrxiv/
article-details/60c753a0bdbb89acf8a3a4b5.

[32] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are Few-Shot learners. May
2020. URL http: //arxiv.org/abs/2005.14165.

[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional
neural networks. In F Pereira, C J C Burges, L Bottou, and K Q Weinberger, editors, Advances in Neural
Information Processing Systems, volume 25, pages 1097-1105. Curran Associates, Inc., 2012. URL https:
//proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b- Paper . pdf.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,
John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529
(7587):484—489, January 2016. ISSN 0028-0836. doi:10.1038/nature16961. URL https://www.nature.com/
articles/nature16961.

[35] OpenAI. GPT-4 technical report. March 2023. URL http: //arxiv.org/abs/2303 .08774.

[36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. December 2014. URL
http://arxiv.org/abs/1412.6980.

[27

a

[31

ey

[34

sy

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 21


[37] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning
research, 3(Jan):993-1022, 2003.

[38] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector
space. arXiv preprint arXiv: 1301.3781, 2013.

[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[40] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep
transformer models for machine translation. arXiv preprint arXiv: 1906.01787, 2019.

[41] Chenguang Wang, Mu Li, and Alexander J Smola. Language models with transformers. arXiv preprint
arXiv:1904.09408, 2019.

[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,
faster, cheaper and lighter. arXiv preprint arXiv: 1910.01108, 2019.

[43] Leo Breiman. Random forests. Mach. Learn., 45(1):5-32, October 2001. ISSN 0885-6125, 1573-0565.
doi: 10.1023/A:1010933404324. URL https: //doi.org/10.1023/A: 1010933404324.

44] Grobid. https: //github.com/kermitt2/grobid, 2008-2023.
45
46

47] Radim Rehurek and Petr Sojka. Gensim—python framework for vector space modelling. NLP Centre, Faculty of
Informatics, Masaryk University, Brno, Czech Republic, 3(2), 2011.

[48] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,
Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and
Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online,
October 2020. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/2020.
emnlp-demos.6.

Sciencebeam parser. https: //https://gitlab.coko.foundation/sciencebeam/sciencebeam- parser.

[44]
[45]
[46] Beautiful soup. https: //beautiful-soup-4.readthedocs.io/en/latest/.
[47]

[49] SD Gardner and Jerry Wackerle. Interactions of detonation waves in condensed explosives. Technical report, Los
Alamos Scientific Lab., Univ. of California, N. Mex., 1965.

[50] Martin Wattenberg, Fernanda Viégas, and Ian Johnson. How to use t-sne effectively. Distill, 2016.
doi: 10.23915/distill.00002. URL http: //distill.pub/2016/misread-tsne.

[51] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv: 1810.04805, 2018.

[52] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay
Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature,
pages 1-9, 2023.

[53] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022.

[54] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances
in neural information processing systems, 33:1877-1901, 2020.

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 22


A Model Hyperparameters

A.1 LDA Model Hyperparameters

The final, best performing, LDA model (trained with the GENSIM Python library) hyperparameters are provided in
Table 6.

Number of Topics 300
Document Topic Prior 1.0
Topic Word Prior ¥300
Chunksize 2000
Passes 20
Iterations 400

Table 6: LDA Model Hyperparameters

A.2 W2V Model Hyperparameters

The final, best performing, W2V model (trained with the GENSIM Python library) hyperparameters are provided in
Table 7

Embedding Dimension 300

Context Window Size 2

Minimum Word Count 20

Model Variant Continuous Bag of Words

Table 7: W2V Model Hyperparameters

A.3 Transformer Model Hyperparameters

The final, best performing, Transformer model (trained with the HuggingFace library) hyperparameters are provided
in Table 8. Hyperparameters not explicitly given are set to their default values according to the Transformer library
version 4.24.0.

Base Transformer Model distilbert-base-uncased
Number of Epochs 100

Train Batch Size 32

Tokens per Sample 512

Vocabulary Size 30522

Table 8: Transformer Model Hyperparameters

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited. 23


A.4 Random Forest Model Hyperparameters

The random forest classification model hyperparameters are provided in Table 9.

Test Size 0.33
Criterion Gini Coefficient
Max Features Sart
Number of Estimators 200

Table 9: Random Forest Model Hyperparameters

DISTRIBUTION A (Log No. 23-058). Approved for Public Release; Distribution is Unlimited.

24
