arX1v:2507.10559v2 [cs.CY] 16 Jul 2025

NLP Meets the World: Toward Improving Conversations With the Public
About Natural Language Processing Research

Shomir Wilson
Department of Human-Centered Computing and Social Informatics
Pennsylvania State University
shomir@psu. edu

Abstract

Recent developments in large language mod-
els (LLMs) have been accompanied by rapidly
growing public interest in natural language pro-
cessing (NLP). This attention is reflected by
major news venues, which sometimes invite
NLP researchers to share their knowledge and
views with a wide audience. Recognizing the
opportunities of the present, for both the re-
search field and for individual researchers, this
paper shares recommendations for communi-
cating with a general audience about the ca-
pabilities and limitations of NLP. These rec-
ommendations cover three themes: vague ter-
minology as an obstacle to public understand-
ing, unreasonable expectations as obstacles to
sustainable growth, and ethical failures as ob-
stacles to continued support. Published NLP
research and popular news coverage are cited to
illustrate these themes with examples. The rec-
ommendations promote effective, transparent
communication with the general public about
NLP, in order to strengthen public understand-
ing and encourage support for research.

1 Introduction

Research communication has become a vital task
for the natural language processing (NLP) research
community, which faces a unique confluence of cir-
cumstances. Public interest in LLMs has led major
news venues to publish non-technical explanations
of how LLMs work (Roose, 2023; Clarke et al.,
2023), predictions about the future of LLM de-
velopment (Waters, 2025), explorations of LLMs’
sociodemographic biases (Rogers, 2025), and con-
cerns about their impacts on labor markets (Hayes,
2025). News articles have also featured interviews
with researchers who critique claims about arti-
ficial general intelligence (AGI) (Heikkila, 2025;
Piquard, 2024). Within scholarly literature, LLMs
are sometimes claimed to be an existential threat
to humanity (Tait and Bensemann, 2024; Groz-
danoff, 2023), while others critique the plausibility

or meaning of AGI (Blili-Hamelin et al., 2025;
Mueller, 2024). Regardless of stance, these top-
ics and others reflect how NLP is experiencing an
unprecedented surge of public attention.

The challenges of public discussion of NLP ex-
tend into culture and language. NLP researchers
speaking with the public also contend with portray-
als of AI in science fiction (Osawa et al., 2022) and
ambiguity over cognitive terms such as reason and
understand (Mitchell and Krakauer, 2023). While
research communication is a familiar task in any
scholarly discipline, little support is available for
the distinct set of challenges that NLP researchers
currently face. This gap contrasts with the impor-
tance of research communication: it encourages
public support for research funding, influences pub-
lic policy, and engages the interest of students who
may become future researchers.

This paper serves as a referenceable set of rec-
ommendations for effective conversations between
NLP researchers and the general public. These
recommendations are especially for researchers
interacting with popular media (e.g., being inter-
viewed for news articles) and promoting their work
on social media. Three problems are described
with accompanying guidance: vague terminology
as an obstacle to public understanding (§3), unrea-
sonable expectations as obstacles to sustainable
growth (§4), and ethical failures as obstacles to
continued support ($5). Rather than supplanting
other guidance on research communication, this
paper fills a notable gap by attending to challenges
specific to the NLP research community.

2 Related Work

This paper follows an established practice of NLP
conference papers that reflect on community chal-
lenges and suggest ways forward. Examples of this
include Rogers and Augenstein’s recommendations
for improving peer review of NLP papers (2020),


Blodgett et al.’s critique of how bias is discussed
by NLP researchers (2020), and Mosbach et al.’s
discussion of interpretability research (2024).

However, the published literature provides little
guidance distinctly for NLP researchers on pub-
lic communication (i.e., in contrast with guidance
that applies to researchers in any discipline). No-
tably, Hudson and Franklin (2023) observe that
explainable AI researchers often rely on perfor-
mance figures to describe their work, making it
difficult to connect with a public audience. Confer-
ences have hosted a series of tutorials on science
communication for AI researchers, most recently
at AAAT 2025 (Smith, 2025), but the reach and
accessibility of those tutorials differ from a pub-
lished paper. Wyrich and Wagner (2023) designed
a course-based exercise for computer science under-
graduates to practice communicating the findings
of a paper from software engineering, a related
field. However, they focus on describing the exer-
cise itself rather than sharing guidance on its topic.

In the broader realm of research communication,
several works provide guidance that is generally ap-
plicable to NLP but do not address the challenges
that NLP researchers currently face. Fontaine et al.
(2019) surveyed the literature on public commu-
nication by researchers within the health sciences.
They noted wide use of social media and proposed
a typology of strategies. These include minimizing
the use of jargon, encouraging public discussion,
and making information actionable, among oth-
ers. The US National Academies (2017) provided
extensive guidance on science communication top-
ics such as handling controversy, understanding
one’s audience, and using social media effectively.
Kuncel and Rigdon (2013) address science commu-
nication for psychology, with a focus on making
statistical methods understandable.

All of the above works are valuable contribu-
tions, and it is appropriate to select among them for
a basic understanding of research communication.
This manuscript builds upon them by focusing on
the unique circumstances of NLP research, reme-
dying a lack of field-specific guidance.

3 Vague Terminology as an Obstacle to
Public Understanding

NLP researchers often describe their work using
terms that public audiences associate with cog-
nition. Sometimes researchers do not intend to
make cognitive claims with these terms, but some-

times the cognitive claims are intentional. Addition-
ally, the research community lacks an agreed-upon
stance on some cognitive claims, partly because of
a lack of agreement for what they mean (Bender
and Koller, 2020). These conditions are difficult
for non-experts to navigate. This section illustrates
with terms used in published research.

3.1 It’s Complicated: NLP’s Relationship
With Cognitive Terms

Terms with cognitive implications (among their
other implications) have a long history in NLP, to
the extent that some are widely interpreted by re-
searchers without referring to cognition. Predict,
for example, has been associated with text classi-
fication at least as early as the 1990s (Riloff and
Lehnert, 1992; Liddy et al., 1994) and possibly ear-
lier. Read also has a long history (Srihari, 1992;
Govindan and Shivaprasad, 1990). Learn (as in
machine learning) originates from a 1959 paper by
Arthur Samuel about programs that play the game
of checkers (Samuel, 1959; Foote, 2021). However,
a non-expert may associate all three of those terms
with the ability to think or understand, broadly
without definitions. Observe that “a human must
understand to predict” is intuitively reasonable, but
within the NLP research community “a language
model must understand to predict” is controversial
within the framing of several papers cited in §1.

Recently LLMs’ capabilities and limitations
have amplified attention to cognitive terms. Hallu-
cinate is widely recognized by the research commu-
nity (Huang et al., 2025), even if a precise meaning
is not agreed upon (Narayanan Venkit et al., 2024).
However, the relevance of understand, reason, and
think are controversial (Bender and Koller, 2020;
Shojaee et al., 2025). Again, the relationships be-
tween these terms are complex within NLP, to an
extent unknowable by an outside audience. A hu-
man who is hallucinating is presumably thinking
(albeit aberratively), but a hallucinating LLM is
not agreed upon by the research community to be
thinking. A further complication happens when
arguments against LLMs’ cognitive abilities nomi-
nally exclude humans from having those same abil-
ities. For example, Shojaee et al. (2025) argue that
language reasoning models (LRMs, closely related
to LLMs) cannot think because of “ta complete ac-
curacy collapse” for complex tasks. The human
brain also has task complexity limits (Marois and
Ivanoff, 2005), but in contrast with LLMs, it is
widely recognized as capable of thinking.


3.2 Recommendations

NLP researchers speaking with the public should
provide scope for cognitive terms they use. Given
the above obstacles, it is helpful to tell a general au-
dience what terms such as predict or reason mean
in a context. If a non-cognitive term can be used in-
stead of a cognitive one, that substitution may pro-
vide greater clarity and transparency. When bench-
marks are relevant, describing their role can demys-
tify how a cognitive or pseudo-cognitive claim is
justified. A general goal is to avoid the possibility
of unintended cognitive implications for an LLM
or other human language technology.

NLP researchers should balance correcting cog-
nitive misconceptions about LLMs with tolerating a
general audience’s verbal shorthand and metaphor.
Humans apply cognitive anthropomorphism to a
variety of objects (Boyer, 1996) such as lamps and
umbrellas (Edwards and Shafer, 2022), and LLMs
display far more sophisticated behavior than those
objects. When cognitive terms surface in discus-
sions about LLMs, a diplomatic strategy is to point
out the verbal shorthand or metaphor, acknowledge
its naturalness, and explain the actual limitations
of the technology. This strategy validates a non-
expert’s experiences with technology while illumi-
nating how cognitive terms can mislead.

4 Unreasonable Expectations as
Obstacles to Sustainable Growth

Predictions of the impacts of LLMs include radical
improvements in biology, neuroscience, economic
development, and human welfare, and the onset
of global peace (Amodei, 2024). The possibility
of an “AI bubble”! (i.e., an unsustainable level of
public interest in AI that exceeds its capabilities)
is widely acknowledged (Horowitch, 2025; Press,
2025). A bubble implies short-term benefits, such
as publicity and abundant resources. However, its
collapse—implied by the metaphor—creates harm
through an unpredictably sharp decline in resources.
This section provides examples from the history of
AI and recommendations for how NLP researchers
can encourage sustainable perceptions of the field.

4.1 Remembering AI Winters

Multiple AI winters are said to have occurred,
and they lack agreed-upon dates. Roger Schank

'There exists a lack of agreement on the distinction be-
tween AI and NLP, with some common ground and some
differences. To avoid misattribution, this paper uses “AI” for
topics where the term currently prevails in popular discussion.

and Marvin Minsky referred to one in the
1970s (Crevier, 1993), and James Hendler referred
to one in the 1980s (Hendler, 2008). This section
uses Hendler’s perspective to structure the discus-
sion.

Hendler described a confluence of three condi-
tions that led to the AI winter of the 1980s. The
first condition was a decrease in government fund-
ing for AI research in the 1970s, specifically in
the USA, which dominated AI research at the time.
He argues the impact was delayed by five to ten
years, as already-approved projects were permitted
to run their course. For the rationale of the cuts,
Hendler quotes a 1973 report by James Lighthill in
the UK that concluded “in no part of the field have
discoveries made so far produced the major im-
pact that was then promised.” (James et al., 1973)
The second condition was an erosion of recogni-
tion for AI’s successes. Hendler argues that expert
systems (Buchanan and Smith, 1988), which were
successful in industry, were “disowned” by AI re-
searchers in favor of topics where less progress had
been made. The third condition was the rapid de-
mocratization of hardware needed to run AI models,
in the sense that the hardware became dramatically
cheaper and easier for individuals to own, thus
disrupting sociotechnical power structures within
research (Hendler, 2008).

These conditions require some subjectivity to
assess, making it difficult to firmly state whether
they exist now or will appear in the near future.
However, there are discernible lessons in them for
how NLP researchers communicate their work.

4.2 Recommendations

For the NLP research community as a whole, the
downsides of moderating expectations are modest
when compared to the downsides of a sudden col-
lapse in public interest. At the individual level,
one researcher’s careful moderation when speaking
about their work is unlikely to change public per-
ception of the field, but strategies exist that fulfill
that moderation while creating individual benefit
as well. Such a strategy is described below.

Public engagement is an opportunity to sup-
port recognition of NLP applications that may
have been inadvertently “disowned” (borrowing
Hendler’s description) for their success. NLP
played a formative role in early search en-
gines (Schwartz, 1998) and enabled early spam
detection (Cormack et al., 2008). Optical char-
acter recognition made a wealth of pre-digital


texts searchable (Hill and Hengchen, 2019), and
authorship identification solved historical myster-
ies (Tweedie et al., 1996). Those examples and oth-
ers show NLP applications as calm technologies,
supporting human work rather than monopolizing
human attention (Weiser and Brown, 1996). They
also may serve as an antidote to hype by illustrating
the expectation that research products need time to
mature. Dialog systems are a strong public-facing
example of the long road to practical use: early
systems such as Eliza (Weizenbaum, 1966) are dis-
tant ancestors of modern virtual assistants, such as
Amazon Alexa and Google Assistant.

5 Ethical Failures as Obstacles to
Continued Support

Ethically dubious practices are known to exist
within the NLP research community, and outside
of it, problematic applications exist as well. Both
are obstacles to how NLP research is received by
a public audience. This section gives examples of
relevant ethical failures and provides suggestions
for how NLP researchers can speak frankly and
constructively when called upon to discuss them
with the public.

5.1 Reckoning With Problematic NLP

Research institutions in many countries have pro-
cedures for reviewing experimental protocols that
involve human subjects. However, sometimes an
experiment complies with requirements while vi-
olating ethical norms, or researchers fail to seek
approval. A recent example is an experiment in
which researchers studied the persuasiveness of an
LLM-powered chatbot by covertly deploying it on
the Reddit forum r/changemyview”, concealing that
it was an LLM. When forum moderators were told
of the deception, they expressed shock, asked for
an apology from the university, and requested that
the research not be published (O’Grady, 2025).

A comprehensive list of ethical failures or mis-
uses of NLP applications is impractical, but ex-
amples are readily available. LLMs have been
used to generate disinformation, potentially ma-
nipulating geopolitics (Fried, 2025). Chatbots
have demonstrated the capacity to respond harm-
fully in conversations with people suffering from
mental illness (Coghlan et al., 2023), and more
broadly, LLMs’ sociodemographic biases have
been shown to harm individuals from minoritized

*https://www.reddit.com/r/changemyview/

groups (Ghosh et al., 2024). The broad-based col-
lection of training data for LLMs raises privacy
concerns and intellectual property concerns (Nov-
elli et al., 2024). LLM-assisted writing has been
shown to produce “cognitive debt” and reduce crit-
ical thinking (Kosmyna et al., 2025).

The above harms and others receive public atten-
tion, to the extent that a 2023 Pew Research survey
found 52% of Americans were more concerned
than excited about AI in their daily lives (Faverio
and Tyson, 2023). This level of concern is demon-
strably relevant to NLP researchers.

5.2 Recommendations

Several popular news articles cited in the above
section show that NLP researchers are called upon
to discuss problematic uses of NLP. Open dialog
about these problems is obligatory and familiar as
part of open science, but further steps can help
to sustain public support. Researchers speaking
with the general public about problematic NLP
should be prepared to explain what should have
been done differently to achieve the same goals, or
why the goals should never have been undertaken.
Public interaction is also an opportunity to point to
research efforts directly aimed at ethical issues in
NLP, showing how human-centered research is a
valuable part of the community. Such research is a
marked contrast with a hypothetical public image
of AI as solely focused on computing.

Within the research community, researchers can
lay the groundwork for strong public support by
recognizing that research that critiques NLP fully
belongs in the community. Such research does not
necessarily create human language technologies,
but it guides their ethical development. This inclu-
sion also encourages grounding NLP research in
human benefit, which demonstrates responsiveness
to the concerns of the general public.

6 Conclusion

Public attention to NLP research comes with bene-
fits, obstacles, and responsibilities. This paper pre-
sented recommendations to help NLP researchers
communicate effectively with the public, focusing
on obstacles that are distinctive to our research
community. It drew upon a variety of sources from
research literature and popular news coverage of
NLP to illustrate those obstacles and how to nav-
igate them, for the prosperity of both individual
researchers and the research community at large.


Acknowledgments

Kenneth Huang and Natalie Parde are gratefully
acknowledged for their feedback on ideas in this
manuscript. This manuscript is based upon work
supported by the National Science Foundation un-
der Grant #2237574.

Limitations

This manuscript is not a comprehensive guide to
research communication, and instead it augments
such guides. In §1 and §2 we remind readers that
our contribution is an NLP-specific addition to ex-
isting literature about research communication, and
in §2 we cite several sources the reader may consult
for more background.

NLP researchers also may face public commu-
nication challenges not covered in this guide. We
identify three challenges of particular concern, but
others are likely to exist. The subjective nature
of the topic space makes it difficult to segment it
in a principled way, but we suggest that providing
evidence-based guidance (as in this manuscript) in
the published literature is more likely to lead to
positive outcomes than a lack of guidance.

References

Dario Amodei. 2024. Machines of loving grace:
How AI could transform the world for the better.
Online essay. https://www.darioamodei.com/
essay/machines-of-loving-grace.

Emily M. Bender and Alexander Koller. 2020. Climbing
towards NLU: On meaning, form, and understanding
in the age of data. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 5185-5198, Online. Association for
Computational Linguistics.

Borhane Blili-Hamelin, Christopher Graziul, Leif
Hancox-Li, Hananel Hazan, El-Mahdi El-Mhamdi,
Avijit Ghosh, Katherine Heller, Jacob Metcalf, Fabri-
cio Murai, Eryk Salvaggio, Andrew Smart, Todd
Snider, Mariame Tighanimine, Talia Ringer, Mar-
garet Mitchell, and Shiri Dori-Hacohen. 2025. Stop
treating “AGI as the north-star goal of ai research.
Preprint, arXiv:2502.03689.

Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5454—
5476, Online. Association for Computational Lin-
guistics.

Pascal Boyer. 1996. What makes anthropomorphism
natural: Intuitive ontology and cultural representa-
tions. Journal of the Royal Anthropological Institute,
pages 83-97.

Bruce G Buchanan and Reid G Smith. 1988. Fundamen-
tals of expert systems. Annual Review of Computer
Science, 3(1):23-58.

Sean Clarke, Dan Milmo, and Garry Blight. 2023. How
AI chatbots like ChatGPT or Bard work — visual
explainer. Accessed: 2025-06-19.

Simon Coghlan, Kobi Leins, Susie Sheldrick, Marc
Cheong, Piers Gooding, and Simon D’ Alfonso.
2023. To chat or bot to chat: Ethical issues with
using chatbots in mental health. Digital health,
9:20552076231183542.

Gordon V Cormack and 1 others. 2008. Email spam
filtering: A systematic review. Foundations and
Trends® in Information Retrieval, 1(4):335-455.

Daniel Crevier. 1993. AI: The Tumultuous History of the
Search for Artificial Intelligence, \st edition. Basic
Books, New York, NY.

Alyssa D Edwards and Daniel M Shafer. 2022. When
lamps have feelings: Empathy and anthropomor-
phism toward inanimate objects in animated films.
Projections, 16(2):27-52.

Michelle Faverio and Alec Tyson. 2023. What the data
says about americans’ views of artificial intelligence.
Pew Research Center — Short Reads.

Guillaume Fontaine, Marc-André Maheu-Cadotte, An-
dréane Lavallée, Tanya Mailhot, Genevieve Rouleau,
Julien Bouix-Picasso, and Anne Bourbonnais. 2019.
Communicating science in the digital and social me-
dia ecosystem: Scoping review and typology of strate-
gies used by health scientists. JMIR Public Health
and Surveillance, 5(3):e 14447.

Keith D. Foote. 2021. A brief history of machine learn-
ing. Online article. https://www.dataversity.
net/a-brief-history-of-machine-learning/.

Ina Fried. 2025. Exclusive: Russian disinformation
floods AI chatbots, study finds. Axios. Based on a
NewsGuard report.

Sourojit Ghosh, Pranav Narayanan Venkit, Sanjana
Gautam, Shomir Wilson, and Aylin Caliskan. 2024.
Do generative AI models output harm while rep-
resenting non-western cultures: Evidence from a
community-centered approach. In Proceedings of
the AAAI/ACM Conference on AI, Ethics, and Society
(AIES), volume 7 of AJES ’24, pages 476-489.

V.K Govindan and A.P Shivaprasad. 1990. Charac-
ter recognition — a review. Pattern Recognition,
23(7):67 1-683.

Boris D Grozdanoff. 2023. The looming shadow: Tak-
ing seriously potential existential threats brought
about by artificial intelligence. Ethical Studies, pages
100-106.


Adam Hayes. 2025. Is AI coming for your job? here’s
how to tell. Investopedia. Accessed: 2025-06-19.

Melissa Heikkilaé. 2025. Margaret mitchell: artificial
general intelligence is ‘just vibes and snake oil’. Fi-
nancial Times. Accessed: 2025-06-19.

James Hendler. 2008. Avoiding another AI winter.
IEEE Intelligent Systems, 23(2):2-4.

Mark J Hill and Simon Hengchen. 2019. Quantifying
the impact of dirty ocr on historical text analysis:
Eighteenth century collections online as a case study.
Digital Scholarship in the Humanities, 34(4):825-
843.

Rose Horowitch. 2025. The computer-science bubble is
bursting. The Atlantic.

Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2025. A survey on hallucination in large lan-
guage models: Principles, taxonomy, challenges, and
open questions. ACM Trans. Inf. Syst., 43(2).

Simon Hudson and Matija Franklin. 2023. Science
communications for explainable artificial intelligence.
Preprint, arXiv:2308.16377.

Lighthill James, Lighthill James, Sutherland Stuart,
Needham Roger, and Longuet-Higgins Christopher.
1973. Artificial intelligence: A general survey. Sci-
ence Research Council.

Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan,
Jessica Situ, Xian-Hao Liao, Ashly Vivian Beres-
nitzky, Iris Braunstein, and Pattie Maes. 2025. Your
brain on ChatGPT: Accumulation of cognitive debt
when using an ai assistant for essay writing task.
Preprint, arXiv:2506.08872.

Nathan R. Kuncel and J. Rigdon. 2013. Communicat-
ing research findings. In Neal W. Schmitt, Scott
Highhouse, and Irving B. Weiner, editors, Handbook
of Psychology: Industrial and Organizational Psy-
chology, 2nd edition, volume 12, pages 43-58. John
Wiley & Sons, Inc., Hoboken, NJ.

Elizabeth D. Liddy, Woojin Paik, and Edmund S. Yu.
1994. Text categorization for multiple users based
on semantic features from a machine-readable dictio-
nary. ACM Trans. Inf. Syst., 12(3):278-295.

René Marois and Jason Ivanoff. 2005. Capacity limits
of information processing in the brain. Trends in
cognitive sciences, 9(6):296—305.

Melanie Mitchell and David C Krakauer. 2023. The
debate over understanding in AI’s large language
models. Proceedings of the National Academy of
Sciences, 120(13):e2215907120.

Marius Mosbach, Vagrant Gautam, Tomas Ver-
gara Browne, Dietrich Klakow, and Mor Geva. 2024.
From insights to actions: The impact of interpretabil-
ity and analysis research on NLP. In Proceedings

of the 2024 Conference on Empirical Methods in
Natural Language Processing, pages 3078-3105, Mi-
ami, Florida, USA. Association for Computational
Linguistics.

Milton Mueller. 2024. The myth of AGI. Internet
Governance Project.

Pranav Narayanan Venkit, Tatiana Chakravorti, Vipul
Gupta, Heidi Biggs, Mukund Srinath, Koustava
Goswami, Sarah Rajtmajer, and Shomir Wilson.
2024. An audit on the perspectives and challenges of
hallucinations in NLP. In Proceedings of the 2024
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 6528-6548, Miami, Florida,
USA. Association for Computational Linguistics.

National Academies of Sciences, Engineering, and
Medicine. 2017. Communicating science effectively:
A research agenda.

Claudio Novelli, Federico Casolari, Philipp Hacker,
Giorgio Spedicato, and Luciano Floridi. 2024. Gen-
erative AI in EU law: Liability, privacy, intellectual
property, and cybersecurity. Computer Law & Secu-
rity Review, 55:106066.

Cathleen O’Grady. 2025. ‘Unethical’ AI research on
reddit under fire. Science, 388(6747):570-571. Per-
spective.

Hirotaka Osawa, Dohjin Miyamoto, Satoshi Hase,
Reina Saijo, Kentaro Fukuchi, and Yoichiro Miyake.
2022. Visions of artificial intelligence and robots in
science fiction: a computational analysis. Interna-
tional Journal of Social Robotics, 14(10):2123-2133.

Alexandre Piquard. 2024. Chatbots are like parrots, they
repeat without understanding. Le Monde (English
edition). Interview with Emily M. Bender; Accessed:
2025-06-19.

Gil Press. 2025. Are we at peak AI bubble and the cusp
of AI moment? Forbes.

Ellen Riloff and Wendy G Lehnert. 1992. Classifying
texts using relevancy signatures. In AAAI, pages 329-
334.

Anna Rogers and Isabelle Augenstein. 2020. What
can we do to improve peer review in NLP? In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020, pages 1256-1262, Online. Association
for Computational Linguistics.

Reece Rogers. 2025. Ai is spreading old stereotypes
to new languages and cultures. WIRED. Accessed:
2025-06-19.

Kevin Roose. 2023. How does ChatGPT really work.
New York Times, 28.

A. L. Samuel. 1959. Some studies in machine learn-
ing using the game of checkers. IBM J. Res. Dev.,
3(3):210-229.


Candy Schwartz. 1998. Web search engines. Jour-
nal of the American Society for Information Science,

49(11):973-982.

Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh,
Maxwell Horton, Samy Bengio, and Mehrdad Fara-
jtabar. 2025. The illusion of thinking: Understand-
ing the strengths and limitations of reasoning mod-
els via the lens of problem complexity. Preprint,
arXiv:2506.06941.

Lucy Smith. 2025. Science communication for AI re-
searchers: Introductory training. In Proceedings of
the 39th AAAI Conference on Artificial Intelligence
(AAAI-25), Tutorial Track. 13:00-14:00 tutorial +
14:00-15:00 drop-in session.

S.N. Srihari. 1992. High-performance reading ma-
chines. Proceedings of the IEEE, 80(7):1120-1132.

Izak Tait and Joshua Bensemann. 2024. Clipping the
risks: Integrating consciousness in AGI to avoid exis-
tential crises. In International Conference on Artifi-
cial General Intelligence, pages 176-182. Springer.

Fiona J Tweedie, Sameer Singh, and David I Holmes.
1996. Neural network applications in stylometry:
The federalist papers. Computers and the Humani-
ties, 30:1-10.

Richard Waters. 2025. The diverging future of AI. Fi-
nancial Times. Accessed: 2025-06-19.

Mark Weiser and John Seely Brown. 1996. Designing
calm technology. PowerGrid Journal, 1(1):75-85.

Joseph Weizenbaum. 1966. Eliza—a computer program
for the study of natural language communication be-
tween man and machine. Communications of the

ACM, 9(1):36-45.

Marvin Wyrich and Stefan Wagner. 2023. Teaching
computer science students to communicate scientific
findings more effectively. In Proceedings of the 45th
International Conference on Software Engineering:
Software Engineering Education and Training, I1CSE-
SEET ’23, page 107-114. IEEE Press.
