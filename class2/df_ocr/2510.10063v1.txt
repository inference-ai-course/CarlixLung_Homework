2510.10063v1 [cs.CL] 11 Oct 2025

arXiv

CLMN: Concept based Language Models via Neural Symbolic Reasoning

Yibo Yang!

Abstract— Deep learning’s remarkable performance in nat-
ural language processing (NLP) faces critical interpretability
challenges, particularly in high-stakes domains like healthcare
and finance where model transparency is essential. While
concept bottleneck models (CBMs) have enhanced interpretabil-
ity in computer vision by linking predictions to human-
understandable concepts, their adaptation to NLP remains
understudied with persistent limitations. Existing approaches
either enforce rigid binary concept activations that degrade tex-
tual representation quality or obscure semantic interpretability
through latent concept embeddings, while failing to capture dy-
namic concept interactions crucial for understanding linguistic
nuances like negation or contextual modification. This paper
proposes the Concept Language Model Network (CLMN), a
novel neural-symbolic framework that reconciles performance
and interpretability through continuous concept embeddings
enhanced by fuzzy logic-based reasoning. CLMN addresses
the information loss in traditional CBMs by projecting con-
cepts into an interpretable embedding space while preserving
human-readable semantics, and introduces adaptive concept
interaction modeling through learnable neural-symbolic rules
that explicitly represent how concepts influence each other
and final predictions. By supplementing original text features
with concept-aware representations and enabling automatic
derivation of interpretable logic rules, our framework achieves
superior performance on multiple NLP benchmarks while pro-
viding transparent explanations. Extensive experiments across
various pre-trained language models and datasets demonstrate
that CLMN outperforms existing concept-based methods in
both accuracy and explanation quality, establishing a new
paradigm for developing high-performance yet interpretable
NLP systems through synergistic integration of neural repre-
sentations and symbolic reasoning in a unified concept space.
Code and data are available at https://github.com/
MichaelYang-1lyx/CLMN.

I. INTRODUCTION

Deep learning has seen widespread application due to its
outstanding performance in fields like image recognition [1]
and natural language processing [18]. However, the complex
neural network structures and numerous parameters make its
decision-making process difficult to interpret, often earning
it the label of a *black box” [10]. This characteristic limits
the use of deep learning in areas where high interpretability
is essential, such as healthcare [6], finance [16], law [26],
and autonomous driving [22].

In the healthcare sector, the opacity of deep learning
models used for diagnosis and treatment decisions can lead
to skepticism among doctors and patients [20]. For instance,
when a model recommends a specific treatment plan, doctors
need to understand the underlying reasons and logic to ensure
its reliability and effectiveness [24]. In finance, the use of

lYibo Yang is with the College of Science, The Hong Kong University of
Science and Technology, Hong Kong yyanggh@connect.ust.hk

deep learning models in risk assessment and credit scoring
can raise regulatory and legal issues. Financial institutions
and regulatory bodies need to be able to explain and verify
the decision-making processes of these models to avoid
potential compliance risks [3].

Clearly, interpretability is crucial in deep learning, as it
enhances user trust in model decisions and ensures safety and
compliance in critical domains. Against this backdrop, con-
cept bottleneck model (CBM) has emerged as a significant
research direction. However, there has been little research
on CBM in the field of NLP, with most studies focusing
on computer vision (CV). It wasn’t until 2024 that the
paper Interpreting Pretrained Language Models via Concept
Bottlenecks [25] first attempted to apply CBM to pretrained
language models (PLM), achieving promising results but
leaving room for improvement.

The fundamental challenge lies in the inherent conflict
between performance and interpretability when adapting
CBM to NLP tasks. Traditional CBM enforces hard con-
cept interventions through binary concept activation, which
inevitably causes information loss in text representations.
While Concept Embedding Models (CEM) [29] alleviate
this issue by projecting concepts into continuous embed-
ding space, their latent space operations obscure human-
understandable semantics. Furthermore, existing approaches
fail to capture the dynamic interactions between concepts —
for instance, how negation concepts like “not severe” modify
disease severity concepts in medical texts. This limitation
stems from their reliance on rigid concept labeling rather
than learning conceptual relationships from data.

In response, we considered CEM, which improved model
performance but compromised interpretability. To address
this, we further incorporated fuzzy logic for interpretable
neural-symbolic concept reasoning [14], [30]. This approach
not only enhanced model performance but also improved
interpretability. Through this method, we achieved concept
explanations and generated logical rules composed of con-
cepts, clarifying the relationship between predictions and
concepts. The overall framework is depicted in Figure 1.

Our contributions are as follows:

e We propose a novel neural-symbolic interpretable
framework for the NLP field: Concept Language Model
Network (CLMN). CLMN effectively addresses long-
standing issues such as the accuracy drop in concept-
based approaches and the inability of early complex
neural-symbolic models to reflect interactions between
concepts.

e We utilize neural-symbolic reasoning to efficiently inter-
pret the interactions between concepts, addressing the


Concept Layer
PLM
PUA ArgMax
ll

iD - - ©) review_majority: 3

Concept Prediction

food_aspect_majority
ambiance_aspect_majority
service_aspect_majority
noise_aspect_majority
cleanliness
price

location

menu variety

waiting time

waiting area

Input Sententce

The experience and food was
good but loud, and the portions
of the food (especially the sides)
were not filling for the price.

=a

review_majority: 3

|-

Fig. 1.

This flowchart details a text - classification framework. An input sentence is input into a PLM. The Concept Layer predicts aspects like food. Its

results, combined with PLM’s, yield a sentiment score. Concepts also flow to the Concept Reasoning Layer, constraining internal model explanations.

efficiency issues in concept-based model interactions.
Additionally, using concept features as a supplement to
original modality features enhances final performance
accuracy.

e We applied our framework to various PLMs and
datasets, conducting extensive experiments. The results
demonstrate that our framework generates semantic
explanations through neural-symbolic reasoning in a
consistent concept space, offering interpretability while
achieving excellent performance.

Il. RELATED WORK

A. Pretrained Language Models

The rapid advancement of pretrained language models
(PLMs) has fundamentally transformed NLP through their
capacity to capture intricate linguistic patterns. Transformer-
based architectures like BERT [5] and RoBERTa [17]
leverage bidirectional self-attention mechanisms to construct
context-aware representations, with empirical studies reveal-
ing their ability to encode syntactic and semantic hierarchies
across attention heads [13]. In contrast, autoregressive mod-
els such as GPT-2 [21] employ unidirectional attention to
optimize generative coherence, demonstrating emergent ca-
pabilities in implicit knowledge representation through next-
token prediction paradigms [21]. While these transformer-
based models dominate contemporary research, recurrent
architectures like LSTM [12] maintain practical relevance
by offering computationally efficient temporal modeling,
particularly in scenarios with sequential output dependencies.

B. Concept Bottleneck Model

CBM [15] represents an innovative approach in deep
learning for image classification and visual reasoning by
introducing a concept bottleneck layer within deep neural
networks. Despite its promising prospects, CBM faces sev-
eral challenges. Firstly, CBM often underperforms compared
to original models without the concept bottleneck layer.
This performance gap arises because CBM cannot fully
extract information from the raw data into the bottleneck
features. Additionally, the expansion of tasks requires CBM
models to balance accuracy and interpretability, whereas tra-
ditional models only focus on prediction accuracy. Secondly,
CBM’s effective performance hinges on extensive dataset
annotations, posing a significant barrier to its widespread
application. Researchers have explored various solutions to
address these issues.

For instance, Oikarinen et al. [19] proposed a label-
free CBM to overcome the limitations of traditional CBM,
offering an effective alternative. Additionally, Yuksekgonul
et al. [28] introduced a posterior concept bottleneck model
that can be applied to various neural networks without sacri-
ficing model performance while retaining its interpretability
advantages. Furthermore, Chauhan et al. [4] extended the
application of CBM to interactive prediction environments
by introducing interaction strategies to select annotated con-
cepts, thereby enhancing prediction accuracy.

However, most research on CBM has predominantly fo-
cused on the CV domain, with relatively limited exploration
in NLP. While some progress has been made in applying
CBMs to NLP tasks, significant gaps and opportunities for


improvement remain. Among the few existing studies, the
work by Zhen Tan [25] represents one of the early efforts
to integrate CBMs into NLP tasks. However, this approach
primarily inserts the concept layer prior to label prediction,
which can lead to feature information loss and a decline in
model accuracy. Furthermore, the relationship between the
predicted concepts and the final labels remains ambiguous.
As such, there is substantial potential for advancing CBM
methodologies in NLP by exploring more effective strategies
for incorporating concept layers while preserving feature in-
tegrity and clarifying the derivation process linking concepts
to labels. This line of inquiry could significantly enhance
interpretability in NLP models.

C. Neural Symbolic Reasoning

To address the lack of a clear derivation relationship
between concepts and the final prediction labels in CBM,
we introduce neural-symbolic reasoning techniques [2], [7],
[8], [11]. These techniques bridge the gap between symbolic
reasoning, traditionally used in artificial intelligence systems,
and the data-driven learning capabilities of neural networks.
One method in neural-symbolic reasoning is Neuro-Symbolic
Forward Reasoning (NS-FR) [23]. NS-FR is a hybrid ap-
proach that combines neural networks with symbolic logic
reasoning. It embeds symbolic reasoning rules within neural
networks, allowing for the derivation of new facts in a dif-
ferentiable manner. By incorporating NS-FR, we can better
interpret the relationship between concepts and prediction
labels in CBM, thereby enhancing the model’s interpretabil-
ity and robustness. Additionally, we observe the application
of neural-symbolic Visual Question Answering (VQA) [27],
which explores a method that integrates neural networks
with symbolic reasoning for VQA tasks. This approach
achieves a decoupling of visual understanding, language
understanding, and reasoning abilities, thereby improving
the model’s performance in handling complex question-
answering tasks. This method not only enhances the accuracy
of VQA models but also provides a more transparent and
interpretable decision-making process.

Inspired by these works, we explore the introduction of
neural symbolic reasoning into the framework of text inter-
pretable classification to further improve the interpretability
and model performance. By incorporating the principles of
NS-FR and neural-symbolic into our research on interpret-
ing pretrained language models, we aim to achieve similar
advantages within the CBM framework. This integration is
expected to clarify the relationship between concepts and
prediction labels, thereby increasing the practical value of
the model.

II. METHODS
A. Preliminaries

Overview of Concept Bottleneck Models (CBMs). Utiliz-
ing the notational framework described in [15], we delve
into a classification scheme characterized by a set of defined
concepts C = pi,...,px. The pertinent dataset is denoted
by c = {pi,---,pre}, where x; € R@ denotes the input

features for each instance i in the index set [N], y; € R@
signifies the associated target labels (with d, indicating the
total number of classifications), and c; € R* encapsulates the
concept vectors. Here, the element c;; within c; measures the
significance of concept p; for instance 7. The aim of CBMs
is to develop two pivotal mappings: firstly, the transformation
h : R? — R*, which converts the feature space into concept
space; secondly, the function m : R* > R*, which inter-
prets the concept space into prediction outputs. The primary
goal is to ensure that the estimated concept vector ¢ = h(x)
and the resulting classification ¢ = m(h(x)) closely match
their actual counterparts, effectively embodying the core
principles of CBMs.

Fuzzy logic rules. Traditional Boolean logic operates on
discrete truth values {0,1}, limiting its capacity to handle
uncertain concept interactions. Fuzzy logic [9] addresses this
by introducing continuous truth degrees through three core
operators:

- Conjunction (A): Implemented via t-norms, with the
product operator a /\ b = ab ensuring differentiable gradients
- Disjunction (V): Defined through t-conorms using a V b =

a+b—ab
- Negation (—): Standard negation -a = 1 — a
Consider classifying apple” using concepts
{ Cred; Cround; Corisp +- A fuzzy rule might express:
Yapple = (Crea A Cound) Vv Corisp (1)

This captures two pathways: 1) non-red coloration with
round shape, or 2) crisp texture regardless of color. For an
apple with partial redness (Creq = 0.4), perfect roundness
(Crounad = 1), and moderate crispness (Cerisp = 0.7):

Yapple = (0.6A1)V0.7 = (0.61)V0.7 = 0.6+0.7—0.42 = 0.88
(2)
The continuous formulation enables gradient-based opti-
mization of concept compositions while preserving logical
interpretability [2]. This reconciles discrete rule-based rea-
soning with neural feature learning through differentiable
surrogates of logical operators.

B. Pretrained Language Model

Our objective is to predict the target label y given an input
text x. We employ a parameterized pre-trained language
model as the foundation of our approach. Initially, the input
text x is fed into the model, which encodes it into a latent
representation z. This process can be formulated as:

Loz y (3)

where x is transformed into the latent representation z,
which is subsequently mapped to the predicted label y. The
overall performance of the task is primarily determined by
the accuracy of the predicted y.


C. Concept Layer and Concept Embedding

To enhance the interpretability of the model and provide a
more explainable prediction for the output label y, we intro-
duce the Concept Layer and Concept Embedding approach.
This method explicitly integrates conceptual representations
into the model, ensuring a more transparent decision-making
process.

1) Concept Layer: The Concept Layer explicitly encodes
the state probabilities for each concept into a structured
vector. Given S concepts, the Concept Layer is represented
as:
pes phe pnt

pees pres punk

The Concept Layer vector CL provides a clear interpreta-
tion of concept states, which is then utilized for downstream
representation learning, activation, and embedding computa-
tions.

2) Concept Representation.: Given an input concept fea-
ture vector, we define a set of predicted concepts C* that
capture high-level semantic information relevant to the task.
Each concept cs, € C can take binary values, indicating its
presence or absence in the input. To represent the concept
states, we define:

C-

Ss

C, = fe eR 5)

where Ct represents the active (true) state of the concept,
and Cy represents the inactive (false) state. These embed-
dings are computed through concept-specific transformation
layers applied to the input feature vector:

Ct =o(Wtr+bt), Cy =o0(Wrr+bz) ©

where W,* and W-> are learnable parameters, and o is a
non-linear activation function.

3) Concept Activation and Embedding Computation.:
To determine the likelihood of a concept being active, we
introduce a scoring function:

Ds = 0(W,C, + bq) (7)

where p, represents the probability that concept c, is active.
Using this probability, we construct the final concept embed-
ding as a weighted sum of the active and inactive states:

és, = psCy +(1—p.)Cy € R® (8)

Stacking all concept embeddings, we obtain the complete
concept embedding matrix:

C = Coneat(éy, é@2, ...,ég) € R°*° (9)

4) Integration with the Prediction Model.: Instead of
relying solely on concept-based predictions, we treat the
concept embeddings as an augmentation of the original
feature representation. The fused representation is computed
as:

F* (x) = ReLU(W/ [x, C] + b1) (10)
where W; and 6; are learnable parameters. The final classi-
fication output is then obtained via a linear transformation:

y* (2) =w" F*(x) +b (11)

D. Neural-Symbolic Concept Reasoning

Given an input instance x with its unified concept embed-
ding matrix C € RICIX¢, where C denotes the concept set and
e the embedding dimension, we establish interpretable deci-
sion rules through differentiable logic operations. Inspired
by recent advances in neuro-symbolic integration [2], our
framework employs two complementary neural operators to
model concept interactions:

1) Concept Polarity Network.: (®;,5 : R° — [0,1]): For
each target class 7 and concept c,; € C, this two-layer
MLP learns to quantify the directional influence of concept
embeddings through sigmoid activation:

where I,,,,; —> 1 indicates positive correlation with class j,
while J, .,; + 0 suggests inhibitory effects.

2) Concept Relevance Network.: (V;,5 : R° — [0,1)):
This parallel network architecture with identical depth com-
putes context-aware concept importance weights:

Ine,j =O (Vi? -ReLU(V(P6, +d) + a?) (13)

Higher I,,.; values signify stronger evidential support

from concept c; for class 7 in instance 2.

The final prediction rule synthesizes these signals through
fuzzy logic operations:

yj (x) = \ (7Ip,s,j V Ir,s,j) = mu {max (1 — Ip,s,j, Lr,s,j)}

cCsEC
(14)
To optimize this neuro-symbolic layer, we minimize a
cross-entropy loss over the rule-based predictions:

(15)

eneural = U(n,y)~D [éce (y(x), y)|

Although we can get predictions via neural symbolic
layer, we can also see such prediction is purely based on
the concept rules we have learned without fully leveraging
the feature vectors, indicating that directly using such a
classifier will have bad performance. In the final step of our
framework, we will improve the final performance.


E. Final Prediction and Network Optimization

The overall training objective consists of multiple compo-
nents: standard classification accuracy, concept learning, and
explainability constraints. The loss function is formulated as:

L= Lpreiliction + an Leoncept + G2L nueral (16)

where Lprediction = illce (y* (@), y)], Leoncept =
Elce(C* j C)| and Lrueral = hlfce (y* (x), y)I.

IV. EXPERIMENTS

A. Experimental Settings

1) Datasets: The dataset utilized in this study is derived
from the CEBaB dataset, focusing on sentiment classification
for restaurant reviews. Specifically, we used the augmented
version of the dataset, referred to as aug-CEBaB-yelp. This
dataset consists of two components: a source concept dataset
(D,) containing human-annotated concepts, such as Food,
Ambiance, Service, and Noise, and an unlabeled concept
dataset (D,,) derived from Yelp reviews, providing a large
number of samples for augmentation. Following augmenta-
tion, the dataset is transformed into D = {Deas Du}, where
Dsq combines human-annotated concepts with augmented
concepts having noisy labels, and D,, includes automatically
labeled data containing both human-specified concepts and
ChatGPT-generated concepts. Each concept in the dataset
takes one of three possible values: Positive, Negative, or
Unknown.

To enrich the dataset, we applied a data augmentation
process consisting of concept set augmentation and noisy
concept label annotation. For concept set augmentation,
the goal was to generate additional high-quality concepts
to expand the original concept set (C;). Using ChatGPT
and employing an in-context learning strategy, we provided
human-specified concepts as references to guide the gener-
ation of new concepts. For example, the prompt, “Besides
{Food, Ambiance, Service, Noise}, what are the additional
important features to evaluate a restaurant review?” was used,
where Food, Ambiance, Service, and Noise are manually
annotated concepts from C,. ChatGPT-generated concepts,
such as Cleanliness and Menu Variety, were then filtered to
remove irrelevant or rare outputs, resulting in an augmented
concept set (C4).

In the noisy concept label annotation step, ChatGPT was
employed to annotate unlabeled data with noisy labels for
both human-specified and ChatGPT-generated concepts. This
was achieved by designing prompts that provided human-
annotated examples as context. For instance, a sample prompt
is: “a. According to the review {textl}, the {conceptl} of
the restaurant is positive. b. According to the review {text2},
the {concept2} of the restaurant is negative. c. According
to the review {text3}, the {concept3} of the restaurant is
unknown. d. According to the review {texti}, how is the
{concepti} of the restaurant? Please answer with one option
in positive, negative, or unknown.’ Using such prompts,
ChatGPT annotated the concepts for unlabeled reviews,
including both original concepts from D, and new concepts

from Cy. The resulting D,, dataset combines noisy labels
for both human-specified and ChatGPT-generated concepts,
enhancing the diversity and scope of the data. The augmented
dataset D integrates high-quality human annotations with
a comprehensive set of ChatGPT-generated concepts and
noisy labels, providing a robust foundation for training
and evaluating sentiment classification models for restaurant
reviews.

2) Backbones: Four distinct language models
adopted as backbones for our study.
Bert-base-uncased, developed by Google, is a bidirectional
Transformer model. With 12 layers, 768 hidden units and
12 attention heads, it extracts context from both directions,
leveraging pre-trained knowledge on diverse corpora for
sentiment understanding.

Roberta-base is an optimized variant of BERT. Despite
sharing a similar architecture, it’s trained with larger batch
sizes and longer schedules, enabling better generalization to
our dataset.

Gpt2, developed by OpenAI, is an autoregressive model.
Different from BERT-like models, it generates text token-
by-token. The base version, with 12 layers, 768 hidden
units and 12 attention heads, can capture complex semantic
dependencies.

Lstm, a type of recurrent neural network, processes sequen-
tial data. Its memory cell structure allows it to capture long-
term dependencies in review text, learning sentiment-related
patterns during training on our dataset.

These backbones’ performance with and without the
CLMN structure helps assess the effectiveness of our ap-
proach.

3) Metrics: To evaluate both the utility and interpretabil-
ity of our models, we use the following metrics:

Task Performance Metrics:

were

e Task Accuracy: This metric measures the proportion
of correctly predicted task labels, providing a straight-
forward evaluation of classification performance.

e Task Macro F1 Score: The macro FI score is calcu-
lated as the harmonic mean of precision and recall in
all classes, equally weighted regardless of class size.
This metric is particularly suitable for datasets with
imbalanced class distributions.

Interpretability Metrics:

e Concept Accuracy: This metric evaluates the model’s
ability to accurately predict concept labels, reflecting
the interpretability of the learned representations.

e Concept Macro F1 Score: Similar to the task-level
Fl score, this metric measures the harmonic mean
of Precision and Recall at the concept level, offering
a balanced evaluation of interpretability performance
across all concepts.

These metrics are used to assess the trade-off between
interpretability and utility across various datasets and training
strategies.

4) Experimental Setup: In our experiment, we conducted
parameter experiments to evaluate the impact of different


TABLE I
PERFORMANCE COMPARISON BETWEEN BACKBONE MODELS AND THEIR CORRESPONDING CLMN OUTPUTS.

Backbone CLMN
Model Acc Fl O-Acc O-F1 C-Acc C-Fl R-Acc R-Fl
bert-base-uncased 69.49 79.72 69.26 79.62 85.85 84.63 65.35 76.49
roberta-base 71.21 80.92 71.56 81.16 87.52 86.09 64.68 76.51
gpt2 63.39 75.39 63.60 76.39 87.12 85.18 62.72 75.76
Istm 47.54 65.65 47.19 64.03 75.65 66.60 38.08 57.10

O = Output-level (final prediction), C = Concept-level, R = Reasoning-level. Acc = Accuracy, Fl = Macro F1.

TABLE II
ABLATION STUDY ON CONCEPT_WEIGHT AND Y2_WEIGHT PARAMETERS.

Model Acc Macro Fl Concept Acc Concept Macro Fl ‘Test Acc Test Macro F1
concept_weight=0, y2_weight=0 68.63 78.90 71.76 44.61 8.35 39:52
concept_weight=100, y2_weight=0 69.57 79.63 87.26 85.63 19.59 47.95
concept_weight=0, y2_weight=10 67.51 79.00 26.46 39.90 67.32 78.85
concept_weight=100, y2_weight=10 69.26 79.62 85.85 84.63 65.35 76.49

parameter settings on the model’s performance, as well as
ablation studies to analyze the contributions of individual
components or modules to the overall effectiveness of the
approach. Through several rounds of parameter tuning and
comparative analysis, we set the maximum token number
of input to 512, the number of training epochs to 25, and
the batch size to 8. The weights for the concept loss and
neural loss were set as a, = 100 and a2 = 10, respectively.
The learning rates were configured individually for different
backbone models: 1e—5 for BERT-base and RoBERTa-base,
le—4 for GPT-2, and 1e—2 for the LSTM-based architecture.

B. Model Utility AND Interpretability

The experience and food was good but loud, and the portions of the
food (especially the sides) were not filling for the price.

OOO0O® - @) fern

food noise _ price location |

food & ~noise & ~price & service & ... & location -> *K xk KK

Fig. 2.

service |.

Interpretability Example

According to Table I, the BERT model integrated with
the CLMN framework achieves concept prediction accuracy
(Acc) and Macro FI scores of 85.85% and 84.63% respec-
tively, demonstrating effective concept recognition capabili-
ties. For final task prediction, the reasoning network attains
65.35% Acc and 76.49% Macro F1 through concept-based
inference, indicating the model’s capacity to leverage inter-
mediate concepts for interpretable decision-making rather
than relying solely on black-box predictions. The 3.91%
accuracy gap between direct prediction (69.26% Acc) and
concept-driven reasoning suggests certain final labels cannot
be fully explained through the identified concepts. By strate-
gically employing direct prediction for final outputs while
retaining concept reasoning traces, the model maintains

competitive performance (69.26% Acc, 79.62% Macro F1)
comparable to non-interpretable backbones, while providing
explicit derivation pathways for most cases. This architecture
successfully preserves classification accuracy within 0.3%
relative degradation compared to baseline models, proving
that interpretability requirements can be incorporated almost
without compromising final task performance. The residual
reasoning gap highlights potential directions for strengthen-
ing concept-task alignment in future work.

C. Ablation Study

To discuss the functions of each component in more
detail, we conducted a series of comprehensive ablation
experiments. Our findings reveal that the introduction of the
concept loss barely weakens the performance of the model
on the final task. In contrast, the incorporation of only the
neural symbolic component leads to a certain degree of
reduction in the final prediction ability of the model. This
is primarily because the additional task effectively diverts
the attention allocation during the model’s training process.
Without the control of concept prediction, the consistency
between the two types of predictions is relatively weak.
However, when both the concept loss component and the
neural symbolic component are added simultaneously, the
model’s final prediction ability is almost on par with that of
the baseline model. Moreover, it maintains the correctness of
the reasoning rules of the neural symbolic component, sig-
nificantly enhancing the interpretability of the entire model.
Detailed results of the ablation study are presented in Table
IL.

D. Interpretability

The example in Figure 2 demonstrates the interpretabil-
ity visualization process of the CLMN framework. In the
original sentence, phrases such as ’food was good”, ”loud”,
and ’not filling for the price’ provide evidence for concept
predictions corresponding to food, noise, and price respec-
tively. The Unknown labels in the concept layer, while poten-
tially participating in subsequent neural symbolic processes,
exhibit less significant contribution tendency compared to


definitive concepts like food and loud during rule formation.
The construction of symbolic rules requires comprehensive
consideration of the entire training dataset to ensure gener-
alization capability.

V. CONCLUSION

This work presents CLMN, a neural-symbolic framework
that bridges performance and interpretability in NLP through
continuous concept embeddings and fuzzy logic operations.
By replacing traditional binary concept activations with
learnable neural-symbolic operators, CLMN preserves con-
textual semantics while enabling dynamic concept interac-
tions and transparent rule extraction. Experiments across
multiple models and datasets show CLMN achieves competi-
tive accuracy with superior concept alignment, demonstrating
that interpretability need not compromise performance. The
framework’s decoupled prediction-explainability architecture
offers a viable path for deploying trustworthy AI in health-
care and finance, with future extensions targeting hierarchical
concept reasoning and multilingual adaptability.

REFERENCES

[1] M. Bansal, M. Kumar, M. Sachdeva, and A. Mittal. Transfer learning
for image classification using vgg19: Caltech-101 image data set.
Journal of ambient intelligence and humanized computing, pages 1—
12, 2023.

[2] P. Barbiero, G. Ciravegna, F. Giannini, M. Espinosa Zarlenga, L. C.
Magister, A. Tonda, P. Lio, F. Precioso, M. Jamnik, and G. Marra.
Interpretable neural-symbolic concept reasoning. In Proceedings of
the 40th International Conference on Machine Learning, volume 202,
pages 1801-1825. PMLR, 2023.

[3] N. Bussmann, P. Giudici, D. Marinelli, and J. Papenbrock. Explainable

machine learning in credit risk management. Computational Eco-

nomics, 57(1):203—216, 2021.

[4] K. Chauhan, R. Tiwari, J. Freyberg, P. Shenoy, and K. Dvijotham.

Interactive concept bottleneck models. In Proceedings of the AAAI

Conference on Artificial Intelligence, volume 37(5), pages 5948-5955,

2023.

[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training

of deep bidirectional transformers for language understanding. arXiv

preprint arXiv: 1810.04805, 2018.

[6] R. ElShawi, Y. Sherif, M. Al-Mallah, and S. Sakr. Interpretability

in healthcare: A comparative study of local machine learning inter-

pretability techniques. Computational Intelligence, 37(4):1633-1650,

2021.

[7] A. Garcez, M. Gori, L. Lamb, L. Serafini, M. Spranger, and S. Tran.

Neural-symbolic computing: An effective methodology for principled

integration of machine learning and reasoning. Journal of Applied

Logics, 6(4):611-632, 2019.

[8] A. d. Garcez, S. Bader, H. Bowman, L. C. Lamb, L. de Penning,

B. Illuminoo, and H. Poon. Neural-symbolic learning and reasoning:

A survey and interpretation. Neuro-Symbolic Artificial Intelligence:

The State of the Art, 342(1):327, 2022.

[9] P. Hajek. Metamathematics of fuzzy logic, volume 4. Springer Science

& Business Media, 2013.

10] V. Hassija, V. Chamola, A. Mahapatra, A. Singal, D. Goel, K. Huang,

S. Scardapane, I. Spinelli, M. Mahmud, and A. Hussain. Interpreting

black-box models: a review on explainable artificial intelligence.

Cognitive Computation, 16(1):45-74, 2024.

11] P. Hitzler, A. Eberhart, M. Ebrahimi, M. K. Sarker, and L. Zhou.

Neuro-symbolic approaches in artificial intelligence. National Science

Review, 9(6):nwac035, 2022.

12] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural

computation, 9(8):1735—1780, 1997.

13] G. Jawahar, B. Sagot, and D. Seddah. What does bert learn about

the structure of language? In ACL 2019-57th Annual Meeting of the

Association for Computational Linguistics, 2019.

14] M. Keber. Neuro-symbolic ai: Integrating symbolic logic into deep
neural networks.

[15

[16

[17

[18

[22

P. W. Koh, T. Nguyen, Y. S. Tang, S. Mussmann, E. Pierson, B. Kim,
and P. Liang. Concept bottleneck models. In International conference
on machine learning, pages 5338-5348. PMLR, 2020.

K. Lin and Y. Gao. Model interpretability of financial fraud detection
by group shap. Expert Systems with Applications, 210:118354, 2022.
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert
pretraining approach. arXiv preprint arXiv: 1907.11692, 2019.

B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,
E. Agirre, I. Heintz, and D. Roth. Recent advances in natural language
processing via large pre-trained language models: A survey. ACM
Computing Surveys, 56(2):1-40, 2023.

T. Oikarinen, S. Das, L. M. Nguyen, and T.-W. Weng. Label-free
concept bottleneck models. arXiv preprint arXiv:2304.06129, 2023.
T. P. Quinn, S. Jacobs, M. Senadeera, V. Le, and S. Coghlan. The three
ghosts of medical ai: Can the black-box present deliver? Artificial
intelligence in medicine, 124:102158, 2022.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog,
1(8):9, 2019.

H. Shao, L. Wang, R. Chen, H. Li, and Y. Liu. Safety-enhanced
autonomous driving using interpretable sensor fusion transformer.
In Concept Embedding Models on Robot Learning, pages 726-737.
PMLR, 2023.

H. Shindo, D. S. Dhami, and K. Kersting. Neuro-symbolic forward
reasoning. arXiv preprint arXiv:2110.09383, 2021.

A. Singh, S. Sengupta, and V. Lakshminarayanan. Explainable deep
learning models in medical image analysis. Journal of imaging,
6(6):52, 2020.

Z. Tan, L. Cheng, S. Wang, B. Yuan, J. Li, and H. Liu. Interpreting
pretrained language models via concept bottlenecks. In Pacific-Asia
Conference on Knowledge Discovery and Data Mining, pages 56-74.
Springer, 2024.

Y. Wu, Y. Liu, W. Lu, Y. Zhang, J. Feng, C. Sun, F. Wu, and
K. Kuang. Towards interactivity and interpretability: A rationale-
based legal judgment prediction framework. In Proceedings of the
2022 conference on empirical methods in natural language processing,
pages 4787-4799, 2022.

K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. Tenenbaum. Neural-
symbolic vqa: Disentangling reasoning from vision and language
understanding. Advances in neural information processing systems,
31, 2018.

M. Yuksekgonul, M. Wang, and J. Zou. Post-hoc concept bottleneck
models. In The Eleventh International Conference on Learning
Representations, 2022.

M. E. Zarlenga, P. Barbiero, G. Ciravegna, G. Marra, F. Giannini,
M. Diligenti, Z. Shams, F. Precioso, S. Melacci, A. Weller, et al.
Concept embedding models. arXiv preprint arXiv:2209.09056, 2022.
X. Zhang and V. S. Sheng. Neuro-symbolic ai: Explainability,
challenges, and future trends. arXiv preprint arXiv:2411.04383, 2024.
