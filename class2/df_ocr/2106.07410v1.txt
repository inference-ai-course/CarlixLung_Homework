2106.07410v1 [cs.AI] 14 Jun 2021

arXiv

Model Explainability in Deep Learning Based
Natural Language Processing

Shafie Gholizadeh Nengfeng Zhou
Corporate Model Risk Corporate Model Risk
Wells Fargo, US Wells Fargo, US
Shafie.Gholizadeh @ wellsfargo.com nengfeng.zhou @wellsfargo.com
Abstract

Machine learning (ML) model explainability has received growing attention, especially in the area related to model risk and
regulations. In this paper, we reviewed and compared some popular ML model explainability methodologies, especially those
related to Natural Language Processing (NLP) models. We then applied one of the NLP explainability methods Layer-wise
Relevance Propagation (LRP) to a NLP classification model. We used the LRP method to derive a relevance score for each
word in an instance, which is a local explainability. The relevance scores are then aggregated together to achieve global variable
importance of the model. Through the case study, we also demonstrated how to apply the local explainability method to false
positive and false negative instances to discover the weakness of a NLP model. These analysis can help us to understand NLP
models better and reduce the risk due to the black-box nature of NLP models. We also identified some common issues due to
the special natures of NLP models and discussed how explainability analysis can act as a control to detect these issues after the
model has been trained.

Index Terms

Natural Language Processing, Explainability, Variable Importance, Relevance Score, Surrogate Model

I. INTRODUCTION

Machine Learning (ML) model explainability has become a vital concern in the application of ML to different fields, such
as finance, medical, marketing and recommendation systems etc. When an interpretable model is available, it’s preferred than
a non-interpretable one when their performances are comparable due to its transparency (2016). However, ML
models are usually not transparent and model explainability analysis can help people to understand the ML models better
and trust them more. In the attachment of the Fed - Supervisory Letter SR 11-7 on guidance on model risk management
(OCC) 2011), evaluation of conceptual soundness is one of the required core elements in effective validation framework. “Key
assumptions and the choice of variables should be assessed, with analysis of their impact on model outputs and particular focus
on any potential limitations.” Model explainability analysis such as variable importance and feature effects methodologies can
help to address the conceptual soundness of a ML model. In the real world, NLP models may be deployed to detect emerging
trends, for example, complaints detection or communications monitoring. In such situations, past performance is not fully
informative of performance for intended use case. Ongoing model explainability analysis is needed to detect the variable
importance changes in the underlying models.

Understanding the learning process in black-boxes is a challenging problem, especially in the case that a model takes
hundreds or thousands of variables as input. It is hard to find out which features have the most impact on the model and
how each of them is impacting the model when processing a single input record or dealing with the whole input data records
in general. Here we review some methods of model explainability that can be utilized in natural language processing. All of
these models are designed to find the impact of different features based on which model makes a certain decision about a
single record (Local Explainability). However, the results of each method can be aggregated over all the input instances to
provide the impact of features/tokens in general (Global Explainability) [2018b). As for a case study, we analyze
the performance of Layer-wise Relevance Propagation (LRP) method for a popular text classifier—i.e., SVM working on Yelp
sentiment data set. We also did same analysis to a customer complaint data in the Bank. Because of confidentiality, we only
use the Yelp example in the paper.

II]. EXPLAINABILITY IN MACHINE LEARNING, AN OVERVIEW
Before we get into the special area of natural language processing, we begin with a brief review of some of the most
common methods in model explainability for machine learning.
A. Gradient based sensitivity analysis (GbSA)

The simplest way to relate the inputs of a black-box to its output is to compute the partial derivative of the output with
respect to each input feature. Computing the sensitivity of output to each independent variable is a computationally affordable
method that inspired many more complicated (and often computationally expensive) algorithms.


B. LIME (Local Interpretable Model-Agnostic Explanations) 2016

In LIME algorithm, we assume the black-box model decision function might be so complicated that it cannot be approximated
by a linear or simple model. To explain the model behavior for any instance, we may generate additional input instances in
the local neighborhood of the original instance and ask the model to provide its decision. It is reasonable to assume the model
behavior in a local neighborhood can be approximated be a simpler method like a linear model.

C. Integrated gradients (IG) (Sundararajan et al.\\2017)

In Integrated Gradients method, for each observation x, we assume a baseline x’. Integrate the partial derivative of the output
with respect to each variable in the way from the baseline to the real observation. |Sundararajan et al.) (2017) suggested the
following formula to compute integrated gradient, where 2; is the i-th dimension on instance x and w’ is the reference for the
baseline input.

1 OF (#' +a x (x#—a!
IG,(a«) 4 (s— 21) [ ( ( Ue
a=0 Ox;
Note that in the formula, the integration is on a curve from the reference to the instance, so the result is not trivially the
integration of the first derivative.

D. DeepLIFT (Shrikumar et al.\\2017)

DeepLIFT is quite similar to GbSA, but instead of taking the partial derivatives, it measures the change in model decision
when a variable is changed with its reference. The results provided by DeepLIFT are possibly more stable than GbSA, but it
requires a reference for each variable to compute difference function A.

E. SHAP explanation (Lundberg and Lee} \2017

Assume that we train the proposed model on every subset S of the set of all variables (F'). Then, including and excluding
each variable i in the subset, we may get a relevance score for that variable. Finally, if we aggregate the results for each
variable (weighted average of its performance in different cases), we get a relevance score for that variable. Note that SHAP
can work on any model agnostic methods like LIME or model-specific methods like LRP. Let f(a) denote the score function
for instance x, then let F' denote the set of all input variables. SHAP explanation is driven through the formula

2017) :
_ [SF] = |S| =)!
‘ . FI!
SCF\ {i}

F-. Layer-wise Relevance Propagation (LRP) 2015)

[fsugi3 (@suci}) — fs(2s)]

1. forward computation
input

_, Y Y f

Fig. 1. The flow of LRP relevance scores in the deep neural network 2018) .

LRP provides full decomposition of model prediction for the input to the impacts of input features. Note that LRP does not
use the partial derivatives of the score function, but it decomposes the function itself. In other word, if a basic question in


Sensitivity Analysis is about “What makes this car more or less a car?’”, then the basic question of LRP is “What makes this

image a car?” 2017a) . Despite the wide range of model-agnostic explainability methods, LRP flows through the
model. The flow of LRP relevance scores in a neural network is illustrated in Fig.

There also exist a few other explainability methods in the literature—e.g., QI (Quantitative Input Influence)
2016) , LOCO (leave one covariate out) 2018) , LIME-SUP (Locally interpretable models and effects based on
supervised partitioning) 2018), additive index model for neural network (Vaughan et al.|[2018) .

h

hy Of (xi)
_ aft) _ oD So
f= ‘ oxi - 2 r \ ax,
| Va | Va ‘.
/ afte J WL
auf =e a
(a) (b) (c)
Fig. 2. The impact of h value on the gradient of prediction function.
input : forward computation : relevance propagation i output

convolution/ weighted

a detection redistribution a
ot OOOO0O0OO0O0): g
eS o = 5 CgOooO OOGOOOOO)-> B
£ Os pan ; i
° S i 2 a i a
5 a 72 ©OOCOO OOOO0000O0H> 8
x 2 2 a R
© co : ijt—1 !
= = 2

yeu at}

Ga
—.W >

pay

(©O000 O0000000)>&
090000) V @00eoy >+O0000000->s

weighted Ay
redistribution

Fig. 3. The schema of LRP method working on a CNN classifier 2017a) .

classification

IlI. EXPLAINABILITY IN NLP

Almost all of the methods introduced in the previous section can be used in natural language processing. However, we should
note their limitations for the special case of NLP. For instance, most of the text representations in text mining come in a high
number of dimensions—much higher that most numerical data sets. Also some explainability methods that fit to bag-of-word
models may fail in embedding models and vice versa. Furthermore, text is set of tokens, not variables. The same indices in
different textual records may refer to different entities. For instance, embedding based representation of a textual document is
more like image than numerical data. Even compared to image, text could be much different and sometimes more complicated
to process, e.g., in a CNN image classifier, the convolution kernel size might be 5x5 or 10x10. But for text classification it is
more likely to be 3x300, 2x100 or 2x300. Furthermore, the transition from local explainability to global explainability is not
easy in NLP. The easiest option is usually extracting local contributions for different tokens and then aggregating the results.
It is not easily applicable when a variety of tokens/features are contributing to the model decisions. Calling for explanation,
we usually demand a paragraph, a plot, or at most one page that summarizes the model performance. By aggregating all the
tokens’ impacts, we obtain a global explanation in the form of an annotated vector with the size of dictionary. Note that even
an absent word may be a significant determinant of the final prediction, because of its absence. Finally, the joint effect of
tokens (n-grams and skip-grams impacts) are not easily distinguishable from the individual impacts.

A. Gradient-Based Sensitivity Analysis

Sensitivity Analysis can be directly or indirectly applied to textual data. Dealing with Tf-Idf continuous space, the sensitivity
of the output to each input token can be directly calculated, but it does not work so easily on the embedding spaces. Assume


that we need to use sensitivity analysis to the D-dimensional word embedding representation of text where each token is
represented by D numbers. Applying GbSA, we will get a vector of size D as the sensitivity of the output to each single
token. So, we need to aggregate these numbers to get only one number—the sensitivity of output to the token. As the simple
summation does not make much sense for the gradient vector, the easiest way would be to use the norm of the vector. This
way, sensitivity Analysis decomposes the gradient square norm of the predicting function to the impacts of the tokens. More
precisely, for a token x, GbSA decomposes || 72 fc(x)||2 where f. is the prediction score on class c. On word embedding,
this is the sum of squared partial derivatives in different embedding dimensions d = 1,...D. Let xq denote the d-th element
on the embedding of token x. Then Let R(«) denote the final relevance score of token x. We can easily derive

.
rata) = (2),

Ga) = Rule) =D (5 )

Then we can immediately observe that for embedding-based NLP, this relevance score does not determine polarity, unless
we use some inappropriate way of aggregation, e.g., the simple summation of elements on the gradient vector as the final
relevance score of the token. Note that an even more important criticism on GbSA is not limited to NLP: Why should we
assume that the partial derivatives are meaningful as explanation? Sensitivity does not necessarily imply relevance, it might
imply pure noise. Also, note that the relevance score is extremely sensitive to the parameter h—i.e., the step in numerical
calculation of the partial derivative, as follows.

fe (yy — felt +h) ~ fel)
Oxg h
As shown in Fig.|2| changing the h value may significantly shift the results. In most applied problems, choosing the optimal
h is a trade-off between variance and bias, but bias is not so important in the case of GbSA. So, choosing a large h may
lead to the more stable results. Note that increasing h is only the simplest way. The extensions of sensitivity analysis like
DeepLIFT and Integrated Gradient are addressing the issue in a similar, but more reliable way, though they may require more
computational cost.

B. LRP 2017a\b

The main difference between LRP and GbSA is that LRP decomposes the function itself to sum of the variables’ impacts,
while Sensitivity Analysis decomposes the gradient square norm of the predicting function.

utilized LRP to decompose the decision function of a CNN text classifier and used the relevance scores
to provide highlighted text. Their algorithm is as follows.

1) Forward-propagate numerical representations (word embeddings) through CNN.

2) Backward-propagate output using LRP.

3) Pool the relevance score associated to each variable/word.

The authors suggested different back-propagation ways for different layer types:

e Dense Layers: Redistribute relevance score proportional to the network weights and the inputs of the layer.

e Max Pooling Layers: Use the ArgMax in the previous layer and go for winner-takes-all.

e Convolutional Layers: Redistribute relevance score proportional to the network weights and the inputs of the layer.

e Input Layer: Relevance score of each word is the simple sum of its embedding elements’ scores.

A schema of LRP method on CNN classifier is shown in Fig. |3] Note that LRP is unable to distinguish the individual
impacts of tokens and the joint effect of the n-grams and skip-grams. But, maybe an extra step of statistical analysis is enough
to retrieve the most important joint effects. Also, the algorithm as described can only work on feed-forward networks. But
(2017b) extended the mentioned algorithm to work on recurrent neural networks as well. The main difference is
the presence of source/gate connections in RNNs, as shown in Fig.

C. Going back from convolutional filters to ngrams

(2018) examined the hypothesis that in a CNN classifier, the filters followed by max-pulling capture the
fingerprint of the ngrams and extract the relevant ones respectively, then the rest of the network uses those features to classify
the records. The authors retrieved the ngrams that could pass the filter’s threshold and clustered them according to their
activation patterns. This way they derived a concrete identity for each filter. They showed that max-pulling is discriminating
the outputs of convolutional layer—i.e., fingerprints of the relevant and non-relevant ngrams. So, their work can be utilized as


Many-to-one weighted linear connection Two-to-one multiplicative interactions

Forward propagation: Z; = Z,°Z,
Ze Gate Neuron, value ranges in [0,1]
Z,; Source Neuron

Forward propagation: z; = 2; Z;-wy + b;

O

O

LRP Backward propagation: LRP Backward propagation:

Zp tdy + psslonl) + 0% R,=R,

Z +e ere R,=0

Fig. 4. Handling different types of connections in RNN based LRP 2017b) .

TABLE I
PROS AND CONS THE DIFFERENT EXPLAINABILITY METHODS.
Model Distinguish Computational Cost Model Notes
Joint impacts Agnostic
Cheap Expensive Unaffordable
GbSA x xX Results might be unsta-
ble in some cases
LRP x
LIME x xX
SHAP x x x
Integrated gradi- x xX Requires a_ reference
ents for each variable
DeepLIFT x x Requires a_ reference
for each variable
Map conv. filters x
to ngrams*

an explainability method to retrieve important ngrams. Generally the idea is quite similar to the LRP method in the sense of
model dependency and trying to go back through the network. However the formulation is different here, and also the final

results are important ngrams. |Zhao et al.| (2020) used the max-pooling concept in (2018) to extract features in the
convolutional layers and utilize SHAP to generate local explainability in text classification models.

D. Self-interpretable CNN for Text Classification

developed an approach for interpreting convolutional neural networks for text classification problems
by exploiting the local-linear models inherent in ReLU-DNNs. To get an overall self-interpretable model, the system of local
linear models from the ReLU DNN are mapped back through the max-pool filter to the appropriate n-grams. The proposed
technique can produce parsimonious models that are self-interpretable and have comparable performance with respect to a
more complex CNN model. This approach is using the exact coefficients of local linear models from ReLU networks.

E. Other explainability methods in NLP

Table|l]shows a brief comparison of the pros and cons of some explainability methods. Here we compare only a few methods,
but note that all of the methods that were mentioned for general machine learning explainability can be utilized for the specific
task of text mining models and actually they have been used in the literature. Also, many other methods specifically designated
for NLP models are available in the literature. (2018) applied visualization techniques to examine how a
convolutional neural network can distinguish NLP features and how each feature is contributing to the model performance.
used fine-grained information to help explain the decision made by classification model. The framework
learns to make classification decisions and generate fine-grained explanations at the same time. It uses an explainable factor
and the minimum risk training approach that learn to generate more reasonable explanations. Their algorithm needs a set of
golden explanations as input. (2017) exploited the internal neurons activation space. They used the activation
on/off space as a source for KNN search in explanatory training data instances. The Authors retrieved explanatory records


with the most similar neural layer activations. For validation, they used some semantic similarity between the test records and

explanatory candidates. We also refer the reader to the other works 2018} {Futrell et al.| [2019
2019 2018) that are highly related to the subject of model explainability in natural language processing.

IV. A CASE STUDY

TABLE II
CONFUSION MATRIX COMPARING THE PREDICTIONS OF SVM MODEL AND ACTUAL LABELS (1 IS FOR BAD REVIEW).

Train Prediction Test Prediction
n=10,000 n = 20,000
SVM=0 | SVM=1 | SVM=0 | SVM=1
Actual = 0 4,286 789 8,239 1,703
Actual = 1 657 4,268 1,520 8,538
Fl-score 0.86 0.84
TABLE III
CONFUSION MATRICES COMPARING THE PREDICTIONS OF CNN SURROGATE MODEL AND THE ACTUAL LABELS OR SVM PREDICTIONS.
Train surrogate
n=10,000
CNN = 0 CNN = |
Actual = 0 4,385 690
Actual = | 753 4,172
Fl-score 0.86
SVM = 0 4,684 259
SVM = 1 454 4,603
Fl-score 0.93

As a case study on the performance of LRP methodology, we used a SVM classifier to run over real world NLP data. The
data set is provided by Yelp as part of a 2013 data set challenge for training and testing the prediction models.
The data set includes 229,907 reviews from Phoenix and contains information about 11,537 businesses, 43,873 users. The
label of review data is customer’s rating from | star to 5 stars. In this case study, we will focus on binary class classification
problem. We transferred the label into binary case. We will relabel star 1 and star 2 as new label 1 (bad review). These are
more important events which can help to improve the business practice by learning from these complaints. We will relabel
star 4 and star 5 as new label 0 (good review). We did not include star 3 data in this analysis.

In the training data we sampled 10,000 observations, using stratified random sampling to get equal number of observation
for each star. There are two reasons that we did not use a very large training data in this analysis. In many real problems, the
labeled data is very expensive to obtain. A training sample size of 10,000 is closer to many real problems. The second reason
for choosing small sample size is that this can help us to evaluate if the SVM model has some over fitting issues and how the
over fitting can impact the model explainability when the model is applied to out of samples.

In the evaluation data we sampled 20,000 observations with same sampling method as training (actual labels available). In
many other real problems, the evaluation data is usually unlabeled. To mimic the real challenge of unlabeled data, we only
need the predicted labels of the evaluation data in the NLP explainability analysis. Note that, in real problems with only large
number of unlabeled data, we can use the trained SVM model to predict the labels for unlabeled evaluation data set first. The
unlabeled data could be very unbalanced as well. We can select some records with the highest probability of positive class
and negative class to create a relative balanced data set.

We used the 300d word embedding of these observations (provided by pre-trained vectors of ConceptNet NumberBatch
v17.06) and got the average of 300 embedding dimensions of each observation to feed into the SVM model. Table || show
the confusion matrix on the training data and evaluation data comparing the predictions of SVM model and actual label (1
is for bad review, 0 for good review). The model’s Fl-score was 86% for training data and 84% for evaluation data. The
performance on evaluation data is very comparable with the training data for the SVM model.

For the next step, we tried a permutation based approach to get the importance of tokens in SVM classifier. To do this, for
each observation, we removed the tokens one-by-one to see how the probability that model assigns the record to the positive
class changes. The contribution of all tokens were kept in a separate list and finally used to provide the list of most influential


TABLE IV
TOP TOKENS AND THEIR NORMALIZED SCORES RETRIEVED BY SVM PERMUTATION BASED APPROACH AND LRP METHOD ON SURROGATE CNN.

SVM Permutation on Evaluation Data CNN-based LRP on Evaluation Data

Rank Word | SVM Score | LRP Score Rank | Word LRP Score | SVM Score
1 mediocre 1.00 0.75 1 | laid 1.00 0.86
2 pathetic 0.96 0.77 2 | unfortunate 0.96 0.68
3 lackluster 0.96 0.96 3 | lackluster 0.96 0.96
4 confusing 0.90 0.41 4 | disappointing 0.85 0.89
5 flavorless 0.90 0.52 5 | crappy 0.80 0.83
6 | disappointing 0.89 0.85 6 | outrageous 0.77 0.28
7 poor 0.86 0.61 7 | pathetic 0.77 0.96
8 worse 0.86 0.61 8 | lacking 0.76 0.77
9 shitty 0.83 0.76 9 | shitty 0.76 0.83
10 crappy 0.83 0.80 10 | mediocre 0.75 1.00

Input Text

Fig. 5. A schema of the CNN classifier.

tokens, i.e., tokens with the highest positive average impact to the positive class. We took this approach on the positive records
of the training data and the positive records of the evaluation data separately.

For the LRP methodology, we used a surrogate CNN model shown in Fig. |5| We trained the CNN model on all of the
records on the training and evaluation data set where the labels were those predicted by SVM. Table [II] show the confusion
matrices comparing the predictions of CNN surrogate model and the actual labels or SVM predictions. The CNN surrogate
model predicted the actual label pretty well, even it’s trained using the SVM prediction as the dependent variable. The CNN
surrogate model and the SVM prediction are very consistent, with a Fl score of 0.93.

Then we used the python package iNNvestigate to run LRP on both positive records of the training data

5. High
High
wonderful e secec en a
lousy | ° ed Po
crappy | . ee cc wee womens» cee fabulous © 000 © wee cee oa efemneee = mame co |
disgusting Sih A stemece me eee ‘ awesome oe er efi
mediocre © 0 00 wee ew 1 secon oon meen emnen © ce lovely © C00 ae ee wecce cee co |~ wma eaawe . |
tasteless Gil Bin. Gi. UaTOBNaER eae oo mew oom = friendly © ee cet eta pcmeamencm penance
disappointing 2° com wait one pene ore eo great . a
poor © 00 cocmmen aoe metas ne superb oe coe re ne e | a
wv
z i | oO
awtal ee # amazing a ne 8
worse 00 © wmen © @ creme com em eee oes co fantastic ee 0000 ese ceueesmEmeD aoe
horrible 04 ce em © come seme cme meme oa oe delightful oe ew wives + ee: a ow |
poorly oe we eee -{- ence om delicious ° en te ee |
lackluster ee a | | mmemee 66 favorite oe Ss ene
nasty  ceadiiemaliiaimatiaiia — ——n favorites eee Ce smmeitesomee + owe le
terrible cclooe @ aman coe coe e yummy ccs oo wo 4 ee
annoying |“ eee ee Se oe terrific ° eo soree conn afe eese « we ce}
t T Tr T T T T T Low |
0.0 0.5 1.0 15 2.0 2:5 3.0 35 z t Low

Impact on model output

25  -20 -15 -10 -05 0.0

Impact on model output

35 —3.0

Fig. 6. Top unigram contributing to bad review (left) and good review (right), where blue dots represent predicted good reviews by SVM model.


TABLE V
TRAINING DATA RECORDS HIGHLIGHTED BASED ON THE LRP SCORES, WHERE RED TOKENS ARE CONTRIBUTING TO THE CLASS | (BAD REVIEW).

ID Star SVM | CNN |} Actual Label = 1 (Bad, Star = 1, 2), SVM Pred. Label = 1 (Bad)

1 1 0.98 | 1.0 Getting worse not better 1 30 appointment for a diagnostic and charge the AC got the car back after 6 00pm Sent a
poor 17 year old kid to pick us up as their courtesy driver once it was finally ready to go

2 1 0.99 | 1.0 Unfortunately there is nothing special about this place My husband got the french dip and myself the mushroom panini
Mine was rather disappointing the mushrooms were minced so tiny and the flavor was semi reminiscent of canned cream

of mushroom soup on a sandwich I hate leaving bad reviews but it wouldn t help anyone if i lied sorry

3 2 1.0 0.99 | Over priced and ‘mediocre food

4 2 1.0 1.0 The nasty youngster working at the Wetzell s Pretzel counter ruined it man She was all pissed at me because she
misheard my order |and I bothered. her to give me the right kind of pretzel Lame Grow up little girl Rude

5 1 1.0 1.0 Duh what a wasteland of ‘crappy products Gift card forced me to pop by in disguise

TABLE VI
FALSE POSITIVE INSTANCES HIGHLIGHTED BY TOKENS’ LRP SCORES, WHERE RED TOKENS ARE CONTRIBUTING TO THE CLASS | (BAD REVIEW).

ID Star SVM |} CNN |} Actual = 0 (Good, Star = 4, 5 ), SVM Pred. Label = 1 (Bad)

1 5 0.99 | 0.94 | I have never had a bad meal or ‘poor service at any Ono location I really like there food and service

2 5 0.97 | 0.99 | It s your standard Dairy Queen Delicious ‘crappy for you food we all grew up on Remember getting sundaes in little
baseball helmets

3 4 0.95 | 0.89 | Great food nice people and mediocre service I enjoy this place often

4 4 1.0 | 1.0 | Not bad Had better had [IIR will be back
5 5 0.59 | 0.37 | Wonderful Wonderful If you leave here less than fullfilled I think you have a food problem There is so much
here you can t possibly about not having enough to eat Sushi lovers this is the best place in town

and positive records of the evaluation data separately. The list of the most influential tokens retrieved by permutation approach
and CNN-based LRP are shown in Table [IV] (based on the evaluation data). For simplicity of comparison the corresponding
score in the rival method is shown to the right of each model score. Note that to filter the rare tokens, we used a threshold of
20. Also, a few training records are shown in Table [V] highlighted by their tokens’ LRP relevance scores. The highlights help
us to see clearly why the model predict these reviews as bad ones.

Note that the python package ‘iNNvestigate’ contains the implementations of LRP, sensitivity analysis, etc. Many of these
methods generated very similar results. For LRP method, it works only on Tensorflow Keras-backend. It does not support
networks including SoftMax activation, since SoftMax is not invertible.

A schema of the CNN classifier is shown in Fig. |5} In the CNN model, for the embedding layer, the same pre-trained
word embedding as SVM model (300-D ConceptNet NumberBatch 17.06) is used with the padding size (maximum number
of tokens in text to be used) of 100. The embedding layer is followed by convolutional filters of sizes 2x300, 3x300, 4x300
(150 filters of each size). We used 40% dropout after the max puling. For model training, the batch size was equal to 30 and
the model was trained in five epochs.

A. The special cases of False Negatives and False Positives

Model explainability methods enable us to examine the source of mistakes made by the model. For NLP classification
models, two particular sources of mistakes are lack of enough usable tokens in the input record and model’s naive trust on
particular tokens. A few false and false negative instances in CNN surrogate model are shown in Table and Table
respectively.

The false positives (FP) are good reviews which are predicted as bad reviews by SVM model. Some of these reviews are
short and the model cannot find enough interpretable tokens and/or ngrams and then tries to decide based on whatever that is
available. Some other FP errors are related to combination of negative word with negative sentiment words, such as “never had
a bad meal or poor service” in example | and “can t possibly complain” in example 5 in Table Negative word is usually
related to bad review by itself. The combination of negative word with negative sentiment words is a challenge task to learn
for NLP classifications, especially when the training samples are not very large. The model may get confused dealing with the


TABLE VII
FALSE NEGATIVE INSTANCES HIGHLIGHTED BY TOKENS’ LRP SCORES, WHERE RED TOKENS ARE CONTRIBUTING TO THE CLASS | (BAD REVIEW).

ID Star | SVM| CNN} Actual = | (Bad, Star = 1, 2 ), SVM Pred. Label = 0 (Good)

1 2 0.32 | 0.7 It was alright If you need something quick and close it will do in a pinch Agree with others that the lack of sauce is
Ree | Egg roll was like any other Nothing too special The steamed veggies were pretty |fresh and crisp so that
was a plus

2 2 0.44 | 0.19 | Not bad but ‘not great Service was really friendly and prompt Food was meh maybe I just ordered the wrong thing

3 2 0.25 | 0.3 I ve tried time and time again to like this place but the pizza hmmm is ‘mediocre at best the pazookie ice cream
cookie is igreat and so is the atmosphere

4 2 0.16 | 0.06 | Fell into the it was on Food Network so let s visit trap It was an okay breakfast but the service was pretty
Not worth driving from Phoenix for the food however the drive itself is very nice and they have a oe
which you can eat your eggs

5 2 0.14 | O.1 If all natural means dry and bland then this cupcakes are definitely natural I was super excited to try Lulus and
then was quickly disappointed They were out of everything and the cake texture was like cardboard Whipped frosting
ummm where is my cream cheese Save your 3 and go grab a happy, hour beer

High High
lousy service I: . not_been_disappointed . . | o
was_lousy = ‘|- poor_customer_service . . |: oe
mediocre_service . ° |- ° food_is_mediocre 2 |: :
treated_poorly ° ° |: ° was_mediocre_at ° oe | ooo.
was tasteless ~~ the_poor service ws }-
was_disgusting oe | eee food_was_mediocre oe | come
not_disappointed oo | cee a not_be disappointed . vr geeo ae | _——o a
just_mediocre ° -| =k) 8 was_very_disappointing ° -{- ° 8
some_crappy ¥ |: . - is_mediocre_at oe |: ee =
too_shabby ° | cd poor_service_and ° . + °
of mediocre ee |- ee not_that_bad om | .
rather disappointing ° | - dry_and_tasteless # {- «
and_crappy . { . Service_was terrible ae | | . .
disgusting_| on is_awful_and ° -{ .
poor customer eek f- ace was_mediocre_and - [- oe
| : ; ‘ 2 : : ‘ Low Low
oo 05 10 15 20 25 30 35 40 BE ee OO

Impact on model output Impact on model output

Fig. 7. Top bigrams (left) and trigrams (right) contributing to bad review prediction.

individual role of a token and its joint impact. When the sample size is not large enough, the joint impact of the bigrams or
trigrams may be not strong enough to offset the combination of individual impacts. And the joint impact estimates of these
negative combination words may also be weakened by other positive ngrams in the same review.

The false negatives (FN) in Table are bad reviews which are predicted as good reviews by SVM model. Most of the
errors are caused by positive comment in the review regarding some specific part of the meal or service, such as “The steamed
veggies were pretty fresh and crisp” in example | and “Service was really friendly” in example 2. And most of these reviews
are star 2, which means they are bad overall with something positive. CNN surrogate model’s prediction are consistent with
SVM for most of these cases.

B. Quantitative methods comparison at global level

We can add the effect tokens in ngrams to get into the list of the most influential ngrams for the model. Fig. [6|shows the most
influential unigram contributing to bad review (left) and good review (right). Here the horizontal axis represent the impact of
a unigram on the model. The blue dots represent the records whose SVM model prediction are 0 (good review). The red dots
represent the records whose SVM model prediction are 1 (bad review). For the reviews which contains top tokens contributing
to bad reviews the SVM model usually predicting them as bad. This shows that each of these tokens is very predictive by
itself for bad reviews. The conclusion is similar for the unigrams contributing to good reviews. Since bad reviews are usually
more interesting for the business to improve in the future, we focus on the analysis of bad review tokens in the case study.


TABLE VIII
DECREASE IN RECALL OF THE SVM PREDICTING BAD REVIEWS BY EXCLUDING TOP N CONTRIBUTING TOKENS RETURNED.

Number of || SVM CNN LRP | CNN GbSA SVM CNN LRP | CNN GbSA

removed to- permutation | train data train data permutation eval. data eval. data

kens train data eval. data
0 0 0 0 0 0 0
50 0.206 0.285 0.211 0.209 0.275 0.212
100 0.325 0.353 0.209 0.325 0.330 0.215
150 0.389 0.485 0.273 0.400 0.472 0.269
200 0.462 0.583 0.277 0.456 0.574 0.269
250 0.550 0.655 0.293 0.535 0.650 0.291
300 0.622 0.711 0.381 0.632 0.700 0.390

TABLE IX

CORRELATION MATRIX OF THE SCORES.

Correlations of | SVM perm | SVM CNN CNN CNN GbSA | CNN GbSA
Scores train perm. eval. | LRP train | LRP eval. | train eval.
SVM perm train 1.00 0.96 0.63 0.66 0.47 0.48
SVM perm. eval. 0.96 1.00 0.64 0.67 0.47 0.49
CNN LRP train 0.63 0.64 1.00 0.90 0.31 0.32
CNN LRP eval. 0.66 0.67 0.90 1.00 0.33 0.36
CNN GbSA train 0.47 0.47 0.31 0.33 1.00 0.95
CNN GbSA eval. 0.48 0.49 0.32 0.36 0.95 1.00

Fig. |7| shows the most influential bigrams and trigrams based on LRP scores, respectively. Compared to the top unigram
tokens, the appearance frequencies of bigrams and trigrams are much less lower. The frequencies of reviews with the ngrams
can be seen from the number of red/blue dots in corresponding to the row of the ngram. The observations without the ngram
is replaced with the mean contribution of the non-missing ngrams, shown as the little bar in the middle of each row. Because
of their lower frequencies, individual bigrams and trigrams impact much less number of reviews, compared to unigrams. So
individually they do not have much impact for global variable importance. As we discussed earlier in the combination of
negative words case in the false positive examples, the joint impact of the bigrams or trigrams are usually not strong enough to
offset the individual unigram impact. This can be seen in the case of “not disappointed”, “not been disappointed” and “not be
disappointed”. These ngrams belong to the top ngrams contributing to the bad reviews, due to the individual unigram impact.
Most of these reviews are still predicted as good review in the SVM model, possibly due to other ngrams (contributing to
good reviews) in the same reviews.

Recall Decrease on Excluding Important Tokens

0.7 7 —— SVM_permute_train
—— CNN_LRP train
0.6 | —— CNN GbSA train
—— SVM_permute_eval
0.5 —— CNN_LRP eval
—— CNN_GbSA_ eval
0.4 4 = =

0.34

0.24

0.14

0 50 100 150 200 250 300

Fig. 8. Decrease in recall of the SVM predicting bad reviews of the evaluation data.

Table [IV] clearly provide some global explanations for the SVM model, based on two different methods. To do a comparison
between the two explainability methods, we asked the SVM models to predict the positive records of evaluation data once again,
after excluding top n relevant tokens returned in each of the four cases (Permutation method/LRP methods on train/evaluation

10


data). We tried it for n=50, 100, 150, 200, 250, 300. To have a more meaningful comparison, we also utilized the same python
package (iNNvestigate) to run GbSA as well, and calculated the relevance scores. Then we aggregated the scores to get the
most contributing tokens (i.e. tokens with the highest average contribution) in the training data and the evaluation data found
by sensitivity analysis.

The comparative results are shown in Table The comparison is based on the false negative ratio when we exclude top
tokens. Note that in each case, the change is measured when model is re-predicting the labels for the evaluation data. The top
tokens are calculated from both the training and evaluation data. For example, in the first case, we exclude top contributing
tokens found in the training data and measure the performance change in the evaluation data.

Fig. |8} shows the comparison visually. It can be seen that removing LRP top tokens will decrease the recall rate the most.
For the same method, the top tokens from training data and evaluation data have very similar performance impact. This is
related to their high score correlation as shown in Table

The correlation among different scores are shown in Table To remove noise in the correlation calculation, only the
words with at least 20 appearances in evaluation data are used. We can see that the highest correlation is between training and
evaluation method for the same methodology, which explained their similar performance impacts in Fig. {8} There are some
correlations between the SVM permutation method and LRP method. The GbSA method has low correlations with the other
two methods.

V. COMMON ISSUES TO WATCH OUT FOR NLP MODEL

Due to the special natures of NLP models, the followings are some common issues to watch out. The explainability analysis
can act as a control to detect these issues after the model has been trained.

a. Out-of-Vocabulary Words (OOV), Non English words or spelling errors. For NLP models using embedding, OOV are
those words which cannot be found in the word embedding mapping file. Without proper handing of the missing values, the
model’s performance will be deteriorated and the model prediction for observations with OOV may be poor or biased. In
models using English embedding, Non English words will become OOV and they are very likely to appeared together in the
same observations. This could lead to very poor performance for these Non English observations. Spelling errors belong to
another source of OOV, which need be be checked and fixed. Model’s performance for observations with OOV need to be
measured separately to assess the impact of OOV.

b. Proxies for contextual information — e.g. words from standard disclaimers. These proxies should be removed in the data
pre-processing before modeling. After a model has been created, checking if these proxies are in top features should be done
as part of the variable importance analysis.

c. Proper nouns — e.g. name of CEO. When proper nouns appeared in top features with strong predictive power, assessment
is needed to make sure they are reasonable. If the connection between a proper noun and dependent variable cannot be justified
or the relationship is likely to change in future, these words should be removed in the data and the model need to be retrained.

d. Handling of Negation, Sarcasm, Idioms. NLP model may not be able to handle these situations well, as we discussed
in some of the false positive examples. Large number of observations are needed for a NLP model to learn these special
situations. Some transfer learning methodologies may benefit a NLP model to learn these words/phrases.

e. Asymmetric treatment of demographic proxies. The unstructured data used in NLP model may contain “digit footprint’,
which can be predictive for some sensitive variables such as gender, race or age etc. This can lead to the asymmetric treatment
for different classes of people. It will cause fairness concern if the model’s predictions are unfavorable for the protected class.

f. Quality of the labels in NLP models. The labels used in the NLP model are usually created by people, either by those
produced the data or the some other reviewers. The labels may be not consistent and are subjective to human judgment errors.
This can impact the model’s prediction power, and cause some of the false positive/negative errors. Some assessment of the
label quality is needed as part of the model validation or review process.

VI. DECLARATION OF INTEREST

The authors report no conflicts of interest. The authors alone are responsible for the content and writing of the paper.

VII. CONCLUSION

As we discussed in the overview section and compared the explainability methods in Table |I} Layer-wise Relevance
Propagation is a reliable and still a computationally affordable explainability method. In our case study, we showed that
explainability provided by LRP methodology outperform the gradient only based and permutation based explainability. By
doing the NLP explainability analysis, we can get a lot insight into the black-box NLP models and reduce the risk of using
a wrong or inappropriate model. The false positive and false negative examples highlighted by LRP scores also helped us to
understand why the model made wrong predictions. This can help us to improve the model in the future.

VII. ACKNOWLEDGMENT
We thank Harsh Singhal for useful discussion. We thank corporate risk - model risk at Wells Fargo for support.

11


REFERENCES

Alber, M., Lapuschkin, S., Seegerer, P., Hagele, M., Schiitt, K. T., Montavon, G., Samek, W., Miiller, K.-R., Dahne, S., and
Kindermans, P.-J. (2019). innvestigate neural networks! Journal of Machine Learning Research, 20(93):1-8.

Arras, L., Horn, F., Montavon, G., Miiller, K.-R., and Samek, W. (2017a). “what is relevant in a text document?”: An
interpretable machine learning approach. PloS one, 12(8):e0181142.

Arras, L., Montavon, G., Miiller, K.-R., and Samek, W. (2017b). Explaining recurrent neural network predictions in sentiment
analysis. arXiv preprint arXiv: 1706.07206.

Bach, S., Binder, A., Montavon, G., Klauschen, F., Miiller, K.-R., and Samek, W. (2015). On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140.

Datta, A., Sen, S., and Zick, Y. (2016). Algorithmic transparency via quantitative input influence: Theory and experiments
with learning systems. In 2016 IEEE symposium on security and privacy (SP), pages 598-617. TEEE.

Futrell, R., Wilcox, E., Morita, T., Qian, P., Ballesteros, M., and Levy, R. (2019). Neural language models as psycholinguistic
subjects: Representations of syntactic state. arXiv preprint arXiv: 1903.03260.

Hu, L., Chen, J., Nair, V. N., and Sudjianto, A. (2018). Locally interpretable models and effects based on supervised partitioning
(lime-sup). arXiv preprint arXiv: 1806.00663.

Jacovi, A., Shalom, O. S., and Goldberg, Y. (2018). Understanding convolutional neural networks for text classification. arXiv
preprint arXiv: 1809.08037.

Koupaee, M. and Wang, W. Y. (2018). Analyzing and interpreting convolutional neural networks in nlp. arXiv preprint
arXiv: 1810.09312.

Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L. (2018). Distribution-free predictive inference for regression.
Journal of the American Statistical Association, 113(523):1094-1111.

Liu, H., Yin, Q., and Wang, W. Y. (2018a). Towards explainable nlp: A generative explanation framework for text classification.
arXiv preprint arXiv:1811.00196.

Liu, X., Chen, J., Nair, V., and Sudjianto, A. (2018b). Model interpretation: A unified derivative-based framework for
nonparametric regression and supervised machine learning. arXiv preprint arXiv: 1808.07216.

Lundberg, S. M. and Lee, S.-I. (2017). A unified approach to interpreting model predictions. In Advances in Neural Information
Processing Systems, pages 4765-4774.

Montavon, G., Samek, W., and Miiller, K.-R. (2018). Methods for interpreting and understanding deep neural networks. Digital
Signal Processing, 73:1-15.

OCC (2011). Supervisory guidance on model risk management: Sr _ letter 11-7 attachment.
https://www.federalreserve. gov/supervisionreg/srletters/ sr1107a1.pdf.

Raaijmakers, S., Sappelli, M., and Kraaij, W. (2017). Investigating the interpretability of hidden layers in deep text mining.
In Proceedings of the 13th International Conference on Semantic Systems, pages 177-180. ACM.

Reis, J., Correia, A., Murai, F., Veloso, A., and Benevenuto, F. (2019). Explainable machine learning for fake news detection.
In Proceedings of the 10th ACM Conference on Web Science, pages 17-26. ACM.

Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). Model-agnostic interpretability of machine learning. arXiv preprint
arXiv: 1606.05386.

Shrikumar, A., Greenside, P., and Kundaje, A. (2017). Learning important features through propagating activation differences.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3145-3153. JMLR. org.

Sundararajan, M., Taly, A., and Yan, Q. (2017). Axiomatic attribution for deep networks. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 3319-3328. JMLR. org.

Vaughan, J., Sudjianto, A., Brahimi, E., Chen, J., and Nair, V. N. (2018). Explainable neural networks based on additive index
models. arXiv preprint arXiv: 1806.01933.

Wang, N., Wang, H., Jia, Y., and Yin, Y. (2018). Explainable recommendation via multi-task learning in opinionated text data.
In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 165-174.
ACM.

Yelp (2013). https:/www.kaggle.con/c/yelp-recsys-2013/data.

Zhao, W., Joshi, T., Nair, V. N., and Sudjianto, A. (2020). Shap values for explaining cnn-based text classification models.
arXiv preprint arXiv:2008.11825.

Zhao, W., Singh, R., Joshi, T., Sudjianto, A., and Nair, V. N. (2021). Self-interpretable convolutional neural networks for text
classification. Manuscript submitted for publication.

12
