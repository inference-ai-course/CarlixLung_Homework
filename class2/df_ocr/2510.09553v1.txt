2510.09553v1 [cs.CL] 10 Oct 2025

arXiv

Hierarchical Indexing with Knowledge
Enrichment for Multilingual Video Corpus
Retrieval

Yu Wang!*, Tianhao Tan”, and Yifei Wang®

1School of Computing and Information, University of Pittsburgh, PA, USA
yuw235@pitt.edu
?Wuhan University of Technology
tantianhao@whut.edu.cn
3Hunan University
wangyifei0411@hnu.edu.cn

Abstract. Retrieving relevant instructional videos from multilingual
medical archives is crucial for answering complex, multi-hop questions
across language boundaries. However, existing systems either compress
hour-long videos into coarse embeddings or incur prohibitive costs for
fine-grained matching. We tackle the Multilingual Video Corpus Re-
trieval (mVCR) task in the NLPCC-2025 M4IVQA challenge with a
multi-stage framework that integrates multilingual semantics, domain
terminology, and efficient long-form processing. Video subtitles are di-
vided into semantically coherent chunks, enriched with concise knowledge-
graph (KG) facts, and organized into a hierarchical tree whose node em-
beddings are generated by a language-agnostic multilingual encoder. At
query time, the same encoder embeds the input question; a coarse-to-fine
tree search prunes irrelevant branches, and only the top-ranked chunks
are re-scored by a lightweight large language model (LLM). This design
avoids exhaustive cross-encoder scoring while preserving chunk-level pre-
cision. Experiments on the mVCR test set demonstrate state-of-the-art
performance, and ablation studies confirm the complementary contribu-
tions of KG enrichment, hierarchical indexing, and targeted LLM re-
ranking. The proposed method offers an accurate and scalable solution
for multilingual retrieval in specialized medical video collections.

Keywords: Multilingual Video Corpus Retrieval - Knowledge Graph -
Tree-Based Search - Large Language Model

1 Introduction

Online video has become a primary medium for disseminating information, and
medical instructional content is increasingly recognized for conveying complex
health topics [1-3]. However, their sheer volume and unstructured nature make it
challenging to locate specific information. This work addresses the Multilingual

* Corresponding author; Email: yuw235Qpitt.edu


2 Y. Wang et al.

Video Corpus Retrieval (mVCR) task in the NLPCC-2025 M4IVQA challenge [4,
5]. The objective is to retrieve the most relevant untrimmed video from a large,
multilingual collection, even when the query language differs from the video’s
subtitles. Effective retrieval faces three key challenges: achieving robust multilin-
gual semantics, efficiently processing lengthy videos rich in medical terminology,
and bridging the gap between concise queries and comprehensive video content
[6-8].

Existing retrieval strategies, however, fail to meet the specific demands of
the mVCR task. Dual-encoder models [9-11] enable fast lookup through shared
query—video embeddings but cannot capture the extensive temporal structure of
long videos [12]. Multilingual alignment techniques, whether via machine trans-
lation [13] or unified embedding spaces [14], often provide insufficient coverage of
the domain-specific terminology crucial to medical content. Sophisticated tem-
poral models [15-17] are typically designed for monolingual data and, there-
fore, miss essential cross-language semantics. Neural re-rankers based on cross-
encoders [18, 6] or LLMs [19] deliver precise relevance scores but incur prohibitive
computational costs when applied as first-stage filters for large corpora, limiting
practical scalability. Consequently, no single approach simultaneously achieves
efficient multilingual retrieval, fidelity to medical terminology [20], and tractable
processing of long videos.

To overcome these limitations, we introduce a multi-stage framework that
delivers efficient and accurate mVCR. Subtitles are segmented into semantic
chunks, enriched with KG facts, and then organized using LaBSE [21] embed-
dings into a hierarchical index for coarse-to-fine retrieval. An embedded query
initiates a tree search that prunes irrelevant branches early to reduce search
costs. Finally, only the top candidate chunks are re-ranked by a lightweight mul-
tilingual LLM, yielding nuanced relevance scores without processing the entire
corpus.

The main contributions of this work are:

(1) A modular architecture that combines semantic chunking, domain-specific
KG enrichment, and multilingual embeddings to enable scalable search in long
medical videos.

(2) A dynamic tree-pruning strategy that balances efficiency and precision
by narrowing the search space before LLM re-ranking.

(3) State-of-the-art results on the mVCR test set, supported by ablation
studies that isolate the impact of KG enrichment, hierarchical indexing, and
LLM re-ranking.

2 Related Work

Early video retrieval struggled with semantic gaps due to its reliance on low-
level features and metadata. Deep dual encoders [9-11] improved scalability via
independent indexing of joint query-video embeddings. However, they often ne-
glected fine-grained cross-modal interactions and struggled to represent long
temporal sequences [12]. Transformers have further enhanced temporal reason-


Hierarchical Indexing with KG Enrichment 3

ing, with models like CLIP4Clip [22] demonstrating the efficacy of frame ag-
gregation. Subsequent refinements in temporal alignment using sliding windows
or segment attention [15,16] also showed promise. Nevertheless, these methods
are predominantly monolingual and general-domain, making them unsuitable for
domain-specific terminology and multilingual queries in mVCR.

Multilingual retrieval initially relied on machine translation, but poor trans-
lation quality limited performance, spurring direct alignment techniques that
embed sentences into unified semantic spaces. Models like LaBSE [21] and other
language-agnostic embeddings proved effective for multilingual sentence retrieval
and on benchmarks like VATEX [14], which contains bilingual captions for short
videos. Nonetheless, VATEX’s short videos differ significantly from the long, pro-
cedural, terminology-dense medical videos in mVCR. Adapting these techniques
to specialized, lengthy, code-mixed content remains challenging.

Knowledge Graphs (KGs) offer structured relational information to mitigate
vocabulary discrepancies between queries and video content. Early strategies
involved query expansion and path-based reasoning for enhanced recall [23].
VideoGraph [24] integrates KG entities with shot-level video features for open-
domain search, while BioSyn [25] uses UMLS to link clinical terms by synthe-
sizing synonyms, improving retrieval in noisy settings. Despite these advances,
most KG-augmented approaches focus on text or short video clips. Effectively
incorporating KG information into long, multilingual instructional videos while
preserving temporal coherence remains unaddressed.

Large Language Models (LLMs) significantly advanced re-ranking by cap-
turing nuanced query-document semantics [26,27]. BERT-based cross-encoders
[18, 6] substantially outperform traditional passage re-ranking methods. Prompt-
ing large generative models for zero-shot relevance scoring also shows consistent
multilingual improvements [19]. Their computational expense remains a primary
drawback, as cross-encoders scale quadratically with input length, making them
unsuitable as first-stage filters for large collections of long videos. This compu-
tational bottleneck significantly challenges the application of advanced semantic
matching to long-form, multilingual video retrieval.

3 Methods

Our multi-stage framework addresses the mVCR challenge of retrieving relevant
videos when the query and subtitle languages differ. At its core, our pipeline
enriches subtitles with KG facts and builds a hierarchical index for each video,
enabling efficient coarse-to-fine retrieval. The system consists of two main phases:
Hierarchical Index Construction (Section 3.2) and Retrieval & Ranking (Sections
3.3-3.4). This design effectively scales to long videos while bridging language gaps
between queries and video content.

3.1 Task Formulation

Let V = {v1,...,vn} denote a collection of N medical instructional videos. Each
video vu; is associated with a set of subtitles S; (in its original language L;) and


4 Y. Wang et al.

a collection of relevant KG triples K;. Let Q be the space of user queries, where
a query @ € Q may be expressed in any supported language Lg. The objective
is to learn a retrieval function f : Q x YV > Ranking that generates a ranked list
of videos from V based on their relevance to a given query Q.

Video Corpus

Knowledge Graph K Query Processing
r

Triples: (er €2)

How to alleviating the problem of head
tilting forward with tools?

(ae eee eee J

i
i
i
i
i
i
i
i

(SEDER, "ZINA", FBI"),

+

("neck muscles”, “are", “inhibited from Query Embedding q
1. Subtitle Loading & Chunking Poor posture"),
, —— | Select triples 5. Dynamic Tree Search
1
| “RAMAN MEME” | “moist heat neck wrap..reliof” | *** subtitle chunk c! ——— 4

Continue until leaf

Saas ine] |stats should be /reactiv SS,
| i -ated
! : Candiate

format as KG facts
Chunks

2. Knowledge Graph Enrichment <i :

i = +
Enriched Chunk (@]) | Bhtrttrrtnnnnnoznnneeeo! 6. LLM Re-ranking
Subtitle
RISK IAUEIN IIS As Similarity
+ > Scored Chunk ¢/ (p=3)
Threshold 8 ‘Scored Chunk ¢/ (p=2)
KG facts
SUBAANAA SR STV c) _[Refine: Explore Children) [Prune:Discard Branch} 9 L___"
ca 6
each @ ul € RY og i
a iy 7. Final Ranking
"i a
3. LaBSE Embedding r—-§ max pooling
2 Top-M Ranked Videos
= = =
S V, (Score 3)
Vz (Score 2)
Vy (Score 2)

Embeddings {uj}

Fig. 1. Overview of proposed multilingual video retrieval pipeline.

3.2. Hierarchical Index Construction

For each video v; € VY, we construct an enriched hierarchical index 7; through
three key steps: segmenting subtitles into semantic chunks, enhancing these
chunks with relevant KG context, and organizing them into a hierarchical tree
structure.

Subtitle Loading and Semantic Chunking. Segmenting lengthy instruc-
tional videos requires preserving semantic coherence. We begin by loading sub-
title data S; = {€1,...,m,} for each video and performing basic text cleaning.
To avoid disrupting the instructional flow of arbitrary divisions, our approach
employs semantic chunking based on the similarity between adjacent subtitle
lines. The LaBSE model, with its strong multilingual representation capabili-
ties, provides embeddings for these subtitle lines:

e(¢,) = LaBSE(é,) € R? (1)


Hierarchical Indexing with KG Enrichment 5

where e(¢;,) represents the d-dimensional embedding for subtitle line ¢,. Chunk
boundaries are detected by identifying points where the cosine similarity between
adjacent embeddings falls below an empirically tuned threshold r:

cos(e(€,),e(lk41)) <7 (2)

This boundary detection yields a sequence of chunks c},... cM * for video v;,

each containing semantically coherent subtitle lines in the original language Lj.

KG Enrichment via Text Concatenation. To enhance semantic representa-
tion with domain knowledge, each chunk c? is enriched using the set of KG triples
K; relevant to its corresponding video v;. We identify medical entities in c} using
a multilingual Named Entity Recognizer, specifically an XLM-RoBERTa model
fine-tuned for multilingual NER [28]. These entities directly guide the selection of
relevant knowledge. Specifically, for each identified entity, we retrieve all triples
from the video’s pre-filtered set K; where it appears as either the subject or
the object. This enrichment occurs via straightforward text concatenation: key
information from the retrieved triples, such as entity relationships and types, is
converted into concise, factual statements and appended to the chunk’s original
text. The resulting enriched chunk, ¢, thus integrates relevant domain knowl-
edge while preserving the original content structure.

Tree Construction. The hierarchical organization of enriched chunks forms the
foundation of our efficient retrieval approach. We encode each enriched chunk
into a vector representation using LaBSE:

ul = LaBSE(2) (3)

where u? represents the enriched chunk in the shared multilingual semantic

space. We implement a two-level clustering strategy to create a structure that
captures both broad topics and specific details. First, K-means partitions the set
of embeddings {u/} for video v; into K coarse clusters that serve as first-level
nodes representing major topics. Then, for each coarse cluster, we apply HAC to
develop deeper tree levels that reveal finer-grained subtopics. Each node n in the
resulting tree 7; stores a representative embedding e,,, calculated as the centroid
of C,,, the set of embeddings corresponding to all chunks descended from node

n: i
en= GT Du (4)

This centroid e, effectively summarizes the semantic content of node n. The
completed hierarchical structure 7;, whose leaf nodes are the individual enriched
chunks ¢, enables the efficient coarse-to-fine search.

3.3. Query Processing and Initial Retrieval

After constructing hierarchical indices for all videos, our system processes in-
coming user queries and performs efficient retrieval across these structured rep-


6 Y. Wang et al.

resentations to identify relevant candidate chunks.

Query Processing. Given a user query @ in language Lg, we encode it using
the same LaBSE model employed during the indexing phase:

q = LaBSE(Q) (5)

This consistent encoding projects the query vector q and the chunk embeddings
u/ into a shared multilingual semantic space, enabling comparisons based on con-
ceptual similarity rather than exact lexical matching. Consequently, the system
gains inherent robustness against linguistic variations, such as those introduced
by translation.

Dynamic Tree Search. Rather than exhaustively comparing the query against
all chunks, we implement a coarse-to-fine search strategy that leverages the hier-
archical index 7; of each video. The search commences by computing the cosine
similarity between the query embedding q and the embeddings e,, of the first-
level (K-means) cluster nodes. Using a predefined relevance threshold 6, only
top-level clusters whose similarity exceeds this threshold are retained, effectively
pruning irrelevant branches early in the search. For each qualifying cluster, the
search recursively descends its subtree (generated by HAC), applying the same
pruning logic at each level to traverse only promising branches. This traversal
continues until reaching leaf nodes (the enriched chunks é/) via unpruned paths.
The resulting candidate set comprises all leaf nodes reached through this process.
Each candidate is recorded as a tuple (video-id, chunk-id, cos(q, u?), ¢/), with its
cosine similarity serving as an initial relevance score. These candidates are then
ranked to form a top-M list, Ctop, for subsequent re-ranking. This hierarchical
search offers significant computational improvements over a brute-force scan. By
dynamically pruning the search space, it ensures the expensive LLM re-ranker
processes only a few top candidates, which is critical for balancing high precision
with the scalability and low latency required for large video corpora.

3.4. LLM Re-ranking and Video Aggregation

While embedding similarity provides efficient initial retrieval, achieving optimal
ranking requires a deeper semantic understanding. In this final phase, we refine
the ranking of candidate chunks using a multilingual LLM and then aggregate
these scores to produce video-level results.

Multilingual LLM Re-ranking. Embedding-based similarity captures broad
semantic relationships but often misses nuanced relevance factors crucial for
medical queries. To address this limitation, we implement LLM-based re-ranking
that leverages multilingual understanding capabilities. For each candidate chunk
G} € Ctop, we prompt the LLM with the original query Q (in language Lg) and
the chunk’s enriched text @ (in its original language L;). The model then pro-
duces a scalar relevance rating p(¢) on a 1-3 scale (where 3 indicates highest


Hierarchical Indexing with KG Enrichment 7

relevance), providing a fine-grained assessment that surpasses simple vector simi-
larity. While the LLM’s nuanced semantic understanding leads to highly effective
scoring, this performance comes with a trade-off: the model’s internal reasoning
is largely opaque, limiting its interpretability.

Video-level Aggregation and Ranking. The mVCR task requires video-level
rankings rather than chunk-level results. We aggregate the chunk scores for each
video v; using max pooling, which assigns each video the highest relevance score
achieved by any of its evaluated chunks:

Score(v;) = max p(@) (6)
@ECioD
where on represents the set of top chunks belonging to video v;. This ap-

proach prioritizes videos containing highly relevant content chunks, even if other
portions are less pertinent. The final output ranks all videos {v1,...,uy} in de-
scending order of Score(v;).

4 Experiments

4.1 Dataset and Evaluation

The dataset features medical instructional videos crawled from YouTube, with
textual content that includes original subtitles and generated captions in both
Chinese and English. The corresponding question-answer pairs consist of Chinese
questions manually authored by medical experts and English questions that
are translated and refined by native-speaking medical doctors. Each question
corresponds to a specific temporal segment of a video, and multiple related
questions may point to the same answer segment. The dataset is divided into
training, validation, and test sets.

Table 1. Statistics of the Medical Instructional Video Dataset, including the number
of videos, QA pairs, vocabulary size, and average lengths for the Train/Dev/Test splits.

Dataset Videos QA pairs Vocab Avg. Ch. Q. Avg. Eng. Q. Avg. Video

Size Len. Len. Len.
Train 1,228 5,840 6,582 17.16 6.97 263.3
Dev 200 983 1,743 17.81 7.26 242.4
Test 200 1,022 2,234 18.22 7.44 310.9

Following the challenge protocol [29], retrieval performance is measured using
Recall@n (R@n) with n € {1, 10,50}, which indicates the percentage of queries
where the correct video appears among the top-n results. The Mean Reciprocal
Rank (MRR) [30] is calculated as:

IV|

1 1
MRR = —— ——_ 7


8 Y. Wang et al.

where |V| represents the number of test queries, and Rank, is the position of the
ground-truth video in the predicted list for the 7-th query. The Overall score,
serving as the main ranking criterion, is calculated by summing the four metrics:

[M|
Overall = $ © Value; (8)

i=1

where |M| = 4 is the number of evaluation metrics, and Value; represents the
value of the i-th metric.

4.2 Main Results

Table 2. Retrieval performance comparison of our proposed framework against other
methods on the mVCR test set.

Method R@1 R@10 R@50 MRR Overall

RANDOMPICK [29,31] 0.0343 0.0523 0.0442 0.0442 0.1674
GEN [32] 0.1311 0.1074 0.0978 0.1142 0.4505
Wjh 0.2744 0.3312 0.4117 0.2551 1.2724
DSG-1 [33, 34] 0.2644 0.3545 0.4414 0.2887 1.3491
sun [35] 0.3121 0.4078 0.4966 0.3245 1.5410
NYU 0.3213 0.4137 0.5104 0.3354 1.5808
DIMA (Ours) 0.3264 0.4211 0.5177 0.3407 1.6059

We evaluate our proposed method against several strong baselines from previ-
ous mVCR challenges using five standard metrics: R@1, R@10, R@50, MRR, and
the Overall score. Key competitors include GEN [32], which implemented a re-
trieval framework based on approaches surveyed in recent literature. DSG-1 [33,
34] developed a two-stage retrieval-reranking pipeline that employed GPT-3.5!
for video summary generation and RoBERTa [36] for initial retrieval, followed by
a CCGS-VCR analyzer for re-ranking. The MQuA approach [35], from the 2024
challenge, leveraged the DeBERTa-v2-710M-Chinese [37] model combined with
Multi-Level Video Moment Refinement (MVMR) and enhanced it with Multi-
lingual Query Paraphrase Generation (MQPG) using Few-shot ChatGPT [38].

As shown in Table 2, our proposed method achieves state-of-the-art perfor-
mance across all evaluation metrics on the mVCR test set, achieving the highest
scores in R@1 (0.3264), R@10 (0.4211), R@50 (0.5177), MRR (0.3407), and
Overall (1.6059).

Compared to GEN [32], our method demonstrates substantial improvements
with absolute increases of 0.1953f in R@1, 0.3137 in R@10, 0.4199F in R@50,
0.22657 in MRR, and 1.1554¢ in Overall score. Against the stronger DSG-1
approach [33,34], we achieve notable improvements: 0.06207 for R@1, 0.0666T
for R@10, 0.07637 for R@50, 0.0520t for MRR, and 0.2568 for Overall score.

' https: //poe.com/GPT-3.5-Turbo.


Hierarchical Indexing with KG Enrichment 9

Furthermore, our method outperforms the sun team’s MQuA approach [35] with
consistent gains of 0.01437 in R@1, 0.01337 in R@10, 0.02117 in R@50, 0.0162t
in MRR, and 0.06497 in Overall score.

These consistent improvements validate the effectiveness of our multi-stage
retrieval framework. The performance gains are particularly significant for R@1
and MRR metrics, highlighting the superior precision of our approach in re-
trieving the most relevant video as the top result for multilingual medical video
retrieval tasks.

4.3 Ablation Study

We conduct ablation experiments to evaluate the impact of each core compo-
nent by systematically removing modules from our framework. Generally, all
reported percentage changes represent relative decreases from the full system’s
performance. Removing domain-specific KG enrichment decreases the Overall
score by 6.6% and R@1 by 7.6%. These results confirm KG enrichment’s crucial
role in enhancing semantic representations, particularly in bridging specialized
medical terminology between queries and video content.

When our hierarchical index is replaced with flat retrieval, performance de-
grades further, with the Overall score dropping by an additional 2.7%. Notable
declines appear in MRR (12.4%) and R@50 (8.4%), showing that flat indexing
struggles with long-form videos even under relaxed retrieval criteria. The mag-
nitude of this performance gap underscores the effectiveness of the hierarchical
organization for efficient pruning and improved precision.

Table 3. Impact of removing key components from the proposed framework on retrieval
performance.

Method R@1 R@10 R@50 MRR Overall

DIMA 0.3264 0.4211 0.5177 0.3407 1.6059
w/o KG Enrichment 0.3017 0.3943 0.4878 0.3154 1.4992
w/o Hierarchical Index 0.2968 0.3887 0.4741 0.2985 1.4581
w/o LLM Re-ranking 0.2855 0.3614 0.4612 0.2911 1.3992

LLM re-ranking is the most critical component, as evidenced by the sub-
stantial 12.9% decrease in Overall score when removed from the pipeline. Per-
formance metrics show the most significant deterioration here, with R@10 and
MRR falling by 14.2% and 14.6% respectively. Such pronounced degradation
relative to other ablations demonstrates that the LLM’s nuanced understand-
ing of query-video semantic relationships is critical for achieving state-of-the-art
performance in multilingual medical video retrieval.

5 Conclusion

We have presented a solution to the mVCR challenge that addresses key limita-
tions in retrieving multilingual medical videos. By integrating language-agnostic


10 Y. Wang et al.

embeddings with domain knowledge and efficient hierarchical search, our frame-
work achieves both computational scalability and multilingual precision. Exper-
iments demonstrate state-of-the-art performance across all metrics, and ablation
studies confirm that each component makes complementary contributions to the
effectiveness of multilingual video retrieval.

To build upon this work, future directions include exploring structured KG
reasoning with methods like graph neural networks to overcome the limitations
of text concatenation. We will also investigate knowledge distillation to create a
more compact and efficient LLM re-ranker. Finally, incorporating visual features
for true multi-modal retrieval remains a key priority to further boost precision
and scalability.

References

1. Shutao Li, Bin Li, Bin Sun, and Yixuan Weng. Towards visual-prompt temporal
answer grounding in instructional video. [EEE Transactions on Pattern Analysis
& Machine Intelligence, (01):1-18, 2024.

2. Jiachen Zhong and Yiting Wang. Enhancing thyroid disease prediction using ma-
chine learning: A comparative study of ensemble models and class balancing tech-
niques. 2025.

3. Yiting Wang, Jiachen Zhong, and Rohan Kumar. A systematic review of machine
learning applications in infectious disease prediction, diagnosis, and outbreak fore-
casting. 2025.

4. Bin Li, Yixuan Weng, Qiya Song, Lianhui Liang, Xianwen Min, and Shoujun Zhou.
Overview of the nlpcc 2024 shared task 7: Multi-lingual medical instructional video
question answering. In Derek F. Wong, Zhongyu Wei, and Muyun Yang, editors,
Natural Language Processing and Chinese Computing, pages 429-439, Singapore,
2025. Springer Nature Singapore.

5. Bin Li, Shenxi Liu, Yixuan Weng, Yue Du, Yuhang Tian, and Shoujun Zhou.
Overview of the nlpcc 2025 shared task 4: Multi-modal, multilingual, and multi-
hop medical instructional video question answering challenge. arXiv preprint
arXiv:2505.06814, 2025.

6. Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun,
Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. Spvit: Enabling faster vision
transformers via latency-aware soft token pruning. In European conference on
computer vision, pages 620-640. Springer, 2022.

7. Zong Ke, Shicheng Zhou, Yining Zhou, Chia Hong Chang, and Rong Zhang. De-
tection of ai deepfake and fraud in online payments using gan-based models. arXiv
preprint arXiv:2501.07033, 2025.

8. Zhenglun Kong, Haoyu Ma, Geng Yuan, Mengshu Sun, Yanyue Xie, Peiyan Dong,
Xin Meng, Xuan Shen, Hao Tang, Minghai Qin, et al. Peeling the onion: Hier-
archical reduction of data redundancy for efficient vision transformer training. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages
8360-8368, 2023.

9. Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K. Roy-
Chowdhury. Learning joint embedding with multimodal cues for cross-modal
video-text retrieval. In Proceedings of the 2018 ACM on International Confer-
ence on Multimedia Retrieval, ICMR ’18, page 19-27, New York, NY, USA, 2018.
Association for Computing Machinery.


10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Hierarchical Indexing with KG Enrichment 11

Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun
Wang. Dual encoding for zero-example video retrieval, 2019.

Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for
video question answering and retrieval, 2018.

Pinrui Yu, Zhenglun Kong, Pu Zhao, Peiyan Dong, Hao Tang, Fei Sun, Xue Lin,
and Yanzhi Wang. Q-tempfusion: Quantization-aware temporal multi-sensor fusion
on bird’s-eye view representation. In Proceedings of the Winter Conference on
Applications of Computer Vision (WACV), pages 5489-5499, February 2025.
Pavel Braslavski, Suzan Verberne, and Ruslan Talipov. Show me how to tie a
tie: Evaluation of cross-lingual video retrieval. In Norbert Fuhr, Paulo Quaresma,
Teresa Goncalves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappel-
lato, and Nicola Ferro, editors, Experimental IR Meets Multilinguality, Multimodal-
ity, and Interaction, pages 3-15, Cham, 2016. Springer International Publishing.
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang
Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-
language research, 2020.

Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-
clip: End-to-end multi-grained contrastive learning for video-text retrieval, 2022.
Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang. Centerclip: Token cluster-
ing for efficient text-video retrieval. In Proceedings of the 45th International ACM
SIGIR Conference on Research and Development in Information Retrieval, SIGIR
22, page 970-981. ACM, July 2022.

Puning Zhao, Rongfei Fan, Shaowei Wang, Li Shen, Qixin Zhang, Zong Ke, and
Tianhang Zheng. Contextual bandits for unbounded context distributions, 2025.
Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert, 2020.
Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, and Jimmy Lin. Zero-
shot cross-lingual reranking with large language models for low-resource languages,
2023.

Bin Li, Bin Sun, Shutao Li, Encheng Chen, Hongru Liu, Yixuan Weng, Yongping
Bai, and Meiling Hu. Distinct but correct: generating diversified and entity-revised
medical response. Science China Information Sciences, 67(3):132106, 2024.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
Language-agnostic bert sentence embedding, 2022.

Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui
Li. Clip4clip: An empirical study of clip for end to end video clip retrieval, 2021.
Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for
academic search via knowledge graph embedding. In Proceedings of the 26th Inter-
national Conference on World Wide Web, WWW ’17, page 1271-1279, Republic
and Canton of Geneva, CHE, 2017. International World Wide Web Conferences
Steering Committee.

Luca Rossetto, Matthias Baumgartner, Narges Ashena, Florian Ruosch, Romana
Pernisch, Lucien Heitz, and Abraham Bernstein. Videograph — towards using
knowledge graphs for interactive video retrieval. In MultiMedia Modeling: 27th
International Conference, MMM 2021, Prague, Czech Republic, June 22-24, 2021,
Proceedings, Part II, page 417-422, Berlin, Heidelberg, 2021. Springer-Verlag.
Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, and Jaewoo Kang. Biomedical entity
representations with synonym marginalization. In Dan Jurafsky, Joyce Chai, Na-
talie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, pages 3641-3650, Online, July
2020. Association for Computational Linguistics.


12

26.

2.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

Y. Wang et al.

Yixian Shen, Hang Zhang, Yanxin Shen, Lun Wang, Chuanqi Shi, Shaoshuai Du,
and Yiyi Tao. Altgen: Ai-driven alt text generation for enhancing epub accessibility.
arXiv preprint arXiv:2501.00118, 2024.

Hang Zhang, Yanxin Shen, Lun Wang, Chuanqi Shi, Shaoshuai Du, Yiyi Tao, and
Yixian Shen. Comparative analysis of large language models for context-aware
code completion using safim framework. arXiv preprint arXiv:2502.15248, 2025.
Rahul Mehta and Vasudeva Varma. Llm-rm at semeval-2023 task 2: Multilingual
complex ner using xlm-roberta. In Proceedings of the 17th International Workshop
on Semantic Evaluation (SemEval-2023), 2023.

Bin Li, Yixuan Weng, Bin Sun, and Shutao Li. Learning to locate visual answer in
video corpus using question. In ICASSP 2023 - 2023 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), page 1-5. IEEE, June
2023.

Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. Expected re-
ciprocal rank for graded relevance. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management, CIKM ’09, page 621-630, New York,
NY, USA, 2009. Association for Computing Machinery.

Yixuan Weng and Bin Li. Visual answer localization with cross-modal mutual
knowledge transfer. In ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.

Bin Li, Yixuan Weng, Qiya Song, Lianhui Liang, Xianwen Min, and Shoujun Zhou.
Overview of the nlpcc 2024 shared task 7: Multi-lingual medical instructional video
question answering. In Derek F. Wong, Zhongyu Wei, and Muyun Yang, editors,
Natural Language Processing and Chinese Computing, pages 429-439, Singapore,
2025. Springer Nature Singapore.

Ningjie Lei, Jinxiang Cai, Yixin Qian, Zhilong Zheng, Chao Han, Zhiyue Liu, and
Qingbao Huang. A two-stage chinese medical video retrieval framework with llm.
In Fei Liu, Nan Duan, Qingting Xu, and Yu Hong, editors, Natural Language
Processing and Chinese Computing, pages 211-220, Cham, 2023. Springer Nature
Switzerland.

Bin Li, Yixuan Weng, Hu Guo, Bin Sun, Shutao Li, Yuhao Luo, Mengyao Qi,
Xufei Liu, Yuwei Han, Haiwen Liang, Shuting Gao, and Chen Chen. Overview
of the nlpcc 2023 shared task: Chinese medical instructional video question answer-
ing. In Fei Liu, Nan Duan, Qingting Xu, and Yu Hong, editors, Natural Language
Processing and Chinese Computing, pages 233-242, Cham, 2023. Springer Nature
Switzerland.

Guyang Yu, Xiaoyang Bi, Jielong Tang, Ming Gu, Tianbai Chen, Zhiqiang Li,
and Miankuan Zhu. Mqua: Multi-level query-video augmentation for multilingual
video corpus retrieval. In Derek F. Wong, Zhongyu Wei, and Muyun Yang, editors,
Natural Language Processing and Chinese Computing, pages 353-364, Singapore,
2025. Springer Nature Singapore.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
optimized bert pretraining approach, 2019.

Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-
enhanced bert with disentangled attention, 2021.

OpenAI. Introducing chatgpt: Optimizing language models for dialogue.
https: //openai.com/index/chatgpt/, November 2022. Accessed 04 May 2025.
