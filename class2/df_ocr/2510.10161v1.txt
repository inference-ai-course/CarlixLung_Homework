2510.10161lv1 [cs.CL] 11 Oct 2025

arXiv

JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

Large Language Model Sourcing: A Survey

Liang Pang, Kangxi Wu, Sunhao Dai, Zihao Wei, Zenghao Duan, Jia Gu, Xiang Li,
Zhiyi Yin, Jun Xu, Huawei Shen, Xueqi Cheng

Abstract—The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, shifting from supporting
objective tasks (e.g., recognition) to empowering subjective decision-making (e.g., planning, decision). This marks the dawn of general
and powerful Al, with applications spanning a wide range of fields, including programming, education, healthcare, finance, and law.
However, their deployment introduces multifaceted risks. Due to the black-box nature of LLMs and the human-like quality of their
generated content, issues such as hallucinations, bias, unfairness, and copyright infringement become particularly significant. In this

context, sourcing information from multiple perspectives is essential.

This survey presents a systematic investigation into provenance tracking for content generated by LLMs, organized around four
interrelated dimensions that together capture both model- and data-centric perspectives. From the model perspective, Model Sourcing
treats the model as a whole, aiming to distinguish content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing architectural components that shape the outputs of
model. From the data perspective, Training Data Sourcing focuses on internal attribution, tracing the origins of generated content back
to the training data of model. In contrast, External Data Sourcing emphasizes external validation, identifying external information used
to support or influence the responses of model. Moreover, we also propose a dual-paradigm taxonomy that classifies existing sourcing
methods into prior-based (proactive traceability embedding) and posterior-based (retrospective inference) approaches. Traceability
across these dimensions enhances the transparency, accountability, and trustworthiness of LLMs deployment in real-world

applications.

Index Terms—Al Safety, Large Language Models, Wartermarking, AIGC Detection, Influence Function, Retrieval-Augmented
Generation, Model Sourcing, Structure Sourcing, Training Data Sourcing, External Data Sourcing.

1. INTRODUCTION

The launch of ChatGPT] marked a watershed moment for
large language models (LLMs), catalyzing rapid advance-
ments that have propelled natural language processing into
a transformative new era. Models such_as DeepSeek [1],
Qwen (2), GPT-4 (3, Claudé?| LLaMA3 (4, and Gemini [5
exemplify this explosive growth. Collectively, they signal a
paradigm shift in artificial intelligence — from supporting
objective tasks such as classification and prediction to en-
abling more subjective, decision-oriented reasoning [6], [7].
These models now exhibit impressive versatility across a
range of domains, including programming |8], [9], educa-
tion [10], healthcare (11), and legal services . As LLMs
increasingly influence high-stakes decisions and enable nat-
uralistic human-AI interactions, they bring us closer to
the frontier of general intelligence and emergent reasoning

capabilities [13].

e Liang Pang, Zhiyi Yin, Huawei Shen and Xueqi Cheng are with the State
Key Laboratory of AI Safety, Institute of Computing Technology, Chinese
Academy of Sciences, Beijing, China. Email: pangliang@ict.ac.cn;

e = Kangxi Wu, Zihao Wei, Zenghao Duan, Jia Gu and Xiang Li are with the
State Key Laboratory of AI Safety, Institute of Computing Technology,
Chinese Academy of Sciences and University of Chinese Academy of
Sciences, Beijing, China.

e Sunhao Dai and Jun Xu are with the Gaoling School of Artificial
Intelligence, Renmin University of China, Beijing, China.

e Kangxi Wu, Sunhao Dai, Zihao Wei, Zenghao Duan and Jia Gu con-
tributed equally to this work.

e Corresponding Author: Liang Pang.

https: / /openai.com/blog/chatgpt

ttps:/ /www.anthropic.com/index/introducing-claude

Nos

Model Perspective

YR
VU 7
Training

DeepSeekGPT-4 LLaMA QWen A Training
Process « Data
* 2
°

%e

Cy

Data Perspective
3. Training Data Sourcing
Docl | Doc2 | Doc3 | Doc4 |

Pa
sugoassnsensnoneercc! . s
“eeneean gomee®

| Output CY
eeeee Information ==000,, Vit

a
- RAG % <==
Layers” @ ye,» Modulars Process % ee nemal

\
s

| Ore. |;
: e|
Neurons < iG
+S —

2. Structure Sourcing

&

==

4. External Data Sourcing

Fig. 1. Four dimensions of sourcing in large language models. From
the model perspective, output information can be attributed either to
the specific model itself (model sourcing) or to its internal architecture
and mechanisms (structure sourcing). From the data perspective, output
information can be traced back to the model's training samples (training
data sourcing) or linked to a particular external corpus used for align-
ment or reference (external data sourcing).

However, this broad applicability also magnifies a new
set of profound risks — rooted in both model architectures
and the data these models consume. In practice, inher-
ent biases embedded in training and retrieved data [14],
such as underrepresentation of marginalized groups, can
propagate societal inequities by influencing model outputs
that reinforce stereotypes, leading to discriminatory out-
puts (15). Hallucinations and factual inaccuracies present


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

another serious concern, especially in sensitive domains
like medicine and law [16], [17], where misinformation
can have real-world consequences. The opaque nature of
LLMs further complicates accountability, particularly when
harmful outcomes arise. Meanwhile, security vulnerabili-
ties such as prompt injection and training data poisoning
create opportunities for malicious exploitation (18), such as
tricking models into generating harmful content or leaking
sensitive information. In addition, the ability of LLMs to
produce manipulative or coercive content raises deep ethical
questions around autonomy, consent, and the erosion of
trust in Al-enabled systems [19]. As LLMs become increas-
ingly embedded in critical applications, confronting these
multifaceted risks becomes not just important but essential.

Among these challenges, two stand out as particularly
foundational: the indistinguishability of generated content
and the structural opacity of the models themselves. These
twin issues highlight the urgent need for robust output
sourcing mechanisms. Given the architectural complexity
and intrinsic limitations of today’s LLMs, failures are not
outliers but structural inevitabilities. Their black-box nature
makes error diagnosis and causal attribution prohibitively
difficult, while their scale renders traditional debugging and
mitigation approaches infeasible. In this context, tracing
the provenance of model outputs offers the most direct
and actionable path toward identifying failure points and
implementing effective safeguards.

Sourcing enables systematic risk mitigation by establish-
ing verifiable chains of accountability throughout the LLM
lifecycle, thereby fostering user trust in high-stakes applica-
tions. From the model perspective, when harmful or non-
compliant outputs arise, provenance sourcing allows precise
attribution to specific models or architectural components.
This traceability facilitates targeted feedback to developers
for model refinement or regulatory oversight. From the data
perspective, in cases involving copyright infringement or
unauthorized content reproduction, sourcing mechanisms
can trace outputs back to the originating training samples
or retrieved external sources. This empowers proper attri-
bution to data creators and supports the development of fair
compensation frameworks. By enabling a bidirectional map-
ping from model outputs back to their underlying model
and data origins, sourcing transforms risk management
from a reactive and ad hoc process into a proactive and
auditable system. It directly addresses two major barriers
to responsible LLM deployment: structural opacity and the
absence of accountability.

To support this vision, our survey introduces a uni-
fied sourcing framework that integrates both model and
data perspectives, systematically categorizing methods into
prior-based and posterior-based paradigms. Prior-based
approaches proactively embed traceable markers during
model training or data preparation, enabling explicit and
verifiable attribution. In contrast, posterior-based methods
retrospectively infer provenance by analyzing model out-
puts, activations, or gradients after generation, offering flex-
ibility without requiring architectural modifications. This
framework provides a systematic foundation for investigat-
ing attribution methods across the full lifecycle of LLM-
generated content. As shown in Fig. our survey cov-
ers four interrelated dimensions: Model, Model Structure,

2

Training Data, and External Data, offering a comprehensive
lens through which to evaluate provenance, responsibility,
and traceability in modern LLM ecosystems.

Model Sourcing focuses on attributing generated con-
tent to specific LLMs or human authors — an increasingly
critical task as Al-generated outputs become harder to dis-
tinguish from human-written text. Prior-based methods in-
troduce watermarking or model-specific signatures during
inference, while posterior-based approaches analyze statistical
properties of the output or internal activation patterns to
infer authorship. This dimension plays a key role in curbing
misinformation by tracing the origins of synthetic content.

Model Structure Sourcing examines how architectural
components (e.g., parameter configurations, attention mech-
anisms, and activation pathways) influence model behav-
ior, thereby shedding light on otherwise opaque decision-
making processes. Prior-based strategies embed traceable
markers into network layers or attention heads at design
time to enable direct structural attribution. In contrast,
posterior-based techniques retrospectively analyze activation
trajectories, parameter gradients, or behavioral fingerprints
to infer which architectural elements contributed to specific
outputs. Together, these methods illuminate the causal re-
lationship between structure and behavior, while exposing
trade-offs between proactive traceability (built-in markers)
and reactive interpretation (after-the-fact inference).

Training Data Sourcing enables causal attribution of
generated outputs to specific training samples, addressing
risks related to biased, sensitive, or noisy data. This dimen-
sion formalizes the relationship between data provenance
and model behavior, advancing solutions to key challenges
in fairness, privacy, and compliance. Prior-based techniques
insert verifiable fingerprints or metadata into training data
during pretraining, allowing for deterministic traceability.
Posterior-based methods infer likely data origins through
memorization detection, influence functions, gradient anal-
ysis, or latent representation similarity. These approaches
enhance our understanding of how training data shapes
model outputs, balancing proactive encoding with statistical
inference.

External Data Sourcing attributes output content to user
inputs or retrieved external knowledge, helping to detect
adversarial prompts, manipulation, or contextual ambigu-
ities. Prior-based frameworks integrate verifiable markers
or source-specific identifiers into external knowledge bases
or prompts during system design. Posterior-based methods
conduct retrospective analyses using latent space align-
ment, attention flow tracing, or input-output linkage pat-
terns to infer contextual dependencies. These techniques
disentangle endogenous model behavior from exogenous
influences, illustrating the contrast between deterministic at-
tribution (via controlled inputs) and probabilistic inference
(via correlation-based analysis).

These four dimensions are selected to cover both the
“supply side” (model and its structure) and “demand side”
(training and external data) of LLM-generated content, en-
suring a comprehensive analysis of provenance. This survey
establishes a unified full-lifecycle provenance framework
for LLMs, fundamentally distinguishing it from prior frag-
mented efforts in text attribution, training data influence, or
model interpretability (Fig. [2}. Our core innovations are:


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

(1) Holistic Scope Across the LLM Content Lifecycle: We
integrate four interconnected dimensions of LLM sourcing
into a cohesive provenance paradigm, including Model,
Model Structure, Training Data, and External Data. This
end-to-end perspective spans content generation, architec-
tural influence, training data causality, and real-time input
interaction, addressing gaps in siloed studies of detection
(output-only), influence estimation (data-only), or introspec-
tion (structure-only).

(2) Dual-Paradigm Taxonomy for Attribution: For each
dimension, we systematize methodologies into prior-based
(proactive traceability embedding) and posterior-based (ret-
rospective inference) approaches. This reveals critical trade-
offs between design-time verifiability (e.g., watermarking,
structural markers) and post-hoc analyzability (e.g., activa-
tion tracing, gradient analysis), advancing beyond single-
paradigm reviews.

2 PRELIMINARIES

This section centers on the core theme of the sourcing of
LLMs, systematically introducing the associated research
questions and motivations, the unified definitions of key
concepts, the scope of the research field, and the classifi-
cation criteria for sourcing methods. The objective of this
chapter is to provide a comprehensive and structured re-
view of the theoretical framework underpinning the tracing
of LLMs, thereby laying a foundation for the detailed explo-
ration of related technical methodologies and application
scenarios in subsequent sections.

2.1

LLMs are rapidly advancing but face a series of critical
challenges that require urgent attention [152]-[154].

From the model perspective, risks associated with LLMs
fundamentally originate from the dual challenges of con-
tent indistinguishability and structural opacity. On the one
hand, the boundaries between LLM-generated and human-
authored content have become increasingly indistinct, as the
outputs are entirely derived from latent linguistic patterns
and statistical regularities learned during training [155|-
(157). This capacity to mimic the complexity and diver-
sity of human language enables generated texts to achieve
substantial convergence with human creations at syntac-
tic, logical, and stylistic levels [158], resulting in highly
anthropomorphic linguistic characteristics. This anthropo-
morphism not only allows for the potential embedding of
socially non-conforming or harmful information but also
provides pronounced concealment. On the other hand, the
inherent structural complexity and “black-box” nature of
contemporary LLMs [159], built on deep neural networks,
constitute critical technical risk factors. Their decision-
making processes resist human interpretability, resulting in
inherently low transparency [160]. This opacity increases
the difficulty of identifying and correcting internal biases
while heightening challenges in predicting and controlling
model behavior. Furthermore, massive parameter counts
and complex parameter interactions render comprehensive
testing and validation practically unattainable, potentially
leading to emergent behaviors or blind spots in specific

Sourcing Problems and Social Impacts

3

scenarios. The fundamental lack of transparency in internal
mechanisms severely hinders auditability, thereby amplify-
ing concerns regarding safety, reliability, and accountability,
especially within high-stakes deployment contexts |161]-
[163]. These intertwined aspects of content indistinguisha-
bility and structural opacity collectively pose significant
application risks.

From the data perspective, risks associated with LLMs
fundamentally stem from inherent training data dependen-
cies and vulnerabilities in external data. The quality of
training data, specifically its comprehensiveness, diversity,
and representativeness, directly governs model behavior
and performance [164], where biases, noise, or incomplete-
ness within datasets can propagate analogous deficiencies
into real-world applications, compromising fairness and
reliability [165], [166]. Furthermore, the potential inclusion
of sensitive personal information or personally identifiable
information in training corpora risks inadvertent memo-
rization during model training, subsequently enabling un-
intended disclosures in generated outputs that threaten
user privacy and violate data protection regulations [167].
Compounding these risks, the dynamic nature of user in-
teractions introduces critical input vulnerabilities; malicious
actors may craft adversarial prompts to manipulate mod-
els into generating harmful content or revealing sensitive
information, destabilizing model behavior and undermin-
ing predictability, particularly absent robust input filtering
mechanisms [168]. The inherent diversity and complexity of
user-generated inputs further amplify uncertainties, posing
unforeseen challenges such as misinterpretations or inap-
propriate responses in multilingual or multicultural con-
texts, thereby necessitating rigorous safeguards to mitigate
risks arising from adversarial exploitation and contextual
ambiguities [169].

In light of these challenges, research into sourcing tech-
nologies has become increasingly important. Sourcing in
LLMs seeks to link model behavior with its internal mech-
anisms and external dependencies, offering critical insights
for addressing current issues.

Sourcing of the model and its generated content
can [170], [171], for instance, track the creators, training
processes, and usage histories of LLMs. This is essential
for ensuring data security and privacy compliance, pre-
venting malicious attacks, identifying copyright violations,
and supporting the regulation of AIGC content [172], 173).
Additionally, sourcing enables the identification of training
data sources, quality, and preprocessing methods, helping
to pinpoint potential sources of bias and data noise [112],
[174]. This allows for targeted improvements to the training
data, reducing the likelihood of hallucinations. Tracing the
quality and applicability of external knowledge bases also
helps identify points where errors may be introduced dur-
ing inference, facilitating more effective optimization (175).
Furthermore, sourcing of model structures can shed light on
architectural designs and parameter configurations. Com-
bined with training data traceability, this enables researchers
to understand the knowledge representations and reason-
ing patterns learned by the model. Such insights can help
address the “black-box” issue, provide clearer explanations
of model behavior, build user trust, and serve as a basis for

debugging and optimization [176].


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

GLTR [20], DetectGPT (21, Fast-DetectGPT [22],
DetectGPT4Code [23], emantic-aware Watermarking

White Box
— Methods } |
3.3.1] al
Posterior-based 883)
Methods
sB3] Black Box
L Methods
Model
Sourcing
sB]
White Box
| Methods } |
Prior-based §

Methods

sB.4] Black Box
i Methods } |

(BaD)
Single Modular
aa Methods

Short ChatGPT Detection poss Domain Detection (26],
Decoding Saategics Detection Feature-based Detection [28],
LLMDet [29], COCO [30], ‘Netra Deepfake Detection
Ghostbuster [32], Raidar [33], GECSCORE [34], DetectGPT-SC (35),
Unsupervised ae Distal Detection , PostMark B7),
Water-Probe [38], RADAR [39], OUTFOX [40]

KGW [41], KGW2-SELFHASH WaterSeeker
Parambistembedded Watermarking , SymMark 5 y
Adaptive Watermark [46], SH eo We UPV Watermark |4
DWR [49], SynthID-Tex KD Watermark Removal |5 Pq],
Unbiased -Watermark [52] £2]

L Black-Box Watermarking 53}, WLM 54], Agent Guide

Posterior-based

Key-Value Memes , Concept Promoters 7],
LM-Debugger [58], 4" metic Hewstic Neurons
PEA [60], DE al MemFlex DINM [63], PCGU [6

NOINTENTEDIT [65], LED cal Sloss re NPTI 68},
, SPT [ZI , ROME [73],

Methods
sii
Connection
Structure ‘_; Modular Methods

si]

Model-level

PFME [69], TruthX |70)
MEMIT

Knowledge Flow F ane Routing |7 Colors Circuits [77,
Knowledge Creal S , 179], Copying ircuits b
Country-City Circuits por
Math Circuits |82], ICL as Gradient Descent |83|
DP Circuit if Shahar (89],UnKE

MoDEM [91], Med-MoE [92], MoE-MLoRA

( TASER (94], DeepSeekMoE [95], DynMoE [96], MoEUT

TRAK [98], CEA [99], EK-FAC [100], TFK [101], RIE
Bayesian-TDA |103], Scaling-IF raf AttributingLC
roa fey aa Fragle-IF oe Dataln

108), Datasam-I
roa fey TIO], Relatif oy DDA [112)

SimBE po atance Atemugon , Inferring-TDA [115],
ICL-TDA [116], FastTrack [117], st (8) KNN-TDA [119]

WASA [120], Sox [P1) Source-Aware [123], Radioactive-
Waters aa MP [124] (124) Injecting-Fictitious-

Knowledge |125|

oD
5
is
Hy
5
[e)
Sp)
o Prior-based Expert Methods
Va
3 Methods
= | Layer-level
wo | Expert Methods
oD ee)
Ss}
5p White Box
S 4 Methods }—
4H Posterior-based
& Methods
I §p3] Black Box
4 Methods p
— Training Data §
Sourcing ——
SB] Watermark-
-_| based Methods }|—
Prior-based §
Methods
ma,
sb-4] Proxy Model
Methods
:
Retriever-

based Methods }|—
Posterior-based

Datamoels [126], Small-to-Large [127], AttriBoT [128], GMValua-
tor [129], DsDm |130)], MATES AST, Group-MA’

PostCite , Rerank , RARR , SearChain

( AGREE , CEG , SmartBook , LLatrieval

ICLCite p20) ve [Ta p40) eR ueprint [142] , ATTR.
FIRST [143] ack as éd Ee a

Methods TT
§[6.3] NLE-based
Methods
External Data §[6.3.2]
Sourcing —_—..___
sie] Prompt-based
Methods
Prior-based § [6-41]
Methods —
s[6-4] Tuning-based
Methods
:

Fig. 2. A taxonomy of large language model sourcing methodologies.

AGREE [136], Self-RAG 147], FRONT [148], FGR [149], APO [150],
ReClaim |151|



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

In summary, sourcing in LLMs addresses the intrinsic
challenges associated with their development. By tracing
the origins of generated content, model structures, training
processes and external data, we can enhance the reliability,
interpretability, and security of LLMs. This ensures their
safe and trustworthy application, advancing the healthy
development of artificial intelligence technologies. Research
into sourcing is thus of both theoretical significance and
practical value in overcoming the current bottlenecks faced
by LLMs.

2.2 Definitions of Two Souring Paradigms

Within the realm of LLM sourcing, methodologies can
be fundamentally categorized into two distinct paradigms
based on their temporal and mechanistic foundations: prior-
based sourcing and posterior-based sourcing. These ap-
proaches diverge in their core mechanisms for establish-
ing content provenance across the LLM lifecycle, including
training, input processing, inference, and generation.

Posterior sourcing emphasizes scenarios where the
model has already been trained and the output has been
generated. It performs statistical consistency tests and sensi-
tivity assessments on candidate sources based on the output
and context, thereby inferring the most likely model and
primary influencing factors. Prior-based sourcing empha-
sizes embedding verifiable evidence into the training data
or the model itself before generation, by implanting stable
identifiers (such as watermarks or data IDs) into corpora,
retrieved documents, or model components. After training
and inference, attribution is achieved simply by detecting
and verifying these identifiers in the output.

To systematically clarify the technical boundaries and
implementation logic of LLM sourcing, this section first uni-
fies the core definitions of two fundamental methodological
paradigms: posterior-based sourcing and prior-based sourc-
ing, across all four dimensions (model, model structure,
training data, external data). These definitions establish a
formal mathematical framework for quantifying attribution
relationships.

First of all, we formalize the generation process from
input to output as follows: First, let x denote the user-
provided input prompt, and y denote the output generated
by the model. Next, a base model instance M; is selected
from a model family, which acquires trainable parameters
@ through learning on the training dataset J. During the
inference phase, in addition to the input x, external context
F (such as documents retrieved or tool execution results)
may be optionally introduced to enrich the conditional
information. Thus, the entire process can be expressed as
an estimation of the conditional generation probability:
P(y | x, E,M;,T, 0), equivalently, it can also be written as
P(y | Mi(a,E | T,@)) emphasize the probability that M;
trained on T to obtain 6 generates y given (2, FE’) (when
external information is not used, simply set F = 9).

2.2.1

For a given output y, under the input prompt and external
data (x, E), training set T, and candidate model family or
human M (with parameters denoted as 0), Let S(g, M,-)
be the sourcing function. Posterior-based sourcing operates

Unified Definition of Posterior-based Sourcing

5

without pre-embedded markers, instead quantifying prove-
nance through sensitivity analysis of the output posterior
distribution P(y | -). Attribution derives from either max-
imizing the generation likelihood or computing gradient-
based influences:

OP(y | Mi(z, E | T,@))

SPS (y, M,-) = argma:
(y ) a Osource

source

?

where source can be a model instance M;, a structural
component parameter 0 C 0,a training data subset To C T,
or an external input Ey C HE. This approach retrospectively
analyzes inherent model properties and output distributions
without prior modifications.

Model sourcing focuses on identifying the most probable
model instance responsible for generating a specific output,
given the input and parameter configurations. This process
isolates the model’s contribution by marginalizing external
data influences. Formally, this is defined as:

SPS (y, M0) = argmax P(y | Mi(a,E|T,@)), ()

where @) signifies the exclusion of external data dependen-
cies. The solution identifies the optimal model M; that
maximizes the posterior probability of generating g under
its parametric and structural constraints.

Model structure sourcing quantifies the contribution of
specific architectural components (e.g., layers, neurons) to
the generation process by analyzing gradient-based sensi-
tivity of the output posterior distribution. This is formally
expressed as:

OP(y | M(x, E | T,?))
066 ,

where MM, denotes a structural component parameterized
by 90 C 9, and the attribution score reflects the directional
influence of 0) on the conditional likelihood P(y | -). The
absence of external data (@) ensures attribution is confined
to intrinsic architectural contributions.

Training data sourcing measures the causal influence
of individual training samples on the model’s output by
computing the gradient of the posterior probability with
respect to training data partitions. This is defined as:

OP(y | M(x, E | T,6))
OTo ,

where D represents the training dataset, 7) C D denotes a
subset of training samples, and the partial derivative quan-
tifies how perturbations in To affect the output likelihood.
This formalism enables tracing y’s provenance to specific
training instances.

External data sourcing evaluates the impact of contextual
or auxiliary inputs on the generation process by analyzing
the sensitivity of the posterior output distribution to varia-
tions in external data. Formally:

OP(y | M(x, E | T,@))

OE ,
where C’ (a subset of £) represents external context, and Eo
denotes a specific input component. The attribution score

captures the extent to which Eo modulates the model’s
predictive confidence in generating y.

SE (y, Ms, 0) = argmax
0060

(2)

eet (y, M, D) = argmax
0

(3)

get (y, M, C) = argmax
0

(4)


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

2.2.2 Unified Definition of Prior-based Sourcing

For a given output g, under the input prompt and external
data (x, E), training set T, and candidate model family or
human MM (with parameters denoted as 6), Let S(g, M,-) be
the sourcing function. Prior-based sourcing embeds iden-
tifiable markers (e.g., watermarks or signatures) during
model training or content generation. Attribution is then
performed by detecting these markers in the generated
outputs and evaluating posterior probabilities conditioned
on their presence. Formally, the first step adds mark to
the source and applies Bayes’ rule to rewrite P(y | -) as
P(- | y), while omitting normalization constants that are
independent of the candidate source.

Step 1: add marker

SPror(y M,-) = argmax P(y | M;(a, E | T,) + source’)

source

= argmax P(M;(x, E | T,0) + source’ | y) ,
source
(5)

where source can be a model instance M;, a structural
component parameter 99 C 6,a training data subset To C T,
or an external input Ey C £. And source’ denotes the
additional effect of embedding markers into the candidate
source. Under the assumption that the main model compo-
nent and the marker component are conditionally indepen-
dent given y, the joint probability can be decomposed as
below.

Step 2: decompose joint probability

SPY, M,-) = argmax P(M;(z, E | T,@) | y)-

source

P(source’ | y).

Since P(M;(x, E | T,@) | y) is approximately invariant
across different candidate sources, it can be regarded as
a constant. Thus, the decision reduces to comparing the
posterior probabilities of the markers.

Step 3: detect marker

SPror(y M,-) = argmax P(source’ | y). (7)
source

This procedure identifies the candidate source whose
embedded marker attains the highest posterior probability
of being detected in the generated output. A critical feature
of prior-based sourcing is that it requires explicit modifica-
tions to the training or generation pipeline to embed such
markers in advance. The central idea is that attribution is
not performed on the raw output y itself, but instead on the
detectable marker carried within y. Consequently, the at-
tribution task reduces to determining which source-specific
marker is present, thereby establishing the provenance of
the output.

Model Sourcing identifies the most probable model in-
stance responsible for generating an output by leverag-
ing pre-embedded prior information. This process isolates
model contributions through explicit detection of embedded
markers:

Sir" (y, M, 0) = argmax P(y | Mi(«, E | T,0) + Mj)

. (8)
= argmax P(M; | y),

6

where M/ is the embedded marker for model M;, and 0)
indicates the exclusion of external data. The solution iden-
tifies the optimal model M;, that maximizes the posterior
probability of generating marker under the y.

Model structure sourcing quantifies the contribution of
specific architectural components (e.g., layers, neurons) by
linking them to embedded structural markers:

Se'"(y, Ms, 0) = argmax P(y| M;(x, B | T,0) +65)
O0€0
; (9)
= argmax P( | y),
O0€0
where M, denotes a structural component parameterized
by 00 C 9, and 9% is the embedded marker. The attribution
score reflects the posterior probability that 6) was responsi-
ble for generating the observed signal.
Training data sourcing traces the causal origin of the
output y to specific training samples by associating them
with embedded markers:

Sp"(y, M, D) = argmax P(y| Mi(w, E | T,6) + T))

To€T
, (10)
= argmax P(T} | y),

ToET

where D is the training dataset, Typ C D is a candidate
subset of samples, and Tj is the embedded marker. The
posterior probability P(Tj§ | y) quantifies how likely T)
influenced the generation of y.

External Data Sourcing evaluates the impact of con-
textual or auxiliary inputs on the generation process by
analyzing their association with the pre-embedded marker
E’:

Sh"(y, M,C) = argmax P(y| Mi(x, B | T,8) + Eo)
EocE
= argmax P(E}, | y),
EgeEkE

(11)

where C’ C FE is the external context, Hy denotes a specific
external input(e.g., a marked prompt prefix), and £7 is its
marker. The posterior probability P(Ej | y) captures the
extent to which Ej directly modulates the generation of y.
This formalism ensures explicit tracing of y’s provenance to
external inputs with embedded identifiers.

2.2.3 Prior-based Sourcing vs. Posterior-based Sourcing

In the previous section, we formally defined two categories
of model tracing paradigms: posterior- and prior-based.
This section will discuss and compare their typical work-
flows and core characteristics.

Posterior-based sourcing is flexible and non-intrusive,
as it works with existing models and data. However, its
conclusions are inherently probabilistic, relying on statistical
consistency rather than definitive markers.

Typical workflow consists of:

1) Collecting the model’s output along with necessary
contextual information (e.g., prompts, retrieved docu-
ments).

2) Comparing outputs against candidate models, datasets,
or external evidence through likelihood estimation or
representation similarity.

3) Estimating the influence of each candidate and provid-
ing probabilistic or ranked attribution results.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

Key characteristics include, (1) Retrospective analysis:
Attribution occurs after generation. (2) No model modifica-
tion needed: Can be applied to existing models and outputs
directly. (3) Inference based on inherent properties: Relies
on patterns in outputs or representations. (4) Probabilistic
attribution: Often less direct and less certain than explicit
markers.

Prior-based sourcing provides explicit and verifiable at-
tribution, but it requires modifications to the data prepa-
ration pipeline or model training process, making it less
flexible in deployment.

The workflow consists of:

1) Embedding markers into the data or model beforehand.

2) Performing standard training and inference.

3) Detecting markers in the generated outputs to deter-
mine attribution.

Key characteristics include, (1) Proactive mechanism:
Markers are embedded before generation. (2) Explicit iden-
tification: Can directly verify the responsible source. (3)
Dependent on marker robustness: Attribution quality relies
on the detectability and stability of embedded information.
(4) Requires system changes: Typically needs adjustments to
training or inference procedures.

In practice, these paradigms are complementary: prior-
based methods offer reliable and deterministic attribution
but require infrastructure changes, while posterior-based
methods are more flexible and broadly applicable, albeit
with probabilistic certainty.

3 SOURCING FROM MODELS

Model sourcing in LLMs refers to the process of determin-
ing and identifying the generated text from humans or a
specific LLM. Accurately identifying the source of human-
written and machine-generated content is fundamental to
combating misinformation, avoiding the misuse of machine-
generated text, and ensuring the authenticity of content.
Additionally, precise identification of specific models pro-
vides robust protection for model intellectual property, en-
abling developers to detect unauthorized use, copying, or
infringement of their proprietary models. Research meth-
ods are broadly categorized into posterior-based and prior-
based methods. The posterior-based analyzes existing text,
attributing it to a specific model. This traceability is crucial
for independent verification and infringement detection,
even when the model’s internal structure is inaccessible.

This chapter provides a systematic review of model
provenance, covering the following aspects: task defini-
tion, datasets and evaluations, prior-based model sourc-
ing, posterior-based model sourcing, challenges and future
work. These discussions aim to introduce the development
trajectory and key technologies in the field, providing valu-
able suggestions and assistance for future research.

3.1. Task Definition

The model sourcing task aims to trace and identify the
origin of generated text by analyzing its linguistic and
structural characteristics, distinguishing between human-
authored content and outputs from specific LLMs. This
task fundamentally constitutes a multi-class classification

Posterior-based Model Sourcing

White-box Token/Words Prob
Intrinsic
2, Probabilistic
Features
Vs
Black-box
Fine-tuning ied Sy)
Characteristic ke +—| Feature engineering
Adversarial learning |—> ===

Prior-based Model Sourcing

White-box

6 Top K Logits with Tag
Tag

I

Black-box

Watermarking

Fig. 3. The methods for model sourcing is divided into two categories:
prior-based methods and posterior-based methods, each of which is
further divided into white-box methods and black-box methods.

problem, wherein the objective is to attribute generated
text samples to their precise source within a predefined
set of candidate models (e.g., GPT, DeepSeek, LLaMA)
or human creators. The classification process enables fine-
grained attribution by leveraging subtle stylistic, syntactic,
and semantic patterns unique to each model or human
writing.

Let a text sample be denoted as 2;, with its corre-
sponding label y; € \ representing the source category,
comprising K classes (including human and K-1 LLMs). The
dataset is defined as D = {(21, y1), (@2, y2),---5(@n; Yn) },
where each instance (2;, y;) € D. The predictive function of
the classifier is formalized as j = f(x, 6), parameterized by
6, which outputs a probability distribution vector 7 € R*.
This vector quantifies the posterior probability of the text
belonging to each candidate class:

where KK = || signifies the total number of classes.

3.2 Datasets and Evaluations

This section introduces commonly used datasets and met-
rics for the task of model tracing. The dataset section is
presented in tabular form to provide detailed information
on various aspects.

3.2.1 Datasets

This section introduces datasets related to model tracing.
In general, such datasets include human-authored texts
along with texts generated by at least one model, involving
models such as ChatGPT, BloomZ, LLaMA, Dolly, GPT-2,
GPT-3, GPT-4, Davinci, FlanT5, and others. Table [1]presents
commonly used datasets for this task.

The Human ChatGPT Comparison Corpus (HC3)
comprises approximately 40K questions and answers. The


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

TABLE 1
Model sourcing datesets used in recent literature

Dataset Domain Language(s) Year Size Project Page
HC3 Finance, Law, Medicine, Psychology, Wiki, QA EN, CH 2023 37175 Link
TweepFa Tweet EN 2020 25,572.
CHEAT 179) Computer Science Abstract EN 2023 50,699 Link:
M4 Wiki, News, Academic Writing, QA AR, BU, CH, EN, IN, RU, UR _2023-~—s- 147,000 Link
MG 181) QA EN 2023 1,819 Link
Ghostbuster 182] Creative Writing, News, Student Essays EN 2023 = 12,685
ces Code EN 2022 13610 Link)
APPS Code EN 2021 10,000 Link
Argu Essay EN 2023 8153 Link:

answers are sourced from ChatGPT and human experts,
including domain-specific experts, high-voted answers by
web users, Wikipedia, and Baidu Baike. The dataset contains
both Chinese and English text and covers domains such
as open-domain, computer science, finance, medicine, law,
and psychology. TweepFake provides 25,572 authentic
tweets from Twitter, equally split between human users and
23 bot accounts. CHEAT [179], the first ChatGPT-written
abstract dataset, contains 15,395 human abstracts from [EEE
Xplore and 35,304 ChatGPT-generated abstracts in com-
puter science with 30 keywords. The multi-generator, multi-
domain, multi-lingual (M4) corpus spans Wiki, News,
Academic Writing, and Question Answering domains across
seven languages (English, Chinese, Russian, Urdu, Indone-
sian, Arabic, Bulgarian), totaling 133,551. MGTBench
serves as a machine-generated text(MGT) detection bench-
mark with reference implementations and datasets in-
cluding TruthfulQA, SQuAD1, and NarrativeQA. Ghost-
buster covers creative writing, news, and student
essays. The creative writing dataset is sourced from the
website with prompt-based craft stories. The news dataset
is based on the Reuters 50-50 authorship identification
dataset [186]. The student essay dataset consists of high
school and university level essays across a range of disci-
plines. CodeContests aggregates problems, solutions,
and test cases from Codeforces and existing programming
datasets in C++, Python, and Java. APPS comprises
problems from various open programming websites, in-
cluding 10,000 coding problems, 131,777 test cases, and
232,421 human-authored Python solutions. ArguGPT
contains 4,115 human-written and 4,038 machine-generated
argumentative articles based on WECCL, TOEFL, and GRE
prompts.

In summary, many datasets in the NLP domain can
be utilized as prompts and questions for generating text
using LLMs. However, a primary challenge lies in the often
noisy nature of directly generated text, requiring further
refinement. Another challenge is that a classifier trained
on one dataset may not necessarily perform well on other
datasets.

3.2.2 Metrics

This section introduces some commonly used classification
metrics for assessing performance. Most of these metrics are
commonly used in the field of NLP for classification tasks.
The commonly used metrics include:

Accuracy represents the proportion of correctly classified
samples out of the total number of samples. It is the most
straightforward evaluation metric. However, in imbalanced
datasets, accuracy may not be a robust metric. Precision
represents the proportion of samples correctly predicted as
the positive class out of all samples predicted as the positive
class by the model. It is useful for assessing the accuracy of
the model’s positive predictions. This metric is particularly
important when classifying human-authored and machine-
generated text. A low precision indicates that many human-
authored texts are being classified as machine-generated,
which would be unfavorable in real-world applications.
Recall represents the proportion of positive class samples
successfully captured by the model out of all true positive
class samples. It is used to measure the model’s sensitivity.
Fl-score is the harmonic mean of precision and recall. It
is particularly suitable for imbalanced datasets [32]. The
Receiver Operating Characteristic(ROC) curve [21], is
a graphical representation with false positive rate (FPR) on
the x-axis and true positive rate (TPR) on the y-axis. AUC
is the area under the ROC curve and is used to measure the
model’s ability to distinguish between positive and negative
samples. For the efficiency of model sourcing, an important
metric is the Time, which measures the time it takes to detect
the text [29]. When the detection speed of the method is
significantly lower than the text generation speed, the value
of that method diminishes.

3.3 Posterior-based Model Sourcing

Posterior-based model sourcing exploits intrinsic statistical
properties of generated text or internal activation patterns
to differentiate human from machine authorship. It can be
categorized into white-box methods, which leverage inter-
nal model characteristics, and black-box methods, which
operate without model access through either supervised or
unsupervised zero-shot approaches.

3.3.1 White Box Methods

This section examines several representative white-box
methods that leverage intrinsic model characteristics, such
as statistical distributions of token probabilities, model
perplexity, and curvature properties of the log-probability
function, for distinguishing human and Al-generated text.
Early work, such as GLTR [20], demonstrates that ma-
chine text is typically more predictable than human writ-
ing and visualizes this via rank/probability/entropy statis-
tics over top-k predictions. Building on a curvature view,


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

Mitchell et al. introduced DetectGPT as a prominent
zero-shot method, observing that machine-generated text
often resides in regions of negative curvature within the
model’s log-probability function. Leveraging this charac-
teristic, they proposed a curvature-based criterion where
perturbing text consistently decreases the log-probability
of machine-generated text, while human-authored text ex-
hibits variable changes. DetectGPT consistently delivered
superior detection accuracy. Furthermore, detection per-
formance peaked when the detection model aligned with
the model used for log-probability calculation. Following
DetectGPT’s emergence, several related methods rapidly de-
veloped. Bao et al. developed Fast-DetectGPT to reduce
high computational costs by introducing the concept of con-
ditional probability curvature, which explains vocabulary
selection differences between humans and large models. By
computing all necessary conditional probabilities in a single
forward pass, this approach drastically reduces expensive
model calls, resulting in not only enhanced performance,
but also a 340-fold speed increase. Additionally, Yang et
al. proposed DetectGPT4Code to tackle challenges in
detecting machine-generated code, achieving state-of-the-
art results on the CodeContest and APPS datasets by em-
ploying an alternative white-box model to estimate next-
token probabilities, leveraging unique statistical properties
in code structure.

Complementing these statistical/curvature detectors,
watermarks are also commonly used for model sourcing,
predominantly as prior methods; however, there are also
some posterior-based watermarking approaches, which add
watermarks to generated text. In deception attacks, attackers
maliciously alter the semantic content of watermarked text
while ensuring the watermark remains detectable. To ad-
dress this issue, a posteriori semantic-aware watermarking
framework has been proposed. It employs a semantic
mapping model that generates green-red token lists based
on the entire input text rather than solely on preceding
context, thereby achieving global semantic awareness.

In summary, white-box posterior approaches capitalize
on intrinsic likelihood and representation signals to separate
human from machine text with fine granularity. This line
of work advances reliable attribution by turning model
internals into stable, verifiable evidence.

3.3.2 Black Box Methods

Black box methods do not require access to the internal
structure of the model when detecting and mainly learn the
characteristics of human-written text and machine-written
text to detect. It mainly includes methods such as fine-
tuning models, feature/statistical pattern-based methods,
and adversarial/dynamic learning frameworks.

A significant category of posterior black-box methods
relies on fine-tuning pre-trained models for detection. These
methods leverage the representational power of Transform-
ers and adapt them to distinguish between human- and
machine-authored texts. For example, Transformer-based
classifiers fine-tuned on ChatGPT outputs demonstrated
superior performance over perplexity-based baselines 25].
Similarly, fine-tuned RoBERTa consistently outperformed
LSTM, HAN, BERT, and XLNet classifiers in domain detec-
tion tasks, achieving transferability with limited in-domain

9

data [26]. OpenAl’s own fine-tuned RoBERTa detector
showed high in-distribution accuracy, though its general-
ization weakened across model scales More advanced
frameworks, such as the COCO model [30], incorporated
coherence graphs and supervised contrastive learning to
enhance detection, especially under low-resource settings.
Earlier systematic comparisons highlighted that fine-
tuned BERT-LARGE classifiers could detect GPT-2 text more
reliably than human evaluators, particularly due to statisti-
cal regularities arising from decoding strategies.

Feature engineering and statistical anomaly detection
methods offer interpretable and often resource-efficient
alternatives. Ghostbuster demonstrated how inter-
pretable features derived from token probability distribu-
tions of weaker models can be combined into effective
classifiers.

Other approaches design handcrafted features [28], or
employ graph-based models such as FAST to capture co-
herence and factual structure at entity and sentence lev-
els (31). Statistical anomaly-based detectors include un-
supervised approaches amplifying n-gram repetition sig-
nals [36], Raidar’s edit-distance method exploiting differen-
tial rewriting behaviors of humans and LLMs (33), and zero-
shot methods like GECSCORE that detect human grammat-
ical errors via GPT-40-Mini correction comparison [34].

As discussed earlier, Fast-DetectGPT [22] and Detect-
GPT4Code also design alternative black-box meth-
ods. Fast-DetectGPT uses surrogate models for conditional
probability estimation 22], DetectGPT4Code substitutes a
smaller white-box code language model as a substitute to
estimate the probability curvature [23]. DetectGPT-SC
achieves zero-shot classification based on masked prediction
consistency under the assumption that LLMs can effectively
infer masked portions of their own generated text.

There are also some methods to add watermarks to
the generated text under the condition of not altering the
semantics, making it detectable. PostMark rewrites the
text to incorporate the semantic watermark words, while
Water-Probe statistically analyzes repeated sampling
outputs for watermark detection by designing prompts.

Furthermore, a growing body of work employs adver-
sarial or dynamic learning frameworks to enhance detec-
tion robustness. RADAR trains detectors jointly with
adversarial rewriters, creating a closed-loop optimization
that strengthens detection against adaptive attacks. OUT-
FOX [40] takes this further by constructing an adaptive
adversarial framework where both detectors and attackers,
powered by LLMs like ChatGPT, evolve together through
contextual learning and adversarially generated examples.
Such methods explicitly address the dynamic co-evolution
of detection and generation strategies.

Finally, hybrid dictionary and proxy-based frameworks
offer scalable alternatives. LLMDet avoids runtime LLM
queries by precomputing model-specific dictionaries of n-
gram frequencies and token probabilities, achieving efficient
source attribution with strong accuracy and reduced com-
putational cost.

3. https://github.com/openai/gpt-2-output-
dataset /tree /master/detector
4. https: / /openai.com/research/gpt-2-1-5b-release


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

In summary, black-box posterior detectors provide
scalable, model-agnostic routes to authorship attribution.
Through discriminative training, interpretable features, and
statistical anomaly cues, they deliver robust decisions with-
out requiring access to logits or parameters. This direction
strengthens real-world deployability by balancing accuracy,
interpretability, and resilience to adaptive attacks.

3.4 Prior-based Model Sourcing

Prior-based methods introduce watermarking or model-
specific signatures during inference to enable authorship
tracing and intellectual property protection. White-box
methods modify generation processes, such as dividing
vocabularies into green and red lists and adjusting to-
ken probabilities. Black-box methods, addressing closed-
source model constraints, construct observable signals
through synonym substitution, contextual hashing, or back-
door-style triggers.

3.4.1. White Box Methods

Adding a watermark involves embedding a hidden repre-
sentation into the text. Such text appears indistinguishable
to the human but can be detected by algorithms, which
can help prevent misuse of generated text at its source. In
simple terms, adding a watermark involves incorporating
a selection strategy into the process of generating text. It
entails dividing the vocabulary of LLMs into distinguishable
“green list” and “red list”.

Kirchenbauer et al. propose a dynamic list division
method, where previous tokens s‘~»)...s—) including
the prompt and generated tokens are hashed to produce a
random seed before the next word is generated. This seed is
then used to divide the vocabulary into green and red lists,
from which the next word is sampled only from the green
list. Based on this method, Kirchenbauer et al. introduce
enhanced hashing approaches such as SelfHash, as well as
advanced detection techniques like WinMax.

To address the limitations in previous methods like
KGW [Al], WaterSeeker adopts a “locate first, detect
later” strategy to pinpoint and verify watermarked seg-
ments in large corpora. It addresses the limitation where wa-
termarked segments embedded in larger documents would
otherwise fail to be detected. In parallel, inference-time
watermarking methods that modify output probabilities
during text generation can be easily circumvented since
users have access to the model’s source code. To address
this problem, Xu et al. explore parameter-embedded
watermarking. Backdoor Watermarking embeds signals by
training LLMs to respond with specific outputs when en-
countering triggers, while Runtime Watermark Distillation
integrates existing runtime watermarking features directly
into model parameters. Hu et al. proposed an unbiased
watermarking technique that balances content traceability
and quality assurance, ensuring that the model output con-
tains watermarks without affecting the performance of the
model in downstream tasks.

Adaptive watermarking techniques extend these foun-
dations. Liu et al. introduce adaptive watermark token
identification, semantic-based logits scaling vector extrac-
tion, and adaptive watermark temperature scaling, which

10

collectively aim to selectively watermark tokens, dynami-
cally scale logits, and minimize text quality degradation.
SymMark further advances adaptability by combin-
ing logit-based and sampling-based methods under Serial,
Parallel, and Hybrid strategies to balance detectability, text
quality, robustness, and security. Building on this, Wang et
al. demonstrate that watermark strength correlates with
the cumulative probability of green-list tokens and propose
MorphMark, a scheme that dynamically adjusts watermark
intensity to optimize the trade-off between detectability
and naturalness. SynthID-Text is a generative text
watermarking technology based on the tournament sam-
pling algorithm. It embeds statistical features by modifying
the sampling process of LLMs, supports both distortion-
free (quality-preserving) and distortion-enabled (detection-
enhanced) modes, and has innovatively combined with
speculative sampling to achieve production-level efficient
deployment.

Watermark security is a critical dimension. Kirchenbauer
et al. emphasize key-based random number generation
to ensure private watermarking. Liu et al. propose using
two separate neural networks for generation and detec-
tion while sharing token embedding parameters, giving the
detector prior knowledge. The asymmetry of computation
further guarantees non-forgeability by preventing erasure
or counterfeit watermark creation.

The retention of watermarks after model distillation has
sparked discussions among researchers. Distillation Resis-
tant Watermarking (DRW) safeguards NLP models
against theft through distillation. By embedding water-
marks tied to specific keys into the target model, the stolen
models inherit the same watermark, making it possible
to trace unauthorized use and detect intellectual property
violations. Yu et al. investigated the robustness of
LLM watermarking schemes against adversarial knowledge
distillation, demonstrating that current n-gram based water-
marks can be effectively removed using proposed targeted
paraphrasing and inference-time neutralization methods
while largely preserving student model performance.

In summary, prior-based white-box sourcing embeds at-
tribution signals during generation, enabling fast, verifiable
detection after the fact. Careful control of sampling and
parameterization allows practitioners to tune the trade-off
between text quality, detectability, and robustness, while
key management and privacy safeguards support secure
operation at scale. This paradigm shifts provenance from
post-hoc inference to enforceable design.

3.4.2 Black Box Methods

The research on watermarking methods for large language
models can be organized around the fundamental challenge
of black-box constraints, where model parameters and logits
are inaccessible. This limitation restricts traditional white-
box watermarking techniques and has prompted a range of
novel approaches designed to ensure ownership verification
and intellectual property protection under such conditions.

Recent approaches pursue complementary levers: text-
space perturbation, data-space backdoors, and agent-level
behavior shaping. On the text side, Yang et al. en-
code a word-level traceability signal via dynamic binary
coding with contextual hashing and synonym substitution,


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

driving the bit-1 proportion in watermarked text to devi-
ate markedly from randomness (over 50%) while preserv-
ing meaning through semantic constraints; crucially, this
supports third-party API users who cannot access inter-
nals. From the data-space perspective, backdoor-style wa-
termarks poison training with covert trigger compositions
so the model learns input-dependent behaviors usable for
ex-post claims of ownership. Gu et al. instantiate this
with WLM, showing robustness to downstream fine-tuning
and over 90% extraction success across tasks using com-
binations of common words as stealthy triggers. Moving
beyond token outputs, Agent Guide embeds behavioral
watermarks by decoupling high-level intentions (behavior)
from low-level actions, injecting probabilistic biases at the
decision layer while keeping action realizations natural.

In summary, prior-based black-box schemes pursue
ownership verification under strict access constraints. By
operating in text, data, or behavior space, they preserve se-
mantics while yielding detectable, tamper-resistant signals
that survive fine-tuning and API boundaries. This research
opens a practical path to provenance assurance when model
internals are unavailable.

3.5 Challenges

Significant contributions have been made in the field of
model sourcing for distinguishing human and machine-
generated text. However, several challenges persist.

Firstly, data-related issues remain prominent. While
datasets exist across various domains, researchers often
generate supplementary data using language models. These
models frequently impose constraints, such as maximum
text length limits during generation. This can lead to length
distribution discrepancies between machine-generated and
human-authored text.

Furthermore, controlling the specific domain of gener-
ated text via prompts presents difficulties. When such dis-
tributional differences are pronounced, classifiers may per-
form well on these generated datasets, but this performance
does not necessarily translate to real-world applications.

Additionally, robustness experiments featured in some
studies typically involve perturbing text by adding, re-
moving, or replacing words. Yet, such perturbations are
less common in practical scenarios. For instance, academic
plagiarism might involve refining portions of ChatGPT-
generated text rather than making minor word-level edits.
Effectively simulating and countering the types of pertur-
bations encountered in real-world settings remains a signif-
icant standardized challenge for this task.

Current research demonstrates the ability to distinguish
human from machine-generated text with high accuracy.
Future work will likely shift towards tracing specific large
model sources, evolving the task from binary classification
to multi-class classification. This progression holds consid-
erable practical significance, such as enabling the attribution
of responsibility for harmful text content.

4 SOURCING FROM MODEL STRUCTURES

Sourcing from model structures in large language models
involves identifying and analyzing how different archi-
tectural components, such as specific layers and attention

11

heads, contribute to generated outputs. This approach is
crucial for understanding the black box problem of LLMs.
By tracing model outputs back to specific architectural
components, this methodology provides precise pathways
for model optimization, enables enhanced model safety
through targeted interventions, and establishes a solid tech-
nical foundation for accountability and regulatory compli-
ance in high-risk applications.

This chapter provides a systematic review of sourcing
from model structures, encompassing several key facets: the
definition of the task, relevant datasets and evaluation pro-
tocols, methodologies, and an outlook on future research.
The aim is to thoroughly clarify the foundational concepts,
critical techniques, and potential pathways for advancement
in this field, thereby offering valuable insights and direction
for both academic research and real-world applications.

4.1. Task Definition

We define the task of sourcing from model structures as
identifying the contributions of a model’s internal compo-
nents to a specific output. This process seeks to determine
which structural elements are most influential in generating
a particular prediction or behavior. Ultimately, the goal is
to elucidate the causal mechanisms between the model’s
internal architecture and its final output.

Abstractly, an LLM architecture can be viewed as a
set of components, denoted by C = {c1,¢2,...,Cm}. Each
component c; represents a distinct, analyzable unit, such
as an attention head, an FFN layer, or a specific group of
parameters. The model itself is a function f(x; ©) that maps
an input x to an output y, conditioned on the complete set
of parameters O that comprises all components.

Given a specific input x and its corresponding output
y, the objective of sourcing from model structures is to
compute a score S(c;,x,y, f) for each component c;. This
score quantifies the importance or causal contribution of
component c; to the generation of the output y.

4.2 Datasets and Evaluations

The deep architectures and training data of large language
models result in an inherent opacity, making it difficult to
attribute their outputs to internal mechanisms 2), [4]. To ad-
dress this challenge, the research community has developed
specialized datasets and evaluation protocols to systemati-
cally probe the inner workings of these models [187].

4.2.1 Datasets

A foundational dataset is LAMA (Language Model Analy-
sis), which uses cloze-style prompts to examine how pre-
trained models store and retrieve factual knowledge '
Building on this idea, ZsRE and COUNTERFACT
introduce counterfactual statements that support causal
tracing and interventions, enabling localization of facts to
layers, neurons, or attention heads. As the field moves
toward compositional knowledge, MQuAKE targets multi-
hop questions to test whether architectural pathways sup-
port coherent multi-step retrieval [190]. To assess stability,
StableKE formalizes principles for robust knowledge en-
coding and releases the tree-structured KEBench to bench-
mark locality and persistence under edits and distributional


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

Posterior-based Structure Sourcing

h | yceceeee h

oh v
Prior-based Structure Sourcing

Expert LLM 1

@ Connection Modular

@® Model-level

lao
et aa)
Expert LLM 3

@ Layer-level

layer ®
h Ve —Ee® Se h
layer @ ;

Mixture-of-Experts

Fig. 4. The methods for structural sourcing can be categorized
into two types: posterior-based and prior-based. Within posterior-
based structural sourcing, there are two approaches: single-modular
and connection-modular. For prior-based structural sourcing, the ap-
proaches are divided into model-level and layer-level.

shifts [89]. For multilingual settings, MLaKE aligns fact
chains across languages to examine cross-lingual represen-
tation and transfer (191). UnKEBench, built from unstruc-
tured text with dense, interwoven facts, enables fine-grained
analyses of entanglement and where complex knowledge
resides within model structure (90). Finally, S?RKE targets
related-knowledge interference by organizing edits around
the same subject to quantify how updating one fact per-
turbs semantically proximate facts and to measure trade-offs
among edit success, locality, and preservation of neighbor-
ing knowledge [192].

As summarized in Table |2} several datasets, including
LAMA, ZsRE, COUNTERFACT, and others, provide bench-
marks for assessing model knowledge sourcing, stability,
and multilingual capabilities.

To diagnose security vulnerabilities, behavioral mis-
alignment, and mechanism-level causes, we rely on a com-
pact yet representative set of safety and reliability bench-
marks. RealToxicityPrompts (RTP) provides 100k web-
derived prompts with toxicity scores to stress-test toxic
degeneration [193]; for active robustness, JailbreakBench
standardizes adversarial jailbreak behaviors and attack
prompts across models [194]; for factual reliability, Truth-
fulQA comprises questions whose common, intuitive an-
swers are false, isolating resistance to misinformation pri-
ors [195]; and beyond behavior, InterpBench contributes
86 IIT-trained mini-Transformers with documented algo-
rithmic subgraphs, serving as a unit testbed for circuit-
level interpretability and causal probes [196]. Together, these
resources cover passive safety, adversarial robustness, truth-
fulness, and circuit-level analysis while keeping evaluation
lightweight; importantly, they are used not only for struc-

12

tural provenance and interpretability, but also for routine
model evaluation, safety benchmarking, ablation studies,
and regression testing across releases.

4.2.2 Evaluation Protocols

To rigorously assess provenance claims and the fidelity of
structural explanations, recent studies adopt hybrid evalu-
ation protocols that integrate behavioral verification with
causal interrogation of model internals [57], [73]. These
protocols are designed to establish not only whether inter-
ventions achieve the desired outcome, but also whether they
preserve non-target behavior and identify components that
exert genuine causal influence rather than mere correlations.
The primary dimensions of evaluation include:

e Edit Success and Locality. Evaluate the effectiveness
of edits through the Edit Success Rate (ESR) while
simultaneously measuring preservation on non-edited
facts and control locality (Specificity). Metrics typically
include bounding non-target A accuracy or A logit
following an edit [73].

e« Compositional and Temporal Consistency. In multi-
hop reasoning or time-sensitive contexts, measure the
consistency of information propagation across reason-
ing paths, robustness under paraphrases or templated
variants, and stability under temporal drift (89), [190].

e Cross-lingual Transfer. For multilingual provenance
traces, assess whether edits or localized components
generalize across aligned factual chains in different
languages, while avoiding collateral degradation in un-
related behaviors [191].

e Safety and Reliability. Complement provenance test-
ing with safety-oriented benchmarks, such as toxicity
rates [197], jailbreak resistance [198], truthfulness [195],
and refusal precision [199], ensuring that structural
interventions do not compromise critical safeguards.

e Causal Validity. Apply interventions such as activation
patching, attention manipulation [200], or weight per-
turbation to quantify causal effects on target logits. Re-
port effect sizes, average treatment effect (ATE) deltas,
and minimal-sufficiency criteria.

e Evidence Faithfulness. When provenance relies on
external evidence, approximate human evaluation via
automated metrics: ALCE for citation precision us-
ing NLI [133]; citation overlap against gold-standard

sets ; LLM-as-judge frameworks for flexible scor-

ing boop and ALiiCE, which augments claim pars-
ing with positional dispersion (CVCP) for fine-grained
evaluation [203].

¢ Circuit-Grounded Scoring. On synthetic benchmarks
such as InterpBench, measure component discovery
through subgraph recovery F1, edit distance, or suc-
cessk for neuron naming and circuit reconstruc-
tion [204]. These criteria enable unit-test-style valida-
tion of structural claims.

In practice, scalable automatic metrics are paired with
targeted human spot-checks, pre-intervention and post-
intervention testing suites. This dual strategy ensures that
evaluations remain reproducible while being sensitive to
fine-grained causal attributions.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

13

TABLE 2
Representative benchmarks on sourcing from model structures.

Dataset Domain Language(s) Size Year
ZsSRE Wikipedia EN 11,301 = 2017
LAMA Wikidata EN 34,000 2019
COUNTERFACT Wikipedia EN 21,919 2022
MQuAKE Wikipedia EN 11,043 2023
KEBench Wikidata EN 3,798 2024
UnKE Wikipedia EN 1,000 2024
MLaKE Wikipedia EN, ZH,JA,FR,DE 9,432 2025
S°RKE YAGO EN 22,064 2025

4.3 Posterior-based Structure Sourcing

Posterior-based structure sourcing refers to the process of
tracing the structural origins of capabilities within a trained
model. This field of research is broadly divided into two
primary approaches [205], [206].

The first approach is single modular function local-
ization. This perspective posits that a model’s capabilities
and behavioral characteristics can be pinpointed to specific,
relatively independent components or regions [56], [73],
[74]. The second approach is connection modular function
localization. In contrast to the former, this viewpoint con-
tends that a model’s capabilities do not arise from isolated
modules but instead manifest as systemic properties that
emerge from the dynamic collaboration among multiple
components [89}, [90].

This approach proves valuable as it enables deeper un-
derstanding of how different model components contribute
to various behaviors and capabilities. Furthermore, it facil-
itates targeted interventions and optimizations without re-
quiring prior structural modifications, thereby enabling flex-
ible post hoc analysis and selective manipulation of model
behavior while preserving the original architecture.The cat-
egorization of these methods is summarized in Table
providing a structured overview of existing approaches.

4.3.1 Sourcing from Single Modular

Model Capability Region. The capabilities of large models
include multilingual capability, semantic grammar capability,
mathematical computation capability, and creative capability,
each of which has its own independent partitions. For
semantic grammar capability, Geva et al. identifies that
transformer feed-forward layers act as key-value memo-
ries, with keys detecting specific input patterns and val-
ues inducing corresponding output distributions. Lower
layers capture shallow patterns, while upper layers detect
complex semantics, forming a hierarchical memory system
influencing model outputs. Geva et al. reveals that
specific parameter vectors within transformer FFN layers
act as “concept promoters”, adding updates to token rep-
resentations that enhance the probability of certain vocab-
ulary items, thereby influencing the model’s predictions.
LM-Debugger locates unsafe regions in transformer
LMs by decomposing FFN layer updates into sub-updates
linked to specific parameters, identifying those that promote
undesired concepts, and enabling targeted interventions to
modify the model’s behavior.

Dai et al. identified the existence of “knowledge
neurons” in Transformer models and updated factual in-

formation by manipulating these Knowledge Neurons (KN)
in pretrained Transformers. Meng et al. proposed a
method to study the storage of key FFN weights and
combined it with the “Rank-One Model Editing (ROME)”
approach to edit factual associations in large language
models. Subsequently, Meng et al. further expanded
the application scenarios of ROME and developed MEMIT,
enabling the batch editing of multiple pieces of knowledge
stored in the model structure at once.

For mathematical computation capability, Nikankin et al.
dentifies neurons in the mid to late layers of LLMs that
implement simple heuristics for arithmetic tasks, revealing
that models rely on these heuristic-driven neurons rather
than robust algorithms for arithmetic reasoning.

Model Alignment Region. Numerous studies have in-
vestigated the specific regions within models that are criti-
cal for achieving alignment. These regions can broadly be
categorized into security alignment areas and behavioral
alignment areas. Security alignment focuses on ensuring that
models resist adversarial prompts and avoid generating
harmful or inappropriate outputs. It often targets regions in-
volved in information sanitation, such as initial transformer
layers. Behavioral alignment, on the other hand, emphasizes
tuning areas that govern logical reasoning and output coher-
ence. This dual approach enables models to maintain ethical
standards while responding accurately to user inputs.

Security alignment region. Unsafe knowledge is often
stored in specific neurons of the MLP module and certain
layers of the model. PEA identifies, through Training
Data Extraction (TDE), that Personally Identifiable Infor-
mation (PII) is stored in the MLP module of large mod-
els. DEPN traces unsafe regions by identifying high-
contributing neurons in feed-forward layers and mitigates
privacy risks like data leakage using privacy attribution
scores. MemFlex leverages gradient localization to de-
tect “unsafe” parameter regions in LLMs, addressing pri-
vacy and copyright concerns. DINM locates unsafe
behaviors with adversarial queries, and detoxifies them
through permanent parameter adjustments via knowledge
editing. PCGU uses gradient partitioning from con-
trastive sentence pairs to locate bias-contributing param-
eters in attention and feed-forward layers. NOINTENTE-
DIT applies gradient-based harmfulness scoring to
localize unsafe regions, particularly in attention and feed-
forward layers, and uses activation suppression to reduce
harmful outputs, ensuring safe inference behavior without
retraining. Additionally, LED targets unsafe layers,
especially in early layers, and realigns them with safe


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

responses to reduce vulnerability to jailbreak attacks, en-
hancing model safety without significant performance loss.
GloSS introduces a global toxic subspace, demonstrat-
ing that toxicity can be systematically suppressed by remov-
ing this subspace from FFN parameters, thereby achieving
state-of-the-art detoxification while preserving general ca-
pabilities without costly retraining.

Behavioral alignment region. The alignment of behavior
in large models at the parameter level can be divided
into various categories, including model personality, the
creation of illusions, flattery, and other related aspects.
For large model personality regions, NPTI identifies
neurons in LLMs’ feed-forward networks associated with
specific personality traits by analyzing responses with op-
posing trait aspects. For large model hallucinations in the
region, PFME [69] locates hallucination-prone regions in
LLMs by analyzing entity-based discrepancies in sentence-
level outputs. The problematic areas are tied to decoder-
layer parameters generating unverifiable content, which
are corrected through targeted contextual adjustments for
factual consistency. TruthX localizes unsafe regions in
decoder-layer representations using a truthful latent space
and contrastive learning. By editing feature vectors tied
to hallucinations, it reduces false outputs, enhancing LLM
truthfulness without retraining. SPT mitigates syco-
phancy in LLMs by identifying and fine-tuning less than
5% of parameters, specifically within attention heads and
feed-forward networks, responsible for undue agreement
behaviors.

4.3.2 Sourcing from Connection Modular

In contrast to the perspective that localizes model capabil-
ities within specific, independent modules, another line of
research proposes and reveals that a model’s abilities do
not stem from isolated components but emerge from the
dynamic cooperation between specialized modules |207].
This systemic viewpoint fundamentally alters our under-
standing of how large language models achieve their re-
markable performance across various tasks (208). A key
insight in this field is that the capabilities of LLMs manifest
as a systemic property, originating from the coordinated
interaction among Attention, FFNs, and Residual Connec-
tions. Leading research teams, including Anthropic, Ope-
nAl, and DeepMind [209], [210], have developed sophisti-
cated methodologies to analyze these interactions, revealing
that while individual components often perform relatively
simple computations, they become exceptionally powerful
when combined.

Dynamic Knowledge Routing. In exploring the mecha-
nisms by which models process factual knowledge, research
has uncovered a complex dynamic process that transcends
the simple localization of knowledge within FFNs.Studies
by Wei et al. demonstrate that knowledge is not stored
in narrowly localized regions of the model, while Deng
et al. [90] further emphasize the important roles played
by both attention heads and FFNs in the processing of
factual knowledge. For instance, a significant illustration
is provided by the analysis of a three-stage dynamic flow
that shows how knowledge is transferred and transformed
within the model [75}. They detail a process where early

14

FEN layers first construct and enrich the semantic repre-
sentation of the input; subsequently, mid-network attention
mechanisms capture and transmit relational cues relevant
to the target object; finally, specialized attention heads in
deeper layers precisely extract the target object from the
enhanced subject representation based on these relational
cues. Furthermore, the collaboration between different com-
ponents on fact-based question-answering tasks has been
examined from a more granular perspective [76]. They
found that specific attention heads first efficiently extract the
subject entity from the context and route its representation
to network regions crucial for answer generation. Following
this, specialized attention heads and FFNs in the middle
layers work in concert to relay and activate this entity
representation. Subsequently, deeper FFN layers leverage
this activated representation to perform complex relational
mapping and knowledge retrieval, thereby producing the
final answer. This work clearly demonstrates the critical role
of attention mechanisms in entity focusing and information
routing, complemented by the synergistic function of FFN
layers in executing complex relational transformations and
knowledge retrieval.

Knowledge Circuits. The “knowledge circuits” frame-
work, a recently prominent approach, seeks to explore in-
ternal model knowledge by identifying key computational
subgraphs that perform specific functions. Olah et al.
first introduced this concept, which was later formalized
by Elhage et al. to reverse-engineer the collaborative
mechanisms responsible for functions such as copying and
tracking. The knowledge circuit perspective emphasizes that
meaningful computation requires the joint participation of
multiple components. Applying this framework to factual
knowledge, experiments have demonstrated that specific
“information-gathering heads”, “relation-rendering heads”
and FFN neurons collectively form circuits that encode a
particular fact or concept. The critical role of these circuits
in storing target knowledge was further verified through di-
rect interventions, such as activation or suppression, within
these pathways [79]. This line of research has since been
extended to map knowledge circuits for more complex
tasks. For instance, researchers have successfully identified
circuits responsible for identifying object colors and
for resolving country-city queries [80]. Collectively, these
findings indicate that a model’s knowledge and capabilities
are distributed across function-specific computational paths
composed of multiple, distinct components.

Hierarchical Reasoning. As task complexity extends
beyond simple fact retrieval, the patterns of inter-module
collaboration likewise evolve. Recent studies have started
to illuminate how advanced cognitive capabilities such as
multi-step reasoning emerge from synergistic interactions
among basic components (81). Although a single module
may not independently execute a complex reasoning algo-
rithm, their coordinated interplay can approximate sophis-
ticated computational processes. Using probing and causal
intervention techniques, Lan et al. identified shared
sub-circuits in the middle to late layers of transformer
models, which are specialized for sequence continuation
tasks, including numerical progression and ordered element
generation. This collaboration exhibits a distinct hierarchical
nature: different layers of the model specialize in differ-


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

ent stages of the reasoning process. Shallow layers may
be responsible for identifying basic patterns and parsing
grammatical structures; middle layers focus on integrating
information and constructing relational mappings; while
deeper layers carry out final logical integration and answer
generation.

Emergent Algorithms. One of the most compelling find-
ings is that the collaboration of components within the
Transformer architecture can simulate classical computa-
tional algorithms. This discovery provides a novel mecha-
nistic explanation for the model’s generalization capabili-
ties and in-context learning. Recent research demonstrates
that LLMs achieve in-context learning by approximating a
gradient-based meta-learning process. The data transforma-
tion performed by a single linear self-attention layer, for
instance, has been shown to be mathematically equivalent
to a single gradient descent step on a regression loss func-
tion [83], [84]. Furthermore, stacking multiple self-attention
layers iteratively refines the model’s predictions. This multi-
layer mechanism not only simulates gradient descent but
can also outperform the standard algorithm on non-linear
regression tasks by dynamically incorporating learning rates
and curvature information [85]. Research has shown that
LLMs can achieve in-context learning by approximating a
gradient-based meta-learning process. For example, some
work reveals that the data transformation performed by a
single linear self-attention layer is mathematically equiv-
alent to executing one step of gradient descent on a re-
gression loss function |86]. Further studies have found that
stacking multiple self-attention layers can iteratively refine
the model’s predictions, an effect that not only simulates
gradient descent but can even outperform standard gradi-
ent descent on non-linear regression tasks by dynamically
adjusting learning rates and curvature information |87].

Beyond optimization algorithms, models can simulate
other computational processes. A study found that a Trans-
former model can simulate a dynamic-programming al-
gorithm to solve the edit-distance problem through the
collaboration of its internal components [88]. They iden-
tified specific attention heads responsible for passing cost
information between sequences, while FFN layers played
the role of updating and storing the minimum edit costs.
These findings provide compelling evidence that model
capabilities are not merely about pattern matching; rather,
within their vast parameter spaces, models have learned
to execute genuine, general-purpose algorithms through the
flexible collaboration of their components. This perspective
also explains why LLMs exhibit powerful zero-shot or few-
shot capabilities when faced with tasks they have never seen
but that are structurally similar to what they know.

4.4 Prior-based Structure Sourcing

In contrast to research that investigates emergent capabili-
ties within pre-trained model parameters, an alternative line
of inquiry endows models with prior structure by explic-
itly partitioning them into domain-specialised experts, of-
ten termed Explicit Domain-Expert Mixture-of-Experts (MoE),
so that distinct sub-modules are dedicated to encoding
and processing information from specific domains [214].
A learned router then identifies the domain of each in-
put and forwards tokens or intermediate representations

15

to the most appropriate expert, enabling compute-efficient
sparse routing [215]. This architectural design inherently
facilitates sourcing by allowing direct attribution of outputs
to activated experts, thereby enhancing interpretability and
traceability without relying solely on post hoc analysis.

During the model training phase, the prior-based trace-
ability method proactively integrates traceable information
into the model architecture, enabling direct verification of
individual components and their respective contributions.
This approach ensures that model outputs can be transpar-
ently traced back to their originating components, providing
clear attribution and interpretability.

Model-level Expert Specialisation. Early evidence for
this paradigm came from MoDEM, which employs a BERT-
based router to dispatch natural-language tasks to language-
model experts fine-tuned for domains such as health and
mathematics, outperforming generalist models of similar
size (91). The idea has since generalised to other modalities.
In medical vision-language reasoning, Med-MoE routes vi-
sual features to modality-specific experts selected accord-
ing to the diagnostic report type, boosting interpretation
accuracy [92]. In recommendation systems, MoE-MLoRA
treats lightweight LoRA adapters as experts to tackle multi-
domain click-through-rate prediction, illustrating that ex-
pert count and data characteristics jointly determine the
actual gains [93].

Layer-level Expert Specialisation. More recent work
pushes expert specialisation into individual layers. TASER
interleaves shared layers with expert FFN layers inside a
single dense-retrieval encoder; its task-aware router yields
higher retrieval accuracy and stronger cross-domain ro-
bustness while using markedly fewer parameters than tra-
ditional bi-encoders [94]. Building on this, DeepSeekMoE
advances layer-level specialization by finely segmenting
experts into a larger pool and activating a flexible subset,
while designating a portion as shared experts to capture
common knowledge. This structure reduces redundancy
among routed experts, promotes ultimate specialization,
and achieves comparable performance to denser models
with reduced computation, as demonstrated in scaled vari-
ants up to 145B parameters [95]. Similarly, DynMoE in-
troduces a dynamic gating mechanism at the layer level,
where each token auto-determines the number of activated
experts via a novel routing strategy, coupled with adaptive
tuning of expert counts during training. This enhances effi-
ciency in sparse MoE transformers for vision, language, and
multimodal tasks, maintaining competitive accuracy while
activating fewer parameters than static counterparts [96].
Additionally, MoEUT extends layer-level MoE to Universal
Transformers by integrating experts into both feedforward
and attention sublayers, with specialized normalization and
grouping schemes that enable parameter sharing across
layers, yielding compute and memory savings while outper-
forming standard transformers on benchmarks like BLiMP
and PIQA [97].

Overall, prior-based structure sourcing through MoE
architectures represents a proactive shift toward inherently
traceable models. By design, these systems allow for the
decomposition of complex behaviors into verifiable expert
contributions, addressing challenges in black-box models
and opening avenues for safer, more accountable AI deploy-


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

16

TABLE 3
Representative posterior-based structure sourcing studies

Setting Region Method Year Target Models
Semantic grammar Geva et al. 2020 ‘Transformer-XL
Capabilit Arithmetic Nikankin et al}! 2024 LLaMA-2-13B
apepry Creative writing Gomez et al. 2023 GPT-4, Claude, PaLM-2
Hardware design creativity | CreativEval |. 2024 GPT-4, Claude
Security (PII) 2024 LLaMA-2-7B
‘ Security (privacy & IP) 2024 GPT-J, GPT-2
Aligument Personality 2024 +LLaMA-2-7B
Sycophancy 2024 GPT-2 XL
Knowledge flow Geva et al. |75) 2023 GPT-2M
S isti Knowledge circuits Yao et al. 79) 2024 LLaMA-2-13B
YNETEISHC In-context learning Akyiirek et al-]8 2023 GPT-2S
Edit-distance DP Todd et al. 88 2024 GPT-2 XL

ment. Future developments may integrate adaptive routing
with advanced logging mechanisms to further automate
sourcing processes.

4.5 Challenges

The study of sourcing from model structures, while promis-
ing, confronts several significant challenges that pave the
way for future research.

Firstly, a primary challenge is the sheer complexity and
scale of modern large language models. As models grow
to trillions of parameters, the combinatorial explosion of
potential interactions between components (neurons, at-
tention heads, layers) makes exhaustive analysis computa-
tionally intractable. Identifying isolated “capability regions”
becomes less feasible as evidence increasingly points toward
complex, synergistic collaboration, where functions are dis-
tributed across diffuse “knowledge circuits”. Distinguishing
true causal mechanisms from mere correlations in such a
vast and interconnected system remains a formidable task.

Secondly, the challenge lies in the granularity and dy-
namics of analysis. Current methodologies often rely on a
static view, analyzing component contributions for a specific
input-output pair. However, a model’s internal processing
is highly dynamic; the functional roles of components and
the collaborative circuits they form may shift depending
on the context of the prompt and the progression of the
generation process. Furthermore, there is no consensus on
the optimal level of granularity for analysis, whether to
focus on individual parameters, neurons, or entire modules
like FFN and attention layers. Understanding how low-level
computations aggregate into high-level emergent behaviors
is a critical open question.

Additionally, the field currently lacks standardized eval-
uation protocols and benchmarks. Different research efforts
employ bespoke methodologies, from causal tracing and
activation patching to gradient-based attribution, making it
difficult to systematically compare the validity and efficacy
of various sourcing techniques. Without common datasets
and tasks designed specifically to test the localization of
known functions or behaviors, it is challenging to validate
findings and measure progress in a rigorous.

Future work must advance beyond manual analysis by
developing scalable, automated sourcing techniques. Key

priorities are establishing robust methods for causal infer-
ence in LLMs to clarify component and creating standard-
ized benchmarks for evaluation. Ultimately, these efforts
will bridge post hoc analysis with prior design, enabling
the creation of more interpretable, efficient, and controllable
next-generation architectures.

5 SOURCING FROM TRAINING DATA

Data sourcing in LLMs refers to the process of tracing and
identifying the specific training data that underpins the
content generated by the model. This chapter provides a
systematic review of data sourcing, addressing the follow-
ing key aspects: the definition of the task, datasets and
evaluation protocols, prior-based data sourcing, posterior-
based data sourcing, and prospects for future research.
These discussions aim to comprehensively elucidate the
fundamental concepts, pivotal technologies, and potential
development trajectories in data sourcing, thereby offering
valuable insights and guidance for research and practical
applications in this domain.

5.1. Task Definition

The task of training data sourcing in large language models
is formally defined as the process of quantitatively assessing
the causal influence of individual training samples or sub-
sets on the model’s generated outputs. Given a generated
text g,a model M parameterized by 0, and a training dataset
D, the objective is to identify which elements 7p C D
most significantly contribute to the production of g. This is
achieved by measuring the sensitivity of the output poste-
rior distribution P(y | x, E, T, 0) to variations in the training
data, often through gradient-based attribution methods that
evaluate how perturbations in specific training instances
affect the likelihood of the generated content. Alternatively,
under a prior-based framework, the task may involve de-
tecting pre-embedded markers within gthat are associable
with particular subsets Ty, thereby enabling direct linkage
between the generated output and its originating training
samples. The goal is to establish a provenance mechanism
that traces model behavior back to influential data points
within the training corpus.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

17

TABLE 4
Summary of representative datasets for training data sourcing.

Dataset Domain Language(s) ‘Training Set Year
FTRACE 101) News EN v 2023
WIKIMIA 216} Wikipedia EN v 2023
Sh ara News EN v 2024
Hallucinatory Xsum News EN v 2024
BookMIA Book EN v 2024
BIOCITE |122 Biography EN v 2024
BookSum ya Book EN v 2025

Posterior-based Training Data Sourcing

]
SBaaaa
9 a ef

Test Text (z) co
So

Sans . Fine-tuning LLM
tt

Training Data (z;e D)

Jw

xXx< KX

Sourcing Scores

Prior-based Traning Data Sourcing

Statiscal Test « /D \

_ Sourcing

i
25

Training Data with Label

Fig. 5. The methods for training data sourcing can be categorized
into two types: posterior-based and prior-based. Within posterior-based
training data sourcing, there are two approaches: white-box and black-
box. For prior-based training data sourcing, the approaches are divided
into watermark-based and proxy model methods.

5.2 Datasets and Evaluations

The systematic evaluation of training data sourcing method-
ologies relies on specialized benchmarks and rigorous met-
rics. This subsection details the datasets and evaluation
protocols essential for assessing the efficacy of both prior-
based and posterior-based sourcing approaches, which op-
erate under fundamentally different paradigms.

5.2.1 Datasets

An empirical evaluation of training data sourcing methods
for large language models necessitates the use of domain-
specific benchmarks tailored to the distinct operational
assumptions of prior-based and posterior-based sourcing
frameworks. Table [A] presents commonly used datasets for
this task.

For posterior-based methods, which infer training data
influence through post hoc analysis of model behavior with-
out prior intervention, several dedicated datasets have been
developed. These typically facilitate evaluation based on ap-
proximate counterfactual influence. For instance, fact trac-
ing datasets such as FTRACE and GPTDynamics [217]
are designed to assess the ability to provenance training
instances that provide semantically or lexically aligned evi-
dence supporting factual claims in model outputs. Similarly,

hallucination tracing datasets like Hallucinatory Xsum |112
and BookSum evaluate the capacity to trace training
samples that may induce hallucinatory generations.

In contrast, prior-based methods, which rely on identifi-
able signals intentionally embedded into the training corpus
before model training, require benchmarks constructed with
explicit, known markers. Datasets such as WIKIMIA [216],
BookMIA [218], and BIOCITE exemplify this category,
featuring syntactic watermarks, lexical signatures, or struc-
tured metadata. These resources enable direct and determin-
istic tracing of model outputs to their source partitions and
are typically used in controlled experimental settings where
the structure, location, and semantics of the embedded
markers are known a priori. Naturally occurring datasets
are generally unsuitable for evaluating prior-based methods
unless they are synthetically augmented or partitioned to
establish ground-truth traceability.

5.2.2 Metrics

The evaluation of training data sourcing methods employs
distinct metrics aligned with the objectives of prior-based
and posterior-based paradigms.

For posterior-based methods, a common evaluation
framework is built upon the concept of the leave-one-out
(LOO) effect, formally defined as AL = L(g; 0_;) — L(g; 4),
where 0_; represents model parameters retrained excluding
a specific training sample z; [220], (221). Given the com-
putational infeasibility of exact LOO estimation for large
language models, evaluations often rely on rank correlation
metrics, such as Pearson or Spearman coefficients, to com-
pare estimated influence scores against approximated LOO
values derived from proxy models or subsampled data. In
generative settings, task-specific metrics are also employed.
For fact tracing, retrieval metrics like Precision@K or mean
reciprocal rank are commonly used [73]. For hallucination
tracing, assessments often combine lexical or topical similar-
ity measures with human judgments of causal plausibility to
provide a holistic evaluation of attribution fidelity (101).

For prior-based methods, evaluation focuses on three
core dimensions [41]. Detection fidelity is quantified using
standard classification metrics including precision, recall,
and F1 score, determining whether a generated sequence
contains watermarks attributable to a predefined source
subset 7) C WD. Robustness is evaluated by subjecting
watermarked outputs to adversarial perturbations such as
paraphrasing, truncation, or targeted watermark removal
attacks, measuring the resilience of the embedded sig-
nals [222]. Finally, preservation of downstream utility is
assessed by monitoring performance degradation through


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

metrics such as perplexity or task-specific accuracy, ensur-
ing that the watermark injection process does not impair the
model’s functional capabilities [223].

5.3 Posterior-based Training Data Sourcing

Posterior-based Training Data Sourcing denotes a family of
techniques that identify training instances which critically
influence a model’s predictions by examining its output
behavior in response to specific inputs. The fundamental
assumption of these methods is that the model’s posterior
behavior inherently reflects the imprint of its training data.
By systematically analyzing this behavior, one can trace
and localize the training samples that most significantly
contribute to a given prediction, as demonstrated in the

Fig. 5]

5.3.1. White Box Methods

White-box posterior-based training data sourcing methods
primarily refer to computational approaches that analyze
the influence of training data on model outputs, given ac-
cess to model parameters or internal representations. These
methods can be broadly categorized into gradient-based in-
fluence score estimation, embedding-based influence score
estimation, and empirical influence score estimation.

Gradient-based Influence Score Estimation. This cate-
gory of methods measures the contribution of training sam-
ples to model predictions using gradient information. The
core operation involves computing the gradients of the loss
function with respect to model parameters for both training
and test samples [220], then assessing influence scores based
on the similarity or alignment between these gradients. In-
tuitively, a training sample whose gradient direction aligns
closely with that of a test sample is considered to have a
greater influence on the model’s prediction. This approach
captures complex nonlinear dependencies within the model
and is particularly suitable for analyzing gradient flow in
deep neural networks. Influence functions, as one central
technique, are effective in identifying the most influential
training samples, especially in detecting outliers or noisy
data points. However, traditional influence function meth-
ods require computing the inverse of the Hessian matrix,
which becomes computationally prohibitive and challeng-
ing to implement in high-dimensional parameter spaces.

To address this, several variants have been proposed,
such as TracIN which approximates influence using
first-order gradients and model checkpoints, RelatIF
which normalizes influence by self-influence, FastIF
which uses k-nearest neighbors for scalability, and Scaling-
IF which employs eigenvalue decomposition for effi-
cient computation. These methods significantly reduce com-
putational overhead while scaling to large datasets and com-
plex architectures. In NLP tasks, empirical studies compare
gradient-based methods (114), including influence functions
and their variants, with similarity-based retrieval methods
in terms of efficiency and complexity when applied to BERT.
While gradient-based methods are theoretically robust, they
remain computationally expensive in practice. To mitigate
this, a reranking strategy commonly used in informa-
tion retrieval is adopted, where a candidate set of potentially
influential samples is first retrieved from the training set

18

before applying more precise attribution methods, greatly
reducing computational demands.

However, the approximation theory of influence func-
tions relies on the assumption that model parameters attain
empirical risk minimization, a condition often violated in
language models due to their architectural complexity and
diverse data distributions (225). Furthermore, challenges
such as data imbalance, noise, and limited resources exac-
erbate the difficulty of achieving empirical risk minimiza-
tion (103). Consequently, subsequent work incorporates
decomposition-based approximation strategies to adapt in-
fluence functions for large language model training data
attribution, including contrastive methods like CEA [226],
dynamic approaches like DDA [112], and extensions such
as Datalnf and RLHF-IF

As shown in Table] the evolution of influence function
formulas across different training data sourcing methods is
summarized.

Embedding-based Influence Score Estimation. This cat-
egory of methods leverages internal representations from
hidden layers to evaluate the effect of training samples
on specific test predictions. The underlying assumption is
that samples close in the embedding space tend to exhibit
semantic similarity and hence exert greater mutual
influence on predictions [230]. The typical procedure in-
volves extracting embedding vectors for both training and
test samples, then computing similarity scores using mea-
sures such as cosine similarity or Euclidean distance [231],
[232]. This category of methods is particularly effective for
models that learn high-quality embeddings, such as pre-
trained models in NLP and computer vision, and compu-
tational efficiency can be improved further via approximate
nearest-neighbor search. LLM Embedding-based Attribu-
tion (LEA) introduces a novel approach that leverages
the linear dependence of hidden state representations across
transformer layers to quantify the contribution of retrieved
context versus internal knowledge in generated responses.
Nevertheless, the quality of influence estimation heavily
depends on the expressiveness of the embedding space with
respect to task-relevant features [234].

Empirical-based Influence Score Estimation. This cate-
gory of methods directly quantifies the impact of training
samples through controlled experiments, often involving
multiple rounds of model retraining, each time excluding
specific samples or groups of samples [235], and
measuring the resulting change in model predictions (174).
Since this approach does not rely on assumptions about
the model’s internal mechanisms, it can accurately capture
complex interactions and dependencies among samples.
However, it is computationally intensive and generally in-
feasible for large language models. To improve practicality,
approximations such as proxy models [236], group-level
removal, and influence propagation methods are commonly
employed.

5.3.2 Black Box Methods

Black box methods offer a powerful paradigm for train-
ing data attribution, operating without direct access to the
model’s internal parameters. These approaches primarily
employ retrieval strategies, ranging from direct vector-space
similarity to more nuanced neighborhood-space analysis, to


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

19

TABLE 5
Evolution of Influence Function Formulas in Training Data Sourcing

Method Formula

Mathematical Interpretation

IF |220

T(z, 2query) = —VoL(Zquery, 6)’ H~1VeL(z, 6)

Vo = gradient w.r.t. parameters 0
H~—'1 = inverse Hessian matrix of empirical loss
0 = optimized model parameters

K

TracIn {106}

k=1

T(z, query) = S> VoL(zquery: 92(x)) ' VoL(2, 94¢%))

64(~) = parameters at checkpoint ¢(k)
4 = number of selected checkpoints

T(z, Zquery )

Trei (2, Zquery ) = T(z z)

RelatIF

I(z, z) = self-influence of training point z

CEA

Tcea (2, Zquery ) = I(z, Zquery ) _ I(z, Zquery )

Zquery = human-corrected version of Zquery

Ippa(z; Zquery ) = Ig(z, Zquery ) = 16, (2% Zquery )

DDA [112 6 = model parameters after training
40 = model parameters before training
FastIF {110 TrastiF (2, query) = KNN(zquery) - Vo L(2, 4) kKNN(Zguery) = k-nearest neighbors of zquery

Scaling-IF

P »
Iscaling-IF = > dy (Gi Vo L(zquery )) (Gis VoL(z)) oi
i=l p

4 = i-th dominant eigenvalue of H
; = corresponding eigenvector
= number of retained eigenvalues

ll
IDataint = nr > (:

+
UdUg )
a
deD A+ Ug Va

DataInf

Va = V6 Lpret(d; 0) = gradient of preference loss
\ = damping parameter
n = dataset size

RLHF [228 “a

pinpoint the most influential training samples for a given
model prediction.

Retrieval-based Data Attribution. This approach to
retrieval-based data attribution relies on measuring the sim-
ilarity between vector representations of training samples
and test examples. In this method, both training and test
data are typically embedded into a high-dimensional vector
space [117], often using internal representations from the
model itself or pre-trained embedding models. The under-
lying assumption is that the training samples most similar
to a test example in this vector space had the greatest
influence on the model’s prediction for that test case ;
. Common similarity measures include cosine similar-
ity, Euclidean distance, and dot product [113]. The process
generally involves computing similarity scores between the
test example and all training samples, then ranking them
to retrieve the top-K most similar instances. This approach
is particularly well-suited to models that operate on vector
representations, such as deep neural networks for text or
image processing. A key advantage is its scalability, as ef-

Trunpar(di) = —VoL(Dyais 9) | HV 6 lpret (di; 8)

Aret = Hessian of Bradley-Terry preference loss
Loref = Cross-entro’ reference loss

P PS-ENUrOpy P
Dya = validation preference dataset

ficient approximate nearest neighbor search algorithms can
handle large datasets. However, the method’s effectiveness
is highly dependent on the quality and relevance of the
vector representations 115}. It may fail to capture complex,
non-linear relationships learned by the model, particularly
if the embedding space does not align well with the model’s
decision boundaries.

Neighbor space retrieval method. This method diverges
from conventional vector similarity approaches by exam-
ining the local neighborhood of test examples within the
model’s decision space. This technique identifies training
samples that exert comparable influence on the model’s
decision-making process as the test case, estimating how
predictions would shift if specific training samples were re-
moved or perturbed [116]. Such analysis captures non-linear
relationships and complex interactions invisible in simple
vector spaces, revealing how different training data regions
shape the model’s decision boundary. Recent advances in
semi-parametric language modeling, such as Nearest Neigh-
bor Speculative Decoding (NEST), demonstrate how token-


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

level retrieval and dynamic span selection can enhance attri-
bution by incorporating real-world text spans while main-
taining fluency [118]. Similarly, investigations into kNN
retrieval augmentation reveal its superior generalization
capabilities over vanilla language models, particularly when
handling over-specified training data where causal relation-
ships are obscured by irrelevant information :

Despite its nuanced insights, this method faces com-
putational challenges with large-scale models and datasets
due to iterative subset processing. Nevertheless, it provides
critical understanding of model behavior by identifying
both semantically similar examples and training samples
that fundamentally influence specific test case decisions.
The approach complements retrieval-augmented frame-
works like NEST, which achieves 1.8x inference speedup
through speculative decoding while providing span-level
attribution [118], and highlights the limitations of para-
metric models that struggle with generalization from over-
specified descriptions [119]. This synergy underscores the
value of neighborhood-based analysis in developing more
interpretable and grounded language models.

5.4 Prior-based Training Data Sourcing

Prior-based Training Data Sourcing refers to a class of tech-
niques that proactively design or modify training data prior
to model training, with the goal of enabling subsequent
traceability or attribution of model outputs to their source
data, as shown in the Fig. [5] The central principle is to embed
traceable prior signals, such as watermarks, or to construct
auxiliary models, such as proxy influence estimators, dur-
ing the training phase. These interventions ensure that the
model’s behavior at inference time reflects the provenance
or composition of its training data, thereby supporting
efficient and controllable data attribution analysis.

5.4.1. Watermark-based Methods

Embedding unique watermark information in the training
corpus is a technique used to achieve training data traceabil-
ity, aiming to ensure that the model can learn and reflect this
watermark information. The WASA framework proposed
by provides an effective solution for content traceabil-
ity and data provenance by training large language models
(LLMs) to learn the mapping between the text from different
data providers and their unique watermarks. Additionally,
the research of Meta on the radiology of watermarked texts
reveals the detectability of watermarked training data, indi-
cating that even embedding a small amount of watermarked
data can make the model output highly recognizable [123].
Recent advancements have introduced novel approaches to
enhance watermark robustness. For instance, Cui et al.
propose injecting fictitious knowledge into training data,
creating watermarks that are linguistically plausible yet se-
mantically unique, making them resistant to preprocessing
filters and effective across model lifecycle stages. Similarly,
Rastogi et al. introduce STAMP, a framework that gen-
erates watermarked rephrasings of content to prove dataset
membership via statistical tests, enabling reliable detection
even in API-only models.

However, watermarking technology faces several chal-
lenges and limitations [237]. Firstly, watermarks are vulner-
able to attacks or removal, significantly diminishing their

20

effectiveness. Current techniques can effectively remove
or alter watermarks while preserving the original data
characteristics [238], thereby compromising their content
traceability. Moreover, applying watermarking frameworks
to pre-trained models is challenging because they require
specific interventions during the training process [42]. This
requirement limits the applicability of watermarking tech-
nology to existing models, especially when training data
or the training process is inaccessible. Lastly, embedding
watermarks may adversely affect the quality of generated
text, as the process can introduce noise or other distur-
bances, reducing the naturalness and readability of the
text [239]. To address these issues, recent studies emphasize
stealthy integration and statistical validation. For example,
fictitious knowledge watermarks align with natural
data distributions to evade detection, while STAMP
leverages paired statistical tests to minimize false positives
and preserve content utility. These approaches highlight the
evolving strategies to balance traceability, robustness, and
practicality in watermarking systems.

5.4.2 Proxy Model Methods

Proxy model methods are designed to approximate training
data influence through surrogate modeling mechanisms.
These approaches aim to estimate or extrapolate influence
patterns indirectly by learning predictive models of data
influence or by exploiting the transferable properties of
influence signals across different model scales.

Data Influence Modeling for Training Data Sourcing.
This approach involves building a separate model to predict
influence scores based on various features of the train-
ing samples and their relationship to the test example. It
treats influence estimation as a machine learning problem
in its own right. The model is trained on a subset of
the data where true influence scores are computed using
more expensive methods like empirical estimation [128].
It then learns to generalize and predict influence scores
for the entire dataset [126]. Features for this model might
include similarity measures, gradient information, model
confidence, and various statistical properties of the samples.

Recent advances have refined this paradigm through
model-aware frameworks. DsDm formulates dataset
selection as an optimization problem, using datamodels
to efficiently approximate the mapping between training
subsets and model performance. By minimizing estimated
target loss, it selects data subsets that demonstrably im-
prove language model performance across diverse tasks.
Similarly, Group-MATES introduces a relational data
influence model that captures interactions between data
points through relationship weights. This group-level ap-
proach, trained via sampled trajectories and optimized with
influence-aware clustering, significantly enhances pretrain-
ing efficiency by modeling complex dependencies beyond
individual contributions.

These methods can potentially combine the strengths of
other approaches while being more computationally effi-
cient once the influence prediction model is trained. They
also allow for more flexible definitions of influence by
incorporating domain knowledge into the feature engineer-
ing process. However, the effectiveness of this approach
depends on the quality and representativeness of the subset


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

used for training the influence model, and it may struggle
to capture very complex or rare influence patterns.

Proxy-Based Scalability Modeling for Training Data
Sourcing. This approach builds on the insight that influ-
ence signals can be efficiently modeled through smaller
surrogate systems. Yu et al. introduce the MATES
framework, which employs a small data influence model
to continuously track the evolving data preferences of a
pretraining model and dynamically select the most effective
training data. By fine-tuning a lightweight proxy model
to approximate locally probed oracle influences, MATES
enables model-aware data selection that significantly re-
duces computational costs while maintaining performance
fidelity. Chang et al. show that gradient-based in-
fluence methods, properly adapted, can retrieve influen-
tial training examples in an 8B-parameter LLM over 160B
tokens. Subsequent work has further shown that influ-
ence estimates computed from smaller proxy models are
highly correlated with those derived from larger target
models [127], validating the transferable nature of influence
signals across scales. Together, these findings demonstrate
that proxy-based modeling provides a scalable and efficient
path toward large-scale training data attribution, although
its accuracy ultimately depends on the strength and stability
of the correlation between proxy and target model behaviors
across architectures and training configurations.

5.5 Challenges

The practice of data provenance in the training of large
models currently faces a number of challenges.

Firstly, the rapid expansion of data volume and the
increasing diversity and complexity of data sources make
the provenance process particularly difficult. Data may
come from various channels, such as public datasets, pri-
vate databases, and web scraping, and undergo multiple
stages of processing, including collection, cleaning, and
annotation, all of which contribute to the complexity of
the data lineage. Moreover, issues surrounding data privacy
and security pose significant challenges. Ensuring effective
provenance while maintaining data privacy remains a criti-
cal problem that needs to be addressed.

Another major challenge lies in the technical complexity.
Training large models involves processing vast amounts
of data and applying sophisticated algorithms. Accurately
tracking and documenting each step of the data processing
pipeline in such a large-scale system requires efficient tech-
nical methods and robust tool support. Current technologies
often face performance bottlenecks when handling large-
scale data, making it difficult to meet the demands for both
real-time processing and accuracy.

Additionally, the lack of standardization and normaliza-
tion in data provenance is an urgent issue. Currently, there
are no unified standards or protocols in the field of data
provenance. Different institutions and research teams use a
variety of methods and tools, which undermines the compa-
rability and reliability of provenance results. The establish-
ment of standardized procedures would not only improve
the efficiency of provenance activities but also foster data
sharing and collaboration between different organizations.

21

6 SOURCING FROM EXTERNAL DATA

External data sourcing in LLMs refers to the process of
tracing, verifying, and attributing the external knowledge
that supports the model’s generated outputs. Unlike train-
ing data sourcing, which targets the internal corpus used
during pretraining and fine-tuning, external data sourcing
focuses on the information that models actively retrieve,
cite, or align with at inference time. This capability is
especially important in applications such as knowledge-
intensive question answering and scientific writing, and
becomes particularly critical in high-stakes domains like
law and medicine, where responses are expected to be
supported by verifiable evidence. This task is crucial for
ensuring transparency, factual consistency, and reliability
of model responses, as it directly addresses whether the
generated content can be grounded in verifiable evidence. In
this section, we provide a systematic review of external data
sourcing from the following key aspects: task definition,
datasets and evaluation protocols, prior-based approaches,
posterior-based approaches, and future directions.

6.1 Task Definition

Following previous works [133], [250], the task of sourcing
from external data can be formally stated as follows: given
a user query g and a corpus of text passages D, the objective
is to produce a set of statements S = {51,...,5i,..., Sn},
where each statement s; is grounded in one or more ev-
idence passages drawn from D. More precisely, for each
statement s;, we seek to identify a corresponding set of
citations C; = {¢i1,.-.,Cij,---,Ci,K }, where each citation
ci,; is a passage from corpus D that supports or provides
context for statement s;, and K is the maximum number of
citations per statement. Typically, the response S is typically
segmented into sentences, with each sentence treated as an
individual statement. Inline citations are provided for each
statement, and these citations are formatted within square
brackets, such as “[1][2]”, and so on, to explicitly attribute
the content to specific passages in the corpus D. The task
requires ensuring that each statement s; is both contextually
relevant to the query gq and adequately supported by the
citations C;, ensuring transparency and accountability in the
generated content. This task is also commonly referred to as
LLM Attribution or LLM Citation.

6.2 Datasets and Evaluations

In this section, we review representative datasets for ex-
ternal data sourcing and summarize the main evaluation
protocols used to assess citation quality.

6.2.1 Datasets

To enhance the reliability and trustworthiness of generative
LLMs, a growing body of work has focused on improving
their attribution capabilities through dedicated datasets.
Early efforts such as ALCE established a founda-
tional benchmark by curating diverse corpora and design-
ing automated metrics for fluency, accuracy, and citation
quality. Subsequently, addressing the need for more spe-
cialized resources, datasets like HAGRID were devel-
oped through human-LLM collaboration to provide genera-
tive retrieval data with explicit attributions. Building upon


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

22

TABLE 6
Summary of representative datasets for external data sourcing.

Dataset Domain Language(s) ‘Training Set Year
BiokKaLMA Biography EN x 2023
Misc. EN x 2023

Wikipedia EN v 2023

Misc. EN v 2024

Wikipedia EN x 2024

Scientific EN v 2024

News CH x 2024

Misc. EN, CH x 2024

Misc. EN x 2024

Meeting EN x 2024

WebCiteS Misc. CH f 2024
CitaLaw |249) Law CH x 2024

this, BiokKaLMA innovatively employed knowledge
graphs as attribution sources, introducing the concept of
“conscious ignorance.” Concomitantly, efforts were directed
towards mitigating hallucination, as exemplified by CRUD-
RAG [245], which integrates citation links and offers a
framework for assessing citation quality. To further refine at-
tribution analysis, proposed a benchmark for complex
question answering, utilizing knowledge graphs for auto-
mated attribution. Complementary to these efforts, datasets
such as WikiRetr were constructed to analyze and en-
hance the ability of LLMs to generate reliable content, while
MISeD explored LLMs for generating source-specific
dialogue data. Recognizing linguistic gaps, WebCiteS
offers a high-quality CH dataset. Focusing on academic
rigor, the REASONS dataset |252| evaluates citation gen-
eration in scientific literature. Addressing the challenges
of long-form content, ADIOSAA reconstructs existing
datasets for post-hoc answer attribution, while Cite ME
evaluates citation attribution under conditions of ambiguity
and unattributability. LongCite further advances this
line by enabling fine-grained, sentence-level attribution in
long-context QA. Beyond general and scientific domains,
CitaLaw introduces the first benchmark tailored to
the legal field, assessing whether LLMs can produce legally
sound responses with accurate and contextually appropri-
ate citations. Taken together, these datasets cover diverse
domains, languages, and task settings, providing a solid
foundation for systematic evaluation and advancement of
attribution in generative information retrieval. To provide
a structured overview, Table |6| summarizes representative
datasets for external data sourcing, including their domains,
languages, availability of training sets, release years, and
project pages.

6.2.2 Evaluation Protocols

Evaluating the quality of citations in LLM-generated re-
sponses is crucial for assessing the effectiveness of external
data sourcing methods. Evaluation protocols typically fall
into two categories: human evaluation and automatic eval-
uation. Human evaluation offers nuanced judgments but
is costly and difficult to scale, whereas automatic methods
provide efficiency and reproducibility at the expense of
interpretability. Below, we review both approaches and the
representative metrics.

Human Evaluations. Human evaluation involves anno-
tators assessing the quality of citations by examining their
relevance and alignment with the generated content. For
example, in ALCE [133], Surge AI is employed to perform
detailed annotation tasks. The evaluation includes two key
metrics:

e Citation Recall: Annotators are presented with a sentence
and all passages it cites and are tasked with determining
whether the passages fully support the sentence.

e Citation Precision: Annotators evaluate whether a spe-
cific citation “fully supports”, “partially supports”, or

“does not support” a given sentence.

Although human evaluation yields high-quality and
fine-grained insights, its cost, subjectivity, and limited scala-
bility constrain its applicability in large-scale benchmarking.

Automatic Evaluations. To address the limitations of
human evaluation, various automatic evaluation methods
have been proposed [133], [201|-[203], [254]. These ap-
proaches seek to replicate or approximate human judgments
with greater efficiency and reproducibility.

ALCE formalizes citation recall and citation preci-
sion in an automatic setting by leveraging the NLI model
TRUE to determine whether cited passages entail the
generated content. Furthermore, Djeddal et al. propose
a metric called citation overlap, which calculates the overlap
between the gold standard set of citations and the entire
set of citations in the generated response for each query,
providing a direct measure of citation alignment. To further
refine citation evaluation, Zhang et al. suggest replac-
ing NLI models with LLMs themselves to judge citation
support, thereby potentially improving the accuracy and
flexibility of citation validation. More recently, ALiiCE
is designed to evaluate citation quality from positional fine-
grained perspective, which parses generated responses into
atomic claims using dependency trees and assesses citation
recall and precision at the level of individual claims. Addi-
tionally, ALiiCE introduces a novel metric, the Coefficient of
Variation of Citation Positions (CVCP), which measures the
dispersion of citation placements within a sentence, offering
insights into the positional fine-grained quality of citations.

In summary, while human evaluation remains the gold
standard for nuanced assessment, automatic metrics are
essential for large-scale, reproducible benchmarking. Re-
cent advances increasingly combine the strengths of both


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

Posterior-based External Data Sourcing

P| \ The answer
Baas is XXX(1][2].
OO} —> | a] + | Bens] [1] xxx
Corpus [2] Xxx
Response Ny | NLI Model i/ J
| NLI Mod a Response
; (With Citation)
Prior-based External Data Sourcing
a Prompt-based Method
Input Output «
ee COOY vee The answer
/ Input Output Na is XXX[1][2].
ICL CoT eee
[2] Xxx
ui & pe Fal
EO H & tee Response
SFT DPO RLHF (With Citation)

Tuning-based Method

XN Ps

Fig. 6. Overview of methods for external data sourcing in LLMs. Prior-
based approaches (top) integrate attribution into the generation process
to generate responses with inline citations directly. Posterior-based ap-
proaches (bottom) validate and enrich model outputs after generation
by retrieving supporting evidence from external corpus.

paradigms, moving towards fine-grained and context-aware
evaluation of citation quality.

6.3 Posterior-based External Data Sourcing

As shown in Fig. |6} posterior-based external data sourcing
methods differ from prior-based approaches in that they op-
erate on the responses generated by LLMs without requiring
direct access to the model’s internal parameters. Instead,
these methods focus on verifying the responses generated
by the LLM by retrieving relevant evidence from external
data sources and adding citations post hoc. Existing work
falls into two main categories: retriever-based methods and
NLI-based methods.

6.3.1 Retriever-based Methods

Retriever-based methods focus on direct finding the most
relevant external evidence to support LLM-generated re-
sponses after the responses have been produced. For in-
stance, Gao et al. propose a naive post-cite framework
that, for each statement in the generated response, identifies
the best-matching passage among the top-100 candidates
retrieved by state-of-the-art dense retrieval models, such
as GTR [256]. The most relevant passage is then cited to
substantiate the claim. Further more, RARR framework not
only retrieves supporting evidence but also applies a post-
processing step to revise the generated outputs [134]. By
incorporating both retrieval and revision, RARR provide
a practical mechanism for improving the factual reliability
of LLM outputs. SearChain interleaves IR with LLM
reasoning by generating a full Chain-of-Query, where each
query-answer node is verified and completed by retrieval
before final summarisation, producing traceable citations at
each reasoning step.

23

6.3.2 NLI-based Methods

While retriever-based methods focus on retrieving the most
relevant evidence, they do not explicitly evaluate whether
the retrieved evidence can validate the claims in the re-
sponse. NLI-based methods address this limitation by in-
corporating natural language inference (NLI) models [257],
[258] to assess the relationship between the claims and the
evidence. For example, Ye et al. propose a frame-
work that extends retriever-based approaches by using an
NLI model to evaluate the retrieved passages. Instead of
simply citing the most relevant passage, the model selects
the passage that maximally supports the claim, as deter-
mined by the entailment relationship. This ensures that
the cited evidence directly validates the statement. Further-
more, CEG uses an LLM equipped with predefined
prompts to serve as the NLI mechanism. The LLM evaluates
whether each segment of the response is factually accurate
according to the retrieved evidence. If a segment is deemed
factual, the corresponding reference document is added as
a citation. However, if a claim lacks sufficient support, the
LLM is prompted to regenerate that segment until every
statement in the response is grounded in verifiable evidence.
By incorporating NLI-based verification, these approaches
address the shortcomings of simple relevance-based re-
trieval, offering a more nuanced and reliable framework for
external data sourcing.

6.4 Prior-based External Data Sourcing

Prior-based external data sourcing refers to methods where
LLMs generate responses along with citations to external
data sources (e.g., retrieved documents) during the genera-
tion process. Unlike posterior-based approaches that verify
outputs after generation, prior-based methods aim to embed
attribution directly into the response generation. To provide
a clearer comparison, Fig. |6] summarizes prior-based and
posterior-based external data sourcing methods Existing
studies can be broadly categorized into prompt-based meth-
ods and tuning-based methods.

6.4.1 Prompt-based Methods

Few-shot ICL and CoT Reasoning. Gao et al. first
applied few-shot In-Context Learning (ICL) to the task of at-
tributed text generation. In this method, LLMs are prompted
with a series of examples, where each example consists of
a query, a set of retrieved passages, and a corresponding
answer with inline citations. By observing these in-context
examples, the LLM learns to generalize citation behaviors,
producing responses grounded in supporting evidence. To
further improve citation accuracy, Ji et al. investigate
the use of Chain-of-Thought (CoT) reasoning. CoT decom-
poses the reasoning process into sequential steps, enabling
LLMs to synthesize accurate answers from multiple docu-
ments while ensuring proper citation attribution. Building
on this, CoTAR further systematically analyze the ef-
fects of citation granularity (e.g., span-level, sentence-level,
and passage-level) on the quality of generated responses.
Their study reveals that GPT-4 performs best when applying
span-level Chain-of-Thought reasoning, emphasizing the
importance of granular reasoning for precise and reliable
citations.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

Task-Specific Enhanced Prompt. Beyond general
prompting strategies, subsequent works introduce task-
specific prompt-based methods tailored for attributed text
generation. Fierro et al. propose a planning-based
framework that enhances citation quality for long-form
question-answering tasks. VTG employs long short-
term memory to retain critical and recent documents, en-
suring relevance throughout the generation process. Ad-
ditionally, a two-tier verifier equipped with an evidence
finder enables the model to reflect on the alignment between
claims and citations, enhancing both precision and cover-
age of retrieved evidence. Moreover, Slobodkin et al.
propose the “Attribute First, Then Generate” framework,
which decomposes the conventional end-to-end generation
process into three explicit steps: content selection, sentence
planning, and sequential sentence generation. By structur-
ing the generation pipeline, this method improves both the
quality of the generated responses and the accuracy of ci-
tations, leading to more verifiable and accountable outputs.
Most recently, MedCite introduces an end-to-end cita-
tion generation and evaluation framework for medical QA,
leveraging multi-stage retrieval and LLM-based reranking
to substantially improve citation precision and recall.

In summary, prompt-based methods improve the ability
of LLMs to generate content grounded in external evidence
through foundational approaches like few-shot ICL and
CoT reasoning, as well as task-specific enhancements that
emphasize structured planning, memory mechanisms, and
fine-grained attribution.

6.4.2 Tuning-based Methods

While prompt-based methods enable LLMs to perform
external data sourcing through instructions and few-shot
demonstrations, these approaches often result in suboptimal
attribution quality. The reliance solely on prompting can-
not fully address the challenge of generating accurate and
fine-grained citations. To overcome these limitations, recent
works have explored fine-tuning LLMs, enabling them to
achieve better attribution performance.

Ye et al. propose AGREE, a holistic framework that
fine-tunes LLMs to self-ground claims in their responses
by providing accurate citations to retrieved documents.
AGREE also introduces a test-time adaptation (TTA) mech-
anism that actively retrieves additional supporting pas-
sages for ungrounded claims, iteratively refining the re-
sponses and improving overall citation quality. To generate
more fine-grained and consistent citations, a two-stage fine-
tuning framework named FRONT was proposed [148]. In
the first stage, the LLM learns to extract supporting quotes
from retrieved documents before generating attributed an-
swers. The second stage applies direct preference optimiza-
tion (DPO) to explicitly align the grounding pro-
cess with the generation process, significantly enhancing
the consistency and precision of citations in the generated
responses. Huang et al. further propose a training
framework that incorporates fine-grained rewards as opti-
mization signals to improve attribution performance. The
framework leverages two training algorithms: rejection sam-
pling and reinforcement learning. By providing reward sig-
nals based on fine-grained attribution quality, this approach

24

teaches LLMs to produce attributable responses with supe-
rior generalization capabilities, particularly in tasks requir-
ing accurate evidence grounding. Similarly, Li et al.
model the attribution task as a preference learning problem
and introduce a fully automated framework for collecting
attribution data. This framework includes a progressive
preference optimization method to alleviate sparse reward
issues, leveraging fine-grained information to enhance ci-
tation precision. To address the challenge of fine-grained
attribution in long-form question-answering tasks, Xia et
al. propose a sentence-level approach, which alternates
between generating answer sentences and corresponding
citations, ensuring that each sentence is explicitly supported
by specific evidence.

In summary, tuning-based methods provide a systematic
way to enhance LLMs’ performance in external data sourc-
ing tasks. By constructing high-quality fine-tuning datasets
and leveraging advanced training strategies, these meth-
ods enable LLMs to produce fine-grained, consistent, and
verifiable citations. This line of research marks a critical
step toward improving the reliability and trustworthiness
of LLM outputs in real-world applications.

6.5 Challenges

Advancements in external data sourcing face a number of
ongoing challenges across evaluation, bias, and granularity.
Firstly, a significant challenge lies in establishing reli-
able and scalable automatic evaluation protocols. Current
automatic evaluation protocols largely rely on NLI models
or use LLMs themselves as entailment judges. However,
Li et al. [260 shows that even with fine-tuning, LLMs
still struggle to surpass an accuracy judge, achieving only
about 80% macro-F1 under a simplified binary classification
setting. This performance gap highlights the need for more
robust and reliable evaluation frameworks that can better
approximate human judgments in diverse scenarios.

Another critical issue is mitigating attribution bias in ci-
tation generation, which is prone to systematic distortions in
source selection. For example, [261] demonstrates that LLMs
exhibit preferences toward particular media outlets, with
political leanings influencing citation tendencies. Similarly,
finds that models display an attribution bias favoring
explicitly human-authored content. Future research should
focus on diagnosing, quantifying, and mitigating such bi-
ases to ensure fairness, diversity, and neutrality in external
data sourcing.

Finally, beyond bias mitigation, there is a growing need
for more fine-grained attribution methods. Most existing
methods provide only coarse-grained citations, often assign-
ing a handful of references to entire responses. Recent work
such as ALiiCE suggests that more fine-grained attri-
bution (i.e., down to the paragraph, sentence, or even phrase
level) is both feasible and desirable. Advancing toward such
granular attribution would enable greater transparency and
verifiability, but requires innovations in data construction,
model design, and evaluation protocols.

7 CONCLUSION

As LLMs transition from supporting objective recogni-
tion tasks to making subjective, high-stakes decisions, the


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

opacity of their architectures and the complexity of their
data pipelines make provenance tracking not just desirable,
but indispensable. This survey introduced a unified, four-
dimensional framework for sourcing in LLMs — spanning
Model Sourcing, Model Structure Sourcing, Training Data
Sourcing, and External Data Sourcing — and systematically
categorized posterior-based and prior-based approaches
within each dimension. By integrating model- and data-
centric perspectives, our framework transforms provenance
from a set of isolated techniques into a coherent strategy for
transparency, accountability, and risk mitigation across the
full lifecycle of LLM-generated content.

This holistic view enables proactive interventions such
as watermarking, structural markers, and data fingerprint-
ing, while also supporting retrospective analyses through
statistical, activation-based, and gradient-based inference.
The trade-offs revealed between proactive verifiability and
post-hoc analyzability provide actionable guidance for de-
velopers, regulators, and researchers seeking to balance
traceability with performance and privacy constraints.

Looking forward, the path to trustworthy LLM deploy-
ment will require not only advancing technical precision
in provenance tracking, but also embedding these mecha-
nisms into governance frameworks, compensation systems
for data creators, and real-time safeguards against harmful
or manipulative content. As LLMs become foundational in-
frastructure for decision-making in society, the provenance
paradigm outlined here offers a practical blueprint for turn-
ing opaque generative processes into auditable, responsible,
and equitable AI systems.

ACKNOWLEDGEMENT

This work is supported by the Beijing Nova Program under
Grants No. 20250484765, and the National Natural Science
Foundation of China (NSFC) under Grants No. 62276248,
and the Youth Innovation Promotion Association CAS un-
der Grants No. 2023111.

REFERENCES

[1] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu,
S. Ma, P. Wang, X. Bi et al., “Deepseek-r1: Incentivizing reasoning
capability in llms via reinforcement learning,” arXiv preprint
arXiv:2501.12948, 2025.

[2] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu,
F. Huang, H. Wei et al., “Qwen2. 5 technical report,” arXiv preprint
arXiv:2412.15115, 2024.

[3] OpenAI, “Gpt-4 technical report,” 2023.

[4] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-
Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan et al., “The
llama 3 herd of models,” arXiv preprint arXiv:2407.21783, 2024.

[5] G.G. Team, “Gemini: A family of highly capable multimodal
models,” 2023.

[6] W. xX. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen,
J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R.
Wen, “A survey of large language models,” 2023.

[7] H. Naveed, A. U. Khan, S. Qiu, M. Sagib, S. Anwar, M. Usman,
N. Akhtar, N. Barnes, and A. Mian, “A comprehensive overview
of large language models,” 2023.

[8] | M. Chen, J. Tworek, H. Jun, Q. Yuan, and H. P. de Oliveira Pinto et
al., “Evaluating large language models trained on code,” 2021.

[9] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan,
Y. Adi, J. Liu, T. Remez, J. Rapin et al., “Code lama: Open
foundation models for code,” arXiv preprint arXiv:2308.12950,
2023.

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

25

W. Gan, Z. Qi, J. Wu, and J. C.-W. Lin, “Large language models
in education: Vision and opportunities,” 2023.

S. Xu, X. Huang, Z. Wei, L. Pang, H. Shen, and X. Cheng,
“Reverse physician-ai relationship: Full-process clinical diagnosis
driven by a large language model,” 2025. [Online]. Available:
J. Lai, W. Gan, J. Wu, Z. Qi, and P. S. Yu, “Large language models
in law: A survey,” 2023.

J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,
D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., “Emer-
gent abilities of large language models,” arXiv _ preprint
arXiv:2206.07682, 2022.

S. Dai, C. Xu, S. Xu, L. Pang, Z. Dong, and J. Xu, “Bias and
unfairness in information retrieval systems: New challenges in
the Ilm era,” in Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, 2024, pp. 6437-6447.

I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Der-
noncourt, T. Yu, R. Zhang, and N. K. Ahmed, “Bias and fairness
in large language models: A survey,” Computational Linguistics,
vol. 50, no. 3, pp. 1097-1179, 2024.

L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,
W. Peng, X. Feng, B. Qin et al., “A survey on hallucination in
large language models: Principles, taxonomy, challenges, and
open questions,” ACM Transactions on Information Systems, vol. 43,
no. 2, pp. 1-55, 2025.

H. Ding, L. Pang, Z. Wei, H. Shen, and X. Cheng, “Retrieve
only when it needs: Adaptive retrieval augmentation for hal-
lucination mitigation in large language models,” arXiv preprint
arXiv:2402.10612, 2024.

M. Q. Li and B. Fung, “Security concerns for large language
models: A survey,” arXiv preprint arXiv:2505.18889, 2025.

V. Methuku and P. K. Myakala, “Digital doppelgangers: Ethical
and societal implications of pre-mortem ai clones,” arXiv preprint
arXiv:2502.21248, 2025.

S. Gehrmann, H. Strobelt, and A. M. Rush, “Gltr: Statistical
detection and visualization of generated text,” 2019.

E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn,
“Detectgpt: Zero-shot machine-generated text detection using
probability curvature,” 2023.

G. Bao, Y. Zhao, Z. Teng, L. Yang, and Y. Zhang, “Fast-detectgpt:
Efficient zero-shot detection of machine-generated text via con-
ditional probability curvature,” 2024.

X. Yang, K. Zhang, H. Chen, L. Petzold, W. Y. Wang, and
W. Cheng, “Zero-shot detection of machine-generated codes,”
2023.

L. An, Y. Liu, Y. Liu, Y. Zhang, Y. Bu, and S. Chang,
“Defending Ilm watermarking against spoofing attacks with
contrastive representation learning,” 2025. [Online]. Available:
https: / /arxiv.org /abs /2504.06575

S. Mitrovic, D. Andreoletti, and O. Ayoub, “Chatgpt or human?
detect and explain. explaining decisions of machine learning
model for detecting short chatgpt-generated text,” 2023.

J. D. Rodriguez, T. Hay, D. Gros, Z. Shamsi, and R. Srinivasan,
“Cross-domain detection of GPT-2-generated technical text,”
in Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human
Language Technologies, M. Carpuat, M.-C. de Marneffe, and
I. V. Meza Ruiz, Eds. Seattle, United States: Association for
Computational Linguistics, Jul. 2022, pp. 1213-1233. [Online].
Available:

D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, “Au-
tomatic detection of generated text is easiest when humans are
fooled,” arXiv preprint arXiv:1911.00650, 2019.

L. Fréhling and A. Zubiaga, “Feature-based detection of auto-
mated language models: tackling gpt-2, gpt-3 and grover,” Peer]
Computer Science, vol. 7, p. e443, 2021.

K. Wu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Llmdet: A
third party large language models generated text detection tool,”
2023. {Oniine] Available: https: /arxiv.org/abs/2305 15004

X. Liu, Z. Zhang, Y. Wang, H. Pu, Y. Lan, and C. Shen,
“Coco: Coherence-enhanced machine-generated text detection
under data limitation with contrastive learning,” 2023. [Online].
Available: https /arvv.org/abs/2212.10341

W. Zhong, D. Tang, Z. Xu, R. Wang, N. Duan, M. Zhou, J. Wang,
and J. Yin, “Neural deepfake detection with factual structure of
text,” arXiv preprint arXiv:2010.07475, 2020.



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

V. Verma, E. Fleisig, N. Tomlin, and D. Klein, “Ghostbuster:
Detecting text ghostwritten b large language 2 models" 2024.
[Online]. Available:
C. Mao, C. Vondrick, H. Wang, ai and J. Yang, aidar: a penerative
ai detection via rewriting,” arXiv preprint arXiv:2401.12970, 2024.
J. Wu, R. Zhan, D. F. Wong, S. Yang, X. Liu, L. S. Chao,
and M. Zhang, “Who wrote this? the key to zero-shot llm-
penerated text detection is gecscore,” 2025. [Online]. Available:

Ry

Detectgpt-sc: Improving detection of
text nevrated by large Taneuage models through self-consistency
with masked predictions,” 2023.
M. Gallé, J. Rozen, G. Kruszewski, and H. Elsahar, “Unsuper-
vised and distributional detection of machine-generated text,”
arXiv preprint arXiv:2111.02878, 2021.
Y. Chang, K. Krishna, A. Houmansadr, J. Wieting, and M. lyyer,
“Postmark: A robust blackbox watermark for large language
models,” ArXiv, vol. abs/2406.14517, 2024.
P. S. Yu, L. Fang, Y. Zhang, A. Liu, L. Wen, L. Pan, X. Hu,
Y. Liu, and S. Guan, “Can watermarked Ilms be identified
users via crafted prompts?” 2024. [Online]. Available:
yee: pone, ig fais (2010). 03168
Hu, en, and T.-Y. Ho, “Radar: Robust ai-text detection
via aimee learning,” Advances in neural information processing
systems, vol. 36, pp. 15077-15095, 2023.
R. Koike, M. Kaneko, and N. Okazaki, “Outfox: Llm-generated
essay detection through in-context learning with adversarially
generated examples,” in Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 38, no. 19, 2024, pp. 21 258-21 266.
J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Gold-
stein, “A watermark for large language models,” 2023.
J. Kirchenbauer, J. Geiping, Y. Wen, M. Shu, K. Saifullah,
K. Kong, K. Fernando, A. Saha, M. Goldblum, and T. Goldstein,
“On the reliability of watermarks for large language models,”
in The Twelfth International Conference on Learning Representations,

2024. [Online]. Available: https:// openreview.net forum?id=
DEJIDCmWOz

P.S. Yu, A. Liu, L. Wen, L. Pan, Y. Lu, I. King, S. Huang,
Z. Gao, and Y. Di, “Waterseeker: Pioneering efficient detection
of watermarked segments in large documents,” 2024. [Online].
Available:|https: / /arxiv.org/ abs/: 2409.05112

Y. Xu, H. Xiong, A. Liu, L. Wen, and X. Hu, “Mark your
Ilm: Detecting the misuse of open-source large language
haps!

models via watermarking,” 2025. [Online]. Available: 2
/ /arxiv.org/abs /2503.04636

Y. Wang, Y. Ren, Y. Cao, and B. Fang, “From trade-off
to synergy: A versatile symbiotic watermarking framework
for large language models,” 2025. [Online]. Available:

/ /arxiv.org i abs /2505.09924
Y. Liu and Y. Bu, “Adaptive text watermark for large language
models,” 2024. [Online]. Available: https: //arxiv. org/ abs /’ 2401.
Z. Wang, T. Gu, B. Wu, and Y. Yang, “Morphmark: Flexible
adaptive watermarking for large language models,” 2025.
[Online]. Available: https: / /arxiv.org/abs/2505.11541
A. Liu, L. Pan, X. Hu, i, L. Wen, I. King, and P. S. Yu, “An
unforgeable publicly verifiable watermark for large language
models,” 2024.
X. Zhao, L. Li, and Y.-X. Wang, “Distillation-resistant watermark-
ing for model protection in nlp,” 2022.
S. Dathathri, A. See, S. Ghaisas, P.-S. Huang, R. McAdam,
J. Welbl, V. Bachani, A. Kaskasoli, R. Stanforth, T. Matejovicova,
J. Hayes, M. Merey, J. Brown-Cohen, R. Bunel, B. Balle, A. Cemgil,
Z. Ahmed, K. Stacpoole, and P. Kohli, “Scalable watermarking for
identifying large language model outputs,” Nature, vol. 634, pp.
818-823, 10 2024.
PS. Yu, A. Liu, L. Wen, L. Pan, X. Hu, Y. Lu, I. King,
and S. Huang, “Can Ilm watermarks robustly prevent
unauthorized knowledge distillation?” 2025. [Online]. Available:
tips: arxiv.org abs 2502. 108
. en, W . Wu, H. Zhang, and H. Huang,
“Unbiased watermark for sce language models,” arXiv preprint
arXiv:2310.10669, 2023.
X. Yang, K. Chen, W. Zhang, C. Liu, Y. Qi,
H. Fang, and N. Yu, i

Watermarking text generated b
black-box language models,” 2023. [Online]. Available:
/ /arxiv.org/ abs /2305.08883

J. Zhang,

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

26

C. Gu, C. Huang, X. Zheng, K.-W. Chang, and C.-J. Hsieh,

“Watermarking pre-trained language models with backdooring,”
2023. [Online]. Available: |htps’/ /arxiv.org/abs/2210.07543)

Z. Yang, L. Zhou, Z. Zhang, and K. Huang, “Agent guide:
A simple agent behavioral watermarking framework,” 2025.
[Oniine). Available: https//arsv.org/abs/ 250405871

M. Geva, R. Schuster, J. Berant, and O. Levy, “Transformer
feed-forward layers are key-value memories,” arXiv preprint
arXiv:2012.14913, 2020.

M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg, “Trans-
former feed-forward layers build predictions by promoting con-
cepts in the vocabulary space,” arXiv preprint arXiv:2203.14680,
2022.

M. Geva, A. Caciularu, G. Dar, P. Roit, S. Sadde, M. Shlain,
B. Tamir, and Y. Goldberg, “Lm-debugger: An interactive tool
for inspection and intervention in transformer-based language
models,” arXiv preprint arXiv:2204.12130, 2022.

Y. Nikankin, A. Reusch, A. Mueller, and Y. Belinkov, “Arithmetic
without algorithms: Language models solve math with a bag of
heuristics,” arXiv preprint arXiv:2410.21272, 2024.

D. Venditti, E. S. Ruzzetti, G. A. Xompero, C. Giannone,
A. Favalli, R. Romagnoli, and F. M. Zanzotto, “Enhancing data
privacy in large language models through private association
editing,” arXiv preprint arXiv:2406.18221, 2024.

X. Wu, J. Li, M. Xu, W. Dong, S. Wu, C. Bian, and
D. Xiong, “DEPN: Detecting and editing privacy neurons
in pretrained language models,” in Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing,
H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for
Computational mo Lan Dec. 2023, pp. 2875- 2886. [Online].
Available:

B. Tian, X. Tang, S. Cheng, O. Liu, M. Wang, T D. Sui, X. Chen,
H. Chen, and N. Zhang, “To forget or not? towards practical
knowledge unlearning for large language models,” arXiv preprint
arXiv:2407.01920, 2024.

M. Wang, N. Zhang, Z. Xu, Z. Xi, S. Deng, Y. Yao,
Q. Zhang, L. Yang, J. Wang, and H. Chen, “Detoxifying large
language models via knowledge editing,” in Proceedings of
the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins,
and V. Srikumar, Eds. Bangkok, Thailand: Association for
Computational afar stleislen Aug. 2024, pp. 3093- ae [Online].
Available:

C. Yu, S. Ibeones A. Kasi, P. Yu, and H. zr “naming bias in
language models by partitioning gradients,” in Findings of the
Association for Computational Linguistics: ACL 2023, 2023, pp. 6032—
6048.

R. Hazra, S. Layek, S. Banerjee, and S. Poria, “Safety
arithmetic: A framework for test-time safety alignment of
language models by steering parameters and activations,” in
Proceedings of the 2024 Conference on Empirical Methods in Natural
Language Processing, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen,
Eds. Miami, Florida, USA: Association for Computational
Linguistics, Nov. 2024, pp. 21759-21776. [Online]. Available:

https: //aclanthology.org /2024.emnlp-main.1212

W. Zhao, Z. Li, Y. Li, Y. Zhang, and J. Sun,
large language models against jailbreak attacks via layer-
specific editing,” in Findings of the Association for Computational

“Defending

vinguistics: EMNLP 2024, Y. Al-Onaizan, M. Bansal, and
Y.-N. Chen, Eds. Miami, Florida, USA: Association for
Computational Lia muistics, Nov. 2024, pp. - 5094-5109. on
Available:
Z. Duan, Z. Yin, Z. Shi, L. Tans, S. fT Wu, Y. Yan, 23 hen,
and X. Cheng, “Gloss over toxicity: Understanding and miti-
gating toxicity in Ilms via global toxic subspace,” arXiv preprint
arXiv:2505.17078, 2025.
J. Deng, T. Tang, Y. Yin, W. Yang, W. X. Zhao, and J.-R. Wen,
“Neuron-based personality trait induction in large language
models,” arXiv preprint arXiv:2410.12327, 2024.
K. Deng, Z. Huang, C. Li, C. Lin, M. Gao, and W. Rong,
“Pfme: A modular approach for fine-grained hallucination de-
tection and editing of large language models,” arXiv preprint
arXiv:2407.00488, 2024.
S. Zhang, T. Yu, and Y. Feng, “Truthx: Alleviating hallucina-
tions by editing large language models in truthful space,” arXiv
preprint arXiv:2402.17811, 2024.



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[71]

[72]

[73]

[74]

[75]

[76]

[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

W. Chen, Z. Huang, L. Xie, B. Lin, H. Li, L. Lu, X. Tian,
D. Cai, Y. Zhang, W. Wang, X. Shen, and J. Ye, “From yes-men
to truth-tellers: Addressing sycophancy in large language
models with pinpoint tuning,” in Forty-first International
Conference on Machine Learning, ICML 2024, Vienna, Austria,
uly 21-27, 2024. OpenReview.net, 2024. [Online]. Available:

https: / /openreview.net/forum?id=d2vONO90Rw

D. Dai, L. Dong, Y. Hao, Z. Sui, B. ang, and F. Wei,

“Knowledge neurons in pretrained transformers,” in Proceedings
of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,
May 22-27, 2022, 5. Muresan, P. Nakov, and A. Villavicencio, Eds.
Association for Computational Linguistics, 2022, pp. 8493-8502.

https://doi.org/10.18653/v1/2022.acl-long.

[Online]. Available:
581

K. Meng, D. Bau, A. Andonian, and Y. Belinkov, “Locating
and editing factual associations in gpt,” in Advances in Neural
Information Processing Systems, vol. 35, 2022.

K. Meng, A. S. Sharma, A. J. Andonian, Y. Belinkov, and
D. Bau, “Mass-editing memory in a transformer,” in The Eleventh
International Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023. _OpenReview.net, 2023. [Online].
Available:
M. Geva, J. Bastings, K. Filippova, and A. Globerson, “Dissecting
recall of factual associations in auto-regressive language models,”
in Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2023, Singapore, December
6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Association
for Computational Lnauistss. 2023, pp. 12 216-12 235. oul.
Available:
A. Ly, K. Z ang, en, ang, L. Eire J. Wen, I. ‘de,
and R. Yan, “Interpreting key "mechanisms of factual recall in
transformer-based language models,” CoRR, vol. abs/2403.19521,
2024. [Online]. Available:
J. Merullo, C. Eickhoff, and E. Pavlick, “Circuit component reuse
across tasks in transformer language models,” in The Twelfth
International Conference on Learning Representations.

C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and
S. Carter, “Zoom in: An introduction to circuits,” Distill, vol. 5,
no. 3, pp. e00 024-001, 2020.

Y. Yao, N. Zhang, Z. Xi, M. Wang, Z. Xu, S. Deng, and H. Chen,
“Knowledge circuits in pretrained transformers,” in Advances in
Neural Information Processing Systems 38: Annual Conference on
Neural Information Processing Systems 2024, NeurIPS 2024, Vancou-
ver, BC, Canada, December 10 - 15, 2024, A. Globersons, L. Mackey,
D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang,
Eds., 2024.

K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and
J. Steinhardt, “Interpretability in the wild: a circuit for indirect
object identification in GPT-2 small,” in The Eleventh International
Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
ve Br. 5, 2023. OpenReview. net, 2023. Ce Available:
. Fata 1, T and N. Goodman, 1 ‘step by step?
reasoning emerges from the locality of experience,” Advances in
Neural Information Processing Systems, vol. 36, pp. 70926-70947,
2023.

M. Lan, P. Torr, and F. Barez, “Towards interpretable sequence
continuation: Analyzing shared circuits in large language mod-
els,” in Proceedings of the 2024 Conference on Empirical Methods in
Natural Language Processing, 2024, pp. 12576-12601.

E. Akyiirek, D. Schuurmans, J. Andreas, T. Ma, and
D. Zhou, “What learning algorithm is in-context learning?
investigations with linear models,” in The Eleventh International

Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
Ma

1-5, 2023. OpenReview.net, 2023. [Online]. Available:
https:// SSE net/forum?id=0g0X4H8yN4lI

Oswald, E. Niklasson, E. Randazzo, J. Sacramento,
A. Zhmoginov, and M. Vladymyrov,
learn in-context by gradient descent,” in
International Conference on Machine Learning, ICML 2023,
23-29 July 2023, Honolulu, Hawaii, USA, ser. Proceedings
of Machine Learning Research, A. Krause, E. Brunskill,
K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds.,
vol. 202. PMLR, 2023, pp. 35151-35174. [Online]. Available:

J. von
A. Mordvintsev,

“Transformers

https: / /proceedings.mlr.press/v202/von-oswald23a.html

[85]

[86]

[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

[96]

[97]

[98]

[99]

[100]

[101]

[102]

27

D. Fu, T. Chen, R. Jia, and V. Sharan, “Transformers learn
to achieve second-order convergence rates for in-context linear
regression,” in Advances in Neural Information Processing Systems
38: Annual Conference on Neural Information Processing Systems
2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15,
2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet,
J. M. Tomezak, and C. Zhang, Eds., 2024. [Online]. Avail-

Jf]

2d4051f03a7038a dtbbe5c7b54e- Abstract-Conterence.
J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacremento,
A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov, “Trans-
formers learn in-context by gradient descent,” in International
Conference on Machine Learning. PMLR, 2023, pp. 35151-35174.
D. Fu, T.-Q. Chen, R. Jia, and V. Sharan, “Transformers learn
to achieve second-order convergence rates for in-context linear
regression,” Advances in Neural Information Processing Systems,
vol. 37, pp. 98 675-98 716, 2024.

E. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace,
and D. Bau, “Function vectors in large language models,” in
The Twelfth International Conference on Learning Representations,

ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net,
https: / /openreview.net/forum?id=

2024. [Online]. Available:

AwyxtyMwaG
. Wei, L. Pang, H. Ding, J. Deng, H. Shen, and X. Cheng, “Stable

knowledge editing in large language models,” 2024.

J. Deng, Z. Wei, L. Pang, H. Ding, H. Shen, and X. Cheng, “Ev-
erything is editable: Extend knowledge editing to unstructured
data in large language models,” arXiv preprint arXiv:2405.15349,
2024.

T. Simonds, K. Kurniawan, and J. H. Lau, “Modem: Mixture of
domain expert models,” arXiv preprint arXiv:2410.07490, 2024.

S. Jiang, T. Zheng, Y. Zhang, Y. Jin, L. Yuan, and Z. Liu, “Med-
moe: Mixture of domain-specific experts for lightweight medical
vision-language models,” arXiv preprint arXiv:2404.10237, 2024.
K. Yaggel, E. German, and A. Ben Siman Tov, “Moe-mlora for
multi-domain ctr prediction: Efficient adaptation with expert
specialization,” arXiv preprint arXiv:2506.07563, 2025.

H. Cheng, H. Fang, X. Liu, and J. Gao, “Task-aware specialization
for efficient and robust dense retrieval for open-domain question
answering,” arXiv preprint arXiv:2210.05156, 2023.

DeepSeek-Al, D. Dai, C. Deng, Z. Sui, X. Wang, H. Wang,
T. Zheng, B. Xiao, Q. Lv, Y. Wang, B. Ding, D. Liu, Z. Chu, M. Yan,
J. Wang, and J. Zhou, “Deepseekmoe: Towards ultimate expert
specialization in mixture-of-experts language models,” 2024.

Z. Li, X. Wang, Y. Zhang, B. Zhang, N. Zhang, S. Cheng,
R. Xu, Y. Tang, Y. Chen, G. Meng, J. Xiao, X. Chen, S. Pu, and
Y. Cao, “Dynamic mixture of experts: An auto-tuning approach
eisiag transformer models, ” 2024. [Online]. Available:
. A. Armenteros,
a Arteta R. Avram, and f B. et al., “Moeut: Mixture-of-experts
universal transformers,” 2024.

S. M. Park, K. Georgiev, A. Ilyas, G. Leclerc, and A. Madry,
“Trak: attributing model behavior at scale,” in Proceedings of the
40th International Conference on Machine Learning, ser. ICML’23.
JMLR.org, 2023.

F. Ladhak, E. Durmus, and T. Hashimoto, “Contrastive error
attribution for finetuned language models,” in Proceedings of
the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), A. Rogers, J. Boyd-Graber,
and N. Okazaki, Eds. Toronto, Canada: Association for
Computational Linguistics, ul. 2023, pp. 11482-11 vat [Online].
Available:
R. Grosse, J. Bae, . Anil, N. hace 2 A. Tamkin, aN Tajdini,
B. Steiner, D. Li, E. Dies, E. Perez, E. Hubinger, K. LukoSiiite,
K. Nguyen, N. Joseph, S. McCandlish, J. Kaplan, and S. R.
Bowman, “Studying large language model generalization
with influence functions,” 2023. [Online]. Available:
//arxiv.org/abs/2308.03296

E. Akyitirek, T. Bolukbasi, F. Liu, B. Xiong, I. Tenney, J. Andreas,
and K. Guu, “Towards tracing factual knowledge in language
models back to the training data,” arXiv preprint arXiv:2205.11482,
2022.

Z. Hammoudeh and D. Lowd, “Identifying a training-set attack’s
target using renormalized influence estimation,” in Proceedings of
the 2022 ACM SIGSAC Conference on Computer and Communications
Security, 2022, pp. 1367-1381.



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[103]

[104]

[105]

[106]

[107]

[108]

[109]

[110]

[111]

[112]

[113]

[114]

[115]

[116]

[117]

[118]

[119]

[120]

[121]

[122]

[123]

[124]

E. Nguyen, M. Seo, and S. J. Oh, “A bayesian approach
to analysing training data attribution in deep learning,” in
Thirty-seventh Conference on Neural Information Processing Systems,
2023. [Online]. Available:
XSCYxDp3yE

A. Schioppa, P. Zablotskaia, D. Vilar, and A. Sokolov,
“Scaling up influence functions,” 2021. [Online]. Available:
N. Konz, . odfrey, M apiro, J. Tu, H. Kvinge,
and D. Brown, “Attributing learned concepts in neural

networks to training data,” 2023. [Online]. Available:
/ /arxiv.org/abs /2310.03149
. Pruthi, F Liu, S. Kale, and M. Sundararajan, “Estimating

training data influence by tracing gradient descent,” Advances in
Neural Information Processing Systems, vol. 33, pp. 19920-19930,
2020.

S. Basu, P. Pope, and S. Feizi, “Influence functions in

deep learning are fragile,” 2021. [Online]. Available:
/ /arxiv.org/abs /2006.14651
Kwon, E. Wu, K. Wu, and J. Zou, “Datainf: Efficiently

estimating data influence in lora-tuned Ilms and_ diffusion

models,” 2024. [Online]. Available: https://arxiv.org/abs/2310.
00902

<

N. Anand, J. Tan, and M. Minakova, “Influence scores at scale
for efficient language data sampling,” 2023. [Online]. Available:
https: / /arxiv.org /abs/2311.16298

H. Guo, N. F Rajani, P. Hase, M. Bansal, and C. Xiong,
“Fastif: Scalable influence functions for efficient model

interpretation and debugging,” 2021. [Online]. Available:

ttps://arxiv.org/abs/2012.15781

. Barshan, M.-E. Brunet, and G. K. Dziugaite, “Relatif:
Identifying explanatory training examples via relative influence,”
2020. [Online]. Available: https: / /arxiv.org /abs /2003.11630

K. Wu, L. Pang, H. Shen, and X. Cheng, ancing training
data attribution for large language models with fitting error
consideration,” in Proceedings of the 2024 Conference on Empirical
Methods in Natural Language Processing, 2024, pp. 14131-14143.

K. Hanawa, S. Yokoi, S. Hara, and K. Inui, “Evaluation
-based explanations,” 2021. [Online]. Available:

: : . Wallace, and S. Singh, “An empir-
ical comparison of instance attribution methods for nlp,” arXiv
preprint arXiv:2104.04128, 2021.

B. Z. H. Zhao, H. J. Asghar, R. Bhaskar, and M. A. Kaafar, “On

inferring training data attributes in machine learning models,”
2019. [Online]. Available: https: / /arxiv.org /abs/1908.10558
M. Fotouhi, M. T. Bahadori, O. Feyisetan, P. Arabshahi, and

D. Heckerman, “Fast training dataset attribution via in-context
learning,” 2025. [Online]. Available: https://arxiv.org/abs/2408.
S. Chen, F. Kang, N. Yu, and R. Jia, “Fasttrack: Fast and
accurate fact tracing for llms,” 2024. [Online]. Available:
https: / /arxiv.org /abs /2404.15157
M. Li, X. Chen, A. Holtzman, B. Chen, J. Lin, S. Yih, and V. Lin,
“Nearest neighbor speculative decoding for llm generation and
attribution,” Advances in Neural Information Processing Systems,
vol. 37, pp. 80 987-81 015, 2024.

T.-R. Chiang, X. V. Yu, J. Robinson, O. Liu, I. Lee, and D. Yo-
gatama, “On retrieval augmentation and the limitations of lan-
guage model training,” arXiv preprint arXiv:2311.09615, 2023.

J. Wang, X. Lu, Z. Zhao, Z. Dai, C.-S. Foo, S.-K. Ng, and B. K. H.
Low, “Wasa: Watermark-based source attribution for large lan-
guage model-generated data,” arXiv preprint arXiv:2310.00646,
2023.

X. Zhao, S. Gunn, M. Christ, J. Fairoze, A. Fabrega, N. Carlini,
S. Garg, S. Hong, M. Nasr, F. Tramer, S. Jha, L. Li, Y.-X. Wang,
and D. Song, “Sok: Watermarking for ai-generated content,”
2025. [Online]. Available: https: / /arxiv.org /abs/2411.18479
M. Khalifa, D. Wadden, E. Strubell, H. Lee, Wang, I. Beltagy,
and H. Peng, “Source-aware training enables knowledge attribu-
tion in language models,” arXiv preprint arXiv:2404.01019, 2024.
T. Sander, P. Fernandez, A. Durmus, M. Douze, and T. Furon,
“Watermarking makes language models radioactive,” arXiv
preprint arXiv:2402.14904, 2024.

S. Rastogi, P. Maini, and D. Pruthi, “Stamp your content: Prov-
ing dataset membership via watermarked rephrasings,” arXiv
preprint arXiv:2504.13416, 2025.

[125]

[126]

[127]

[128]

[129]

[130]

[131]

[132]

[133]

[134]

[135]

[136]

[137]

[138]

[139]

[140]

[141]

[142]

[143]

[144]

[145]

28

X. Cui, J. T.-Z. Wei, S. Swayamdipta, and R. Jia, “Robust data
watermarking in language models by injecting fictitious knowl-
edge,” arXiv preprint arX1v:2503.04036, 2025.

A. Ilyas, S. M. Park, L. Engstrom, G. Leclerc, and A. Madry,
“Datamodels: Predicting predictions from training data,” 2022.

[Online]. Available: https: //arxiv.org /abs/2202.00622
A. Khaddaj, L. Engstrom, and A. Madry, “Small-to-large
generalization: Data influences models consistently across scale,”

2025. [Online]. Available: https: / /arxiv.org /abs/2505.16260

F. Liu, N. Kandpal, and C. Raffel, “Attribot: A bag of tricks

for efficiently approximating leave-one-out context attribution,”
2025. [Online]. Available: https: / /arxiv.org /abs/2411.15102

J. Yang, W. Deng, B. Liu, Y. Huang, J. Zou, and X. Li,

“Gmvaluator: Similarity-based data valuation for generative
models,” 2025. [Online]. Available: https://arxiv.org/abs/2304.

10701

L. Engstrom, A. Feldmann, and A. Madry, “Dsdm: Model-
aware dataset selection with datamodels, 2024,” URL https://arxiv.
org/abs/2401.12926, 2024.

Z. Yu, S. Das, and C. Xiong, “Mates: Model-aware data selection
for efficient pretraining with data influence models,” Advances in
Neural Information Processing Systems, vol. 37, pp. 108 735-108 759,
2024.

Z. Yu, F. Peng, J. Lei, A. Overwijk, W.-t. Yih, and C. Xiong, “Data-
efficient pretraining with group-level data influence modeling,”
arXiv preprint arXiv:2502.14709, 2025.

T. Gao, H. Yen, J. Yu, and D. Chen, “Enabling large language
models to generate text with citations,” in Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing,
Dec. 2023, pp. 6465-6488.

L. Gao, Z. Dai, P. Pasupat, A. Chen, A. T. Chaganty, Y. Fan,
V. Zhao, N. Lao, H. Lee, D.-C. Juan et al., “Rarr: Researching
and revising what language models say, using language models,”
in Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), 2023, pp. 16 477—
16508.

S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-
the-chain: Interactively enhancing large language models with
search for knowledge-intensive tasks,” in Proceedings of the ACM
Web Conference 2024, 2024, pp. 1362-1373.

X. Ye, R. Sun, S. O. Arik, and T. Pfister, “Effective large lan-
guage model adaptation for improved grounding,” arXiv preprint
arXiv:2311.09533, 2023.

W. Li, J. Li, W. Ma, and Y. Liu, “Citation-enhanced generation for
Ilm-based chatbot,” arXiv preprint arXiv:2402.16063, 2024.

R. G. Reddy, D. Lee, Y. R. Fung, K. D. Nguyen, Q. Zeng, M. Li,
Z. Wang, C. Voss, and H. Ji, “Smartbook: Ai-assisted situa-
tion report generation for intelligence analysts,” arXiv preprint
arXiv:2303.14337, 2023.

X. Li, C. Zhu, L. Li, Z. Yin, T. Sun, and X. Qiu, “Llatrieval: Llm-
verified retrieval for verifiable generation,” in Proceedings of the
2024 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), 2024, pp. 5453-5471.

B. Ji, H. Liu, M. Du, and S.-K. Ng, “Chain-of-thought improves
text generation with citations in large language models,” in
Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38,
no. 16, 2024, pp. 18 345-18 353.

M. Berchansky, D. Fleischer, M. Wasserblat, and P. Izsak, “Cotar:
Chain-of-thought attribution reasoning with multi-level granu-
larity,” in Findings of the Association for Computational Linguistics:
EMNLP 2024, 2024, pp. 236-246.

C. Fierro, R. K. Amplayo, F. Huot, N. De Cao, J. Maynez,
S. Narayan, and M. Lapata, “Learning to plan and generate text
with citations,” arXiv preprint arXiv:2404.03381, 2024.

A. Slobodkin, E. Hirsch, A. Cattan, T. Schuster, and I. Dagan,
“Attribute first, then generate: Locally-attributable grounded text
generation,” arXiv preprint arXiv:2403.17104, 2024.

H. Sun, H. Cai, B. Wang, Y. Hou, X. Wei, S. Wang, Y. Zhang, and
D. Yin, “Towards verifiable text generation with evolving mem-
ory and self-reflection,” arXiv preprint arXiv:2312.09075, 2023.

J. Zeng, H. Liu, Z. Dai, X. Tang, C. Luo, S. Varshney, Z. Li,
and Q. He, “Cite before you speak: Enhancing context-response
grounding in e-commerce conversational Ilm-agents,” arXiv
preprint arXiv:2503.04830, 2025.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[146]

[147]

[148]

[149]

[150]

[151]

[152]

[153]

[154]

[155]

[156]

[157]

[158]

[159]

[160]

[161]

[162]

[163]

[164]

[165]

[166]

[167]

X. Wang, M. Tan, Q. Jin, G. Xiong, Y. Hu, A. Zhang, Z. Lu, and
M. Zhang, “Medcite: Can language models generate verifiable
text for medicine?” arXiv preprint arXiv:2506.06605, 2025.

A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-
rag: Learning to retrieve, generate, and critique through self-
reflection,” 2024.

L. Huang, X. Feng, W. Ma, Y. Gu, W. Zhong, X. Feng, W. Yu,
W. Peng, D. Tang, D. Tu et al., “Learning fine-grained grounded
citations for attributed large language models,” arXiv preprint
arXiv:2408.04568, 2024.

C. Huang, Z. Wu, Y. Hu, and W. Wang, “Training language
models to generate text with citations via fine-grained rewards,”
arXiv preprint arXiv:2402.04315, 2024.

D. Li, Z. Sun, B. Hu, Z. Liu, X. Hu, X. Liu, and M. Zhang,
“Improving attributed text generation of large language models
via preference learning,” arXiv preprint arXiv:2403.18381, 2024.

S. Xia, X. Wang, J. Liang, Y. Zhang, W. Zhou, J. Deng, F. Yu, and
Y. Xiao, “Ground every sentence: Improving retrieval-augmented
Ilms with interleaved reference-claim generation,” arXiv preprint
arXiv:2407.01796, 2024.

S. G. Ayyamperumal and L. Ge, “Current state of llm risks and
ai guardrails,” arXiv preprint arXiv:2406.12934, 2024.

B. Harandizadeh, A. Salinas, and F. Morstatter, “Risk and re-
sponse in large language models: Evaluating key threat cate-
gories,” arXiv preprint arXiv:2403.14988, 2024.

L. Wu, J. Li, W. Zou, and J. Qi, “Risks of large language models
misalignment: Multi-stakeholder obligations and governance,”
Available at SSRN 5059890.

J. Wu, S. Yang, R. Zhan, Y. Yuan, D. F. Wong, and L. S. Chao, “A
survey on llm-generated text detection: Necessity, methods, and
future directions,” ArXiv, vol. abs/2310.14724, 2023.

V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and
S. Feizi, “Can ai-generated text be reliably detected?” 2025.
[Online Availabe: htps:/ /arxi.orgabs/2308.11156

P. Parshakov, I. Naidenova, S. Paklina, N. Matkin, and
C. Nesseler, “Users favor llm-generated content — until the
know it’s ai,” 2025. [Online]. Available:
C. R. Jones and B. K. Bergen, “Lies, damned lies, and
distributional language statistics: Persuasion and deception
with large language models,” 2024. [Online]. Available:

https: / /arxiv.org / abs /2412.17128

R. Rohan, L. I. D. Faruk, K. Puapholthep, and D. Pal, “Unlocking
the black box: Exploring the use of generative ai (chatgpt) in

information systems research,” in Proceedings of the 13th Interna-
tional Conference on Advances in Information Technology, 2023, pp.
1-9.

C. Singh, J. P. Inala, M. Galley, R. Caruana, and J. Gao, “Rethink-
ing interpretability in the era of large language models,” arXiv
preprint arXiv:2402.01761, 2024.

M. W. Shen, “Trust in ai: Interpretability is not necessary or suf-
ficient, while black-box interaction is necessary and sufficient,”
arXiv preprint arXiv:2202.05302, 2022.

Y. Liu, Y. Yao, J.-F. Ton, X. Zhang, R. G. H. Cheng, Y. Klochkov,
M. F. Taufig, and H. Li, “Trustworthy llms: A survey and guide-
line for evaluating large language models’ alignment,” arXiv
preprint arXiv:2308.05374, 2023.

H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang,
D. Yin, and M. Du, “Explainability for large language models: A
survey,” 2023.

H. Chen, A. Waheed, X. Li, Y. Wang, J. Wang, B. Raj, and
M. I. Abdin, “On the diversity of synthetic data and its impact
on training large language models,” 2024. [Online]. Available:

https:/ /arxiv.org /abs/2410.15226

N. Mehrabi, FE. Morstatter, N. Saxena, K. Lerman, and
A. Galstyan, “A survey on bias and fairness in machine learning,”
2022. [Online]. Available: |https:/ /arxiv.org /abs/1908.09635|

S. Dai, C. Xu, S. Xu, L. Pang, Z. Dong, and J. Xu, “Unifying
bias and unfairness in information retrieval: New challenges in
the Ilm era,” in Proceedings of the Eighteenth ACM International
Conference on Web Search and Data Mining, 2025, pp. 998-1001.

G. Feretzakis and V. S. Verykios, “Trustworthy ai: Securing
sensitive data in large language models,” AI, vol. 5,
no. 4, p. 2773-2800, Dec. 2024. [Online]. Available:

//dx.doi.org/10.3390/ai5040134

[168]

[169]

[170]

[171]

[172]

[173]

[174]

[175]
[176]

[177]

[178]

[179]

[180]

[181]
[182]

[183]

[184]

[185]

[186]

[187]

[188]

[189]

[190]

[191]

29

B. Yan, K. Li, M. Xu, Y. Dong, Y. Zhang, Z. Ren, and X. Cheng,
“On protecting the data privacy of large language models (Ilms):
A survey,” arXiv preprint arXiv:2403.05156, 2024.
M. Miranda, E. S. Ruzzetti, A. Santilli, F M. Zanzotto, S. Bratiéres,
and E. Rodola, “Preserving privacy in large language mod-
els: A survey on current threats and solutions,” arXiv preprint
arXiv:2408.05212, 2024.
K. Wu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Llmdet: A
third party large language models generated text detection tool,”
2023.
B. Park and J. Choi, “Identifying the source of generation for large
language models,” arXiv preprint arXiv:2407.12846, 2024.
A. Krausova and V. Moravec, “Disappearing authorship: Ethical
protection of ai-generated news from the perspective of copyright
and other laws,” J. Intell. Prop. Info. Tech. & Elec. Com. L., vol. 13,
p- 132, 2022.
S. B. Samudrala, A. Bansal, P. R. A. Puchakayala, and S. Patil,
“Ethical challenges and accountability in generative ai: Manag-
ing copyright violations and misinformation in responsible ai
systems,” Utilitas Mathematica, vol. 121, pp. 365-377, 2024.
L. Zhang, H. Wu, L. Zhang, F. Xu, J. Cao, F. Li, and B. Niu,
“Training data attribution: Was your model secretly trained on
data created by mine?” arXiv preprint arXiv:2409.15781, 2024.
G. Abdelrahman, Q. Wang, and B. Nunes, “Knowledge tracing: A
survey,” ACM Computing Surveys, vol. 55, no. 11, pp. 1-37, 2023.
T. Spinner, “Visual, interactive deep model debugging: Support-
ing ai development and explainability,” 2024.
B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue,
and Y. Wu, “How close is chatgpt to human experts? comparison
corpus, evaluation, and detection,” 2023.
T. Fagni, F. Falchi, M. Gambini, A. Martella, and M. Tesconi,
“Tweepfake: About detecting deepfake tweets,” PLOS ONE,
vol. 16, no. 5, p. e0251415, May 2021. [Online]. Available:
ne. //dx.doi. org /10. 1371 /journal. pone.0251415
. Feng, and y arge-scale dataset
for detecting chatgpt-written abstracts,” 2023.
Y. Wang, J. Mansurov, P. Ivanov, J. Su, A. Shelmanov, A. Tsvigun,
C. Whitehouse, O. M. Afzal, T. Mahmoud, A. F. Aji, and P. Nakov,
“M4: Multi-generator, multi-domain, and multi-lingual black-box
machine-generated text detection,” 2023.
X. He, X. Shen, Z. Chen, M. Backes, and Y. Zhang, “Mgtbench:
Benchmarking machine-generated text detection,” 2023.
V. Verma, E. Fleisig, N. Tomlin, and D. Klein, “Ghostbuster:
Detecting text ghostwritten by large language models,” 2023.
Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, T. Hubert, P. Choy,
C. de Masson d’Autume, I. Babuschkin, X. Chen, P.-S. Huang,
J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz,
E. Sutherland Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu,
and O. Vinyals, “Competition-level code generation with
alphacode,” Science, vol. 378, no. 6624, p. 1092-1097, Dec. 2022.
[Online]. Available: http://dx.doi.org/10. 7126 /science.abq1158
D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora,
E. Guo, C. Burns, S. Puranik, H. He, D. Sone, and J. Steinhardt,
“Measuring coding challenge competence with apps,” 2021.
Y. Liu, Z. Zhang, W. Zhang, S. Yue, X. Zhao, X. Cheng, Y. Zhang,
and H. Hu, “Argugpt: evaluating, understanding and identifying
argumentative essays generated by gpt models,” 2023.
J. Houvardas and E. Stamatatos, “N-gram feature selection for
authorship identification,” in International conference on artificial
intelligence: Methodology, systems, and applications. Springer, 2006,
pp. 77-86.
S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying
large language models and knowledge graphs: A roadmap,”
arXiv preprint arXiv:2306.08302, 2023.
F. Petroni, T. Rocktaschel, P. Lewis, A. Bakhtin, Y. Wu, A. H.
Miller, and S. Riedel, “Language models as knowledge bases?”
arXiv preprint arXiv:1909.01066, 2019.
O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot re-
lation extraction via reading comprehension,” arXiv preprint
arXiv:1706.04115, 2017.
Z. Zhong, Z. Wu, C. D. Manning, C. Potts, and D. Chen,
“Mquake: Assessing knowledge editing in language models via
multi-hop questions,” arXiv preprint arXiv:2305.14795, 2023.
Z. Wei, J. Deng, L. Pang, H. Ding, H. Shen, and X. Cheng, “Mlake:
Multilingual knowledge editing benchmark for large language
models,” in Proceedings of the 31st International Conference on



JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[192]

[193]

[194]

[195]

[196]

[197]

[198]

i991 T

[200]

[201]

[202]

[203]

[204]

[205]

Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January
19-24, 2025, O. Rambow, L. Wanner, M. Apidianaki, H. Al-
Khalifa, B. D. Eugenio, and S. Schockaert, Eds. Association
for con Ptationa! Fimguistics. - 4457-4473. Online].
Available:
Z. Duan,

3 . Jing, J.
X. Cheng, “Related Lapwlede perturbation matters: Rethinking

multiple pieces of knowledge editing in same-subject,” arXiv
preprint arXiv:2502.06868, 2025.

S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith,
“Realtoxicityprompts: Evaluating neural toxic degeneration
in language models,” in Findings of the Association for
Computational Linguistics: EMNLP 2020. Online: Association
for comPatationa! Hinguistics, 2020, - 3356- 3369. one
Available:
P. Chao, E. D . Croce,
V. Sehwag, E. Dobriban, N. Flammarion, G. J. Pappas, F. Tramer,
H. Hassani, and E. Wong, “Jailbreakbench: An open robustness
benchmark for jailbreaking large language models,” in Advances
in Neural Information Processing Systems (Datasets and Benchmarks
Track), 2024.

S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring
how models mimic human falsehoods,” in Proceedings of
the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Dublin, Ireland: Association
for Computational Linguistics, 2022, pp. 3214-3252. [Online].
Available:|https:/ /aclanthology.org /2022.acl-long.229/
R. Gupta, I. Arcuschin, T. Kwa, and A. Garriga-Alonso, “Interp-
bench: Semi-synthetic transformers for evaluating mechanistic
interpretability techniques,” in Advances in Neural Information
Processing Systems (NeurIPS 2024), Datasets and Benchmarks Track,
2024.

T. S. Luong, T. Le, L. Ngo Van, and T. H. Nguyen,

“Realistic evaluation of toxicity in large language models,”
arXiv

preprint arXiv:2405.10659, 2024. Available:

https: / /arxiv.org /abs /2405.10659
. Zhou, J. Lou, Z. Huang, Z. Qin, S. Yang, and W. Wang,
“Don’t say no: Jailbreaking Ilm by suppressing refusal,” in
Findings of the Association for Computational Linguistics: ACL
2025. Association for Computational Linguistics, Jul. 2025,
pp. 25 224-25 249. Be cece aa foie Available: frepay acaniologioe/|
ao et al. eae to refuse: Making large language models
more trustworthy,”

[Online].

in Proceedings of the 2024 Conference on

Empirical Methods in Natural Language Processing. Association
for

Computational Linguistics, 2024. [Online]. Available:

https://aclanthology.org /2024.emnlp-main.212.pdf

u, H. Lu, J. Rozner, T. Icard, C. Potts, and N. D.

A. Geiger, Z.
Goodman, “Inducing causal structure for interpretable neural
in Proceedings of the 38th International Conference
PMLR, 2021, pp.

networks,”

on Mirching Tears 3821 3832. [Online].

oulier, K. Pinel- RVaSEat,
S. Ratreriko, aad L. Tamine, “An eveluaifon framework for at-
tributed information retrieval using large language models,”
Proceedings of the 33rd ACM International Conference on Ta feiatioh
and Knowledge Management, 2024, pp. 5354-5359.

W. Zhang, M. Aliannejadi, Y. Yuan, J. Pei, J-H. Huang, and
E. Kanoulas, “Towards fine-grained citation evaluation in gener-
ated text: A comparative analysis of faithfulness metrics,” arXiv
preprint arXiv:2406.15264, 2024.

Y. Xu, J. Gao, X. Yu, B. Bi, H. Shen, and X. Cheng, “Aliice: Eval-
uating positional fine-grained citation generation,” arXiv preprint
arXiv:2406.13375, 2024.

A. Conmy, E. Phillips, N. Nanda, R. Shah, and N. Elhage,
“Towards automated circuit discovery for mechanistic
interpretability,” arXiv preprint arXiv:2304.14997, 2023. [Online].

Available:|https: / /arxiv.org /abs/2304.14997
M. Wang, ao, Z. Xu, Qiao, Deng, P. Wang,

X. Chen, J. Gu, Y. Jiang, P. Xie, F. Huang, H. Chen, and
N. Zhang, “Knowledge mechanisms in large language models:
A survey and perspective,” in Findings of the Association for
Computational Linguistics: EMNLP 2024, Miami, Florida, USA,
November 12-16, 2024, Y. Al-Onaizan, M. Bansal, and Y. Chen,

Eds. Association for Computational Linguistics, 2024, pp.
7097-7135. [Online]. Available: https://doi.org/10.18653/v1/
2024.findings-emnlp.416

30

[206] Z. Xu, S. Wang, K. Xu, H. Xu, M. Wang, X. Deng,
Y. Yao, G. Zheng, H. Chen, and N. Zhang, “Easyedit2: An
easy-to-use steering framework for editing large language
models,” CoRR, vol. abs/2504.15133, 2025. [Online]. Available:
https://doi.org/10.48550/arXiv.2504.15133

L. Berti, F. Giorgi, and G. Kasneci, “Emergent abilities in large
language models: A survey,” CoRR, vol. abs/2503.05788, 2025.
[Online]. Available: pips: //doi. oer 10.48550 /arXiv.2503.05788
D. C. Krakauer, J. W. Krakauer, and M. Mitchell, “Large anguage
models and emergence: A complex systems perspective,”

CoRR, vol. abs/2506.11135, 2025. [Online]. Available:

/doi.org/ 0. 48550/arXiv.2506.11135
. Chen, A. Pearce, N. L.

Tener, C. Cites D. Abrahams, S. Carter, B. Hosmer, J. Marcus,
M. Sklar, A. Templeton, T. Bricken, C. McDougall, H. Cunning-
ham, T. Henighan, A. Jermyn, A. Jones, A. Persic, Z. Qi, T. B.
Thompson, S. Zimmerman, K. Rivoire, T. Conerly, C. Olah, and
J. Batson, “On the biology of a large language model,” Transformer
Circuits Thread, 2025.

F. Jiang, C. Pan, L. Dong, K. Wang, O. A. Dobre, and M. Debbah,
“From large AI models to agentic AI: A tutorial on future
intelligent eomnnvctications CoRR, vol. abs/ 2505. 22311, a
[Online]. Available:
N. Elhage, N. Nanda, xe sson, T. 2 an,
A. Askell, Y. Bai, A. Chen, T. Coney et vi aN t hematcal
framework for transformer circuits,” Transformer Circuits Thread,
vol. 1, no. 1, p. 12, 2021.

C. Gémez-Rodriguez and P. Williams, “A confederacy of models:
A comprehensive evaluation of Ilms on creative writing,” arXiv
preprint arXiv:2310.08433, 2023.

M. DeLorenzo, V. Gohil, and J. Rajendran, “Creativeval: Evalu-
ating creativity of llm-based hardware code generation,” arXiv
preprint arXiv:2404.08806, 2024.

N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le,
G. E. Hinton, and J. Dean, “Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer,” arXiv preprint
arXiv:1701.06538, 2017.

W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling
to trillion parameter models with simple and efficient sparsity,”
arXiv preprint arXiv:2101.03961, 2021.

W. Shi, A. Ajith, M. Xia, Y. Huang, D. Liu, T. Blevins, D. Chen, and
L. Zettlemoyer, “Detecting pretraining data from large language
models,” arXiv preprint arXiv:2310.16789, 2023.

Y. Chai, Q. Liu, S. Wang, Y. Sun, Q. Peng, and H. Wu, “On training
data influence of gpt models,” arXiv preprint arXiv:2404.07840,
2024.

M.-A. Panaitescu-Liess, Z. Che, B. An, Y. Xu, P. Pathmanathan,
S. Chakraborty, S. Zhu, T. Goldstein, and F. Huang, “Can water-
marking large language models prevent copyrighted text genera-
tion and hide training data?” in Proceedings of the AAAI Conference
on Artificial Intelligence, vol. 39, no. 23, 2025, pp. 25 002-25 009.
[219] J. Wang, X. Lu, Z. Zhao, Z. Dai, C.-S. Foo, S.-K. Ng, and B. K. H.
Low, “Source attribution for large language model-generated
data,” arXiv preprint arXiv:2310.00646, 2023.

P. W. Koh and P. Liang, “Understanding black-box predictions
via influence functions,” in International conference on machine
learning. PMLR, 2017, pp. 1885-1894.

Z. Li, W. Zhao, Y. Li, and J. Sun, “Do influence functions work on
large language models?” arXiv preprint arXiv:2409.19998, 2024.
M. Christ, S. Gunn, and O. Zamir, “Undetectable watermarks
for language models,” in The Thirty Seventh Annual Conference on
Learning Theory. PMLR, 2024, pp. 1125-1139.

X. Zhao, P. Ananth, L. Li, and Y.-X. Wang, “Provable robust wa-
termarking for ai-generated text,” arXiv preprint arXiv:2306.17439,
2023.

E. Akyurek, T. Bolukbasi, F Liu, B. Xiong, I. Tenney,
J. Andreas, and K. Guu, “Towards tracing knowledge in
language models back to the training data,” in Findings
of the Association for Computational Linguistics: EMNLP 2022,
Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Abu
Dhabi, United Arab Emirates: Association for Computational
pinguistics, Dec. 2022, 2429— 2446. Dean Available:

[207]

[208]

[209]

[210]

[211]

[212]

[213]

[214]

[215]

[216]

[217]

[218]

[220]

[221]

[222]

[223]

[224]

[225] Ng, .
functions are the answer, then what is the question?” Advances in
Neural Information Processing Systems, vol. 35, pp. 17 953-17 967,
2022.


JOURNAL OF I4TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[226]

[227]

[228]

[229]

[230]

[231]

[232]

[233]

[234]

[235]

[236]

[237]

[238]

[239]

[240]

[241]

[242]

[243]

[244]

[245]

[246]

[247]

F. Ladhak, E. Durmus, and T. Hashimoto, “Contrastive er-
ror attribution for finetuned language models,” arXiv preprint
arXiv:2212.10722, 2022.

Y. Kwon, E. Wu, K. Wu, and J. Zou, “Datainf: Efficiently esti-
mating data influence in lora-tuned Ilms and diffusion models,”
arXiv preprint arXiv:2310.00902, 2023.

T. Min, H. Lee, Y. Kwon, and K. Lee, “Understanding im-
pact of human feedback via influence functions,” arXiv preprint
arXiv:2501.05790, 2025.

K. Hanawa, S. Yokoi, S. Hara, and K. Inui, “Evaluation of
similarity-based explanations,” arXiv preprint arXiv:2006.04528,
2020.

A. Ilyas, S. M. Park, L. Engstrom, G. Leclerc, and A. Madry,
“Datamodels: Predicting predictions from training data,” in
Proceedings of the 39th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, K. Chaudhuri,
S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,
Eds., vol. 162. PMLR, 17-23 Jul 2022, pp. 9525-9548. [Online].

Available:|https: / / proceedings.mlr.press/v162/ilyas22a.html
J. Yang, Deng, B. Liu, Y. Huang, J. Zou, and X. Li, “Gmvalua-

tor: Similarity-based data valuation for generative models,” arXiv
preprint arXiv:2304.10701, 2023.

T. Gao, H. Yen, J. Yu, and D. Chen, “Enabling large lan-
guage models to generate text with citations,” arXiv preprint
arXiv:2305.14627, 2023.

R. Fayyazi, M. Zuzak, and S. J. Yang, “Llm embedding-based
attribution (lea): Quantifying source contributions to genera-
tive model’s response for vulnerability analysis,” arXiv preprint
arXiv:2506.12100, 2025.

B. Muller, J. Wieting, J. H. Clark, T. Kwiatkowski, S. Ruder, L. B.
Soares, R. Aharoni, J. Herzig, and X. Wang, “Evaluating and
modeling attribution for cross-lingual question answering,” arXiv
preprint arXiv:2305.14332, 2023.

P. W. W. Koh, K.-S. Ang, H. Teo, and P. S. Liang, “On the accuracy
of influence functions for measuring group effects,” Advances in
neural information processing systems, vol. 32, 2019.

K. Guu, A. Webson, E. Pavlick, L. Dixon, I. Tenney, and
T. Bolukbasi, “Simfluence: Modeling the influence of individual
training examples by simulating training runs,” arXiv preprint
arXiv:2303.08114, 2023.

V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and
S. Feizi, “Can ai-generated text be reliably detected?” arXiv
preprint arXiv:2303.11156, 2023.

Q. Pang, S. Hu, W. Zheng, and V. Smith, “Attacking llm
watermarks by exploiting their strengths,” arXiv preprint
arXiv:2402.16187, 2024.

J. Piet, C. Sitawarin, V. Fang, N. Mu, and D. Wagner, “Mark my
words: Analyzing and evaluating language model watermarks,”
arXiv preprint arXiv:2312.00273, 2023.

T. A. Chang, D. Rajagopal, T. Bolukbasi, L. Dixon, and I. Tenney,
“Scalable influence and fact tracing for large language model
pretraining,” arXiv preprint arXiv:2410.17413, 2024.

X. Li, Y. Cao, L. Pan, Y. Ma, and A. Sun, “Towards verifiable
generation: A benchmark for knowledge-aware language model
attribution,” arXiv preprint arXiv:2310.05634, 2023.

E. Kamalloo, A. Jafari, X. Zhang, N. Thakur, and J. Lin, “Hagrid:
A human-llm collaborative dataset for generative information-
seeking with attribution,” arXiv preprint arXiv:2307.16883, 2023.
N. Hu, J. Chen, Y. Wu, G. Qi, S. Bi, T. Wu, and J. Z. Pan,
“Benchmarking large language models in complex question
answering attribution using knowledge graphs,” arXiv preprint
arXiv:2401.14640, 2024.

Y. Saxena, D. Tilwani, A. Mohammadi, E. Raff, A. Sheth,
S. Parthasarathy, and M. Gaur, “Attribution in scientific literature:
New benchmark and methods,” arXiv preprint arXiv:2405.02228,
2024.

Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu,
H. Liu, T. Xu, and E. Chen, “Crud-rag: A comprehensive chinese
benchmark for retrieval-augmented generation of large language
models,” ACM Transactions on Information Systems, 2024.

J. Zhang, Y. Bai, X. Lv, W. Gu, D. Liu, M. Zou, S. Cao,
L. Hou, Y. Dong, L. Feng et al., “Longcite: Enabling Ilms to
generate fine-grained citations in long-context qa,” arXiv preprint
arXiv:2409.02897, 2024.

O. Press, A. Hochlehnert, A. Prabhu, V. Udandarao, O. Press,
and M. Bethge, “Citeme: Can language models accurately cite
scientific claims?” arXiv preprint arXiv:2407.12861, 2024.

[248]

[249]

[250]

[251]

[252]

[253]

[254]

[255]

[256]

[257]

[258]

[259]

[260]

[261]

[262]

31

H. Deng, C. Wang, X. Li, D. Yuan, J. Zhan, T. Zhou, J. Ma, J. Gao,
and R. Xu, “Webcites: Attributed query-focused summarization
on chinese web search results with citations,” arXiv preprint
arXiv:2403.01774, 2024.

K. Zhang, W. Yu, S. Dai, and J. Xu, “Citalaw: Enhancing Ilm
with citations in legal domain,” Findings of the Association for
Computational Linguistics: ACL 2025, 2025.

D. Li, Z. Sun, X. Hu, Z. Liu, Z. Chen, B. Hu, A. Wu, and M. Zhang,
“A survey of large language models attribution,” arXiv preprint
arXiv:2311.03731, 2023.

L. Golany, F. Galgani, M. Mamo, N. Parasol, O. Vandsburger,
N. Bar, and I. Dagan, “Efficient data generation for source-
grounded information-seeking dialogs: A use case for meeting
transcripts,” arXiv preprint arXiv:2405.01121, 2024.

D. Tilwani, Y. Saxena, A. Mohammadi, E. Raff, A. Sheth,
S. Parthasarathy, and M. Gaur, “Reasons: A benchmark for
retrieval and automated citations of scientific sentences using
public and proprietary Ilms,” arXiv preprint arXiv:2405.02228,
2024.

A. Sancheti, K. Goswami, and B. V. Srinivasan, “Post-hoc an-
swer attribution for grounded and trustworthy long document
comprehension: Task, insights, and challenges,” arXiv preprint
arXiv:2406.06938, 2024.

X. Yue, B. Wang, Z. Chen, K. Zhang, Y. Su, and H. Sun, “Auto-
matic evaluation of attribution by large language models,” arXiv
preprint arXiv:2305.06311, 2023.

O. Honovich, R. Aharoni, J. Herzig, H. Taitelbaum, D. Kukliansy,
V. Cohen, T. Scialom, I. Szpektor, A. Hassidim, and Y. Ma-
tias, “True: Re-evaluating factual consistency evaluation,” arXiv
preprint arXiv:2204.04991, 2022.

J. Ni, C. Qu, J. Lu, Z. Dai, G. H. Abrego, J. Ma, V. Y. Zhao,
Y. Luan, K. B. Hall, M.-W. Chang et al., “Large dual encoders are
generalizable retrievers,” arXiv preprint arXiv:2112.07899, 2021.
L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng, “Text match-
ing as image recognition,” in Proceedings of the AAAI conference on
artificial intelligence, vol. 30, no. 1, 2016.

J. Guo, Y. Fan, X. Ji, and X. Cheng, “Matchzoo: A learning,
practicing, and developing system for neural text matching,”
in Proceedings of the 42Nd international ACM SIGIR conference on
research and development in information retrieval, 2019, pp. 1297—
1300.

R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,
and C. Finn, “Direct preference optimization: Your language
model is secretly a reward model,” Advances in Neural Information
Processing Systems, vol. 36, 2024.

Y. Li, X. Yue, Z. Liao, and H. Sun, “Attributionbench:
How hard is automatic attribution evaluation?” arXiv preprint
arXiv:2402.15089, 2024.

S. Dai, Z. Cao, W. Wang, L. Pang, J. Xu, S.-K. Ng, and T.-S. Chua,
“Media source matters more than content: Unveiling political
bias in Ilm-generated citations,” EMNLP, 2025.

A. Abolghasemi, L. Azzopardi, S. H. Hashemi, M. de Rijke, and
S. Verberne, “Evaluation of attribution bias in generator-aware
retrieval-augmented large language models,” in Findings of the
Association for Computational Linguistics: ACL 2025, 2025.
