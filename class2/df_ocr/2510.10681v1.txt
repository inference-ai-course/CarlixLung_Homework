arXiv:2510.10681v1 [cs.CL] 12 Oct 2025

Preprint. Under review.

REPRO: TRAINING LANGUAGE MODELS TO FAITH-
FULLY RECYCLE THE WEB FOR PRETRAINING

Zichun Yu, Chenyan Xiong

Language Technologies Institute, Carnegie Mellon University
{zichunyu, cx}@andrew.cmu.edu

ABSTRACT

High-quality pretraining data is the fossil fuel of large language models (LLMs),
yet its reserves are running low for frontier models. In this paper, we introduce
REPRO, a novel web recycling method that trains a relatively small LM with re-
inforcement learning to generate effective and faithful rephrasings of pretraining
data. Specifically, we design one quality reward and three faithfulness rewards,
optimizing the LM rephraser to convert organic data into high-quality rephras-
ings while maintaining its core semantics and structure. In our experiment, we
train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb.
Pretraining results on 400M and 1.4B models demonstrate that REPRO delivers
4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream
tasks. REPRO also outperforms ReWire, the state-of-the-art web recycling method
that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data
pool. Experiments with different amounts of recycled data highlight that REPRO
improves organic data efficiency by 2-3. Individual and distributional analyses
validate that REPRO preserves more critical information and faithfully reflects the
characteristics of organic data compared to prompting-based methods. Together,
these results show that REPRO provides an efficient and controllable path to ef-
fectively harness the “fossil fuel” of LLM pretraining. We open-source our code,

rephraser, and recycled data at https: //github.com/cxcscmu/RePro.

1 INTRODUCTION

The continued scaling of large lan- 0-300 Ls = 4 +40% Beret (4B)
guage models (LLMs) is threatened 5 a ae
. oe . . 9

by the diminishing supply of high- “A295 1s S70 (7B)

quality pretraining data (Villalobos & ws 0.291 J 50

et al., 2024; Maini et al., 2025). = 0.290 “A, ReWire (70B) o= ,

While the web provides vast amounts 4 0.286 % 30 —

of content, referred to as organic data, R 0.285 Maes iG “es

standard data pipelines often filter out WRAP (7B) -"
72B 144B I88B Support Omit Contradict

the majority of it as “low-quality”
to ensure pretraining quality (Weber
et al., 2024; Li et al., 2024). The
shortage of high-quality data thus
leads to a looming “data wall” that
impedes further progress in LLM pre-
training (Nguyen et al., 2025).

To address the scarcity of high-

Organic Data Pool

(a) Downstream performance

Relationship to Organic

(b) Key point recall

Figure 1: (a) Pretraining performance of 1.4B model and
(b) ratio of key points in organic data that are supported /
omitted / contradicted in the recycled version across three
web recycling methods (rephraser size in parentheses).

quality data, one promising path is to “recycle” the low-quality web data by rephrasing, thereby
increasing the amount of usable pretraining data (Nguyen et al., 2025). Previous approaches by
prompting LLMs (e.g., Llama-3.3-70B-Instruct (Dubey et al., 2024)) have shown promising results,
matching or even surpassing the performance of doubling organic data (Maini et al., 2024; Nguyen
et al., 2025). However, these methods face two major limitations. First, the computational cost of
rephrasing with large models is prohibitively high; second, prompts alone may not faithfully pre-
serve the semantics and structure of organic data (Bi et al., 2025), which is critical for the reliability


Preprint. Under review.

and richness of the pretraining corpus. These challenges underscore the need for a cost-efficient and
faithful web recycling method for LLM pretraining.

In this paper, we introduce REPRO, an efficient and faithful web recycling method to effectively
harness the “fossil fuel” of LLM pretraining. Our approach leverages reinforcement learning (RL)
to train a relatively small rephraser using two categories of rewards: quality and faithfulness. Specif-
ically, we choose DataMan score (Peng et al., 2025), the state-of-the-art quality assessment metric,
as the quality reward to incentivize high-quality rephrasings. For faithfulness rewards, we employ
BERTScore (Zhang et al., 2020; Zhu & Hauff, 2021), structure preservation, and length alignment
to maintain the semantic meaning, structural diversity, and length distribution of organic data. To-
gether, these reward functions guide our rephraser to produce high-quality pretraining data while
faithfully preserving the characteristics of organic data.

We train our rephraser from Qwen3-4B (Yang et al., 2025), and employ it to recycle a 72B organic
data pool sampled from DCLM-RefinedWeb (Li et al., 2024; Nguyen et al., 2025). We pretrain
400M and 1.4B models on the high-quality portion of recycled data. Evaluation results on 22 down-
stream tasks, covering reasoning, understanding, and knowledge, demonstrate that REPRO achieves
+4.7% to +14.0% average accuracy (DCLM Core score) gains over the organic baseline. As shown
in Figure 1a, REPRO also outperforms (1) the state-of-the-art web recycling method ReWire, which
prompts Llama-3.3-70B-Instruct with chain-of-thought reasoning, and (2) organic baseline when
enlarging the data pool by 4x. We also increase the amount of recycled data in pretraining and find
that the optimal performance of REPRO is achieved when recycling twice the amount of high-quality
organic data. This means that REPRO can boost organic data efficiency by 2-3 x—the proportion of
high-quality data in the final pretraining set compared to high-quality organic data.

Further ablation studies show that our reward functions effectively optimize the rephraser to gen-
erate high-quality data while maintaining the semantic meaning and structural diversity of organic
data. As a representative study, we measure how many key points in one organic data are sup-
ported, omitted, or contradicted in its recycled version using the method from Qi et al. (2024). As
shown in Figure 1b, REPRO achieves the highest number of 95% supported key points while signifi-
cantly reducing the ratio of omitted key points by up to 92% compared to prompting-based methods
WRAP (Maini et al., 2024) and ReWire. In-depth analysis of rephrasing operations shows that RE-
PRO flexibly applies diverse operations such as paraphrasing, removing, and clarifying to enhance
data quality. These results highlight that REPRO recycles web data in a faithful manner, effectively
alleviating the data scarcity issue in LLM pretraining.

We summarize the main contributions of our work as follows:

1. We propose REPRO, a novel web recycling method for better organic data efficiency by
training a relatively small LM with RL to perform effective and faithful rephrasing.

2. We design one quality and three faithfulness rewards to optimize our rephraser to generate
high-quality data while maintaining the semantics, structure, and length of organic data.

3. REPRO outperforms the state-of-the-art recycling method (ReWire) despite using a 17x
smaller rephraser and boosts the organic data efficiency by 2-3. Analyses confirm that
REPRO faithfully preserves the essential semantics and structure of organic data.

2 RELATED WORK

The scaling of large language models (LLMs) has expanded along three primary dimensions: model
size, compute budget, and pretraining data volume (Kaplan et al., 2020; Hoffmann et al., 2022).
In the early stages of LLM development, the main bottleneck was compute, which has since been
largely alleviated by advances in hardware efficiency, novel model architectures (Shazeer et al.,
2017; Liu et al., 2024), and the selective use of high-quality data (Engstrom et al., 2024; Wettig et al.,
2024; Yu et al., 2024). Recent forecasts indicate that another critical constraint lies in the quantity of
pretraining data, as the supply of organic (human-generated) text available on the internet is expected
to be rapidly exhausted (Villalobos et al., 2024). Meanwhile, standard data curation pipelines often
discard a large portion of the collected web data to ensure the quality of pretraining (Weber et al.,
2024; Li et al., 2024). These practices highlight the scarcity of high-quality data, which has become
anew bottleneck for further scaling (Muennighoff et al., 2023).


Preprint. Under review.

Rephraser Training Web Recycling

Organic Data Pool Trained

Rephraser
a +-0

-
~e

» Quality Filtering ¥

Recycled Data Pool
-f®\

Organic Data Recycled Data

Rephraser

IN
lis’
I

a a

t ~
if Faithfulness Rewards if Quality Reward t

1
1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

‘| Semantics, Structure, Length 11 DataMan yt
N aN j

1

Figure 2: Overview of REPRO. We train our rephraser with quality and faithfulness rewards to
optimize it to faithfully recycle the web.

As high-quality organic data becomes increasingly scarce, synthetic data emerges as a principled
complement due to its flexibility and scalability (Havrilla et al., 2024). Recent work highlights the
effectiveness of synthetic data in various training stages, spanning from pretraining (Maini et al.,
2024), mid-training (Wang et al., 2025), to post-training (Li et al., 2025). In mid- and post-training,
synthetic data generation typically leverages a set of high-quality seed data and employs LLMs to
extract its essence or produce similar examples (Wang et al., 2023), which has been widely adopted
to to incentivize math (Ge et al., 2024; Zhou et al., 2025b), reasoning (Wang et al., 2025), and
instruction-following (Yue et al., 2024; Li et al., 2025) abilities. In contrast, synthetic data gener-
ation for pretraining is more challenging and less understood, confronting issues such as lack of
diversity (Havrilla et al., 2024). Successful attempts, including Wikipedia-style rephrasing (Maini
et al., 2024), guided rewriting (Nguyen et al., 2025), and topic-seeded generation (Li et al., 2023;
Ben Allal et al., 2024; Hao et al., 2025), can enhance pretraining mixtures in both data quantity and
quality (Su et al., 2025; Abdin et al., 2024; Maini et al., 2025).

Though promising, theory and practice caution that the indiscriminate use of synthetic data can trig-
ger model collapse (Shumailov et al., 2024; Havrilla et al., 2024; Dohmatob et al., 2025), degrading
generalization capabilities (Gerstgrasser et al., 2024; Feng et al., 2025). Specifically, Shumailov
et al. (2024) show rapid degeneration when successive models are trained on their predecessors’ out-
puts, with scaling-law changes explaining this as the tail-knowledge loss in synthetic data (Dohma-
tob et al., 2024). The distribution collapse issue is particularly risky in pretraining, as it not only
degrades pretraining performance but harms more on the post-training outcomes (Chen et al., 2024).
To prevent the potential collapse in pretraining, ProX (Zhou et al., 2025a) and RefineX (Bi et al.,
2025) restrict the data synthesis output to conservative programs, refining pretraining data with a
predefined set of operations, e.g., deletion and normalization.

3. METHODS

In this section, we present REPRO, an effective and faithful web recycling method for pretraining.
We first introduce the web recycling setup ($3.1) and then our reinforcement learning approach to
optimize an LM rephraser to faithfully recycle the web (§3.2). Our pipeline is illustrated in Figure 2.

3.1 RECYCLING PRETRAINING DATA

The construction of pretraining data starts from an organic data pool Dog, obtained from web
sources. As incorporating low-quality data into pretraining can significantly degrade model per-
formance (Li et al., 2024), only high-quality samples are retained from D,,g. Formally, we define a
quality function Q(-) with a threshold Tog to select the high-quality subset:

Dore-hq = {x € Dorg | Q(z) 2 Tore } (1)

In data-limited scenarios (e.g., training frontier LLMs (Maini et al., 2025)), the total number of
tokens in Dorg-ng, denoted by Borg-hg, is insufficient to cover the unique pretraining token budget B.
To address this shortage, we introduce a language model rephraser R, which transforms an organic
data sample «x into a recycled data sample x’ given a rephrasing prompt p. All rephrasings then form


Preprint. Under review.

the recycled data pool Dec:
Dree = {x' = R(p, x) | x € Dorg}- (2)
As with organic data, recycled samples are filtered by Q(-) to select the highest-quality portion:
Dree-hq = {© € Drec | Q(X) = Tree}, (3)
where Tyee is set so that the total number of tokens in Dyec-hg 18 B — Borg-hq-
The final pretraining dataset is then constructed from both organic and recycled high-quality subsets:

Dyinal = Dorg-hq U Drec-hq: (4)
3.2 TRAINING A FAITHFUL REPHRASER WITH RL

Table 1: Reward functions used in our RL for training an effective and faithful LM rephraser.

Reward Type Functionality Formal Definition

DataMan Quality Generate higher-quality data DataMan(recycled) - DataMan(organic)
BERTScore Faithfulness Keep semantics consistent BERTScore(organic, recycled) > TgERrscore
Structure Faithfulness Keep text structure consistent Structure(organic, recycled) == 1

Length Faithfulness Penalize free-form generation Len(recycled) < Tiengtn * Len(organic)

To avoid model collapse when using synthetic data in pretraining (Chen et al., 2024), we leverage
reinforcement learning (RL) to train a faithful rephraser for web recycling. Specifically, we design
two types of rewards, quality and faithfulness, to guide RL optimization. The quality reward en-
courages the rephraser to produce outputs that are of higher quality than the organic data, while
faithfulness rewards ensure that the rephrased data faithfully preserves the core semantic, structure,
and length of the original text. The specific reward functions are as follows:

* Data Quality (DataMan): To encourage the generation of high-quality data, this reward
incentivizes the rephraser to produce outputs x’ of better quality than the organic data wx.
Specifically, we choose DataMan (Peng et al., 2025), the state-of-the-art quality assessment
method that prompts an LM to evaluate pretraining data across 13 criteria (e.g., coherence,
topic focus, knowledge novelty) and assign an overall score.

TDataMan(2, x’) = DataMan(a’) — DataMan(z:) (5)
* Semantic Faithfulness (BERTScore): To ensure that the rephrased text preserves the

core meaning of the organic data, we reward semantic similarity between x and x’ when
their BERTScore (Zhang et al., 2020) is higher than a preset threshold TgErrtscore-

T'BERTScore(, x’) = 1[BERTScore(, x’) Pa TBERTScore | (6)

* Structural Faithfulness (Structure): To encourage the rephraser to maintain the high-
level textual structure (e.g., Markdown) of the organic data and prevent the loss of the
structural diversity, we apply a structure comparison LM to assess structural faithfulness
by in-context learning.

Tstracture(@, 2’) = 1[Structure(x, x’) = 1] (7)

* Length Faithfulness (Length): To further penalize uncontrolled or free-form generation
that deviates significantly from the organic data, we impose a constraint on the length of
the recycled text relative to the original by a factor of Ty engin.

TLength(@, v") = 1[Len(z’) < TLengtn * Len(x)] (8)
The final reward function r(, x’) combines these individual rewards with weighting coefficients .:

/
r(a, x ) = ADataManT DataMan + ABERTScore BERTScore + structure? Structure + ALengthT Length: (9)

We employ Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024) for RL train-
ing, starting from the base model Rpase with organic data x sampled from Dgypo C Dorg. GRPO


Preprint. Under review.

enhances training stability by normalizing advantage estimates from a group of n generated outputs
{x},...,2/,}. The advantage of x’, is used to update the policy via a clipped surrogate objective:

Ret = arg Max Een -Dpo, [min (PA, clip(P,1—¢,1 +6) A;) ~ Dx (R||Rbase)| , (10)
LTR
Tr(a |p,t) 4 _ r(x, x) — mean(r)
where P = TR. | pay = std(n) sandr = {r(z,2x}),...,r(z,z),)}. AUD
mpe(x’ | p,x) denotes the probability of the rephraser model generating x’ from x. The final opti-
mized model R,, generates the recycled data pool D,.,- from the organic data pool Do:., which then
constitutes the pretraining dataset following the steps in Section 3.1.

4 EXPERIMENTAL SETUP

Pretraining Data and Model. We conduct our main experiments on the DCLM dataset (Li et al.,
2024). Specifically, we follow ReWire (Nguyen et al., 2025) and randomly sample 72B tokens
from DCLM-RefinedWeb as our organic data pool Dog. DCLM-RefinedWeb applies rule-based
filtering and global deduplication to Common Crawl (Penedo et al., 2023), but not model-based
filtering, making it a moderate-quality data source well suited for recycling. The unique token
budget B is 14.4B in our main experiment, and we explore different budgets in later analyses. The
quality function Q is DCLM-fastText, the best-performing classifier in data selection from DCLM-
RefinedWeb. Following the quality filtering threshold 7,,.=0.018112 in DCLM, the amount of high-
quality organic data Borg-hq is 7.2B. For our pretraining models, we use a decoder-only architecture
and adopt two commonly used parameter scales (400M and 1.4B) from previous works (Yu et al.,
2025; Nguyen et al., 2025), and train all models from scratch.

Evaluation. We evaluate pretrained models on 22 downstream tasks in either zero-shot or few-shot
manners. These tasks provide a comprehensive assessment of essential model abilities, including
commonsense reasoning, language understanding, reading comprehension, symbolic problem solv-
ing, and world knowledge. We use centered accuracy as the primary evaluation metric, where accu-
racy per task is mapped to 0 for random guessing and | for perfect accuracy. The average centered
accuracy across all tasks is denoted as “Core score”. Following Yu et al. (2025), we exclude Com-
monsenseQA from the original DCLM-Core due to its high instability and limited informativeness.
Full task details and their few-shot numbers are provided in Appendix B.1.

Baselines. We compare REPRO with (1) organic data only; (2) WRAP (Maini et al., 2024):
Wikipedia-style rephrasing using Mistral-7B-Instruct-vO.1 (Jiang et al., 2023); (3) ProX (Zhou
et al., 2025a): fine-grained edits of organic data via model-generated programs; and (4)
ReWire (Nguyen et al., 2025): guided rewriting with chain-of-thought reasoning using Llama-3.3-
70B-Instruct (Dubey et al., 2024). As ReWire’s code has not been open-sourced, we randomly
sample 7.2B tokens from its released data. This may give ReWire a performance advantage, since it
essentially utilizes a larger pool (our organic + their organic). These baselines cover state-of-the-art
web recycling techniques such as prompting, program-based editing, and chain-of-thought reason-
ing. Some concurrent works, such as RefineX (Bi et al., 2025) and Beyond Web (Maini et al., 2025),
have not open-sourced their data and code, so we leave the comparison with them in the future.
More baseline details can be found in Appendix B.2.

Implementation Details. We initialize our rephraser model R by Qwen3-4B (Yang et al., 2025).
In RL, we set the cutting threshold Tggrrscore a8 0.65, and set Trengin aS 1.25. We utilize released
DataMan (1.5B) and BERTScore (350M) models for their reward calculation and construct our
structure comparison model by prompting Qwen3-4B with few-shot examples. The coefficients
ADataMan> ABERTScore> AStructures ANd Apengtn are set to 3, 1, 1, 1, respectively. The size of the RL dataset
Derpo 18 41k. Derpo Consists solely of organic data with a DataMan score below 5, since 5 is the
maximum score and thus cannot be further improved. The clipping € in GRPO is 0.2, @ is 0.005,
and the number of rollouts n per input is 8. To examine whether a strong prior can improve RL, we
also add an optional supervised fine-tuning (SFT) stage before RL, where we use GPT-40 (Achiam
et al., 2023) as the teacher to generate 50k example rephrasings to warm up our rephraser. Unless
otherwise stated, REPRO denotes RL w/o SFT. In the inference stage, we adopt v1 1m (Kwon et al.,
2023) framework for efficient text generation using our rephraser. We conduct a key hyperparameter
analysis in Appendix A.3 and provide all prompts used in our experiments in Appendix B.5.


Preprint. Under review.

Table 2: Benchmarking different web recycling methods on DCLM with 400M and 1.4B pretraining
models. Bold and underline indicate the best and second-best results.

Commonsense Language Reading Symbolic World
Reasoning Understanding Comprehension Problem Knowledge Core

Method | Pool Unique Data (3 tasks) (6 tasks) (3 tasks) (5 tasks) (5 tasks) (22 tasks)
400M Setting: 400M model, 28.8B training tokens
Organic | 72B 7.2B 0.23613 0.27079 0.03724 0.14535 0.20126 ~—-0.18990
Organic | 72B 14.4B 0.26953 0.25781 0.05623 0.14991 0.17683 ~—- 0.18899
WRAP |72B 7.2B+7.2B 0.24784 0.25798 0.06269 0.16303 0.20067 ~=—0.19536
ProX 72B 7.2B+7.2B 0.24252 0.25403 0.06884 0.16528 0.20647 0.19623
ReWire | 72B 7.2B + 7.2B 0.24051 0.26453 0.06232 0.17392 0.21246 0.20125
REPRO |72B 7.2B +7.2B 0.28454 0.27792 0.07181 0.19409 0.21154 0.21658
1B Setting: 1.4B model, 28.8B training tokens
Organic | 72B 7.2B 0.32348 0.38371 0.19584 0.19795 0.28745 =: 0.28578
Organic | 72B 14.4B 0.31483 0.37916 0.15112 0.17507 ~—-:0.28311 ~——- 0.27108
WRAP |72B 7.2B+7.2B 0.33860 0.36873 0.20071 0.20492 =0.27576 ~=—- 0.28335
ProX 72B 7.2B+7.2B 0.34246 0.38253 0.21735 0.18487 0.29641 0.29004
ReWire | 72B 7.2B + 7.2B 0.33330 0.37400 0.23369 0.17641 0.31186 0.29029
REPRO |72B 7.2B+7.2B 0.36776 0.38519 0.20832 0.20597 = 0.30304 =: 0.29929

5 EVALUATION RESULTS

In this section, we present our main results on DCLM, along with different unique token budgets
($5.1). Then, we perform ablation studies (§5.2), dive into the RL training dynamics with different
reward choices (§5.3), and analyze reward-related feature distributions of recycled data (§5.4). We
conclude with an in-depth analysis of the specific operations performed by our rephraser ($5.5).
Additional results and ablations can be found in Appendix A.

5.1 MAIN RESULTS

Overall Performance. Table 2 demonstrates the overall performance of different web recycling
methods on DCLM-RefinedWeb. First, using 7.2B high-quality organic data outperforms using
14.4B, confirming the necessity of quality filtering. Our method, REPRO, significantly outperforms
using organic data only, achieving 4.7%-14.0% relative improvements in Core scores. This indi-
cates that our recycled data can effectively complement organic data in the data-limited setup to
enhance the pretraining performance of LLMs. Our method also consistently outperforms all base-
lines across different pretraining model scales, including the one (ReWire) that prompts a much
larger model (Llama-3.3-70B-Instruct) to rephrase the corpus. The actual efficiency advantage of
REPRO is provided in Appendix A.1, where it achieves a 36.7 speedup compared to ReWire.
In summary, these results highlight that even a relatively small (4B) model can learn without the
guidance of an LLM to be an effective rephraser to generate high-quality recycled data.

Improving Organic Data Efficiency. We also evaluate the 9 | oat)
effectiveness of REPRO under different unique token bud- > 14.4B -

gets B. Specifically, we vary B from 14.4B (our main ex- a oe
periment) to 21.6B and 28.8B (our total training budget) in | 0.280

the 1B setup. As shown in Figure 3, REPRO consistently 2 21.6B 9.297
outperforms the organic-only baseline across different B by rs :

a large margin. Among these budgets, the best performance a 0.267 Organic

of REPRO is achieved at B=14.4B, while B=21.6B yieldsa _5+28.8B 0.283] RePro
similar performance. This implies that REPRO can poten- ,&

tially improve organic data efficiency by 2-3x. However, = 0.24 0.26 0.28 0.30
further increasing B to 28.8B leads to a performance drop, Core Score

likely due to the inclusion of more moderate-to-low quality Figure 3: Performance of REPRO
data. Overall, these results highlight the effectiveness of our wrt, different unique token budgets.
method in increasing the amount of high-quality pretraining data.


Preprint. Under review.

Table 3: Ablation studies on different components of REPRO in the 400M setup.

Commonsense Language Reading Symbolic World
Reasoning Understanding Comprehension Problem Knowledge Core
Method Pool Unique Data (3 tasks) (6 tasks) (3 tasks) (5 tasks) (5 tasks) (22 tasks)
400M Setting: 400M model, 28.8B training tokens
Organic 72B 14.4B 0.26953 0.25781 0.05623 0.14991 0.17683 0.18899
Prompting 72B 7.2B+7.2B 0.24310 0.26758 0.05075 0.17392 0.20196 0.19847
SFT 72B 7.2B+7.2B 0.24447 0.24920 0.04013 0.16564 0.21009 0.19216
RL w/o Faithfulness | 72B 7.2B + 7.2B 0.22357 0.25633 0.06822 0.16961 0.20376 0.19456
SFT + Full RL 72B 7.2B+7.2B 0.29210 0.25965 0.07141 0.16988 0.21632 0.20816
REPRO (Full RL) 72B 7.2B+7.2B 0.28454 0.27792 0.07181 0.19409 0.21154 0.21658
1.0 1.00

wey 1:05 z z - 1.0
g Es Ss —RL w/ Faithfulness iy —RL w/ Faithfulness
® 1.00 oo Z 0.75 =RL w/o Faithfulness Zz 0.8 RL w/o Faithfulness|
a RL w/ Faithfulness 2 0.5 —RL w/ Faithfulness S ~
= 0.75 —RL w/o Faithfulness go —RL w/o Faithfulness} EI 0.50 S 0.6

n £ 3)
3 i= gs Ss
3 a4 0.25 8
A 0.50 a a 404

0.0
0 400 800 1200 0 400 800 1200 0 400 800 1200 0 400 800 1200
Training Steps Training Steps Training Steps Training Steps
(a) DataMan (b) BERTScore (c) Structure (d) Length

Figure 4: Validation curves of (a) DataMan, (b) BERTScore, (c) structure, and (d) length rewards
during our RL training with and without faithfulness rewards.

5.2 ABLATION STUDIES

We conduct ablation studies in the 400M setup to investigate the effectiveness of each component in
our rephraser training. Comparisons include our base rephraser (Qwen3-4B) with direct prompting,
the rephraser after SFT using GPT-40 rephrasings, and the rephraser from RL training without faith-
fulness rewards. Note that we do not include the model trained without the quality reward, as in this
case, the rephraser would simply learn to copy the organic data to maximize faithfulness rewards.
As compared in Table 3, straightforward prompting or SFT alone could not greatly benefit overall
performance compared to the organic baseline. This implies a significant performance gap between
prompting-based rephrasers (even as strong as GPT-40) and our training-based rephraser, which has
been specifically optimized for generating better recycled data.

With SFT prior, RL may achieve further gains on world knowledge tasks but falls behind in the over-
all Core score. We hypothesize that the SFT data generated by GPT-40 may contain distillation-style
rephrasings that benefit knowledge-intensive tasks but hinder generalization to others, a common is-
sue noted in previous works (Su et al., 2025; Maini et al., 2025). This comparison highlights that
REPRO does not rely on supervised signals from an external LLM to perform effective recycling.
Removing faithfulness rewards from RL leads to a substantial performance drop in Core score, in-
dicating that the faithfulness of recycled data is also crucial to the effectiveness of web recycling.

5.3. EFFECTIVENESS OF REWARD FUNCTIONS

This set of experiments studies the effect of reward functions. In Figure 4, we plot the training dy-
namics of our RL training with and without faithfulness rewards. The rewards shown are calculated
on a validation set of 128 randomly sampled data from Dgrpo, using mean value as the aggregation
function. We observe that with all rewards enabled, DataMan, BERTScore, and structure rewards
all steadily improve during training (Figures 4a, 4b, and 4c), while the length reward remains stably
high (Figure 4d). This indicates that these four rewards can be effectively optimized together (in-
stead of trade-offs) to improve the capability of our rephraser to generate high-quality and consistent
recycled data. In contrast, without faithfulness rewards as constraints (by setting their coefficients
to zero), DataMan reward quickly converges to a high value (Figure 4a), but all other three rewards
drop significantly (Figures 4b, 4c, and 4d).

We further explore the impact of using other quality rewards in our RL training. Specifically, we
investigate two additional quality reward options: (1) DCLM-fastText, a n-gram-based classifier that


Preprint. Under review.

~—

60/ _/ Organic ry i Prompting _/ Organic ia) _ Organic
Prompting g RL w/o BERTScore 754 _ Prompting ‘3g _] Prompting
S40 RL la P) RL =< RL w/o Structure A RL w/o Length
~—
3 2 31 oR 2 “
ce 20 8 Fy 25 4
Oo ro
| l 21 eg
0 a. Ps oom © ——
1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 oo wes? 4k 8k 12k
DataMan Score BERTScore ys ee x0 fe) Character Length
(a) DataMan (b) BERTScore (c). Structure (d) Length

Figure 6: Reward-related feature distributions of 30,000 recycled data generated by different
rephrasers. Features are (a) DataMan score, (b) BERTScore, (c) structure type, and (d) text length.

performs the best in data selection from DCLM-RefinedWeb, and (2) training data influence (Yu
et al., 2024), which measures the actual training effect of each data point on the reference task given
a pretrained model checkpoint. Following Yu et al. (2025), we use our 400M organic baseline at 10k
steps as the model checkpoint and adopt FLAN (Wei et al., 2022) as the reference task.

As illustrated in Figure 5, the fastText reward quickly saturates at 1.0

0.05

a high value, while the influence reward remains low and drops £9 004 8
sharply to 0 (no rephrasing) at the end. A case study in AppendixC 2 — —fastText 5
shows that the rephraser trained with fastText reward tends to gen- & ne ~ Influence saad
erate text with an academic tone, which hacks the n-gram-based S04 0.02 &
classifier to get high scores. In contrast, the rephraser trained with % > 0.01e
the influence reward struggles to explore rephrasing strategies that ~ =
consistently improve the reward, likely because this signal is too 0 100 200 300 400
fine-grained and difficult to optimize upon. Our findings suggest Training Steps

that a useful metric for data selection may not be an appropriate Figure 5: RL training with
reward for rephraser RL. A good quality reward should not only DCLM-fastText or data influ-
correlate well with the final pretraining performance, but also be ence as quality rewards.
robust to reward hacking and can be effectively optimized.

5.4 DISTRIBUTIONAL ANALYSES

This set of experiments examines the effectiveness of each reward function used in our RL train-
ing from a distributional perspective. To analyze each reward function, we plot the distributions of
relevant features for organic data alongside data generated by different rephrasers. Comparisons in-
clude our base rephraser (Qwen3-4B) with direct prompting, the rephraser trained without a specific
reward, and our final rephraser trained with all rewards. All features are calculated on a set of 30k
randomly sampled instances from our data pool.

First, we compare the distributions of the DataMan score in Figure 6a. REPRO effectively shifts the
distribution towards higher DataMan scores, boosting the proportion of data with a Score of 5 from
20% to 60%. In contrast, direct prompting yields a marginal improvement over organic data, with
only 25% of recycled data achieving a Score of 5.

With faithfulness rewards, REPRO successfully preserves the original characteristics at the distribu-
tion level. As shown in Figure 6b, REPRO maintains semantic similarity with an average BERTScore
of 0.75, compared to 0.69 for direct prompting and 0.56 for RL without the BERTScore reward, un-
derscoring the necessity of this reward for semantic preservation.

We further assess structural diversity by prompting Gemini 2.5 Flash-Lite to classify the text struc-
ture as plain text, Markdown, blog/forum, or others. As shown in Figure 6c, direct prompting tends
to transform Markdown-style text into plain text, whereas RL without the structure reward over-
generates Markdown-style text, harming structural diversity. The structural distribution of our final
recycled data aligns most closely with that of organic data.

Similarly, for text length (Figure 6d), we observe that direct prompting tends to produce shorter,
summary-like text, while RL without the length reward generates much longer text that may in-


Preprint. Under review.

[a
Organic: Nutch 1.6 (CC)/CC Organic: Deaths were 43,000 British, 15,000 N
WarcExport 1.0 isPartOf: CC-MAIN- French, 8,700 Australians, 2,700 New

2015-40 operator: CommonCrawl -~” Zealanders and 1,370 Indians

Admin description: Wide crawl... “ie Recycled: The campaign resulted in substantial
Recycled: casualties, including 43,000 British, 15,000
French, 8,700 Australian, 2,700 New Zealand,
and 1,370 Indian soldiers.

eee ee ie Organic: The Allies withdrew from the campaign

which is celebrated on ANZAC Day

Recycled: Despite initial advances, the Allies
ultimately withdrew from the campaign. The event
is commemorated annually on ANZAC Day.

Dardanelles Campaign

Recycled: During World War I,
British and colonial forces launched
an attack on the region in 1915 as part
of the Dardanelles Campaign.

Figure 7: Operations performed by REPRO and their corresponding cases.

troduce extraneous information (see Appendix C for more details). Once again, our recycled data
aligns most closely with the length distribution of the organic data.

In summary, these results confirm the effectiveness of each reward function in our RL framework.
They coordinate together to simultaneously improve data quality and shape the overall distribution
of recycled data to mirror that of organic data, enabling an effective and faithful recycling process.

5.5 REPHRASING OPERATIONS

Finally, we analyze the specific operations performed by our rephraser to better understand its be-
haviors. We randomly sample 100 instances from our recycled data pool and again prompt Gemini
2.5 Flash-Lite to classify the operations (verb + noun) performed by our rephraser. We categorize
the extracted operations into 5 primary types: paraphrasing, removing, clarification, reorganization,
and summarization. The distributions and examples of each operation are illustrated in Figure 7. We
observe that paraphrasing is the most popular operation, which can improve writing quality by fixing
grammatical errors and enhancing fluency. Removing is the second common one, which eliminates
irrelevant content such as advertisements and metadata. Other operations like clarification, reorga-
nization, and summarization are also frequently used to enhance the informativeness and coherence
of the text. Overall, these results demonstrate that our rephraser learns to flexibly apply a variety
of operations (instead of a manually defined set) to effectively recycle web data into high-quality
training data while respecting the original content.

6 CONCLUSION

In this paper, we introduce REPRO, a novel web recycling method that trains a 4B language model
to generate high-quality and faithful rephrasings of web data. Pretraining results show that RE-
PRO achieves 4.7%- 14.0% relative accuracy improvements over organic-only baselines, outperforms
state-of-the-art recycling method ReWire, which prompts a 70B model, and even surpasses an or-
ganic baseline whose data pool is enlarged by 4x. Distributional analyses confirm that REPRO
maintains the semantic meaning, structural diversity, and length balance of organic data.

In conclusion, our research provides two key takeaways for effective web recycling. First, rephras-
ing does not necessarily require a large language model; small models trained with carefully de-
signed rewards can recycle the web more effectively and faithfully. Second, preserving the essential
characteristics of organic data not only helps maintain the original distribution but also enhances
pretraining performance with recycled data. Future work can explore more diverse and verifiable
reward signals, such as a checklist, to further improve the quality and faithfulness of recycled data.
We hope that our work inspires more cost-efficient and reliable approaches to alleviate the data wall
and enable sustainable scaling of LLM pretraining.


Preprint. Under review.

ACKNOWLEDGMENTS

We thank CMU Foundation and Language Model (FLAME) Center and Amazon Web Services
(AWS) for providing support of computational resources. We thank Baolong Bi for idea discussions
and Young Jin Ahn, Hao Kang, and Gabriel Moreira for their valuable feedback on this work.

REFERENCES

Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,
Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 techni-
cal report. ArXiv preprint, 2024.

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical
report. ArXiv preprint, 2023.

Loubna Ben Allal, Anton Lozhkov, and Daniel van Strien. Cosmopedia: How to create large-scale
synthetic data for pre-training. Hugging Face Blog, 2024.

Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei,
Junfeng Fang, Jiafeng Guo, and Xueqi Cheng. RefineX: Learning to refine pre-training data at
scale from expert-guided programs. ArXiv preprint, 2025.

Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning
about physical commonsense in natural language. In Proc. of AAAI, 2020.

Hao Chen, Abdul Waheed, Xiang Li, Yidong Wang, Jindong Wang, Bhiksha Raj, and Marah I
Abdin. On the diversity of synthetic data and its impact on training large language models. ArXiv
preprint, 2024.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proc. of
NAACL-HLT, 2019.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? Try ARC, the ai2 reasoning chal-
lenge. ArXiv preprint, 2018.

Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. A tale of tails: Model
collapse as a change of scaling laws. In Proc. of ICML, 2024.

Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, and Julia Kempe. Strong model collapse. In
Proc. of ICLR, 2025.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
ArXiv preprint, 2024.

Logan Engstrom, Axel Feldmann, and Aleksander Madry. DsDm: Model-aware dataset selection
with datamodels. In Proc. of ICML, 2024.

Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, and Julia Kempe. Beyond model
collapse: Scaling up with synthesized data requires verification. In Proc. of ICLR, 2025.

Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data
creation with 1,000,000,000 personas. ArXiv preprint, 2024.

Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Tomasz Korbak, Henry
Sleight, Rajashree Agrawal, John Hughes, Dhruv Bhandarkar Pai, Andrey Gromov, Dan Roberts,
Diyi Yang, David L. Donoho, and Sanmi Koyejo. Is model collapse inevitable? breaking the
curse of recursion by accumulating real and synthetic data. In Proc. of COLM, 2024.

Xintong Hao, Ruijie Zhu, Ge Zhang, Ke Shen, and Chenggang Li. Reformulation for pretraining
data augmentation. ArXiv preprint, 2025.

10


Preprint. Under review.

Alex Havrilla, Andrew Dai, Laura O’ Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fab-
rizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, et al. Surveying the effects
of quality, diversity, and complexity in synthetic data from large language models. ArXiv preprint,
2024.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. In Proc. of ICLR, 2021.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-
nigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre.
An empirical analysis of compute-optimal large language model training. In Proc. of NeurIPS,
2022.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-
lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,
Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas
Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv preprint, 2023.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. ArXiv preprint, 2020.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proc. of SOSP, 2023.

Hector J Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. KR,
2012.

Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal,
Etash Guha, Sedrick Keh, Kushal Arora, et al. DataComp-LM: In search of the next generation
of training sets for language models. In Proc. of NeurIPS, 2024.

Xiaochuan Li, Zichun Yu, and Chenyan Xiong. Montessori-Instruct: Generate influential training
data tailored for student learning. In Proc. of ICLR, 2025.

Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
Textbooks are all you need ii: phi-1.5 technical report. ArXiv preprint, 2023.

Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. ArXiv preprint,
2024.

Pratyush Maini, Skyler Seto, Richard Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
Rephrasing the web: A recipe for compute and data-efficient language modeling. In Proc. of
ACL, 2024.

Pratyush Maini, Vineeth Dorna, Parth Doshi, Aldo Carranza, Fan Pan, Jack Urbanek, Paul Burstein,
Alex Fang, Alvin Deng, Amro Abbas, et al. BeyondWeb: Lessons from scaling synthetic data for
trillion-scale pretraining. ArXiv preprint, 2025.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? A new dataset for open book question answering. In Proc. of EMNLP, 2018.

Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra
Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language
models. In Proc. of NeurIPS, 2023.

Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, and
Xian Li. Recycling the web: A method to enhance pre-training data quality and quantity for
language models. In Proc. of COLM, 2025.

11


Preprint. Under review.

Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset:
Word prediction requiring a broad discourse context. In Proc. of ACL, 2016.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli,
Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The Refined-
Web dataset for Falcon LLM: Outperforming curated corpora with web data only. In Proc. of
NeurIPS, 2023.

Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, and Junbo Zhao. DataMan: Data
manager for pre-training large language models. In Proc. of ICLR, 2025.

Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu. Long2RAG:
Evaluating long-context & long-form retrieval-augmented generation with key point recall. In
Findings of EMNLP, 2024.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQUAD: 100,000+ questions
for machine comprehension of text. In Proc. of EMNLP, 2016.

Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering
challenge. TACL, 2019.

Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives:
An evaluation of commonsense causal reasoning. In Proc. of AAAI, 2011.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An ad-
versarial winograd schema challenge at scale. In Proc. of AAAI, 2020.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Yang Wu, et al. DeepSeekMath: Pushing the limits of mathemati-
cal reasoning in open language models. ArXiv preprint, 2024.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In
Proc. of ICLR, 2017.

Tlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal.
AI models collapse when trained on recursively generated data. Nature, 2024.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond the imitation
game: Quantifying and extrapolating the capabilities of language models. TMLR, 2023.

Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,
Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming common craw] into a
refined long-horizon pretraining dataset. In Proc. of ACL, 2025.

Bojan Tunguz. 200,000+ jeopardy! questions. Kaggle, 2019.

Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn.
Will we run out of data? limits of Ilm scaling based on human-generated data. In Proc. of ICML,
2024.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions.
In Proc. of ACL, 2023.

Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. OctoThinker: Mid-training incentivizes
reinforcement learning scaling. ArXiv preprint, 2025.

Maurice Weber, Daniel Y Fu, Quentin Gregory Anthony, Yonatan Oren, Shane Adams, Anton
Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun,
Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Re, Irina
Rish, and Ce Zhang. RedPajama: an open dataset for training large language models. In Proc. of
NeurIPS, 2024.

12


Preprint. Under review.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proc. of
ICLR, 2022.

Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. QuRating: Selecting high-
quality data for training language models. In Proc. of ICML, 2024.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Ly, et al. Qwen3 technical report. ArXiv preprint, 2025.

Zichun Yu, Spandan Das, and Chenyan Xiong. MATES: Model-aware data selection for efficient
pretraining with data influence models. In Proc. of NeurIPS, 2024.

Zichun Yu, Fei Peng, Jie Lei, Arnold Overwijk, Wen-tau Yih, and Chenyan Xiong. Group-level data
selection for efficient pretraining. In Proc. of NeurIPS, 2025.

Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. MAmmoTH2?: Scaling instructions from
the web. In Proc. of NeurIPS, 2024.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a
machine really finish your sentence? In Proc. of ACL, 2019.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. BERTScore: Eval-
uating text generation with bert. In Proc. of ICLR, 2020.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation
models. In Findings of NAACL-HLT, 2024.

Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example:
Lifting pre-training data quality like experts at scale. In Proc. of ICML, 2025a.

Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong
Liu, and Eric P. Xing. MegaMath: Pushing the limits of open math corpora. In Proc. of COLM,
2025b.

Peide Zhu and Claudia Hauff. Evaluating bert-based rewards for question generation with reinforce-
ment learning. In Proc. of SIGIR, 2021.

13


Preprint. Under review.

A ADDITIONAL RESULTS

This section presents additional results that complement our main findings.

A.1 EFFICIENCY COMPARISON

We provide a detailed comparison of the training Table 4: H100 hours of rephrasing 72B to-
and inference hours required for each rephrasing Kens. We exclude ProX, as it is an Apes
method in Table 4. REPRO demonstrates significant sed method rather than rephrasing.
efficiency advantages over other approaches, with

a 1.2 speedup compared to WRAP and a 36.7 x WRAP ReWire REPRO
speedup compared to ReWire. This confirms that our Trainin 192
recycling method is not only more effective but also Faferene: 2.095 63.360 1.536
more cost-efficient, making it a practical choice for Total 2095 63.360 1.728
large-scale pretraining. : : :

A.2. IMPROVEMENTS ON ALL DATAMAN CRITERIA

In this section, we present the improvements of REPRO on all DataMan criteria. As shown in
Figure 8, despite only being optimized for the overall score, REPRO consistently outperforms the
organic data and prompting baseline across all individual criteria. For some subjective criteria like
knowledge novelty and creativity, the ratio of Score=5 remains low after rephrasing. This is ex-
pected, as these aspects are inherently challenging to enhance through rephrasing alone, especially
given our rephraser is trained to faithfully preserve the original content. In summary, these results
highlight the effectiveness of REPRO in improving various dimensions of data quality.

Accuracy Coherence Language Consistency Semantic Density Knowledge Novelty
100 J Organic a 100. _| 60
P. 80 60
rompting ial
—T5 WE)
3 RL ail! 60 4
2 n 40
350 50 lt
: | ‘ 20 i
25 L 20 25 20 :
mas Ee ee | a | a a i= |
a rr a SC
Topic Focus Creativity Professionalism Style Consistency Grammatical Diversity
| 40 a 60 fj] 60 a]
80 io 80
S60 30 40 60 40
2 =
3 40 20 —| 40 i.
m4 20. 20
20 iL 10 h eal || 20
0 == oiize cell plea V5 0 efi 0 Fin J
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Structural Standardization Originality ioo Sensitivity Overall Score
60 ll) 80 +p) 6° iy of
WS
S40 60 40
2 | 50
3 40
20 r 20 =
| | : : |
a ‘i ime ——— me He Em ll
rr a re

Figure 8: Improvements of REPRO on all DataMan criteria.

A.3. HYPERPARAMETER STUDIES

In this study, we vary the coefficient of DataMan reward Apataman to 1, 3 (our main setup), and 6.
As shown in Figure 9, all reward curves increase steadily and show a very similar pattern, showing
the robustness of our method. Upweighting DataMan reward can make it increase faster, yet at the
cost of larger fluctuations in faithfulness rewards, and vice versa. Our final choice strikes a decent
balance between DataMan and faithfulness rewards. Our compute resources do not support a larger
RL, e.g., training Qwen3-8B, while concurrent work (Maini et al., 2025) finds a diminishing return
when using an 8B rephraser compared to 3B.

14


Preprint. Under review.

be) 1.000
g s g 2
Bs 809 2 0.95 $
o : : 3 0.975
a2 0.6 rm Fe 2
g 5 2 Es
S —Apataman=3 a 0.8 oS: 0.90 Bp 0.950
sg 04 rial o I 5
S VU. —Apataman=! Pa) |
aa 0.925
= £3 0.7 % 0.85
0 200 400 600 800 0 200 400 600 800 0 200 400 600 800 0 200 400 600 800
Training Steps Training Steps Training Steps Training Steps
(a) DataMan (b) BERTScore (c) Structure (d) Length

Figure 9: Validation curves of (a) DataMan, (b) BERTScore, (c) structure, and (d) length rewards
during our RL training with different DataMan coefficients.

Table 5: Data selection with different quality functions.

Commonsense Language Reading Symbolic World
Reasoning Understanding Comprehension Problem Knowledge Core

Quality Function Pool Unique Data (3 tasks) (6 tasks) (3 tasks) (5S tasks) (5 tasks) (22 tasks)
1B Setting: 1.4B model, 28.8B training tokens

Random 72B 7.28 0.28256 0.33930 0.19857 0.16795 0.23611 0.24998
DCLM-fastText 72B 7.28 0.32348 0.38371 0.19584 0.19795 0.28745 ~—-0.28578
DCLM-fastText 72B 14.4B 0.31483 0.37916 0.15112 0.17507 0.28311 ~—-0.27108
DataMan 72B 7.2B 0.31369 0.32591 0.17524 0.16676 0.27709 ~—-0.25643
DCLM-fastText + DataMan|72B 7.2B + 7.2B 0.32643 0.37210 0.19902 0.19010 0.28630 0.28141

For Tpertscore, We Set it as 0.65, since we find that about two-thirds of the recycled data by prompting
can achieve a BERTScore above this value. It is a representative cutoff that reflects the majority
semantic similarity level of the recycled data while still leaving room to improve rephrasing quality.

A.4. DATAMAN AS QUALITY FUNCTION

In this section, we evaluate the effectiveness of using DataMan as a quality function for select-
ing organic data. As shown in Table 5, either using DataMan selection only or combining it with
DCLM-fastText selection could not beat using DCLM-fastText alone. This suggests that (1) simply
merging multiple quality functions may not enlarge the amount of high-quality data, highlighting the
importance of recycling, and (2) the reward in rephraser training is not necessarily the best quality
function for data selection, which strengthens our findings in Section 5.3.

B ADDITIONAL EXPERIMENT DETAILS

This section provides additional details about our experiments.

B.1 EVALUATION TASKS

We list all evaluation task names and their few-shot numbers in Table 6.

B.2 BASELINES

We provide the implementation details of the baselines we compare against in our main results:

¢ WRAP (Maini et al., 2024): Following their original paper, we adopt Mistral-7B-Instruct-
v0.1 (Jiang et al., 2023) as the rephraser, using their Wikipedia-style prompt.

¢ ProX (Zhou et al., 2025a): This method uses Llama-3-70B-Instruct (Dubey et al., 2024) to
annotate chunk-level programs to polish the organic data. Then, they train a 0.3B model
to learn the annotated programs and perform final edits. We emphasize that their recy-
cling mechanism relies on program-based operations, in contrast to our generation-based
rephrasing. These two mechanisms are essentially orthogonal, and we believe future work
can leverage both to more effectively recycle the web.

15


Preprint. Under review.

Table 6: All evaluation task names and their few-shot numbers.

Category Task #Shots
copa (Roemmele et al., 2011) 0
Commonsense Reasoning openbook_ga (Mihaylov et al., 2018) 0
piqa (Bisk et al., 2020) 10
bigbench_language_identification (Srivastava et al., 2023) 10
hellaswag_zeroshot (Zellers et al., 2019) 0
2 hellaswag (Zellers et al., 2019) 10
Language Understanding lambada_openai (Paperno et al., 2016) 0
winograd (Levesque et al., 2012) 0
winogrande (Sakaguchi et al., 2020) 0
boolq (Clark et al., 2019) 10
Reading Comprehension coqa (Reddy et al., 2019) 0
squad (Rajpurkar et al., 2016) 10
agi_eval_Isat_ar (Zhong et al., 2024) 3
bigbench_cs_algorithms (Srivastava et al., 2023) 10
Symbolic Problem bigbench_dyck_languages (Srivastava et al., 2023) 10
bigbench_operators (Srivastava et al., 2023) 10
bigbench_repeat_copy_logic (Srivastava et al., 2023) 10
arc_challenge (Clark et al., 2018) 10
arc_easy (Clark et al., 2018) 10
World Knowledge bigbench_qa_wikidata (Srivastava et al., 2023) 10
jeopardy (Tunguz, 2019) 10
mmlu_fewshot (Hendrycks et al., 2021) 5

¢ ReWire (Nguyen et al., 2025): This method employs guided rewriting with chain-of-
thought reasoning using Llama-3.3-70B-Instruct (Dubey et al., 2024). As their code has
not been open-sourced, we randomly sample 7.2B tokens from their released data. This
may give them a performance advantage, since their organic pool differs from ours, mean-
ing their method in our main results actually utilizes a larger pool.

B.3. TRAINING HYPERPARAMETERS

We provide our training hyperparameters in Table 7.

Table 7: Training details.

Hyperparameter | 400MLM 1.4B LM Rephraser
Steps 27462 54923 2000

Batch size 512 256 24
Sequence length 2048 2048 4096 (20481 + 20480)
Max learning rate 3e-3 3e-3 le-6
Optimizer AdamW AdamW AdamW
Scheduler Cosine Cosine Cosine

B.4. INFERENCE HYPERPARAMETERS

In the inference stage, we adopt v11m (Kwon et al., 2023) framework for efficient text generation.
We set the generation parameters to temperature=1.0, top_p=0.9, and max_tokens=2048. For docu-
ments exceeding this length, we split them into smaller chunks, process each chunk independently,
and finally concatenate the results to obtain the complete output.

16


Preprint. Under review.

B.5 PROMPTS

REPRO Prompt

Your task is to read and paraphrase the provided text following these instructions:
- Delete clearly irrelevant content:
- Website headers, navigation bars, or menu items (e.g., “Home — About — Contact’)
- Unrelated HTTP links (e.g., ads, trackers, developer tools)
- Generic footers (e.g., contact info, privacy policies, unsubscribe links)
- Empty lines or decorative elements (e.g., “—”)
- Preserve all content that is relevant and meaningful:
- Informative or independently useful
- Related to the topic, even tangentially
- Provides context, background, or supporting value
- Includes technical terms, key concepts, factual details, reasoning, and examples
- Handle mixed-relevance sentences carefully:
- Remove only the irrelevant fragment if the rest remains coherent
- Delete the whole sentence if the remainder loses meaning
- Do not alter meaningful content unnecessarily:
- Only delete or modify when content is clearly meaningless or off-topic
- Preserve the original structure, logic, and depth of the text
- Do not add explanations, notes, assumptions, or claims not found in the original text
Here is the text:
{Organic Text}
Task:
After thoroughly reading the above text, paraphrase it in high-quality and clear English
following the instructions.
Start your response immediately with “Here is a paraphrased version:” and then provide the
paraphrased text.

DataMan Prompt

Please score the text on fourteen evaluation criteria and specify its domain:
Text: {Text}

Domain: _

[1]Accuracy:_/5

[2]Coherence: _/5

[3]Language Consistency: _/5
[4]Semantic Density:_/5
[5]Knowledge Novelty:_/5
[6]Topic Focus: _/5
[7]Creativity:_/5
[8]Professionalism:_/5

[9]Style Consistency:_/5
[10]Grammatical Diversity:_/5
[11]Structural Standardization: _/5
[12]Originality:_/5
[13]Sensitivity: /S

[14]Overall Score:_/5

17


Preprint. Under review.

Structure Prompt

[Instruction]

You are given two pieces of text: an original pretraining data sample and a rephrased version.
Your task is to judge if the rephrased version preserves the **structure** of the original
sample.

- By “structure”, we mean formatting, style, and presentation (e.g., paragraphing, JSON, list
format, code blocks, markdown usage, plain text style).

- Do NOT consider semantic meaning. Ignore whether the words are the same or the content
is equivalent.

- Focus only on whether the rephrased sample follows the same textual structure as the
original (e.g., if the original is plain text paragraphs, the rephrased should also be plain text;
if the original has bullet lists, the rephrased should also have bullet lists).

[Output]

Output **only** ‘1° if the structure is preserved.

Output **only** ‘O° if the structure is not preserved.

[Examples]

Example 1:

Original:

This is a paragraph.

This is another line.

Rephrased:

Here is a rewritten paragraph.

Here is another line of text.

Explanation: Both are plain text paragraphs, no special formatting. Structure preserved.
Output: 1

Example 2:

Original:

- Item one

- Item two

Rephrased:

First item. Second item.

Explanation: The original uses a bullet list, while the rephrased is plain sentences. Structure
not preserved. Output: 0

Example 3:

Original:

{{“name”: “Alice”, “age”: 30}}

Rephrased: {{“person”: “A.’, “years”: 30}}

Explanation: Both are JSON objects with the same structured format. Structure preserved.
Output: 1

Explanation: The original is plain code with no markdown fences, while the rephrased in-
troduces code fences. Structure not preserved. Output: 0

[Original]

{Organic Text}

[Rephrased]

{Recycled Text}

18


Preprint. Under review.

Structure Classification Prompt

Consider the following web page:

Content:

{Text}

Your task is to carefully classify the structure of the given web page. Here, structure refers
to the way the content is represented — for example, its markup language, formatting con-
ventions, or encoding style — rather than its topic or purpose. The structure should NOT be
HTML.

Please respond with only the name of the structure type, without any additional explanation,
commentary, or extra text.

Operation Classification Prompt

Based on the original text and its rephrased version, extract the key operations that were
performed to transform the original text into the rephrased text.
Each operation should be described as one verb + one noun, e.g., “removing ads”.
Focus on significant changes such as rewording, restructuring, removing, or clarifying con-
tent, while ignoring minor edits like punctuation or spacing adjustments.
Respond strictly in JSON format: {
“operations”: [

operation1,

operation2,

operation3,

]

}
[Original Text]: {Organic Text}
[Rephrased Text]: {Recycled Text}

WRAP Prompt

For the following paragraph give me a diverse paraphrase of the same in high quality English
language as in sentences on Wikipedia.

Here is the paragraph:

{Text}

Start your response immediately with “Here is a paraphrased version:” and then provide the
paraphrased text.

19


Preprint. Under review.

ReWire Prompt

Below is a draft from an AI Assistant when trying to accomplish task or solving a problem.
Analyze and understand the task and problem(s) to be solved. Then pretend to be the expert
who is most skillful to acomplish this task, write down the detailed thinking process and
internal monologue that went into identifying a strategy and lay out a plan about how to
solve this problem. Experts usually apply meta-reasoning and planning to reason about how
to best accomplish the task before jumping to solution.

Deliberate meta-reasoning also involves reflection which can help identify issues and take
a step back to explore other paths. Below are some generic examples of starting questions
experts could ask themselves during meta-reasoning process. The expert will come up with
the most relevant questions that can help with their thinking process, which are also very
specific to the task.

Let’s first try to understand the task and exactly what problem(s) to be solved. What is the
core issue or problem that needs to be addressed? What are the key assumptions underlying
this problem?

How can I break down this problem into smaller, more manageable parts? How can I sim-
plify the problem so that it is easier to solve?

What kinds of solution typically are produced for this kind of problem specification? Given
the problem specification and the current best solution, have a guess about other possible
solutions. Let’s imagine the current best solution is totally wrong, what other ways are there
to think about the problem specification.

What is the best way to modify this current best solution, given what you know about these
kinds of problem specification?

Am I on the right track? Let’s check our progress so far.

Let’s make a step by step plan and implement it with good notion and explanation.

Finally, write an improved response after thinking about how to accomplish the task. Take
information and details from the original draft whenever they are useful. Therefore, the
improved response should not be shorter than the original response. The improved response
should have better formatting and readability, with more coherent and in-depth reasoning,
while removing any noise or digression. Note that the best experts chosen to answer each
prompt may be different, so please make sure the you do not sound like the same expert for
all tasks.

IMPORTANT: Start your analysis and thinking right away. DO NOT add any filler text,
explanations or notes about your response. Put the thinking and planning between <thinking
starts> and <thinking ends>, and the improved response between <improved response
starts> and <improved response ends>.

Original Draft: {Text}

20


Preprint. Under review.

C CASE STUDY

Finally, we conduct an interesting case study below. We find that the recycled text from ReWire or
the rephraser trained without faithfulness rewards introduces extraneous information loosely related
to the original text (e.g., the assassination of Archduke Franz Ferdinand) and alters the structure of
the organic data from markdown to plain text. This may increase the quality score but reduce the
overall faithfulness of recycled data, posing a risk of model collapse. WRAP also adds content that
is not present in the organic data. In contrast, REPRO strictly adheres to the original content and
structure while improving the writing quality by removing irrelevant advertisements and metadata.

Organic Data
Related Ads

5 Facts About Gallipoli

Here are 5 facts about Gallipoli

- Gallipoli is a peninsular in Turkey

- British and colonial forces attacked the peninsula in 1915 during World War I

- It was known as the Dardanelles Campaign

- Deaths were 43,000 British, 15,000 French, 8,700 Australians, 2,700 New Zealanders and
1,370 Indians

- The Allies withdrew from the campaign which is celebrated on ANZAC Day

blog comments powered by Disqus”’url’:”*http://Sfactsabout.com/articles/S-facts-about-
Gallipoli/’’warcinfo”:”robots: classic

hostname: ip-10-137-6-227.ec2.internal

software: Nutch 1.6 (CC)/CC WarcExport 1.0

isPartOf: CC-MAIN-2015-40

operator: CommonCraw! Admin

description: Wide crawl of the web for September 2015

publisher: CommonCrawl

format: WARC File Format 1.0

conformsTo: http://bibnum.bnf.fr/WARC/WARC _ISO_28500_version1 latestdraft.pdf

Recycled Data (REPRO)

**Gallipoli: Key Historical Facts**

Gallipoli is a peninsula in Turkey that played a significant role in military history.

During World War I, British and colonial forces launched an attack on the region in 1915 as
part of the Dardanelles Campaign.

The campaign resulted in substantial casualties, including 43,000 British, 15,000 French,
8,700 Australian, 2,700 New Zealand, and 1,370 Indian soldiers.

Despite initial advances, the Allies ultimately withdrew from the campaign.

The event is commemorated annually on ANZAC Day.

21


Preprint. Under review.

Recycled Data (REPRO w/ SFT)
5 Key Points About Gallipoli

1. **Location**: Gallipoli is a peninsula situated in Turkey.

2. **Historical Conflict**: It was the site of an attack by British and colonial forces during
World War I in 1915, known as the Dardanelles Campaign.

3. **Casualties**: The campaign resulted in significant losses, with 43,000 British troops,
15,000 French soldiers, 8,700 Australians, 2,700 New Zealanders, and 1,370 Indian soldiers
killed.

4. **Military Withdrawal**: Due to the heavy casualties and strategic challenges, the Allies
decided to withdraw from the campaign.

5. **Memorial Day**: This historical event is commemorated annually on ANZAC Day.

Recycled Data (RL w/o Faithfulness)

Gallipoli is a strategically significant peninsula located in northwest Turkey, situated be-
tween the Aegean Sea and the Dardanelles Strait. This location played a pivotal role during
the First World War, specifically during the 1915 campaign known as the Dardanelles or
the Battle of Gallipoli, a major Allied military operation initiated by Britain and its colonial
allies. The objective was to open a new front in the Ottoman Empire to relieve pressure on
the Russian Empire and to liberate Constantinople from Turkish control.

The Gallipoli campaign saw a significant deployment of British, French, Australian, New
Zealand, and Indian troops. The operation commenced on May 19, 1915, with the aim of
capturing the Turkish-held Gallipoli Peninsula and forcing the Ottomans to withdraw from
the Eastern Front. However, the campaign faced numerous challenges, including treacher-
ous terrain, strong Turkish defenses, and logistical difficulties, which contributed to a costly
stalemate.

The Allied forces suffered heavy casualties during the campaign, with estimates ranging
from 43,000 British, 15,000 French, 8,700 Australians, 2,700 New Zealanders, and 1,370
Indian soldiers killed, in addition to numerous wounded. Despite their efforts, the Allies
were unable to achieve their strategic objectives, and the campaign ended in disaster, lead-
ing to the withdrawal of British and French forces from the peninsula in January 1916.

This pivotal event in military history has been immortalized in Australian and New Zealand
military traditions, and ANZAC Day, observed on April 25th, commemorates the service and
sacrifice of these nations’ soldiers in the First World War, including the gallantry displayed
at Gallipoli. The commemorations emphasize themes of bravery, sacrifice, and national
identity, serving as a reminder of the significant contributions made by Australian and New
Zealand soldiers to the Allied war efforts.

Recycled Data (fastText as Quality Reward)

The text outlines five significant facts regarding Gallipoli. First, it describes Gallipoli as a
geographical region located in Turkey. Additionally, it highlights the historical context of
the area, specifically the military operations that took place there during World War I. Fur-
thermore, the text provides a detailed account of the casualties sustained during the conflict,
emphasizing the substantial loss of life on both sides. Finally, it mentions the historical
significance of the campaign, including the eventual withdrawal of the Allied forces and its
connection to ANZAC Day. Overall, the text serves as a concise summary of the key points
related to Gallipoli’s historical importance.

22


Preprint. Under review.

Recycled Data (WRAP)

Five Noteworthy Factoids Concerning Gallipoli

Gallipoli, a coastal region in Turkey, has been the focus of five intriguing facts that have
garnered significant attention.

Firstly, Gallipoli is a peninsula located in Turkey, and it has become the focal point of his-
torical events, particularly during World War I.

Secondly, the British and colonial forces launched an attack on the peninsula in 1915, which
was known as the Dardanelles Campaign.

Thirdly, the casualties during the campaign were significant, with 43,000 British, 15,000
French, 8,700 Australians, 2,700 New Zealanders, and 1,370 Indians losing their lives.
Fourthly, the Allies withdrew from the campaign, marking a significant moment in the his-
tory of World War I.

Finally, the campaign has become a symbol of bravery and sacrifice, and it is celebrated
annually on ANZAC Day, which stands for Australian and New Zealand Army Corps Day.

Recycled Data (ReWire)

The Gallipoli campaign was a pivotal event in World War I, marking a significant turning
point in the conflict. Located on the Gallipoli peninsula in Turkey, the campaign was fought
between the Allied Powers, including British, French, Australian, New Zealand, and Indian
forces, and the Ottoman Empire.

To understand the context of the campaign, it is essential to consider the events lead-
ing up to the war. In 1914, the assassination of Archduke Franz Ferdinand sparked a chain
reaction of diplomatic crises and military mobilizations, eventually leading to the outbreak
of World War I. The Allied Powers, seeking to gain a strategic advantage, set their sights on
the Dardanelles, a narrow strait that connected the Aegean Sea to the Sea of Marmara, and
ultimately, to the Black Sea.

The Gallipoli campaign, also known as the Dardanelles Campaign, began on Febru-
ary 19, 1915, when British and French warships launched a bombardment of the Ottoman
defenses. The campaign was intended to capture the strategic location of Constantinople,
now Istanbul, and secure a vital trade route. However, the Ottoman forces, led by Mustafa
Kemal, were better prepared than anticipated, and the campaign quickly turned into a
disastrous defeat for the Allies.

The campaign itself was marked by a series of bloody battles, including the landing
at Anzac Cove, the Battle of Lone Pine, and the Battle of Chunuk Bair. Despite being out-
numbered, the Ottoman forces held their ground, and the Allies suffered heavy casualties.
The death toll was staggering, with 43,000 British, 15,000 French, 8,700 Australian, 2,700
New Zealand, and 1,370 Indian soldiers losing their lives.

The aftermath of the campaign was marked by the withdrawal of the Allied forces in
January 1916. The failure of the campaign had significant consequences, including the
resignation of Winston Churchill, who had been a strong advocate for the campaign. The
legacy of the Gallipoli campaign is still commemorated today, with ANZAC Day, April 25,
marking the anniversary of the landing at Anzac Cove.

In conclusion, the Gallipoli campaign was a pivotal event in World War I, marking a
significant turning point in the conflict. The campaign’s failure had far-reaching conse-
quences, including the resignation of key leaders and a shift in the balance of power. Today,
the campaign is remembered as a testament to the bravery and sacrifice of the soldiers who
fought and died on the Gallipoli peninsula.

23
