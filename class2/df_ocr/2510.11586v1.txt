2510.11586v1 [cs.CL] 13 Oct 2025

arXiv

Survey Response Generation:
Generating Closed-Ended Survey Responses In-Silico
with Large Language Models

Georg Ahnert!, Anna-Carolina Haensch”*“, Barbara Plank?*, Markus Strohmaier!>°
University of Mannheim; 2LMU Munich; *Munich Center for Machine Learning;
‘University of Maryland, College Park; >GESIS Cologne; °CSH Vienna

Correspondence: georg.ahnert [at] uni-mannnheim [dot] de

Abstract

Many in-silico simulations of human survey
responses with large language models (LLMs)
focus on generating closed-ended survey re-
sponses, whereas LLMs are typically trained
to generate open-ended text instead. Previous
research has used a diverse range of methods
for generating closed-ended survey responses
with LLMs, and a standard practice remains
to be identified. In this paper, we systemati-
cally investigate the impact that various Sur-
vey Response Generation Methods have on
predicted survey responses. We present the
results of 32 mio. simulated survey responses
across 8 Survey Response Generation Meth-
ods, 4 political attitude surveys, and 10 open-
weight language models. We find significant
differences between the Survey Response Gen-
eration Methods in both individual-level and
subpopulation-level alignment. Our results
show that Restricted Generation Methods per-
form best overall, and that reasoning output
does not consistently improve alignment. Our
work underlines the significant impact that Sur-
vey Response Generation Methods have on sim-
ulated survey responses, and we develop prac-
tical recommendations on the application of
Survey Response Generation Methods.

1 Introduction

A growing body of research simulates human sur-
vey responses by prompting large language mod-
els (LLMs) to answer survey questions (Argyle
et al., 2023, inter alia). While generative LLMs
are designed to generate open-ended text, previ-
ous studies have implemented various approaches
to constraining LLMs to closed-ended survey re-
sponses (Ma et al., 2024). We define Survey Re-
sponse Generation Methods as techniques used to
elicit closed-ended responses from large language
models to survey questions on attitudes, opinions,
and values. Previous research has shown that

Persona & Question

User Prompt, e.g.:

Instructions

System Prompt, e.g.:

Ideologically, | am a liberal. 2

lam a woman.
am from Kansas.
In the 2016 presidential Q

election, | voted for

You are a political scientist
predicting survey responses.

You only respond in the following
JSON format:
{"response": <response_option>}

prompt

‘O} Large Language Model
‘CS e.g., OLMo 2 32B Instruct

Survey Token Restricted Open
Probability- Generation Generation
Response Based Methods Methods
Generation methods Restrict LLM Generate
Methods Match token output to valid open-ended
probs with re- response response,
sponse options _ options only then classify it
closed-ended survey responses
Evaluation

Individual-Level Alignment Subpopulation-Level Alignment

predicted response predicted human

response distr. response distr.
human
survey —
=
response

Figure 1: Survey Response Generation Methods
Elicit Closed-Ended Survey Responses From LLMs.
We prompt all models with a combined Persona & Ques-
tion Prompt to predict political attitudes in the U.S. or
Germany. All implemented Survey Response Genera-
tion Methods elicit closed-ended survey responses from
the LLMs we investigate. We evaluate the individual-
level alignment of these responses against human survey
data, and the distribution alignment in subpopulations
against human response distributions.

the closed-ended responses of an LLM can vary
strongly from its open-ended responses (R6ttger
et al., 2024; Wang et al., 2024), but a standard
Survey Response Generation Method for social
simulations with LLMs has not yet been identified.

In this paper, we evaluate 8 diverse Sur-
vey Response Generation Methods against 4 hu-
man survey datasets on both individual-level and
subpopulation-level alignment, as shown in Fig-
ure 1. We base our evaluations on three influential
studies that simulate human survey responses on


Accesses Enforces Restricts Generates Generates
Token- Format w/ LLM Open Out- | Probability
Probabilities | Instructions | Vocabulary | put First! | Distribution
First-Token Probabilities Vv Vv v
Token Prob.- =: ;
Based Methods First-Token Restricted v Vv v Vv
Answer Prefix Vv Vv v Vv
Restricted Restricted Choice v Vv
Generation Restricted Reasoning Vv Vv Vv
Methods Verbalized Distribution v 4 Vv
Open Generation {) Open-Ended Classification ? Vv
Methods Open-Ended Distribution 2 Vv Vv

Table 1: Overview of Survey Response Generation Methods. Based on previous research (Ma et al., 2024;
Rottger et al., 2024, inter alia), we implement a range of Token Probability-Based Methods ll, Restricted Generation
Methods Ml, and Open Generation Methods ® for the production of closed-ended survey responses with LLMs.
'With reasoning models, all methods generate open-ended reasoning output first. >The LLM is unrestricted at first,
but restricted in its vocabulary and through formatting instructions in the second, classification step.

political attitudes across multiple countries and
languages (Argyle et al., 2023; Von Der Heyde
et al., 2025; Santurkar et al., 2023). We repli-
cate their findings while including novel Survey
Response Generation Methods. We find that Re-
stricted Generation Methods perform best, and that
reasoning output does not improve performance.
Instructing the model to verbalize probabilities for
all response options consistently yields the best
distributional alignment.

With this study, we contribute to the literature
in three ways: (i) we present results from exten-
sive evaluations of Survey Response Generation
Methods with diverse survey datasets, prompt per-
turbations, LLMs, and decoding parameters for a
total of 32 mio. simulated survey responses. (ii)
We highlight the significant impact of Survey Re-
sponse Generation Methods on simulated survey
responses, and (iii) we develop practical recom-
mendations on which Survey Response Genera-
tion Method to use.

2 Survey Response Generation Methods

A growing body of research uses LLMs to simulate
human survey responses to questions on attitudes,
opinions, and values in-silico (Argyle et al., 2023;
Park et al., 2024; Boelaert et al., 2025, inter alia).
In these studies, an LLM is provided with a de-
scription of a persona and a survey question and
prompted to predict the survey response of this in-
dividual. A large fraction of human survey data
is available in a closed-ended format, i.e., each
question has a set of response options—categorical
or ordinal—from which a participant can choose.

Researchers, thus, deploy a diverse range of Sur-
vey Response Generation Methods for generating
closed-ended survey responses with LLMs as well.

2.1 Token Probability-Based Methods

This set of Survey Response Generation Methods
assumes access to token probabilities and that there
are tokens that uniquely identify a single response
option. For instance, the token “Don” would en-
code Donald Trump as a response option in a ques-
tion on 2016 U.S. vote choice. We extract its token
probability directly from the model output. To in-
crease robustness, the probabilities of tokens that
encode the same response option are added up—for
instance, “donald”, “Tru’’, etc.

The First-Token Probabilities Method Bi is a
popular implementation of this approach that ex-
tracts probabilities directly on the first output to-
ken that a model generates (Argyle et al., 2023;
Dominguez-Olmedo et al., 2024; Santurkar et al.,
2023; Holtdirk et al., 2025, inter alia).

Since many LLM inference providers only return
the top & output tokens and not probabilities over
the whole vocabulary, it could happen that some
response options do not get a token probability
assigned.' We therefore additionally implement the
First-Token Restricted Method ® that restricts
the vocabulary of an LLM to only output tokens
from the possible response options.

Wang et al. (2024) showed that the first-token
probability of an LLM might not always align with
its open-ended response, which instead may start
with a prefix, e.g.,“My answer is ”. One potential

'e.g., the OpenAI API returns the top 20 tokens only.


mitigation would be to consider token probabilities
only after a fixed response-prefix. We implement
the prefix above in the Answer Prefix Method

2.2 Restricted Generation Methods

This set of methods uses formatting instructions in
the system prompt of an LLM to obtain an output
that can be easily parsed (Hartmann et al., 2023;
Motoki et al., 2023, inter alia). We additionally
restrict the vocabulary of the LLM to only the valid
response options. While the latter is not strictly
necessary for these methods, it ensures that the
model follows the formatting instructions provided.

For the Restricted Choice Method &, we dy-
namically define a JSON schema for each survey
question that forces a simple JSON format and only
allows for valid response options to be generated
by the LLM inside this JSON as follows:

{"answer_option”: <a valid response option>}
The Restricted Reasoning Method § extends the
previous method by first forcing the model to gen-
erate reasoning before it generates its choice of
response option. The resulting JSON is formatted
as follows:

{"reasoning”: <any string>,

"answer_option”: <a valid response option>}
While the previous two methods force the model to
generate a single response for each prediction, the
Verbalized Distribution Method © restricts the
model to generate a probability distribution over all
response options, following Meister et al. (2025).
The resulting JSON is formatted as follows:

{<response_option_A>: <probability>,
<response_option_B>: <probability>, ... }
Note that we still obtain a distribution over all re-
sponse options per individual.

2.3. Open Generation Methods

These Survey Response Generation Methods are
inspired by a line of work which argues that LLM
evaluations with survey questions should be based
on open-ended LLM responses (ROttger et al.,
2024; Wright et al., 2024; Myrzakhan et al., 2024).
Our goal is different, as we aim to simulate human
survey responses instead of evaluating the LLM
itself (see also Sorensen et al., 2024). Still, for the
Open Generation Methods, we do not restrict the
output of the model in any way. Instead, we ob-
tain an open-ended response from each LLM and,
in a second step, prompt the same model to clas-
sify this output according to the survey question
and response options at hand. The Open-Ended

Classification Method & implements this two-step
approach using the Restricted Choice Method for
the classification step, i.e., classifying the output
for a single selected response option. The Open-
Ended Distribution Method ® uses the Verbal-
ized Distribution Method for the classification step,
respectively.

3 Experimental Setup

3.1 Datasets

In this paper, we present a comprehensive evalua-
tion of the above described Survey Response Gen-
eration Methods and compare the obtained pre-
dictions to survey responses from human partic-
ipants. We focus on political attitudes, as this has
been a popular subject for in-silico surveys in the
past. Our evaluation spans multiple countries, two
languages, and several response option scales, as
shown in Table 2. We predict vote choice from the
2016 American National Election Study (ANES,
2016), replicating study 2 from Argyle et al. (2023)
with an English-language prompt. We further repli-
cate Von Der Heyde et al. (2025)’s study on vote
choice in Germany, predicting responses from the
2017 German Longitudinal Study (GLES, 2017)
with a German language prompt. As these two
datasets might have leaked into the training data of
state-of-the-art LLMs, we also predict self-reported
vote choice from the 2025 German federal elec-
tion (GLES, 2025).? Finally, we partially* repli-
cate Santurkar et al. (2023) by simulating ques-
tions 48-54 from wave 92 of the American Trends
Panel (ATP, 2021).

>All Llama 3 models and the OLMo training data was
published before the survey fieldwork period, and the Qwen
models shortly thereafter, so it is unlikely that survey results
from the 2025 German election have leaked into the models.

3Partial replication as we implement multiple, more com-
putationally expensive Survey Response Production Methods

Survey #Indivi- #Questions & Lang. & | #Options &
duals! Topic Country | Scale Type
ANES 2016 | 4270 1: vote choice EN, US 3: categorical
GLES 2017 | 1976 1: vote choice DE, DE 9: categorical
GLES 2025 | 6771 1: vote choice DE, DE |} 10: categorical
ATP 2021 5007 7: social & cul- EN, US 5: ordinal

tural change (Likert-scale)

Table 2: Evaluation Datasets. We simulate political
attitudes across multiple languages, countries, and re-
sponse scales. 'Excluding individuals with missing data
on the simulated survey questions. *Random sample out
of 10221 participants in wave 92.


3.2 Prompt Design

To perform predictions for the ANES 2016 and
GLES 2017 datasets, we include the same per-
sona attributes and use the same prompt format
as Argyle et al. (2023), and Von Der Heyde et al.
(2025). For the GLES 2025 dataset, we use the
same prompt format as for the GLES 2017 dataset,
only replacing the year in the survey question.
For the ATP 2021 dataset, we use Santurkar et al.
(2023)’s “bio” format, which is closest to the other
prompts. We run all evaluations with the following
system prompt : “You are a political scientist pre-
dicting responses to the following question:...”,
and include formatting instructions for the Token
Probability-Based and Restricted Generation Meth-
ods. All prompt templates are provided in Ap-
pendix Section B.

3.3. Response Option Scales

To investigate the robustness of all Survey Re-
sponse Generation Methods against rephrasing of
the prompt, we include 4 variants of the response
option scale that the models are prompted with.
The Full Text variants consist of the full text of
each response option, without using an index—e.g.,
‘Clinton’, ‘Trump’, ‘Non-voter’ for a question on
2016 U.S. vote choice. The Indexed variants, also
sometimes referred to as multiple choice questions
(Balepur et al., 2025), use an index for the response
options instead—e.g., ‘A’, ‘B’, ‘C’. Additionally,
we evaluate the prompt both in the original order
of response options and in a reversed order, as
previous work has identified the option order to be
a major source of variation in the output (Tjuatja
et al., 2024; Rupprecht et al., 2025).

3.4 Language Models

We perform our evaluations on 10 open-weight,
instruction-tuned and reasoning models of dif-
ferent sizes from the Llama 3 (3B, 8B, 70B; Llama
Team, 2024), OLMo 2 (1B, 7B, 32B; OLMo Team,
2025), and Qwen 3 (8B, 32B; Qwen Team, 2025)
families of models—see Appendix Table 8 for the
specific model IDs. We also include Qwen 3 8B
and Qwen 3 32B with enabled reasoning output in
our evaluations. We compare responses obtained
from greedy decoding and from the model default
temperatures for 3 random seeds.* For the Open
Generation Methods, we scale temperature for the

“We only use 1 seed for the ATP 2021 dataset to save
computational resources.

first, open-ended response step, but keep the default
temperature of the model for the classification step.
We evaluate the first-token probabilities of reason-
ing models at the first token after the reasoning
output. We also report a stratified baseline ob-
tained from randomly shuffling the human survey
responses in each dataset. For more computational
details, see Appendix A.

3.5 Evaluation

We evaluate all Survey Response Generation Meth-
ods by comparing the generated survey responses
to human survey data.

Individual-Level Alignment First, we calculate
the macro avg. Fl-score of the generated sur-
vey responses against the individual human sur-
vey responses. For all methods that generate an
individual-level distribution across all response op-
tions (see Table 1), we select the most probable
response option for evaluation.

Subpopulation-Level Alignment Second, we
evaluate the alignment of responses on a
subpopulation-level by aggregating individual re-
sponses. This is different from previous research
that simulated subpopulations directly (e.g., San-
turkar et al., 2023). However, simulating individual
survey responses first is more versatile as it enables,
e.g., individual-level imputation of missing human
survey data.

We split the set of respondents into subpop-
ulations by considering all unique values of all
persona attributes that were originally included
in the simulation by Argyle et al. (2023), Von
Der Heyde et al. (2025), and Santurkar et al. (2023),
e.g., women & men, people from different states,
etc. For age, we construct age brackets by floor-
ing to multiples of 10. For all methods that do
not generate an individual-level distribution across
all response options (see Table 1), we create said
distributions through one-hot encoding. We nor-
malize all individual-level distributions to sum up
to 1. We then calculate the distribution over re-
sponse options for a subpopulation as the mean
across individual-level distributions. We report the
subpopulation-level alignment between the gener-
ated distributions and the distribution found in the
human survey data for the respective subpopula-
tion using total variation distance for categorical
response options (ANES & GLES datasets) and
1-Wasserstein distance for ordinal response options
(ATP 2021).


° ° 2) 9
Nv w FS in

macro avg. Fl-score

°
a

0.0

Survey Response
Generation Method
First-Token Probabilities
First-Token Restricted
Answer Prefix

Restricted Choice
Restricted Reasoning
Verbalized Distribution
Open-Ended Classification
Open-Ended Distribution

Response Options
Full Text
Indexed

Model Size
1-3B
7-8B

ANES 2016

GLES 2017

GLES 2025

“

pat

we

ye stratified baseline

ATP 2021

Itt tt
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by macro avg. Fl-score

tt

90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by macro avg. Fl-score

1 1 Intl Ia
h-hI-HE +t
| Se ie Sm Ge a
1 tt {EH
Wott ba

i
aim il

I at
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by macro avg. Fl-score

It a at
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by macro avg. Fl-score

32-70B HE

Decoding Strategy

default temperature |{§|] [1 1H) MMI HIM U1 MILE 0!
greedy decoding |NIINIMI IM! IH 00M tnt

TATE ee
TOE

Figure 2: Individual-Level Alignment Between In-Silico Generated and Human Survey Responses by Dataset
(Columns) and Simulation Specification. Top: macro avg. Fl-score (+) for each aggregated simulation specifica-
tion, mean across the respective runs. Bottom: simulation specification—Survey Response Generation Method,
response option variant, model size, and decoding strategy—sorted by macro avg. Fl-score (—>). Invalid responses
are counted as incorrect. Individual-level alignment varies strongly between Survey Response Generation
Methods. For subpopulation-level alignment, see Appendix Figures 4 & 5.

4 Individual-Level Alignment

First, we view survey response generation as
a prediction task and evaluate individual-level
alignment. Figure 2 shows macro average F1-
scores (top) of aggregated simulation specifica-
tions (bottom)—method, response options variant,
model size, and decoding strategy.

Overall, we observe that Survey Response
Generation Methods have a large impact on
individual-level alignment. Even for specifica-
tions that surpass the stratified baseline, we see a
difference of > 0.35 on the GLES 2017 and GLES
2025 datasets. Token-Probability Based Methods
can yield comparable F1-scores for larger models,
but perform poorly for smaller models. This is
partially because these methods are more prone to
generate invalid responses, especially for reasoning
models (see Appendix Figure 11). Restricted Gen-
eration Methods, and in particular the Restricted
Choice Method, perform best across most datasets.
LLMs with more parameters also generally out-
perform smaller models, but we observe no clear
pattern for response option scales and decoding
strategies.

To investigate the specific impact of Survey Re-
sponse Generation Methods on individual-level
alignment, we fit OLS regressions on each dataset.
Table 3 shows the regression coefficients of all
Survey Response Generation Methods compared
to the First-Token Probabilities Method as a ref-
erence, as it is one of the most popular methods.
We find that the Restricted Choice Method leads
to significant improvements in individual-level
alignment, followed by the Restricted Reason-
ing Method and the Open-Ended Classification
Method. The Verbalized Distribution Method and
the Open-Ended Distribution Method yield signifi-
cant improvements for some datasets, even if they
are designed to generate probability distributions
across all response options rather than a single re-
sponse. We observe similar patterns when using
accuracy as a metric (see Appendix Table 9). Open
Generation Methods generally improve individual-
level alignment, but show smaller coefficients for
most datasets. This indicates that long, open-ended
“reasoning” does not improve the results in our task,
while also being orders of magnitude less computa-
tionally efficient (see Appendix Figure 3).


ANES GLES GLES ATP Model Size
2016 2017 2025 2021 1-3B 7-8B_ | 32-70B
Intercept 448* .045* .054 .095* First-Token Probabilities | 0.34 0.23 0.27
First-Token Restricted |-.000 .000 -.001 .001* First-Token Restricted 0.07 0.15 0.53
Answer Prefix -.015* .015* .016* .001 Answer Prefix 0.11 0.24 0.32
Restricted Choice O19* .225* .241* .028* Restricted Choice 0.18 0.49 0.74
Restricted Reasoning .013* .206* .207* .038* Restricted Reasoning 0.15 0.48 0.69
Verbalized Distribution | .005  .276* .252* .009 Verbalized Distribution 0.07 0.41 0.60
Open-Ended Classif. .025* .206* .196* .026* Open-Ended Classif. 0.20 0.54 0.76
Open-Ended Distrib. .012* .206* .194* .018 Open-Ended Distrib. 0.14 0.45 0.69

Table 3: Regression Coefficients for Individual-Level
Alignment (t). OLS regression for each dataset with
macro avg. Fl-score (t) in each simulation specification
as the dependent variable. We use Survey Response
Generation Method, response option scale, and LLM as
independent variables and include all interactions. We
show coefficients for the Survey Response Generation
Methods (Reference: First-Token Probabilities BI) and
include additional coefficients in Appendix Table 12.
Highest coefficient in bold, second highest underlined.
The Restricted Choice Method & consistently leads
to a significant improvement. * p < 0.05

4.1 Individual-Level Robustness

In addition to accuracy, we evaluate the robustness
of the generated survey responses against prompt
perturbations on an individual level. We show
the mean agreement between the response option
scales—Full Text / Indexed, original / reversed—in
Table 4. We observe that especially smaller mod-
els (1-3B parameters) often generate disagreeing
survey responses. Across all model sizes, Token
Probability-Based Methods show little agreement.
This indicates that they are subject to biases against
certain response options (e.g., A-bias). Out of the
Survey Response Generation Methods we evaluate,
Restricted Choice and Open-Ended Classification
yield the highest agreement across all model sizes,
and the Restricted Reasoning Method as well as
Open-Ended Distribution are generally not far off.

On one hand, individual-level robustness could
be desirable, as the generated survey responses
should not be influenced by perturbations of the
prompt or of the response option scales. On the
other hand, perfect agreement could not be desir-
able, as persona prompts only provide limited infor-
mation about the individuals they aim to simulate,
and human survey responses are often not uniquely
predictable given these attributes. This is exempli-
fied by difficult-to-predict cases.

Table 4: Individual-Level Robustness Across Scales.
Mean Fleiss’s « () across all datasets, results with more
than 10% invalid values are excluded. We underline k >
0.4 and use bold font for « > 0.6. Token Probability-
Based Methods HM show poor individual-level ro-
bustness across response option scales. Separate re-
sults for each dataset and model, as well as agreement
across random seeds are shown in Appendix Figure 8.

5 Subpopulation-Level Alignment

While manually investigating the individuals who
are the most difficult to predict, we found that their
reported vote choice often runs counter to what we
would intuitively expect given their other attributes,
as shown in Table 5. Purely evaluating Survey Re-
sponse Generation Methods with individual-level
alignment would not be able to account for such
cases. We thus additionally consider response dis-
tributions in subpopulations.

We again fit an OLS regression model using
TLV and 1-Wasserstein metrics as the dependent
variables. We show the coefficients for the Survey
Response Generation Methods in Table 6, with the
Restricted Choice Method as a reference, as it has
previously shown good individual-level alignment.

political party US _...| true predicted
ideology identification state vote choice vote choice
a strong Democrat CA .| Trump Clinton
extr. conserv. astrong Republican TX . | Clinton Trump
conservative Indep. leaning Rep. AZ . | Clinton Trump
liberal Indep. leaning Dem. OH .| Trump Clinton
conservative astrong Republican NJ .| Non-Voter Trump

Table 5: Most Difficult to Predict Cases in the ANES
2016 dataset, as identified by a calibrated logistic re-
gression with out-of-fold predictions obtained from 5-
fold cross-validation. All five predictions have a true
class probability of + 0. See Appendix B for a full list
of persona attributes included in the simulation. Given
the limited information available, we would not expect
an LLM to correctly predict the vote choice reported by
these individuals in the ANES survey.


We see that subpopulation-level alignment
varies significantly across the Survey Response
Generation Methods, even if simulation specifi-
cations that yield many invalid responses are ex-
cluded. We find that the Verbalized Distribution
Method generates well-aligned survey responses
across 3 out of 4 datasets. The Restricted Rea-
soning Method and the Open-Ended Classifica-
tion Method also yield good subpopulation-level
alignment, even if they generate a single survey re-
sponse and not a distribution over response options
for each individual. We observe that the Token
Probability-Based Methods often perform worse
than the other methods, i.e., token probabilities
are not well aligned. Regression coefficients for
Jensen-Shannon divergence as a dependent vari-
able are presented in Appendix Table 10 and show
similar trends.

5.1 Reasoning Models

For both individual-level and subpopulation-level,
we find that methods that do not perform open-
ended “reasoning”—especially the Verbalized Dis-
tribution Method—can yield well-aligned survey
responses. This prompts further investigation of
two dedicated reasoning models: Qwen 3 8B &
Qwen 3 32B. We observe that reasoning output
does not consistently improve subpopulation-
level alignment, and in some cases even degrades
alignment—e.g., for the best performing Survey
Response Generation Method on the GLES 2025
dataset for Qwen 3 8B: Restricted Reasoning (see
Appendix Figure 7). This is in line with previous
findings which show that chain-of-thought reason-
ing mostly improves model output on mathemat-
ical and logic tasks (Sprague et al., 2024). We
observe neither an improvement nor a degradation
of alignment for Open Generation Methods, but
these methods also generally do not lead to the
highest subpopulation-level alignment for the two
Qwen models.

5.2 A Global Perspective On
Subpopulation-Level Alignment

Total variation distance and 1-Wasserstein distance
compare in-silico generated survey responses with
human survey responses separately in each sub-
population. The results from all subgroups then
have to be aggregated by using a (weighted) av-
erage or a regression model, as shown in Table 6.
Distance correlation is a measure of dependence
between random vectors that enables an alternative,

ANES GLES GLES | ATP

2016 82017 2025 | 2021
Intercept 368* .331* = .235* | .070*
First-Token Prob. -.004* .360* .611* |-.008
First-Token Restricted |-.003* .302* .251* |-.001
Answer Prefix -.044* .026* .178* |-.000
Restricted Reasoning |-.004 -.065* -.011 |-.008*
Verbalized Distrib. -.048* -.108* -.012* | .005*
Open-Ended Classif. |-.015  -.092* .008 | .003
Open-Ended Distrib. |-.030* -.117* -.004 | .007

Table 6: Regression Coefficients for Subpopulation-
Level Alignment (|). OLS regression for each dataset,
with total variation distance (|) as the dependent vari-
able for the ANES and GLES datasets and 1-Wasserstein
distance (|) for the ATP 2021 dataset. Results with
more than 10% invalid values were excluded. We
use Survey Response Generation Method, response op-
tion variant, and LLM as independent variables and in-
clude all interactions. We show coefficients for Survey
Response Generation Methods (Reference: Restricted
Choice ®) and include additional coefficients in Ap-
pendix Table 13. Verbalized Distribution " leads to
significant improvements on most datasets. * p < 0.05

global perspective on subpopulation-level align-
ment (Székely et al., 2007) and has previously been
used to compare human to model judgment distribu-
tions (Chen et al., 2024). High distance correlation
indicates that the dependency structure between
subpopulations and survey response distributions
in the generated data resembles that in human sur-
vey data. We calculate the distance correlation for
each simulation specification and show the results
in the rightmost column of Table 7 alongside aggre-
gated results for previously discussed individual-
level and subpopulation-level assessments.” Again,
we find that the Restricted Choice and the Ver-
balized Distribution Method yield the best global
alignment.

6 Related Work

Generating Survey Responses with LLMs Pre-
vious research has investigated how perturbations
of the prompt impact the responses of a model to
survey questions (Tjuatja et al., 2024; Dominguez-
Olmedo et al., 2024; Rupprecht et al., 2025, inter
alia). For instance, MclIlroy-Young et al. (2024)
investigated option-order effects, i.e., the tendency
of LLMs to respond to survey questions differently

Regression coefficients by dataset for distance correlation
are shown in Appendix Table 11.


Individual- Subpop.-
Level Level
Align- | Robu- | Align- | Global
ment | stness | ment | Align.
Intercept -.397* | -.371* | .082 .007
First-Token Restrict. | .074 | -.569* | -.316* | .342*
Answer Prefix -.750* | -.260 ald5 147
Restricted Choice -763* | .812* | -.360* | .379*
Restricted Reasoning} .996* | .617* | -.284* | .263*
Verbalized Distrib. .756* | .447* | .183* | .384*
Open-Ended Classif. | .069 403 | -.174 | .302*
Open-Ended Distrib. | .024 | -.120 | .008 | .322*

Table 7: Regression Coefficients Across All Surveys
(normalized, +). OLS regressions for individual-level
alignment (macro avg. Fl-score), robustness (Fleiss’ x),
subpopulation-level alignment (1— total variation dis-
tance / 1-Wasserstein distance) and global alignment
(distance correlation). Results from each metric are
z-score normalized separately on each dataset and re-
sults with more than 10% invalid values were excluded.
We use dataset, Survey Response Generation Method,
response option variant, and LLM as independent vari-
ables and include all interactions. We show coefficients
for Survey Response Generation Methods (Reference:
First-Token Probabilities MJ) and include additional coef-
ficients in Appendix Table 14. Restricted Generation
Methods MM! consistently lead to significant im-
provements. * p < 0.05

when the order in which the response options are
presented is changed. Recently, Cummins (2025)
demonstrated that the survey responses generated
by LLMs can vary widely across input-text specifi-
cations and models. In this paper, we instead focus
on closed-ended output of LLMs, and still find
significant differences between simulation speci-
fications. Future research should combine both
prompts and Survey Response Generation Meth-
ods for a holistic assessment of how LLMs generate
survey responses.

Few papers have investigated the specific impact
of Survey Response Generation Methods so far.
Wang et al. (2024) identified that the most prob-
able first output token often does not match the
open-ended responses of an LLM when prompted
with survey questions. Through our assessments,
we go beyond dissimilarities in the generated re-
sponses and identify the Survey Response Gener-
ation Method that works best for a given survey
and LLM. Meister et al. (2025) compared differ-
ent methods for generating response distributions
and found that the Verbalized Distribution Method

works best, which is why we also implement it. Our
work goes further, as we simulate the responses of
individual survey participants instead of subpopu-
lations directly. We evaluate Survey Response Gen-
eration Methods on non-English datasets (GLES
2017, GLES 2025), and compare a wider range of
methods, including Open Generation Methods.

Response Generation in Other Settings An-
other line of research has investigated closed-ended
and open-ended model responses in other contexts.
Rottger et al. (2024) found that adding instructions
on the response format, and forcing models to, e.g.,
‘take a clear stance’, alters the response option that
a model chooses. Tam et al. (2024) and Long et al.
(2025) also observed a negative impact of format
instructions for mathematical reasoning tasks. We
extend this line of research from evaluations of
LLM alignment and mathematical reasoning to the
simulation of survey responses, including persona
prompts and human survey data for comparison.

7 Recommendations and Conclusion

In this paper, we present a systematic assessment
of Survey Response Generation Methods for
generating closed-ended survey responses in-silico
with large language models. Our evaluations span
8 Survey Response Generation Methods, 4 politi-
cal attitude datasets across multiple countries and
languages, and 10 open-weight LLMs, as well as
various robustness checks.

Recommendations: (i) We argue that the choice
of Survey Response Generation Method should
be well-justified for in-silico surveys since we
find significant differences between these meth-
ods. (ii) We do not recommend the use of Token
Probability-Based Methods BM", as they gener-
ate misaligned survey responses. (iii) For predict-
ing closed-ended survey responses, we suggest to
consider Restricted Generation Methods BE!
first, as they consistently showed significant im-
provement over other methods while also being
more computationally efficient than Open Genera-
tion Methods BN.

This paper should serve as a starting point for
future research on how to generate valid, reliable,
and useful survey responses with LLMs.


Limitations

Our main focus in this paper lies on the evalua-
tion of Survey Response Generation Methods for
the in-silico simulation of surveys on political atti-
tudes. We include a diverse range of datasets across
countries with different political systems and lan-
guages and aim to replicate influential studies on in-
silico surveys Argyle et al. (2023); Von Der Heyde
et al. (2025); Santurkar et al. (2023). Many of our
overall findings generalize across these datasets.
Still, further evaluation should be performed in
non-Western contexts and on surveys of attitudes,
opinions, and values that go beyond the topics that
we have studied. The already large variety of simu-
lation specifications we have included—Survey Re-
sponse Generation Methods, LLM, response option
scale, decoding strategy, etc_—also created com-
putational constraints that did not allow us to in-
vestigate, for instance, the impact of language and
country/political context independently between
the ANES and GLES datasets. Future research
should also investigate further perturbations to the
response options scales such as a missing midpoint
option, which have been found to impact human
and LLM survey responses (Tjuatja et al., 2024;
Rupprecht et al., 2025; McIlroy- Young et al., 2024).
Other applications of closed-ended questions, e.g.,
for LLM benchmarking or to investigate model
alignment, are beyond the scope of this paper, and
it remains to be demonstrated whether our results
generalize to these contexts as well.

We define Survey Response Generation Meth-
ods as being concerned with how closed-ended
survey responses are generated by and can be ex-
tracted from LLMs. This specifically limits the
scope of our paper to exclude different approaches
to prompting LLMs for survey simulation (Lutz
et al., 2025; Long et al., 2025; Park et al., 2024).
Instead, we adopt a persona prompt template from
each study that we replicate (Argyle et al., 2023;
Von Der Heyde et al., 2025; Santurkar et al., 2023).
For each model and dataset, we evaluate all Survey
Response Generation Methods using 4 response
option scale variants, {Full Text, Indexed} x
{original order, reversed order}, and 3 random
seeds® to investigate the robustness of our findings.
Still, alternative persona prompts (e.g., interview-
style) have been shown to positively influence the
LLM response alignment (Lutz et al., 2025) and

°For the ATP 2021 dataset, we only used 1 seed but include
responses to 7 different questions.

might interact differently with different Survey Re-
sponse Generation Methods. Finally, while we do
investigate the joint impact of temperature and
top-k for a subset of the Survey Response Gen-
eration Methods, more advanced decoding strate-
gies (Zhang et al., 2024; Garces Arias et al., 2025,
inter alia) are also out of the scope of this paper.

We note that treating human survey responses
as ground truth ignores biases in human survey re-
sponse (Groves and Lyberg, 2010). Future research
should therefore consider evaluations of in-silico
survey responses that can be performed without
this assumption.

Ethical Considerations

Survey Response Generation Methods are fre-
quently under-reported or insufficiently described
in existing research, which poses challenges for
reproducibility and transparency. In-silico surveys
represent an emerging and active area of inquiry
within both NLP and survey methodology. These
approaches hold considerable promise for advanc-
ing survey research, for example through applica-
tions in pre-testing instruments or imputing miss-
ing data. Nevertheless, uncritical applications—
particularly those aimed at directly predicting sur-
vey outcomes—carry the risk of distorting public
opinion, with a heightened potential to misrepre-
sent the perspectives of marginalized populations.
Furthermore, unresolved epistemological questions
persist regarding the extent to which simulated sur-
vey responses can meaningfully inform our under-
standing of the populations they aim to represent.
Finally, issues of inferential privacy warrant careful
consideration, as individuals may not consent to
the simulation of their responses, particularly in
cases where they have deliberately chosen not to
participate in surveys.

Acknowledgments

We thank Marlene Lutz, Tobias Schumacher, Indira
Sen, Florian Lemmerich, Florian Keusch, Frauke
Kreuter, and the members of the FK?RG research
meeting for their helpful feedback on earlier ver-
sions of this project.

References
ANES. 2016. 2016 Time Series Study.

Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R.
Gubler, Christopher Rytting, and David Wingate.


2023. Out of One, Many: Using Language Mod-
els to Simulate Human Samples. Political Analysis,
31(3):337-351.

ATP. 2021. The American Trends Panel.

Nishant Balepur, Rachel Rudinger, and Jordan Lee
Boyd-Graber. 2025. Which of These Best Describes
Multiple Choice Evaluation with LLMs? A) Forced
B) Flawed C) Fixable D) All of the Above. In Pro-
ceedings of the 63rd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 3394-3418, Vienna, Austria. Associa-
tion for Computational Linguistics.

Julien Boelaert, Samuel Coavoux, Etienne Ollion,
Ivaylo Petev, and Patrick Prag. 2025. Machine
Bias. How Do Generative Language Models Answer
Opinion Polls? Sociological Methods & Research,
54(3):1156-1196.

Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert
Litschko, Anna Korhonen, and Barbara Plank. 2024.
“Seeing the Big through the Small”: Can LLMs Ap-
proximate Human Judgment Distributions on NLI
from a Few Explanations? In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2024,
pages 14396-14419, Miami, Florida, USA. Associa-
tion for Computational Linguistics.

Jamie Cummins. 2025. The threat of analytic flexi-
bility in using large language models to simulate
human data: A call to attention. arXiv preprint.
ArXiv:2509.13397 [cs].

Ricardo Dominguez-Olmedo, Moritz Hardt, and Celes-
tine Mendler-Diinner. 2024. Questioning the Sur-
vey Responses of Large Language Models. In Ad-
vances in Neural Information Processing Systems,
volume 37, pages 45850-45878.

Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang
Lai, Ziyi Xu, Yilong Zhao, and Tianqi Chen. 2025.
XGrammar: Flexible and Efficient Structured Gen-
eration Engine for Large Language Models. arXiv
preprint. ArXiv:2411.15100 [cs].

Esteban Garces Arias, Meimingwei Li, Christian
Heumann, and Matthias Assenmacher. 2025. Decod-
ing Decoded: Understanding Hyperparameter Effects
in Open-Ended Text Generation. In Proceedings of
the 31st International Conference on Computational
Linguistics, pages 9992-10020, Abu Dhabi, UAE.
Association for Computational Linguistics.

GLES. 2017. GLES 2017 Post-Election Cross Section.
GLES. 2025. GLES 2025 Post-Election Cross Section.

R. M. Groves and L. Lyberg. 2010. Total Survey Error:
Past, Present, and Future. Public Opinion Quarterly,
74(5):849-879.

Jochen Hartmann, Jasper Schwenzow, and Maximil-
ian Witte. 2023. The political ideology of conversa-
tional AI: Converging evidence on ChatGPT’s pro-
environmental, left-libertarian orientation.

10

Tobias Holtdirk, Dennis Assenmacher, Arnim Bleier,
and Claudia Wagner. 2025. Learning from Conve-
nience Samples: A Case Study on Fine-Tuning LLMs
for Survey Non-response in the German Longitudinal
Election Study. arXiv preprint. ArXiv:2509.25063
[cs].

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
Memory Management for Large Language Model
Serving with PagedAttention. In Proceedings of the
29th Symposium on Operating Systems Principles,
pages 611-626, Koblenz Germany. ACM.

Llama Team. 2024. The Llama 3 Herd of Models. arXiv
preprint. ArXiv:2407.21783 [cs].

Do Xuan Long, Kenji Kawaguchi, Min-Yen Kan, and
Nancy Chen. 2025. Aligning Large Language Mod-
els with Human Opinions through Persona Selection
and Value—Belief—Norm Reasoning. In Proceedings
of the 31st International Conference on Computa-
tional Linguistics, pages 2526-2547, Abu Dhabi,
UAE. Association for Computational Linguistics.

Marlene Lutz, Indira Sen, Georg Ahnert, Elisa Rogers,
and Markus Strohmaier. 2025. The Prompt Makes
the Person(a): A Systematic Evaluation of Sociode-
mographic Persona Prompting for Large Language
Models. arXiv preprint. ArXiv:2507.16076 [cs].

Bolei Ma, Xinpeng Wang, Tiancheng Hu, Anna-
Carolina Haensch, Michael A. Hedderich, Barbara
Plank, and Frauke Kreuter. 2024. The Potential and
Challenges of Evaluating Attitudes, Opinions, and
Values in Large Language Models. In Findings of the
Association for Computational Linguistics: EMNLP
2024, pages 8783-8805, Miami, Florida, USA. Asso-
ciation for Computational Linguistics.

Reid MclIlroy- Young, Katrina Brown, Conlan Olson,
Linjun Zhang, and Cynthia Dwork. 2024. Order-
Independence Without Fine Tuning. Advances in
Neural Information Processing Systems, 37:72818—
72839.

Nicole Meister, Carlos Guestrin, and Tatsunori
Hashimoto. 2025. Benchmarking Distributional
Alignment of Large Language Models. In Proceed-
ings of the 2025 Conference of the Nations of the
Americas Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(Volume I: Long Papers), pages 24-49, Albuquerque,
New Mexico. Association for Computational Linguis-
tics.

Fabio Motoki, Valdemar Pinho Neto, and Victor Ro-
drigues. 2023. More human than human: measuring
ChatGPT political bias. Public Choice, 198:3-23.

Aidar Myrzakhan, Sondos Mahmoud Bsharat, and
Zhigiang Shen. 2024. Open-LLM-Leaderboard:
From Multi-choice to Open-style Questions for
LLMs Evaluation, Benchmark, and Arena. arXiv
preprint. ArXiv:2406.07545 [cs].


OLMo Team. 2025. 2 OLMo 2 Furious. arXiv preprint.
ArXiv:2501.00656 [cs].

Joon Sung Park, Carolyn Q Zou, Aaron Shaw, Ben-
jamin Mako Hill, Carrie Cai, Meredith Ringel Morris,
Robb Willer, Percy Liang, and Michael S Bernstein.
2024. Generative Agent Simulations of 1,000 People.
arXiv preprint.

Qwen Team. 2025. Qwen3 Technical Report. arXiv
preprint. ArXiv:2505.09388 [cs].

Jens Rupprecht, Georg Ahnert, and Markus Strohmaier.
2025. Prompt Perturbations Reveal Human-Like
Biases in LLM Survey Responses. arXiv preprint.
ArXiv:2507.07188 [cs].

Paul Ro6ttger, Valentin Hofmann, Valentina Pyatkin,
Musashi Hinck, Hannah Kirk, Hinrich Schuetze, and
Dirk Hovy. 2024. Political Compass or Spinning
Arrow? Towards More Meaningful Evaluations for
Values and Opinions in Large Language Models. In
Proceedings of the 62nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 15295-15311, Bangkok, Thai-
land. Association for Computational Linguistics.

Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
Lee, Percy Liang, and Tatsunori Hashimoto. 2023.
Whose Opinions Do Language Models Reflect? In
Proceedings of the 40th International Conference
on Machine Learning, pages 29971-30004. PMLR.
ISSN: 2640-3498.

Taylor Sorensen, Jared Moore, Jillian Fisher,
Mitchell Gordon, Niloofar Mireshghallah, Christo-
pher Michael Rytting, Andre Ye, Liwei Jiang,
Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin
Choi. 2024. Position: a roadmap to pluralistic
alignment. In Proceedings of the 41st International
Conference on Machine Learning, volume 235 of
ICML’24, pages 46280-46302, Vienna, Austria.
JMLR.org.

Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez,
Dongwei Jiang, Manya Wadhwa, Prasann Singhal,
Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Dur-
rett. 2024. To CoT or not to CoT? Chain-of-thought
helps mainly on math and symbolic reasoning. arXiv
preprint. ArXiv:2409.12183 [cs].

Gabor J. Székely, Maria L. Rizzo, and Nail K. Bakirov.
2007. Measuring and testing dependence by correla-
tion of distances. The Annals of Statistics, 35(6).

Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-
Yen Lin, Hung-yi Lee, and Yun-Nung Chen. 2024.
Let Me Speak Freely? A Study On The Impact Of
Format Restrictions On Large Language Model Per-
formance. In Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing:
Industry Track, pages 1218-1236, Miami, Florida,
US. Association for Computational Linguistics.

Lindia Tjuatja, Valerie Chen, Tongshuang Wu, Ameet
Talwalkwar, and Graham Neubig. 2024. Do LLMs

11

Exhibit Human-like Response Biases? A Case Study
in Survey Design. Transactions of the Association
for Computational Linguistics, 12:1011—1026.

Leah Von Der Heyde, Anna-Carolina Haensch, and
Alexander Wenz. 2025. Vox Populi, Vox AI? Using
Large Language Models to Estimate German Vote
Choice. Social Science Computer Review. Publisher:
SAGE Publications.

Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-
Genzel, Paul R6ttger, Frauke Kreuter, Dirk Hovy,
and Barbara Plank. 2024. “My Answer is C”: First-
Token Probabilities Do Not Match Text Answers in
Instruction-Tuned Language Models. In Findings of
the Association for Computational Linguistics: ACL
2024, pages 7407-7416, Bangkok, Thailand. Associ-
ation for Computational Linguistics.

Dustin Wright, Arnav Arora, Nadav Borenstein, Sr-
ishti Yadav, Serge Belongie, and Isabelle Augenstein.
2024. LLM Tropes: Revealing Fine-Grained Values
and Opinions in Large Language Models. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2024, pages 17085-17112, Miami, Florida,
USA. Association for Computational Linguistics.

Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-
Sung Ferng, Heinrich Jiang, and Yiran Chen. 2024.
SLED: Self Logits Evolution Decoding for Improv-
ing Factuality in Large Language Models. In Ad-
vances in Neural Information Processing Systems,
volume 37, pages 5188-5209. Curran Associates,
Inc.

Short Name Huggingface Model ID

Llama 3B meta-llama/Llama-3.2-3B-Instruct
Llama 8B meta-llama/Llama-3.1-8B-Instruct
Llama 70B meta-llama/Llama-3.3-70B-Instruct
OLMo 1B allenai/OLMo-2-0425-1B-Instruct
OLMo 7B allenai/OLMo-2-1124-7B-Instruct
OLMo 32B allenai/OLMo-2-0325-32B-Instruct
Qwen 8B Qwen/Qwen3-8B

Qwen 32B Qwen/Qwen3-32B

Qwen 8B (R) Qwen/Qwen3-8B with Reasoning
Qwen 32B (R) | Qwen/Qwen3-32B with Reasoning

Table 8: Language Models. We evaluate all Survey Re-
sponse Generation Methods on 10 open-weight LLMs.

A Computational Details

We run all our experiments the 10 open-weight
instruction tuned and reasoning models shown in
Table 8. For language model inference, we use
vllm (Kwon et al., 2023) version 0.10.1.1 and
the xgrammar (Dong et al., 2025) backend for in-
ference with structured outputs. We ran all
our experiments on 2 NVIDIA H100 GPUs (tensor-
parallel). Our experiments for the ANES 2016


Survey Response Generation Method

lam First-Token Probabilities mmm Restricted Choice

{@m_ First-Token Restricted lm Restricted Reasoning
Answer Prefix | Verbalized Distribution
a ls
c
°
Qa
un
2
& 100 ms +
c
a
un
o
a
w 10ms4
£
a)
= i
6 i
| | ok
Llama 3B Llama 8B Llama 70B OLMo 1B OLMo 7B

Mmm Open-Ended Classification
Mm Open-Ended Distribution

Joulll

Qwen 8B Qwen 8B (R) Qwen 32B- = Qwen 32B (R)

Figure 3: Mean GPU Time for A Single Survey Response. We run all models using vilm on 2 NVIDIA H100
GPUs (tensor-parallel). We report GPU time instead of token count to accommodate for optimizations such as
automatic prefix caching, but also for the overhead that is created by restricting the vocabulary of an LLM with
structured outputs. Considering the log-scale y-axis, Open Generation Methods, larger models, and in particular
reasoning models require orders of magnitude more GPU time than Token Probability-Based Methods, or the

Restricted Choice Method.

dataset had a total runtime of 88h, for the GLES
2017 dataset of 121h, for the GLES 2025 dataset
of 297h, and for the ATP 2021 dataset of 108h.
We ran our additional experiments on the impact
of decoding hyper-parameters (see Figure 9) on 2
NVIDIA RTX PRO 6000 Blackwell GPUs with
a total runtime of 29h. To save computational re-
sources, we only generated open-ended output once
for each simulation specification (model, seed, re-
sponse scale, temperature) and then classified the
same output separately for the Open-Ended Classi-
fication and for the Open-Ended Distribution Meth-
ods. Figure 3 shows the average GPU Time spent
to generate a single survey response with a given
Survey Response Generation Method and LLM.

B_ Prompts

Tables 15—19 contain all system and user prompts
that we used in our evaluations.

C. Additional Results

12


ANES GLES GLES ATP

2016 2017 2025 2021
Intercept 541 .260* = .339* .179*
First-Token Restricted |-.000 -.006 -.074* .017*
Answer Prefix -.007  -.005  -.057* .020*
Restricted Choice .0O1* .126* .086* .014*
Restricted Reasoning | .035* .138* .078* .017*
Verbalized Distribution |-.001  .178* .163* .012*
Open-Ended Classif. -002* .064* .016 .013*
Open-Ended Distrib. /-.003* .091* .045  .009*

Table 9: Regression Coefficients for Individual-Level
Accuracy (ft). OLS regression for each dataset with
accuracy (+) in each simulation specification as the de-
pendent variable. We use Survey Response Generation
Method, response option scale, and LLM as independent
variables and include all interactions. We show coef-
ficients for the Survey Response Generation Methods
(Reference: First-Token Probabilities MI). For macro
avg. Fl-score as a dependent variable see Table 12.
*p < 0.05

ANES GLES GLES

2016 2017 2025
Intercept .199* 171* .094*
First-Token Probabilities |-.020* L77* .333*
First-Token Restricted —_|-.018* .243* .200*
Answer Prefix -.063*  .006 .106*
Restricted Reasoning -.012*  -.069*  -.018
Verbalized Distribution |-.071*  -.092*  -.003
Open-Ended Classif. -.018 -.086* — -.003
Open-Ended Distrib. -.048* -.098*  -.014

Table 10: Regression Coefficients for Subpopulation-
Level Jensen-Shannon Divergence (|). OLS regres-
sion for each dataset, with Jensen-Shannon divergence
({) as the dependent variable for the ANES and GLES
datasets. Results with more than 10% invalid values
were excluded. We use Survey Response Generation
Method, response option variant, and LLM as indepen-
dent variables and include all interactions. We show
coefficients for Survey Response Generation Methods
(Reference: Restricted Choice ®). For total varia-
tion distance as a dependent variable, see Table 13.
*p < 0.05

13

ANES GLES GLES ATP

2016 2017 2025 2021
Intercept 850*  .852* .927* .656*
First-Token Prob. -.002  -.041* -.062* -.044*
First-Token Restricted |-.002  -.170* -.256* .005
Answer Prefix -.021* -.263* -.309* 001
Restricted Reasoning |-.009* .010 -.036* .020*
Verbalized Distrib. 001 -.004 -.057* -.034*
Open-Ended Classif. |-.008* .017* .011* .013
Open-Ended Distrib. |-.004 -.203  .014* -.010

Table 11: Regression Coefficients for Subpopulation-
Level Distance Correlation (+). OLS regression for
each dataset, with Distance Correlation (+) as the de-
pendent variable. Results with more than 10% invalid
values were excluded. We use Survey Response Gen-
eration Method, response option variant, and LLM as
independent variables and include all interactions. We
show coefficients for Survey Response Generation Meth-
ods (Reference: Restricted Choice ®). For total vari-
ation distance as a dependent variable, see Table 13.
*n < 0.05


ANES 2016 GLES 2017 GLES 2025 aaa ATP 2021
eth 9 1
074 fi f
1 0.12 4
5 0-6 4 - d ¥
a2 1 a a
< 054 : © 0.10 4 “amy
S in
ic] 1 ha, cd
= 0.44 hon, te  oo8 4 "he,
= tt hy a
3 ins, 8
2 aan stratified baseline “a, stratified baseline a 0.06 4
oatstetidedtocwics | | | PN SPORE ey Mags ss || Posseeegeeeteeeseseseesseallggsseeeeees
“ “ay tratified baseli
“ =, ee ee
0.1

Survey Response
Generation Method
First-Token Probabilities
First-Token Restricted
Answer Prefix

Restricted Choice
Restricted Reasoning
Verbalized Distribution
Open-Ended Classification
Open-Ended Distribution

Response Options
Full Text
Indexed

Model Size
1-3B
7-8B

32-70B

Decoding Strategy
default temperature
greedy decoding

I rt
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Total Variation Dist.

io LI i
tl ll
a
mH Tit i
1 tmnt
te
Mmm itt
Hott Pt ou

EEE
EE ee ee

Mm ttt tim Bint
WG WT GH
Moti wn ne

TOE ee
TU tT

I tt
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Total Variation Dist.

mi
i el
tH

Tt mo mit
ill TOT
tout tt

RL BOLL eC Le |
Ue

I a
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Total Variation Dist.

t
TH 1 f
IrHerttt it)
PEP Eb te
et me LAL
Ptett i til
i Hl L |
Ue

MT a

I ot
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by 1-Wasserstein Dist.

Mt ou wit ot

I ATH
1H

Wht l

ee Se |

tH
FA

URE ee
ee

iE es |
ee ee
Wim te

ERR LLU RU
ST TR ee

t more than 10%
invalid responses

t more than 10%
invalid responses

t more than 10%
invalid responses

t more than 10%
invalid responses

Figure 4: Subpopulation-Level Alignment: Total Variation Distance/1-Wasserstein Distance. For the ANES
and GLES datasets, we use total variation distance to measure alignment on categorical response options. For the
ATP dataset, we use 1-Wasserstein Distance to measure alignment on ordinal response options. Top: alignment
metric (lower is better) for each aggregated simulation specification, mean across the respective runs. Bottom:
simulation specification—Survey Response Generation Method, response option variant, model size, and decoding
strategy—sorted by the respective alignment metric. Specifications that lead to more than 10% invalid responses are
excluded.

ANES 2016 GLES 2017 GLES 2025 ATP 2021
0.9 4 4 i. 4 Yaa 4
cc
£o84 4 f 4 4
£ i 9g Pd af ; oot
5 074 F | a 1 Statiied papeliig. gaa...
i t 1 stratified baseline al on
5 Pt LY | ff PRPS SpPetpterpeespeesptestetrste dl Lssleeeh stratified baseline f
$06 4 4 J 1
é |
'
0.5 4 4 4 4

Survey Response
Generation Method
First-Token Probabilities
First-Token Restricted
Answer Prefix

Restricted Choice
Restricted Reasoning
Verbalized Distribution
Open-Ended Classification
Open-Ended Distribution

Response Options
Full Text
Indexed

Model Size
1-3B
7-8B

32-70B

Decoding Strategy
default temperature
greedy decoding

a tt
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Distance Correlation

HOTA
TE

A a
I aL en Ree
i ee |

Te Re
HN TE TT

T more than 10%
invalid responses

a
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Distance Correlation

att EET

tt |
Prd bet Hil
Ted Ml |
| Tibet
Ht

| ee ce i

W010
11 Mn a

LL
YT Ee

BLE
Pe

tT more than 10%
invalid responses

tt

90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Distance Correlation

L |
nt
Us ee |
Pie vloi @
ne ie
it oe oe
‘| PEP It

i nL
11 |

OE |
LL

SO RT tt
BUHL IML TM Tt

T more than 10%
invalid responses

I ot
90 80 70 60 50 40 30 20 10
top-k simulation specification
ranked by Distance Correlation

I |
Pi a
Piatt teat
! FH
es eee |
Pil 1 ol
Weta

Re ee
11 2

TM
1 ae
Wt 1 TT) tm

T more than 10%
invalid responses

Figure 5: Subpopulation-Level Alignment—Global Perspective: Distance Correlation. Top: Distance corre-
lation (higher is better) for each aggregated simulation specification, mean across the respective runs. Bottom:
simulation specification—Survey Response Generation Method, response option variant, model size, and decoding
strategy—sorted by distance correlation. Specifications that lead to more than 10% invalid responses are excluded.

14


ANES 2016 GLES 2017. GLES2025 ATP 2021
Intercept 0.448** 0.148** 0.133** 0.097**
Survey Response First-Token Restricted -0.000** -0.103** -0.081** -0.002
Generation Method —_ Answer Prefix -0.015** -0.088** -0.064** -0.001
Restricted Choice 0.019** 0.121** 0.161** 0.025**
Restricted Reasoning 0.013** 0.078** 0.127** 0.036**
Verbalized Distribution 0.005 0.173** 0.172** 0.007
Open-Ended Classif. 0.025** 0.103** 0.116** 0.024**
Open-Ended Distrib. 0.012** 0.102** 0.114** 0.016
Response Option Full Text, Reversed -0.009** 0.011 -0.010 0.001
Variant Indexed -0.279** 0.002 -0.024* 0.097**
Indexed, Reversed -0.298** -0.028** -0.042** 0.022**
Model Llama 3B -0.021** 0.019** -0.001 -0.003
Llama 70B -0.033** 0.178** 0.151** 0.033**
OLMo 1B 0.003** -0.031** -0.032** -0.037**
OLMo 7B 0.020** 0.037** 0.028** 0.030**
OLMo 32B 0.114** 0.130** 0.229** -0.001
Qwen 8B 0.010** 0.080** 0.111** -0.019**
Qwen 8B (R) -0.285** 0.065** -0.028** 0.051**
Qwen 32B 0.064** 0.204** 0.181** -0.067**
Qwen 32B (R) -0.004 0.146** 0.070** 0.085**

Table 12: Regression Coefficients for Individual-Level Accuracy (+). OLS regression per dataset with macro
avg. Fl-score as the dependent variable (higher is better). We use Survey Response Generation Method, response
option variant, and LLM name as independent variables, include all interactions, and use clustered standard
errors across seeds. Reference: First-Token Probabilities Method + Full Text response options + Llama 8B.
*p < 0.05, ** p < 0.01

15


ANES 2016 GLES 2017. =GLES 2025 | ATP 2021
Intercept 0.368** 0.331** 0.235** 0.070**
Survey Response First-Token Probabilities -0.004** 0.360** 0.611** -0.008
Generation Method _First-Token Restricted -0.003** 0.302** 0.251** -0.001
Answer Prefix -0.044** 0.026** 0.178** -0.000
Restricted Reasoning -0.004 -0.065** -0.011 -0.008*
Verbalized Distribution -0.048** -0.108** -0.012** 0.005*
Open-Ended Classification | -0.015 -0.092** 0.008 0.003
Open-Ended Distribution -0.030** -0.117** -0.004 0.007
Response Option Full Text, Reversed 0.025** -0.049%* 0.025** -0.002
Variants Indexed 0.059** -0.018 0.048** -0.003
Indexed, Reversed 0.033** -0.017 0.081** 0.005
Model Llama 3B -0.063** 0.051** 0.167** 0.016
Llama 70B 0.019** -0.077** -0.076** 0.020**
OLMo 1B -0.123** -0.036** -0.010 0.025
OLMo 7B -0.189** 0.058** 0.152** 0.063**
OLMo 32B -0.037** -0.040** -0.006 0.040**
Qwen 8B 0.003* -0.016* 0.009* 0.050**
Qwen 8B (R) 0.002 -0.047** -0.028** 0.006**
Qwen 32B -0.100** -0.127** -0.082** 0.017**
Qwen 32B (R) -0.040** -0.107** -0.070** -0.012**

Table 13: Regression Coefficients for Subpopulation-Level Alignment (|). OLS regression per dataset with total
variation distance (lower is better) as the dependent variable for the ANES and GLES datasets and 1-Wasserstein
distance (lower is better) as the dependent variable for the ATP 2021 dataset. We use Survey Response Generation
Method, response option variant, and LLM name as independent variables, include all interactions, and use clustered
standard errors across seeds. Reference: Restricted Choice Method + Full Text response options + Llama 8B.
Results with more than 10% invalid values were excluded. * p < 0.05, **p < 0.01

16


Individual-Level Subpopulation-Level
Accuracy Robustness Alignment Global Align.
(Macro Avg. (Fleiss’ & (Total Var./ (Distance
Fl-Score) Across Scales) 1-Wasserst.) Correlation)
Intercept -0.397* -0.371** 0.082 0.007
Survey Response First-Token Restricted 0.074 -0.569* -0.316** 0.342**
Generation Method Answer Prefix -0.750* -0.260 0.175 0.147
Restricted Choice 0.763* 0.812** -0.360** 0.379**
Restricted Reasoning 0.996** 0.617* -0.284** 0.263*
Verbalized Distribution 0.756* 0.447* 0.183* 0.384**
Open-Ended Classif. 0.069 0.403 -0.174 0.302*
Open-Ended Distrib. 0.024 -0.120 0.008 0.322*
Dataset ATP -0.495 0.068 0.149 -0.746**
GLES -0.315 -0.266 -0.678** -0.857**
GLES2025 -0.306 -0.235 -0.365** -0.530**
Response Option Full Text, Reversed -0.357* -0.311* -0.131** -0.060
Variants Indexed -0.425* -0.335** -0.428** -0.190**
Indexed, Reversed -0.779** -0.503** -0.494** -0.473**
Model Llama 3B -0.472* -0.689** 0.511** -0.861**
Llama 70B 0.273 0.654** 0.191** -0.647**
OLMo 1B -0.638** -1.286** 0.270** -1.187**
OLMo 7B 0.231 -0.291* 0.407** -0.007
OLMo 32B 0.307 0.542** 0.257** 0.076
Qwen 8B 0.033 0.275 -0.535** -0.439**
Qwen 8B (R) 0.326 0.584** -0.088 -0.083
Qwen 32B 0.461* 0.525** 0.332** 0.050
Qwen 32B (R) 0.527* 0.646** 0.166** 0.030

Table 14: Regression Coefficients For Evaluation Metrics (normalized, +). OLS regression on evaluation
outcomes for individual-level accuracy, and robustness (Fleiss’ «), as well as subpopulation-level alignment (1—
total variation distance / 1-Wasserstein distance) and global alignment (distance correlation). Results from each
metric are z-score normalized separately on each dataset. We use dataset, Survey Response Generation Method,
response option variant, and LLM as independent variables and include all interactions. Reference: ANES 2016 +
First-Token Probabilities Hl + Full Text response options + Llama 8B. Results with more than 10% invalid values
were excluded. The Verbalized Distribution Method © consistently leads to a significant improvements.
*p < 0.05, **p < 0.01

17


Survey Response
Generation Method

Formatting Instructions
(System Prompt)

Token Probability
-Based Methods

You are a political scientist predicting responses to the following question: {QUESTION}. These are the
possible answer options: {RESPONSE OPTIONS}. You only respond with the most probable answer option.

Restricted Choice

You are a political scientist predicting responses to the following question: {QUESTION}. These are
the possible answer options: {RESPONSE OPTIONS}. You only respond with the most probable answer option
in the following JSON format:

~** json

{

"answer": {RESPONSE OPTIONS}

3

Restricted Reasoning

You are a political scientist predicting responses to the following question: {QUESTION}. These are
the possible answer options: {RESPONSE OPTIONS}. You always reason about the possible answer options
first.

You respond with your reasoning and the most probable answer option in the following JSON format:
~** json

{

"reasoning": <your reasoning about the answer options>,

"answer": <{RESPONSE OPTIONS}>

}

Verbalized Distribution

You are a political scientist predicting responses to the following question: {QUESTION}. These are
the possible answer options: {RESPONSE OPTIONS}. You only respond with a probability for each answer
option in the following JSON format:

~** json

{

{RESPONSE OPTION 1}: <probability>,

{RESPONSE OPTION 2}: <probability>,

{...}

3

Open Generation Methods
(Step 1: Open Generation)

You are a political scientist predicting responses to the following question: {QUESTION}.

Open-Ended Classification
(Step 2: Classification!)

You are an expert annotator. These are the possible labels: {RESPONSE OPTIONS}. You only respond with
the most probable label in the following JSON format:

~** json

{

"answer": <{RESPONSE OPTIONS}>

}

Open-Ended Distribution
(Step 2: Classification!)

You are an expert annotator. These are the possible labels: {RESPONSE OPTIONS}. You only respond with
a probability for each answer option in the following JSON format:

~** json

{

{RESPONSE OPTION 1}: <probability>,

{RESPONSE OPTION 2}: <probability>,

{...}

3

Table 15: Formatting Instructions (System Prompts) for English—ANES 2016 & ATP 2021 Datasets. The
QUESTION and RESPONSE OPTIONS are identical to what was used by Argyle et al. (2023) and Santurkar et al. (2023)
respectively. 'Additional user prompts for the classification step are shown in Table 16.

Survey Response
Generation Method

Classification Instructions
(User Prompt)

Open-Ended Classification /
Open-Ended Distribution
(Step 2: Classification)

Below is a STATEMENT that responds to the question: {QUESTION} Does the following STATEMENT express
preference for exactly one of the following answer options: <{RESPONSE OPTIONS}>? If yes, which
option is preferred?

STATEMENT: {OPEN-ENDED MODEL RESPONSE}

Table 16: Classification Instructions (User Prompts) for English—ANES 2016 & ATP 2021 Datasets. These
prompts are only used in the second, classification step of the Open-Ended Classification and the Open-Ended
Distribution Methods. The user prompts for all other Survey Response Generation Methods, as well as for the first,
open-ended step of the Open Generation Methods are shown in Table 19.

18


Survey Response Formatting Instructions

Generation Method (System Prompt)

Token Probability Du bist ein Politikwissenschaftler, der Antworten auf die folgende Frage vorhersagt: {QUESTION}. Dies

-Based Methods sind die méglichen Antwortoptionen: {RESPONSE OPTIONS}. You only respond with the most probable answer
option.

Restricted Choice Du bist ein Politikwissenschaftler, der Antworten auf die folgende Frage vorhersagt: {QUESTION}

Dies sind die mdglichen Antwortoptionen: {RESPONSE OPTIONS}. Du antwortest ausschlieBlich mit der
wahrscheinlichsten Antwortoption im folgenden JSON-Format:

json
{
"antwort”: {RESPONSE OPTIONS}
3
Restricted Reasoning Du bist ein Politikwissenschaftler, der Antworten auf die folgende Frage vorhersagt: {QUESTION}.

Dies sind die mdglichen Antwortoptionen: {RESPONSE OPTIONS}. Du argumentierst immer zuerst Uber die
moglichen Antwort-Optionen.

Du antwortest mit deiner Argumentation und der wahrscheinlichsten Antwort-Option im folgenden
JSON-Format:

~** json

{

"argumentation”: <deine Argumentation Uber die Antwort-Optionen>,

"antwort”: <{RESPONSE OPTIONS}>

}

Verbalized Distribution Du bist ein Politikwissenschaftler, der Antworten auf die folgende Frage vorhersagt: {QUESTION}
Dies sind die moglichen Antwortoptionen: {RESPONSE OPTIONS}. Du antwortest ausschlieBlich mit einer
Wahrscheinlichkeit fiir jede Antwort-Option im folgenden JSON-Format:

~** json

{

{RESPONSE OPTION 1}: <Wahrscheinlichkeit>,

{RESPONSE OPTION 2}: <Wahrscheinlichkeit>,

{...}

3

Open Generation Methods Du bist ein Politikwissenschaftler, der Antworten auf die folgende Frage vorhersagt: {QUESTION}.
(Step 1: Open Generation)

Open-Ended Classification Du bist ein erfahrener Annotator. Das sind die moglichen Labels: {RESPONSE OPTIONS}. Du antwortest

(Step 2: Classification!) nur mit dem wahrscheinlichsten Label im folgenden JSON-Format:

~** json

{

"antwort”: <{RESPONSE OPTIONS}>

3
Open-Ended Distribution Du bist ein erfahrener Annotator. Das sind die moglichen Labels: {RESPONSE OPTIONS}. Du antwortest
(Step 2: Classification!) nur mit einer Wahrscheinlichkeit flr jede Antwortoption im folgenden JSON-Format:

~** json

{

{RESPONSE OPTION 1}: <Wahrscheinlichkeit>,
{RESPONSE OPTION 2}: <Wahrscheinlichkeit>,
{...}

3

Table 17: Formatting Instructions (System Prompts) for German—GLES 2017 & GLES 2025 Datasets. The
QUESTION and RESPONSE OPTIONS are identical to what was used by Von Der Heyde et al. (2025). ‘Additional user
prompts for the classification step are shown in Table 18.

Survey Response Classification Instructions

Generation Method (User Prompt)

Open-Ended Classification / Nachfolgend findest du eine AUSSAGE, die auf die Frage {QUESTION} antwortet. Driickt die folgende
Open-Ended Distribution AUSSAGE eine Praferenz fiir genau eine der folgenden Antwortoptionen aus: <{RESPONSE OPTIONS}>? Wenn
(Step 2: Classification) ja, welche Option wird bevorzugt?

AUSSAGE: {OPEN-ENDED MODEL RESPONSE}

Table 18: Classification Instructions (User Prompts) for German—GLES 2017 & GLES 2025 Datasets. These
prompts are only used in the second, classification step of the Open-Ended Classification and the Open-Ended
Distribution Methods. The user prompts for all other Survey Response Generation Methods, as well as for the first,
open-ended step of the Open Generation Methods are shown in Table 19.

19


Dataset

Persona & Question (User Prompt)

ANES 2016
(English)

Racially, I am {row.race}. {"I like to discuss politics with my family and friends.” if row.discuss_politics == ’Yes’
else "I never discuss politics with my family or friends.”} Ideologically, I am {row. ideology}. Politically, I am an
{row.party}. I {row.church_goer}. I am {int(row.age)} years old. I am a {row.gender}. I am {row.political_interest}
interested in politics. It makes me feel {row.patriotism} to see the American flag. I am from {row.state}.

In the 2016 presidential election, I voted for

GLES 2017
(German)

Ich bin {row.Alter} Jahre alt und {row.Geschlecht}. Ich habe {row.Bildung}, ein {row.Haushaltseinkommen} monatliches
Haushalts-Nettoeinkommen und bin {rowL’Erwerbstatigkeit’]}. Ich bin {rowL’Religiositat’]}. Politisch-ideologisch
ordne ich mich {row[’Links-Rechts-Einstufung’]} ein. Ich identifiziere mich {row[’Parteiidentifikation Starke’ ]}
{rowL’Parteiidentifikation’]}. Ich lebe in {row[’Ost-West’]}. Ich finde, die Regierung sollte die Einwanderung
{rowL’ Zuwanderung’]} und {rowl’Einkommensunterschiede verringern’]} ergreifen, um die Einkommensunterschiede zu

verringern.

Habe ich bei der Bundestagswahl 2017 gewahlt und wenn ja, welcher Partei habe ich meine Zweitstimme gegeben?

GLES 2025
(German)

— same as for 2017 with only the year in the question changed —

ATP 2021
(English)

In politics today, I consider myself a {row.POLPARTY}. I would describe my political views as {row.POLIDEOLOGY}.
My present religion is {row.RELIG}. I am {row.RACE}. The highest level of education I have completed is:
{row.EDUCATION}. Last year, my total family income from all sources, before taxes was {row. INCOME}. I currently
reside in the {row.CREGION}. I identify as {row.SEX}.

{QUESTION}

Table 19: Persona & Question Prompts (User Prompts) for All Datasets. All prompts closely follow the
templates provided by Argyle et al. (2023); Von Der Heyde et al. (2025); Santurkar et al. (2023). During the
evaluations, we omit sentences with missing data on at least one of the variables.

20


Qwen 3 8B Qwen 3 32B
ANES 0
2016 ? g
8
GLES Gee ©
&
2017 g ?
GLES @0 gs
te) ce
2025 °, ’
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
macro avg. F1-score (+) macro avg. F1-score (+)
ATP ©° = oe, o—e a
2021 —, =. =
T — T T T T — T
0.10 0.15 0.20 0.25 0.10 0.15 0.20 0.25

macro avg. Fl-score (—)

Reasoning and Impact
@ reasoning enabled
O reasoning disabled

Survey Response Generation Method

@ First-Token Probabilities @ Restricted Choice

@ First-Token Restricted
Answer Prefix

@ Restricted Reasoning
© Verbalized Distribution

macro avg. Fl-score (—)

— reasoning improves accuracy
reasoning degrades accuracy

@ Open-Ended Classification
@ Open-Ended Distribution
stratified baseline

Figure 6: Individual-Level Accuracy With / Without
Reasoning. Mean results shown for default temperature,
Full Text response option variants. Methods with more
than 1% invalid responses are excluded.

Qwen 3 8B Qwen 3 32B
ANES & OF tr95 2
eo eo
2016 °° e °
GLES co |? -.-%
2017 a 8 e
é °
GLES oe of
2025 é. e @
0.2 0.3 0.4 0.2 0.3 0.4
Total Variation Dist. (<) Total Variation Dist. (<)
ATP —o |, ———
2021 we %
T i _—— T T —— T T
0.04 0.06 0.08 0.10 0.12 0.04 0.06 0.08 0.10 0.12

1-Wasserstein Dist. («)

1-Wasserstein Dist. (<)

Reasoning and Impact
@ reasoning enabled
O reasoning disabled

— reasoning improves alignment
reasoning degrades alignment

Survey Response Generation Method

@ First-Token Probabilities @ Restricted Choice

©@ First-Token Restricted @ Restricted Reasoning
Answer Prefix ® Verbalized Distribution

@ Open-Ended Classification
@ Open-Ended Distribution
stratified baseline

Figure 7: Subpopulation-Level Alignment With /
Without Reasoning. Mean results shown for de-
fault temperature, Full Text response option variants.
Methods with more than 1% invalid responses are ex-
cluded. Reasoning does not consistently improve
subpopulation-level alignment. For individual-level
alignment, see Appendix Figure 6.

21


ANES 2016

Agreement Across
Random Seeds (kT)

First-Token Probabilities
First-Token Restricted

Answer Prefix -
Restricted Choice
Restricted Reasoning 0.86]0.85 0.87
Verbalized Distribution
Open-Ended Classif.

Open-Ended Distrib.

0.85]0.86 0.86
0.89]0.86 0.86
PP LP PF © PF OC PS

2° @ ©” 40 S & &
LL Wo WM LS
FFE PF PF NM oF °

GLES 2017 Agreement Across

Random Seeds (kT)
First-Token Probabilities -
First-Token Restricted
Answer Prefix
Restricted Choice -WZiey
Restricted Reasoning —@ezaqitBr 0.29
Verbalized Distribution -QZ058 © > [etosa ty)

Open-Ended Classif.
Open-Ended Distrib.

0.82] 0.09 0.31 0.65
0.79} 0.02 | 0.42 0.63
QP PP © HP FS PO
& @ go ge ge”
o

SS >
ww OF

GLES 2025 Agreement Across

Random Seeds (xT)

First-Token Probabilities -
First-Token Restricted
Answer Prefix +0
Restricted Choice
Restricted Reasoning —WRENGRE «(ERIE
Verbalized Distribution 4QEEN@EER © -> (Oa)
Open-Ended Classif.
Open-Ended Distrib.

10983) 0.07 0.28 (OF i » 0.70

OB} 0.03 0.34 0.70 0.70

x

@ S & 2

SL s ee SS

SF FE YK OY So os Pi
© se

(a) Individual-Level Agreement Across Seeds. The
First-Token Probability Method and the First-Token Re-
stricted Method achieve perfect agreement across all non-
reasoning models, as the output probabilities of the first
token are deterministic given the same prompt. No agree-
ment across seeds is calculated for the ATP 2021 dataset,
as we evaluated this dataset with only 1 seed to save com-
putational resources.

ANES 2016

Agreement Across
Response Option Scales (kT)

First-Token Restricted
Answer Prefix -

0.14 0.62 0.86]0.86 0.87 0.81 0.80

Verbalized Distribution

Open-Ended Classification
Open-Ended Distribution

GLES 2017 Agreement Across

Response Option Scales (xT)

First-Token Probabilities -
First-Token Restricted
Answer Prefix -ie 30s

-0.09 (GR8G) -0.02

Catia (aeeveM@alel(eeMe 0.31 0.33 /0.84]0.13 0.31 0.71]0.61 061/080
i XokciegrelestoM ctetelal lates 0.18 0.18 /0i8310.03 0.23 |0.69}0.54 0.59 0.67
Verbalized Distribution —RI SRE) © 77 meyers) 0.59 0.62

Open-Ended Classification -QRERREIT-7) RREIEPY 9-7) IEICE Ni
Open-Ended Distribution —(ezateet) 9° Toko a tot) 59 0.59 0.64 0.64

OQ” o rw) <)

WW gO DM

FHKE PF FS 9”
e ol

GLES 2025 Agreement Across

Response Option Scales (kT)

First-Token Probabilities -
First-Token Restricted
Answer Prefix -
i Xetiaa tanto m@larel(eeMe 0.20 0.30 /0.87]0.11 0.19 0.78]0.75 0.61083 079

Xokiag telecom ct-telal are pes 0.18 0.22 |igs}0.02 0.22 |0.76]0.57 0.56 074 075
AV(claeY-][P4s\e Baal olOla(elame 0.28 0.27 |0.831-0.03 0.28 0.73]0.57 0.58 0.62 0.76
Open-Ended Classification -[oesoyieieiy 9"
Open-Ended Distribution (Bier) 7
WYP PY MG KO

Oo Oo Oo oO S &
CL @ SS V8 OS PO SF
vw Ss oF oF oe SS So?

ATP 2021 Agreement Across

Response Option Scales (kT)
First-Token Probabilities - -0.13

First-Token Restricted 0 0. 10.27 -0.13 0.08]0.24 -0.23 0.04 -0.03
Answer Prefix -@ezatuety) 0.01 -0.02 0.26 0.06 0.20
Restricted Reasoning

Verbalized Distribution
Open-Ended Classification -EROr Ss

-0.00'0.45 0.18 0.42

Open-Ended Distribution —GREROeEy TE:

e °” 40 S & @
< Ka WM go OW SLD
RIN Sia RG

Ss x RG
3
model

(b) Individual-Level Agreement Across Response Op-
tion Scales. A sufficient agreement is often desirable, as
perturbations in the response options scales should not
impact the response that is generated by a model.

Figure 8: Individual-Level Agreement. Mean Fleiss’s « (+), results with more than 10% invalid values excluded.
Perfect agreement across seeds or scales might not be considered desirable, as it indicates overly confident individual-
level predictions given the variance in human survey responses. For an evaluation of individual-level calibration,
see also Figure 10.

22


GLES 2017: Restricted Choice GLES 2017: Restricted Choice

Llama 8B OLMo 7B Qwen 8B g Llama 8B OLMo 7B Qwen 8B Fr
0460 045
a 5 a
g =
e ft x i 2
S 9.3 g & 100 038
- i - wa g
2 i
VA il 029 N/A 028
woe ie ie 1.0 1.5 2'0 Wn oe io 1s 2 1.0 1.5 2.0 win ob a0 as a 1.0 1.5 2.0 € Noe oie 1.0 1.5 20 weak 1 1s 2 1.0 1.5 2.0 we oe to as 1.0 1.5 2.0 e
Temperature Temperature Temperature Temperature Temperature Temperature
GLES 2017: Verbalized Distribution GLES 2017: Verbalized Distribution
Llama 8B OLMo 7B Qwen 8B g Llama 8B OLMo 7B Qwen 8B 3
0.4 8 042
5 a 5 fa)
¥ 50 a ¥ 50 g
& 100 03 p S 100 038
F 500 g F 500 g
i -e aes ff ce a5
N/A 0.5 1.0 1.5 2.0 N/A 0.5 1.0 1.5 2.0 N/A 0.5 1.0 1.5 2.0 & N/A 0.5 1.0 1.5 20 win ob 10 1's 2 0.5 1.0 1.5 2.0 we 0.5 1.0 1.5 2.0 e
Temperature Temperature Temperature Temperature Temperature Temperature
GLES 2017: Open-Ended Distribution GLES 2017: Open-Ended Distribution
Llama 8B OLMo 7B Qwen 8B g Llama 8B OLMo 7B Qwen 8B Fr
0.4 8 042
5 a 5 a
¥ 30 e ¥ 50 g
i ay
© 100 03 g © 100 os 3
F 500 g F 500 g
NA 02 § a ce , 025
N/A 0.5 1.0 1.5 2'0 Meco aon 1.0 1.5 2.0 wn os 10 a 1.0 1.5 20 € N/A 0.5 1.0 1.5 2.0 Wao 10 2 1.0 1.5 2.0 oe 1.0 1.5 20 e
Temperature Temperature Temperature Temperature Temperature Temperature

(a) Decoding Strategy Impacts Individual-Level Alignment. (b) Decoding Strategy Impacts Subpopulation-Level Align-

Macro avg. Fl-score (+)—results generally degrade with in- ment. Total variation distance ({.)—results improve for Llama

creasing temperature. 8B and OLMo 7B with the Restricted Choice Method, but are
otherwise mostly stable.

Figure 9: Decoding Strategies. For 3 Survey Response Generation Methods (Restricted Choice, Verbalized
Distribution, and Open-Ended Distribution) and 3 medium-size LLMs (Llama 8B, OLMo 7B, and Qwen 8B), we
investigate a diverse range of temperature and top-k values during decoding. N/A stands for greedy decoding or
full vocabulary respectively. Results with more than 10% invalid values excluded.

23


Individual-Level Calibration of Individual-Level Calibration of

ANES 2016 Answer Option Probabilities (1) GLES 2017 Answer Option Probabilities (1)
First-Token Probabilities - 0.98 0.99 0.76 0.82 0.84 EE} os 0.76 0.66 First-Token Probabilities -
First-Token Restricted - 0.98 0.98 0.89 0.76 0.82 0.82 1.05 0.76 First-Token Restricted 5 1.38 | 1.17 0.83 1.09 0.80 a 0.80 1.05
Answer Prefix - 0.74 0.67 0.86 1.00 0.75 0.74 0.83 Answer Prefix - 0.89 0.88 0.84 0.95 0.92 0.87 1.14 0.74 0.89
Restricted Choice - not applicable Restricted Choice - not applicable
Restricted Reasoning - not applicable Restricted Reasoning - not applicable

Verbalized Distribution - 0.91 0.74

0.76 070 0.70 0.67 ia) 0.60| Verbalized Distribution - 0.82 0.77 0.64 0.95 0.84 0.70 0.72 0.68 0.67 0.63

Open-Ended Classif. - not applicable Open-Ended Classif. - not applicable
Open-Ended Distrib. - 0.88 0.83 0.79 0.79 0.74 0.73 0.88 0.88 0.80 0.80 Open-Ended Distrib. - 1.01 1.00 0.64 0.97 0.84 0.69 0.92 0.91 0.74 0.74
Ce es le ha ee ee
DP VM VP VY YW YB VW OC Ww OQ DS VM VP VY YW YB Y PB YW
OW WN WY AN COMENS e Ww MO SY AN CANS “>
FP OLS OS PL PP OLS OS SS
REN se FMP SS of s WPF EK Y OF S e s
[eo of [os of
Individual-Level Calibration of Individual-Level Calibration of
GLES 2025 Answer Option Probabilities (1) ATP 2021 Answer Option Probabilities (1)
First-Token Probabilities - 12 0.90 0.90 First-Token Probabilities - 0.93 0.92 |= 0.89 1.10 fas gEC 1.25
First-Token Restricted 5137 Fey 0.76 1.05 0.74 0.98 O71 First-Token Restricted - 0.92 0.95 | =)/1.15 0.89 1.07 77) 75) 1.12 0.99
Answer Prefix - 0.90 0.88 0.80 0.95 0.93 0.81 0.94 0.66 0.80 Answer Prefix - 0.88 0.92 0.93 1.07 0.85 0.94 | 7 0.95 |1.11
Restricted Choice - not applicable Restricted Choice - not applicable
Restricted Reasoning - not applicable Restricted Reasoning - not applicable

Verbalized Distribution - 0.79 0.80 [BF] 0.97 0.86 | 0.67 (XT) 0,63 | 0.59 Verbalized Distribution - 0.90 0.97 0.86 0.97 0.95 0.80 0.85 0.83 0.80 0.77

Open-Ended Classif. - not applicable Open-Ended Classif. - not applicable

Open-Ended Distrib. - 1.00 1.01 2 0.98 0.85 Ea 0.83 0.83 Open-Ended Distrib. g 1% Hie 1.03 1.01 1.03 0.99
rood ee rood ie ee ee

DT VM VP Y LW GY WB LOC Ww DP VD VY VY LW Ww
e e 2» oY e & a @ e & e ze e RS a € @
FFP es o Ef ae we cS ee S o
a
ley

Figure 10: Individual-Level Calibration (|). Mean Brier score (|) across all response options, results with
more than 10% invalid values excluded. For Survey Response Generation Methods that generate individual-level
distributions across response options, we can evaluate whether high “confidence” of a model in a response option
corresponds with correct predictions on the individual level. The Brier score calculates individual-level calibration
as the mean squared error between the confidences and the one-hot encoded human survey responses. Well-
calibrated Survey Response Generation Methods are desirable, as they accurately capture individual-level prediction
uncertainty. We find that larger models are generally better calibrated than smaller ones. Token Probability-Based
Methods can be poorly calibrated for most models, while the Verbalized Distribution Method leads to the best
individual-level calibration.

24


ANES dataset Responses (share) Gg invalid © partially valid Ml valid

Llama 3B Llama 8B Llama 70B OLMo 1B OLMo 7B OLMo 32B Qwen 8B Qwen 8B (R)

re)
=
o
=]
w
N
wo
Qa
=
o
5}
w
N
wo
2

First-Token Probabilities I I) (i i [i i i es
First-Token Restricted i i] i I es (ee
Answer Prefix i i SS SS eS eS es ee ee ee

Restricted Choice i LS ES I SS a es ee
Restricted Reasoning] TT LTT [LTT LT LT Li Lh hl
Verbalized Distributio! TTT LTT LT Li Li Li Lh i i
Open-Ended Classificatio! I [I [i [i [i i i i i
Open-Ended Distribution TTT! [ [i [i i i i ES
GLES 2017 dataset Responses (share) Mall invalid 8 partially valid Mm valid
Llama 3B Llama 8B Llama 70B OLMo 1B OLMo 7B OLMo 32B Qwen 8B) Qwen 8B (R) Qwen32B Qwen 32B (R)

First-Token Probabilities A i i a a a a a
First-Token Restricted Ls Sy SS ES ES
Answer Prefix i i Sy ee ee ee ee

Restricted Choice i A Ly Sy SS ES a ee ee
Restricted Reasoning TT TT LT Li Li Lh Lh Lh LS ee
Verbalized Distribution TT LT LT LT i i Ll
Open-Ended Classification T/T DTT LT Li Li Li Ll hh
Open-Ended Distribution TT! i [i [i i i i ES
GLES 2025 dataset Responses (share) Gm invalid © partially valid @ valid
Llama 3B Llama 8B Llama 70B OLMo 1B OLMo 7B OLMo 32B Qwen 8B Qwen8B(R) Qwen32B Qwen 32B (R)

FirstToken Probabilities 0 (EE
First-Token Restricted = i] i I i es | es
Answer Prefix i i i ee es eee eee

Restricted Choice LD A LS SS SS ES ES SS
Restricted Reasoning) TT LTT LT Li LT Lh Lh ht
Verbalized Distributio!) IT [Ei [i [i [i i i i i
Open-Ended Classification TT DT LT Li i Ll Ll
Open-Ended Distribution TT I i i i i
ATP dataset Responses (share) MEM invalid © partially valid Mm valid
Llama 3B Llama 8B Llama 70B OLMo 1B OLMo 7B OLMo 32B Qwen 8B Qwen8B(R) Qwen32B Qwen 32B (R)

isthe rhs = — i a a Ga Gees Gea Gay Gey
First-Token Restricted TTT DTT LT Li Li Li Ll LS LS
Answer Prefix i i Sy SS ee ee

Restricted Choice A ES SS a es ee
Restricted Reasoning A LS a a a a a
Verbalized Distributio!) IT [i [i [i [i i i i i
Open-Ended Classification I [i [i [i [i i i i i
Open-Ended Distribution I [i [i [i i i i i i

Figure 11: Fractions of LLM Responses That Are Invalid. We count model output that does not contain any
of the possible closed-ended response options as invalid. For Survey Response Generation Methods that generate
distributions over possible response options (see Table 1), we count model output that only contains probabilities
for some of the response options as partially valid.

25
