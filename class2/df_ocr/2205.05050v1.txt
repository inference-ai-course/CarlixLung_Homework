arX1v:2205.05050v1 [cs.CL] 10 May 2022

White-box Testing of NLP models with Mask Neuron Coverage

Arshdeep Sekhon and Yangfeng Ji and Matthew B. Dwyer and Yanjun Qi
University of Virginia, USA

Abstract

Recent literature has seen growing interest in
using black-box strategies like CHECKLIST
for testing the behavior of NLP models. Re-
search on white-box testing has developed a
number of methods for evaluating how thor-
oughly the internal behavior of deep models
is tested, but they are not applicable to NLP
models. We propose a set of white-box testing
methods that are customized for transformer-
based NLP models. These include MASK NEU-
RON COVERAGE (MNCOVER) that measures
how thoroughly the attention layers in models
are exercised during testing. We show that MN-
COVER can refine testing suites generated by
CHECKLIST by substantially reduce them in
size, for more than 60% on average, while re-
taining failing tests — thereby concentrating the
fault detection power of the test suite. Further
we show how MNCOVER can be used to guide
CHECKLIST input generation, evaluate alter-
native NLP testing methods, and drive data
augmentation to improve accuracy.

1 Introduction

Previous NLP methods have used black-box test-
ing to discover errors in NLP models. For instance,
Checklist(Ribeiro et al., 2020) introduces a black-
box testing strategy as a new evaluation methodol-
ogy for comprehensive behavioral testing of NLP
models. CheckList introduced different test types,
such as prediction invariance in the presence of
certain perturbations.

Black-box testing approaches, like Checklist,
may produce distinct test inputs that yield very
similar internal behavior from an NLP model. Re-
quiring that generated tests are distinct both from
a black-box and a white-box perspective — that
measures test similarity in terms of latent repre-
sentations — has the potential to reduce the cost
of testing without reducing its error-detection ef-
fectiveness. Researchers have explored a range of
white-box coverage techniques that focus on neu-

ron activations and demonstrated their benefit on
architecturally simple feed-forward networks (Pei
et al., 2017; Tian et al., 2018; Ma et al., 2018a;
Dola et al., 2021). However, transformer-based
NLP models incorporate more complex layer types,
such as those computing self-attention, to which
prior work is inapplicable.

In this paper, we propose a suite of white-box
coverage metrics. We first adapt the k-multisection
neuron coverage measure from (Ma et al., 2018a)
to Transformer architectures. Then we design a
novel MNCOVER coverage metric, tailored to NLP
models. MNCOVER focuses on the neural modules
that are important for NLP and designs strategies to
ensure that those modules’ behavior is thoroughly
exercised by a test set. Our proposed coverage
metric, when used to guide test generation, can
cost-effectively achieve high-levels of coverage.

Figure 1 shows one example of how MNCOVER
can work in concert with CheckList to produce a
small and effective test set. The primary insight
is that not all text sentences contain new informa-
tion that will improve our confidence in a target
model’s behavior. In this list, multiple sentences
were generated with similar syntactic and seman-
tic structure. These sentences cause the activation
of sets of attention neurons that have substantial
overlap. This represents a form of redundancy in
testing an NLP model. Coverage-based filtering
seeks to identify when an input’s activation of at-
tention neurons is subsumed by that of prior test
inputs — such inputs are filtered. In the Figure the
second and third sentences are filtered out because
their activation of attention neurons is identical to
the first test sentence. As we show in §4 this form
of filtering can substantially reduce test suite size
while retaining tests that expose failures in modern
NLP models, such as BERT.

The primary contributions of the paper lie in:

¢ Introducing MNCOVER a test coverage metric
designed to address the attention-layers that


Test Set
Selection
using

Coverag

Figure 1: Example of our proposed MNCOVER’s filtering on a
set of test examples.

are characteristic of NLP models and to ac-
count for the data distribution by considering
task-specific important words and combina-
tions.

Demonstrating through experiments on 2 NLP
models (BERT, Roberta), 2 datasets (SST-2,
QQP), and 24 sentence transformations that
MNCOVER can substantially reduce the size of
test sets generated by CheckList, by 64% on
average, while improving the failure detection
of the resulting tests, by 13% on average.
Demonstrating that MNCOVER provide an ef-
fective supplementary criterion for evaluating
the quality of test sets and that it can be used
to generate augmented training data that im-
proves model accuracy.

2 Background

Coverage for testing deep networks The research
of Coverage testing focuses on the concept of "ade-
quacy criterion" that defines when “enough” testing
has been performed. The white-box coverage test-
ing has been proposed by multiple recent studies to
test deep neural networks (Pei et al., 2017; Maetal.,
2018a,b; Dola et al., 2021). DeepXplore (Pei et al.,
2017), a white-box differential testing algorithm,
introduced Neuron Coverage for DNNs to guide
systematic exploration of DNN’s internal logic. Let
us use D to denote a set of test inputs (normally
named as a test suite in behavior testing). The Neu-
ron Coverage regarding D is defined as the ratio
between the number of unique activated neurons
(activated by D) and the total number of neurons
in that DNN under behavior testing. A neuron is
considered to be activated if its output is higher
than a threshold value (e.g., 0). Another closely re-
lated study, DeepTest (Tian et al., 2018), proposed
a gray-box, neuron coverage-guided test suite gen-
eration strategy. Then, the study DeepGauge (Ma
et al., 2018a) expands the neuron coverage defini-
tion by introducing the kmultisection neuron cover-
age criteria to produce a multi-granular set of DNN
coverage metrics. For a given neuron n, the kmulti-
section neuron coverage measures how thoroughly

a given set of test inputs like D covers the range
[lown, highn]. The range [lown, highy] is divided
into k equal bins (i.e., k-multisections), for k > 0.
For D and the target neuron n, its k-multisection
neuron coverage is then defined as the ratio of the
number of bins covered by D and the total number
of bins, i.e., k. For an entire DNN model, the k-
multisection neuron coverage is then the ratio of all
the activated bins for all its neurons and the total
number of bins for all neurons in the DNN.

Transformer architecture NLP is undergoing a
paradigm shift with the rise of large scale Trans-
former models (e.g., BERT, DALL-E, GPT-3)
that are trained on unprecedented data scale and
are adaptable to a wide range of downstream
tasks(Bommasani et al., 2021).

These models embrace the Transformer archi-
tecture (Vaswani et al., 2017) and can capture
long-range pairwise or higher-order interactions
between input elements. They utilize the self-
attention mechanism(Vaswani et al., 2017) that en-
ables shorter computation paths and provides paral-
lelizable computation for learning to represent a se-
quential input data, like text. Transformer receives
inputs in the general form of word tokens. The se-
quence of inputs is converted to vector embeddings
that get repeatedly re-encoded via the self-attention
mechanism. The self-attention can repeat for many
layers, with each layer re-encoding and each layer
maintaining the same sequence length. At each
layer, it corresponds to the following operations to
learn encoding of token at position 7:

aq’ = softmax ((W%h;) ' (W*h,)/Vd) (1)

M
j=l
hi, _ o(hyw" + bi) W° + bo. (3)

Here W* is the key weight matrix, W4 is the query
weight matrix, W” is the value weight matrix, W”
and W? are transformation matrices, and b; and
bz are bias vectors.

3 Method

State-of-the-art NLP models are large-scale with
millions of neurons, due to large hidden sizes and
multiple layers. We propose to simplify and view
these foundation models (Bommasani et al., 2021)
through two levels of granularity: (1) Word Level:
that includes the position-level embeddings at each


Activated

False

MA +MA

MA +MA+MA+MA
MA
—____________ test
MA +MA+ MA +MA put
WORD

impoRTANcEfm

MASK

VanillaCoverage=

MaskedCoverage=

layer and (2) Pairwise Attention Level: that in-
cludes the pairwise self-attention neurons between
two positions at each layer. In the rest of this pa-
per, we denote the vector embeddings at location i
for layer | as h’ : and name these as the word level
neurons at layer |. We also denote the a) at layer
l and head h as any ;,. and call them as the attention
level neurons at layer I.

3.1 Extending Neuron Coverage (COVER) for
Testing NLP Model
Now we use the above two layers’ view we pro-
posed, to adapt the vanilla neuron coverage con-
cepts from the literature to NLP models. First,
we introduce a basic definition: "activated neuron
bins" (Ma et al., 2018b):

Definition 1 Activated Neuron Bins (ANB): For
each neuron, we partition the range of its values
(obtained from training data) into B bins/sections.
We define ANB for a given text input if the input’s
activation value from the target neuron falls into
the corresponding bin range.

Then we adapt the above definition to the NLP
model setting, by using the after-mentioned two
layers’ view. We design two phrases: Word Neu-
ron Bins, and Activated Word Neuron Bins in the
following Definition (2).

Definition 2 Activated Word Neuron Bins(AWB):
We discretize the possible values of each neuron in
hi! (whose d-th embedding dimension is h’¥,) into
B sections. We propose a i function Ow who takes
two arguments, as by (hl? dt X) for a given input x.
bw hile. x) = 1 if it is an activated word neuron
bin (shortened as AWB), else 0 if not activated.

Similarly, for our attention neuron at layer J,
head k, word position 7 and position j: alk, we
introduce the definition of "attention neuron bins"
and "Activated Attention Neuron Bins" in the fol-

sweet

Figure 2: A visual depiction of MNCOVER for

Masked te False C b; — Nyy — — Atentioh ij
overage FE : mae
= = —_— IN WS PES] Asweetl | SSS LT ks: i
~ WEE = Bea VS & tn
: —_—_ By Ree 1 —— — : Cesc Cara Grsacxe story

masking neurons to measure coverage.

lowing Definition (3).

Definition 3 Activated Attention Neuron Bins
(AAB): We discretize the possible values of neuron
air into B sections. We denote the state of the b‘”
iacaon ef this attention neuron using balaid?, ae
balaj,’,x) = 1 if it is an activated attention
neuron bin (denoted by AAB) by an input x and

balaid?, x) = 0 ifnot activated.
N(AWB(x)) = So dw(ha?.x) 4)
ltdb
N(AAB(x)) =} Galaje’sx) — )
ijbk

The coverage, denoted by COVER, of a dataset
T for a target model is then defined as the ratio
between the number of “activated" neurons and
total neurons:

N(AWB)
N(WB)

+ \N(AAB)

COVER = + \N(AB)

(6)

Here, A is a scaling factor.

Now let us assume the total number of layers
be D, total number of heads H, maximum length
L, total bins B and total embedding size be E.
Considering the example case of the BERT(Devlin
et al., 2019) model, total number of word level
neurons to be measured are then L x EF x D =
128 x 768 x 13 ~ 0.1million. The total number of
the attention level neurons is then Lx Dx Hx D =
128 x 128 x 12 x 12 ~ 2million.

3.2 MASK NEURON COVERAGE (MNCOVER)

However, accounting for every word and atten-
tion neuron’s behavior for a large pre-trained model
like BERT is difficult for two reasons: (1). If we
desire to test each neuron at the output of all trans-
former layers in each BERT layer, we need to ac-
count for the behavior of every neuron, which for a


;
=

TEST MODEL f

(FIXED PARAMETERS)

INPUT LEARNT INTERACTION

MASK My

LEARNT WORD
MASK My

Figure 3: Learning the masks prior to testing: Globally impor-
tant words and interactions are learnt by masking inputs to a
target model.

large pre-trained model like BERT is in the order
of millions.

(2). If we test every possible neuron, we need to
track many neurons that are almost irrelevant for a
target task and/or model. This type of redundancy
makes the behavior testing less confident and much
more expensive.

To mitigate these concerns, we propose to only
focus on important words and their combinations
that may potentially contain ‘surprising’ new infor-
mation for the model and hence need to be tested.

We assume we have access to a word level
importance mask, denoted by Mw € {0, 1}!V
and the interaction importance mask by My €
{0, IV*IVl_ Bach entry in My, € {0, 1} rep-
resents the importance of word w;. Similarly, each
entry in Ma4x,x,; € {0, 1} represents the impor-
tance of interaction between token w;, and We;-
These masks aim for filtering out unimportant
tokens (and their corresponding neurons at each
layer) for measuring coverage signals. We apply
the two masks at each layer to mask out unim-
portant attention pairs to prevent them from being
counted towards coverage calculation. With the
masks, the AWB and AAB are revised and we then
define MASK NEURON COVERAGE (MNCOVER)
accordingly:

N(Mask-AWB(x)) = S> Mbox, * bw(hd? X)
ltdb
N(Mask-AAB(x)) = S> Max, x; * ba(ajp’ sx)
igkb
ee = N(Mask-AWB) + AN(Mask-AAB)
N(WB) + AN(AB)
(7)

3.3 Learning Importance Masks

In this section, we explain our mask learning
strategy that enables us to learn globally impor-
tant words and their pairwise combinations for
a model’s prediction without modifying a target
model’s parameters.

We learn the two masks through a bottleneck
strategy, that we call WIMASK layer. Given a tar-
get model f, we insert this mask bottleneck layer

between the word embedding layer of a pretrained
NLP model and the rest layers of this model. Fig-
ure 3 shows a high level overview of the mask
bottleneck layer. Using our information bottleneck
layer, we learn two masks : (1) a word level mask
Ma, (2) an interaction mask My.
Learning Word-Pair Interaction Importance
Mask: My, The interaction mask aims to discover
which words globally interact for a prediction task.
We treat words as nodes and represent their in-
teractions as edges in an interaction graph. We
represent this unknown graph as a matrix M4 =
{Maij}yyy- Each entry Max, x, € {0,1} is
a binary random variable, such that My;; ~
Sigmoid(A;;), follows Bernoulli distribution with
parameter Sigmoid(;;). M.4;; specifies the pres-
ence or absence of an interaction between word 2
and word j in the vocabulary V. Hence, learning
the word interaction graph reduces to learning the
parameter matrix A = {A;;}vxv. In Section 3.3.1,
we show how J (and therefore M4) is learned
through a variational information bottleneck loss
formulation (details in Section (A.2)).

Based on the learnt interaction mask M 4, each
word embedding x; is revised using a graph based
summation from its interacting neighbors’ embed-

ding xj, 7 € N(i):
w) (8)

/
eam to( ty

o(-) is the ReLU non-linear activation function and
W c¢ R”¥*# is a weight matrix. We denote the
resulting word representation vector as e;. Here
j € N(2), and N(i) denotes those neighbor nodes
of x; on the graph My and in x. Eq. (8) is mo-
tivated by the design of Graph convolutional net-
works (GCNs) that were introduced to learn useful
node representations that encode both node-level
features and relationships between connected nodes
(Kipf and Welling, 2016). Differently in our work,
we need to learn the graph M4, through the A pa-
rameter. We can compute the simultaneous update
of all words in input text x together by concatenat-
ing all e’. This gives us one matrix E’ € Myr" ;
where L is the length of input and #7 is the embed-
ding dimension of x;.

; Xj

t oa

Learning Word Importance Mask: My This
word mask aims to learn a global attribution word
mask My. Aiming for better word selection,
Myy is designed as a learnable stochastic layer


with My € {0,1}”. Each entry in My (e.g.,
Mw. € {0,1} for word w) follows a Bernoulli
distribution with parameter p,,. The learning re-
duces to learning the parameter vector p.

During inference, for an input text x, we get a
binary vector My, from My that is of size L.
Its i-th entry Myx, € {0,1} is a binary random
variable associated with the word token at the 7-th
position. My, denotes how important each word
is in an input text x. Then we use the following
operation (a masking operation) to generate the fi-
nal representation of the 7-th word: z; = My x,e’.
We then feed the resulting Z to the target model f.
3.3.1. Learning Word and Interaction Masks

for a target model f/:

During training, we fix the parameters of target
model f and only train the WIMASK layerto get
two masks.

We learn this trainable layer using the following
loss objective, with the derivation of each term
explained in the following section:

L(x, f(x), y) = rx) ,g + Bsparset sparse
+ Bilpriormwy,. + Bglprionnn , (9)

First, we want to ensure that model predictions
with WIMASK layer added are consistent with the
original prediction f(x). Hence, we minimize the
cross entropy loss ¢ f(x), between f(x) and the
newly predicted output 7 (when with the bottleneck
layer).

Then sparse iS the sparsity regularization on
Max, (priormy, is the KL divergence between
My and a random bernoulli prior. Similarly,
Cpriorm., is the KL divergence between M 4 anda
random bernoulli prior. We provide detailed deriva-
tions in Section A.2.

4 Experiments

Our experiments are designed to answer the follow-
ing questions:

1. Will a test set filtered by MNCOVER find more
errors from a SOTA NLP model?

2. Does MNCOVER achieve test adequacy faster,
i.e. achieve higher coverage in fewer samples?

3. Does MNCOVER help us compare existing
testing benchmarks?

4. Can MNCOVER help us automatically select
non-redundant samples for better augmenta-
tion?

Datasets and Models We use pretrained model
BERT-base(Devlin et al., 2019) and RoBERTa-
base(Liu et al., 2019) provided by (Morris et al.,
2020) finetuned on SST-2 dataset and Quora Ques-
tion Pair (QQP) dataset. For the QQP dataset, we
use the model finetuned on the MRPC dataset. We
train a word level mask (My) and an interaction
mask (M_,4) for each of these settings. We use a
learning rate of le — 05, 8; = 0.001, 6, = 0.001,
and 6, = 0.001 for all models.

We have provided the test accuracy of the target
models and the models trained with masks in Ta-
ble 5. Note that the ground truth labels here are
the predictions from the target model f without the
WIMASK layer, as our goal is to ensure fidelity of
the WIMASK +f to the target model f. Table 5
shows that training the WIMASK +f model main-
tains the target model’s predictions f as indicated
by higher accuracies.

4.1 Experiment 1: Removing Redundant Test
Inputs during Model Testing

Motivation CHECKLIST (Ribeiro et al., 2020) pro-
vides a method to generate a large number of test
cases corresponding to a target template. It intro-
duces different transformations that can be used
to generate samples to check for a desired be-
havior/functionality. For example, to check for
a model’s behavior w.r.t typos in input texts, it gen-
erates examples with typos and queries the target
model. CHECKLIST then compares failure rates
across models for the generated examples to iden-
tify failure modes. However, it does not provide a
method to quantify the number of tests that need
to be generated or to determine which examples
provide the most utility in terms of fault detection.
Such a blind generation strategy may suffer from
sampling bias and give a false notion of a model’s
failure modes.

Setup We evaluate MNCOVER’s ability to redun-
dant test samples out of the generated tests for a
target model. The different transformations used
are summarized in Column I of Table 1, Table 2,
and Table 6. We measure failure rate on an ini-
tial test set of size N = 1500. We then filter the
generated tests based on our MNCOVER coverage
criteria: if adding a test example into a test suite
does not lead to an increase in its coverage, it is
discarded. We then measure failure rate of the new
filtered test set.

ResultsIn Table 1, we observe that MNCOVER
can select more failure cases for the BERT model


‘Test Transformation Name Failure Rate (%) Dataset Size Reduction (%)

D D+COVER D+MNCOVER  Ap-ymycover | D#COVER D+MNCOVER
change names 5.14 100.00i6.86 100.00i6.86 94.8611.73 62.84 62.84
add negative phrases 6.80 100.0019.16 99.3419.16 92.5410.36 75.40 75.99
protected: race 68.00 100.0073 02 99.3772.13 31.374.13 44.33 43.54
used to,but now 29.87 53.3046.27 53.3046.75 23.4316.88 84.61 84.61
protected: sexual 86.83 100.00g7.31 100.00gs.99 13.172.15 83.83 86.83
change locations 8.69 21.4338.31 21.438 31 12.74_0.38 86.47 86.93
change neutral words with BERT 9.80 20.0013.04 20.8413 39 11.043 59 73.40 73.88
contractions 2.90 5.523 94 8.224 34 5.321.44 34.80 34.80
2 typos 11.60 18.0011.14 16.00i0.87 4.40_0.73 10.00 10.00
change numbers 3.20 7.14394 7.14329 3.940.09 87.70 88.44
typos 6.60 10.006.45 10.006.24 3.40_0.36 10.00 10.00
neutral words in context 96.73 100.0096.91 100.009¢6.82 3.270.09 21.33 28.50
protected: religion 96.83 100.0093.42 100.0093 02 3.17119 89.67 89.67
add random urls and handles 15.40 14.639.17 17.8611.56 2.46_-3.84 87.60 87.60
simple negations: not neutral is still neutral 97.93 100.00o3.49 100.00os.34 2.070.41 45.67 46.03
simple negations: not negative 10.40 12.009.56 12.429 90 2.02_0.50 80.11 80.37
my opinion is what matters 41.53 42.4936.79 43.1436.79 1.61_4.74 84.16 84.32
punctuation 5.40 6.805.37 6.806.26 1.400.86 10.00 OT
Q &A: yes 0.40 1.330.50 1.710.49 1.316.09 82.33 83.14
Q & A: no 85.20 86.3376 42 86.2277.16 1.02-8.04 82.34 83.12
simple negations: negative 6.13 7.335.63 7.125.70 0.99-0.44 78.63 78.71
reducers 0.13 0.27.10 1.03 0.10 0.90_0.03 67.00 66.77
intensifiers 1.33 1.050.31 1.831.01 0.49_0.33 66.65 66.96
Average Improvement - 13.511.10 13.781.55 13.781.55 62.99 63.57

Table 1: Failure Rate(%) obtained using BERT model on the original dataset D, the dataset filtered using COVER coverage
(D+COVER columns) and the dataset filtered with MNCOVER coverage (D+MNCOVER columns) from the Sentiment Test Suite.
We report both the max failure rate as well as the mean in the subscript across 10 thresholds of coverage. Rows are sorted
regarding the failure rate difference between the dataset filtered using MNCOVER and the original dataset (column A p+mvcover)-

across all 23 transformation on SST-2 dataset.
On average, both MNCOVER and COVER can
help reduce more than 60% of the test suite size,
and MNCOVER achieves a slight advantage over
COVER. In Table 2, when using RoBERTa model,
MNCOVER based filtering wins over the original
dataset in 21 of 23 cases. The average improvement
of MNCOVER regarding error detection is 7.29% on
RoBERTa model and 13.78% on BERT model. We
include similar results on QQP dataset in Table 6.

4.2 Experiment 2: Achieving Higher Dataset
Coverage with Fewer Data Points

Motivation We revisit the question: given a test
generation strategy, does adding more test samples
necessarily add more information? In this set of
experiments, we appeal to the software engineering
notion of “coverage” as a metric of test adequacy.
We show that we can reach a target level of test ad-
equacy faster, i.e. a higher coverage, hence achiev-
ing more rigorous behavior testing, with fewer test
examples, by using coverage as an indicator of
redundant test samples.

Setup: We use the training set as seed examples
and generate samples using transformations used
in the previous experiment listed in Table 1. Sim-
ilar to the previous set of examples, we disregard
an example if the increase in its coverage is be-
low threshold. We vary these threshold values
€ {le—04, le—03, le —02, le—01, 0.0}. Higher
the threshold, more number of examples get fil-

tered out.

Results: In Figure 4, we show that using our cover-
age guided filtering strategy, we are able to achieve
coverage with a fewer number of samples than
without coverage based filtering.

Even with a threshold of 0.0, we are able
to significantly reduce the number of samples
that achieve the same coverage as the unfiltered
set: we are able to achieve an average reduc-
tion across transformations (higher the better) of
71.17%, 45.94%, 28.52%, 11.33% and 2.83% for
{0.0, le—04, le—03, le —02, le —01} thresholds
respectively.

4.3 Experiment 3: MNCOVER as a Metric to
Evaluate Testing Benchmarks

Motivation: In this set of experiments, we utilize
coverage as a test/benchmark dataset evaluation
measure. Static test suites, such as the GLUE
benchmark, saturate and become obsolete as mod-
els become more advanced. To mitigate the satu-
ration of static benchmarks with model advance-
ment, (Kiela et al., 2021) introduced Dynabench,
a dynamic benchmark for Natural Language Infer-
ence(NLI). Dynabench introduced a novel human-
and-model-in-the-loop dataset, consisting of three
rounds that progressively increase in difficulty and
complexity. This results in three sets of training,
validation and test datasets, with increasing com-
plexity testing datasets. We use MNCOVER as an
additional validation measure for the datasets.


‘Test Transformation Name Failure Rate (%) Dataset Size Reduction (%)

D D+COVER D+MNCOVER  Ap+mncover | DHCOVER D+MNCOVER
Q&A: yes 46.20 100.0051.45 100.0051.45 53.805.25 82.37 81.58
protected: race 61.67 100.00¢66.00 100.0066 .00 38.334,34 44,33 44.33
neutral words in context 80.87 100.00g3.43 100.00g3.31 19.139.44 21.27 21.39
protected: religion 73.00 74.1962.31 85.7163.64 12.71_9.36 89.67 89.67
protected: sexual 91.00 100.00g4.25 100.00g4.25 9.00_-6.75 83.83 83.83
simple negations: not neutral is still neutral 91.53, 100.00o2.93 100.00o2.24 8.470.71 45.87 46.11
add negative phrases 29.60 36.4623.15 36.7203.15 7.12_6.45 75.40 75.40
Q&A: no 57.53 61.6753.60 62.4153. 60 4.87_3.93 82.31 82.31
simple negations: not negative 95.40 100.0096.11 100.0095.92 4.600.52 80.14 80.27
2 typos 5.20 6.405.10 7.335.33 2.130.13 0.00 10.00
change neutral words with BERT 9.20 11.117.81 11.117.48 1.91_1.72 73.40 72.45
intensifiers 1.13 2.68169 2.681.387 1.550.73 66.65 66.46
change names 4.53 4.88957 5.812.37 1.289.416 62.84 62.84
punctuation 4.80 6.004.03 6.003 .96 1.20_0.84 10.00 10.00
change locations 6.16 7.325.28 7.32444 1.16-1.72 86.47 87.43
typos 3.00 3.601.90 4.00239 1.00_-0.61 10.00 10.00
simple negations: negative 1.33 2.00.07 1.99120 0.65_0.13 78.63 78.68
used to,but now 52.73 53.3046.27 53.3045.58 0.56_7.16 84.61 83.74
my opinion is what matters 56.47 59.0050.32 56.86.9.87 0.39-6.60 84.20 84.20
contractions 1.00 1.37.74 1.370.63 0.37_0.37 34.80 34.80
reducers 0.40 0.340.14 0.570.207 0.17-0.13 67.05 66.35
change numbers 2.50 1.630.48 2.410.72 —0.09_1.78 87.70 87.70
add random urls and handles 11.40 8.335.22 8.654.55 —2.75_6.85 87.60 88.18
Average Improvement - 6.68_-1.10 7.29-1.85 7.29-1.85 63.01 62.95

Table 2: Failure Rate(%) obtained using RoBERTa model on the original dataset D, the dataset filtered using COVER coverage
(D+COVER columns) and the dataset filtered with MNCOVER coverage (D+MNCOVER columns) from the Sentiment Test Suite.
We report both the max failure rate as well as the mean in the subscript across 10 thresholds of coverage. Rows are sorted
regarding the failure rate difference between the dataset filtered using MNCOVER and the original dataset (column A p+mvcover)-

Test Set MNCOVER
Al 0.175
Al+A2 0.182
Al+A2+4+A3 0.185
A2 0.179
A3 0.181

Table 3: MNCOVER Values on the Dynabench test sets
Setup: We test the ROBERTA-Large model pro-
vided by Dynabench trained on training data from
all three rounds of the benchmark. We use 10 as
the number of bins and \ = 1.0.

Results: We measure coverage achieved by each
of the test sets individually as well as in combina-
tion. We have summarized the results in Table 3.
The test sets indeed provide more novel test inputs
to the model as indicated by the increasing cover-
age as the test sets from each split are taken into
consideration. The low values arise from a large ar-
chitecture, (24-layer, 1024-hidden, 16-heads) that
is potentially still unexplored with 1000 samples
from each test set.

4.4 Experiment 4: Coverage Guided
Augmentation

Motivation: Data augmentation refers to strategies

for increasing the diversity of training examples

without explicitly collecting new data. This is usu-

ally achieved by transforming training examples

using a transformation. A number of automated

approaches have been proposed to automatically se-
lect these transformations including like (Xie et al.,
2019). Since computing MNCOVER does not re-
quire retraining, and the input selection can indicate
the usefulness of a new sample, we propose to use
MNCOVER to select transformed samples, in order
to add them into the training set for improving test
accuracy.

Setup: In this set of experiments, we focus on us-
ing coverage to guide generation of augmented
samples. We propose a greedy search algorithm
to coverage as guide to generate a new training
set with selected augmentations. The procedure
is described in Algorithm 1 and is motivated by a
similar procedure from (Tian et al., 2018). This
is a coverage-guided greedy search technique for
efficiently finding combinations of transformations
that result in higher coverage. We use transforma-
tions described in Section (A.3) and BERT model
pretrained on the datasets.

We then add the coverage selected samples into
the training set and retrain a target model. Using
BERT model as base, Table 4 shows the test ac-
curacy, when with or without adding the selected
samples into the training set. We also show the
size of the augmentation set. Our results show that
using MNCOVER to guide data augmentation can
improve test accuracy in both SST-2 and QQP.


change numbers

change names typos 1

— Max Value
8 === Alpha: 1.
— Alpha:
— Alpha
— Alpha: 1.0 wi
GH — Alpha:

— Alpha ith Filtering threshold le

0 10000 20000 30000 40000 0 200
Number of Examples

change numbers

400
Number of Examples

change names

600 800 0 10000 20000 +=» 30000» 40000» 50000
Number of Examples

0 10000 20000 30000 40000 0 200
Number of Examples

00
Number of Examples

600 800 0 10000 20000» 30000» 40000» 50000
Number of Examples

Figure 4: MNCOVER is able to achieve higher coverage with a fewer number of samples than without coverage based filtering.
For this experiment we use the ROBERTa model. In the top row, we do not shuffle the examples and the bottom row with
shuffling. Even with a threshold of 0.0, we are able to significantly reduce the number of samples that achieve the same coverage
as the unfiltered set: we are able to achieve an average reduction (across transformations) of 28.83%, 54.06% , 71.48% , 88.67%

and 97.17% for {0.0, le — 04, le — 03, le — 02, le
Dataset | Coverage Test Size of
Threshold | Accuracy | Augmented Set
Baseline 90.22 0
SST-2 Random 90.45 6541
MNCOVER 90.41 6541
Baseline 90.91 0
QQP Random 90.96 14005
MNCOVER 91.03 14005

Table 4: The test accuracy after adding the augmented set
generated using coverage guidance to the training set on SST-
2 and QQP dataset.

5 Related Work

Our work connects to a few topics in the literature.

Testing for Natural Language Processing Recent
literature has shown that deep learning models of-
ten exhibit unexpectedly poor behavior when de-
ployed “in the wild". This has led to a growing in-
terest in testing NLP models. The pioneering work
in this domain is CHECKLIST (Ribeiro et al., 2020),
that provides a behavioral testing template for deep
NLP models. A different paradigm is proposing
more thorough and extensive evaluation sets. For
example, (Kiela et al., 2021) and (Koh et al., 2021)
proposed new test sets reflecting distribution shifts
that naturally arise in real-world language applica-
tions. On a similar line, (Belinkov and Glass, 2019;
Naik et al., 2018) introduced challenge set based
testing. Another line of work has focused on per-
turbation techniques for evaluating models, such
as logical consistency (Ribeiro et al., 2019), ro-
bustness to noise (Belinkov and Bisk, 2017), name
changes (Prabhakaran et al., 2019), and adversaries
(Ribeiro et al., 2018).

01} thresholds respectively.

Subset SelectionOur MNCOVER can be used as a
guide for filtering test inputs, and hence is a data
selection approach. Previous work have looked at
finding representative samples from training and/or
interpretation perspectives. For example, submodu-
lar optimization from (Lin and Bilmes, 2009, 2010)
provides a framework for selecting examples that
minimize redundancy with each other to select rep-
resentative subsets from large data sets. These
methods are part of the “training the model" stage,
targeting to achieve higher accuracy with fewer
training samples. Moreover, Influence Functions
from (Koh and Liang, 2020) provide a strategy to
interpret black box models by discovering impor-
tant representative training samples. The influence
function can explain and attribute a model’s pre-
diction back to its training samples. Differently,
MNCOVER is a test suite evaluation approach.

6 Conclusion

This paper proposes MNCOVER to perform white-
box coverage-based behavior testing on NLP mod-
els. We design MNCOVER to consider Transformer
models’ properties, focusing on essential words
and important word combinations. Filtering test
sets using the MNCOVER helps us reduce the test
suite size and improve error detection rates. We
also demonstrate that MNCOVER serves as a practi-
cal criterion for evaluating the quality of test sets.
It can also help generate augmented training data
to improve the model’s generalization.


References

Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic
and natural noise both break neural machine transla-
tion. arXiv preprint arXiv:1711.02173.

Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics, 7:49-72.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al. 2021. On the opportunities
and risks of foundation models. arXiv preprint
arXiv:2108.07258.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.

Swaroopa Dola, Matthew B. Dwyer, and Mary Lou
Soffa. 2021. Distribution-aware testing of neu-
ral networks using generative models. In 43rd
IEEE/ACM International Conference on Software
Engineering. To appear.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144.

Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh
Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-
gen, Grusha Prasad, Amanpreet Singh, Pratik Ring-
shia, et al. 2021. Dynabench: Rethinking bench-
marking in nlp. arXiv preprint arXiv:2104. 14337.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv: 1609.02907.

Pang Wei Koh and Percy Liang. 2020. Understanding
black-box predictions via influence functions.

Pang Wei Koh, Shiori Sagawa, Henrik Mark-
lund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga,
Richard Lanas Phillips, Irena Gao, et al. 2021.
Wilds: A benchmark of in-the-wild distribution
shifts. In International Conference on Machine
Learning, pages 5637-5664. PMLR.

Hui Lin and Jeff Bilmes. 2009. How to select a good
training-data subset for transcription: Submodular
active selection for sequences. Technical report,
WASHINGTON UNIV SEATTLE DEPT OF ELEC-
TRICAL ENGINEERING.

Hui Lin and Jeff Bilmes. 2010. An application of the
submodular principal partition to training data sub-
set selection. In NIPS workshop on Discrete Opti-
mization in Machine Learning. Citeseer.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Dangi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.

Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun,
Minhui Xue, Bo Li, Chunyang Chen, Ting Su,
Li Li, Yang Liu, et al. 2018a. Deepgauge: Multi-
granularity testing criteria for deep learning sys-
tems. In Proceedings of the 33rd ACM/IEEE In-
ternational Conference on Automated Software En-
gineering, pages 120-131.

Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu,
Jianjun Zhao, and Yadong Wang. 2018b. Combi-
natorial testing for deep learning systems. arXiv
preprint arXiv: 1806.07723.

John X. Morris, Eli Lifland, Jin Yong Yoo, Jake
Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack:
A framework for adversarial attacks, data augmenta-
tion, and adversarial training in nlp.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
arXiv preprint arXiv: 1806.00692.

Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana.
2017. Deepxplore: Automated whitebox testing of
deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles, pages
1-18.

Vinodkumar Prabhakaran, Ben Hutchinson, and Mar-
garet Mitchell. 2019. Perturbation sensitivity anal-
ysis to detect unintended model biases. arXiv
preprint arXiv: 1910.04210.

Danilo Rezende and Shakir Mohamed. 2015. Varia-
tional inference with normalizing flows. In Interna-
tional conference on machine learning, pages 1530-

1538. PMLR.

Marco Tulio Ribeiro, Carlos Guestrin, and Sameer
Singh. 2019. Are red roses red? evaluating con-
sistency of question-answering models. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6174-6184.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging nlp models. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 856-865.

Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,
and Sameer Singh. 2020. Beyond accuracy: Behav-
ioral testing of nlp models with checklist.

Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray.
2018. Deeptest: Automated testing of deep-neural-
network-driven autonomous cars. In Proceedings of
the 40th international conference on software engi-
neering, pages 303-314.


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998-6008.

Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-
ong, and Quoc V Le. 2019. Unsupervised data aug-
mentation for consistency training. arXiv preprint
arXiv: 1904. 12848.

A Appendix

A.1_ Deriving How Two Masks are Used

To learn these global masks, we update each
preloaded word embedding x;Vi € {1,..., Z} us-
ing embeddings from words that interact with x;
as defined by the learnt interaction matrix M 4,.
Specifically, to get interaction-based word compo-
sition, we use the following formulation:

Here, e’; is the updated word embedding for to-
ken x; after taking into account its interaction
scores with other words in the sentence E =
[ei,...,ez]. This is motivated from the mes-
sage passing paradigm from (Kipf and Welling,
2016), where we treat each word in a sentence
as a node in a graph. Using Equation 10, we
effectively augment a word’s embedding using
information from words it interacts with. Note
that we normalize M 4,, using D-?!/2M,4,D~!/?,
where D is the diagonal node degree matrix for
Max. 9(Max, ex;}) VJ € {1,---, LZ} is the ag-
gregation function. Equation 10 formulation repre-
sents words and their local sentence level neighbor-
hoods’ aggregated embeddings. Specifically, we
use g(M 4x, Ex) = h(My4, Ex). Here, h is a non-
linearity function, we use the ReLU non linearity.
Simplifying our interaction based aggregation, if
two words x;, x; are related in a sentence, we rep-
resent each word using e/ = (e; +0(a;;(e;+;))).
Similarly, e’, = (e; + 7(aji(e; + e;))). Further, to
select words based on interactions, we add a word
level mask Myy after the word embeddings, where
Mwy = [Mwx,,---,Mwx,]-

zi = My; *e;, where My; € {0,1} is a bi-
nary random variable. Z = [z1,..., Zz] represents
word level embeddings input into a model for a
specific input sentence after passing through the
bottleneck layer.

A.2 Deriving the Loss
We introduce a bottleneck loss:

rp = marzl(Z; Y) — BI(Z; X) (11)

Given X, we assume E’ and My are inde-

pendent of each other. We write q(Z|X) =

q(Mw|X)q(E’|X). From Equation 10, e; =

e; +ReLU(M4,Eq,....))- g(E’|X) can be written
The lower bound to be maximized is:

L = Eyzixmylog(p(y”|Mw, Ma, x"))

—BiKL(q(My|x™)||pro(Mw)) (2)
— Bg K L(q(Ma|x")||Pao(M.a))
We use the bernoulli distribution prior
(a non informative prior) for each
word-pair interaction dol(MAz;,x;|Xi, Xj].

Poo(Maz) = [1i2y I1j=1 Pao(Max,,x,)» hence
Pao(Maz,,x;) = Bernoulli(0.5). This leads to:

Bg K L(q(Max|x"")||pao(Ma)) =

13
—BjHa(Maxlx™)

Similarly, we use the same _ bernoulli

distribution prior for the word mask,
Pro(My ) = Thiz1 Pro(Mwx,). and
pro(Myx,) = Bernoulli(0.5):

BK L(q(Mwx|x")||Pao(Mw)) = (14)

— Br Hg(Mw |x")

We also add a sparsity regularization on M4, to
encourage learning of sparse interactions. Finally,
we have the following loss function:

L = —(Exp(y|x™, Ma, Mw)+

6:Hy(Mw|x™)
+8gHq(Max|x™))+

Bsparse||[Max||1

(15)

As Ma, is a binary graph sampled from a
Bernoulli distribution with parameter y, to train
the learnt parameter y, we use the Gumbel-
Softmax(Jang et al., 2016) trick to differentiate
through the sampling layer. To learn the word
mask My, we use the amortized variational in-
ference(Rezende and Mohamed, 2015). We use a
single-layer feedforward neural network as the in-
ference network q¢(R,,,)|x,» Whose parameters are
optimized with the model parameters during train-
ing. We use Gumbel-Softmax for training with
discrete word mask.


——_ Max(harya)
‘| bing

__' _ | Activated Neuron
Bin(ANB)

Min(hasaya)

Figure 5: A schematic of k-multisection coverage in a DNN
model.

Algorithm 1: Coverage Guided Greedy
Search to generate Augmented Set G

Result: Test Set G
Set of Transformations 7’, Initial Seed Test set S;
while S is not empty do
text0 = S.popQ);
cov0 = cov(text0);
text = text0;
Tqueue = ¢;
iter = 0;
while iter < maxIter do
if Tqueue is not empty then
| T= Tqueue.dequeue();
else
| Tl =RandomFrom(T);
end
T2 = RandomFrom(T);
textl = ApplyTransform(text, T1, T2);
if covinc(textl, covO) and
CosineSim(textl, text) then

text = text];
Tqueue.enqueue(T1);
Tqueue.enqueue(T2);
G.append(text);
break;
else
| iter += 1;
end
end
end
Model Dataset | Test Accuracy
SST-2 99.31
BERT
QQP 99.77
SST-2 97.36
RoBERT:
oee"* | OOP 99.66

Table 5: Test accuracy (in %) of models trained with WIMASK
layer. Note that the ground truth labels here are the predictions
from the target model f without the WIMASK layer, as our
goal is to ensure fidelity of the WIMASK +f to the target
model f. The original models’ accuracies are summarized in
Table 4.

A.3 More Details and Results on
Experiments
For Experiment 4.4 Coverage Guided Augmen-
tion, the set of transformations we consider are :
RandomSynonymlnsertion, WordS wapEmbedding,

WordSwapChangeLocation, WordSwapChange-
Name, WordSwapChangeNumber, WordS wapCon-
tract, WordSwapExtend, WordSwapHomoglyph-

Swap, WordSwapMaskedLM, WordSwapQW-
ERTY, WordS wapNeighboringCharacter-
Swap, WordS wapRandomCharacterDele-
tion, WordSwapRandomCharacterInsertion,

WordSwapRandomCharacterSubstitution ,Ran-

domSwap, and WordSwap WordNet.


‘Test Transformation Name Failure Rate (%) Dataset Size Reduction (%)

D D+COVER D+MNCOVER Ap ymncover | DtCOVER D+MNCOVER
Change first name in one of the questions 63.00 100.0059.40 100.0064.49 37.001.49 98.20 98.20
add one typo 19.40 28.5723.17 29.4193 52 10.014.12 88.00 88.00
Product of paraphrases(q1) * paraphrases(q2) 95.00 100.00i00.00 100.00400.00 5.005.00 99.00 99.00
Replace synonyms in real pairs 8.37 13.337.89 12.508.81 4.130.44 73.31 73.31
Symmetry: f(a, b) = f(b, a) 6.00 8.334.383 10.004.61 4.00—1.39 82.00 82.00
Testing implications 15.07 15.257.90 15.257.46 0.19_7.60 99.29 99.29
same adjectives, different people v3 100.00 100.00:100.00 — 100.00100.00 0.000.00 81.82 81.82
same adjectives, different people 100.00 =100.00j00.00 100.00400.00 0.000.00 81.48 81.48
Change same location in both questions 5.00 0.000.00 0.000.00 —5.00_5.00 96.60 96.60
Average Improvement * 5.96 —0.96 6.15_-0.33 6.15_0.33 88.86 88.86

Table 6: Failure Rate(%) obtained using BERT model on the original dataset D, the dataset filtered using COVER coverage
(D+COVER columns) and the dataset filtered with MNCOVER coverage (D+MNCOVER columns) from the QQP Suite. We report
both the max failure rate as well as the mean in the subscript across 10 thresholds of coverage. Rows are sorted regarding the
failure rate difference between the dataset filtered using MNCOVER and the original dataset (column Ap+mncover). We use 200
samples in this case.
