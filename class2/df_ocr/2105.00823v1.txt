arXiv:2105.00823v1 [cs.CL] 3 May 2021

Switching Contexts: Transportability Measures for NLP

Guy Marshall*', Mokanarangan Thayaparan*', Philip Osborne’, André Freitas"?
Department of Computer Science, University of Manchester, United Kingdom!
Idiap Research Institute, Switzerland?

{guy.marshall, philip.osborne}@postgrad.manchester.ac.uk
{mokanarangan.thayaparan, andre.freitas}@manchester.ac.uk

Abstract

This paper explores the topic of transporta-
bility, as a sub-area of generalisability. By
proposing the utilisation of metrics based on
well-established statistics, we are able to esti-
mate the change in performance of NLP mod-
els in new contexts. Defining a new mea-
sure for transportability may allow for bet-
ter estimation of NLP system performance in
new domains, and is crucial when assessing
the performance of NLP systems in new tasks
and domains. Through several instances of
increasing complexity, we demonstrate how
lightweight domain similarity measures can be
used as estimators for the transportability in
NLP applications. The proposed transportabil-
ity measures are evaluated in the context of
Named Entity Recognition and Natural Lan-
guage Inference tasks.

1 Introduction

The empirical evaluation of the quality of NLP
models under a specific task is a fundamental
part of the scientific method of the NLP commu-
nity. However, commonly, many proposed mod-
els are found to perform well in the specific con-
text in which they are evaluated and state-of-the-
art claims are usually found not transportable to
similar but different settings. The current evalua-
tion metrics may only indicate which algorithm or
setup performs best: they are unable to estimate
performance in a new context, to demonstrate in-
ternal validity, or to verify causality. To offset
this, statistical significance testing is sometimes
applied in conjunction with performance measures
(e.g. Fl-score, BLEU) to attempt to establish va-
lidity. However, statistical significance testing has
been shown to be lacking. Dror et al. (2018) re-
viewed NLP papers from ACL17 and TACL17 and

*

equal contribution

found that only a third of these papers use signifi-
cance testing. Further, many papers did not spec-
ify the type of test used, and some even employed
an inappropriate statistical test.

Performance is measured in NLP tasks primar-
ily through F1 score or task-specific metrics such
as BLEU. The limited scope of these as per-
formance evaluation techniques has been shown
to have issues. Sggaard et al. (2014) highlights
the data selection bias in NLP system perfor-
mance. Gorman and Bedrick (2019) show issues
of using standard splits, as opposed to random
splits. We support their statement that “practition-
ers who wish to firmly establish that a new sys-
tem is truly state-of-the-art augment their evalua-
tions with Bonferroni-corrected random split hy-
pothesis testing”. In an NLI task, using SNLI and
MultiNLI datasets with a set of different models,
it has been shown that permutations of training
data leads to substantial changes in performance
(Schluter and Varab, 2018).

Further, the lack of transportability for NLP
tasks has been raised by specialists in applied do-
mains. For example, healthcare experts have ex-
pressed their frustration in the limitations of algo-
rithms built in research settings for practical ap-
plications (Demner-Fushman and Elhadad, 2016)
and the reduction of performance “outside of
their development frame” (Maddox and Matheny,
2015). More generally, “machine learning re-
searchers have noted current systems lack the abil-
ity to recognize or react to new circumstances they
have not been specifically programmed or trained
for” (Pearl, 2019).

The advantages of “more transportable” ap-
proaches, such as BERT, in terms of their perfor-
mance in multiple different domains, is currently
not expressed (other than the prevalence of such ar-
chitectures across a range of state-of-the-art tasks
and domains). To support analysis and investiga-


tion into the insight that could be gained by ex-
amination of these properties, we suggest metrics
and a method for measuring the transportability of
models to new domains. This has immediate rel-
evance for domain experts, wishing to implement
existing solutions on novel datasets, as well as for
NLP researchers wishing to assemble new dataset,
design new models, or evaluate approaches.

To support this, we propose feature gradient,
and show it to have promise as a way to gain lexi-
cal or semantic insight into factors influencing the
performance of different architectures in new do-
mains. This differs from data complexity, being
a comparative measure between two datasets. We
aim to start a conversation about evaluation of sys-
tems in a broader setting, and to encourage the cre-
ation and utilisation of new datasets.

This paper focuses on the design and evaluation
of a lightweight transportability measure in the
context of the empirical evaluation of NLP models.
A further aim is to provide a category of measures
which can be used to estimate the stability of the
performance of a system across different domains.
An initial transportability measure is built by for-
malising properties of performance stability and
variation under a statistical framework. The pro-
posed model is evaluated in the context of Named
Entity Recognition tasks (NER) and Natural Lan-
guage Inference (NLI) tasks across different do-
mains.

Our contribution is to present a measure that
evaluates the transportability and robustness of an
NLP model, to evaluate domain similarity mea-
sures to understand and anticipate the transporta-
bility of an NLP model, and to compare state of
the art models across different datasets for NER
and NLI.

2 Relevant background and related work

2.1 Terminology

To quote Campbell and Stanley (2015), “Exter-
nal validity asks the question of generalizabil-
ity: To what populations, settings, treatment vari-
ables, and measurement variables can this ef-
fect be generalized?”. For Pearl and Bareinboim
(2014), transportability is how generalisable an ex-
perimentally identified causal effect is to a new
population where only observational studies can
be conducted. “However, there is an important
difference, not often distinguished, between what
might be called the potential (or generic) transfer-

ability of a study and its actual (or specific) trans-
ferability to another policy or practice decision
context at another time and place.” (Walker et al.,
2010)

Bareinboim and Pearl (2013) explore transfer
of causal information, culminating in an algorithm
for identifying transportable relations. Transporta-
bility in this sense does not permit retraining in
the new population, and guides our choices in this
paper. Other definitions of transfer learning al-
low for training of the model in the new context
(Pan and Yang, 2010), or highlight the distinction
between evidential knowledge and causal assump-
tions (Singleton et al., 2014).

2.2 Transportability: Models evaluated
across different datasets

Rezaeinia et al. (2019) consider improving trans-
portability by demonstrating word embeddings’
accuracy degrades over different datasets, and pro-
pose an algorithmic method for improved word
embeddings by using word2vec, adding gloVe
when missing, and filling any further missing
values with random entries. In a medical tag-
ging task, Ferrandez et al. (2012) used different
train/test datasets, and compared precision and re-
call with self-trained vs transported-trained, find-
ing that some tag-categories performed better than
others. They postulate that degradation differ-
ences were due to the differing prevalence of en-
tities in the transported training data. Another
term from this domain is “portability”, in the sense
that a model could be successfully used with con-
sideration of implementation issues such as dif-
ferent data formats and target NLP vocabularies
(Carroll et al., 2012). Blitzer et al. (2007) created
a multi-domain dataset for sentiment analysis, and
propose a measure of domain similarity for senti-
ment analysis based on the distance between the
probability distributions in terms of characteristic
functions of linear classifiers.

In image processing, domain transfer is an ac-
tive area of research. Panetal. (2010) propose
transfer component analysis as a method to learn
subspaces which have similar data properties and
data distributions in different domains. They state
that domain adaptation is “a special setting of
transfer learning which aims at transferring shared
knowledge across different but related tasks or do-
mains”. In computer vision, Peng et al. (2019)
combine multiple datasets into a larger dataset Do-


mainNet, and consider multi-source domain adap-
tation, formalising for binary classification. They
demonstrate multi-source training improves model
accuracy, and publish baselines for state of the art
methods.

2.3. Generalisability

The language used in literature is not consistent.
Bareinboim and Pearl (2013) highlights that gen-
eralisability goes under different “rubrics” such
as external validity, meta-analysis, overgeneralisa-
tion, quasi-experiments and heterogeneity.

Boulenger et al. (2005) disambiguate terms in
the context of healthcare economics (such as gen-
eralisability, external validity, and transferability),
and created a self-reporting checklist to attempt to
quantify transferability. They define generalisabil-
ity as “the degree to which the results of a study
hold true in other settings’, and “the data, meth-
ods and results of a given study are transferable
if (a) potential users can assess their applicabil-
ity to their setting and (b) they are applicable to
that setting”. They advocate a user-centric view
of transferability, considering specific usability as-
pects such as explicit currency conversion rates.

Antonanzas et al. (2009) create a transferabil-
ity index at general, specific and global levels.
Their “general index” is comprised of “critical
factors”, which utilise Boulenger et al.’s factors,
adding subjective dimensions.

3 Transportability in NLP
3.1 Definitions

To support a rigorous discussion, notational con-
ventions are introduced. Extending the choices of
Pearl and Bareinboim (2011), we denote a domain
D with population II, governed by feature proba-
bility distribution P, which is data taken from a
particular domain. We denote the source with a 0
subscript.

Definition 1. Generalisability: A system V has
performance p for solving task To in domain Dp.
Generalisability is how the system V performs for
solving task T; in domain Dj, relative to the origi-
nal task, without retraining.

Special cases, such as transportability or trans-
ference, have some i,7 = O in the definition
above.

Definition 2. Transportability: A system V has
performance p for solving task To in domain Dp.

Source Context Do

Source data Ho Se
1
‘Transportability ' _ i) i
_ Performance 7, get {
~ \ Ff \
| ; \} 1
| Samples ) | | i
1
2 | Performance i
Internal oa Ni \ Variation 7,,, ii a» 1
Validity | a, ac :)
i
1
1
meeps f 1
1
\
1
; ae \ |
1 1
t 1 Tar i
1 1
1
\ 1
1
1
1
1

Target Contexts Dj

Figure 1: Schematic representation of the definitions

Transportability is the performance of system V
for solving task To in a new domain TD,, relative to
the original task, without retraining.

Across multiple D;, we have relative perfor-
mance T,(Do,D;), from which we can estab-
lish statistical measures for transportability perfor-
mance and variation.

Transfer learning is a specific way of achiev-
ing transportability (between populations or do-
mains) or generalisability (including between
tasks). Singleton et al. (2014) state that “‘trans-
port encompasses transfer learning in attempting
to use statistical evidence from a source on a target,
but differs by incorporating causal assumptions
derived from a combination of empirical source
data and outside domain knowledge.”. Note that
this is different to generalisation in the Machine
Learning sense, which is akin to internal validity
(Marsland, 2011). Figure 1 shows the definitions
associated with transportability discussed in this
paper.

Table 1 summarises terminology, of how the tar-
get differs from source (Wo, To, Do(Ho)).

Term w

Cross-validation O 0
New modeling i O
Transportability O 0
Transferability 0 O
Generalisability O 0

Table 1: Terminology through variation from a source.
Table body is subscripts.

Chance, bias and confounding are the three
broad categories of “threat to validity”. Broadly,
chance and bias can be assessed by cross-validity,
as it applies a model to the same task in the same
domain on different data population. Confound-
ing, error in interpretation of what is being mea-


sured, is more difficult to assess. Transportability
is concerned with the transfer of learned informa-
tion, with particular advances in the transport of
causal knowledge.

Generalisability is the catch-all term for how ex-
ternally valid a result or model is. Any combina-
tion of task, domain and data can be used.

3.2 Transportability performance

We define transportability performance 7, as the
gradient of the change in the performance metric’s
score from one domain to another. This measure
does not take into account the underlying probabil-
ity distributions, only the change in resulting per-
formance measure.

Dy, Diy = BEE
PoP) THT Do)

(1)

The measure uses a ratio in order to allow compar-
ison between different systems. To generalise this
measure across different settings, we can take an
average to give Equation 2. Note that this is the
average percentage change in performance, not an
aggregated performance measure.

(W,T,D;)
ere m 2)
p(v, T, Do)
An analogous definition holds for different tasks
over the same domain, 7,(7’).

3.3 Performance variation

Performance variation reflects how stable perfor-
mance is across different contexts and can include,
for example, to what extent the sampling method
from the source data effects the performance met-
ric of the algorithm. Part of this is data representa-
tiveness, the extent to which the source data repre-
sentation also represents the target data.

More formally, performance variation
Ts(V,T,D) is the change in performance of
(W,T,D) across different contexts. This is useful
in order to gain specific insight into external
validity and generalisability. Indeed, we can
assess the change in performance between source
context Dp and target context D;. The source
context has a privileged position, in that it is this
space which the “learning” takes place, and the
proposed metric for performance variation to
multiple different domains is based on 7, to reflect
this. Through repeated measurement in different
contexts, we can go further.

Definition 3. Performance Variation: For a model
trained on domain Do and applied on n new do-
mains D;, we define the performance variation as
the coefficient of variation of performance across
this set of domains so that:

ee (t%(Do,Di)—T (Do))?

_ 1 n—-1
Fear (Po) = (14 a) FAD)
(3)

The 1 + a term corrects for bias. In order to
be meaningful, the target contexts must to have a
good coverage of different domains. Enumerating
these would be a task of ontological proportions,
but can be pragmatically approximated by using
the available Gold Standard datasets.

We can also assess ability to generalise not just
over different domains, but also different tasks,
provided they can be meaningfully assessed by
the same performance measure. We can consider
n different domain-task combinations, and with
Tp = oi j-0 TY, Ti, Dj)/n, this gives a more
general form for Equation 3, with n large:
~i>0, 720 (tp (U,Ti,Dj)—Tp)”

Tp

Tvar =

In the case where different tasks cannot be as-
sessed by the same measure, we are still able to
compare different systems by looking at how the
respective measures change.

3.3.1 Performance variation properties

For a purely random system, the transportability
should be related to how similar the distributions
of “answers” in the test dataset are. A random
system should really be transportable by our mea-
sures. Similarly, we can consider trivial systems,
such as identity and constant functions, which are
necessarily entirely transportable. That is, for a
system that is an identity function VW = J, tT) =
f(P), and Tyar(l,T,Di) = Tvar(I,T,D;) = 9,
Vi, 7. Note that we would not expect the same per-
formance of these functions on different tasks.

A stable system will have Tya,(W,T,Do)
Tvar(V,T,D;)Vi, reflecting that it is resilient to
the domain on which it is trained.

3.3.2 Factors influencing performance
variation

Through repeated measurement, we can quantify
how f-score changes with respect to different


measures A (e.g. dataset complexity), oA, with

other properties held constant.

NLP system performance is dependent on A.
This list may include gold standard feature distri-
bution (in terms of representativeness of the se-
mantic or linguistic phenomena), and task diffi-
culty or sensitivity.

Users of NLP systems would benefit from be-
ing able to estimate the performance of an existing
NLP system on a new domain, without perform-
ing the full implementation. Important for the per-
formance of an NLP system, especially for few or
zero shot learning, is having a common set of fea-
tures (or phenomena) across domains. We proceed
to propose three measures of increasing complex-
ity, in order to attempt to understand how “similar”
two domains are.

Lexical feature difference: A measure
grounded on lexical features (i.e. bag of words).
The intuition behind this measure is for treating
the set of lexical features as a representation. Lin-
guistic space is observed as materialised tokens,
which in turn are in some higher-dimensional
semantic space, which enable interpretation. The
measure considers the overlap of these linguistic
spaces, and indeed the extent to which the lin-
guistic space is covered by the data. Due to the
simplicity of this measure, correlation between
this and actual transportability performance is
likely to be weaker than other measures but is
simpler to calculate.

[Pi Pol Pal |; >0

[Di
(5)
Where |D;| is the number of features in the target
domain D;, and |D;™Do| is the number of features
overlapping. This measure is then the proportion
of unseen features in the new dataset. If all fea-
tures of D; are found in Dp, then the feature differ-
ence is 0. If no features of D; are found in Do, then
the feature difference is 1. The feature overlap is
task specific, and therefore appropriate to consider
for transportability, but not generalisability.

In the simplest case, the transported perfor-
mance of a bag of words model should be precisely
the lexical feature difference combined with distri-
butions of the source and target domains. The fea-
ture set can range from binary lexical features to la-
tent vector spaces. For different models, which tar-
get different aspects of semantic phenomena, dif-
ferent semantic and syntactic features will matter

Lexical Feature Difference = 1—

more. For this reason, considering a set of mea-
sures for domain complexity is warranted. In the
context of this work, two measures are used over
more complex feature spaces.

Cosine distance: Specifically, we use Doc2 Vec
(Le and Mikolov, 2014) to embed the documents
from each domain in a 300-dimensional feature-
vector space, normalise, and calculate cosine dis-
tance to compare source and target domains.

Kullback-Leibler — divergence: Considering
each domain as a distribution of features, we can
use relative entropy to understand the difference
between the source and target domains. Similar
to cosine distance, we convert the corpus to a
vector using Doc2Vec and normalize. We treat
these values as discrete probability distributions
to calculate the KL divergence.

The usefulness of any of these domain simi-
larity measures depends on the semantic phenom-
ena and supporting corpora underlying the system,
for example if the system requires a large train-
ing dataset, it may be more appropriate to use a
measure which considers the underlying probabil-
ity distributions in each feature. In this case, we
can restrict to the case of the same task in order
to keep the essential features reasonably consis-
tent across domains. This makes this a measure
of transportability (rather than generalisability).

There are additional dimensions of transporta-
bility potentially worthy of further investigation
and quantification: (i) domain similarity (e.g.
missing features), (ii) data efficiency (redun-
dant/repeated features), (iii) data preparation (ini-
tial setup and formatting) and (iv) data manipula-
tion required (data pipeline).

4 Experiments

4.1 Setup

The experiments aim to evaluate the consistency
of the proposed transportability measures in the
context of two standard tasks: named entity recog-
nition and natural language inference. For repro-
ducibility purposes the code and supporting data
are available online!.

We calculated the Fl score of multiple models
on multiple datasets (Figure 2). Note that in gen-
eral the applicability of the proposed transporta-
bility measures are not limited to the use of Fl

‘https://github.com/ai-systems/transportability


NER: NLI:
Stanford NER SNLI

CoNNL-2003 SpaCy v2 MultiNLI » BERT
ELMo SciTail

Figure 2: Overview of the experiments undertaken, in-
dicating the models being applied to each dataset

Dataset Model

Stanford SpaCy ELMo

CoNLL Train 98.69 99.32 99.97
5003 “Dev 93.22, —Ss 81.56 —*98.17
Test 88.78 88.11 93.79

Wiki 66.31 52.14 79.4

Train 51.63 27.03 36.3

WNUT Dev 53.59 32.23 48.8

Test 47.11 26.28 58.1

Table 2: NER F1 scores for different models trained on
CoNLL dataset transported across different corpora

score, but this is simpler as the same measure
applies for both tasks. All models and datasets
are standard. For NER, the datasets were chosen
as they have the consistent tags: Location, Per-
son and Organisation. Stanford NER (Finkel et al.,
2005) is a CRF classifier, SpaCy v2 is a CNN,
ELMo (Peters et al., 2018) is a vector embedding
model which outperforms GloVe and word2vec.
Each of the three models used are trained on
the CoNLL-2003 dataset (Sang and De Meulder,
2003). We evaluated these models on CoNLL-
2003, Wikipedia NER (Ghaddar and Langlais,
2017) (Wiki) and WNUT datasets (Baldwin et al.,
2015) for NER in twitter microposts.

For NLI, we chose to use standard datasets.
SNLI (Bowman et al., 2015) is well established
with a limited range of NLI statements, MultiNLI
(Williams et al., 2018) is multigenre with a more
diverse range of texts, and SciTail (Khot et al.,
2018) is based on scientific exam questions. We
applied BERT (Devlin et al., 2018), a state of the
art embedding model, to these datasets.

4.2 Results

NER: Table 2 shows results for the NER task,
trained on CoNLL. Unsurprisingly, all models per-
formed better when the target was in the CoNLL
domain. The reduced performance on Wiki
was more extreme than expected, particularly for
ELMo, which was expected to be resilient to do-

main change (i.e. transportable). Table 6 and
Table 4 illustrate the transportability and domain
similarity scores for different NER models respec-
tively.

NLI: Table 3 shows results for the NLI task, us-
ing BERT. We find that, despite the vast training
data, BERT’s performance is substantially higher
when it has been trained on data from that do-
main. BERT trained on SciTail performs poorly
when transported to SNLI or MultiNLI. Table 7
and Table 5 illustrates the transportability and do-
main similarity scores for different NLI corpora.

4.3 Analysis

Every model had 7, < 1, meaning they performed
worse on the new domain. This is as expected,
though this would not be true in general.

NER: Examining the Fl scores (88.11 vs.
88.78) of SpaCy and Stanford they appear almost
comparable. However, the latter transports much
more effectively, with 7, score difference (0.671
Vs 0.524 when transporting to Wiki) (refer Ta-
ble 6).

ELMo is one of the state of the art approaches
for NER, as evidenced by the high F1 scores for
the source corpus. However, Stanford NER trans-
ports equally well, and when transported outper-
forms ELMo for twitter domain. While the ab-
solute Fl score difference between them is 5, the
Tp Scores are almost identical, with a difference of
0.003. In terms of transportability, it is notable that
an approach that employs CRF tagger with linguis-
tic features outperforms significantly the CNN-
based SpaCy approach and stands in comparison
to a computationally expensive model like ELMo.

Stanford NER also has the lowest Tyg;. This
indicates this to be the most robust model out of
the three. This conclusion was facilitated by the
Tp and Tyr Measures.

NER for English is assumed to be an accom-
plished task as supported by the traditional Fl
scores. By using 7, we argue that there is a need
for more robust models, with better transportabil-
ity performance.

Figure 3a and Figure 3b illustrates the decrease
in F1 scores as cosine distance and KL divergence
increase. A simple 3 parameter non-linear regres-
sion model on KL Divergence and Cosine distance
is able to predict the Fl score with an mean error
of 3.33 and 2.66 respectively. Considering the lex-
ical difference has similar results (Table 4). This


Target Dataset

Source Dataset

SNLI Dataset MultiNLI Dataset SciTail Dataset
Train Dev Test Train Dev Train Dev Test
SNLI (Train) 96.81 90.83 90.40 72.51 72.29 54.04 61.34 52.72

Multi NLI (Train)
SciTail (Train)

77.13 79.05 79.31
42.68 44.36 44.20

83.50
44.49

66.52 67.79 67.26
99.88 94.78 93.08

97.78
47.49

Table 3: NLI accuracy scores for BERT model trained on one dataset transported to a different dataset

100

80 |)

60}

F1 Score

40 | mae

—1— ELMo NER
20 | 1 Stanford NER
—4— SpaCy NER

0

0 2 4 6 8 10 12 14 #16
Cosine Distance x 10~?

(a) NER F1 scores Vs Doc2Vec cosine distance
from training (CoNLL) corpus

100

80 +

60 F

F1 Score

40 -

—4+— ELMo NER a

20 }| _. stanford NER 1

—«— SpaCy NER

0

0 0.5 1 1.5 2 2.5
KL Divergence

(b) NER F1 scores Vs KL Divergence from
training (CoNLL) corpus

Figure 3: NER F1 score plotted against different measures of corpus similarity

Dataset Lexical Cosine KL
Diver-
gence

Train 0.000 0.000 0.000

CoNLL Dev 0.121 0.001 0.345

Test 0.197 0.003 0.463
Wiki 0.290 0.007. 0.701
Train 0.421 0.134 2.129
WNUT Dev 0.511 0.167 1.473
Test 0.481 0.130 1.137

Table 4: Domain similarity scores between the training
corpus (CoNLL-2003) across other NER datasets

implies that by using these measures we may be
able to anticipate the accuracy of a model in a new
domain based on easy to compute domain similar-
ity, which is straightforward to compute.

NLI: Applying BERT to different domains was
not as resilient to domain transport as we ex-
pected. The average 7, is 0.612 over transported
domains, despite these being standard corpora
from the domains. We found MultiNLI(Train) to
be more transportable than the others, since its
performance in new domains is not much worse

than new data from the same domain. This is as
expected, since MultiNLI has been built to have
good domain coverage. Specifically, MultiNLI
has 7, = 0.744 and Tra, = 8.582, whilst SNLI
has T, = 0.646 and Ty, = 15.22 and SciTail has
Tp = 0.446 and Tyq, = 3.921. SciTail transports
poorly, and does so reliably! SNLI transports in
between, but variably, being quite “hit or miss”
with different samples of SciTail. These results
suggest a threshold for 7, of perhaps 0.8 as being
“appropriate” for transportability performance. A
threshold for 7,,, 1s more difficult to establish and
would benefit from further investigation. Clearly,
these measures depend on the domains chosen.

As with NER, we found lexical difference in-
dicative of transported performance, and that for
NLI, accuracy scores decrease with increasing lex-
ical difference, cosine distance and KL divergence
(Tables 3 and 5, and Figures 4a and 4b). A simple
3 parameter non-linear regression model on KL
Divergence and Cosine distance is able to predict
the accuracy score with an mean error of 3.98 and
1.95 respectively.

4.4 Discussion

Tp and T,q; aS complementary to traditional
measures. We are not breaking new ground in


SNLI

Dataset Measurement
Train Dev
SNLI Lexical 0.000 0.003
(Train) Cosine 0.000 0.002
KL Divergence 0.000 3.277
MultiNLI Lexical 0.008 0.008
(Train) Cosine 0.008 0.018
KL Divergence 11.07 7.613
SciTail Lexical 0.282 0.282
(Train) Cosine 0.233 0.230
KL Divergence 11.17 7.04

MultiNLI SciTail

Test Train Dev Train Dev Test

0.003 0.086 0.088 0.136 0.115 0.119
0.002 0.008 0.007 0.233 0.242 0.242
4.283 6.489 8.982 16.02 17.50 18.20
0.008 0.000 0.008 0.063 0.063 0.047
0.016 0.000 0.002 0.298 0.307 0.306
6.333 0.000 3.342 33.10 35.27 34.69
0.282 0.277 0.278 0.000 0.028 0.025
0.231 0.262 0.298 0.000 0.001 0.002
7.492 5.220 6.682 0.000 1.097 1.424

Table 5: Domain similarity scores between the source training corpus and target corpora

100

80

60

Accuracy

— | [— SNLI
207). MultiNLI
2 SciTail
0 i i il il

0 5 10 15 20 25 30

Cosine Distance x 10~?

(a) NLI accuracy Vs Doc2Vec cosine distance
from source corpus

100¢

80

z= 60 |
S
< 40 = 4
_ | [-— SNLI
20 5). MultiNLI 1
—«— SciTail
0 I L il
0 10 20 30

KL Divergence

(b) NLI accuracy Vs Lexical Divergence from
training corpus

Figure 4: NLI accuracy score plotted against different measures of corpus similarity

terms of evaluation methodology, but the exper-
iments demonstrate that traditional Fl and ac-
curacy measures do not capture a complete pic-
ture. Transportability measure are not only sim-
ple enough to calculate and convey but also eval-
uates a model with regards to generalisability and
robustness.

Low cost ways of anticipating performance for
a new task or domain. Most of the state of the
art models are computationally expensive. With
the transportability and domain similarity mea-
sures we are able to predict performance in a new
domain with reasonable accuracy. These similar-
ity measures are relatively simpler to run.

5 Conclusion

We have presented a model of transportability for
NLP tasks, together with metrics to allow for the
quantification in the change in performance. We
have shown that the proposed transportability mea-
sure allows for direct comparison of NLP systems’

Stanford SpaCy ELMo
Tp(wiki) 0.671 0.524 0.794
Tp(wnut) 0.514 0.287 0.477
Tp(wnut & wiki) 0.553 0.346 0.556
Tyor 15.051 35.171 32.666

Table 6: Transportability measures for NER models

SNLI MultiNLI  SciTail
Tp 0.646 0.744 0.446
Tvar 15.22 8.582 3.921

Table 7: Transportability measures for NLI corpora


performance in new contexts. Further, we demon-
strated domain similiarity as a measure to model
corpus and domain complexity, and predict NLP
system performance in unseen domains. This pa-
per lays the foundations for further work in more
complex transportability measures and estimation
of NLP system performance in new contexts.

References

Fernando Antonanzas, Roberto Rodriguez-Ibeas,
Carmelo Juarez, Florencia Hutter, Reyes Lorente,
and Mariola Pinillos. 2009. Transferability indices
for health economic evaluations: methods and
applications. Health economics, 18(6):629-643.

Timothy Baldwin, Marie-Catherine de Marneffe,
Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu.
2015. Shared tasks of the 2015 workshop on noisy
user-generated text: Twitter lexical normalization
and named entity recognition. In Proceedings of the
Workshop on Noisy User-generated Text, pages 126-
135.

Elias Bareinboim and Judea Pearl. 2013. A general al-
gorithm for deciding transportability of experimen-
tal results. Journal of causal Inference, 1(1):107-
134.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th annual meeting of the asso-
ciation of computational linguistics, pages 440-447.

Stephanie Boulenger, John Nixon, Michael Drum-
mond, Philippe Ulmann, Stephen Rice, and Gerard
de Pouvourville. 2005. Can economic evaluations
be made more transferable? The European Journal
of Health Economics, 6(4):334-346.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Association for Computational Linguistics.

Donald T Campbell and Julian C Stanley. 2015. Exper-
imental and quasi-experimental designs for research.
Ravenio Books.

Robert J Carroll, Will K Thompson, Anne E Eyler,
Arthur M Mandelin, Tianxi Cai, Raquel M Zink, Jen-
nifer A Pacheco, Chad S Boomershine, Thomas A
Lasko, Hua Xu, et al. 2012. Portability of an algo-
rithm to identify rheumatoid arthritis in electronic
health records. Journal of the American Medical In-
formatics Association, 19(e1):e162—e169.

D Demner-Fushman and Noemie Elhadad. 2016. As-
piring to unintended consequences of natural lan-
guage processing: a review of recent developments

in clinical and consumer-generated text processing.
Yearbook of medical informatics, 25(01):224—233.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv: 1810.04805.

Rotem Dror, Gili Baumer, Segev Shlomoyv, and Roi Re-
ichart. 2018. The hitchhiker’s guide to testing statis-
tical significance in natural language processing. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1383-1392.

Oscar Ferrandez, Brett R South, Shuying Shen, F Jeff
Friedlin, Matthew H Samore, and Stéphane M
Meystre. 2012. Generalizability and comparison of
automatic clinical text de-identification methods and
resources. In AMIA Annual Symposium Proceed-
ings, volume 2012, page 199. American Medical In-
formatics Association.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd annual meet-
ing on association for computational linguistics,
pages 363-370. Association for Computational Lin-
guistics.

Abbas Ghaddar and Phillippe Langlais. 2017. Winer:
A wikipedia annotated corpus for named entity
recognition. In Proceedings of the Eighth Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 413-422.

Kyle Gorman and Steven Bedrick. 2019. We need to
talk about standard splits. In Proceedings of the 57th
annual meeting of the association for computational
linguistics, pages 2786-2791.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Interna-
tional conference on machine learning, pages 1188-

1196.

Thomas M Maddox and Michael A Matheny. 2015.
Natural language processing and the promise of big
data: Small step forward, but many miles to go.
Circulation. Cardiovascular quality and outcomes,

8(5):463.

Stephen Marsland. 2011. Machine learning: an algo-
rithmic perspective. Chapman and Hall/CRC.

Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and
Qiang Yang. 2010. Domain adaptation via transfer
component analysis. IEEE Transactions on Neural
Networks, 22(2):199-210.


Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. ZEEE Transactions on knowledge
and data engineering, 22(10):1345-1359.

Judea Pearl. 2019. The seven tools of causal inference,
with reflections on machine learning. Communica-
tions of the ACM, 62(3):54—60.

Judea Pearl and Elias Bareinboim. 2011. Transporta-
bility of causal and statistical relations: A formal ap-
proach. In Twenty-Fifth AAAI Conference on Artifi-
cial Intelligence.

Judea Pearl and Elias Bareinboim. 2014. External va-
lidity: From do-calculus to transportability across
populations. Statistical Science, 29(4):579-595.

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang,
Kate Saenko, and Bo Wang. 2019. Moment match-
ing for multi-source domain adaptation. In Proceed-
ings of the IEEE International Conference on Com-
puter Vision, pages 1406-1415.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proc. of NAACL.

Seyed Mahdi Rezaeinia, Rouhollah Rahmani, Ali Gh-
odsi, and Hadi Veisi. 2019. Sentiment analysis
based on improved pre-trained word embeddings.
Expert Systems with Applications, 117:139-147.

Erik F Sang and Fien De Meulder. 2003.  Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. arXiv
preprint cs/0306050.

Natalie Schluter and Daniel Varab. 2018. When data
permutations are pathological: the case of neural nat-
ural language inference. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 4935-4939.

Kyle W Singleton, Alex AT Bui, and William Hsu.
2014. Transfer and transport: incorporating causal
methods for improving predictive models. Journal
of the American Medical Informatics Association,
21(e2):e374—-e375.

Anders Sggaard, Anders Johannsen, Barbara Plank,
Dirk Hovy, and Héctor Martinez Alonso. 2014.
What’s in a p-value in nlp? In Proceedings of the
eighteenth conference on computational natural lan-
guage learning, pages 1-10.

Damian G Walker, Yot Teerawattananon, Rob Ander-
son, and Gerry Richardson. 2010. Generalisabil-
ity, transferability, complexity and relevance. In
Evidence-Based Decisions and Economics, pages
56-66. Wiley and Sons.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American

Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112-1122. Association for
Computational Linguistics.
