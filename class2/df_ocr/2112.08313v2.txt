arX1v:2112.08313v2 [cs.CL] 9 May 2022

Measure and Improve Robustness in NLP Models: A Survey

Xuezhi Wang
Google Research
xuezhiw@google.com

Abstract

As NLP models achieved state-of-the-art per-
formances over benchmarks and gained wide
applications, it has been increasingly impor-
tant to ensure the safe deployment of these
models in the real world, e.g., making sure
the models are robust against unseen or chal-
lenging scenarios. Despite robustness being
an increasingly studied topic, it has been sepa-
rately explored in applications like vision and
NLP, with various definitions, evaluation and
mitigation strategies in multiple lines of re-
search. In this paper, we aim to provide a uni-
fying survey of how to define, measure and
improve robustness in NLP. We first connect
multiple definitions of robustness, then unify
various lines of work on identifying robust-
ness failures and evaluating models’ robust-
ness. Correspondingly, we present mitigation
strategies that are data-driven, model-driven,
and inductive-prior-based, with a more system-
atic view of how to effectively improve robust-
ness in NLP models. Finally, we conclude
by outlining open challenges and future direc-
tions to motivate further research in this area.

1 Introduction

NLP models, especially with the recent advances
of large pre-trained language models have achieved
great progress and gained wide applications in the
real world. Despite the performance gains, NLP
models are still fragile and brittle to out-of-domain
data (Hendrycks et al., 2020a; Wang et al., 2019d),
adversarial attacks (McCoy et al., 2019; Jia and
Liang, 2017; Jin et al., 2020), or small perturba-
tion to the input (Ebrahimi et al., 2018; Belinkov
and Bisk, 2018). Those failures could hinder the
safe deployment of these models in the real world,
and impact NLP models’ trustworthiness to users.
As a result, an increasing line of work has been
conducted to understand robustness issues in the
language technologies communities. Still, diverse
sets of research across multiple dimensions and

Haohan Wang
Carnegie Mellon University Georgia Institute of Technology
haohanw@cs.cmu.edu

Diyi Yang

dyang888@gatech.edu

numerous levels of depth exist and are scattered
across various communities; for instance, using
a variety of definitions on a wide range of very
different NLP tasks. In this work, we provide a uni-
fying overview of what is robustness in NLP, how
to identify robustness failures and evaluate model’s
robustness, and systematic ways to improve robust-
ness, as well as a conceptual schema categorizing
ongoing research directions. We identify gaps be-
tween the to-date robustness work, the technical
opportunities, and discuss possible paths forward.

2 Definitions of Robustness in NLP

Robustness, despite its specific definitions in var-
ious lines of research, can typically be unified as
follows: denote the input as x, and its associated
gold label for the main task as y, assume a model
f is trained on (x, y) ~ D and its prediction over
x as f(x); now given test data (x’,y') ~ D' 4D,
we can measure a model’s robustness by its perfor-
mance on D’, e.g., using the model’s robust accu-
racy (Tsipras et al., 2019; Yang et al., 2020), de-
fined as E(./ yy p'[f (2") = y’]. Existing literature
on robustness in NLP can be roughly categorized
by how D’ is constructed: by synthetically per-
turbing the input (Section 2.1), or D’ is naturally
occurring with a distribution shift (Section 2.2).

The above definition works for a range of NLP
tasks like text classification and sequence labeling
where y is defined over a fixed set of discrete la-
bels. For tasks like text generation, robustness is
less well defined and can manifest as positional
bias (Jung et al., 2019; Kryscinski et al., 2019), or
hallucination (Maynez et al., 2020; Parikh et al.,
2020; Zhou et al., 2021). One major challenge here
is a lack of robust metrics in evaluating the quality
of the generated text (Sellam et al., 2020; Zhang
et al., 2020b), i.e., we need a reliable metric to
determine the relationship between f(z’) and y/
when both are open-ended texts.


2.1 Robustness against Adversarial Attacks

In one line of research, D’ is constructed by per-
turbations around input x to form 2’ (x’ typically
being defined within some proximity of x). This
topic has been widely explored in computer vision
under the concept of adversarial robustness, which
measures models’ performances against carefully
crafted noises generated deliberately to deceive the
model to predict wrongly, pioneered by (Szegedy
et al., 2013; Goodfellow et al., 2015), and later
extended to NLP, such as (Ebrahimi et al., 2018;
Alzantot et al., 2018; Li et al., 2019; Feng et al.,
2018; Kuleshov et al., 2018; Jia et al., 2019; Zang
et al., 2020; Pruthi et al., 2019; Wang et al., 2019e;
Garg and Ramakrishnan, 2020; Tan et al., 2020a,b;
Schwinn et al., 2021; Li et al., 2021; Boucher et al.,
2022) and multilingual adversaries (Yang et al.,
2019; Tan and Joty, 2021). The generation of ad-
versarial examples primarily builds upon the ob-
servation that we can generate samples that are
meaningful to humans (e.g., by perturbing the sam-
ples with changes that are imperceptible to humans)
while altering the prediction of the models for this
sample. In this regard, human’s remarkable ability
in understanding a large set of synonyms (Li et al.,
2020) or interesting characteristics in ignoring the
exact order of letters (Wang et al., 2020b) are often
opportunities to create adversarial examples. A re-
lated line of work such as data-poisoning (Wallace
et al., 2021) and weight-poisoning (Kurita et al.,
2020) exposes NLP models’ vulnerability against
attacks during the training process. One can refer
to more comprehensive reviews and broader dis-
cussions on this topic in Zhang et al. (2020c) and
Morris et al. (2020b).

Assumptions around Label-preserving and
Semantic-preserving Most existing work in vi-
sion makes a relatively simplified assumption that
the gold label of x’ remains unchanged under a
bounded perturbation over 7, i.e., y/ = y, anda
model’s robust behaviour should be f(z’) = y
(Szegedy et al., 2013; Goodfellow et al., 2015).
A similar line of work in NLP follows the same
label-preserving assumption with small text pertur-
bations like token and character swapping (Alzan-
tot et al., 2018; Jin et al., 2020; Ren et al., 2019;
Ebrahimi et al., 2018), paraphrasing (Lyyer et al.,
2018; Gan and Ng, 2019), semantically equiva-
lent adversarial rules (Ribeiro et al., 2018), and
adding distractors (Jia and Liang, 2017). How-
ever, this label-preserving assumption might not

always hold, e.g., Wang et al. (2021b) studied sev-
eral existing text perturbation techniques and found
that a significant portion of perturbed examples are
not label-preserving (despite their label-preserving
assumptions), or the resulting labels have a high
disagreement among human raters (i.e., can even
fool humans). Morris et al. (2020a) also call for
more attention to the validity of perturbed examples
for a more accurate robustness evaluation.

Another line of work aims to perturb the input x
to x’ in small but meaningful ways that explicitly
change the gold label, i.e., y/ 4 y, under which
case the robust behaviour of a model should be
f(a’) = y/ and f(x’) 4 y (Gardner et al., 2020;
Kaushik et al., 2019; Schlegel et al., 2021). We
believe these two lines of work are complementary
to each other, and both should be explored in fu-
ture research to measure models’ robustness more
comprehensively.

One alternative notion is whether the perturba-
tion from z to 2’ is “semantic-preseving” (Alzan-
tot et al., 2018; Jin et al., 2020; Ren et al., 2019)
or “semantic-modifying” (Shi and Huang, 2020;
Jia and Liang, 2017). Note this is slightly dif-
ferent from the above label-preserving assump-
tions, as it is defined over the perturbations on
(x, x’) rather than making an assumption on (y, 7’),
e.g., semantic-modifying perturbations can be ei-
ther label-preserving (Jia and Liang, 2017; Shi and
Huang, 2020) or label-changing (Gardner et al.,
2020; Kaushik et al., 2019).

2.2 Robustness under Distribution Shift

Another line of research focuses on (’, y’) drawn
from a different distribution that is naturally-
occurring (Hendrycks et al., 2021), where robust-
ness can be defined around model’s performance
under distribution shift. Different from work on
domain adaptation (Patel et al., 2015; Wilson and
Cook, 2020) and transfer learning (Pan and Yang,
2010), existing definitions of robustness are closer
to the concept of domain generalization (Muan-
det et al., 2013; Gulrajani and Lopez-Paz, 2021), or
out-of-distribution generalization to unforeseen dis-
tribution shifts (Hendrycks et al., 2020a), where the
test data (either labeled or unlabeled) is assumed
not available during training, i.e., generalization
without adaptation. In the context of NLP, robust-
ness to natural distribution shifts can also mean
models’ performance should not degrade due to the
differences in grammar errors, dialects, speakers,


languages (Craig and Washington, 2002; Blodgett
et al., 2016; Demszky et al., 2021), or newly col-
lected datasets for the same task but in different
domains (Miller et al., 2020). Another closely con-
nected line of research is fairness, which has been
studied in various NLP applications, see (Sun et al.,
2019) for a more in-depth survey in this area. For
example, gendered stereotypes or biases have been
observed in NLP tasks including co-reference reso-
lution (Zhao et al., 2018a; Rudinger et al., 2017),
occupation classification (De-Arteaga et al., 2019),
and neural machine translation (Prates et al., 2019;
Font and Costa-jussa, 2019).

2.3 Connections and A Common Theme

The above two categories of robustness can be uni-
fied under the same framework, i.e., whether D’
represents a synthetic distribution shift (via adver-
sarial attacks) or a natural distribution shift. Exist-
ing work has shown a model’s performance might
degrade substantially in both cases, but the trans-
ferability of the two categories is relatively under-
explored. In the vision domain, Taori et al. (2020)
investigate models’ robustness to natural distribu-
tion shift, and show that robustness to synthetic
distribution shift might offer little to no robustness
improvement under natural distribution shift. Some
studies show NLP models might not generalize to
unseen adversarial patterns (Huang et al., 2020;
Jha et al., 2020; Joshi and He, 2021), but more
work is needed to systematically bridge the gap be-
tween NLP models’ robustness under natural and
synthetic distribution shifts.

To better understand why models exhibit a lack
of robustness, some existing work attributed this to
the fact that models sometimes utilize spurious cor-
relations between input features and labels, rather
than the genuine ones, where spurious features are
commonly defined as features that do not causally
affect a task’s label (Srivastava et al., 2020; Wang
and Culotta, 2020b): they correlate with task labels
but fail to transfer to more challenging test con-
ditions or out-of-distribution data (Geirhos et al.,
2020). Some other work defined it as “prediction
rules that work for the majority examples but do
not hold in general” (Tu et al., 2020). Such spuri-
ous correlations are sometimes referred as dataset
bias (Clark et al., 2019; He et al., 2019), annota-
tion artifacts (Gururangan et al., 2018), or group
shift (Oren et al., 2019) in the literature. Further,
evidence showed that controlling model’s learning

in spurious features will improve model’s perfor-
mances in distribution shifts (Wang et al., 2019a,b);
also, discussions on the connections between adver-
sarial robustness and learning of spurious features
have been raised (Ilyas et al., 2019; Wang et al.,
2020a). Theoretical discussions connecting these
fields have also been offered by crediting a reason
of model’s lack of robustness in either distribution
shift or adversarial attack to model’s learning of
spurious features (Wang et al., 2021c).

Further, in certain applications, model “robust-
ness” can also be connected with models’ insta-
bility (Milani Fard et al., 2016), or models hav-
ing poorly-calibrated uncertainty estimation (Guo
et al., 2017), where Bayesian methods (Graves,
2011; Blundell et al., 2015), dropout-based (Gal
and Ghahramani, 2016; Kingma et al., 2015) and
ensemble-based approaches (Lakshminarayanan
et al., 2017) have been proposed to improve mod-
els’ uncertainty estimation. Recently, Ovadia et al.
(2019) have shown models’ uncertainty estimation
can degrade significantly under distributional shift,
and call for more work to ensure a model “knows
when it doesn’t know” by giving lower uncertainty
estimates over out-of-distribution data. This is an-
other example where models can be less robust
under distributional shifts, and again emphasizes
the need of building more unified benchmarks to
measure a model’s performance (e.g., robust accu-
racy, calibration, stability) under distribution shifts,
in addition to in-distribution accuracy.

3 Robustness in Vision vs. in NLP

Despite the widely study of robustness in vision,
the study of robustness in NLP cannot always di-
rectly borrow the ideas. We categorize the main
differences with the three following points:

Continuous vs. Discrete in Search Space The
most obvious characteristic is probably the discrete
nature of the space of text. This particularly posed
a challenge towards the adversarial attack and de-
fense regime when the study in vision is transferred
to NLP (Lei et al., 2019; Zhang et al., 2020c), in the
sense that simple gradient-based adversarial attacks
will not directly translate to meaningful attacks in
the discrete text space, and multiple novel attack
methods are proposed to fill the gap, as we will
discuss in later sections.

Perceptible to Human vs. Not On a related
topic, one of the most impressive property of ad-


versarial attack in vision is that small perturbation
of the image data imperceptible to human are suffi-
cient to deceive the model (Szegedy et al., 2013),
while this can hardly be true for NLP attacks. In-
stead of being imperceptible, the adversarial at-
tacks in NLP typically are bounded by the fact that
the meaning of the sentences are not altered (de-
spite being perceptible). On the other hand, there
are ways to generate samples where the changes,
although being perceptible, are often ignored by
human brain due to some psychological prior on
how a human processes the text (Anastasopoulos
et al., 2019; Wang et al., 2020b).

Support vs. Density Difference of the Data Dis-
tributions Another difference is more likely seen
in the discussion of the domain adaptation of vision
and NLP study. In vision study, although the im-
ages from training distribution and test distribution
can be sufficiently different, the train and test distri-
butions mostly share the same support (the pixels
are always sampled from a 0-255 integer space),
although the density of these distributions can be
very different (e.g., photos vs. sketches). On the
other hand, domain adaptation of NLP sometimes
studies the regime where the supports of the data
differ, e.g., the vocabularies can be significantly
different in cross-lingual studies (Abad et al., 2020;
Zhang et al., 2020a).

A Common Theme Despite the disparities be-
tween vision and NLP, the common theme of push-
ing the model to generalize from D to D’ preserves.
The practical difference between D and D’ is more
than often defined by the human’s understanding
of the data, and can differ in vision and NLP as
humans perceive and process images and texts in
subtly different ways, which creates both opportu-
nities for learning and barriers for direct transfer.
Certain lines of research try to bridge the learning
in the vision domain to the embedding space in the
NLP domain, while other lines of research create
more interpretable attacks in the discrete text space
(see Table 1 for these two lines of work). How
those two lines of research transfer to each other,
or complement each other, is not fully explored and
calls for additional research.

4 Identify Robustness Failures

As robustness gained increasing attention in NLP
literature, various lines of work have proposed
ways to identify robustness failures in NLP models.

Existing works can be roughly categorized by how
the failures are identified, among which a large
portion of work relies on human priors and error
analyses over existing NLP models (Section 4.1),
and other lines of work adopt model-based ap-
proaches (Section 4.2). The identified robustness
failure patterns are usually organized into challeng-
ing/adversarial benchmark datasets to more accu-
rately measure an NLP model’s robustness. In Ta-
ble 1, we organize commonly used perturbation
types for identifying models’ robustness failures,
and in Table 2 we summarize common robustness
benchmarks for each NLP task.

4.1 Human Prior and Error Analyses Driven

An increasing body of work has been conducted on
understanding and measuring robustness in NLP
models (Tu et al., 2020; Sagawa et al., 2020b;
Geirhos et al., 2020) across various NLP tasks,
largely relying on human priors and error analyses.

Natural Language Inference Naik et al. (2018)
sampled misclassified examples and analyzed their
potential sources of errors, which are then grouped
into a typology of common reasons for error. Such
error types then served as the bases to construct
the stress test set, to further evaluate whether NLI
models have the ability to make real inferential
decisions, or simply rely on sophisticated pattern
matching. Gururangan et al. (2018) found that
current NLI models are likely to identify the la-
bel by relying only on the hypothesis, and Poliak
et al. (2018) provided similar augments that us-
ing a hypothesis-only model can outperform a set
of strong baselines. Kaushik et al. (2019) asked
humans to generate counterfactual NLI examples,
to better understand what features are causal and
encourage models to learn those features.

Question Answering Jia and Liang (2017) pro-
posed to generate adversarial QA examples by con-
catenating an adversarial distracting sentence at the
end of a paragraph. Miller et al. (2020) built four
new test sets for the Stanford Question Answer-
ing Dataset (SQuAD) and found most question-
answering systems fail to generalize to this new
data, calling for new evaluation metrics towards
natural distribution shifts.

Machine Translation Belinkov and Bisk (2018)
found that character-based neural machine trans-
lation (NMT) models are brittle under noisy data,
where noises (e.g., typos, misspellings, etc) are


Space Perturbation level Methods
HotFlip (Ebrahimi et al., 2018), DeepWordBug (Gao et al., 2018),
Character level Synthetic-Noise (Karpukhin et al., 2019)
Discrete GenAdv (Alzantot et al., 2018), PWWS (Ren et al., 2019),
Word-level SEM (Wang et al., 2019e), BERT-ATTACK (Li et al., 2020),
TextFooler (Jin et al., 2020), SememePSO (Zang et al., 2020)
Sent level AdvSQuAD (Jia and Liang, 2017), SCPNs (lyyer et al., 2018),
SERRE HES CAT-Gen (Wang et al., 2020c), TAILOR (Ross et al., 2021)
. CheckList (Ribeiro et al., 2020), Polyjuice (Wu et al., 2021),
Mixed:types MAYA (Chen et al., 2021c)
5 : AT & VAT (Miyato et al., 2017), Natural-adversary (Zhao et al., 2018b),
Continuous | Embedding space FreeLB (Zhu et al., 2020), ALUM (Liu et al., 2020)

Table |: Perturbation types for identifying robustness failures and improving robustness in NLP.

Task

Robustness Benchmarks

Natural Language Inference

Stress-test (Naik et al., 2018), HANS (McCoy et al., 2019),
Counterfactual-NLI (Kaushik et al., 2019), ANLI (Nie et al., 2020)

Question Answering

AdvSQuAD (Jia and Liang, 2017), Adv-QA (Bartolo et al., 2020),
Natural-Perturbed-QA (Khashabi et al., 2020),
Natural-shift-QA (Miller et al., 2020), SAM (Schlegel et al., 2021)

Paraphrase Identification

PAWS (Zhang et al., 2019b), PAWS-X (Yang et al., 2019),
Modify-with-Shared-Words (Shi and Huang, 2020)

Co-reference

WinoGender (Rudinger et al., 2018), WinoBias (Zhao et al., 2018a)

Named Entity Recognition

OntoRock (Lin et al., 2021), SeqAttack (Simoncini and Spanakis, 2021)

Table 2: A list of robustness benchmarks (challenging or adversarial datasets) and their corresponding tasks.

synthetically generated using possible lexical re-
placements. Data augmentation with artificially-
introduced grammatical errors (Anastasopoulos
et al., 2019) or with random synthetic noises (Vaib-
hav et al., 2019; Karpukhin et al., 2019) can make
the system more robust to such spurious patterns.
On the other hand, Wang et al. (2020b) showed
another approach by limiting the input space of
the characters so that the models will be likely to
perceive data typos and misspellings.

Syntactic and Semantic Parsing Robust pars-
ing has been studied in several existing works (Lee
et al., 1995; Ait-Mokhtar et al., 2002). More recent
work showed that neural semantic parsers are still
not robust against lexical and stylistic variations,
or meaning-preserving perturbations (Marzinotto
et al., 2019; Huang et al., 2021), and proposed ways
to improve their robustness through data augmenta-
tion (Huang et al., 2021) and adversarial learning
(Marzinotto et al., 2019).

Text Generation Existing work found that text
generation models also suffer from robustness is-
sues, e.g., text summarization models suffer from
positional bias (Jung et al., 2019), layout bias

(Kryscinski et al., 2019), and a lack of faithfulness
and factuality (Kryscinski et al., 2019; Maynez
et al., 2020; Chen et al., 2021b); data-to-text mod-
els sometimes hallucinate texts that are not sup-
ported by the data (Parikh et al., 2020; Wang et al.,
2020d). In addition, Sellam et al. (2020); Zhang
et al. (2020b) pointed out the deficiency of exist-
ing automatic evaluation metrics and proposed new
metrics to better align the generation quality with
human judgements.

Connection with Dataset Biases The robust-
ness failures can sometimes be attributed to dataset
biases, i.e., biases introduced during dataset col-
lection (Fouhey et al., 2018) or human annotation
artifacts (Gururangan et al., 2018; Geva et al., 2019;
Rudinger et al., 2017), which could affect how well
a model trained from this dataset generalizes, and
how accurately we estimate a model’s performance.
For example, Lewis et al. (2021) show there is a
significant test-train data overlap in a set of open-
domain question-answering benchmarks, and many
QA models perform substantially worse on ques-
tions that cannot be memorized from training data.
In natural language inference, McCoy et al. (2019)


show that commonly used crowdsourced datasets
for training NLI models might make certain syn-
tactic heuristics more easily adopted by statistical
learners. Further, Bras et al. (2020) propose to use
a lightweight adversarial filtering approach to filter
dataset biases, which is approximated using each
instance’s predictability score.

4.2 Model-based Identification

In addition to the human-prior and error-analysis
driven approaches which are usually specific to
each task, other lines of work identify robustness
failures that are task-agnostic like white-box text at-
tack methods (Ebrahimi et al., 2018; Alzantot et al.,
2018; Jin et al., 2020), and even input-agnostic like
universal adversarial triggers (Wallace et al., 2019a)
and natural attack triggers (Song et al., 2021).

Another line of work proposes to learn an addi-
tional model to capture biases, e.g., in visual ques-
tion answering, Clark et al. (2019) train a naive
model to predict prototypical answers based on the
question only irrespective of the context; He et al.
(2019); Utama et al. (2020a) propose to learn a
biased model that only uses dataset-bias related
features. This framework has also been used to
capture unknown biases assuming that the lower
capacity model learns to capture relatively shallow
correlations during training (Clark et al., 2020). In
addition, Wang and Culotta (2020a) identify model
shortcuts by training classifiers to better distinguish
“spurious” correlations from “genuine” ones based
on human annotated examples.

Model-in-the-loop vs. Human-in-the-loop
Some work adopts human-in-the-loop to gener-
ate challenging examples, e.g., Counterfacutal-NLI
(Kaushik et al., 2019) and Natural-Perturbed-QA
(Khashabi et al., 2020). Other work applies model-
in-the-loop to increase the likelihood that the per-
turbed examples are challenging for state-of-the-art
models, but it might also introduce biases towards
the particular model used. For example, SWAG
(Zellers et al., 2018) was introduced that fooled
most models at the time of publishing but was soon
“solved” after BERT (Devlin et al., 2019) was in-
troduced. As a result, Yuan et al. (2021) present
a study over the transferability of adversarial ex-
amples, and Contrast Sets (Gardner et al., 2020)
intentionally avoid using model-in-the-loop. Fur-
ther, more recent work adopts adversarial human-
and-model-in-the-loop to create more difficult ex-
amples for benchmarking, e.g., Adv-QA (Bartolo

et al., 2020), Adv-Quizbowl (Wallace et al., 2019b),
ANLI (Nie et al., 2020), and Dynabench (Kiela
et al., 2021).

5 Improve Model Robustness

Correspondingly, there are multiple lines of direc-
tions that try to improve robustness in NLP mod-
els. Depending on where and how the intervention
is applied, those approaches can be categorized
into the following categories: data-driven (Sec-
tion 5.1), model-based and training-scheme-based
(Section 5.2), inductive-prior-based (Section 5.3)
and finally causal intervention (Section 5.4).

5.1 Data-driven Approaches

Data augmentation recently gained a lot of interest,
in improving performance in low-resourced lan-
guage settings, few-shot learning, mitigating biases,
and improving robustness in NLP models (Feng
et al., 2021; Dhole et al., 2021). Techniques like
Mixup (Zhang et al., 2018), MixText (Chen et al.,
2020), CutOut (DeVries and Taylor, 2017), Aug-
Mix (Hendrycks et al., 2020b), HiddenCut (Chen
et al., 2021a), have been shown to substantially
improve the robustness and the generalization of
models. Such mitigation strategies are operated at
the data level, and often hard to be interpreted in
terms of how and why mitigation works.

Other lines of work deal with spans or regions
associated within data points to prevent models
from heavily relying on spurious patterns. To make
NLP models more robust on sentiment analysis and
NLI tasks, Kaushik et al. (2019) proposed curating
counterfactually augmented data via a human-in-
the-loop process, and showed that models trained
on the combination of this augmented data and
original data are less sensitive to spurious patterns.
Differently, Wang et al. (2021d) performed strate-
gic data augmentation to perturb the set of “‘short-
cuts” that are automatically identified, and found
that mitigating these leads to more robust models in
multiple NLP tasks. This line of mitigation strate-
gies closely relates to how spurious correlations
can be measured and identified, as many of the
challenging or adversarial examples (Table 1) can
sometimes be used to augment the original model
to improve its robustness, either in the discrete in-
put space as additional training examples (Liu et al.,
2019; Kaushik et al., 2019; Anastasopoulos et al.,
2019; Vaibhav et al., 2019; Khashabi et al., 2020),
or in the embedding space (Zhu et al., 2020; Zhao


et al., 2018b; Miyato et al., 2017; Liu et al., 2020).

5.2 Model and Training-based Approaches

Pre-training Recent work has demonstrated pre-
training as an effective way to improve NLP
models’ out-of-distribution robustness (Hendrycks
et al., 2020a; Tu et al., 2020), potentially due to
its self-supervised objective and the use of large
amounts of diverse pre-training data that encour-
ages generalization from a small number of exam-
ples that counter the spurious correlations. Tu et al.
(2020) showed a few other factors can also con-
tribute to robust accuracy, including larger model
size, more fine-tuning data, and longer fine-tuning.
A similar observation is made by Taori et al. (2020)
in the vision domain, where the authors found train-
ing with larger and more diverse datasets offer bet-
ter robustness consistently in multiple cases, com-
pared to various robustness interventions proposed
in the existing literature.

Training with a Better Use of Minority Exam-
ples Further, there are several works that propose
to robustify the models via a better use of minority
examples, e.g., examples that are under-represented
in the training distribution, or examples that are
harder to learn. For example, Yaghoobzadeh et al.
(2021) proposed to first fine-tune the model on the
full data, and then on minority examples only.

In general, the training strategy with an empha-
sis on a subset of samples that are particularly hard
for the model to learn is sometimes also referred
to as group DRO (Sagawa et al., 2020a), as an ex-
tension of vanilla distributional robust optimization
(DRO) (Ben-Tal et al., 2013; Duchi et al., 2021).
Extensions of DRO are mostly discussing the strate-
gies on how to identify the samples considered as
minority: Nam et al. (2020) trained two models
in parallel, where the “debiased” model focuses
on examples not learned by the “biased” model;
Lahoti et al. (2020) used an adversary model to
identify samples that are challenging to the main
model; Liu et al. (2021) proposed to train the model
a second time via up-weighting examples that have
high training losses during the first time.

When to Use Data-driven or Model-based Ap-
proaches? In many cases both the data and the
model can contribute to a model’s lack of ro-
bustness, hence data-driven and model-based ap-
proaches could be combined to further improve a
model’s robustness. One interesting phenomenon

observed by (Liu et al., 2019) is to attribute mod-
els’ robustness failures to blind spots in the training
data, or the intrinsic learning ability of the model.
The authors found that both patterns are possible:
in some cases models can be inoculated via be-
ing exposed to a small amount of challenging data,
similar to the data augmentation approaches men-
tioned in Section 5.1; on the other hand, some
challenging patterns remain difficult which con-
nects to the larger question around generalizability
to unseen adversarial and counterfactual patterns
(Huang et al., 2020; Jha et al., 2020; Joshi and
He, 2021), which is relatively under-explored but
deserves much attention.

5.3. Inductive-prior-based Approaches

Another thread is to introduce inductive bias (i.e., to
regularize the hypothesis space) to force the model
to discard some spurious features. This is closely
connected to the human-prior-based identification
approaches in Section 4.1 as those human-priors
can often be used to re-formulate the training ob-
jective with additional regularizers. To achieve
this goal, one usually needs to first construct a
side component to inform the main model about
the misaligned features, and then to regularize the
main model according to the side component. The
construction of this side component usually relies
on prior knowledge of what the misaligned fea-
tures are. Then, methods can be built accordingly
to counter the features such as label-associated
keywords (He et al., 2019), label-associated text
fragments (Mahabadi et al., 2020), and general
easy-to-learn patterns of data (Nam et al., 2020).
Similarly, Clark et al. (2019, 2020); Utama et al.
(2020a,b) propose to ensemble with a model ex-
plicitly capturing bias, where the main model is
trained together with this “bias-only” model such
that the main model is discouraged from using bi-
ases. More recent work (Xiong et al., 2021) shows
the ensemble-based approaches can be further im-
proved via better calibrating the bias-only model.
Furthermore, additional regularizers have been in-
troduced for robust fine-tuning over pre-trained
models, e.g., mutual-information-based regulariz-
ers (Wang et al., 2021a) and smoothness-inducing
adversarial regularization (Jiang et al., 2020).

In a broader scope, given that one of the main
challenges of domain adaptation is to counter the
model’s tendency in learning domain-specific spu-
rious features (Ganin et al., 2016), some methods


contributing to domain adaption may have also pro-
gressed along the line of our interest, e.g., domain
adversarial neural network (Ganin et al., 2016).
This line of work also inspires a family of methods
forcing the model to learn auxiliary-annotation-
invariant representations with a side component
(Ghifary et al., 2016; Wang et al., 2017; Rozantsev
et al., 2018; Motiian et al., 2017; Li et al., 2018;
Wang et al., 2019c; Vernikos et al., 2020).

Despite the diverse concrete ideas introduced,
the above is mainly training for small empirical
loss across different domains or distributions in
addition to forcing the model to be invariant to
domain-specific spurious features. As an exten-
sion along this direction, invariant risk minimiza-
tion IRM) (Arjovsky et al., 2019) introduces the
idea of invariant predictors across multiple environ-
ments, which was later followed and discussed by
a variety of extensions (Choe et al., 2020; Ahmed
et al., 2020; Rosenfeld et al., 2021). More recently,
Dranker et al. (2021) applied IRM in natural lan-
guage inference and found that a more naturalistic
characterization of the problem setup is needed.

5.4 Causal Intervention

Casual analyses have also been utilized to exam-
ine robustness. Srivastava et al. (2020) leverage
humans’ common sense knowledge of causality to
augment training examples with a potential unmea-
sured variable, and propose a DRO-based approach
to encourage the model to be robust to distribution
shifts over the unmeasured variables. Balashankar
et al. (2021) study the effect of secondary attributes,
or confounders, and propose context-aware coun-
terfactuals that take into account the impact of sec-
ondary attributes to improve models’ robustness.
Veitch et al. (2021) propose to learn approximately
counterfactual invariant predictors dependent on
causal structures of the data, and show it can help
mitigate spurious correlations in text classification.

5.5 Connections between Mitigations

Connecting these methods conceptually, we con-
jecture three different mainstream approaches: one
is to leverage the large amount of data by taking ad-
vantages of pre-trained models, another is to learn
invariant representations or predictors across do-
mains or environments, while most of the rest build
upon the prior on what the spurious patterns are and
encourage the models to not rely on those patterns.
Then the solutions are invented through countering
model’s learning of these patterns by either data

augmentation, reweighting (the minorities), ensem-
ble, inductive-prior design, and causal interven-
tion. Interestingly, statistical work has shown that
many of these mitigation methods are optimizing
the same robust machine learning generalization
error bound (Wang et al., 2021c).

6 Open Questions

In addition to the challenges mentioned above, we
list below a few open questions that call for addi-
tional research going forward.

Identifying Unknown Robustness Failures Ex-
isting identification around robustness failures rely
heavily on human priors and error analyses, which
usually pre-define a small or limited set of patterns
that the model could be vulnerable to. This re-
quires extensive amount of expertise and efforts,
and might still suffer from human or subjective bi-
ases in the end. How to proactively discover and
identify models’ unrobust regions automatically
and comprehensively remains challenging.

Interpreting and Mitigating Spurious Correla-
tions Interpretability matters for large NLP mod-
els, especially key to the robustness and spurious
patterns. How can we develop ways to attribute or
interpret these vulnerable portions of NLP models
and communicate these robustness failures with
designers, practitioners, and users? In addition,
recent work (Wallace et al., 2019c; Wang et al.,
2021d; Zhang et al., 2021) show interpretability
methods can be utilized to better understand how
a model makes its decision, which in turn can be
used to uncover models’ bias, diagnose errors, and
discover spurious correlations.

Furthermore, the mitigation of spurious corre-
lations often suffers from the trade-off between
removing shortcuts and sacrificing model perfor-
mance (Yang et al., 2020; Zhang et al., 2019a). Ad-
ditionally, most existing mitigation strategies work
in a pipeline fashion where defining and detect-
ing spurious correlations are prerequisites, which
might lead to error cascades in this process. How
to design end-to-end frameworks for automatic mit-
igation deserves much attention.

Unified Framework to Evaluate Robustness
With a variety of potential spurious patterns in NLP
models, it becomes increasingly challenging for
developers and practitioners to quickly evaluate
the robustness and quality of their models. This
calls for more unified benchmarking efforts such as


CheckList (Ribeiro et al., 2020), Reliability Test-
ing (Tan et al., 2021), Robustness Gym (Goel et al.,
2021) and Dynabench (Kiela et al., 2021), to facili-
tate fast and easy evaluation of robustness.

User Centered Measures and Mitigation In-
stead of passively detecting spurious correlations
from a post-processing perspective, how to ap-
proach robustness from a user centric perspective
needs further investigation. Based on the dual-
process models of information processing, humans
use two different processing styles (Evans, 2010).
One is a quick and automatic style that relies on
well-learned information and heuristic cues. The
other is a qualitatively different style that is slower,
more deliberative, and requires more reflective rea-
soning. Would these well-learned information and
heuristic rules be leveraged to help design better
human priors to measure and mitigate spurious cor-
relations? If users or stakeholders are involved in
this process, collecting a set of test cases where a
system might perform well for the wrong reasons
could help design sanity tests.

Connections between Human-like Linguis-
tic Generalization and NLP Generalization
Linzen (2020) argue NLP models should behave
more like humans to achieve better generalization
consistently. It is interesting to note that how
humans process information in NLP tasks exactly
is still under exploration, and to what extent
models should leverage human-knowledge is still
a debatable topic.! Nonetheless, if we can better
understand and utilize the robustness properties
in human perception, we can potentially advance
models’ robustness in a more meaningful way.

7 Conclusion

In this paper, we provided a unifying overview
over robustness definitions, evaluations and mit-
igation strategies in the NLP domain. We also
highlighted open challenges in this area to motivate
future research, encouraging people to think deeply
about more comprehensive benchmarks, transfer-
ability and validity of adversarial examples, uni-
fied framework to evaluate and improve robustness,
user-centered measures and mitigation, and finally
how to potentially achieve human-like linguistic
generalization more meaningfully.

‘http: //www.incompleteideas.net/
IncIdeas/BitterLesson.html

Acknowledgements

The authors would like to thank reviewers for their
helpful insights and feedback. This work is funded
in part by a grant from Google.

References

Alberto Abad, Peter Bell, Andrea Carmantini, and
Steve Renals. 2020. Cross lingual transfer learn-
ing for zero-resource domain adaptation. In ICASSP
2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 6909-6913.

Faruk Ahmed, Yoshua Bengio, Harm van Seijen, and
Aaron Courville. 2020. Systematic generalisation
with group invariant predictions. In International
Conference on Learning Representations.

S. Ait-Mokhtar, J.-P. Chanod, and C. Roux. 2002.
Robustness beyond shallowness: Incremental deep
parsing. Nat. Lang. Eng., 8(3):121-144.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial ex-
amples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2890-2896, Brussels, Belgium. Association
for Computational Linguistics.

Antonios Anastasopoulos, Alison Lui, Toan Q Nguyen,
and David Chiang. 2019. Neural machine transla-
tion of text from non-native speakers. In Proceed-
ings of NAACL-HLT, pages 3070-3080.

Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and
David Lopez-Paz. 2019. Invariant risk minimization.
arXiv preprint arXiv: 1907.02893.

Ananth Balashankar, Xuezhi Wang, Ben Packer,
Nithum Thain, Ed Chi, and Alex Beutel. 2021. Can
we improve model robustness through secondary at-
tribute counterfactuals? In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing.

Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas-
tian Riedel, and Pontus Stenetorp. 2020. Beat the
AI: Investigating Adversarial Human Annotation for
Reading Comprehension. Transactions of the Asso-
ciation for Computational Linguistics, 8:662—678.

Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and natural noise both break neural machine transla-
tion. In International Conference on Learning Rep-
resentations.

Aharon Ben-Tal, Dick Den Hertog, Anja De Waege-
naere, Bertrand Melenberg, and Gijs Rennen. 2013.
Robust solutions of optimization problems affected
by uncertain probabilities. Management Science,
59(2):34 1-357.


Su Lin Blodgett, Lisa Green, and Brendan O’Connor.
2016. Demographic dialectal variation in social
media: A case study of African-American English.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1119-1130, Austin, Texas. Association for Compu-
tational Linguistics.

Charles Blundell, Julien Cornebise, Koray
Kavukcuoglu, and Daan Wierstra. 2015. Weight
uncertainty in neural network. In Proceedings of
the 32nd International Conference on Machine
Learning, volume 37 of Proceedings of Machine
Learning Research, pages 1613-1622, Lille, France.
PMLR.

N. Boucher, I. Shumailov, R. Anderson, and N. Pa-
pernot. 2022. Bad characters: Imperceptible nlp
attacks. In 2022 2022 IEEE Symposium on Secu-
rity and Privacy (SP), pages 773-790, Los Alamitos,
CA, USA. IEEE Computer Society.

Ronan Le Bras, Swabha Swayamdipta, Chandra Bhaga-
vatula, Rowan Zellers, Matthew Peters, Ashish Sab-
harwal, and Yejin Choi. 2020. Adversarial filters
of dataset biases. In Proceedings of the 37th Inter-
national Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research,
pages 1078-1088. PMLR.

Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi
Yang. 2021a. Hiddencut: Simple data augmentation
for natural language understanding with better gener-
alizability. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 4380-4390.

Jiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-
Text: Linguistically-informed interpolation of hid-
den space for semi-supervised text classification. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2147—
2157, Online. Association for Computational Lin-
guistics.

Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth.
2021b. Improving faithfulness in abstractive sum-
marization with contrast candidate generation and
selection. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 5935-5941, Online. Association for
Computational Linguistics.

Yangyi Chen, Jin Su, and Wei Wei. 2021c. Multi-
granularity textual adversarial attack with behavior
cloning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 4511-4526, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.

Yo Joong Choe, Jiyeon Ham, and Kyubyong Park.

2020. An empirical study of invariant risk minimiza-
tion. arXiv preprint arXiv:2004.05007.

Christopher Clark, Mark Yatskar, and Luke Zettle-

moyer. 2019. Don’t take the easy way out: En-
semble based methods for avoiding known dataset
biases. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
4069-4082, Hong Kong, China. Association for
Computational Linguistics.

Christopher Clark, Mark Yatskar, and Luke Zettle-

moyer. 2020. Learning to model and ignore dataset
bias with mixed capacity ensembles. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing: Findings, pages
3031-3045.

Holly Craig and Julie Washington. 2002. Oral lan-

guage expectations for african american preschool-
ers and kindergartners. American Journal of Speech-
Language Pathology, 11:59-70.

Maria De-Arteaga, Alexey Romanov, Hanna Wal-

lach, Jennifer Chayes, Christian Borgs, Alexandra
Chouldechova, Sahin Geyik, Krishnaram Kentha-
padi, and Adam Tauman Kalai. 2019. Bias in bios:
A case study of semantic representation bias in a
high-stakes setting. In Proceedings of the Confer-
ence on Fairness, Accountability, and Transparency,
FAT* °19, page 120-128, New York, NY, USA. As-
sociation for Computing Machinery.

Dorottya Demszky, Devyani Sharma, Jonathan Clark,

Vinodkumar Prabhakaran, and Jacob Eisenstein.
2021. Learning to recognize dialect features. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
2315-2338, Online. Association for Computational
Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Terrance DeVries and Graham W Taylor. 2017.

Improved regularization of convolutional neu-
ral networks with cutout. arXiv preprint
arXiv: 1708.04552.

Kaustubh D. Dhole, Varun Gangal, Sebastian

Gehrmann, Aadesh Gupta, Zhenhao Li, Saad
Mahamood, Abinaya Mahendiran, Simon Mille,
Ashish Srivastava, and Samson Tan et al. 2021.
Nl-augmenter: A framework for task-sensitive
natural language augmentation.


Yana Dranker, He He, and Yonatan Belinkov. 2021.
Irm - when it works and when it doesn’t: A test case
of natural language inference. In Neural Informa-
tion Processing Systems (NeurIPS).

John C Duchi, Peter W Glynn, and Hongseok
Namkoong. 2021. Statistics of robust optimization:
A generalized empirical likelihood approach. Math-
ematics of Operations Research.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. HotFlip: White-box adversarial exam-
ples for text classification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
31-36, Melbourne, Australia. Association for Com-
putational Linguistics.

Jonathan St BT Evans. 2010. Intuition and reasoning:
A dual-process perspective. Psychological Inquiry,
21(4):313-326.

Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,
Pedro Rodriguez, and Jordan Boyd-Graber. 2018.
Pathologies of neural models make interpretations
difficult. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 3719-3728, Brussels, Belgium. Association
for Computational Linguistics.

Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chan-
dar, Soroush Vosoughi, Teruko Mitamura, and Ed-
uard Hovy. 2021. A survey of data augmentation
approaches for NLP. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021,
pages 968-988, Online. Association for Computa-
tional Linguistics.

Joel Escudé Font and Marta R. Costa-jussa. 2019.
Equalizing gender biases in neural machine trans-
lation with word embeddings techniques. CoRR,
abs/1901.03116.

David F. Fouhey, Weicheng Kuo, Alexei A. Efros, and
Jitendra Malik. 2018. From lifestyle vlogs to every-
day interactions. In CVPR.

Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a
bayesian approximation: Representing model uncer-
tainty in deep learning. In International Conference
on Machine Learning, pages 1050-1059.

Wee Chung Gan and Hwee Tou Ng. 2019. Improv-
ing the robustness of question answering systems
to question paraphrasing. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 6065-6075, Florence,
Italy. Association for Computational Linguistics.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Frangois Laviolette,
Mario Marchand, and Victor S. Lempitsky. 2016.
Domain-adversarial training of neural networks. J.
Mach. Learn. Res., 17:59:1-59:35.

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-
jun Qi. 2018. Black-box generation of adversarial
text sequences to evade deep learning classifiers. In
2018 IEEE Security and Privacy Workshops (SPW),
pages 50-56. IEEE.

Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan
Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,
Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,
Nitish Gupta, Hannaneh Hajishirzi, Gabriel [lharco,
Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-
son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer
Singh, Noah A. Smith, Sanjay Subramanian, Reut
Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.
2020. Evaluating models’ local decision boundaries
via contrast sets. In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
1307-1323, Online. Association for Computational
Linguistics.

Siddhant Garg and Goutham Ramakrishnan. 2020.
BAE: BERT-based adversarial examples for text
classification. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 6174-6181, Online. As-
sociation for Computational Linguistics.

Robert Geirhos, J6érm-Henrik Jacobsen, Claudio
Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A. Wichmann. 2020.
Shortcut learning in deep neural networks. Nature
Machine Intelligence, 2(11):665-673.

Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019.
Are we modeling the task or the annotator? an inves-
tigation of annotator bias in natural language under-
standing datasets. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 1161-1166, Hong Kong, China. As-
sociation for Computational Linguistics.

Muhammad Ghifary, W Bastiaan Kleijn, Mengjie
Zhang, David Balduzzi, and Wen Li. 2016. Deep
reconstruction-classification networks for unsuper-
vised domain adaptation. In European Conference
on Computer Vision, pages 597-613. Springer.

Karan Goel, Nazneen Fatema Rajani, Jesse Vig,
Zachary Taschdjian, Mohit Bansal, and Christopher
Ré. 2021. Robustness gym: Unifying the NLP eval-
uation landscape. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Demonstrations, pages 42-55,
Online. Association for Computational Linguistics.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adversar-
ial examples (2014). In International Conference on
Learning Representations.

Alex Graves. 2011. Practical variational inference for
neural networks. In Advances in Neural Informa-


tion Processing Systems 24, pages 2348-2356. Cur-
ran Associates, Inc.

Ishaan Gulrajani and David Lopez-Paz. 2021. In
search of lost domain generalization. In Interna-
tional Conference on Learning Representations.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q.
Weinberger. 2017. On calibration of modern neu-
ral networks. In Proceedings of the 34th Interna-

tional Conference on Machine Learning - Volume 70,
ICML 17, page 1321-1330. JMLR.org.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A
Smith. 2018. Annotation artifacts in natural lan-
guage inference data. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 107-112.

He He, Sheng Zha, and Haohan Wang. 2019. Unlearn
dataset bias in natural language inference by fitting
the residual. In Proceedings of the 2nd Workshop on
Deep Learning Approaches for Low-Resource NLP
(DeepLo 2019), pages 132-142.

Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam
Dziedzic, Rishabh Krishnan, and Dawn Song. 2020a.
Pretrained transformers improve out-of-distribution
robustness. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguis-
tics.

Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret
Zoph, Justin Gilmer, and Balaji Lakshminarayanan.
2020b. AugMix: A simple data processing method
to improve robustness and uncertainty. Proceedings
of the International Conference on Learning Repre-
sentations (ICLR).

Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob
Steinhardt, and Dawn Song. 2021. Natural adver-
sarial examples. In CVPR.

Shuo Huang, Zhuang Li, Lizhen Qu, and Lei Pan. 2021.
On robustness of neural semantic parsers. In Pro-
ceedings of the 16th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Main Volume, pages 3333-3342, Online.
Association for Computational Linguistics.

William Huang, Haokun Liu, and Samuel R. Bowman.
2020. Counterfactually-augmented SNLI training
data does not yield better generalization than unaug-
mented data. In Proceedings of the First Workshop
on Insights from Negative Results in NLP, pages 82—
87, Online. Association for Computational Linguis-
tics.

Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras,
Logan Engstrom, Brandon Tran, and Aleksander
Madry. 2019. Adversarial examples are not bugs,
they are features. In Advances in Neural Informa-
tion Processing Systems, pages 125-136.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume I (Long Papers), pages 1875-1885, New
Orleans, Louisiana. Association for Computational
Linguistics.

Rohan Jha, Charles Lovering, and Ellie Pavlick. 2020.
Does data augmentation improve generalization in
nlp?

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2021-2031, Copenhagen, Denmark. Association for
Computational Linguistics.

Robin Jia, Aditi Raghunathan, Kerem Goksel, and
Percy Liang. 2019. Certified robustness to adver-
sarial word substitutions. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 4129-4142, Hong Kong,
China. Association for Computational Linguistics.

Haoming Jiang, Pengcheng He, Weizhu Chen, Xi-
aodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.
SMART: Robust and efficient fine-tuning for pre-
trained natural language models through principled
regularized optimization. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 2177-2190, Online. Asso-
ciation for Computational Linguistics.

Di Jin, Zhijing Jin, Joey Zhou, and Peter Szolovits.
2020. Is BERT really robust? Natural language at-
tack on text classification and entailment. In AAAI.

Nitish Joshi and He He. 2021. An investigation of
the (in)effectiveness of counterfactually augmented
data.

Taehee Jung, Dongyeop Kang, Lucas Mentch, and Ed-
uard Hovy. 2019. Earlier isn’t always better: Sub-
aspect analysis on corpus and system biases in sum-
marization. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),
pages 3324-3335, Hong Kong, China. Association
for Computational Linguistics.

Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, and
Marjan Ghazvininejad. 2019. Training on synthetic
noise improves robustness to natural noise in ma-
chine translation. In Proceedings of the 5th Work-
shop on Noisy User-generated Text (W-NUT 2019),
pages 42-47.


Divyansh Kaushik, Eduard Hovy, and Zachary Lipton.
2019. Learning the difference that makes a differ-
ence with counterfactually-augmented data. In Jnter-
national Conference on Learning Representations.

Daniel Khashabi, Tushar Khot, and Ashish Sabharwal.
2020. More bang for your buck: Natural perturba-
tion for robust question answering. In Proceedings
of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 163-
170, Online. Association for Computational Linguis-
tics.

Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh
Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-
gen, Grusha Prasad, Amanpreet Singh, Pratik Ring-
shia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,
Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mo-
hit Bansal, Christopher Potts, and Adina Williams.
2021. Dynabench: Rethinking benchmarking in
NLP. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4110-4124, Online. Association for
Computational Linguistics.

Durk P Kingma, Tim Salimans, and Max Welling.
2015. Variational dropout and the local reparame-
terization trick. In Advances in Neural Information
Processing Systems, volume 28. Curran Associates,
Inc.

Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-
Cann, Caiming Xiong, and Richard Socher. 2019.
Neural text summarization: A critical evaluation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 540—
551, Hong Kong, China. Association for Computa-
tional Linguistics.

Volodymyr Kuleshov, Shantanu Thakoor, Tingfung
Lau, and Stefano Ermon. 2018. Adversarial exam-
ples for natural language classification problems.

Keita Kurita, Paul Michel, and Graham Neubig. 2020.
Weight poisoning attacks on pretrained models. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2793—
2806, Online. Association for Computational Lin-
guistics.

Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee,
Flavien Prost, Nithum Thain, Xuezhi Wang, and
Ed Chi. 2020. Fairness without demographics
through adversarially reweighted learning. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 728-740. Curran Associates, Inc.

Balaji Lakshminarayanan, Alexander Pritzel, and
Charles Blundell. 2017. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In
Proceedings of the 31st International Conference on
Neural Information Processing Systems, NIPS’17,

page 6405-6416, Red Hook, NY, USA. Curran As-
sociates Inc.

Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and
Gil Chang Kim. 1995. A robust parser based on
syntactic information. In Seventh Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Dublin, Ireland. Association for
Computational Linguistics.

Qi Lei, Lingfei Wu, Pin- Yu Chen, Alex Dimakis, Inder-
jit S. Dhillon, and Michael J Witbrock. 2019. Dis-
crete adversarial attacks and submodular optimiza-
tion with applications to text classification. In Pro-
ceedings of Machine Learning and Systems, vol-
ume 1, pages 146-165.

Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.
2021. Question and answer test-train overlap in
open-domain question answering datasets. In Pro-
ceedings of the 16th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Main Volume, pages 1000-1008, Online.
Association for Computational Linguistics.

Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris
Brockett, Ming-Ting Sun, and Bill Dolan. 2021.
Contextualized perturbation for textual adversarial
attack. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 5053-5069, Online. Association for
Computational Linguistics.

Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C
Kot. 2018. Domain generalization with adversarial
feature learning. In Proc. IEEE Conf. Comput. Vis.
Pattern Recognit.(CVPR).

Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting
Wang. 2019. Textbugger: Generating adversarial
text against real-world applications. In 26th An-
nual Network and Distributed System Security Sym-
posium, NDSS 2019, San Diego, California, USA,
February 24-27, 2019. The Internet Society.

Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,
and Xipeng Qiu. 2020. BERT-ATTACK: Adversar-
ial attack against BERT using BERT. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
6193-6202, Online. Association for Computational
Linguistics.

Bill Yuchen Lin, Wenyang Gao, Jun Yan, Ryan
Moreno, and Xiang Ren. 2021. RockNER: A simple
method to create adversarial examples for evaluating
the robustness of named entity recognition models.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages
3728-3737, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.

Tal Linzen. 2020. How can we accelerate progress to-
wards human-like linguistic generalization? In Pro-


ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5210-
5217. Association for Computational Linguistics.

Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi
Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy
Liang, and Chelsea Finn. 2021. Just train twice: Im-
proving group robustness without training group in-
formation. In Proceedings of the 38th International
Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages
6781-6792. PMLR.

Nelson F, Liu, Roy Schwartz, and Noah A. Smith. 2019.
Inoculation by fine-tuning: A method for analyz-
ing challenge datasets. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume I (Long and Short
Papers), pages 2171-2179, Minneapolis, Minnesota.
Association for Computational Linguistics.

Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu
Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.
2020. Adversarial training for large neural language
models. arXiv preprint arXiv:2004.08994.

Rabeeh Karimi Mahabadi, Yonatan Belinkov, and
James Henderson. 2020. End-to-end bias mitigation
by modelling biases in corpora. In Proceedings of
the 58th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2020, Online, July 5-10,
2020, pages 8706-8716. Association for Computa-
tional Linguistics.

Gabriel Marzinotto, Géraldine Damnati, Frédéric
Béchet, and Benoit Favre. 2019. Robust semantic
parsing with adversarial learning for domain gener-
alization. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Industry Papers), pages 166—
173, Minneapolis, Minnesota. Association for Com-
putational Linguistics.

Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 1906-1919, On-
line. Association for Computational Linguistics.

Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019.
Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3428-3448,
Florence, Italy. Association for Computational Lin-
guistics.

Mahdi Milani Fard, Quentin Cormier, Kevin Canini,
and Maya Gupta. 2016. Launch and iterate: Reduc-
ing prediction churn. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett, editors,
Advances in Neural Information Processing Systems
29, pages 3179-3187. Curran Associates, Inc.

John Miller, Karl Krauth, Benjamin Recht, and Ludwig
Schmidt. 2020. The effect of natural distribution
shift on question answering models. In Proceedings
of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine
Learning Research, pages 6905-6916. PMLR.

Takeru Miyato, Andrew M. Dai, and Ian Goodfel-
low. 2017. Adversarial training methods for semi-
supervised text classification. In Proceedings of the
International Conference on Learning Representa-
tions.

John Morris, Eli Lifland, Jack Lanchantin, Yangfeng
Ji, and Yanjun Qi. 2020a. Reevaluating adversar-
ial examples in natural language. In Findings of the
Association for Computational Linguistics: EMNLP
2020, pages 3829-3839, Online. Association for
Computational Linguistics.

John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,
Di Jin, and Yanjun Qi. 2020b. TextAttack: A frame-
work for adversarial attacks, data augmentation, and
adversarial training in NLP. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations,
pages 119-126, Online. Association for Computa-
tional Linguistics.

Saeid Motiian, Marco Piccirilli, Donald A Adjeroh,
and Gianfranco Doretto. 2017. Unified deep super-
vised domain adaptation and generalization. In The
IEEE International Conference on Computer Vision
(ICCV), volume 2, page 3.

Krikamol Muandet, David Balduzzi, and Bernhard
Schélkopf. 2013. Domain generalization via invari-
ant feature representation. In Proceedings of the
30th International Conference on Machine Learn-
ing, volume 28 of Proceedings of Machine Learn-
ing Research, pages 10-18, Atlanta, Georgia, USA.
PMLR.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2340-2353.

Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho
Lee, and Jinwoo Shin. 2020. Learning from fail-
ure: Training debiased classifier from biased classi-
fier. In Advances in Neural Information Processing
Systems.

Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 4885-4901, Online. Association
for Computational Linguistics.

Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto,
and Percy Liang. 2019. Distributionally robust lan-
guage modeling. In EMNLP/IJCNLP (1).


Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,
D. Sculley, Sebastian Nowozin, Joshua Dillon, Bal-
aji Lakshminarayanan, and Jasper Snoek. 2019. Can
you trust your model's uncertainty? evaluating
predictive uncertainty under dataset shift. In Ad-
vances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345-1359.

Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann,
Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and
Dipanjan Das. 2020. ToTTo: A controlled table-to-
text generation dataset. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1173-1186, On-
line. Association for Computational Linguistics.

Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and
Rama Chellappa. 2015. Visual domain adaptation:
A survey of recent advances. [EEE Signal Process-
ing Magazine, 32(3):53-69.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language in-
ference. In Proceedings of the Seventh Joint Con-
ference on Lexical and Computational Semantics,

pages 180-191.

Marcelo Prates, Pedro Avelar, and Luis Lamb. 2019.
Assessing gender bias in machine translation: a case
study with google translate. Neural Computing and
Applications, 32.

Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lip-
ton. 2019. Combating adversarial misspellings with
robust word recognition. In Proceedings of the 57th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 5582-5591, Florence, Italy.
Association for Computational Linguistics.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial ex-
amples through probability weighted word saliency.
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
1085-1097, Florence, Italy. Association for Compu-
tational Linguistics.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging NLP models. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 856-865, Melbourne, Australia. Association
for Computational Linguistics.

Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,
and Sameer Singh. 2020. Beyond accuracy: Be-
havioral testing of NLP models with CheckList. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4902—
4912, Online. Association for Computational Lin-
guistics.

Elan Rosenfeld, Pradeep Kumar Ravikumar, and An-
drej Risteski. 2021. The risks of invariant risk min-
imization. In International Conference on Learning
Representations.

Alexis Ross, Tongshuang Wu, Hao Peng, Matthew E.
Peters, and Matt Gardner. 2021. Tailor: Generating
and perturbing text with semantic controls.

Artem Rozantsev, Mathieu Salzmann, and Pascal Fua.
2018. Beyond sharing weights for deep domain
adaptation. [EEE transactions on pattern analysis
and machine intelligence, 41(4):801-814.

Rachel Rudinger, Chandler May, and Benjamin
Van Durme. 2017. Social bias in elicited natural lan-
guage inferences. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Process-
ing, pages 74-79, Valencia, Spain. Association for
Computational Linguistics.

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, New Orleans, Louisiana.
Association for Computational Linguistics.

Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto,
and Percy Liang. 2020a. Distributionally robust neu-
ral networks. In International Conference on Learn-
ing Representations.

Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and
Percy Liang. 2020b. An investigation of why over-
parameterization exacerbates spurious correlations.

Viktor Schlegel, Goran Nenadic, and Riza Batista-
Navarro. 2021. Semantics altering modifications for
evaluating comprehension in machine reading. In
Thirty-Fifth AAAI Conference on Artificial Intelli-
gence, pages 13762-13770. AAAI Press.

Leo Schwinn, René Raab, An Nguyen, Dario Zanca,
and Bjoern Eskofier. 2021. Exploring misclassifica-
tions of robust neural networks to enhance adversar-
ial attacks.

Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020. BLEURT: Learning robust metrics for text
generation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7881-7892, Online. Association for Computa-
tional Linguistics.

Zhouxing Shi and Minlie Huang. 2020. Robustness to
modification with shared words in paraphrase identi-
fication. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2020, pages 164-171,
Online. Association for Computational Linguistics.

Walter Simoncini and Gerasimos Spanakis. 2021. Se-
gAttack: On adversarial attacks for named entity
recognition. In Proceedings of the 2021 Conference


on Empirical Methods in Natural Language Process-
ing: System Demonstrations, pages 308-318, On-
line and Punta Cana, Dominican Republic. Associ-
ation for Computational Linguistics.

Liwei Song, Xinwei Yu, Hsuan-Tung Peng, and
Karthik Narasimhan. 2021. Universal adversarial at-
tacks with natural triggers for text classification. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 3724-3733, Online. Association for Compu-
tational Linguistics.

Megha Srivastava, Tatsunori Hashimoto, and Percy
Liang. 2020. Robustness to spurious correlations via
human annotations. In Proceedings of the 37th Inter-
national Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research,
pages 9109-9119. PMLR.

Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth
Belding, Kai-Wei Chang, and William Yang Wang.
2019. Mitigating gender bias in natural language
processing: Literature review. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 1630-1640, Florence,
Italy. Association for Computational Linguistics.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013. Intriguing properties of neural
networks. arXiv preprint arXiv: 1312.6199.

Samson Tan and Shafiq Joty. 2021. Code-mixing on
sesame street: Dawn of the adversarial polyglots. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 3596-3616, Online. Association for Compu-
tational Linguistics.

Samson Tan, Shafiq Joty, Kathy Baxter, Araz Taeihagh,
Gregory A. Bennett, and Min-Yen Kan. 2021. Re-
liability testing for natural language processing sys-
tems. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and
the I1th International Joint Conference on Natu-
ral Language Processing (Volume I: Long Papers),
pages 4153-4169, Online. Association for Computa-
tional Linguistics.

Samson Tan, Shafiq Joty, Min-Yen Kan, and Richard
Socher. 2020a. It’s morphin’ time! Combating
linguistic discrimination with inflectional perturba-
tions. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 2920-2935, Online. Association for Computa-
tional Linguistics.

Samson Tan, Shafiq Joty, Lav Varshney, and Min-Yen
Kan. 2020b. Mind your inflections! Improving NLP
for non-standard Englishes with Base-Inflection En-
coding. In Proceedings of the 2020 Conference on

Empirical Methods in Natural Language Process-
ing (EMNLP), pages 5647-5663, Online. Associa-
tion for Computational Linguistics.

Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas
Carlini, Benjamin Recht, and Ludwig Schmidt.
2020. Measuring robustness to natural distribution
shifts in image classification. In Advances in Neural
Information Processing Systems, volume 33, pages
18583-18599. Curran Associates, Inc.

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,
Alexander Turner, and Aleksander Madry. 2019. Ro-
bustness may be at odds with accuracy. In Interna-
tional Conference on Learning Representations.

Lifu Tu, Garima Lalwani, Spandana Gella, and He He.
2020. An empirical study on robustness to spuri-
ous correlations using pre-trained language models.
Transactions of the Association for Computational
Linguistics, 8:621-633.

Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna
Gurevych. 2020a. Mind the trade-off: Debiasing
NLU models without degrading the in-distribution
performance. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 8717-8729, Online. Association for
Computational Linguistics.

Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna
Gurevych. 2020b. Towards debiasing NLU models
from unknown biases. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 7597-7610, On-
line. Association for Computational Linguistics.

Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and
Graham Neubig. 2019. Improving robustness of ma-
chine translation with synthetic noise. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, volume

2019.

Victor Veitch, Alexander D’Amour, Steve Yadlowsky,
and Jacob Eisenstein. 2021. Counterfactual invari-
ance to spurious correlations in text classification.
In Advances in Neural Information Processing Sys-
tems.

Giorgos Vernikos, Katerina Margatina, Alexandra
Chronopoulou, and Ion Androutsopoulos. 2020. Do-
main adversarial fine-tuning as an effective regular-
izer. CoRR, abs/2009.13366.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019a. Universal adversar-
ial triggers for attacking and analyzing NLP. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 2153-—
2162, Hong Kong, China. Association for Computa-
tional Linguistics.


Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Ya-
mada, and Jordan Boyd-Graber. 2019b. Trick Me If
You Can: Human-in-the-Loop Generation of Adver-
sarial Examples for Question Answering. Transac-
tions of the Association for Computational Linguis-
tics, 7:387—-401.

Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subra-
manian, Matt Gardner, and Sameer Singh. 2019c.
AllenNLP interpret: A framework for explaining
predictions of NLP models. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP): System Demonstrations, pages
7-12, Hong Kong, China. Association for Compu-
tational Linguistics.

Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh.
2021. Concealed data poisoning attacks on NLP
models. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 139-150, Online. Association for
Computational Linguistics.

Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan,
Ruoxi Jia, Bo Li, and Jingjing Liu. 2021a. Infobert:
Improving robustness of language models from an
information theoretic perspective. In International
Conference on Learning Representations.

Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan,
Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah,
and Bo Li. 2021b. Adversarial glue: A multi-task
benchmark for robustness evaluation of language
models.

Haohan Wang, Songwei Ge, Zachary C. Lipton, and
Eric P. Xing. 2019a. Learning robust global repre-
sentations by penalizing local predictive power. In
Advances in Neural Information Processing Systems,

pages 10506-10518.

Haohan Wang, Zexue He, Zachary C. Lipton, and
Eric P. Xing. 2019b. Learning robust representa-
tions by projecting superficial statistics out. In 7th
International Conference on Learning Representa-
tions, ICLR 2019.

Haohan Wang, Zeyi Huang, Hanlin Zhang, and Eric
Xing. 2021c. Toward learning human-aligned cross-
domain robust models by countering misaligned fea-
tures.

Haohan Wang, Aaksha Meghawat, Louis-Philippe
Morency, and Eric P. Xing. 2017. Select-additive
learning: Improving generalization in multimodal
sentiment analysis. In 2017 IEEE International Con-
ference on Multimedia and Expo, ICME 2017, pages
949-954. IEEE Computer Society.

Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P
Xing. 2020a. High-frequency component helps ex-
plain the generalization of convolutional neural net-
works. In Proceedings of the IEEE/CVF Conference

on Computer Vision and Pattern Recognition, pages
8684-8694.

Haohan Wang, Zhenglin Wu, and Eric P. Xing. 2019c.
Removing confounding factors associated weights
in deep neural networks improves the prediction ac-
curacy for healthcare applications. In Biocomputing
2019: Proceedings of the Pacific Symposium, pages
54-65.

Haohan Wang, Peiyan Zhang, and Eric P Xing.
2020b. Word shape matters: Robust machine
translation with visual embedding. arXiv preprint
arXiv:2010.09997.

Huazheng Wang, Zhe Gan, Xiaodong Liu, Jingjing Liu,
Jianfeng Gao, and Hongning Wang. 2019d. Adver-
sarial domain adaptation for machine reading com-
prehension. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP).

Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer,
Kang Li, Jilin Chen, Alex Beutel, and Ed Chi. 2020c.
CAT-gen: Improving robustness in NLP models via
controlled adversarial text generation. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
5141-5146, Online. Association for Computational
Linguistics.

Tianlu Wang, Diyi Yang, and Xuezhi Wang. 2021d.
Identifying and mitigating spurious correlations for
improving robustness in nlp models. arXiv preprint
arXiv:2110.07736.

Xiaosen Wang, Hao Jin, and Kun He. 2019e. Natural
language adversarial attacks and defenses in word
level. arXiv preprint arXiv:1909.06723.

Zhao Wang and Aron Culotta. 2020a. Identifying spu-
rious correlations for robust text classification. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020, pages 3431-3440, Online.
Association for Computational Linguistics.

Zhao Wang and Aron Culotta. 2020b. Robustness to
spurious correlations in text classification via auto-
matically generated counterfactuals. In AAAI.

Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu,
and Changyou Chen. 2020d. Towards faithful neural
table-to-text generation with content-matching con-
straints. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1072-1086, Online. Association for Computa-
tional Linguistics.

Garrett Wilson and Diane J. Cook. 2020. A survey of
unsupervised deep domain adaptation. ACM Trans.
Intell. Syst. Technol., 11(5).

Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer,
and Daniel Weld. 2021. Polyjuice: Generating coun-
terfactuals for explaining, evaluating, and improving


models. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 6707-6723, Online. Association for Computa-
tional Linguistics.

Ruibin Xiong, Yimeng Chen, Liang Pang, Xueqi
Cheng, Zhi-Ming Ma, and Yanyan Lan. 2021. Un-
certainty calibration for ensemble-based debiasing
methods. In Thirty-Fifth Conference on Neural In-
formation Processing Systems.

Yadollah Yaghoobzadeh, Soroush Mehri, Remi Ta-
chet des Combes, T. J. Hazen, and Alessandro Sor-
doni. 2021. Increasing robustness to spurious cor-
relations using forgettable examples. In Proceed-
ings of the 16th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Main Volume, pages 3319-3332, Online. Associa-
tion for Computational Linguistics.

Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang,
Russ R Salakhutdinov, and Kamalika Chaudhuri.
2020. A closer look at accuracy vs. robustness. In
Advances in Neural Information Processing Systems,
volume 33, pages 8588-8601. Curran Associates,
Inc.

Yinfei Yang, Yuan Zhang, Chris Tar, and Jason
Baldridge. 2019. PAWS-X: A cross-lingual ad-
versarial dataset for paraphrase identification. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 3687-—
3692, Hong Kong, China. Association for Computa-
tional Linguistics.

Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh,
and Kai-Wei Chang. 2021. On the transferability
of adversarial attacks against neural text classifier.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1612-1625, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.

Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,
Meng Zhang, Qun Liu, and Maosong Sun. 2020.
Word-level textual adversarial attacking as combina-
torial optimization. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 6066-6080, Online. Association
for Computational Linguistics.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 93—
104, Brussels, Belgium. Association for Computa-
tional Linguistics.

Dejiao Zhang, Ramesh Nallapati, Henghui Zhu, Feng
Nan, Cicero dos Santos, Kathleen McKeown, and

Bing Xiang. 2020a. Unsupervised domain adap-
tation for cross-lingual text labeling. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing: Findings, pages
3527-3536.

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P.
Xing, Laurent El] Ghaoui, and Michael I. Jordan.
2019a. Theoretically principled trade-off between
robustness and accuracy. In International Confer-
ence on Machine Learning.

Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin,
and David Lopez-Paz. 2018. mixup: Beyond empir-
ical risk minimization. In International Conference
on Learning Representations.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020b.  Bertscore:
Evaluating text generation with bert. In Interna-
tional Conference on Learning Representations.

Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi,
and Chenliang Li. 2020c. Adversarial attacks on
deep-learning models in natural language process-
ing: A survey. ACM Transactions on Intelligent Sys-
tems and Technology (TIST), 11(3):1-41.

Yu Zhang, Peter Tilo, AleS Leonardis, and Ke Tang.
2021. A survey on neural network interpretability.
IEEE Transactions on Emerging Topics in Computa-
tional Intelligence, 5(5):726-742.

Yuan Zhang, Jason Baldridge, and Luheng He. 2019b.
PAWS: Paraphrase adversaries from word scram-
bling. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
1298-1308, Minneapolis, Minnesota. Association
for Computational Linguistics.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018a. Gender bias
in coreference resolution: Evaluation and debiasing
methods. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers), pages 15-20,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018b.
Generating natural adversarial examples. In ICLR.

Chunting Zhou, Graham Neubig, Jiatao Gu, Mona
Diab, Francisco Guzman, Luke Zettlemoyer, and
Marjan Ghazvininejad. 2021. Detecting halluci-
nated content in conditional neural sequence gener-
ation. In Findings of the Association for Computa-
tional Linguistics: ACL-IJCNLP 2021, pages 1393-
1404. Association for Computational Linguistics.

Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas
Goldstein, and Jingjing Liu. 2020. Freelb: En-
hanced adversarial training for language understand-
ing. In JCLR.
