arX1v:2206.09755v1 [cs.CL] 20 Jun 2022

Square One Bias in NLP:
Towards a Multi-Dimensional Exploration of the Research Manifold

Sebastian Ruder*
Google Research
ruder@google.com

Abstract

The prototypical NLP experiment trains a stan-
dard architecture on labeled English data and
optimizes for accuracy, without accounting
for other dimensions such as fairness, inter-
pretability, or computational efficiency. We
show through a manual classification of re-
cent NLP research papers that this is indeed
the case and refer to it as the square one ex-
perimental setup. We observe that NLP re-
search often goes beyond the square one setup,
e.g, focusing not only on accuracy, but also
on fairness or interpretability, but typically
only along a single dimension. Most work
targeting multilinguality, for example, consid-
ers only accuracy; most work on fairness or
interpretability considers only English; and
so on. We show this through manual clas-
sification of recent NLP research papers and
ACL Test-of-Time award recipients. Such one-
dimensionality of most research means we are
only exploring a fraction of the NLP research
search space. We provide historical and re-
cent examples of how the square one bias has
led researchers to draw false conclusions or
make unwise choices, point to promising yet
unexplored directions on the research mani-
fold, and make practical recommendations to
enable more multi-dimensional research. We
open-source the results of our annotations to
enable further analysis. !

1 Introduction

Our categorization of objects, say screwdrivers or
NLP experiments, is heavily biased by early pro-
totypes (Sherman, 1985; Das-Smaal, 1990). If the
first 10 screwdrivers we see are red and for hexagon
socket screws, this will bias what features we learn
to associate with screwdrivers. Likewise, if the
first 10 NLP experiments we see or conduct are in

*The authors contributed equally to this work.
‘https://github.com/google-research/
url-nilp

Ivan Vuli¢*
University of Cambridge
iv250@cam.ac.uk

Anders S¢gaard*
University of Copenhagen
soegaard@di.ku.dk

°
°

Efficiency

interpretability

Oe
Square one

MUltiting ual;
ty 2 0 0

Figure 1: Visualization of contributions of ACL 2021
oral papers along 4 dimensions: multilinguality, fair-
ness and bias, efficiency, and interpretability (indicated
by color). Most work is clustered around the SQUARE
ONE or along a single dimension.

sentiment analysis, this will likely also bias how
we think of NLP experiments in the future.

In this position paper, we postulate that we can
meaningfully talk about the prototypical NLP ex-
periment, and that the existence of such an exper-
imental prototype steers and biases the research
dynamics in our community. We will refer to this
prototype as NLP’s SQUARE ONE—and to the bias
that follows from it, as the SQUARE ONE BIAS. We
argue this bias manifests in a particular way: Since
research is a creative endeavor, and researchers aim
to push the research horizon, most research papers
in NLP go beyond this prototype, but only along
a single dimension at a time. Such dimensions
might include multilinguality, efficiency, fairness,
and interpretability, among others. The effect of the
SQUARE ONE BIAS is to baseline novel research
contributions, rewarding work that differs from the
prototype in a concise, one-dimensional way.

We present several examples of this effect in
practice. For instance, analyzing the contributions
of ACL 2021 papers along 4 dimensions, we ob-


serve that most work is either clustered around
the SQUARE ONE or makes a contribution along
a single dimension (see Figure 1). Multilingual
work typically disregards efficiency, fairness, and
interpretability. Work on efficient NLP typically
only performs evaluations on English datasets, and
disregards fairness and interpretability. Fairness
and interpretability work is also mostly limited to
English, and tends to disregard efficiency concerns.

We argue that the SQUARE ONE BIAS has sev-
eral negative effects, most of which amount to the
study of one of the above dimensions being biased
by ignoring the others. Specifically, by focusing
only on exploring the edges of the manifold, we are
not able to identify the non-linear interactions be-
tween different research dimensions. We highlight
several examples of such interactions in Section 3.
Overall, we encourage a focus on combining multi-
ple dimensions on the research manifold in future
NLP research, and delve deeper into studying their
(linear and non-linear) interactions.

Contributions. We first establish that we can
meaningfully talk about the prototypical NLP ex-
periment, through a series of annotation experi-
ments and surveys. This prototype amounts to ap-
plying a standard architecture to an English dataset
and optimizing for accuracy or Fl. We discuss the
impact of this prototype on our research commu-
nity, and the bias it introduces. We then discuss the
negative effects of this bias. We also list work that
has taken steps to overcome the bias. Finally, we
highlight blind spots and unexplored research direc-
tions and make practical recommendations, aiming
to inspire the community towards conducting more
“multi-dimensional’ research (see Figure 1).

2 Finding the Square One

In order to determine the existence and nature of a
SQUARE ONE, we assess contemporary research
in NLP along a number of different dimensions.

Dimensions. We identify potential themes in NLP
research by reviewing the Call for Papers, publi-
cation statistics by area, and paper titles of recent
NLP conferences. We focus on general dimensions
that are not tied to a particular task and are applica-
ble to any NLP application.” We furthermore focus
on dimensions that are represented in a reasonable
fraction of NLP papers (at least 5% of ACL 2021

>For instance, we do not consider multimodality, as a task
or model is inherently multimodal or not.

oral papers).* Our final selection focuses on 4 di-
mensions along which papers may make research
contributions: multilinguality, fairness and bias, ef-
ficiency, and interpretability. Compared to prior
work that annotates the values of ML research pa-
pers (Birhane et al., 2021), we are not concerned
with a paper’s motivation but whether its practi-
cal contributions constitute a meaningful departure
from the SQUARE ONE. For each paper, we an-
notate whether it makes a contribution along each
dimension as well as the languages and metrics it
employs for evaluation. We provide the detailed
annotation guidelines in Appendix A.1.

ACL 2021 Oral Papers. We annotate the 461 pa-
pers that were presented orally at ACL 2021, a
representative cross-section of the 779 papers ac-
cepted to the main conference. The general statis-
tics from our classification of ACL 2021 papers
are presented in Table 1. In addition, we highlight
the statistics for the conference areas (tracks) cor-
responding to 3 of the 4 dimensions“, as well as
for the top 5 areas with the most papers. We show
statistics for the remaining areas in Appendix A.2.
We additionally visualize their distribution in Fig-
ure 1. Overall, almost 70% of papers evaluate only
on English, clearly highlighting a lack of language
diversity in NLP (Bender, 2011; Joshi et al., 2020).
Almost 40% of papers only evaluate using accuracy
and/or F1, foregoing metrics that may shed light
on other aspects of model behavior. 56.6% of pa-
pers do not study any of the four major dimensions
that we investigated. We refer to this standard ex-
perimental setup—evaluating only on English and
optimizing for accuracy or another performance
metric without considering other dimensions—as
the SQUARE ONE.

Regarding work that moves from the SQUARE
ONE, most papers make a contribution in terms of
efficiency, followed by multilinguality. However,
most papers that evaluate on multiple languages are
part of the corresponding MT and Multilinguality
track. Despite being an area receiving increasing at-
tention (Blodgett et al., 2020), only 6.3% of papers
evaluate the bias or fairness of a method. Overall,
only 6.1% of papers make a contribution along two

3Privacy, interactivity, and other emerging research areas
are excluded based on this criterion.

“Unlike EACL 2021, NAACL-HLT 2021 and EMNLP
2021, ACL 2021 had no area associated with efficiency. To
compensate for this, we annotated the 20 oral papers of the
“Efficient Models in NLP” track at EMNLP 2021 (see Ap-
pendix A.3).


Area # papers English Accuracy/F1 Multilinguality Fairness and bias Efficiency Interpretability >1 dimension
ACL 2021 oral papers 461 69.4% 38.8% 13.9% 6.3% 17.8% 11.7% 6.1%
MT and Multilinguality 58 0.0% 15.5% 56.9% 5.2% 19.0% 6.9% 13.8%
Interpretability and Analysis 18 88.9% 27.8% 5.6% 0.0% 5.6% 66.7% 5.6%
Ethics in NLP 6 83.3% 0.0% 0.0% 100.0% 0.0% 0.0% 0.0%
Dialog and Interactive Systems 42 90.5% 21.4% 0.0% 9.5% 23.8% 2.4% 2.4%
Machine Learning for NLP 42 66.7% 40.5% 19.0% 4.8% 50.0% 4.8% 9.5%
Information Extraction 36 80.6% 91.7% 8.3% 0.0% 25.0% 5.6% 8.3%
Resources and Evaluation 35 77.1% 42.9% 5.7% 8.6% 5.7% 14.3% 5.7%
NLP Applications 30 73.3% 43.3% 0.0% 10.0% 20.0% 10.0% 0.0%

Table 1: The number of ACL 2021 oral papers (top row) and of papers in each area (bottom rows) as well as the
fractions that only evaluate on English, only use accuracy / Fl, make contributions along one of four dimensions,
and make contributions along more than a single dimension (from left to right).

or more of these dimensions. Among these, joint
contributions on both multilinguality and efficiency
are the most common (see Figure 1). In fact, 22
of the 26 two-or-more-dimensional papers focus
on efficiency, and 17 of these on the combination
of multilinguality and efficiency. This means less
than 1% of the ACL 2021 papers consider combi-
nations of (two or more of) multilinguality, fairness
and interpretability. We find this surprising, given
these topics are considered among the most popular
topics in the field.

Some areas have particularly concerning statis-
tics. A large majority of research work in dia-
log (90.5%), summarization (91.7%), sentiment
analysis (100%), and language grounding (100%)
is done only on English; however, ways of ex-
pressing sentiment (Volkova et al., 2013; Yang and
Eisenstein, 2017; Vilares et al., 2018) and visu-
ally grounded reasoning (Liu et al., 2021a; Yin
et al., 2021) do vary across languages and cul-
tures. Systems in the top tracks tend to evaluate
efficiency, but in general do not consider fairness or
interpretability of the proposed methods. Even the
creation of new resources and evaluation sets (cf.,
Resource and Evaluation in Table 1) seems to be
directed towards rewarding and enabling SQUARE
ONE experiments; favoring English (77.1%), and
with modest efforts on other dimensions. Notably,
we only identified a single paper that considers
three dimensions (Renduchintala et al., 2021). This
paper considers gender bias (Fairness) in relation
to speed-quality (Efficiency) trade-offs in multilin-
gual machine translation (Multilinguality). Finally,
we observe that best-paper award winning papers
are not more likely to consider more than one of the
four dimensions. Only | in 8 papers did; the best
paper (Xu et al., 2021), like most two-dimensional
ACL 2021 papers, considered multilinguality and
efficiency.

Year Paper Language Metric
1995 Grosz et al. (1995) English n/a
1995 Yarowsky (1995) English acc.
1996 _ Berger et al. (1996) English acc.
1996 Carletta (1996) n/a n/a
2010 Baroni and Lenci (2010) English acc.
2010 =‘ Turian et al. (2010) English Fi
2011  Taboada et al. (2011) English acc.
2011 Ott et al. (2011) English acc./Fy

Table 2: Test-of-Time Award 2021-22 papers

Test-of-Time Award Recipients. Current papers
provide us with a snapshot of actual current re-
search practices, but the one-dimensionality of the
best paper award winning papers at ACL 2021 sug-
gest the SQUARE ONE BIAS also biases what we
value in research, i.e., our perception of ideal re-
search practices. This can also be seen in the papers
that have received the ACL Test-of-Time Award in
the last two years (Table 2). Seven in eight papers
included empirical evaluations performed exclu-
sively on English data. Six papers were exclusively
concerned with optimizing for accuracy or F.

Blackbox NLP Papers. Finally, we check if more
multi-dimensional papers were presented at a work-
shop devoted to one of the above dimensions. The
rationale is that if everyone at a workshop already
explores one of these dimensions, including an-
other may be a way to have an edge over other
submissions. Unfortunately, this does not seem to
be the case. We manually annotated the first 10 pa-
pers in the Blackbox NLP 2021 program? that were
available as pre-prints at the time of submission.
Of the 10 papers, only one included more than one
dimension (Abdullah et al., 2021). This number
aligns well with the overall statistics of ACL 2021
(6.1%). All the other Blackbox NLP papers only
considered interpretability for English.

Shttps://blackboxnlp.github.io/


3 Square One Bias: Examples

In the following, we highlight both historical and
recent examples touching on different aspects of re-
search in NLP that illustrate how the gravitational
attraction of the SQUARE ONE has led researchers
to draw false conclusions, unconsciously steer stan-
dard research practices, or make unwise choices.

Architectural Biases. One pervasive bias in our
models regards morphology. Many of our mod-
els were not designed with morphology in mind,
arguably because of the poor/limited morphology
of English. Traditional n-gram language models,
for example, have been shown to perform much
worse on languages with elaborate morphology due
to data sparsity problems (Khudanpur, 2006; Ben-
der, 2011; Gerz et al., 2018). Such models were
nevertheless more commonly used than more lin-
guistically informed alternatives such as factored
language models (Bilmes and Kirchhoff, 2003)
that represent words as sets of features. Word em-
beddings have been widely used, in part because
pre-trained embeddings covered a large part of the
English vocabulary. However, word embeddings
are not useful for tasks that require access to mor-
phemes, e.g., semantic tasks in morphologically
rich languages (Avraham and Goldberg, 2017).
While studies have demonstrated the ability of
word embeddings to capture linguistic information
in English, it remains unclear whether they capture
the information needed for processing morphologi-
cally rich languages (Tsarfaty et al., 2020). A bias
towards morphologically rich languages is also ap-
parent in our tokenization algorithms. Subword
tokenization performs poorly on languages with
reduplication (Vania and Lopez, 2017), while byte
pair encoding does not align well with morphol-
ogy (Bostrom and Durrett, 2020). Consequently,
languages with productive morphological systems
also are disadvantaged when shared ‘language-
universal’ tokenizers are used in current large-scale
multilingual language models (Acs, 2019; Rust
et al., 2021) without any further vocabulary adapta-
tion (Wang et al., 2020; Pfeiffer et al., 2021).
Another bias in our models relates to word or-
der. In order for n-gram models to capture inter-
word dependencies, words need to appear in the
n-gram window. This will occur more frequently
in languages with relatively fixed word order com-
pared to languages with relatively free word order
(Bender, 2011). Word embedding approaches such
as skip-gram (Mikolov et al., 2013) adhere to the

same window-based approach and thus have sim-
ilar weaknesses for languages with relatively free
word order. LSTMs are also sensitive to word or-
der and perform worse on agreement prediction in
Basque, which is both morphologically richer and
has a relatively free word order (Ravfogel et al.,
2018) compared to English (Linzen et al., 2016).
They have also been shown to transfer worse to dis-
tant languages for dependency parsing compared
to self-attention models (Ahmad et al., 2019). Such
biases concerning word order are not only inher-
ent in our models but also in our algorithms. A
recent unsupervised parsing algorithm (Shen et al.,
2018) has been shown to be biased towards right-
branching structures and consequently performs
better in right-branching languages like English
(Dyer et al., 2019). While the recent generation
of self-attention based architectures can be seen
as inherently order-agnostic, recent methods focus-
ing on making attention more efficient (Tay et al.,
2020) introduce new biases into the models. Specif-
ically, models that reduce the global attention to a
local sliding window around the token (Liu et al.,
2018; Child et al., 2019; Zaheer et al., 2020) may
incur similar limitations as their n-gram and word
embedding-based predecessors, performing worse
on languages with relatively free word order.°

The singular focus on maximizing a performance
metric such as accuracy introduces a bias towards
models that are expressive enough to fit a given
distribution well. Such models are typically black-
box and learn highly non-linear relations that are
generally not interpretable. Interpretability is gen-
erally studied in papers focusing exclusively on
this topic; a recent example is BERTology (Rogers
et al., 2020). Studies proposing more interpretable
methods typically build on state-of-the-art meth-
ods (Weiss et al., 2018) and much work focuses
on leveraging components such as attention for in-
terpretability, which have not been designed with
that goal in mind (Serrano and Smith, 2019; Wiegr-
effe and Pinter, 2019). As a result, researchers
eschew directions focusing on models that are in-
trinsically more interpretable such as generalized
additive models (Hastie and Tibshirani, 2017) and
their extensions (Chang et al., 2021; Agarwal et al.,
2021) but which have so far not been shown to
match the performance of state-of-the-art methods.

®An older work of Khudanpur (2006) argues that free
word order is less of a problem as local order within phrases
is relatively stable. However, it remains to be seen to what
degree this affects current models.


As most datasets on which models are evaluated
focus on sentences or short documents, state-of-
the-art methods restrict their input size to around
512 tokens (Devlin et al., 2019) and leverage meth-
ods that are inefficient when scaling to longer
documents. This has led to the emergence of a
wide range of more efficient models (Tay et al.,
2020), which, however, are rarely used as baseline
methods in NLP. Similarly, the standard pretrain-
fine-tune paradigm (Ruder et al., 2019) requires
separate model copies to be stored for each task,
and thus restricts work on multi-domain, multi-
task, multi-lingual, multi-subpopulation methods
that is enabled by more efficient and less resource-
intensive (Schwartz et al., 2020) fine-tuning meth-
ods (Houlsby et al., 2019; Pfeiffer et al., 2020)

In sum, (what we typically consider as) standard
baselines and state-of-the-art architectures favor
languages with some characteristics over others and
are optimized only for performance, which in turn
propagates the SQUARE ONE BIAS: If researchers
study aspects such as multilinguality, efficiency,
fairness or interpretability, they are likely to do
so with and for commonly used architectures (i.€.,
often termed ‘standard architectures’), in order to
reduce (too) many degrees of freedom in their em-
pirical research. This is in many ways a sensible
choice in order to maximize perceived relevance—
and thereby, impact. However, as a result, multi-
linguality, efficiency, fairness, interpretability, and
other research areas inherit the same biases, which
typically slip under the radar.

Annotation Biases. Many NLP tasks can be cast
differently and formulated in multiple ways, and
differences may result in different annotation styles.
Sentiment, for example, can be annotated at the
document, sentence or word level (Socher et al.,
2013). In machine comprehension, answers are
sometimes assumed to be continuous, but Zhu et al.
(2020) annotate discontinuous spans. In depen-
dency parsing, different annotation guidelines can
lead to very different downstream performance
(Elming et al., 2013). How we annotate for a task
may interact in complex ways with dimensions
such as multilinguality, efficiency, fairness, and in-
terpretability. The Universal Dependencies project
(Nivre et al., 2020) is motivated by the observa-
tion that not all dependency formalisms are easily
applicable to all languages. Aligning guidelines
across languages has enabled researchers to ask in-
teresting questions, but such attempts may limit the

analysis of outlier languages (Croft et al., 2017).

Other examples of annotation guidelines interact-
ing with the above dimensions exist: Slight nuances
in how annotation guidelines are formulated can
lead to severe model biases (Hansen and S¢gaard,
2021a) and hurt model fairness. In interpretability,
we can use feature attribution methods and word-
level annotations to evaluate interpretability meth-
ods applied to sequence classifiers (Rei and S¢-
gaard, 2018), but we cannot directly use feature at-
tribution methods to obtain rationales for sequence
labelers. Annotation biases can also stem from the
characteristics of the annotators, including their do-
main experience (McAuley and Leskovec, 2013),
demographics (Jgrgensen and Sggaard, 2021), or
educational level (Al Kuwatly et al., 2020).

Annotation biases form an integral part of the
SQUARE ONE BIAS: In NLP experiments, we com-
monly rely on the same pools of annotators, e.g.,
computer science students, professional linguists,
or MTurk contributors. Sometimes these biases
percolate through reuse of resources, e.g., through
human or machine translation into new languages.
Examples of such recycled resources include the
ones introduced by Conneau et al. (2018) and Kass-
ner et al. (2021), among others. Even when such
translation-based resources resonate with syntax
and semantics of the target language, and are fluent
and natural, they still suffer from translation arte-
facts: they are often target-language surface realiza-
tions of source-language-based conceptual thinking
(Majewska et al., 2022). As a consequence, eval-
uations of cross-lingual transfer models on such
data typically overestimate their performance as
properties such as word order and even the choice
of lexical units are inherently biased by the source
language (Vanmassenhove et al., 2021). Put sim-
ply, the choice of the data creation protocol, e.g.,
translation-based versus data collection directly in
the target language (Clark et al., 2020) can yield
profound differences in model performance for
some groups, or may have serious impact on the
interpretability or computational efficiency (e.g.,
sample efficiency) of our models.

Selection Biases. For many years, the English
Penn Treebank (Marcus et al., 1994) was an inte-
gral part of the SQUARE ONE of NLP. This corpus
consists entirely of newswire, i.e., articles and edi-
torials from the Wall Street Journal, and arguably
amplified the (existing) bias toward news articles.
Since news articles tend to reflect a particular set


of linguistic conventions, have a certain length, and
are written by certain demographics, the bias to-
ward news articles had an impact on the linguistic
phenomena studied in NLP (Judge et al., 2006), led
to under-representation of challenges with handling
longer documents (Beltagy et al., 2021), and had
impact on early papers in fairness (Hovy and Sg-
gaard, 2015). Note how such a bias may interact in
non-linear ways with efficiency, i.e., efficient meth-
ods for shorter documents need not be efficient for
longer ones, or fairness, i.e., what mitigates gender
biases in news articles need not mitigate gender
biases in product reviews.

Protocol Biases. In the prototypical NLP experi-
ment, the dataset is in the English language. As a
consequence, it is also standard protocol in multi-
lingual NLP to use English as a source language
in zero-shot cross-lingual transfer (Hu et al., 2020).
In practice, there are generally better source lan-
guages than English (Ponti et al., 2018; Lin et al.,
2019; Turc et al., 2021), and results are heavily
biased by the common choice of English. For in-
stance, effectiveness and efficiency of few-shot
learning can be impacted by the choice of the
source language (Pfeiffer et al., 2021; Zhao et al.,
2021). English also dominates language pairs in
machine translation, leading to lower performance
for non-English translation directions (Fan et al.,
2020), which are particularly important in multilin-
gual societies. Again, such biases may interact in
non-trivial ways with dimensions explored in NLP
research: It is not inconceivable that there is an
algorithm A that is more fair, interpretable or effi-
cient than algorithm B on, say, English-to-Czech
transfer or translation, but not on German-to-Czech
or French-to-Czech.

Organizational Biases. The above architectural,
annotation, selection and protocol biases follow
from the SQUARE ONE BIAS, but they also con-
serve the SQUARE ONE. If our go-to architectures,
resources, and experimental setups are tailored to
some languages over others, some objectives over
others, and some research paradigms over others,
it is considerably more work to explore new sets of
languages, new objectives, or new protocols. The
organizational biases we discuss below may also
reinforce the SQUARE ONE BIAS.

The organization of our conferences and review-
ing processes perpetuates certain biases. In par-
ticular, both during reviewing and for later pre-
sentation at conferences, papers are organized in

areas. Upon submission, a paper is assigned to
a single area. Reviewers are recruited for their
expertise in a specific area, which they are associ-
ated with. Such a reviewing system incentivizes
papers that make contributions to the chosen area,
in order to appeal to the reviewers of this area and
implicitly penalizes papers that make contributions
along multiple dimensions, as reviewers unfamil-
iar with the related areas may not appreciate their
inter-disciplinary or inter-areal magnitude or value.
Even new initiatives that seek to improve review-
ing such as ARR’ adhere to this area structure® and
thus further the SQUARE ONE BIAS. A review-
ing system that allows papers to be associated with
multiple dimensions of research and that assigns
reviewers with complementary expertise—similar
to TACL®—would ameliorate this situation. Once
a paper is accepted, presentations at conferences
are organized by areas, limiting audiences in most
cases to members of said area and thereby reducing
the cross-pollination of ideas.!°

Unexplored Areas of the Research Manifold.
The discussed biases, which seem to originate from
the SQUARE ONE BIAS, leave areas of the research
manifold unexplored. Character-based language
models are often reported to perform well for mor-
phologically rich languages or on non-canonical
text (Ma et al., 2020), but little is known about
their fairness properties, and attribution-based in-
terpretability methods have not been developed for
such models. Annotation biases that stem from
annotator demographics have been studied for En-
glish POS tagging (Hovy and Sggaard, 2015) or
English summarization (Jgrgensen and Sggaard,
2021), for example, but there has been very little
research on such biases for other languages. While
linguistic differences among genders is shared
among some languages, genders differ in very dif-
ferent ways between other languages, e.g., Span-

Taclrollingreview wOorg/

Swww.2022.aclweb. org/callpapers

°transacl.org/index.php/tacl

10 Another previously pervasive organizational bias, which
is now fortunately being institutionally mitigated within the
*ACL community through dedicated mentoring programs and
improved reviewing guidelines, concerned penalizing research
papers for their non-native writing style, where it was fre-
quently suggested to the authors whose native language is not
English to ‘have their paper proofread by a native speaker’. As
one hidden consequence, this attitude might have set a higher
bar for the native speakers of minor and endangered languages
working on such languages to put their research problems in
the spotlight, that way also implicitly hindering more work of
the entire community on these languages.


ish and Swedish (Johannsen et al., 2015). We dis-
cuss important unexplored areas of the research
manifold in §5, but first we briefly survey existing,
multi-dimensional work, 1.e., the counter-examples
to our claim that NLP research is biased to one-
dimensional extensions of the square one.

4 Counter-Examples

Most of the exceptions to our thesis about the ‘one-
dimensionality’ of NLP research, in our classifica-
tion of ACL 2021 Oral Papers, came from studies
of efficiency in a multilingual context. Another
example of this is Ahia et al. (2021), who show that
for low-resource languages, weight pruning hurts
performance on tail phenomena, but improves ro-
bustness to out-of-distribution shifts—this is not ob-
served in the SQUARE ONE (high-resource) regime.
There are also studies of fairness in a multilin-
gual context. Huang et al. (2020), for example,
show significant differences in social bias for mul-
tilingual hate speech systems across different lan-
guages. Zhao et al. (2020) study gender bias in
multilingual word embeddings and cross-lingual
transfer. Gonzalez et al. (2020) also study gender
bias, but by relying on reflexive pronominal con-
structions that do not exist in the English language;
this is a good example of research that would not
have been possible taking SQUARE ONE as our
point of departure. Dayanik and Padé (2021) study
adversarial debiasing in the context of a multilin-
gual corpus and show some mitigation methods are
more effective for some languages rather than oth-
ers. Nozza (2021) studies multilingual toxicity clas-
sification and finds that models misinterpret non-
hateful language-specific taboo interjections as hate
speech in some languages. There has been much
less work on other combinations of these dimen-
sions, e.g., fairness and efficiency. Hansen and
Sggaard (2021b) show that weight pruning has dis-
parate effects on performance across demographics
and that the min-max difference in group disparities
is negatively correlated with model size. Renduch-
intala et al. (2021) observe that techniques to make
inference more efficient, e.g., greedy search, quan-
tization, or shallow decoder models, have a small
impact on performance, but dramatically amplify
gender bias. In a rare study of fairness and inter-
pretability, Vig et al. (2020) propose a methodol-
ogy to interpret which parts of a model are causally
implicated in its behavior. They apply this method-
ology to analyze gender bias in pre-trained Trans-

formers, finding that gender bias effects are sparse
and concentrated in small parts of the network.

5 Blind Spots

We identified several under-explored areas on the
research manifold. The common theme is a lack
of studies of how dimensions such as multilingual-
ity, fairness, efficiency, and interpretability interact.
We now summarize some open problems that we
believe are particularly important to address: (1)
While recent work has begun to study the trade-off
between efficiency and fairness, this interaction
remains largely unexplored, especially outside of
the empirical risk minimization regime; (ii) fair-
ness and interpretability interact in potentially
many ways, i.e., interpretability techniques may af-
fect the fairness of the underlying models (Agarwal,
2021), but rationales may also, for example, be bi-
ased toward certain demographics in how they are
presented (Feng and Boyd-Graber, 2018; Gonzalez
et al., 2021); (iii) finally, multilinguality and in-
terpretability seem heavily underexplored. While
there exists resources for English for evaluating in-
terpretability methods against gold-standard human
annotations, there are, to the best of our knowledge,
no such resources for other languages.!!

6 Contributing Factors

We finally highlight possible factors that may con-
tribute to the SQUARE ONE BIAS.

Biases in NLP Education. We hypothesize that
early exposure to predominantly English-centric
experiment settings and tasks using a single per-
formance metric may potentially propagate further
to more advanced NLP research. To investigate to
what extent this may be the case, we created a short
questionnaire, which we sent to a geographically
diverse set of teachers, including first authors from
the last Teaching NLP workshop (Jurgens et al.,
2021), asking about the first experiment that they
presented in their NLP 101 course. We received
71 responses in total. Our first question was: The
last time you taught an introductory NLP course,
what was the first task you introduced the students
to, or that they had to implement a model for?
The relative majority of respondents (31.9%) said
sentiment analysis, while 10.1% indicated topic

'lWe again note that there are other possible dimensions,
not studied in this work, that can expose more blind spots: e.g.,
fairness and multi-modality, multilinguality and privacy.


Year Book Language Task

1999 = Manning and Schiitze (1999) English-French = Alignment
2009 — Jurafsky and Martin (2009) English LM

2009 Bird et al. (2009) English Name cl.
2013 Sggaard (2013) English Doc.cl.
2019 Eisenstein (2019) English Doc.cl.

Table 3: First experiments in NLP textbooks. The ob-
jective across all books is optimizing for performance
(AER, perplexity, or accuracy), rather than fairness, in-
terpretability or efficiency.

classification.!* More importantly, we also asked
them about the language of the data used in the
experiment, and what metric they optimized for.
More than three quarters of respondents reported
that they used English language training and eval-
uation data and more than three quarters of the
respondents asked the students to optimize for ac-
curacy or Fl. The choice of using English lan-
guage datasets is particularly interesting in contrast
to the native languages of the teachers and their
students: In around two thirds of the classes, most
students shared an L1 language that was not En-
glish; and less than a quarter of the teachers were
L1 English speakers themselves. We extend this
analysis to prototypical NLP experiments in un-
dergraduate and graduate research based on five
exemplary NLP textbooks, spanning 20 years (see
Table 3). We observe that they, like the teachers
in our survey, take the same point of departure: an
English-language experiment where we use super-
vised learning techniques to optimize for a standard
performance metric, e.g., perplexity or error. We
note an important difference, however: While the
first four books largely ignore issues relating to
fairness, interpretability, and efficiency, the most
recent NLP textbook in Table 3 (Eisenstein, 2019)
discusses efficiency (briefly) and fairness (more
thoroughly). Overall, we believe that teachers and
educational materials should engage as early as
possible with the multiple dimensions of NLP in
order to sensitize researchers regarding these topics
at the start of their careers.

Commercial Factors. For commercially focused
NLP, there is an incentive to focus on settings with
many users, such as major languages. Similarly,
as long as users do not mind using highly accu-
rate black-box systems, researchers working on
real-world applications can often afford to ignore
dimensions such as interpretability and fairness.

"The remaining responses included NER, language model-
ing, language identification, hate speech detection, etc.

Momentum of the Status Quo. The SQUARE
ONE is well supported by existing infrastructure,
resources, baselines, and experimental results. Any
work that seeks to depart from the standard setting
has to work harder, not only to build systems and
resources in order to establish comparability with
existing work but also needs to argue convincingly
the importance of such work. We provide practical
recommendations in the next section on how we
can facilitate such research as a community.

7 Discussion

Is SQUARE ONE BIAS not the Flipside of Sci-
entific Protocol? One potential argument for a
community-wide SQUARE ONE BIAS is that when
studying the impact of some technique f, say a
novel regularization term, we want to compare
some system with and without f, i.e., control for all
other factors. To maximize impact and ease work-
load, it makes sense at first sight to stick to a system
and experimental protocol that is familiar or well-
studied. Always returning to the SQUARE ONE is
a way to control for all other factors and relating
new findings to known territory. The reason why
this is only seemingly a good idea, however, is that
the factors we study in NLP research, may be non-
linearly related. The fact that t makes for a positive
net contribution under one set of circumstances,
does not imply that it would do so under different
circumstances. This is illustrated most clearly by
the research surveyed in §3. Ideally, we thus want
to study the impact of ¢ under as many circum-
stances as possible, but in the absence of resources
to do so, it is a better (collective) search strategy to
apply t to a random set of circumstances (within
the space of relevant circumstances, of course).

Comment on Meta-Research. This paper can
be seen in the line of other meta-research (Davis,
1971; Lakatos, 1976; Weber, 2006; Bloom et al.,
2020) that seeks to analyze research practices and
whether a scientific field is heading in the right
direction. Within the NLP community, much of
such recent discussion has focused on the nature
of leaderboards and the practice of benchmarking
(Ethayarajh and Jurafsky, 2020; Ma et al., 2021).

Should Each Paper Aim to Cover All Dimen-
sions? We believe that a researcher should aspire
to cover as many dimensions as possible with their
research. Considering the dimensions of research
encourages us to think more holistically about our
research and its final impact. It may also accel-


erate progress as follow-up work will already be
able to build on the insights of multi-dimensional
analyses of new methods. It will also promote the
cross-pollination of ideas, which will no longer be
confined to their own sub-areas. While such multi-
dimensional research may be cumbersome at the
moment, we believe with the proper incentives and
support, we can make it much more accessible.

Practical Recommendations. What can we do
to incentivize and facilitate multi-dimensional re-
search? i) Currently, most NLP models are eval-
uated by one or two performance metrics, but we
believe dimensions such as fairness, efficiency, and
interpretability need to become integral criteria for
model evaluation, in line with recent proposals of
more user-centric leaderboards (Ethayarajh and Ju-
rafsky, 2020; Ma et al., 2021). This requires new
tools, e.g., to evaluate environmental impact (Hen-
derson et al., 2020), as well as new benchmarks,
e.g., to evaluate fairness (Koh et al., 2021) or ef-
ficiency (Liu et al., 2021b). ii) We believe sepa-
rate conference tracks (areas) lead to unfortunate
silo effects and inhibit multi-dimensional research.
Rather, we imagine conference submissions could
provide a checklist with dimensions along which
they make contributions, similar to reproducibil-
ity checklist. Reviewers can be assigned based on
their expertise corresponding to different dimen-
sions. iii) Finally, we recommend awareness of
research prototypes and encourage reviewers and
chairs to prioritize research that departs from pro-
totypes in multiple dimensions, in order to explore
new areas of the research manifold.

8 Conclusion

We identified the prototypical NLP experiment
through annotation experiments and surveys. We
highlighted the associated SQUARE ONE BIAS,
which encourages research to go beyond the proto-
type in a single dimension. We discussed the prob-
lems resulting from this bias, by studying the area
Statistics of a recent NLP conference as well as by
discussing historic and recent examples. We finally
pointed to under-explored research directions and
made practical recommendations to inspire more
multi-dimensional research in NLP.

Acknowledgments

Ivan Vulié is funded by the ERC PoC Grant Mul-
tiConvAI (no. 957356) and a research donation
from Huawei. Anders S¢gaard is sponsored by the

Innovation Fund Denmark and a Google Focused
Research Award. We thank Jacob Eisenstein for
valuable feedback on a draft of this paper and the
suggestion of the term “square one’.

References

Badr Abdullah, Iuliia Zaitova, Tania Avgustinova,
Bernd Mobius, and Dietrich Klakow. 2021. How
familiar does that sound? Cross-lingual represen-
tational similarity analysis of acoustic word embed-
dings. In Proceedings of the Fourth BlackboxNLP
Workshop on Analyzing and Interpreting Neural Net-
works for NLP, pages 407-419.

Judit Acs. 2019. Exploring BERT’s Vocabulary. Blog
Post.

Rishabh Agarwal, Levi Melnick, Nicholas Frosst,
Xuezhou Zhang, Ben Lengerich, Rich Caruana, and
Geoffrey Hinton. 2021. Neural additive models: In-
terpretable machine learning with neural nets. In
Proceedings of NeurIPS 2021.

Sushant Agarwal. 2021. Trade-offs between fairness
and interpretability in machine learning. In Proceed-
ings of the IJCAI 2021 Workshop on AI for Social
Good.

Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker.
2021. The low-resource double bind: An empirical
study of pruning for low-resource machine transla-
tion. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021, pages 3316-3333.

Wasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard
Hovy, Kai-Wei Chang, and Nanyun Peng. 2019. On
difficulties of cross-lingual transfer with order differ-
ences: A case study on dependency parsing. In Pro-
ceedings of NAACL-HLT 2019, pages 2440-24572.

Hala Al Kuwatly, Maximilian Wich, and Georg Groh.
2020. Identifying and measuring annotator bias
based on annotators’ demographic characteristics.
In Proceedings of the Fourth Workshop on Online
Abuse and Harms, pages 184-190.

Oded Avraham and Yoav Goldberg. 2017. The inter-
play of semantics and morphology in word embed-
dings. In Proceedings of EACL 2017, pages 422-
426.

Marco Baroni and Alessandro Lenci. 2010. —Dis-
tributional memory: A_ general framework for
corpus-based semantics. Computational Linguistics,
36(4):673-721.

Iz Beltagy, Arman Cohan, Hannaneh Hajishirzi, Sewon
Min, and Matthew E. Peters. 2021. Beyond para-
graphs: NLP for long sequences. In Proceedings of
NAACL-HLT 2021: Tutorials, pages 20-24.

Emily M. Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology, 6(3):1—26.


Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39-71.

Jeff Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Companion Volume of the Proceedings of HLT-
NAACL 2003-Short Papers, pages 4-6.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python: An-

alyzing Text with the Natural Language Toolkit.
O’Reilly, Beijing.

Abeba Birhane, Pratyusha Kalluri, Dallas Card,
William Agnew, Ravit Dotan, and Michelle Bao.
2021. The values encoded in machine learning re-
search. CoRR, abs/2106.15590.

Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna M. Wallach. 2020. Language (technology) is
power: A critical survey of "bias" in NLP. CoRR,
abs/2005.14050.

Nicholas Bloom, Charles I Jones, John Van Reenen,
and Michael Webb. 2020. Are ideas getting harder
to find? American Economic Review, 110(4):1104—
44.

Kaj Bostrom and Greg Durrett. 2020. Byte pair encod-
ing is suboptimal for language model pretraining. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020, pages 4617-4624.

Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249-254.

Chun-Hao Chang, Sarah Tan, Ben Lengerich, Anna
Goldenberg, and Rich Caruana. 2021. How inter-
pretable and trustworthy are GAMs? In Proceed-
ings of KDD 2021, pages 95-105.

Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019. Generating long sequences with
sparse Transformers. CoRR, abs/1904.10509.

Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan
Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
Jennimaria Palomaki. 2020. TyDi QA: A bench-
mark for information-seeking question answering in
typologically diverse languages. Transactions of the
Association for Computational Linguistics, 8:454—
470.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
ina Williams, Samuel Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018. XNLI: Evaluating
cross-lingual sentence representations. In Proceed-
ings of EMNLP 2018, pages 2475-2485.

William Croft, Dawn Nordquist, Katherine Looney,
and Michael Regan. 2017. Linguistic typology
meets universal dependencies. In Proceedings of the
15th International Workshop on Treebanks and Lin-
guistic Theories (TLT15), pages 63-75.

Edith A. Das-Smaal. 1990. Biases in categorization.
volume 68 of Advances in Psychology, pages 349—
386. North-Holland.

Murray S Davis. 1971. That’s interesting! towards
a phenomenology of sociology and a sociology of
phenomenology. Philosophy of the social sciences,
1(2):309-344.

Erenay Dayanik and Sebastian Pad6. 2021. Disentan-
gling document topic and author gender in multiple
languages: Lessons for adversarial debiasing. In
Proceedings of the Eleventh Workshop on Compu-
tational Approaches to Subjectivity, Sentiment and
Social Media Analysis, pages 50-61.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. In Proceedings of NAACL-HLT 2019.

Chris Dyer, Gabor Melis, and Phil Blunsom. 2019. A
critical analysis of biased parsers in unsupervised
parsing. CoRR, abs/1909.09428.

Jacob Eisenstein. 2019. Introduction to Natural Lan-
guage Processing. Adaptive Computation and Ma-
chine Learning series. MIT Press.

Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders Sg¢gaard. 2013. Down-stream effects of
tree-to-dependency conversions. In Proceedings of
NAACL-HLT 2013, pages 617-626.

Kawin Ethayarajh and Dan Jurafsky. 2020. Utility is in
the eye of the user: A critique of NLP leaderboards.
In Proceedings of EMNLP 2020, pages 4846-4853.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
Ma, Ahmed El-Kishky, Siddharth Goyal, Man-
deep Baines, Onur Celebi, Guillaume Wenzek,
Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-
taliy Liptchinsky, Sergey Edunov, Edouard Grave,
Michael Auli, and Armand Joulin. 2020. Beyond
English-Centric Multilingual Machine Translation.
arXiv preprint arXiv:2010.11125.

Shi Feng and Jordan L. Boyd-Graber. 2018. What can
AI do for me: Evaluating machine learning interpre-
tations in cooperative play. CoRR, abs/1810.09648.

Daniela Gerz, Ivan Vuli¢é, Edoardo Maria Ponti, Roi
Reichart, and Anna Korhonen. 2018. On the rela-
tion between linguistic typology and (limitations of)
multilingual language modeling. In Proceedings of
EMNLP 2018, pages 316-327.

Ana Valeria Gonzalez, Maria Barrett, Rasmus Hvin-
gelby, Kellie Webster, and Anders S¢gaard. 2020.
Type B reflexivization as an unambiguous testbed
for multilingual multi-task gender bias. In Proceed-
ings of EMNLP 2020, pages 2637-2648.


Ana Valeria Gonzalez, Anna Rogers, and Anders S¢-
gaard. 2021. On the interaction of belief bias and ex-
planations. In Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021, pages
2930-2942.

Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203-225.

Victor Petrén Bach Hansen and Anders Sggaard. 202 1a.
Guideline bias in Wizard-of-Oz dialogues. In Pro-
ceedings of the Ist Workshop on Benchmarking:
Past, Present and Future, pages 8-14.

Victor Petrén Bach Hansen and Anders S¢gaard. 202 1b.
Is the lottery fair? evaluating winning tickets across
demographics. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021,
pages 3214-3224.

Trevor J. Hastie and Robert J. Tibshirani. 2017. Gener-
alized additive models. Routledge.

Peter Henderson, Jieru Hu, Joshua Romoff, Emma
Brunskill, Dan Jurafsky, and Joelle Pineau. 2020.
Towards the systematic reporting of the energy and
carbon footprints of machine learning. Journal of
Machine Learning Research, 21(248):1-43.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efficient transfer learning for NLP.
In Proceedings of ICML 2019, pages 2790-2799.

Dirk Hovy and Anders Sggaard. 2015. Tagging perfor-
mance correlates with author age. In Proceedings of
ACL-IJCNLP 2015, pages 483-488.

Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. XTREME: A Massively Multilingual Multi-
task Benchmark for Evaluating Cross-lingual Gener-
alization. In Proceedings of ICML 2020.

Xiaolei Huang, Linzi Xing, Franck Dernoncourt, and
Michael J. Paul. 2020. Multilingual Twitter cor-
pus and baselines for evaluating demographic bias
in hate speech recognition. In Proceedings of LREC
2020, pages 1440-1448.

Anders Johannsen, Dirk Hovy, and Anders Sggaard.
2015. Cross-lingual syntactic variation over age and
gender. In Proceedings of CoNLL 2015, pages 103—
112.

Anna Jgrgensen and Anders Sgégaard. 2021. Evaluation
of summarization systems across gender, age, and
race. In Proceedings of the Third Workshop on New
Frontiers in Summarization, pages 51-56.

Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The State and
Fate of Linguistic Diversity and Inclusion in the
NLP World. In Proceedings of ACL 2020.

John Judge, Aoife Cahill, and Josef van Genabith. 2006.
QuestionBank: Creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 497-504.

Dan Jurafsky and James H. Martin. 2009. Speech and
language processing : an introduction to natural
language processing, computational linguistics, and
speech recognition. Pearson Prentice Hall, Upper
Saddle River, N.J.

David Jurgens, Varada Kolhatkar, Lucy Li, Margot
Mieskes, and Ted Pedersen, editors. 2021. Proceed-
ings of the Fifth Workshop on Teaching NLP.

Nora Kassner, Philipp Dufter, and Hinrich Schiitze.
2021. Multilingual LAMA: Investigating knowl-
edge in multilingual pretrained language models. In
Proceedings of EACL 2021, pages 3250-3258.

Sanjeev P Khudanpur. 2006. Multilingual language
modeling. Multilingual Speech Processing, page
169.

Pang Wei Koh, Shiori Sagawa, Henrik Mark-
lund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga,
Richard Lanas Phillips, Irena Gao, Tony Lee, Eti-
enne David, Ian Stavness, Wei Guo, Berton A. Earn-
shaw, Imran S. Haque, Sara Beery, Jure Leskovec,
Anshul Kundaje, Emma Pierson, Sergey Levine,
Chelsea Finn, and Percy Liang. 2021. WILDS: A
benchmark of in-the-wild distribution shifts. In Pro-
ceedings of ICML 2021.

Imre Lakatos. 1976. Falsification and the methodology
of scientific research programmes. In Can theories
be refuted?, pages 205-259. Springer.

Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,
Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani,
Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios
Anastasopoulos, Patrick Littell, and Graham Neu-
big. 2019. Choosing Transfer Languages for Cross-
Lingual Learning. In Proceedings of ACL 2019.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521-
535.

Fangyu Liu, Emanuele Bugliarello, Edoardo Maria
Ponti, Siva Reddy, Nigel Collier, and Desmond EI-
liott. 2021a. Visually Grounded Reasoning across
Languages and Cultures. In Proceedings of EMNLP
2021.

Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating Wikipedia by Summariz-
ing Long Sequences. In Proceedings of ICLR 2018.


Xiangyang Liu, Tianxiang Sun, Junliang He, Lingling
Wu, Xinyu Zhang, Hao Jiang, Zhao Cao, Xuanjing
Huang, and Xipeng Qiu. 2021b. Towards efficient
nlp: A standard evaluation and a strong baseline.
arXiv preprint arXiv:2110.07038.

Wentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shi-
jin Wang, and Guoping Hu. 2020. CharBERT:
Character-aware pre-trained language model. In
Proceedings of the 28th International Conference on
Computational Linguistics, pages 39-50, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.

Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya
Jain, Ledell Wu, Robin Jia, Christopher Potts, Ad-
ina Williams, and Douwe Kiela. 2021. Dynaboard:
An evaluation-as-a-service platform for holistic next-
generation benchmarking. CoRR, abs/2106.06052.

Olga Majewska, Evgeniia Razumovskaia,
Edoardo Maria Ponti, Ivan Vulic, and Anna
Korhonen. 2022. Cross-lingual dialogue dataset

creation via outline-based generation. CoRR,

abs/2201.13405.

Christopher D. Manning and Hinrich Schiitze. 1999.
Foundations of Statistical Natural Language Pro-

cessing. The MIT Press, Cambridge, Mas-
sachusetts.

Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann _ Bies,

Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate ar-
gument structure. In Human Language Technology:
Proceedings of a Workshop.

Julian John McAuley and Jure Leskovec. 2013. From
amateurs to connoisseurs: Modeling the evolution of
user expertise through online reviews. In Proceed-
ings of WWW 2013, page 897-908.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Distributed Representations of Words
and Phrases and their Compositionality. In Proceed-
ings of NeurIPS 2013.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Jan Haji¢, Christopher D. Manning, Sampo
Pyysalo, Sebastian Schuster, Francis Tyers, and
Daniel Zeman. 2020. Universal Dependencies v2:
An evergrowing multilingual treebank collection. In
Proceedings of LREC 2020, pages 4034-4043.

Debora Nozza. 2021. Exposing the Limits of Zero-shot
Cross-lingual Hate Speech Detection. In Proceed-
ings of ACL 2021, pages 907-914.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of ACL
2011, pages 309-319.

Jonas Pfeiffer, Ivan Vuli¢é, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
In Proceedings of EMNLP 2020, pages 7654-7673.

Jonas Pfeiffer, Ivan Vuli¢, Iryna Gurevych, and Sebas-
tian Ruder. 2021. UNKs everywhere: Adapting mul-
tilingual language models to new scripts. In Pro-
ceedings of EMNLP 2021.

Edoardo Maria Ponti, Roi Reichart, Anna Korhonen,
and Ivan Vuli¢. 2018. Isomorphic transfer of syntac-
tic structures in cross-lingual NLP. In Proceedings
of ACL 2018, pages 1531-1542.

Shauli Ravfogel, Yoav Goldberg, and Francis Tyers.
2018. Can LSTM learn to capture agreement? the
case of Basque. In Proceedings of the 2018 EMNLP
Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pages 98-107.

Marek Rei and Anders S¢gaard. 2018. Zero-shot se-
quence labeling: Transferring knowledge from sen-
tences to tokens. In Proceedings of NAACL-HLT
2018, pages 293-302.

Adithya Renduchintala, Denise Diaz, Kenneth
Heafield, Xian Li, and Mona Diab. 2021. Gender
bias amplification during speed-quality optimization
in neural machine translation. In Proceedings of
ACL-IJCNLP 2021, pages 99-109.

Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
2020. A primer in BERTology: What we know
about how BERT works. Transactions of the Associ-
ation for Computational Linguistics, 8:842-866.

Sebastian Ruder, Matthew E Peters, Swabha
Swayamdipta, and Thomas Wolf. 2019. Trans-
fer learning in natural language processing. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Tutorials, pages 15-18.

Phillip Rust, Jonas Pfeiffer, Ivan Vuli¢é, Sebastian
Ruder, and Iryna Gurevych. 2021. How Good is
Your Tokenizer? On the Monolingual Performance
of Multilingual Language Models. In Proceedings
of ACL-IJCNLP 2021, pages 3118-3135.

Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren
Etzioni. 2020. Green AI. Communications of the
ACM, 63(12):54—-63.

Sofia Serrano and Noah A. Smith. 2019. Is attention
interpretable? In Proceedings of ACL 2019, pages
2931-2951.

Yikang Shen, Zhouhan Lin, Chin-wei Huang, and
Aaron Courville. 2018. Neural Language Modeling
by Jointly Learning Syntax and Lexicon. In Pro-
ceedings of ICLR 2018.

Tracy Sherman. 1985. Categorization skills in infants.
Child Development, 56(6):1561-—73.


Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP 2013, pages 1631-
1642.

Anders Sggaard. 2013. Semi-supervised learning and
domain adaptation for NLP. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers, United States.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Computational Lin-
guistics, 37(2):267-307.

Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald
Metzler. 2020. Efficient transformers: A survey.
CoRR, abs/2009.06732.

Reut Tsarfaty, Dan Bareket, Stav Klein, and Amit
Seker. 2020. From SPMRL to NMRL: What did
we learn (and unlearn) in a decade of parsing
morphologically-rich languages (MRLs)? In Pro-
ceedings of ACL 2020, pages 7396-7408.

Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei
Chang, and Kristina Toutanova. 2021. Revisiting
the Primacy of English in Zero-shot Cross-lingual
Transfer. arXiv preprint arXiv:2106.16171.

Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL 2010, pages 384-394.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proceedings of ACL 2017, pages 2016-2027.

Eva Vanmassenhove, Dimitar Shterionov, and Matthew
Gwilliam. 2021. Machine translationese: Effects of
algorithmic bias on linguistic complexity in machine
translation. In Proceedings of the EACL 2021, pages
2203-2213.

Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
Sharon Qian, Daniel Nevo, Yaron Singer, and Stu-
art M Shieber. 2020. Investigating gender bias in
language models using causal mediation analysis. In
Proceedings of NeurIPS 2020.

David Vilares, Haiyun Peng, Ranjan Satapathy, and
Erik Cambria. 2018. BabelSenticNet: A common-
sense reasoning framework for multilingual senti-
ment analysis. In 2018 IEEE Symposium Series
on Computational Intelligence (SSCI), pages 1292—
1298.

Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment analy-
sis in social media. In Proceedings of EMNLP 2013,
pages 1815-1827.

Zihan Wang, Karthikeyan K, Stephen Mayhew, and
Dan Roth. 2020. Extending multilingual BERT to
low-resource languages. In Findings of EMNLP
2020, pages 2649-2656.

Ron Weber. 2006. Reach and grasp in the debate over
the is core: An empty hand? Journal of the Associa-
tion for Information Systems, 7(10):28.

Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. Ex-
tracting Automata from Recurrent Neural Networks
Using Queries and Counterexamples. In Proceed-
ings of ICML 2018.

Sarah Wiegreffe and Yuval Pinter. 2019. Attention
is not not explanation. In Proceedings of EMNLP-
IJCNLP 2019, pages 11-20.

Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng,
and Lei Li. 2021. Vocabulary learning via optimal
transport for neural machine translation. In Proceed-
ings of ACL-IJCNLP 2021, pages 7361-7373.

Yi Yang and Jacob Eisenstein. 2017. Overcoming lan-
guage variation in sentiment analysis with social at-
tention. Transactions of the Association for Compu-
tational Linguistics, 5:295-307.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of ACL 1995, pages 189-196.

Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng,
and Kai-Wei Chang. 2021. Broaden the vision: Geo-
diverse visual commonsense reasoning. In Proceed-
ings of EMNLP 2021, pages 2115-2129.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. In Proceedings of NeurIPS 2020.

Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,
Kai-Wei Chang, and Ahmed Hassan Awadallah.
2020. Gender bias in multilingual embeddings and
cross-lingual transfer. In Proceedings of ACL 2020,
pages 2896-2907.

Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulic,
Roi Reichart, Anna Korhonen, and Hinrich Schiitze.
2021. A closer look at few-shot crosslingual trans-
fer: The choice of shots matters. In Proceedings of
ACL-IJNCLP 2021, pages 5751-5767.

Ming Zhu, Aman Ahuja, Da-Cheng Juan, Wei Wei,
and Chandan K. Reddy. 2020. Question answer-
ing with long multiple-span answers. In Findings
of the Association for Computational Linguistics:
EMNLP 2020, pages 3840-3849, Online. Associa-
tion for Computational Linguistics.


A Appendix

A.1 Annotation guidelines

For multilinguality, we consider papers that eval-
uate on 3 languages, or 4 languages if they focus
on MT (as the standard MT experiment includes
two languages). For fairness and bias, we consider
papers that improve fairness in a specific setting or
analyze the bias of a method, e.g. regarding gen-
der. For efficiency, we consider papers that analyze
memory, speed, or computational complexity. For
interpretability, we consider papers that interpret
or explain a model’s predictions.

In every case, we consider papers that make a
practical contribution to a dimension and provide
quantifiable results along the dimension. For multi-
linguality, fairness and bias, and efficiency, a practi-
cal contribution constitutes the use of an evaluation
metric that is appropriate for the specific setting.
For interpretability, this may include a user study,
an analysis of correlation results, or a qualitative
analysis of interpretable features.

A.2 Analysis of remaining areas at ACL 2021

We provide statistics for the remaining areas at
ACL 2021 in Table 4.

A.3 Analysis of Efficiency area at EMNLP
2021

We annotated the 20 papers presented orally at
EMNLP 2021 in the “Efficient Models in NLP”
area. Among the presented papers, 19/20 are mono-
lingual and 17 focus only on English. Among the
other two, one focuses on Indonesian and one on
Chinese. The last paper focuses on MT with multi-
ple languages. Papers mainly evaluate using accu-
racy and/or Fl and many papers evaluate on GLUE.
There is a single two-dimensional paper according
to our criteria (the paper focusing on MT, which
makes contributions on multilinguality and effi-
ciency) while two other papers can be considered
two-dimensional but cover dimensions that we do
not annotate, i.e. privacy and robustness respec-
tively. This analysis corroborates our findings that
research papers depart from SQUARE ONE in such
dedicated conference areas/tracks, but largely only
across a single dimension.


Area # papers English Accuracy/F1 Multilinguality Fairness and bias Efficiency Interpretability >1 dimension

Question Answering 24 95.8% 41.7% 4.2% 4.2% 8.3% 4.2% 0.0%
Sentence-level Semantics 23 87.0% 56.5% 8.7% 0.0% 4.3% 17.4% 4.3%
Computational Social Science 18 77.8% 66.7% 0.0% 22.2% 0.0% 16.7% 0.0%
Language Generation 18 83.3% 0.0% 11.1% 5.6% 11.1% 11.1% 5.6%
Sentiment Analysis 18 100.0% 72.2% 0.0% 0.0% 11.1% 11.1% 0.0%
Summarization 12 91.7% 0.0% 0.0% 8.3% 0.0% 8.3% 0.0%
Semantics: Lexical Semantics 12 58.3% 41.7% 25.0% 0.0% 16.7% 0.0% 8.3%
Information Retrieval i) 91.7% 8.3% 0.0% 0.0% 0.0% 0.0% 8.3%
Language Grounding to Vision 11 100.0% 18.2% 0.0% 0.0% 9.1% 27.3% 0.0%
Syntax 10 40.0% 20.0% 30.0% 0.0% 20.0% 10.0% 20.0%
Best Paper Session 8 50.0% 50.0% 12.5% 0.0% 25.0% 25.0% 12.5%
Speech and Multimodality 6 66.7% 33.3% 16.7% 0.0% 0.0% 0.0% 0.0%
Phonology and Morphology 6 33.3% 33.3% 33.3% 0.0% 0.0% 16.7% 16.7%
Linguistic Theories 6 100.0% 16.7% 0.0% 0.0% 16.7% 33.3% 0.0%
Theme 5 20.0% 40.0% 20.0% 20.0% 20.0% 20.0% 20.0%

Table 4: The number of papers in the remaining areas as well as the fractions that only evaluate on English, only
use accuracy / Fl, make contributions along one of four dimensions, and make contributions along more than a
single dimension (from left to right).
