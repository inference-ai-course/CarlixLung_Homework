arXiv:2311.02205v2 [cs.CL] 8 Nov 2023

An Introduction to Natural Language
Processing Techniques and Framework for
Clinical Implementation in Radiation Oncology

R. Khanmohammadi!, MM. Ghassemi !, K. Verdecchia”, A. I. Ghanem ®, L.
Bing’, I. J. Chetty 2, H. Bagher-Ebadian ’, F. Siddiqui”, M. Elshaikh 2, B.
Movsas7? and K. Thind 2*

' Department of Computer Science and Engineering, Michigan State University, USA
* Department of Radiation Oncology, Henry Ford Cancer Institute, USA

3 Alexandria Clinical Oncology Department, Alexandria University, Egypt
Correspondence”:

Kundan Thind
kthind1@nhfhs.org

ABSTRACT

Natural Language Processing (NLP) is a key technique for developing Medical Artificial
Intelligence (Al) systems that leverage Electronic Health Record (EHR) data to build diagnostic
and prognostic models. NLP enables the conversion of unstructured Clinical text into structured
data that can be fed into Al algorithms. The emergence of the transformer architecture and
large language models (LLMs) has led to remarkable advances in NLP for various healthcare
tasks, such as entity recognition, relation extraction, sentence similarity, text summarization,
and question answering. In this article, we review the major technical innovations that underpin
modern NLP models and present state-of-the-art NLP applications that employ LLMs in radiation
oncology research. However, these LLMs are prone to many errors such as hallucinations,
biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical
deployment. As such, we propose a comprehensive framework for assessing the NLP models
based on their purpose and clinical fit, technical performance, bias and trust, legal and ethical
implications, and quality assurance, prior to implementation in clinical radiation oncology. Our
article aims to provide guidance and insights for researchers and clinicians who are interested in
developing and using NLP models in clinical radiation oncology.

Keywords: Artificial Intelligence, Radiation Oncology, Natural Language Processing, Large Language Models, Personalized Medicine

1 INTRODUCTION

Artificial intelligence (AI) is transforming healthcare by improving patient outcomes, optimizing clinical
workflows, and reducing costs (1). A key area where AI is being rapidly evolving is precision medicine,
which tailors personalized medical care to patient’s genes, environments, and lifestyles (2). For example,
medical practitioners have the capability to detect disease-causing genetic mutations (3) or develop
customized sedation protocols for individual patients (4) by employing machine learning algorithms, which
serve as a pivotal element within the realm of AI. As such, advanced AI models are being constructed
within medicine for personalized disease prevention, care, and even prophylaxis. The ongoing trend of
increased healthcare data collection will further aid innovative applications of these technologies in the
future.


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

Natural Language Processing (NLP) is a subclass of AI that enables machines to understand and interpret
human language. The goal of NLP is to develop algorithms and models that are capable of processing,
analyzing, and generating natural language text and speech. The history of natural language processing
dates back to the 1950s, when early language translation programs were developed (5). However, limited
computing power and lack of data slowed progress (6). The introduction of statistical methods in the
1980s led to more sophisticated language models (7). The development of neural networks and deep
learning in the 2010s led to significant advancements in NLP, culminating in the development of the famous
transformer architecture in 2017 (8). Riding on the capabilities of the transformer architecture, a new breed
of models emerged, referred to as Large Language Models (LLMs). Notable LLMs include Bidirectional
Encoder Representations from Transformers (BERT) (9), GPT-4 (10), and ChatGPT (11). The development
of these technologies has revolutionized language processing and opened new possibilities for interacting
with machines using natural language.

The transformer architecture has paved the way for the development of large language models, which are
now at the forefront of NLP research. Language modeling (12), machine translation (13), and sentiment
analysis (14) have been shown to perform exceptionally well with these models. However, training such
models is a complex and resource-intensive process requiring significant hardware and data investments.
Researchers have explored the use of transfer learning to address this challenge (15), a technique that
allows pre-trained models to be fine-tuned on specific tasks with limited data. Indeed, the real-world
applications of NLP often begin with fine-tuning these pre-trained models to suit the specific needs of
the task. Furthermore, zero-shot learning has also emerged as a promising method for generalizing new
tasks without explicit training (16). This is making these models easier to deploy, albeit evaluating their
performance is becoming more challenging as input data has turned to be more expansive and complex
(17). As a result, researchers and practitioners in the field are facing a significant challenge as they must
find ways to accurately assess the quality of these models when applied to prospective real-world data (18).

Modern NLP models have shown enormous potential in healthcare, specifically in radiation oncology
(19, 20, 21, 22), where precision in radiation targeting is crucial to a successful treatment (23). These
models can analyze vast amounts of data from Electronic Health Record (EHRs) and medical literature to
provide insights into disease staging, treatment options and patient outcomes (24). In addition, relevant
clinical information can be extracted from unstructured data sources, such as physician notes and radiology
reports (25), so clinicians can make better decisions about patient care. Despite this, the lack of clinical
evaluation and proper validation for many of these models poses a significant challenge to their widespread
adoption. The clinical evaluation of these models involves rigorous testing based on well-defined metrics
and benchmark datasets. This includes assessing the potential risks and benefits in prospective use of these
models, and thorough evaluation of impact on patient outcomes. The application of NLP models to the
field of clinical radiation oncology has the potential to enhance its outcomes and efficiency, contingent
upon the models’ ability to exhibit their validity, safety, fairness and reliability as quantified by a rigorous
evaluation framework. This requires collaboration between clinicians, data scientists, and regulatory bodies
to develop detailed and robust evaluation frameworks that can ensure that these models are integrated into
clinical workflows safely and effectively (26, 27).

In the sections below, we review the transformer architecture, that has been foundational to development
of LLMs. We will also discuss the training and fine-tuning for LLMs, and present recent application in
radiation oncology. Lastly, we will identify the existing challenges and limitations for the clinical adoption
of LLMs, and propose a checklist that serves as a preliminary guideline to facilitate the safe, fair, and
effective application of these algorithms in clinical settings.



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

We employed a two-pronged approach to literature retrieval to ensure a comprehensive review of modern
NLP techniques and their applicability in radiation oncology. First, we utilized ArXiv to capture the most
recent advancements in the NLP field, recognizing that cutting-edge research is often disseminated through
this preprint server before formal peer review. Search terms used on ArXiv included combinations of
”Natural Language Processing,” Transformer Architecture,” Large Language Models,” and Clinical
Applications.” Subsequently, we explored PubMed to obtain NLP research applications in Radiation
Oncology. Our search terms for PubMed encompassed ’Natural Language Processing,” ’Radiation
Oncology,” Electronic Health Records,” and ”Clinical AI Applications.” While this article does not
strictly adhere to the criteria of a systematic review, combining these resources ensured a timely coverage
of both technical advancements and research applications, that aided in design of clinical implementation
framework of these models in radiation oncology.

2 MATERIALS AND METHODS
2.1 Literature review

We employed a two-pronged approach to literature review to ensure a comprehensive review of modern
NLP techniques and their applicability in radiation oncology. First, we utilized ArXiv to capture the most
recent advancements in the NLP field, recognizing that cutting-edge research is often disseminated through
this preprint server before formal peer review. Search terms used on ArXiv included combinations of
*Natural Language Processing,” Transformer Architecture,” Large Language Models,” and Clinical
Applications.” Subsequently, we explored PubMed to obtain NLP research applications in Radiation
Oncology. Our search terms for PubMed encompassed ’Natural Language Processing,” ’Radiation
Oncology,” Electronic Health Records,” and ”Clinical AI Applications.” This literature was critical
for summarizing the foundational elements of modern NLP pipelines, and their use in radiation oncology
research, as well as for considerations for the design of implementation framework of these models in
clinical radiation oncology. Below, we summarize the key foundational elements of the modern NLP
models.

2.2 Recurrent Neural Networks

Traditionally, NLP architecture was based on Recurrent Neural Networks (RNNs). As a class of neural
networks that excels at processing sequential data, RNNs provided a stepping stone for linguistic analysis,
enabling models to capture temporal dynamics and dependencies within text. However, these architectures
faced considerable difficulties when dealing with long-term dependencies due to issues known as vanishing
or exploding gradients (28). To address these limitations, Hochreiter and Schmidhuber introduced in 1997
a new variant of RNNs named Long Short-Term Memory (LSTM) networks (29). LSTMs differ from
traditional RNNs primarily in their internal mechanisms, using advanced gating, cell states, and internal
feedback loops.

LSTMs performed significantly more effectively at retaining and learning long-term dependencies in data.
They formed the backbone of many successful models tackling a variety of NLP tasks, including machine
translation, sentiment analysis, and language modeling. Yet, even with the advancements brought forth
by LSTMs, they weren’t exempt from challenges. Their recurrent nature implied sequential computation,
limiting parallelization and computational efficiency. Moreover, while LSTMs did address the issue of
capturing dependencies over longer stretches of text, they weren’t always perfect. This paved the way for



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

the transformer architecture, which not only enabled parallel computation but also harnessed self-attention
mechanisms to capture long-range dependencies more effectively (8).

2.3 Transformer architecture

The transformer architecture was introduced in the 2017 paper ’’Attention is All You Need” by Vaswani
et al. (8), revolutionizing NLP by replacing RNNs with self-attention mechanisms, enabling parallel
computation and capturing long-range dependencies more effectively. Fundamentally, the transformer is
composed of an encoder and a decoder. The encoder processes the input sequence, creating a context-rich
representation, which the decoder then uses to produce the desired output. For instance, in sequence-to-
sequence tasks like translations, the encoder might process an English sentence, and the decoder would
generate its French counterpart (30).

The transformer architecture represents a significant improvement over previous NLP algorithms, such as
LSTM (29), in two critical aspects. Firstly, transformers excel at learning context from the writing structure
and capturing long range dependencies via the mechanism of multi-headed attention (31). This allows for
native context understanding that was previously infeasible with LSTM. Secondly, transformers facilitate
the parallelization of computations during training, which results in faster and more efficient pre-training
(32).

°’

The transformer processes an input text by breaking it down into discrete elements known as ’tokens.’
Tokens are the building blocks of text, and depending on the granularity, a token can represent an entire
word (e.g., Hello’), a sub-word, or even a character. Unlike sequential processing, transformers deal with
these tokens concurrently, which enables capturing long-range dependencies among various parts of a
sentence, a critical aspect for comprehending its overall meaning. Before being processed, each token
undergoes ’input embedding’, converting it into a dense vector representation that captures its semantic
essence. Given the parallel nature of the transformer, it lacks a native understanding of the order of tokens.
This limitation is addressed by adding ’ positional encoding’ to the embeddings, ensuring the model discerns
the sequence order.

The architecture consists of multiple layers, each comprising two primary components: a self-attention
mechanism and a feed-forward network. Rather than having multiple sub-layers, each layer functions as
follows: the self-attention mechanism first computes an ’attention score’ between each pair of tokens in the
input sequence. These scores are then used to proportionally weigh the respective values of the tokens. The
sum of these weighted values forms the output of the self-attention mechanism for that layer. Following this,
a feed-forward neural network, a simple two-layer perceptron, refines the output from the self-attention
mechanism. Each of these two components, the self-attention mechanism and the feed-forward network,
is augmented by a residual connection and layer normalization, ensuring robust gradient flow during
training. Iterating this structure, the transformer executes multiple rounds of self-attention and feed-forward
processing, effectively deriving context from text, cementing its position as a dominant tool for NLP tasks.

The transformer employs a scaled dot-product attention mechanism for self-attention. It calculates
attention scores of size L x L from an input sequence of length L and dimensionality D. Each score (i,
j) delineates the similarity between the i-th and j-th tokens. A softmax function then normalizes these
attention scores, creating a probability distribution. The self-attention sublayer’s final output is achieved by
computing a weighted sum of the input sequence, using the normalized scores as weights. This mechanism
lets the model emphasize different sequence segments, thus capturing long-range dependencies, an essential
trait for modeling sequences. Stacking these sublayers enables the transformer to discern language nuances
and context, making it a pivotal foundation for NLP algorithms.



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

output probabilities

Linear (h)

Decoder

@ residual connections

e self-attention

Encoder

Add & Norm

Add & Norm Multi-Head
P| Attention (f)

(d)

Add & Norm

Add & Nor
(c) i alll Masked

(b) Multi-Head Multi-Head (e)
i —_— Attention

Positional
Encoding

Positional
Encoding

Input
Embedding

(a) Embedding

inputs outputs

Figure 1. Vaswani et al. (8) illustration of the transformer architecture. First, input words are embedded
and encoded based on their position in the input sequence (a). In the encoder, the Multi-head Attention
enables each word to attend to other words in the sequence, capturing relationships and dependencies (b).
Then, the original input embeddings are added to the self-attention outputs and normalized to integrate
attended information while preserving the input (c). Next, a neural network captures complex patterns and
interactions (d) before representations are added and normalized. On the other hand, the decoder’s masked
self-attention allows the decoder to capture dependencies among its own outputs (e). The decoder’s second
self-attention (f) attends to the encoded input sequence, allowing it to access the information from the
input and align it with the current decoding position. Similar to the encoder, outputs are further processed
with another feed-forward neural network (g), added, and normalized. Finally, the decoder applies a linear
transformation (h) followed by a softmax activation function to generate the probability distribution over
the vocabulary to select the next token.



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

In summary, the profound improvement in efficiency and contextual comprehension has catalyzed the
evolution of LLMs, positioning the transformer as the cornerstone of modern NLP algorithms (12). The
ensuing section delves into LLMs’ intricacies, surveys their diverse applications, and evaluates their
potential in healthcare, targeting improved clinical outcomes. Figure | visually encapsulates the discussed
Transformer architecture.

2.4 Large Language Models

As discussed, the transformers architecture is a foundational element in the field of NLP. Its significant
potential is most clearly realized in the advent and evolution of LLMs, which scale up the capabilities of
transformers to unprecedented dimensions. LLMs have emerged as an evolution of Language Models (LMs),
which, over the past year, have become a prominent sub-field in the domain of AI. By comprehending the
patterns and structures of languages, LLMs analyze the preceding words in a sequence in order to predict
the probability of a specific word sequence occurring within a given context, to predict what words appear
next (33). These were initially based on simple statistical models and have evolved into more complex
algorithms based on neural networks over time. The advent of LMs coincided with the rise of machine
learning, simplifying the development of powerful models (34). Several applications of this ability exist,
including speech recognition, machine translation, and chatbots. LLMs go byoned the LMs capability
to predict the likelihood of a sequence of words, by generating full passages of text, offering improved
performance and broader applicability. LLMs undergo extensive training on massive corpora of data,
consisting of billions of parameters, and are subsequently adapted for specific downstream tasks. The
potential of LLMs was first demonstrated with the release of OpenAI’s Generative Pre-trained Transformer
(GPT) models in 2018, followed by GPT-2 in 2019, GPT-3 in 2020, and GPT-4 in 2023. The first three
iterations have 117 million, 1.5 billion, and 175 billion parameters, respectively, while no official size
information exists for the fourth generation.

LMs are categorized into three key architectures (35, 36). Encoder-only models like BERT process
the entire input text at once, producing a context-aware vector for each token. They excel in tasks that
require understanding context (37), as they are trained on tasks like masked language prediction. The
encoder-only models are primarily designed for tasks where the relationship between parts of a text needs
to be comprehensively understood, without the need for sequence generation. On the contrary, decoder-only
models like GPT predict the next word based on preceding words, making them suitable for text generation
tasks. This architecture is inherently designed for generating new sequences, given a context. Finally,
encoder-decoder models, such as T5 (38) and Bidirectional Auto-Regressive Transformers (BART) (39),
combine both approaches. They use an encoder to process the input text and a decoder to generate the
output, providing a balance between context understanding and sequence generation. T5 casts all tasks as a
text-to-text problem, predicting output text from input, while BART is trained to reconstruct original text
from a corrupted version, allowing it to generate more coherent text. Despite this broad categorization,
many models combine specific elements of the encoder and decoder architecture to fulfill specific goals.
The rationale behind the adoption of a particular architecture, be it encoder-only, decoder-only, or a
combination of both, revolves around the model’s goal and the nature of the linguistic task it aims to excel
in.

In spite of the remarkable language understanding capabilities, these models still require task-specific
fine-tuning to attain optimal performance (40). This fine-tuning process entails exposing the model to
data pertaining to the target task and modifying its parameters accordingly. The employment of these
models has considerably diminished the necessity for copious amounts of task-specific data and enabled



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

the development of highly precise and efficient NLP systems for various applications, such as language
translation, question answering, and sentiment analysis (41). LLMs, especially those like GPT-3 and
beyond, have demonstrated potential for ’zero-shot” or ’few-shot” learning. In a zero-shot scenario, these
models attempt tasks they’ve never explicitly trained on, leveraging their extensive pre-training on vast
datasets. For instance, ChatGPT, when used in a generalized context like chat-based applications, can
generate responses in a manner that seems it was fine-tuned for a plethora of tasks. This vast and implicit
understanding enables it to generalize impressively across numerous tasks. However, while this broad
applicability is a transformative advancement in NLP, there might still be niche tasks where dedicated,
task-specific fine-tuning delivers superior results.

Furthermore, the rise of LLMs has been pivotal in the proliferation of novel NLP applications, such
as sophisticated language generation and chatbots that have the potential to redefine human-machine
interaction. Their accessibility has been further augmented by the development of platforms and libraries
such as Hugging Face’s transformers library (42), which offers pre-trained models and user-friendly APIs,
facilitating easier model fine-tuning and deployment. Comprehensive resources for crafting and training
these models are also made available through platforms like Google’s TensorFlow (43) and Meta’s PyTorch.

In the medical field, LLMs like Google’s Med-PaLM (44) are making significant strides. Med-PaLM,
designed for the medical domain, was the first AI to surpass the pass mark on USMLE style questions. Its
successor, Med-PaLM 2 (45), further improved accuracy in medical exams and consumer health queries.
Additionally, the MED-PALM M (46) model integrates text and imaging data, opening new frontiers in
medical applications like radiomics for tumor characterization and therapy response prediction (? ).

3 RESULTS - RESEARCH APPLICATIONS
3.1 Recent LLM Applications in Radiation Oncology

Natural language processing has been explored for applications in cancer (22), and recent advances
promise improved cancer care through the use of big data such as real-world data from the EHR and the
oncology information system (OIS) (47). In comprehensive reviews by Yim et al. (19) and Bitterman
et al. (21), a diverse range of applications of NLP in the field of radiation oncology were discussed. These
encompassed applications of specific information extraction from clinical notes (48), standardization
of treatment planning structures (49, 50), and identification of standardized treatment locations (51).
Furthermore, the application of NLP in toxicity data extraction was showcased, with a particular focus on
the use of the clinical Text Analysis and Knowledge Extraction System (cTAKES) (20, 52). The CTAKES!
is an open-source natural language processing system developed by the Mayo Clinic, designed to extract
clinically relevant information from free text medical record data.. The review also highlighted the role
of advanced NLP methods, specifically high-performance deep learning models, in recognizing specific
radiation therapy entities (53, 54), as well as their pivotal contribution in expanding cancer registries
through the extraction of relevant information from clinical text (55, 56, 57). Additionally, the potential of
NLP in identifying latent trends in radiation oncology literature was explored (58, 59).

The emergence of LLMs has enabled new possibilities for NLP pipelines in various domains of healthcare,
and specifically in oncology and radiation oncology. We present a brief overview of some of the cutting-
edge NLP applications that leverage state-of-the-art LLMs and elucidate their potential to transform clinical
practice below:

1 https://ctakes.apache.org/



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

Enhancing Electronic Health Record (EHR Summarization): LLMs can play a crucial role in
summarizing electronic health records EHRs for radiation oncology. The Soft Prompt-Base Calibration
(SPeC) pipeline introduced by Chuang et al. (60) addresses the challenge of increased output variance in
EHR summarization. By incorporating SPeC, LLMs can provide more reliable and uniform summaries of
vital medical information, aiding healthcare providers in making informed decisions and improving patient
outcomes.

De-identification of Radiation Oncology Data: De-identification of sensitive medical data is of utmost
importance in radiation oncology research. LLMs, such as ChatGPT and GPT-4, offer advanced named
entity recognition (NER) capabilities, enabling the development of robust de-identification frameworks.
Liu et al. (61) proposed a GPT-4-enabled de-identification framework (DeID-GPT) that effectively masks
private information from unstructured medical text, ensuring data privacy while preserving the original
meaning and structure.

Clinical Text Mining for Radiation Oncology: LLMs, when applied to clinical text mining tasks in
radiation oncology, can provide valuable insights. Tang et al. (62) explored the use of ChatGPT for
biological named entity recognition and relation extraction. To mitigate privacy concerns and improve
performance, they introduced a framework that generates high-quality synthetic data with ChatGPT and
fine-tunes local models for downstream tasks. This approach not only enhances performance but also
reduces the time and effort required for data collection and labeling.

Radiation Oncology Education and Knowledge Expansion: LLMs, such as ChatGPT, hold promise as
educational tools in radiation oncology. Evaluating the performance of ChatGPT in answering questions
within the scope of radiation oncology exams, as demonstrated by Gilson et al. (63), highlights the potential
of LLMs in medical education. These models can provide logical justifications and internal information,
supporting learning and expanding knowledge in the field.

Radiation Treatment Documentation and Discharge Summaries: LLMs like ChatGPT can streamline
the process of generating radiation treatment documentation and discharge summaries. Patel and Lam (64)
discuss the potential benefits of utilizing ChatGPT to input briefs and quickly generate comprehensive
discharge summaries. While manual checking by a radiation oncologist is essential, this application of
LLMs has the potential to improve efficiency and documentation quality in radiation oncology.

3.2 Current shortcomings of NLP models

The proliferation of LLMs has facilitated the development of conversational AI applications with
widespread use cases. However, a significant challenge with these models, especially in the realm of
healthcare and medicine, is their propensity for generating hallucinations—outputs that are illegitimate,
irrelevant, or incorrect in relation to the original question (65, 66). While there have been improvements
with newer models like GPT-4, hallucinations remain a pressing concern. In the medical context, such
erroneous outputs can lead to misinformed decisions, potentially jeopardizing patient safety (67). As LLMs
are increasingly envisioned for applications like monitoring, diagnosing, and treating patients, ensuring the
reliability and accuracy of their outputs becomes paramount.

Another pivotal aspect of LLMs’ performance is their embedded bias (68, 69) raising the need for
collaboration between health professionals and data scientists to avoid encoding historical health disparities
into the future. An evaluation of biases in existing NLP models used in psychiatry was presented by Straw
and Callison-Burch (70) in a literature review of the use of NLP in mental health. The primary analysis of
mental health terminology in GloVe (71) and Word2Vec (72) embeddings showed that significant levels of



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

bias exist in the categories of religion, race, gender, nationality, and sexuality. Ultimately, they conclude
that cross-disciplinary collaboration and communication are imperative to minimize the risks associated
with health inequalities and provide recommendations for preventing these biases from occurring again due
to AI and data-driven algorithmic decisions.

LLMs, trained on vast internet datasets, may harbor risks of producing offensive or discriminatory content,
known as model toxicity (73). Evaluations are vital to prevent biased outputs, stereotype reinforcement, or
misinformation spread. This is typically done by analyzing model outputs and benchmark toxic datasets
(74). In addition to toxicity, other challenges such as legal concerns around AI liability in decision-making
are emerging (75), requiring the creation of more reliable, safe, and legally compliant models. These
legal concerns underscore the need for a robust framework of accountability and transparency to guide the
use and deployment of these powerful AI tools, precluding any possibility for generation of any medical
decision without ensuring proper review by health care providers.

Overall, the development and deployment of LLMs has become more accessible and easier to use on large
cohorts of input data with rapid, fully implementable packages. On the other hand, these models generated
comprehensive output, which is prone to issues as described above. This has resulted in performance
evaluation of LLMs being a challenging task compared to the previous class of NLP algorithms (76).
Further, training and evaluating LLMs requires significant computational resources, which limits many
low-resource environments, that might be in a real need for these advancements, from fully utilizing this
powerful technology. Moreover, evaluating LLMs’ quality is often challenging, as nuances in natural
languages require a variety of benchmarks. This challenge becomes particularly acute when LLMs are
deployed in clinical settings.

Careful consideration must be made to all aspects of LLM development, performance, bias, and trust
prior to implementation in the clinic. As such, rigorous evaluation and validation are required to ensure
that NLP models are reliable and safe for use in the clinic. Rapid development and deployment of this
technology implies a number of clinical applications that will be proposed for use in the field of radiation
oncology. While these will aim to improve cancer care, a safe and methodical approach should taken to
ensure a comprehensive evaluation of these algorithms prior to clinical deployment. As such, in the next
section, we propose a checklist that can be utilized for step-wise evaluation of NLP models prior to clinical
deployment in radiation oncology.

4 RESULTS - CLINICAL CONSIDERATIONS
4.1 Framework for Clinical Implementation

This section presents a comprehensive framework for the clinical implementation of NLP systems in
radiation oncology. The framework consists of three main components: (1) evaluation of the purpose and
clinical fit of the NLP system, (2) commissioning of the NLP system, and (3) quality assurance of the NLP
system. The commissioning section includes sub-sections of technical performance, bias and trust, and
legal and ethical scope. A graphical overview of the framework and a checklist of relevant questions for the
clinical commissioning team are provided in Figure 2.

4.2 Purpose and clinical integration

The first consideration for clinical implementation should be to define the purpose of the NLP model. This
should include scope of the clinical problem including the specific context, appropriate NLP solution, and
the expected benefit to clinic. The definition of clinical problem, and expected solution by NLP model can



Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

What is the algorithm purpose, and benefit to clinic?

How will it redefine workflows, processes, and user
responsibilities?

What are the details of education and training plan for
expert users?

Tachnicalleerformance How is the algorithm performance being evaluated?

What are the model limitations and assumptions?

How are the potential biases being tested?

Bias and Trust

5)
.
<
:
m7
2

How is the trustworthiness being evaluated?

e

Does the device pass all legal requirements?

Comm

22g MTs CU GENTS) What is the plan for harmful decision liability?

Quality Assurance What is the quality management plan for this device?

Figure 2. Framework for assessing natural language processing algorithms before they are deployed in
clinical settings. The framework consists of several categories and questions that can help evaluate the
objectives, outcomes, limitations, reliability, validity, and quality control measures of the algorithms, as
well as their implications for clinical practice and patient safety.

be general, however, it should include concrete elements that can be empirically tested and validated before
proceeding to routine use. Typically, new innovations in radiation oncology are assessed for efficiency
and efficacy before clinical consideration. Therefore, depending on the objective, NLP algorithm should
be appraised comprehensively for efficiency and efficacy of clinical tasks and processes. A well-defined
objective and a clear expectation for efficiency and efficacy are essential for thorough performance testing
and consideration for clinical use.

Beyond the purpose, it is important to detail the expected change resultant from the new algorithm. As
such, consideration and plan should be detailed for how the new algorithm may assist or augment the
expert clinical user, and lead to a change in workflow management, and processes in the clinic. A clear
education, and training plan should also be outlined for clinical users such that the expert user can use the
new algorithm to its maximal capacity.

4.3. Commissioning
4.3.1 Technical performance

NLP algorithm technical performance is evaluated on specific development datasets that is reflective of
the expected clinical performance. The development dataset is usually divided into three subsets: train,

10


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

development/validation, and test. The train set is used to train the initial algorithm, the validation set is
used to adjust the algorithm parameters for specific tasks, and the test set is used to measure the algorithm
performance. The important consideration for implementation team is to know that the testing subset should
be independent from the training and tuning subsets. The level of this independence can be categorized as
external validation, internal validation and cross-validation. External validation, where the testing subset
comes from a different source than the training and tuning subsets, is the most rigorous evaluation method.
Internal validation, where the testing subset is separated from the training and tuning subsets within a
single source dataset, is the next best method. Cross-validation, where the testing subset overlaps with
the training and tuning subsets, is a weaker method due to potential bias (77). Nested Cross- Validation,
however, serves as a more conservative alternative to traditional Cross-Validation, thereby reducing the
risk of information leakage among different sample sub-cohorts (78). In this method, Cross-Validation is
performed within the training set to choose the model parameters, and an external Cross-Validation loop is
used to estimate the error of the chosen model.

Once the dataset selection is validated, the algorithm performance evaluation can be appraised. Generally,
performance evaluation is closely associated with the NLP task at hand. NLP tasks can be broadly
categorized into classification tasks, named entity recognition, entity abstraction, summarization tasks, and
question answering tasks (79). Named entity recognition and entity abstraction can be evaluated using
metrics of negative predictive value and positive predictive value to ascertain the sensitivity and specificity
of the algorithm. The threshold for these metrics strictly depends on the use case of missing a positive
case or falsely identifying a negative case as positive. Summarization tasks can be evaluated using the
Rouge (80) metrics, which evaluate the overlap between the machine-generated and human-generated
summaries to ascertain the summarization’s quality. Finally, question answering tasks can be evaluated
using metrics like Bilingual Evaluation Understudy (BLEU) (81). BLEU is a widely used metric for
machine translation, evaluating the quality of translated text by comparing it to a set of high-quality
reference translations. It is important to understand the performance metric utilized to benchmark the
algorithm, and the appropriateness to prospective clinical utilization.

4.3.2 Bias and trust

Technically, bias can be minimized by optimizing for the lowest generalization error, which is measured
by the out-of-sample error and the gap between ground truth and prediction (82). This gap can be caused
by model inaccuracy, sampling error, or noise. The generalization error can be reduced by choosing the
right algorithm, tuning it well, and using large cohorts of diverse data. However, tuning of model should be
balanced as overfitting to variance also results to poor generalization error, and poor outputs (83). Therefore,
the optimal model should capture the data’s meaningful patterns without being overfitted, as demonstrated
in figure 3. Beyond the technical principle of minimizing generalization error, the algorithm should have
a low bias in the domains of statistical bias, algorithmic, and societal bias (84, 85, 86). Statistical bias
can be minimized by ensuring diverse dataset with reasonable variance to ensure adequate modeling for
varied outcomes. As such, there should be sufficient samples and adequate training for outcome modelling
of relatively rare conditions. Algorithmic bias can be minimized by appropriate training and fine-tuning,
such that underlying data patterns are appropriately represented. This step in particular ensures that all
the training data is appropriately represented, and that the subsets for training, fine-tuning and testing are
well defined, and have minimal leakage (87). Lastly, Societal bias, which results from discrimination for,
or against, a person or group, or a set of ideas or beliefs, in a way that is prejudicial or unfair. Often it
shows as bias based on sub-group inequity such as gender, race, disability, sexual orientation, geography
and other traits. Embedded societal bias results in poor outcomes, which can easily be amplified by the AI

11


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

Underfitting zone Overfitting zone

Generalization

capacity
Optimal Capacity

Figure 3. This figure illustrates the bias-variance tradeoff as a function of model capacity. The x-axis
represents model capacity, ranging from low to high. Three key trends are displayed: Bias (descending
curve), Variance (ascending curve), and Generalization Error (U-shaped curve). In the area of low capacity,
the model is prone to underfitting, represented by high bias and low variance. As we move towards higher
capacities, the model starts to overfit, demonstrated by low bias but high variance. The optimal model
capacity is depicted where the generalization error is minimized, balancing bias and variance. This point
represents the most effective model complexity for preventing both underfitting and overfitting, thus
achieving optimal performance on unseen data.

algorithms (88). Here, it is critical that the training data is diverse, and includes adequate representation
from all groups, and sub-groups of patients. Embedded societal biases in the training data need correction,
and the performance of the algorithm should be modeled and tested for different sub-groups. Even though
this necessitates acquiring larger volumes of diverse data for robust analysis, which poses a significant
challenge, it remains an essential task.

This is because algorithm fairness historically has not accounted for complex relationships between
biological, environmental, and social factors that result in varied outcomes (89). As such, this has been a
widely discussed problem in AI implementation, and a few tools are publicly available for bias detection
that can be utilized during algorithm testing. TCAV (Testing with Concept Activation Vectors) (90) is a
Google-developed framework that helps to interpret and analyze the behavior of machine learning models,
while audit-AI (91) and IBM’s AI Fairness 360 (AIF360) (92, 93) examine machine learning models to
detect race, gender, location, and other sources of bias and discrimination. While these tools may be helpful
overall, the testability for narrow use cases may be restrictive. If the algorithm under consideration has
resulted in manuscripts and clinical trials, the CLAIM (CheckList for Artificial Intelligence in Medical
Imaging) (94), Consort-AI (95), and CLAMP (Clinical Language Annotation, Modeling, and Processing)
(96) publications can be utilized and translated to ensure adequate study organization, scientific reporting,
and robust performance testing including sub-group representation (87, 97, 98). Lastly, it is important to
remember that the original data used for development and testing of NLP (Natural Language Processing)
pipelines may not be representative of an institution’s local data. Therefore, the clinic must ensure that
the algorithm ranks high on the fairness scale when applied to diverse patient populations, and as such,
the tools and concepts mentioned above can aid in the clinical implementation. Clinician trust in NLP
algorithm should be evaluated next on the path toward clinical implementation. Trust is a psychological
mechanism that deals with the uncertainty between known and unknown, where the algorithm transparency,
predictability, and fairness can play a large role in the trustworthiness of clinicians (99, 100). Algorithm

12


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

fairness has been discussed in the previous section, where minimization of bias is critical. The transparency
of the algorithm is related to the explainability and interpretability of the results, so that the algorithm can
map the output to a selection of inputs (101). In general, the more advanced an algorithm is, the lower the
explainability (102). However, to promote trust, explainability, and interpretability is increasingly being
incorporated in the more advanced algorithms (103). Beyond the explainability, the trust in the algorithm is
largely based on the predictability of outcomes, especially when faced with conflicting inputs.

Similar to the interpretability approaches for machine learning described, NLP methods also fall into
the categories of model-specific and model-agnostic interpretability (104). For instance, the attention
mechanism itself provides some level of model-specific interpretability. They offer a glimpse into the
working of the model by illustrating the importance of different words or phrases in the input for the model’s
decision-making (8). This way, clinicians can potentially understand which parts of a patient’s history
or report the algorithm deemed significant. On the model-agnostic side, techniques such as LIME (105)
(Locally Interpretable Model-agnostic Explanations) and SHAP (SHapley additive exPlanations) (106) have
also been applied to NLP. For example, LIME can provide insight into the model’s decisions by perturbing
the input and observing the model’s output, thereby explaining individual predictions (107). SHAP values
can provide a global view of feature importance across all predictions, demonstrating how much each
feature (or words in a textual modality) contributes to the model’s decisions. For example, a study by
Khanmohammadi et al. (108) utilized SHAP for interpretability in clinical NLP tasks, demonstrating the
most significant sound features in predicting fetal biological sex using Phonocardiogram signals. These
methods enhance the transparency of NLP algorithms and provide clinicians with a better understanding of
how the algorithms arrive at their conclusions. Published NLP algorithms are designed to work on specific
tasks, with the embedded assumption that the training and test data are generated from a similar statistical
distribution. This assumption may easily be violated in clinical scenarios, where input data may not match
the statistical assumptions, and the algorithm’s stability under these inputs will be directly associated with
the clinician’s trust in them. To test for predictability of outcomes, and an algorithm’s stability, the concept
of adversarial testing can be used to estimate algorithm performance under unstable inputs (109). There
are several methods for performing adversarial testing, with the evasion method perhaps being the most
suitable for testing the edges of the NLP model (110). This testing would encompass actively modifying
input data that represents the most extreme clinical scenario and analyzing the algorithm output for a)
transparency - does the model explain, or interpret the outputs to specific inputs? ; b) predictability — does
it show the same result every time?; c) fairness — does the output represent a sensible answer that is rooted
in representation of all sub-groups within the clinical setting? (111)

4.3.3 Legal and ethical considerations

Legal and regulatory frameworks ensure safe and ethical use of AI algorithms in healthcare. These
address the potential risks and challenges, and cover three main aspects of AI development and deployment,
namely: how medical devices are regulated, how health data privacy is protected, and how liability is
assigned for any harm caused by faulty, erroneous, or unsafe algorithm recommendations (112). Algorithm
regulation strictly follows the Food and Drug Administration (FDA) guidelines in the United States of
America, and significant progress has been made by the retaliatory agency in defining standards and
guiding principles for AI algorithms. NLP algorithms, being a sub-class of AI will fall under the category
of Software as a Medical Device (SaMD). The FDA has established a regulatory framework IN 2019 for
these devices (113), which proposes a risk-based regulation based on the intended use of the device and
the patient’s risk from inaccurate output. In 2021, FDA proposed an action plan, and guiding principles
in collaboration with Canada and UK to ensure safe, effective and quality SaMD use (114, 115). Recent

13


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

publications have discussed the regulation of LLM based chat-bots as medical devices (116). Therefore,
it is critical to understand the level of regulation for the device under consideration, and the regulatory
implications from FDA to ensure effective clinical use. Next, health data privacy is pivotal to ensure that
patient data is kept confidential and secure. All applications and data should meet regulatory compliance
under the HIPAA standards (117). Informed consent should also be considered to inform patients how
the NLP application uses their data, which may be waived for research applications. Both the adherence
to HIPAA standards, and informed consent should be evaluated for the algorithm under consideration
to ensure patient data privacy, and protection. Lastly, it is crucial to consider liability for any inaccurate
decision by the NLP algorithms, whereby mechanisms for addressing any legal concerns during application
use should be clarified (118). Despite the best intentions, flaws or errors in algorithm output can lead to
patient harm, and the liability systems are incorporated into device safety to ensure patient safety and
prevent unreliable, or unethical device behaviour (119, 120). Therefore, the mechanism for addressing
legal issues must be reviewed and accepted by the clinic prior to clinical implementation.

4.4 Quality Assurance

The technical and clinical performance of model is estimated during the commissioning process. A
Quality Management Program (QMP) details the performance tests, frequency of testing, expected outputs,
and plan of action for inadequate performance. Further it should include a quality improvement section,
where the inadequate performance by the algorithm under routine use can be evaluated and discussed
in detail to ensure safe and quality clinical care. As such, QMP can be divided into routine quality
assurance, and case-specific testing for quality improvement (121, 122). Routine quality assurance must
be performed periodically with a stable reference dataset every time to ensure the stability of outputs.
Ideally, the reference data is a sub-set of the data that is utilized during the commissioning process, and
is representative of routine clinical data and use cases. This step should also be used for clinical release
of device after downtime, or minor changes, such as change in computation hardware. A comprehensive
data logging system is recommended for structured collection of algorithm input, output and stability.
This should a pivotal piece of the quality improvement component, whereby, the unexpected performance
by the algorithm, can be traced back to the inputs, and root cause analysis can be performed to uphold
safe and quality driven clinical care. Review of case-specific performance further allows for identification
of model limitations, that can facilitate future model revisions (122). We believe that new and emergent
use cases for the algorithm should be evaluated fully by testing purpose, clinical fit, commissioning and
quality assurance plan as outlined in section 3.1 to 3.5. This rigorous step will ensure that the algorithm
performance is suitable to ensure safe and high-quality clinical care.

5 CONCLUSION

In this article, we have discussed the recent advances and applications of NLP in radiation oncology. NLP
is a powerful tool that can transform unstructured clinical narratives into structured data for medical AI
systems. NLP models utilizing LLMs, and based on self-attention transformer architecture can perform
multiple domain-specific tasks through transfer learning, which reduces the need for large annotated data
sets and training burden. These models demonstrate remarkable performance. However, before these
models can be implemented and used in routine clinical care, they need to be rigorously evaluated for their
validity, functionality, viability, safety, and ethical use. We have also proposed a checklist that non-AI
experts can use to assess the suitability of NLP models for their clinical needs. This checklist aptly discusses
key areas of algorithm training, tuning, transparency and interpretability, bias, fairness, as well as legal

14


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

and ethical concerns. Overall, these novel NLP techniques can enable the creation of more advanced AI
models, which can improve patient outcomes and expedite the progress of precision medicine in radiation
oncology, under appropriate ethical and technical constraints.

REFERENCES

1 .Bohr A, Memarzadeh K. The rise of artificial intelligence in healthcare applications (2020), 25-60.
doi:10.1016/B978-0- 12-818438-7.00002-2.

2 Johnson KB, Wei WQ, Weeraratne D, Frisse ME, Misulis K, Rhee K, et al. Precision medicine,
ai, and the future of personalized health care. Clinical and translational science 14 (2021) 86-93.
doi:10.1111/cts.12884.

3 .Malebary S, Khan Y. Evaluating machine learning methodologies for identification of cancer driver
genes. Scientific Reports 11 (2021) 12281. doi:10.1038/s41598-021-91656-8.

4 Eghbali N, Alhanai T, Ghassemi M. Patient-specific sedation management via deep reinforcement
learning. Frontiers in Digital Health 3 (2021). doi:10.3389/fdgth.202 1.608893.

5 [Dataset] Nwagwu W. The rise and rise of natural language processing research, 1958-2021 (2022).
doi:10.21203/rs.3.rs-2265814/v1.

6 .[Dataset] Thompson NC, Greenewald K, Lee K, Manso GF. The computational limits of deep learning
(2022).

7 .Hirschberg J, Manning CD. Advances in natural language processing. Science 349 (2015) 261-266.
doi:10.1126/science.aaa8685.

8 .[Dataset] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all
you need (2017).

9 [Dataset] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional
transformers for language understanding (2019).

10 [Dataset] OpenAI. Gpt-4 technical report (2023).

11 .[Dataset] OpenAI. ChatGPT: OpenAI’s Language Model. https://openai.com/research/
chatgpt (2023).

12 .[Dataset] Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al. A survey of large language models
(2023).

13 .[Dataset] Wang L, Lyu C, Ji T, Zhang Z, Yu D, Shi S, et al. Document-level machine translation with
large language models (2023).

14 [Dataset] Susnjak T. Applying bert and chatgpt for sentiment analysis of lyme disease in scientific
literature (2023).

15 .[Dataset] Alyafeai Z, AlShaibani MS, Ahmad I. A survey on transfer learning in natural language
processing (2020).

16 .[Dataset] Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero-shot
reasoners (2023).

17 .Ahad A, TALPUR M, Jumani A. Natural language processing challenges and issues: A literature
review. GAZI UNIVERSITY JOURNAL OF SCIENCE 36 (2023). doi:10.35378/gujs. 1032517.

18 Bhatt S, Jain R, Dandapat S, Sitaram S. A case study of efficacy and challenges in practical human-in-
loop evaluation of NLP systems using checklist. Proceedings of the Workshop on Human Evaluation
of NLP Systems (HumEval) (Online: Association for Computational Linguistics) (2021), 120-130.

19 .Yim WW, Yetisgen M, Harris W, Kwan S. Natural language processing in oncology: A review. JAMA
oncology 2 (2016). doi:10.1001/jamaoncol.2016.0213.

15


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

.Savova G, Danciu I, Alamudun F, Miller T, Lin C, Bitterman D, et al. Use of natural language

processing to extract clinical cancer phenotypes from electronic medical records. Cancer Research 79
(2019) canres.0579.2019. doi:10.1158/0008-5472.CAN- 19-0579.

-Bitterman DS, Miller TA, Mak RH, Savova GK. Clinical natural language processing for radiation

oncology: A review and practical primer. International journal of radiation oncology, biology, physics
110 (2021) 641-655. doi:10.1016/j.ijrobp.2021.01.044.

.Kehl KL, Xu W, Lepisto E, Elmarakeby H, Hassett MJ, Van Allen EM, et al. Natural language

processing to ascertain cancer outcomes from medical oncologist notes. JCO Clinical Cancer
Informatics (2020) 680-690. doi:10.1200/CCI.20.00020. PMID: 32755459.

Santoro M, Strolin S, Paolani G, Della Gala G, Bartoloni A, Giacometti C, et al. Recent applications

of artificial intelligence in radiotherapy: Where we are and beyond. Applied Sciences 12 (2022) 3223.
doi:10.3390/app 12073223.

.Netherton TJ, Cardenas CE, Rhee DJ, Court LE, Beadle BM. The emergence of artificial intelligence

within radiation oncology treatment planning. Oncology 99 (2021) 124-134. doi:10.1159/000512172.

.Wahid K, Glerean E, Sahlsten J, Jaskari J, Kaski K, Naser M, et al. Artificial intelligence for radiation

oncology applications using public datasets. Seminars in Radiation Oncology 32 (2022) 400-414.
doi:10.1016/j.semradonc.2022.06.009.

-Huynh E, Hosny A, Guthier C, Bitterman DS, Petit SF, Haas-Kogan DA, et al. Artificial intelligence

in radiation oncology. Nature reviews. Clinical oncology 17 (2020) 771-781. doi:10.1038/
s41571-020-0417-8.

Parkinson C, Matthams C, Foley K, Spezi E. Artificial intelligence in radiation oncology: A review of

its current status and potential application for the radiotherapy workforce. Radiography 27 (2021)
S$63-S68. doi:https://doi.org/10.1016/j.radi.2021.07.012. The Future Role of the Radiographer.

.Mardikoraem M, Wang Z, Pascual N, Woldring D. Generative models for protein sequence modeling:

recent advances and future directions. Briefings in Bioinformatics 24 (2023) bbad358. doi:10.1093/
bib/bbad358.

-Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 9 (1997) 1735-1780.

doi:10.1162/neco.1997.9.8.1735.

-Khanmohammadi R, Mirshafiee MS, Rezaee Jouryabi Y, Mirroshandel SA. Prose2poem: The blessing

of transformers in translating prose to persian poetry. ACM Trans. Asian Low-Resour. Lang. Inf.
Process. 22 (2023). doi:10.1145/3592791.

-Hernandez A, Amigé JM. Attention mechanisms and their applications to complex systems. Entropy

(Basel) 23 (2021) 283. doi:10.3390/e23030283.

.[Dataset] Zhuang B, Wu Q, Shen C, Reid I, van den Hengel A. Parallel attention: A unified framework

for visual object discovery through dialogs and queries (2017).

.[Dataset] Jing K, Xu J. A survey on neural network language models (2019).
.Nguyen G, Dlugolinsky S, Bobak M, Tran V, Lopez Garcia A, Heredia I, et al. Machine learning and

deep learning frameworks and libraries for large-scale data mining: A survey. Artif: Intell. Rev. 52
(2019) 77-124. doi:10.1007/s10462-018-09679-z.

.[Dataset] Fu Z, Lam W, Yu Q, So AMC, Hu S, Liu Z, et al. Decoder-only or encoder-decoder?

interpreting language model as a regularized encoder-decoder (2023).

.[Dataset] Yang J, Jin H, Tang R, Han X, Feng Q, Jiang H, et al. Harnessing the power of Ilms in

practice: A survey on chatgpt and beyond (2023).

16


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

ao

54

.Khanmohammadi R, Mirshafiee MS, Allahyari M. Coper: a query-adaptable semantics-based search

engine for persian covid-19 articles. 2021 7th International Conference on Web Research (ICWR)
(2021), 64-70. doi:10.1109/ICWRS5S1868.2021.9443151.

.[Dataset] Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of

transfer learning with a unified text-to-text transformer (2020).

.[Dataset] Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, et al. Bart: Denoising

sequence-to-sequence pre-training for natural language generation, translation, and comprehension
(2019).

.Gupta N. A pre-trained vs fine-tuning methodology in transfer learning. Journal of Physics:

Conference Series 1947 (2021) 012028. doi:10.1088/1742-6596/1947/1/012028.

-Ramdan A, Heryana A, Arisal A, Kusumo RBS, Pardede HF. Transfer learning and fine-tuning

for deep learning-based tea diseases detection on small datasets. 2020 International Conference on
Radar, Antenna, Microwave, Electronics, and Telecommunications (ICRAMET) (2020), 206-211.
doi: 10.1 109/ICRAMETS51080.2020.9298575.

.[Dataset] Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Huggingface’s

transformers: State-of-the-art natural language processing (2020).

.[Dataset] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: Large-scale

machine learning on heterogeneous systems (2015). Software available from tensorflow.org.

Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode

clinical knowledge. Nature 620 (2023) 172-180. doi:10.1038/s41586-023-06291-2.

.[Dataset] Singhal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Hou L, et al. Towards expert-level

medical question answering with large language models (2023).

.[Dataset] Tu T, Azizi S, Driess D, Schaekermann M, Amin M, Chang PC, et al. Towards generalist

biomedical ai (2023).

.Wu H, Wang M, Wu J, Francis F, Chang YH, Shavick A, et al. A survey on clinical natural

language processing in the united kingdom from 2007 to 2022. npj Digital Medicine 5 (2022) 186.
doi:10.1038/s41746-022-00730-6.

.Wang L, Luo L, Wang Y, Wampfler J, Yang P, Liu H. Information extraction for populating lung

cancer clinical research data (2019), vol. 2019, 1-2. doi:10.1109/ICHI.2019.8904601.

.American association of physicists in medicine task group 263: Standardizing nomenclatures in

radiation oncology. International Journal of Radiation Oncology*Biology*Physics 100 (2018)
1057-1066. doi:https://doi.org/10.1016/).ijrobp.2017.12.013.

Syed K, Sleeman W, Ivey K, Hagan M, Palta J, Kapoor R, et al. Integrated natural language processing

and machine learning models for standardizing radiotherapy structure names. Healthcare 8 (2020).
doi: 10.3390/healthcare8020120.

.Walker G, Soysal E, Qi W. Development of a natural language processing tool to extract radiation

treatment sites. Cureus 11 (2019). doi:10.7759/cureus.6010.

-Hong J, Fairchild A, Tanksley J, Palta M, Tenenbaum J. Natural language processing for abstraction

of cancer treatment toxicities: accuracy versus human experts. JAMIA Open 3 (2020). doi:10.1093/
jamiaopen/ooaa064.

-Bitterman D, Miller T, Harris D, Lin C, Finan S, Warner J, et al. Extracting radiotherapy treatment

details using neural network-based natural language processing. International Journal of Radiation
Oncology*Biology*Physics 108 (2020) e771-e772. doi:10.1016/j.1jrobp.2020.07.219.

.Bitterman D, Miller T, Harris D, Lin C, Finan S, Warner J, et al. Extracting relations between

radiotherapy treatment details. Proceedings of the 3rd Clinical Natural Language Processing

17


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

Workshop (Online: Association for Computational Linguistics) (2020), 194-200. doi:10.18653/
v1/2020.clinicalnIp-1.21.

.[Dataset] National Cancer Institute. Surveillance, Epidemiology, and End Results Program (SEER).
[Online] (Year). Accessed May 31, 2023.

.[Dataset] American College of Surgeons. National Cancer Database. [Online] (2020). Accessed May
31, 2023.

.[Dataset] for Radiation Oncology (ASTRO) AS. New registry launched to track and improve the
quality of cancer care delivered in the u.s. [Online] (2020). Accessed May 31, 2023.

-Rahman R, Fell G, Ventz S, Arfe A, Vanderbeek A, Trippa L, et al. Deviation from the proportional

hazards assumption in randomized phase 3 clinical trials in oncology: Prevalence, associated factors
and implications. Clinical Cancer Research 25 (2019) clincanres.3999.2018. doi:10.1158/1078-0432.
CCR- 18-3999.

.Oztiirk H, Ozgiir A, Schwaller P, Laino T, Ozkirimli E. Exploring chemical space using natural
language processing methodologies for drug discovery. Drug Discovery Today 25 (2020) 689-705.
doi:https://doi.org/10.1016/j.drudis.2020.01.020.

.Chuang YN, Tang R, Jiang X, Hu X. Spec: A soft prompt-based calibration on mitigating performance
variability in clinical notes summarization. arXiv preprint arXiv:2303.13035 (2023).

[Dataset] Liu Z, Yu X, Zhang L, Wu Z, Cao C, Dai H, et al. Deid-gpt: Zero-shot medical text
de-identification by gpt-4 (2023).

.[Dataset] Tang R, Han X, Jiang X, Hu X. Does synthetic data generation of Ilms help clinical text

mining? (2023).

Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How does chatgpt perform
on the united states medical licensing examination? the implications of large language models for
medical education and knowledge assessment. JMIR Medical Education 9 (2023) e45312.

.Patel S, Lam K. ChatGPT: the future of discharge summaries? The Lancet Digital Health 5 (2023)

e107-e108. doi:10.1016/S2589-7500(23)00021-3. Publisher: Elsevier Ltd.

.[Dataset] Dziri N, Milton S, Yu M, Zaiane O, Reddy S. On the origin of hallucinations in conversational

models: Is it the datasets or the models? (2022).

Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, et al. Survey of hallucination in natural language generation.

ACM Comput. Surv. 55 (2023). doi:10.1145/3571730.

.Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the feasibility of chatgpt in healthcare:

An analysis of multiple clinical and research scenarios. Journal of Medical Systems 47 (2023) 1-5.

.[Dataset] Liang PP, Wu C, Morency LP, Salakhutdinov R. Towards understanding and mitigating

social biases in language models (2021).

.Nadeem M, Bethke A, Reddy S. StereoSet: Measuring stereotypical bias in pretrained language models.

Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (Online:
Association for Computational Linguistics) (2021), 5356-5371. doi:10.18653/v 1/2021 .acl-long.416.

Straw I, Callison-Burch C. Artificial intelligence in mental health and the biases of language based

models. PloS one 15 (2020) e0240376.

.Pennington J, Socher R, Manning C. GloVe: Global vectors for word representation. Proceedings of

the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Doha, Qatar:
Association for Computational Linguistics) (2014), 1532-1543. doi:10.3115/v1/D14-1162.

.[Dataset] Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in

vector space (2013).

18


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

73 [Dataset] Welbl J, Glaese A, Uesato J, Dathathri S, Mellor J, Hendricks LA, et al. Challenges in
detoxifying language models (2021).

74 [Dataset] Park YA, Rudzicz F. Detoxifying language models with a toxic corpus (2022).

75 .Rodrigues R. Legal and human rights issues of ai: Gaps, challenges and vulnerabilities. Journal of
Responsible Technology 4 (2020) 100005. doi:https://doi.org/10.1016/j.jrt.2020. 100005.

76 .Bender EM, Gebru T, McMillan-Major A, Shmitchell S. On the dangers of stochastic parrots: Can
language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability,
and Transparency (New York, NY, USA: Association for Computing Machinery) (2021), FAccT ’21,
610-623. doi:10.1145/3442188.3445922.

77 .Steyerberg EW, Harrell FE. Prediction models need appropriate internal, internal-external, and
external validation. Journal of clinical epidemiology 69 (2016) 245-7.

78 .Wainer J, Cawley G. Nested cross-validation when selecting classifiers is overzealous for most
practical applications. Expert Systems with Applications 182 (2021) 115222. doi:https://doi.org/10.
1016/j.eswa.2021.115222.

79 .Patwardhan N, Marrone S, Sansone C. Transformers in the real world: A survey on nlp applications.
Information 14 (2023). doi:10.3390/info 14040242.

80 .Lin CY. ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches
Out (Barcelona, Spain: Association for Computational Linguistics) (2004), 74-81.

$1 .Papineni K, Roukos S, Ward T, Zhu WJ. Bleu: a method for automatic evaluation of machine
translation. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics
(Philadelphia, Pennsylvania, USA: Association for Computational Linguistics) (2002), 311-318.
doi:10.3115/1073083.1073135.

82 .Vokinger K, Feuerriegel S, Kesselheim A. Mitigating bias in machine learning for medicine.
Communications Medicine 1 (2021). doi:10.1038/s43856-021-00028-w.

83 .Montesinos Lé6pez OA, Montesinos Lépez A, Crossa J. Overfitting, model tuning, and evaluation of
prediction performance. Multivariate statistical machine learning methods for genomic prediction
(Springer) (2022), 109-139.

84 .Norori N, Hu Q, Aellen FM, Faraci FD, Tzovara A. Addressing bias in big data and ai for health care:
A call for open science. Patterns 2 (2021) 100347. doi:https://doi.org/10.1016/j.patter.2021.100347.

85 .[Dataset] Mehrabi N, Morstatter F, Saxena N, Lerman K, Galstyan A. A survey on bias and fairness
in machine learning (2022).

86 .Olteanu A, Castillo C, Diaz F, Kiciman E. Social data: Biases, methodological pitfalls, and ethical
boundaries. SSRN Electronic Journal (2016). doi:10.2139/ssrn.2886526.

87 .El Naga I, Li H, Fuhrman J, Hu Q, Gorre N, Chen W, et al. Lessons learned in transitioning to
ai in the medical imaging of covid-19. Journal of Medical Imaging (Bellingham, Wash.) 8 (2021)
010902-—10902. doi:10.1117/1.JMI.8.S1.010902. T32 EB002103/EB/NIBIB NIH HHS/United States.

88 .Panch T, Mattie H, Atun R. Artificial intelligence and algorithmic bias: Implications for health
systems. Journal of Global Health 9 (2019). doi:10.7189/jogh.09.020318.

89 McCradden M, Joshi S, Mazwi M, Anderson J. Ethical limitations of algorithmic fairness solutions
in health care machine learning. The Lancet Digital Health 2 (2020) e221-—e223. doi:10.1016/
$2589-7500(20)30065-0.

90 .[Dataset] Kim B, Wattenberg M, Gilmer J, Cai C, Wexler J, Viegas F, et al. Interpretability beyond
feature attribution: Quantitative testing with concept activation vectors (tcav) (2018).

91 [Dataset] Pymetrics. audit-ai. https://github.com/pymetrics/audit—ai (2020).

19


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

92

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

Speicher T, Heidari H, Grgic-Hlaca N, Gummadi KP, Singla A, Weller A, et al. A unified approach

to quantifying algorithmic unfairness: Measuring individual &amp;group unfairness via inequality
indices. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
&amp; Data Mining (New York, NY, USA: Association for Computing Machinery) (2018), KDD 718,
2239-2248. doi:10.1145/3219819.3220046.

.Zemel RS, Wu LY, Swersky K, Pitassi T, Dwork C. Learning fair representations. International

Conference on Machine Learning (2013).

-Mongan J, Moy L, Kahn CEJ. Checklist for artificial intelligence in medical imaging (claim): A

guide for authors and reviewers. Radiology. Artificial intelligence 2 (2020) e200029. doi:10.1148/
ryai.2020200029.

-Liu X, Cruz Rivera S, Moher D, Calvert MJ, Denniston AK. Reporting guidelines for clinical trial

reports for interventions involving artificial intelligence: the consort-ai extension. Nature medicine 26
(2020) 1364-1374. doi: 10.1038/s41591-020-1034-x.

Soysal E, Wang J, Jiang M, Wu Y, Pakhomov S, Liu H, et al. CLAMP - a toolkit for efficiently

building customized clinical natural language processing pipelines. Journal of the American Medical
Informatics Association : JAMIA 25 (2018) 331-336. doi:10.1093/jamia/ocx132.

-Liu X, Rivera SC, Moher D, Calvert MJ, Denniston AK. Reporting guidelines for clinical trial

reports for interventions involving artificial intelligence: the consort-ai extension. BMJ 370 (2020).
doi:10.1136/bmj.m3 164.

Soysal E, Wang J, Jiang M, Wu Y, Pakhomov S, Liu H, et al. CLAMP — a toolkit for efficiently

building customized clinical natural language processing pipelines. Journal of the American Medical
Informatics Association 25 (2017) 331-336. doi:10.1093/jamia/ocx 132.

.[Dataset] Asan O, Bayrak E, Choudhury A. Artificial intelligence and human trust in healthcare:

Focus on clinicians (preprint) (2019). doi:10.2196/preprints. 15154.

.Varkey B. Principles of clinical ethics and their application to practice. Medical Principles and

Practice 30 (2020). doi:10.1159/000509119.

.Linardatos P, Papastefanopoulos V, Kotsiantis SB. Explainable ai: A review of machine learning

interpretability methods. Entropy 23 (2020).

-Herm LV, Heinrich K, Wanner J, Janiesch C. Stop ordering machine learning algorithms by their

explainability! a user-centered investigation of performance and explainability. International Journal
of Information Management 69 (2023) 102538. doi:10.1016/j.ijinfomgt.2022. 102538.

Shin D. The effects of explainability and causability on perception, trust, and acceptance: Implications

for explainable ai. International Journal of Human-Computer Studies 146 (2021) 102551. doi:https:
//doi.org/10.1016/j.ijhes.2020.102551.

.[Dataset] Carrillo A, Canté LF, Noriega A. Individual explanations in machine learning models: A

survey for practitioners (2021).

.[Dataset] C3ai. LIME (Local Interpretable Model-Agnostic

Explanations). https://c3.ai/glossary/data-science/
lime-local-interpretable-model-agnostic-explanations/ (Accessed 2023).

.[Dataset] Lundberg S, Lee SI. A unified approach to interpreting model predictions (2017).
.[Dataset] Ribeiro MT, Singh S, Guestrin C. ’why should i trust you?”: Explaining the predictions of

any classifier (2016).

.[Dataset] Khanmohammadi R, Mirshafiee MS, Ghassemi MM, Alhanai T. Fetal gender identification

using machine and deep learning algorithms on phonocardiogram signals (2021).

20


Khanmohammadi et al. Framework for assessing NLP use in clinical Radiation Oncology

109 .Goodfellow I, McDaniel P, Papernot N. Making machine learning robust against adversarial inputs.
Communications of the ACM 61 (2018) 56-66. doi:10.1145/3134599.

110 Biggio B, Corona I, Maiorca D, Nelson B, S rndié N, Laskov P, et al. Evasion attacks against machine
learning at test time. Advanced Information Systems Engineering (Springer Berlin Heidelberg) (2013),
387-402. doi:10.1007/978-3-642-40994-3_25.

111 .[Dataset] Borkar J, Chen PY. Simple transparent adversarial examples (2021).

112 .Drabiak K, Kyzer S, Nemov V, El Naga I. Ai and ml ethics, law, diversity, and global impact. The
British journal of radiology (2023) 20220934. doi:10.1259/bjr.20220934.

113 .[Dataset] US Food and Drug Administration. Artificial intelligence and machine learning discussion
paper. [Online] (2021). Accessed: May 30, 2023.

114 [Dataset] _US Food and Drug Administration. Artificial Intelligence
and Machine Learning Software as a Medical Device. https://
www.fda.gov/medical-—devices/software-medical-—device-samd/
artificial-—intelligence-and-machine-learning-software-medical-—device
(2021). Accessed: May 30, 2023.

115 [Dataset] US Food and Drug’ Administration. Good Machine Learning
Practice for Medical Device Development: Guiding Principles. https://

www.fda.gov/medical-—devices/software-medical-—device-samd/

good-machine-learning-practice-medical-device-development-guiding-princif
(2021). Accessed: May 30, 2023.

116 .Gilbert S, Harvey H, Melvin T, Vollebregt E, Wicks P. Large language model ai chatbots
require approval as medical devices. Nature Medicine 29 (2023) 2396-2398. doi:10.1038/
$41591-023-02412-6.

117 .Naik N, Hameed B, Shetty D, Swain D, Shah M, Paul R, et al. Legal and ethical consideration in
artificial intelligence in healthcare: Who takes responsibility? Frontiers in Surgery 9 (2022) 862322.
doi:10.3389/fsurg.2022.862322.

118 Smith H. Clinical ai: opacity, accountability, responsibility and liability. AJ & SOCIETY 36 (2021).
doi:10.1007/s00146-020-01019-6.

119 Smith H, Fotheringham K. Artificial intelligence in clinical decision-making: Rethinking liability.
Medical Law International 20 (2020) 131-154. doi:10.1177/0968533220945766.

120 .Gerke S, Minssen T, Cohen G. Ethical and legal challenges of artificial intelligence-driven healthcare
(United States: Elsevier Inc.) (2020), 295-336. doi:10.1016/B978-0- 12-818438-7.00012-5.

121 .Amurao M, Gress D, Keenan M, Halvorsen P, Nye J, Mahesh M. Quality management, quality
assurance, and quality control in medical physics. Journal of Applied Clinical Medical Physics 24
(2023). doi:10.1002/acm2.13885.

122 .Vandewinckele L, Claessens M, Dinkla A, Brouwer C, Crijns W, Verellen D, et al. Overview of
artificial intelligence-based applications in radiotherapy: Recommendations for implementation and
quality assurance. Radiotherapy and Oncology 153 (2020) 55-66. doi:https://doi.org/10.1016/j.
radonc.2020.09.008. Physics Special Issue: ESTRO Physics Research Workshops on Science in
Development.

21
