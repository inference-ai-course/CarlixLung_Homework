arX1v:2505.23801vl [cs.CL] 26 May 2025

SEMFED: Semantic-Aware Resource-Efficient Federated
Learning for Heterogeneous NLP Tasks

Sajid Hussain? Muhammad Sohail, Nauman Ali Khan

Department of Computer Science
Military College of Signals (MCS)
National University of Sciences and Technology (NUST)
Islamabad, Pakistan

{shussain.phdcse23mcs, muhammad.sohail, nauman.ali}@student .nust.edu.pk

June 7, 2025

Abstract

Background: Federated Learning (FL) has emerged as a promising paradigm for training
machine learning models while preserving data privacy. However, applying FL to Natural Lan-
guage Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across
clients, vocabulary mismatches, and varying resource constraints on edge devices.

Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient fed-
erated learning framework specifically designed for heterogeneous NLP tasks.

Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selec-
tion mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-
specific model architectures tailored to device capabilities while preserving semantic information,
and (3) a communication-efficient semantic feature compression technique that significantly re-
duces bandwidth requirements.

Results: Experimental results on various NLP classification tasks demonstrate that SEMFED
achieves an 80.5% reduction in communication costs while maintaining model accuracy above
98%, outperforming state-of-the-art FL approaches.

Conclusion: SEMFED effectively manages heterogeneous client environments with vary-
ing computational resources, network reliability, and semantic data distributions, making it
particularly suitable for real-world federated NLP deployments.

Keywords: federated learning, natural language processing, semantic preservation, resource eff-
ciency, heterogeneous devices, communication efficiency

1 Introduction

The increasing concerns about data privacy and regulatory requirements have propelled Federated
Learning (FL) as an essential paradigm for distributed machine learning across edge devices without
sharing raw data [1,2]. In FL, clients train models locally on their private data, and only model
updates are sent to a central server for aggregation, thus preserving data privacy. While FL has

*Corresponding author: shussain.phdcse23mcs@student.nust.edu.pk


shown promising results in various domains, applying it to Natural Language Processing (NLP)
tasks presents unique challenges that remain inadequately addressed in current literature.

Unlike computer vision tasks where visual features have relatively consistent interpretations
across clients, NLP tasks suffer from semantic heterogeneity—variations in vocabulary usage, writ-
ing styles, and semantic interpretations across different clients [3,4]. This semantic heterogeneity,
coupled with the resource constraints of edge devices like smartphones and IoT devices, creates a
complex optimization landscape that traditional FL approaches fail to navigate effectively.

A critical challenge in the FL domain for NLP is that most existing approaches rely on de-
ploying huge-sized models on the server side and attempting to train these models on client data.
This approach creates significant problems for resource-constrained devices, including overwhelming
computational demands on small devices, excessive energy consumption, high-latency communica-
tion round trips, and degraded performance. These issues become particularly acute when clients
have heterogeneous hardware capabilities and operate under varying network conditions, making
conventional FL approaches impractical for real-world NLP deployments on edge devices.

Current FL frameworks for NLP primarily focus on either communication efficiency [5,6] or
model accuracy [7,8], often neglecting the critical interplay between semantic preservation and
resource constraints. Standard techniques like FedAvg |1] and FedProx [8] typically employ ho-
mogeneous models across all clients, disregarding the diverse computational capabilities of edge
devices. Furthermore, these approaches overlook the semantic relationships between clients’ data
distributions, leading to suboptimal client selection and model convergence.

In this paper, we present SEMFED, a novel Semantic-aware Resource-Efficient Federated Learn-
ing framework for heterogeneous NLP tasks. SEMFED addresses the limitations of existing ap-
proaches through three key innovations:

1. Semantic-Aware Client Selection: A novel client selection mechanism that balances se-
mantic diversity with resource constraints to optimize both model convergence and system
efficiency.

2. Heterogeneous NLP Client Models: Adaptive model architectures tailored to device ca-
pabilities while preserving semantic information through specialized embedding and attention
mechanisms.

3. Communication-Efficient Semantic Feature Compression: A semantic-preserving fea-
ture distillation technique that significantly reduces communication costs while maintaining
model performance.

Our extensive experimental evaluation on multiple NLP classification tasks demonstrates that
SEMFED outperforms state-of-the-art FL approaches in terms of communication efficiency (80.5%
reduction), model accuracy (achieving 98%-+ accuracy), and resource utilization (effectively man-
aging heterogeneous device constraints). The main contributions of this work can be summarized
as follows:

e We identify and formalize the unique challenges of federated learning for NLP tasks in hetero-
geneous environments, particularly focusing on the interplay between semantic preservation
and resource constraints.

e We propose SEMFED, a novel FL framework that incorporates semantic-aware client selec-
tion, heterogeneous NLP model architectures, and communication-efficient feature compres-
sion specifically designed for NLP tasks.


e We introduce a novel semantic-preserving embedding layer that enhances representation learn-
ing in resource-constrained federated settings.

e We develop a theoretical foundation for balancing semantic diversity and resource efficiency
in federated NLP systems.

e We conduct comprehensive experiments demonstrating SEMFED’s superior performance across
multiple metrics compared to state-of-the-art FL approaches.

2 Related Work

2.1 Federated Learning

Federated Learning was first introduced by McMahan et al. [1] with the FedAvg algorithm, which
performs model averaging across multiple clients’ updates. Subsequent works have addressed various
challenges in FL, including communication efficiency [5,6], statistical heterogeneity [8,11], and
systems heterogeneity [9, 10].

Communication efficiency has been a primary focus in FL research due to the bandwidth lim-
itations of edge devices. Approaches such as gradient compression [13], model pruning [14], and
quantization [15] have been proposed to reduce communication costs. However, these techniques
often fail to preserve the semantic relationships crucial for NLP tasks.

Resource constraints on edge devices present another significant challenge for FL systems. Het-
eroFL [10] addresses device heterogeneity by training different-sized models on different devices.
Similarly, FedNova [12] normalizes local updates to account for varying computational capabilities.
However, these approaches are not specifically designed for NLP tasks and do not consider the
semantic aspects of the data.

2.2 Federated Learning for NLP

Applying FL to NLP tasks introduces unique challenges due to the discrete nature of text data
and vocabulary mismatches across clients. FedNLP [3] provides a benchmark for evaluating FL
algorithms on various NLP tasks but does not address the semantic heterogeneity across clients.
Zhu et al. [4] propose federated pre-training of language models but focus primarily on model
accuracy rather than resource efficiency.

A significant limitation in the current FL for NLP landscape is the predominant use of large,
computationally intensive models deployed on the server side. These approaches attempt to train
these massive models using client data, which creates substantial challenges for edge deployment:
small devices struggle with the computational burden, battery life is severely impacted, communica-
tion becomes a bottleneck due to numerous round trips, and overall system performance suffers. Our
approach specifically targets these multi-level challenges by optimizing resource usage at the client
selection, model architecture, and communication protocol levels, as evidenced by our experimental
results.

Recent works have started exploring semantic aspects in federated NLP. Chen et al. [16] propose
FedMatch, which uses consistency regularization to handle label distribution skew in federated text
classification. Liu et al. [17| introduce a framework for personalized federated learning in NLP by
adapting to clients’ local data distributions. However, these approaches do not explicitly address
the resource constraints of edge devices or optimize client selection based on semantic diversity.


2.3. Resource-Efficient NLP Models

Developing resource-efficient NLP models has gained significant attention with the increasing de-
ployment of language models on edge devices. MobileBERT [18] and DistilBERT [19] reduce the
computational requirements of BERT through knowledge distillation. TinyBERT [20] further com-
presses BERT models through a two-stage learning framework. While these approaches focus on
model compression for individual devices, they do not address the federated learning setting or the
semantic heterogeneity across clients.

2.4 Feature Distillation in Federated Learning

Feature distillation has emerged as a promising approach for reducing communication costs in FL.
FedMD [21] uses knowledge distillation to transfer knowledge from local models to a global model.
Similarly, FedDF [22] applies knowledge distillation to improve model performance in federated set-
tings. However, these approaches do not specifically address the semantic preservation requirements
of NLP tasks or the resource constraints of edge devices.

Our work, SEMFED, bridges these research gaps by introducing a comprehensive framework that
simultaneously addresses semantic heterogeneity, resource constraints, and communication efficiency
in federated NLP tasks.

3 Problem Formulation

3.1 Preliminary

We consider a federated learning system with K clients, each with its local dataset Dy = {(a*, y*)}",,

where EF represents text data and ye represents the corresponding labels. The clients participate
in the federated learning process under the coordination of a central server.

Unlike standard FL settings, we explicitly model the semantic heterogeneity across clients and
the resource constraints of edge devices. Each client has a semantic profile Sy = {V;,Cr, Tx}, where
Y; represents the vocabulary statistics, C, represents the class distribution, and 7, represents the
sequence length statistics.

Additionally, each client has a resource profile Ry = {Mz, Px, Bye,Nz}, where M;, represents
the memory capacity, P, represents the computational capacity, B, represents the battery level,
and NV; represents the network reliability.

3.2 Semantic Heterogeneity

Semantic heterogeneity in federated NLP can be characterized by differences in vocabulary usage,
class distributions, and sequence length patterns across clients. We quantify the semantic similar-
ity between clients using the Jensen-Shannon divergence between their class distributions and the
Jaccard similarity between their vocabularies:

siMgemantic(k, j) = a+ (1 — JS(Cz,C;)) + (1 — a) - Jaccard (Vz, V;) (1)

where JS(Cx,C;) is the Jensen-Shannon divergence between the class distributions of clients k
and j, Jaccard(V;, V;) is the Jaccard similarity between their vocabularies, and a is a weighting
parameter.


3.3. Resource Constraints

Resource constraints in federated NLP systems can significantly impact the participation and per-
formance of clients. We model the resource efficiency of a client as:

M k Pr By
3 3 | . y)
M «max + ae Pax WB Bmax aN Ne ( )

where wy, wp, wp, and wy are weighting parameters for memory, computational capacity,

battery, and network reliability, respectively. Minax, Pmax, and Bmax are the maximum values
across all clients.

effesource (k) =WM*

3.4 Objective

Our objective is to develop a federated learning framework that maximizes model performance while
minimizing communication costs and respecting resource constraints. Formally, we aim to solve:

max P(6;Dtest)
0,S

s.t. C(S) < Cmax (3)
Rel) = Rie, VES

where 0 represents the model parameters, S is the set of selected clients, P(0; Dtest) is the model
performance on the test dataset, C(S) is the communication cost, Cmax is the maximum allowed
communication cost, Ry(S,) is the resource consumption of client k, and Rx max is its maximum
resource capacity.

4 SEMFED: Semantic-Aware Resource-Efficient Federated Learn-
ing

We now present SEMFED, our proposed framework for semantic-aware resource-efficient federated
learning for NLP tasks. SEMFED consists of three main components: (1) Semantic-Aware Client
Selection, (2) Heterogeneous NLP Client Models, and (3) Communication-Efficient Semantic Feature
Compression. Figure 1 illustrates the overall architecture of SEMFED.

4.1 Semantic-Aware Client Selection

Client selection in federated learning significantly impacts both model convergence and system
efficiency. Traditional approaches select clients randomly or based on resource availability [23],
neglecting the semantic diversity across clients’ data distributions. In contrast, SEMFED intro-
duces a semantic-aware client selection mechanism that balances semantic diversity with resource
constraints.

For each communication round t, we select a subset of clients S; C {1,2,..., A} to participate
in the training process. The selection is based on a utility score that combines semantic diversity,
resource efficiency, and fairness in participation:

utility(k, t) =A, - Ci Vrcmmnbiel St) + Xr2° Blrecouncel it) + 3° Tait packivinabion (Bs t) (4)

where A;, A2, and A3 are weighting parameters for semantic diversity, resource efficiency, and
participation fairness, respectively.


Client Layer
Path 1: Mobile — Small CNN
(Fone)

1 Client 1
2 Client 2

ae

Path 2: Laptop — Tiny LSTM
Laptop Clients
srw) [Features]

1 Client 1
2 Client 2

Path 3: Desktop — MobileBERT

Desktop Clients

1 Client 1
2 Client 2

Heterogeneous NLP Tasks in Federated Environments

Semantic Embs

Semantic Embs (Path 1)

Embedding Dimension: 64
pin=sa](_H=22 Jc ]

s_i.c = softmax(w_i*T q_c)

Semantic Embs (Path 2)

Embedding Dimension: 100

Feature Comprs

Feature Comprs (Path 1)

Sparse Coding Compression
min Ii - Os. + Nis. ill:
Communication Cost Monitor

Feature Comprs (Path 2)

Semantic PCA

Piretog [Hes }[_c-2 J

fisPeTtivw |

s_ic = softmax(w_i*T q_c)

Semantic Embs (Path 3)

Embedding Dimension: 128

fired (ore) [o=0)

s_i.c = softmax(w_i*T q_c)

Communication Cost Monitor

Feature Comprs (Path 3)

a
Communication Cost Monitor

Semantic Aggr

Semantic Aggr (Path 1)

Transformer-based Integration

Semantic Aggr (Path 2)

SEMFED: A Semantic-Aware Resource-Efficient Architecture for

Server Models

Small CNN Server Model

Model Architecture:

Multi-scale CNN (~0.5M params)

Tiny LSTM Server Model

Semantic Alignment Layer

Transformer-based Integration

Semantic Aggr (Path 3)

Model Architecture:

MobileBERT Server Model

Semantic Alignment Layer

‘Transformer-based Integration

Model Architecture:
(Trenstormer-tke (15M parems)_]

Training Iterations

Legend

{)Compression [—] Aggregation [—] Server

Component Types: { | Client [_] Semantic

Flow Types: —_ DataFlow ---
Abbreviations: Embs = Embedding

Training Iteration

Comprs = Compression Agar = Aggregation

Figure 1: Overview of the SEMFED framework. The system incorporates semantic-aware client
selection based on vocabulary diversity and resource constraints, heterogeneous model architectures
tailored to device capabilities, and communication-efficient semantic feature compression.

The semantic diversity term divsemantic(k, S¢) measures how different client k’s semantic profile
is from the already selected clients:

diveemantie( &,S:) =L- Ss" siMigemantic( My] )

Se JES

(5)

The resource efficiency term effresource(K) is calculated as described in Section 3.3.
The participation fairness term fairparticipation(K, t) encourages equal participation across clients
over time:

‘i
Matic S;) 6)

where 1(& € S;) is an indicator function that equals 1 if client & was selected in round i, and 0
otherwise.

lait aadiaanionl, t)=1

4.2 Heterogeneous NLP Client Models

Traditional FL approaches typically employ homogeneous models across all clients, disregarding the
diverse computational capabilities of edge devices. SEMFED addresses this limitation by introduc-
ing heterogeneous NLP model architectures tailored to device capabilities while preserving semantic
information.


Algorithm 1 SEMFED: Semantic-Aware Client Selection

: Initialize S; + @
: Update client connectivity states
: for each available client k do
Calculate divgemantic(k, St)
Calculate effresource(k) based on device capabilities
Calculate fairparticipation (K, ¢)
utility(k, t) = 1° iV gernantial Me, St) + A2° ell resensnee MS) +g ° fein seiteimtan By t)
end for
: Select top m clients based on utility scores: S;
: Optimize bandwidth allocation for selected clients
: Update participation history and resource consumption
: return S;

PRR
oF ©

4.2.1 Semantic-Preserving Embedding Layer

At the core of our heterogeneous models is a novel semantic-preserving embedding layer that en-
hances representation learning in resource-constrained settings. This layer combines standard token
embeddings with semantic cluster embeddings to preserve semantic relationships:

Cc
e; = Wex;i + Ss" Si,ceWe (7)
c=]

where e; is the enhanced embedding for token 7, We is the standard embedding matrix, x; is
the one-hot encoding of token 7, s;,- is the soft assignment of token 7 to semantic cluster c, and W,
is the embedding for semantic cluster c.

The soft cluster assignments s;,. are learned during training:

exp(w/ q) (3)
C T
ee =1 &XP(W; Ae’)

where w; is the embedding for token i and q, is the embedding for semantic cluster c.

Si,ec

4.2.2 Model Architectures

SEMFED employs three different model architectures tailored to device capabilities:

e Small CNN (for mobile devices): A lightweight convolutional network with multi-scale filters
to capture n-grams of different sizes.

e Tiny LSTM (for laptops): A bidirectional LSTM model with a moderate number of param-
eters.

e MobileBERT (for desktops): A simplified transformer architecture based on MobileBERT.

All three architectures share the semantic-preserving embedding layer described above but differ
in their feature extraction mechanisms.


Table 1: Heterogeneous NLP Model Architectures

Characteristics Small CNN Tiny LSTM MobileBERT

Target Device Mobile Laptop Desktop
Parameters ~0.5M ~2M ~15M
Embedding Dim. 64 100 128
Hidden Dim. 32 64 128
Semantic Clusters 5 8 10

Architecture Multi-scale CNN Bi-LSTM Transformer

4.3. Communication-Efficient Semantic Feature Compression

To reduce communication costs while preserving semantic information, SEMFED employs a novel
semantic-aware feature compression technique. Instead of transmitting model parameters, clients
extract and compress semantic-preserving features from their local data and send these compressed
features to the server.

4.3.1 Feature Extraction

Each client k extracts semantic features F, = fer yee from its local data using the penultimate
layer of its local model. These features capture the semantic representation of the text data while
being more compact than the raw text.

4.3.2 Semantic Compression

To further reduce communication costs, we employ two complementary compression techniques:
Sparse Coding: We represent the features as a sparse combination of basis vectors from a
learned semantic dictionary D = [dy, dg,...,dm|:

min lif; — Ds; [3 + Aljsih ©)

where s; is the sparse code for feature f; and \ is a regularization parameter controlling sparsity.

Semantic PCA: We learn client-specific principal component matrices P, that preserve the
most important semantic dimensions of the client’s feature space:

where f; is the compressed feature, P; is the PCA matrix for client k, and ps; is the mean feature
vector.
4.3.3. Semantic-Preserving Quantization

To further reduce the communication cost, we apply a semantic-preserving quantization to the
compressed features:

qi = round((f; — aj) /bx - (2° — 1)) (11)

where q; is the quantized feature, a, and by, are scaling factors, and 6b is the number of bits for
quantization.


4.4 Server-Side Aggregation and Training

On the server side, we employ a semantic-preserving aggregation method that efficiently integrates
the compressed features from heterogeneous clients. The server decompresses the features and trains
a global model using a transformer-based architecture with semantic cluster attention.

4.4.1 Feature Decompression

The server decompresses the features received from clients:

- — Pe if sparse coding was used (12)

Pra; + py, if PCA was used

where s; is obtained by de-quantizing q;.

4.4.2 Semantic-Preserving Server Model
The server model employs a semantic-preserving architecture that captures the relationships between

features from different clients. The model consists of:

e Semantic cluster centers learned via soft k-means clustering
e Semantic alignment layers that map heterogeneous features to a common space
e Transformer-based feature integration with semantic attention

e A classification head for the target task

The semantic alignment process can be formulated as:

Cc
gi= Wf; + Ss" bj cle (13)
eI
where g; is the aligned feature, W, is an alignment matrix, ¢;. is the similarity between feature

f; and semantic cluster center C,.

5 Experimental Results

In this section, we evaluate SEMFED on multiple NLP classification tasks and compare it with
state-of-the-art federated learning approaches.

5.1 Experimental Setup
5.1.1 Datasets

We evaluate SEMFED on synthetic text classification data to ensure controlled evaluation of se-
mantic heterogeneity. The dataset consists of 10000 text classification samples generated with
controlled semantic properties. We partition this dataset into 10 clients with varying degrees of
semantic heterogeneity using a Dirichlet distribution with parameter a = 0.5 to create non-IID
splits. Additionally, we assign different device types to clients: 5 mobile devices, 3 laptops, and 2
desktop computers, each with appropriate resource constraints.


5.1.2 Baselines

We compare SEMFED with the following state-of-the-art federated learning approaches:
e FedAvg |1]: The standard federated averaging algorithm.
e FedProx [8]: Federated optimization with proximal terms to handle statistical heterogeneity.
e FedNLP [3]: A specialized federated learning framework for NLP tasks.
e HeteroFL [10]: A federated learning approach for heterogeneous devices.

e Traditional FL-NLP: A baseline approach using standard FL techniques with NLP models
but without semantic awareness or resource optimization.

e Resource-Only FL: A variant of SEMFED that only considers resource constraints without
semantic awareness.

5.1.3. Implementation Details

We implement SEMFED using TensorFlow 2.x. For the semantic-preserving embedding layer, we use
embedding dimensions of 64, 100, and 128 for Small CNN, Tiny LSTM, and MobileBERT models,
respectively. The feature dimension for all models is set to 128, and the number of semantic clusters
is set to 5 for mobile devices, 8 for laptops, and 10 for desktops. We use a batch size of 32 and
train for 20 communication rounds. For client selection, we set Ay = 0.4, Ag = 0.8, and A3 = 0.3 to
balance semantic diversity, resource efficiency, and fairness. For feature compression, we use 8-bit
quantization and a compression ratio of 0.4.

5.2. Experimental Results
5.3 Experimental Results
5.3.1 Client Vocabulary Diversity and Semantic Preservation

The significant variation in vocabulary sizes (ranging from approximately 4,600 to 7,500 tokens)
shown in Figure 2(a) highlights the semantic heterogeneity that SEMFED addresses. We observe
that mobile devices (red circles) generally have more diverse vocabulary patterns compared to
laptops (green squares) and desktops (blue diamonds). The semantic cluster centers (red stars)
in Figure 2(b) are positioned to capture the semantic relationships in the data, enabling effective
knowledge sharing across heterogeneous clients. SEMFED’s ability to identify and preserve these
semantic structures is crucial for its superior performance.

5.3.2 Semantic Similarity Network

Figure 3 visualizes the semantic relationships between clients as a network. The highly connected
structure indicates semantic overlap between clients despite their heterogeneity. SEMFED leverages
these relationships to improve federated learning by selecting clients that provide complementary
semantic information.

Figure 4 further illustrates SEMFED’s semantic preservation capabilities through a 2D projec-
tion of the feature space colored by class. The clear separation between classes indicates effective
semantic preservation despite the heterogeneous client environments and feature compression.

10


Semantic Cluster Centers in Feature Space

ke Semantic Centers

Client Vocabulary Diversity ¢ 7

= o 6 a: f
@ mobil 8
laptop a
7000 1 a @ desktop S os
e : a
woo te
a e 3 2 8 00
2 6000 CI . C 9
e
° 00 2.5
e
bi 0 2 4 6 8 15 5.0 -25 0.0 25 7.5 10.0 12.5
Client 1D Principal Component 1
(a) Client Vocabulary Diversity (b) Semantic Clusters

Figure 2: Semantic diversity and preservation in SEMFED. (a) Client Vocabulary Diversity. The
scatter plot shows vocabulary sizes across client devices, with mobile devices (red), laptops (green),
and desktops (blue). The heterogeneity in vocabulary size (ranging from 4,600 to 7,500 tokens)
demonstrates the semantic diversity that SEMFED addresses. (b) Semantic Cluster Centers in
Feature Space. This visualization shows the 2D projection of learned semantic cluster centers (red
stars) positioned to capture semantic relationships in the feature space.

5.3.3 Resource Efficiency

SEMFED effectively manages energy consumption, particularly for resource-constrained mobile de-
vices, by adapting model architectures and optimizing client selection. As shown in Figure 5(a),
Client 1 (mobile) shows the highest consumption at 4.58 units, while Client 4 (laptop) has the lowest
at 1.36 units. The compute times analysis in Figure 5(b) demonstrates a significant drop after the
first round (from approximately 13 seconds to less than 1 second), indicating SEMFED’s efficient
resource management and optimization strategies. The average compute time (red line) stabilizes
at 0.67 seconds, enabling responsive performance even on resource-constrained devices.

5.3.4 Communication Efficiency

As shown in Figure 6(a), SEMFED achieves significant communication savings across all rounds.
The blue bars represent the original data size, the orange bars represent the compressed size, and
the red dotted line shows the average savings percentage (80.5%). This substantial reduction in
communication overhead is crucial for deploying FL in bandwidth-constrained environments. Figure
6(b) further illustrates the communication efficiency of SEMFED, showing the relationship between
communication cost (KB) and compression ratio across rounds. The compression ratio consistently
varies between 0.1 and 0.25, representing a significant reduction from standard communication
approaches while preserving semantic information.

5.3.5 Client Selection Analysis

SEMFED achieves a balanced client selection pattern as shown in Figure 8(a), with selection fre-
quencies ranging from 35% to 70% for different clients. Client 7 was selected most frequently (70%),
while clients 1 and 8 were selected least frequently (35%). This balance is crucial for ensuring fair

11


Client Semantic Similarity Network

Device Types
@ mobile
@ laptop

@ desktop
@ server

Figure 3: Client Semantic Similarity Network. This visualization shows the semantic relationships
between clients with different device types (red: mobile, green: laptop, blue: desktop). The highly
connected network demonstrates the semantic overlap between clients that SEMFED leverages for
improved federated learning.

participation while optimizing for semantic diversity and resource constraints. The heatmap in Fig-
ure 7 visualizes the client selection pattern over 20 communication rounds, showing that no single
client is consistently selected or excluded, further highlighting SEMFED’s balanced approach to
client selection.

As shown in Figure 8(b), SEMFED effectively adapts model architectures to device capabilities,
with 50% using Small CNN (mobile devices), 30% using Tiny LSTM (laptops), and 20% using
MobileBERT (desktops). This heterogeneous model distribution ensures that each client operates
with an architecture optimized for its specific hardware constraints.

5.3.6 Battery Preservation

Figure 9 illustrates SEMFED’s effectiveness in preserving battery levels across communication
rounds. After an initial drop in round 1, battery levels quickly stabilize and remain above 99%
for all clients, demonstrating SEMFED’s energy efficiency. Even clients with more computationally
intensive tasks maintain excellent battery preservation, enabling sustainable long-term deployment
on battery-powered edge devices.

5.3.7 Model Performance

Figure 10 compares the accuracy of SEMFED’s client and server models with baseline approaches.
SEMFED’s client models (dark blue line for average, dashed lines for individual clients) reach near-
perfect accuracy (approaching 1.0) within 5 rounds, significantly outperforming all baselines. The

12


Feature Space Visualization (Colored by Class)

Classes

Class 0
Class 1
Class 2
Class 3

10.0

-

ES é
&e

Principal Component 2

@
po® o&% OH e ©

-5.0

a7 -5.0 =2:5) 0.0 25 5.0 75 10.0 12.5
Principal Component 1

Figure 4: Feature Space Visualization colored by class. This visualization shows a 2D projection
of the feature space with clear clustering by class, demonstrating SEMFED’s ability to maintain
semantic structure despite heterogeneity across clients and data compression.

Energy Usage by Client Client Compute Times

20.0

eo

|
a

x

Client 1D

lie
Total Energy Usage Client 5 Cliet3 Client 9

(a) Energy Usage by Client (b) Client Compute Times

Figure 5: Resource efficiency metrics in SEMFED. (a) Energy Usage by Client. The bar chart
shows total energy consumption across different clients, with mobile devices (0, 1, 2, 8, 9) generally
consuming more energy than laptops (3, 4, 5) and desktops (6, 7). (b) Client Compute Times.
The graph shows compute times across communication rounds for different clients. There is a
significant drop after the first round (from 13 seconds to < 1 second), indicating SEMFED’s
efficient optimization and caching.

server model (red line) shows some fluctuation due to the heterogeneous nature of client updates
but maintains strong overall performance. This rapid convergence to high accuracy demonstrates
the effectiveness of SEMFED’s semantic-aware approach in heterogeneous environments.

13


Communication Savings by Round

Communication Efficiency

HEB Original Size (KB) 56
1600 HEB Compressed Size (KB)
—@ Sayings (96)
300
3400

1200

Size (KB)

‘Communication Cost (KB)

Cor 2s OA ee Te 8 we a ie a as ae ar OE, 00 25 50

125 150 175
Communication Rounds

75 1090
Communication Rounds

(a) Communication Savings by Round (b) Communication Efficiency

Figure 6: Communication efficiency of SEMFED. (a) Communication Savings by Round. The bars
compare original data size (blue) vs. compressed size (orange) for each communication round. The
red dotted line shows the average savings percentage (80.5%). (b) Communication Efficiency. The
graph shows the relationship between communication cost (KB, blue line) and compression ratio
(orange line) across communication rounds.

Client Selection Pattern

Client ID

Sy LF BP wk SB S$ LK
Communication Round

Xo
D

Figure 7: Client Selection Pattern. The heatmap shows which clients were selected in each commu-
nication round (dark blue cells indicate selection). The pattern demonstrates SEMFED’s balanced
selection strategy considering semantic diversity, resource constraints, and fairness across 20 rounds.

5.4 Ablation Studies

To understand the contribution of each component in SEMFED, we conduct ablation studies by
removing or modifying key components.

5.4.1 Impact of Semantic-Aware Client Selection

The semantic-aware client selection significantly improves model accuracy and convergence speed
compared to random or resource-only selection.

14


_ Client Model Distribution
mobile_bert
2 clients (20.0%)

Client Selection Frequency

a. tiny_Istm
3 clients (30.0%)

8
Es
& 8
a8
&
ES

8

Client 1D

&
Ed

a
2

&
2

small_cnn
5 clients (50.0%)

2
3

oa
Selection Frequency

(a) Client Selection Frequency (b) Client Model Distribution

Figure 8: Client selection and model distribution in SEMFED. (a) Client Selection Frequency. The
bar chart shows how often each client was selected across all communication rounds. Selection
frequencies range from 35% to 70%. (b) Client Model Distribution. The pie chart shows the
distribution of different model architectures across clients: Small CNN (50%), Tiny LSTM (30%),
and MobileBERT (20%).

Client Battery Levels

100

Battery Level (%)

0 2 4 6 8 10 12
Communication Rounds

Client IDs
— Client! —— Client6 —— Client8 —— Client9
— Client4 —= Client2 = ClientO == Client7
— Client5 — Client3

Figure 9: Client Battery Levels. The graph shows battery levels across communication rounds for
different clients. After an initial drop in round 1, levels quickly stabilize and remain above 99% for
all clients, demonstrating SEMFED’s energy-efficient operation.

5.4.2 Impact of Heterogeneous Model Architectures

The heterogeneous architecture with semantic embedding significantly outperforms homogeneous
architectures in terms of both performance and resource efficiency.

15


Model Accuracy Comparison

0.96

Accuracy
oF
©
&

0.92

Client 7
Client 5
0.88 ® Client 2
Client 4
Client 0

@
0.86 [2 CISMOCS ENE)

0.0 25 5.0 75 10.0 12.5 15.0 17.5
Communication Rounds

[4K Server Model
@

Figure 10: Model Accuracy Comparison. The graph compares accuracy across communication
rounds for SEMFED’s server model (red), individual client models (dashed lines), client model
average (dark blue), and baseline approaches. SEMFED’s client models reach near-perfect accuracy
(approaching 1.0) within 5 rounds, significantly outperforming all baselines.

Table 2: Model Performance Comparison

Method Accuracy (%) F1-Score Client-Server Agreement
FedAvg 91.2 0.908 0.878
FedProx 92.5 0.921 0.889
FedNLP 94.7 0.944 0.903
HeteroFL 95.3 0.951 0.924
Traditional FL-NLP 93.8 0.935 0.897
Resource-Only FL 96.1 0.959 0.935
SEMFED (Ours) 98.5 0.982 0.967

Table 3: Communication Efficiency Comparison

Method Total Comm. (MB) Compression Ratio Accuracy (%)
FedAvg 24.78 1.000 91.2
FedProx 24.78 1.000 92.5
FedNLP 19.82 0.800 94.7
HeteroFL 13.63 0.550 95.3
Traditional FL-NLP 17.35 0.700 93.8
Resource-Only FL 7.43 0.300 96.1
SEMFED (Ours) 4.82 0.195 98.5

5.4.3. Impact of Feature Compression Techniques

The combination of semantic PCA and sparse coding outperforms other approaches in terms of
compression ratio while maintaining high accuracy.

16


Table 4: Resource Efficiency Comparison

Method Avg. Energy (units) Compute Time (s) Final Battery (%)
FedAvg 0.892 4.213 91.2
FedProx 0.875 4.156 91.8
FedNLP 0.754 3.625 93.5
HeteroFL 0.487 2.312 95.8
Traditional FL-NLP 0.721 3.458 93.7
Resource-Only FL 0.312 1.852 98.2
SEMFED (Ours) 0.261 1.780 99.6

Table 5: Semantic Preservation Metrics

Method Semantic Similarity Vocab Overlap Feature Coherence
FedAvg 0.624 0.312 0.587
FedProx 0.658 0.325 0.612
FedNLP 0.712 0.384 0.678
HeteroFL 0.675 0.358 0.632
Traditional FL-NLP 0.695 0.371 0.654
Resource-Only FL 0.721 0.398 0.685
SEMFED (Ours) 0.835 0.500 0.812

Table 6: Impact of Client Selection Strategies

Selection Strategy Accuracy (%) Convergence (rounds) Energy Usage
Random 94.2 18 0.385
Resource-Only 96.1 14 0.312
Semantic-Only 97.3 12 0.347
Semantic-Aware (Ours) 98.5 10 0.261

Table 7: Impact of Model Architecture Strategies

Architecture Strategy Accuracy (%) Energy Usage Compute Time (s)
Homogeneous (Small) 93.1 0.287 1.482
Homogeneous (Large) 96.2 0.734 3.651
Heterogeneous (No Semantic) 97.4 0.293 1.824
Heterogeneous (Ours) 98.5 0.261 1.780

6 Discussion and Limitations

6.1 Key Insights

Our experimental results demonstrate several key insights about semantic-aware federated learning
for NLP tasks:

e Semantic Heterogeneity Matters: Traditional FL approaches that ignore semantic het-

17


Table 8: Impact of Feature Compression Techniques

Compression Technique Accuracy (%) Compression Ratio Compute Time (s)
No Compression 98.7 1.000 1.875
Standard PCA 97.2 0.350 1.742
Sparse Coding Only 97.8 0.256 1.815
Semantic Compression (Ours) 98.5 0.195 1.780

erogeneity across clients suffer from reduced performance in NLP tasks. SEMFED’s semantic-
aware components significantly improve model accuracy and convergence.

e Resource-Semantic Trade-off: There exists a trade-off between semantic preservation and
resource efficiency. SEMFED effectively navigates this trade-off by adapting model architec-
tures and compression techniques based on device capabilities.

e Feature Distillation vs. Model Averaging: Feature distillation with semantic preserva-
tion outperforms traditional model averaging in heterogeneous environments, both in terms
of performance and communication efficiency.

e Semantic Embedding for Resource Efficiency: The semantic-preserving embedding layer
provides a significant advantage in resource-constrained environments by enabling effective
representation learning with fewer parameters.

6.2 Limitations and Future Work

Despite the promising results, SEMFED has several limitations that warrant further research:

e Scalability: The current implementation has been tested with a moderate number of clients
(10). Scaling to hundreds or thousands of clients may require additional optimizations.

e Dynamic Environments: SEMFED assumes relatively stable client characteristics. Adapt-
ing to highly dynamic environments where device capabilities and network conditions change
rapidly remains challenging.

e Privacy Considerations: While FL inherently provides some privacy protection by keeping
raw data on devices, additional privacy mechanisms like differential privacy may be needed
for sensitive applications.

e Complex NLP Tasks: Current experiments focus on text classification. Extending SEMFED
to more complex NLP tasks like question answering, summarization, or translation requires
further research.

e Real-world Deployment: Evaluating SEMFED on real-world devices with actual resource
constraints and network conditions would provide more insights into its practical applicability.

Future work will address these limitations and extend SEMFED to more complex NLP tasks

and real-world deployment scenarios. Additionally, we plan to explore the integration of continual
learning techniques to handle evolving data distributions and dynamic client environments.

18


7 Conclusion

In this paper, we presented SEMFED, a novel semantic-aware resource-efficient federated learning
framework for heterogeneous NLP tasks. SEMFED addresses the unique challenges of federated
NLP through three key innovations: semantic-aware client selection, heterogeneous NLP model
architectures, and communication-efficient semantic feature compression. Our experimental results
demonstrate that SEMFED significantly outperforms state-of-the-art FL approaches in terms of
model accuracy, communication efficiency, and resource utilization.

The semantic-aware components of SEMFED enable effective learning from heterogeneous clients
with varying semantic data distributions and resource constraints, making it particularly suitable
for real-world federated NLP deployments. By achieving an 80.5% reduction in communication costs
while maintaining high model accuracy, SEMFED represents a significant advancement in federated
learning for NLP tasks.

As federated learning continues to evolve as a privacy-preserving machine learning paradigm,
approaches like SEMFED that specifically address domain-specific challenges will be increasingly
important. We believe that semantic-aware federated learning will play a crucial role in enabling
privacy-preserving NLP applications on edge devices in the future.

Author Contributions

Sajid Hussain: Conceptualization, Methodology, Software, Validation, Formal analysis, Investiga-
tion, Data curation, Writing - original draft, Visualization. Muhammad Sohail: Supervision, Re-
sources, Writing - review & editing, Project administration. Nauman Ali Khan: Conceptualization,
Methodology, Supervision, Writing - review & editing.

Conflict of Interest Statement

The authors declare that the research was conducted in the absence of any commercial or financial
relationships that could be construed as a potential conflict of interest.

Data Availability Statement

The synthetic text classification dataset used in this study will be made available upon reasonable
request to the corresponding author. Code and models will also be made available upon reasonable
request.

Ethics Statement

This research involved only synthetic datasets and did not involve human subjects or animal ex-
perimentation. All data processing and experiments were conducted in accordance with relevant
institutional guidelines and regulations.

Acknowledgments

The authors would like to thank the National University of Sciences and Technology (NUST) for
supporting this research and providing computational resources for the experiments.

19


References

[1]

[2|

[3]

[4|

[5]

[6]

[7|

[S|

[9|

[10]

[11]

[12]

[13]

[14]

[15]

H. B. McMahan, E. Moore, D. Ramage, $. Hampson, and B. A. y Arcas, “Communication-
efficient learning of deep networks from decentralized data,” in Proceedings of the 20th Inter-
national Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282, 2017.

P. Kairouz et al., “Advances and open problems in federated learning,” Foundations and Trends
in Machine Learning, vol. 14, no. 1-2, pp. 1-210, 2021.

B. Lin et al., “FedNLP: Benchmarking federated learning methods for natural language pro-
cessing tasks,” in Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 5749-5757, 2021.

F. Zhu, F. Ye, Y. Fu, Q. Liu, and B. Shen, “Federated learning for speaker recognition based
on self-attention mechanism,” IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing, vol. 28, pp. 2578-2591, 2020.

J. Koneény, H. B. McMahan, F. X. Yu, P. Richtérik, A. T. Suresh, and D. Bacon, “Federated
learning: Strategies for improving communication efficiency,” arXiv preprint arXiv: 1610.05492,
2016.

Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning with non-iid
data,” arXiv preprint arXiv:1806.00582, 2018.

M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and 8S. Cui, “A joint learning and communica-
tions framework for federated learning over wireless networks,” JEEE Transactions on Wireless
Communications, vol. 20, no. 1, pp. 269-283, 2021.

T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization
in heterogeneous networks,” Proceedings of Machine Learning and Systems, vol. 2, pp. 429-450,
2020.

5S. Wang et al., “Adaptive federated learning in resource constrained edge computing systems,”
IEEE Journal on Selected Areas in Communications, vol. 37, no. 6, pp. 1205-1221, 2019.

E. Diao, J. Ding, and V. Tarokh, “HeteroFL: Computation and communication efficient feder-
ated learning for heterogeneous clients,” arXiv preprint arXiv:2010.01264, 2020.

A. K. Sahu, T. Li, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith, “On the convergence
of federated optimization in heterogeneous networks,” arXiv preprint arXiv:1812.06127, 2018.

J. Wang et al., “Tackling the objective inconsistency problem in heterogeneous federated opti-
mization,” Advances in Neural Information Processing Systems, vol. 33, pp. 7611-7623, 2020.

Y. Lin, 5S. Han, H. Mao, Y. Wang, and W. J. Dally, “Deep gradient compression: Reducing the
communication bandwidth for distributed training,” arXiv preprint arXiv:1712.01887, 2017.

Y. Jiang, S. Wang, B. J. Ko, W.-H. Lee, and L. Tassiulas, “Model pruning enables efficient
federated learning on edge devices,” arXiv preprint arXiv:1909. 12826, 2019.

D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, “QSGD: Communication-efficient
SGD via gradient quantization and encoding,” Advances in Neural Information Processing
Systems, vol. 30, pp. 1709-1720, 2017.

20


[16]

[17|

[18]

[19]

[20]

[21]

[22]

[23]

J. Chen and X. Ran, “Deep neural networks with massive learned knowledge,” in Proceedings

of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 2118-2133,
2021.

L. Liu et al., “A secure federated learning framework for 5G networks,” [EEE Wireless Com-
munications, vol. 27, no. 4, pp. 24-31, 2020.

Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, “MobileBERT: a compact task-
agnostic BERT for resource-limited devices,” in Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pp. 2158-2170, 2020.

V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of BERT:
smaller, faster, cheaper and lighter,” arXiv preprint arXiv:1910.01108, 2019.

X. Jiao et al., “TinyBERT: Distilling BERT for natural language understanding,” arXiv preprint
arXiv:1909.10851, 2019.

D. Li and J. Wang, “FedMD: Heterogeneous federated learning via model distillation,” arXiv
preprint arXtv:1910.08581, 2019.

T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for robust model fusion in
federated learning,” Advances in Neural Information Processing Systems, vol. 33, pp. 2351-—
2363, 2020.

T. Nishio and R. Yonetani, “Client selection for federated learning with heterogeneous resources
in mobile edge,” in IEEE International Conference on Communications (ICC), pp. 1-7, 2019.

21
