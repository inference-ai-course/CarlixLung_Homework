arXiv:2204.10192v2 [cs.CL] 25 Sep 2022

Residue-Based Natural Language Adversarial Attack Detection

Vyas Raina
ALTA Institute, Cambridge University
vr313@cam.ac.uk

Abstract

Deep learning based systems are susceptible
to adversarial attacks, where a small, imper-
ceptible change at the input alters the model
prediction. However, to date the majority of
the approaches to detect these attacks have
been designed for image processing systems.
Many popular image adversarial detection ap-
proaches are able to identify adversarial exam-
ples from embedding feature spaces, whilst in
the NLP domain existing state of the art de-
tection approaches solely focus on input text
features, without consideration of model em-
bedding spaces. This work examines what dif-
ferences result when porting these image de-
signed strategies to Natural Language Process-
ing (NLP) tasks - these detectors are found to
not port over well. This is expected as NLP
systems have a very different form of input:
discrete and sequential in nature, rather than
the continuous and fixed size inputs for im-
ages. As an equivalent model-focused NLP
detection approach, this work proposes a sim-
ple sentence-embedding "residue" based detec-
tor to identify adversarial examples. On many
tasks, it out-performs ported image domain de-
tectors and recent state of the art NLP specific
detectors |.

1 Introduction

In the last decade deep learning based models have
demonstrated success in a wide range of applica-
tion areas, including Natural Language Processing
(NLP) (Vaswani et al., 2017) and object recogni-
tion (He et al., 2015). These systems may be de-
ployed in mission critical situations, where there is
the requirement for a high level of robustness. How-
ever, Szegedy et al. (2014) demonstrated that deep
models have an inherent weakness: small perturba-
tions in the input can yield significant, undesired,
changes in the output from the model. These input

‘Code is available at: https://github.com/
rainavyas/NAACL-2022-Residue-—Detector

Mark Gales
ALTA Institute, Cambridge University
mjfg@cam.ac.uk

perturbations were termed adversarial examples
and their generation adversarial attacks.

Adversarial attacks have been developed for sys-
tems operating in various domains: image sys-
tems (Serban et al., 2020; Biggio and Roli, 2017;
Bhambri et al., 2019) and NLP systems (Lin et al.,
2014; Samanta and Mehta, 2017; Rosenberg et al.,
2017). The characteristics of the input can be
very different between these application domains.
Broadly, the nature of inputs can be described us-
ing two key attributes: static (fixed length) vs se-
quential and continuous vs discrete. Under this
categorisation, image inputs are continuous and
static, whilst NLP inputs are discrete and sequen-
tial. This work argues that due to the fundamental
differences in the input and resulting adversarial
perturbations in the different domains, adversarial
attack behaviour can vary significantly from one
domain to another. Hence, the extensive research
on exploring and understanding adversarial pertur-
bation behaviour in the continuous, static world of
image systems does not necessarily transfer well to
the NLP tasks.

For adversarial attack generation, a number of
specific NLP attacks have been proposed that are
designed for NLP task inputs (Lin et al., 2014;
Samanta and Mehta, 2017; Rosenberg et al., 2017;
Huang et al., 2018; Papernot et al., 2016; Grosse
et al., 2016; Sun et al., 2018; Cheng et al., 2018;
Blohm et al., 2018; Neekhara et al., 2018; Raina
et al., 2020; Jia and Liang, 2017; Minervini and
Riedel, 2018; Niu and Bansal, 2018; Ribeiro et al.,
2018; Iyyer et al., 2018; Zhao et al., 2017). How-
ever, there has been less research on developing
defence schemes. These defence strategies can be
split into two main groups: model modification,
where the model or data is altered at training time
(e.g. adversarial training (Yoo and Qi, 2021)) and
detection, where external systems or algorithms are
applied to trained models to identify adversarial at-
tacks. As model modification approaches demand


re-training of models, detection approaches are usu-
ally considered easier for implementation on de-
ployed systems and thus are often preferred. Hence,
this work investigates the portability of popular de-
tection approaches designed for image systems to
NLP systems. Furthermore, this work introduces a
specific NLP detection approach that exploits the
discrete nature of the inputs for NLP systems. This
approach out-performs standard schemes designed
for image adversarial attack detection, as well as
other NLP detection schemes.

The proposed NLP specific detection approach
will be referred to as residue detection, as it is
shown that adversarial attacks in the discrete, word
sequence space result in easily detectable residual
components in the sentence embedding space. This
residue can be easily detected using a simple lin-
ear classifier operating in the encoder embedding
space. In addition, this work shows that even when
an adversary has knowledge of the linear residual
detector, they can only construct attacks at a frac-
tion of the original strength. Hence this work ar-
gues that realistic (word level, semantically similar)
adversarial perturbations at the natural language in-
put of NLP systems leave behind easily detectable
residue in the sentence embedding. Interestingly,
the residue detection approach is shown to perform
poorly when used to detect attacks in the image do-
main, supporting the hypothesis that the nature of
the input has an important influence on the design
of effective defence strategies.

2 Related Work

Previous work in the image domain has analysed
the output of specific layers in an attempt to iden-
tify adversarial examples or adversarial subspaces.
First, Feinman et al. (2017) proposed that adver-
sarial subspaces have a lower probability density,
motivating the use of the Kernel Density (KD) met-
ric to detect the adversarial examples. Nevertheless,
Ma et al. (2018) found Local Intrinsic Dimension-
ality (LID) was a better metric in defining the sub-
space for more complex data. In contrast to the
local subspace focused approaches of KD and LID,
Carrara et al. (2019b) showed that trajectories of
hidden layer features can be used to train a LSTM
network to accurately discriminate between authen-
tic and adversarial examples. Out performing all
previous methods, Lee et al. (2018) introduced an
effective detection framework using Mahalanobis
Distance Analysis (MDA), where the distance is

calculated between a test sample and the closest
class-conditional Gaussian distribution in the space
defined by the output of the final layer of the clas-
sifier. Li and Li (2016) also explored using the
output of convolutional layers for image classifica-
tion systems to identify statistics that distinguish
adversarial samples from original samples. They
find that by performing a PCA decomposition the
statistical variation in the least principal directions
is the most significant and can be used to separate
original and adversarial samples. However, they
argue this is ineffective as an adversary can eas-
ily suppress the tail distribution. Hence, Li and
Li (2016) extract statistics from the convolutional
layer output to train a cascade classifier to sepa-
rate the original and adversarial samples. Most
recently, Mao et al. (2019) avoid the use of artifi-
cially designed metrics and combine the adversarial
subspace identification stage and the detecting ad-
versaries stage into a single framework, where a
parametric model adaptively learns the deep fea-
tures for detecting adversaries.

In contrast to the embedding space detection
approaches, Cohen et al. (2019) shows that influ-
ence functions combined with Nearest Neighbour
distances perform comparably or better than the
above standard detection approaches. Other de-
tection approaches have explored the use of un-
certainty: Smith and Gal (2018) argues that ad-
versarial examples are out of distribution and do
not lie on the manifold of real data. Hence, a dis-
criminative Bayesian model’s epistemic (model)
uncertainty should be high. Therefore, calcula-
tions of the model uncertainty are thought to be
useful in detecting adversarial examples, indepen-
dent of the domain. However, Bayesian approaches
aren’t always practical in implementation and thus
many different approaches to approximate this un-
certainty have been suggested in literature (Leibig
et al., 2017; Gal, 2016; Gal and Ghahramani, 2016).

There are a number of existing NLP specific
detection approaches. For character level attacks,
detection approaches have exploited the grammat-
ical (Sakaguchi et al., 2017) and spelling (Mays
et al., 1991; Islam and Inkpen, 2009) inconsisten-
cies to identify and detect the adversarial samples.
However, these character level attacks are unlikely
to be employed in practice due to the simplicity
with which they can be detected. Therefore, detec-
tion approaches for the more difficult semantically
similar attack samples are of greater interest, where


the meaning of the textual input is maintained with-
out compromising the spelling or grammatical in-
tegrity. To tackle such word-level, semantically
similar examples, Zhou et al. (2019) designed a
discriminator to classify each token representation
as part of an adversarial perturbation or not, which
is then used to ‘correct’ the perturbation. Other
detection approaches (Raina et al., 2020; Han et al.,
2020; Minervini and Riedel, 2018) have shown
some success in using perplexity to identify adver-
sarial textual examples. Most recently, Mozes et al.
(2020) achieved state of the art performance with
the Frequency Guided Word Substitution (FGWS)
detector, where a change in model prediction after
substituting out low frequency words is revealing
of adversarial samples.

3  Adversarial Attacks

An adversarial attack is defined as an imperceptible
change to the input that causes an undesired change
in the output of a system. Often, an attack is found
for a specific data point, x. Consider a classifier Fa,
with parameters 6, that predicts a class label for an
input data point, x, sampled from the input distri-
bution V. A successful adversarial attack is where
a perturbation 6 at the input causes the system to
miss-classify,

When defining adversarial attacks, it is impor-
tant consider the interpretation of an imperceptible
change. Adversarial perturbations are not consid-
ered effective if they are easy to detect. Hence, the
size of the perturbation must be constrained:

G(x,x+6) <e, (2)

where the function G() describes the form of con-
straint and «€ is a selected threshold of imperceptibil-
ity. Typically, when considering continuous space
inputs (such as images), a popular form of the con-
straint of Equation 2, is to limit the perturbation in
the J, norm, with p € [1, 00), e.g. ||6||p < «.

For whitebox attacks in the image domain, the
dominant attack approach has proven to be Pro-
jected Gradient Descent (PGD) (Kurakin et al.,
2016). The PGD approach, iteratively updates the
adversarial perturbation, 6, initialised as 69 = O.
Each iterative step moves the perturbation in the
direction that maximises the loss function, £, used
in the training of the model,

Ois1 = clip.(6; + aV5,L(x+6;;6)), (3)

where a is an arbitrary step-size parameter and the
clipping function, clip,, ensures the impercepti-
bility constraint of Equation 2 is satisfied.

When considering the NLP domain, a sequen-
tial, discrete input of ZL words, can be explicitly
represented as,

X= WiLL = W1,W2,---,WL-1,WL, (4)

where, the discrete word tokens, w 1.7, are often
mapped to a continuous, sequential word embed-
ding (Devlin et al., 2019) space,

hy., = hy, ho,...,hyp—i, hp. (5)

Attacks must take place in the discrete text space,

/ / / /
X+0 = Wye = WW, Wo)---)Wy_y) WL (6)

This requires a change in the interpretation of the
perturbation 6. It is not simple to define an ap-
propriate function G() in Equation 2 for word se-
quences. Perturbations can be measured at a char-
acter or word level. Alternatively, the perturba-
tion could be measured in the vectorized embed-
ding space (Equation 5), using for example /,,-norm
based (Goodfellow et al., 2015) metrics or cosine
similarity (Carrara et al., 2019a), which have been
used in the image domain. However, constraints in
the embedding space do not necessarily achieve im-
perceptibility in the original word sequence space.
The simplest approach is to use a variant of an edit-
based measurement (Li et al., 2018), £e(), which
counts the number of changes between the original
sequence, w1.7 and the adversarial sequence Wh. L>
where a change is a swap/addition/deletion, and
ensures it is smaller than a maximum number of
changes, NV,

Le(wi:n, Wiz) < N. (7)

For the NLP adversarial attacks this work only
examines word-level attacks, as these are consid-
ered more difficult to detect than character-level
attacks. As an example, for an input sequence of L
words, a N-word substitution adversarial attack,

w'.n» applied at word positions n1,n2,...,nN
; ; }
gives the adversarial output, w}.;,
! _ !
Wy! = W1,++s, Wn,—-1, W1, Wn, 41) seey
: 8
Wny—1,WN; Wny+ls:+-,WL- ( )

The challenge is to select which words to replace,
and what to replace them with. A simple yet ef-
fective substitution attack approach that ensures a


small change in the semantic content of a sentence
is to use saliency to rank the word positions, and to
use word synonyms for the substitutions (Ren et al.,
2019). This attack is termed Probability Weight
Word Saliency (PWWS). The highest ranking word
word can be swapped for a synonym from a pre-
selected list of given synonyms. The next most
highly ranked word is substituted in the same man-
ner and the process is repeated till the required N
words have been substituted.

The above approach is limited to attacking spe-
cific word sequences and so cannot easily be gener-
alised to universal attacks (Moosavi-Dezfooli et al.,
2016), where the same perturbation is used for
all inputs. For this situation, a simple solution
is concatenation (Wang and Bansal, 2018; Blohm
et al., 2018), where for example, the same NV-length
sequence of words is appended to each input se-
quence of words, as described in Raina et al. (2020).
Here,

Whey = W1,---,WL, Wi, ---, WN: (9)

In both the substitution attack (Equation 8) and
the concatenation attack (Equation 9), the size of
the attack can be measured using the number of
edits, Le(wi.p, w}.,,) = N.

4 Adversarial Attack Detection

For a deployed system, the easiest approach to de-
fend against adversarial attacks is to use a detection
process to identify adversarial examples without
having to modify the existing system.

For the image domain Section 2 discusses many
of the standard detection approaches. In this work,
we select two distinct approaches that have been
generally successful: uncertainty (Smith and Gal,
2018), where adversarial samples are thought to
result in greater epistemic uncertainty and Maha-
lanobis Distance (Lee et al., 2018), where the Ma-
halanobis distance in the embedding space is indica-
tive of how out of distribution a sample is (adversar-
ial samples are considered more out of distribution).
In the NLP domain, when excluding trivial gram-
mar and spelling based detectors, perplexity based
detectors can be used (Raina et al., 2020). Many
other NLP specific detectors (Zhou et al., 2019;
Han et al., 2020; Minervini and Riedel, 2018) have
been proposed, but Mozes et al. (2020)’s FGWS
detector is considered the state of art and is thus se-
lected for comparison. Here low frequency words
in an input are substituted for higher frequency

words and the change in model prediction is mea-
sured - adversarial samples are found to generally
have a greater change. This work introduces a
further NLP specific detector: residue detection,
described in detail in Section 4.1.

When considering any chosen detection mea-
sure Fg, a threshold 6 can be selected to decide
whether an input, wy .z, is adversarial or not, where
Fa(wi:t) > 8, implies that w).7 is an adversarial
sample. To assess the success of the adversarial
attack detection processes, precision-recall curves
are used. For the binary classification task of iden-
tifying an input as adversarially attacked or not, at
a given threshold /, the precision and recall val-
ues can be computed as prec = TP/TP + FP and
rec = TP/TP + FN, where TP, FP and EN are
the standard true-positive, false-positive and false-
negative definitions. A single point summary of
precision-recall curves is given with the F, score.

4.1 Residue Detection

In this work we introduce a new NLP detection
approach, residue detection, that aims to exploit
the nature of the NLP input space, discrete and
sequential. Here we make two hypotheses:

1. Adversarial samples in an encoder embedding
space result in larger components (residue)
in central PCA eigenvector components than
original examples.

2. The residue is only significant (detectable) for
systems operating on discrete data (e.g. NLP
systems).

The rationale behind these hypotheses is discussed
next.

Deep learning models typically consist of many
layers of non-linear activation functions. For exam-
ple, in the NLP domain systems are usually based
on layers of the Transformer architecture (Vaswani
et al., 2017). The complete end-to-end model F,()
can be treated as a two stage process, with an ini-
tial set of layers forming the encoding stage, Fen()
and the remaining layers forming the output stage,
Fei(), Le. Fy(x) = Fer (Fen(X)).

If the encoding stage of the end-to-end classifier
is sufficiently powerful, then the embedding space
Fen(x) will have compressed the useful informa-
tion into very few dimensions, allowing the output
stage to easily separate the data points into classes
(for classification) or map the data points to a con-
tinuous value (for regression). A simple Principal


Component Analysis (PCA) decomposition of this
embedding space can be used to visualize the level
of compression of the useful information. The PCA
directions can be found using the eigenvectors of
the covariance matrix, C, of the data in the en-
coder embedding space. If {q;}¢_,, where d is the
dimension of the encoder embedding space, repre-
sent the eigenvectors of C ordered in descending
order by the associated eigenvalue in magnitude,
then it is expected that almost all useful information
is contained within the first few principal directions,
{qi}?_,, where p < d. Hence, the output stage,
F1() will implicitly use only these useful compo-
nents. The impact of a successful adversarial per-
turbation, Fen(x + 0), is the significant change in
the components in the principal eigenvector direc-
tions {q;}7_,, to allow fooling of the output stage.
Due to the complex nature of the encoding stage
and the out of distribution nature of the adversarial
perturbations, there are likely to be residual compo-
nents in the non-principal {qi}, 41 eigenvector
directions. These perturbations in the non-principal
directions are likely to be more significant for the
central eigenvectors, as the encoding stage is likely
to almost entirely compress out components in the
least principal eigenvector directions, {qi }¢_ 4 449
where d’ = d. Hence, {qi}e, 4.1 can be viewed
as a subspace containing adversarial attack residue
that can be used to identify adversarial examples.
The existence of adversarial attack residue in
the central PCA eigenvector directions, {qi}e, a8
suggests that in the encoder embedding space,
Fen(x), adversarial and original examples are lin-
early separable. This motivates the use of a simple
linear classifier as an adversarial attack detector,
P(adv|x) = o(WFen(x) +b), (10)
where W and 0 are the parameters of the linear
classifier to be learnt and o is the sigmoid function.
The above argument cannot predict how signifi-
cant the residue in the central eigenvector space is
likely to be. For the discrete space NLP attacks, the
input perturbations are semantically small, whilst
for continuous space image attacks the perturba-
tions are explicitly small using a standard J,,-norm.
Hence, it is hypothesised that NLP perturbations
cause larger errors to propagate through the system,
resulting in more significant residue in the encoder
embedding space than that for image attacks. Thus,
the residue technique is only likely to be a feasible
detection approach for discrete text attacks.

The hypotheses made in this section are analysed
and empirically verified in Section 5.3.

5 Experiments

5.1 Experimental Setup

Table | describes four NLP classification datasets:
IMDB (Maas et al., 2011); Twitter (Saravia et al.,
2018); AG News (Zhang et al., 2015) and DB-
pedia (Zhang et al., 2015). Further, a regression
dataset, Linguaskill-Business (L-Bus) (Chambers
and Ingham, 2011) is included. The L-Bus data is
from a multi-level prompt-response free speaking
test i.e. candidates from a range of proficiency lev-
els provide open responses to prompted questions.
Based on this audio input a system must predict a
score of 0-6 corresponding to the 6 CEFR (Council
of Europe, 2001) grades. This audio data was tran-
scribed using an Automatic Speech Recognition
system with an average word error rate of 19.5%.

Dataset #Train #Test #Classes
IMDB 25,000 25,000 2
Twitter 16,000 2000 6
AG News — 120,000 7600 4
DBpedia 560,000 70,000 14

L-Bus 900 202 1
Table 1: NLP Datasets.

All NLP task models were based on the Trans-
former encoder architecture (Vaswani et al., 2017).
Table 2 indicates the specific architecture used for
each task and also summarises the classification
and regression performance for the different tasks.
For classification tasks, the performance is mea-
sured by top | accuracy, whilst for the regression
task (L-Bus), the performance is measured using
Pearson Correlation Coefficient (PCC).

Dataset Transformer Performance
IMDB BERT Acc: 93.8%
Twitter ELECTRA Acc: 93.3%
AG News BERT Ace: 94.5%
DBpedia ELECTRA — Acc: 99.2%
L-Bus BERT PCC: 0.749

Table 2: Performance of models (BERT (Devlin et al.,
2018), ELECTRA (Clark et al., 2020)).

Table 3 shows the impact of realistic adversar-
ial attacks on the tasks: substitution (sub) attack
(Equation 8), which replaces the N most salient
tokens with a synonym defined by WordNet’, as

*https://wordnet .princeton.edu/


dictated by the PWWS attack algorithm described
in Section 3; or a targeted universal concatenation
(con) attack (Equation 9), used for the regression
task on the L-Bus dataset, seeking to maximise the
average score output from the system by appending
the same N words to the end of each input. For
classification tasks, the impact of the adversarial at-
tack is measured using the fooling rate, the fraction
of originally correctly classified points, misclassi-
fied after the attack, whilst for the regression task,
the impact is measured as the average increase in
the output score.

Dataset Attack N Impact
IMDB sub 25 Fool: 0.70
Twitter sub 6 Fool: 0.77
AG News sub 40 Fool: 0.65
DBpedia sub 25 Fool: 0.52
L-Bus con 3. Score: +0.51

Table 3: Impact of different N-word adversarial at-
tacks.

5.2 Results

Section 4.1 predicts that adversarial attacks in the
discrete text space leave residue in a system’s en-
coder embedding space that can be detected using
a simple linear classifier. Hence, using the 12-
layer Transformer encoder’s output CLS token em-
bedding as the encoder embedding space for each
dataset’s trained system (Table 2), a simple linear
classifier, as given in Equation 10, was trained °
to detect adversarial examples from the adversarial
attacks given for each dataset in Table 3. The train-
ing of the detection linear classifier was performed
on the training data (Table 1) augmented with an
equivalent adversarial example for each original
input sample in the dataset. Using the test data
samples augmented with adversarial examples (as
defined by Table 3), Table 4 compares the efficacy
of the linear residue detector to other popular de-
tection strategies + (from Section 4) using the best
F, score. It is evident from the high F-scores, that
for most NLP tasks the linear detection approach is

31r=0.02, epochs=20, batch size=200, #769 parameters

‘Detection Strategies: Mahalanobis Distance (MD) used
the same train-test split as the residue approach; Perplexity
(Perp) was calculated using the language model from Chen
et al. (2016); Uncertainty (Unc) used the best measure out
of mutual information, confidence, KL-divergence, expected
entropy and entropy of expected and reverse mutual informa-
tion; and FGWS was implemented using the code given at
https://github.com/maximilianmozes/fgws.

better than other state of the art NLP specific and
ported image detection approaches.

Dataset Res Perp FGWS MD _ Unc

IMDB 0.91 0.68 0.87 0.77 0.75

Twitter 0.84 0.67 0.76 0.77 0.78
AGNews 0.95 0.69 0.89 0.78 0.75
DBpedia 0.80 0.67 0.82 0.78 0.90
L-Bus 0.99 0.68 0.91 0.75 0.81
Table 4: Fy-score performance of detection ap-

proaches.

However, an adversary may have knowledge of
the detection approach and may attempt to design
an attack that directly avoids detection. Hence, for
each dataset, the attack approaches were repeated
with the added constraint that any attack words that
resulted in detection were rejected. The impact
of attacks that suppress detection have been pre-
sented in Table 5. Generally, it is shown across all
NLP tasks that an adversary that attempts to avoid
detection of its residue by a previously trained lin-
ear Classifier, can only generate a significantly less
powerful adversarial attack.

Dataset Without With
IMDB 0.70 0.19
Twitter 0.77 0.23
AG News 0.65 0.16
DBpedia 0.52 0.14
L-Bus +0.51 +0.23

Table 5: Fooling rate (classification) or score (regres-
sion) with and without attack modified to avoid detec-
tion.

5.3 Analysis

The aim of this section is verify that the success of
the residue detector can be explained by the two
main hypotheses made in Section 4.1. The claim
that residue is left by adversarial samples in the
central PCA eigenvector components is explored
first. For each NLP task a PCA projection matrix
is learnt in the encoder embedding space using the
original training data samples (Table 1). Using
the test data, the residue in the embedding space
can be visualized through a plot of the average
(across the data) component, p; = 4 ee Pig in
each eigenvector direction, q; of the original and
attacked data, where

pag = |Fen ly)" al 5 (11)


with x; being the jth data point. Figure 1 shows
an example plot for the Twitter dataset, where p; is
plotted against the eigenvalue rank, i for the origi-
nal and attacked data examples. Residue plots for
other datasets are included in Appendix A. Next,
it is necessary to verify that the residue detector
specifically uses the residue in the central eigen-
vector components to distinguish between original
and adversarial samples. To establish this, each en-
coder embedding, Fen(x)’s components not within
a target subspace of PCA eigenvector directions
{qi} , are removed, i.e. we have a projected em-
bedding, x?) = Fen(®)— id fp,p+w) q? Fen(x) qi,
where w is a window size to choose. Now, us-
ing Fe:(x ?)) and a residue detector trained us-
ing the modified embeddings, x"), the classifier’s
(Fe1(x))) accuracy and detector performance
(measured using F1 score) can be found. Figure 2
shows the performance of the classifier (F.1 (x!) ))
and the detector for different start components p,
with the window size, w = 5. It is clear that the
principal components hold the most important in-
formation for classifier accuracy, but, as hypothe-
sised in Section 4.1, it is the more central eigen-
vector components that hold the most information
useful for the residue detector, i.e. the subspace de-
fined by {q;}}2; holds the most detectable residue
from adversarial examples.

iJ
10 — Original

—— Attacked

10° 4

10-? 4

Average Component Size

10-3 4

10-4

ie) 100 200 300 400 500 600 700 800
Eigenvalue Rank

Figure 1: Encoder Embedding Space Residue Plot.

The second hypothesis in Section 4.1 claims that
the existence of residue in the central eigenvector
components is due to the discrete nature of NLP
adversarial attacks. Hence, to analyze the impact
of the discrete aspect of the attack, an artificial
continuous space attack was constructed for the
Twitter NLP system, where the continuous input
embedding layer space (Equation 5) of the system

--- Classifier Accuracy
—— Detector F1 f 0.80

Classifier Accuracy
Detector Fl

(0) 10 20 30 40 50 60 70 80
component, p

Figure 2: Performance of classifier and detector with
windowed projection of encoder embedding space.

is the space in which the attack is performed. Us-
ing the Twitter emotion classifier, a PGD (Equation
3) attack was performed on the input embeddings
for each token, where the perturbation size was
limited to be « = 0.1 in the 1, norm, achieving a
fooling rate of 0.73. Note that this form of attack
is artificial, as a real adversary can only modify the
discrete word sequence (Equation 4). To compare
the influence of discrete and continuous attacks
on the same system, the average (across dataset)
lz and 1, norms of the perturbations in the input
layer embedding space were found. Further, a sin-
gle value summary, N,, of the residue plot (e.g.
Figure 1), was calculated for each attack. N, is the

average difference in standard deviations between
(orig)

the original component mean, 7; and attack
(attack)
mean, p; ,
(attack) (orig)
14 |i — Pi
No =F (12)
i=1 Var j [oi]

Table 6 reports these metrics for the discrete and
artificial continuous NLP adversarial attacks on the
Twitter system >. It is apparent that perturbation
sizes for the discrete attacks are significantly larger.
Moreover, NV, is significantly smaller for the con-
tinuous space attack, indicating that the residue left
by continuous space adversarial attacks is smaller.

Attack No lp error loo error
Discrete 1.201 50.2+19.2 3.26+0.86
Continuous 0.676 5.3543.95 0.08+0.03

Table 6: Comparison of token level discrete attack and
input embedding layer continuous PGD attack.

Similar trends were found across all datasets (Table A.2)


To explicitly observe the impact of the nature
of data on detectors, adversarial attacks are con-
sidered in four domains: the discrete, sequential
NLP input space (NLP-disc); the artificial con-
tinuous, sequential embedding space of an NLP
model (NLP-cont); the continuous, static image in-
put space (Img-cont) and a forced discretised, static
image input space (Img-disc). For the NLP-disc
and NLP-cont the same attacks as in Table 6 are
used. For the continuous image domain (Img-cont),
a VGG-16 architecture image classifier trained on
CIFAR-100 (Krizhevsky et al., 2009) image data
(achieving a top-5 accuracy of 90.1%) and attacked
using a standard 1,, PGD approach (Equation 3) is
used. For the discrete image domain (Img-disc),
the CIFAR-100 images, X € gin were discre-
tised using function Q, : Zee > ZEXR, where
Z, = {0, et 225, ..., 255}. In this work 2-
bit quantization was used, i.e. g = 4. With this
quantization, a VGG-16 architecture was trained to
achieve 78.2% top-5 accuracy. To perform a dis-
crete space attack, a variant of the PWWS synonym
substitution attack (Section 3) was implemented,
where synonyms were interpreted as closest permit-
ted quantisation values and N pixel values were
substituted. For these different domains, Table 7
compares applicable detection approaches (certain
NLP detection approaches are not valid outside
the word sequence space) using the best F; score,
where different attack perturbation sizes are consid-
ered (N substitutions for discrete attacks and for
continuous attacks |6| < ¢ for perturbation 6).

Domain Attack Res Unc MD
N=3 | 0.80 0.74 0.73
NLP-disc | 26 | 0.84 0.78 0.77
«=0.1 | 0.67 0.71. 0.70

NLP-cont | 93 | 0.67 0.80 0.85
Ime-dise | N=200 | 0.78 0.67 0.70
8 N=400 | 0.84 0.68 0.72
ime-cont | “12 | 0.68 0.70 0.72
8 e=48 | 0.83 0.81 0.87

Table 7: Portability of detection approaches.

In the discrete domains, the residue detection
approach is better than all the other approaches.
However, in the continuous data type domains, the
Mahalanobis Distance dominates as the detection
approach, with the residue detection approach per-
forming the worst. As predicted by the second
hypothesis of Section 4.1, the lack of success of

the residue detection approach is expected here -
the residue detection approach is only successful
for discrete space attacks.

To verify that the residue detection approach is
agnostic to the type of attack, the residue detector
trained on substitution attack examples was evalu-
ated on concatenation attack examples. Using the
Twitter dataset, a N = 3 concatenation attack was
applied, achieving a fooling rate of 0.59. In this
setting, the residue detector (trained on the N = 6
substitution adversarial examples) achieved a F1
score of 0.81, which is comparable to the original
score of 0.84 (from Table 4). This shows that even
with different attack approaches similar forms of
residue are produced, meaning a residue detector
can be used even without knowledge of the type of
adversarial attack.

6 Conclusions

In recent years, deep learning systems have been
deployed for a large number of tasks, ranging from
the image to the natural language domain. How-
ever, small, imperceptible adversarial perturbations
at the input, have been found to easily fool these
systems, compromising their validity in high-stakes
applications. Defence strategies for deep learning
systems have been extensively researched, but this
research has been predominantly carried out for
systems operating in the image domain. As a result,
the adversarial detection strategies developed, are
inherently tuned to attacks on the continuous space
of images. This work shows that these detection
strategies do not necessarily transfer well to attacks
on natural language processing systems. Hence, an
adversarial attack detection approach is proposed
that specifically exploits the discrete nature of per-
turbations for attacks on discrete sequential inputs.

The proposed approach, termed residue detec-
tion, demonstrates that imperceptible attack pertur-
bations on natural language inputs tend to result
in large perturbations in word embedding spaces,
which result in distinctive residual components.
These residual components can be identified using
a simple linear classifier. This residue detection ap-
proach was found to out-perform both detection ap-
proaches ported from the image domain and other
state of the art NLP specific detectors.

The key finding in this work is that the nature
of the data (e.g. discrete or continuous) strongly
influences the success of detection systems and
hence it is important to consider the domain when


designing defence strategies.

7 Limitations, Risks and Ethics

A limitation of the residue approach proposed in
this work is that it requires training on adversarial
examples, which is not necessary for other NLP de-
tectors. This means there is a greater computational
cost associated with this detector. Moreover, asso-
ciated with this limitation is a small risk, where in
process of generating creative adversarial examples
to build a robust residue detector, the attack gen-
eration scheme may be so strong that it can more
easily evade detection from other existing detectors
already deployed in industry. There are no further
ethical concerns related to this detector.

8 Acknowledgements

This paper reports on research supported by
Cambridge Assessment, University of Cambridge.
Thanks to Cambridge English Language Assess-
ment for support and access to the Linguaskill-
Business data. The authors would also like to thank
members of the ALTA Speech Team.

References

Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi,
and Arun Balaji Buduru. 2019. A study of black
box adversarial attacks in computer vision. CoRR,
abs/1912.01667.

Battista Biggio and Fabio Roli. 2017. Wild patterns:
Ten years after the rise of adversarial machine learn-
ing. CoRR, abs/1712.03141.

Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang
Yu, and Ngoc Thang Vu. 2018. Comparing attention-
based convolutional and recurrent neural networks:
Success and limitations in machine reading compre-
hension. CoRR, abs/1808.08744.

Fabio Carrara, Rudy Becarelli, Roberto Caldelli, Fab-
rizio Falchi, and Giuseppe Amato. 2019a. Adver-
sarial examples detection in features distance spaces.
In Computer Vision — ECCV 2018 Workshops, pages
313-327, Cham. Springer International Publishing.

Fabio Carrara, Rudy Becarelli, Roberto Caldelli, Fab-
rizio Falchi, and Giuseppe Amato. 2019b. Adversar-
ial Examples Detection in Features Distance Spaces:

Subvolume B, pages 313-327. ECCV.

Lucy Chambers and Kate Ingham. 2011. The BULATS
online speaking test. Research Notes, 43:21—25.

X. Chen, X. Liu, Y. Qian, M. J. F. Gales, and P. C.
Woodland. 2016. Cued-rnnlm — an open-source

toolkit for efficient training and evaluation of recur-
rent neural network language models. In 20/6 IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 6000-6004.

Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin- Yu Chen,
and Cho-Jui Hsieh. 2018. Seq2sick: Evaluating the
robustness of sequence-to-sequence models with ad-
versarial examples. CoRR, abs/1803.01128.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, and
Christopher D. Manning. 2020. ELECTRA: pre-
training text encoders as discriminators rather than
generators. CoRR, abs/2003.10555.

Gilad Cohen, Guillermo Sapiro, and Raja Giryes. 2019.
Detecting adversarial samples using influence func-
tions and nearest neighbors. CoRR, abs/1909.06872.

Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Cambridge University Press.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.

Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and
Andrew B. Gardner. 2017. Detecting adversarial
samples from artifacts.

Y. Gal. 2016. Uncertainty in deep learning.

Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a
bayesian approximation: Representing model uncer-
tainty in deep learning.

Ian Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adversar-
ial examples. In International Conference on Learn-
ing Representations.

Kathrin Grosse, Nicolas Papernot, Praveen Manoha-
ran, Michael Backes, and Patrick D. McDaniel.
2016. Adversarial perturbations against deep neu-
ral networks for malware classification. CoRR,
abs/1606.04435.

Wenjuan Han, Liwen Zhang, Yong Jiang, and Kewei
Tu. 2020. Adversarial attack and defense of struc-
tured prediction models.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recog-
nition. CoRR, abs/1512.03385.

Alex Huang, Abdullah Al-Dujaili, Erik Hemberg, and
Una-May O’Reilly. 2018. Adversarial deep learn-
ing for robust detection of binary encoded malware.
CoRR, abs/1801.02950.


Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web it 3-grams. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
3 - Volume 3, EMNLP ’09, page 1241-1249, USA.
Association for Computational Linguistics.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume I (Long Papers), pages 1875-1885, New
Orleans, Louisiana. Association for Computational
Linguistics.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
CoRR, abs/1707.07328.

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
2009. Cifar-100 (canadian institute for advanced re-
search). cifar.

Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio.
2016. Adversarial machine learning at scale. CoRR,
abs/1611.01236.

Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.
2018. A simple unified framework for detecting out-
of-distribution samples and adversarial attacks.

Christian Leibig, Vaneeda Allken, M. Ayhan, Philipp
Berens, and S. Wahl. 2017. Leveraging uncertainty
information from deep neural networks for disease
detection. Scientific Reports, 7.

Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting
Wang. 2018. Textbugger: Generating adversar-
ial text against real-world applications. CoRR,
abs/1812.05271.

Xin Li and Fuxin Li. 2016. Adversarial examples de-
tection in deep networks with convolutional filter
statistics. CoRR, abs/1612.07767.

Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
Lubomir D. Bourdev, Ross B. Girshick, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollar, and
C. Lawrence Zitnick. 2014. Microsoft COCO: com-
mon objects in context. CoRR, abs/1405.0312.

Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Er-
fani, Sudanthi N. R. Wijewickrema, Michael E.
Houle, Grant Schoenebeck, Dawn Song, and James
Bailey. 2018. Characterizing adversarial sub-

spaces using local intrinsic dimensionality. CoRR,
abs/1801.02613.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of

the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142-150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He,
and Hui Xue. 2019. Learning to characterize adver-
sarial subspaces.

Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517-522.

Pasquale Minervini and Sebastian Riedel. 2018. Ad-
versarially regularising neural NLI models to in-
tegrate logical background knowledge. CoRR,
abs/1808.08609.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and Pascal Frossard. 2016. Universal
adversarial perturbations. CoRR, abs/1610.08401.

Maximilian Mozes, Pontus Stenetorp, Bennett Klein-
berg, and Lewis D. Griffin. 2020. Frequency-guided
word substitutions for detecting textual adversarial
examples. CoRR, abs/2004.05887.

Paarth Neekhara, Shehzeen Hussain, Shlomo Dub-
nov, and Farinaz Koushanfar. 2018. Adversarial re-
programming of sequence classification neural net-
works. CoRR, abs/1809.01829.

Tong Niu and Mohit Bansal. 2018. Adversarial over-
sensitivity and over-stability strategies for dialogue
models. CoRR, abs/1809.02079.

Nicolas Papernot, Patrick D. McDaniel, Ananthram
Swami, and Richard E. Harang. 2016. Crafting ad-
versarial input sequences for recurrent neural net-
works. CoRR, abs/1604.08275.

Vyas Raina, Mark J.F. Gales, and Kate M. Knill. 2020.
Universal Adversarial Attacks on Spoken Language
Assessment Systems. In Proc. Interspeech 2020,
pages 3855-3859.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial ex-
amples through probability weighted word saliency.
In ACL (1), pages 1085-1097.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
ial rules for debugging NLP models. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 856-865, Melbourne, Australia. Association
for Computational Linguistics.

Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval
Elovici. 2017. Generic black-box end-to-end attack
against rnns and other API calls based malware clas-
sifiers. CoRR, abs/1707.05970.


Keisuke Sakaguchi, Matt Post, and Benjamin Van

Durme. 2017. Grammatical error correction
with neural reinforcement learning. CoRR,
abs/1707.00299.

Suranjana Samanta and Sameep Mehta. 2017. To-
wards crafting text adversarial samples. CoRR,
abs/1707.02812.

Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 3687-3697, Brussels, Belgium. Association
for Computational Linguistics.

Alex Serban, Erik Poll, and Joost Visser. 2020. Adver-
sarial examples on object recognition: A comprehen-
sive survey. CoRR, abs/2008.04094.

Lewis Smith and Yarin Gal. 2018. Understanding mea-
sures of uncertainty for adversarial example detec-
tion.

Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang,
and Jiayu Zhou. 2018. Identify susceptible loca-
tions in medical records via adversarial attacks on
deep predictive models. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowl-
edge Discovery &amp; Data Mining, KDD ’18,
page 793-801, New York, NY, USA. Association for
Computing Machinery.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural
networks. In International Conference on Learning
Representations.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. CoRR, abs/1706.03762.

Yicheng Wang and Mohit Bansal. 2018. Robust ma-
chine comprehension models via adversarial train-
ing. CoRR, abs/1804.06473.

Jin Yong Yoo and Yanjun Qi. 2021. Towards improv-
ing adversarial training of nlp models.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems, volume 28. Curran Associates, Inc.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2017.
Generating natural adversarial examples. CoRR,
abs/1710.11342.

Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei
Wang. 2019. Learning to discriminate perturbations
for blocking adversarial attacks in text classification.
CoRR, abs/1909.03084.

Appendix A

Training Details

For each NLP dataset, pre-trained base (12-layer,
768-hidden dimension, 110M parameters) Trans-
former encoders © were fine-tuned during train-
ing. Table A.1 gives the training hyperparameters:
learning rate (Ir), batch size (bs) and the number of
training epochs. In all training regimes an Adam
optimizer was used. With respect to hardware,
NVIDIA Volta GPU cores were used for training
all models.

Dataset Model Ir bs epochs
IMDB BERT le-5 8 2
Twitter ELECTRA le-5 8 2
AG News BERT le-5 8 2
DBpedia ELECTRA le-5 8 2
L-Bus BERT le-6 16 5

Table A.1: Training Hyperparameters

Experiments

Figure A.1 presents the impact of adversarial at-
tacks of different perturbation sizes, N on each
NLP dataset. All classification datasets’ models
underwent saliency ranked, N-word substitution
attacks described in Equation 8, whilst the regres-
sion dataset, L-Bus, was subject to a N-word con-
catenation attack as in Equation 9. For the classi-
fication tasks the impact of the adversarial attacks
was measured using fooling rate, whilst for the L-
Bus dataset task, the average output score from the
system is given. Figure A.2 gives the encoder em-
bedding space PCA residue plots for all the datasets
not included in the main text.

Table A.2 compares the impact on error sizes
(using /2 and /,, norms) and the residue plot met-
ric, N for the original text space discrete attacks
and an artificial input embedding space continuous
attack. The purpose of this table is to present the
results for the datasets not included in the main text
in Table 6.

Shttps://huggingface.co/transformers/
pretrained_models.html


07
0.8 | — Saliency Ranking — Saliency Ranking
on Random Ranking 0.8 0.6 | ~~ Random Ranking
0.6 05
0.6
2 2
£05 £ icf
é Fe & 0.4
Bae Poa Fog
3 3 5 0.
2 03 2 2
0.2
0.2 0.2
0.1
O14 — Saliency Ranking
a 00 —— Random Ranking 0.0
0 5 020615 20S 8S o 2 4 6 8 Ww 2 0 10 20 30 40
N N N
(a) IMDB (b) Twitter (c) AG News
— Saliency Ranking 4.4} — Attack
0.6) Random Ranking — Random
os 4.2
g
204 v 4.0
Ea fs}
203 S
38 2 3.8
0.2
3.6
01
34
0.0
0 5 10 15 20 25 30 35 0 2 4 6 8 10 12
N N
(d) DBpedia (e) L-Bus

Figure A.1: Adversarial Attack Impact against N-word attack.

Dataset Attack No lg error [oo error
Discrete 0.181 73.54995 4.02+0.95

IMDE Continuous | 0.111 6.73+398 0.09+0.03
. Discrete 1.201 50.24492 3.26+0.36
Twitter | Continuous | 0.676 5.354395 0.0820.03
Discrete 0.642 67.91081 3.35+0.95

AG News Continuous | 0.393 5.414419 0.09+40.04
; Discrete 1.355 57.44186 3.29+0.88
DBpedia | Continuous | 0.991 6.571353 0.0940.04
L-Bus Discrete 0.201 94.6439, 5.91+1.12
Continuous | 0.135 8.224354 0.07+0.04

Table A.2: Comparison of token level discrete attack and input embedding layer continuous PGD attack.


Average Component Size

Average Component Size

— a
10 — Original qo! — orginal a0 — original
— Attacked es idtackesi — Attacked
10°
10° 8 4 10°
a a
e 2
2 107 2
g a
ES
A 5 107}
107 fe s
& 102
$10 4
2 2
10-2 10-3 10-
10-*
0 100 200 300 400 500 600 700 800 0 100 200 300 +400 +500 +600 700 800 0 100 200 300 400 500 600 700 800
Eigenvalue Rank Eigenvalue Rank Eigenvalue Rank
(a) IMDB (b) Twitter (c) AG News
— original — original
— Attacked — Attacked
— Original
1
10° 10° 10! — Attacked
g
g s
2 a
2 2
J 107
10“? 5 2 190
8 Eg i
8 8
10? g
10-2 Ea
107}
oO 100 200 300 400 500 600 700 800 [) 100 200 300 400 500 600 700 800 t) 1000 2000 3000 4000

Eigenvalue Rank

(d) DBpedia

Eigenvalue Rank

(e) L-Bus

Eigenvalue Rank

(f) CIFAR-100

Figure A.2: Encoder Embedding space residue plot using PCA decomposition.
