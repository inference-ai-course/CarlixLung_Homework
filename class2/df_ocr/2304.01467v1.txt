OC] 4 Apr 2023:

2304.01467v1 [math.

1V

arX

A Partial Exact Penalty Function Approach for
Constrained Optimization

Nachuan Xiao * Xin Liut and Kim-Chuan Toh +
April 5, 2023

Abstract

In this paper, we focus on a class of constrained nonlinear optimization problems (NLP), where
some of its equality constraints define a closed embedded submanifold M in IR". Although[NLP]
can be solved directly by various existing approaches for constrained optimization in Euclidean
space, these approaches usually fail to recognize the manifold structure of M. To achieve better
efficiency by utilizing the manifold structure of M in directly applying these existing optimization
approaches, we propose a partial penalty function approach for[NLP] In our proposed penalty func-
tion approach, we transform|NLP]into the corresponding constraint dissolving problem (CDP) in
the Euclidean space, where the constraints that define M are eliminated through exact penaliza-
tion. We establish the relationships on the constraint qualifications between [NLP]and CDP, and
prove that [NLP] and CDP have the same stationary points and KKT points in a neighborhood of
the feasible region under mild conditions. Therefore, various existing optimization approaches de-
veloped for constrained optimization in the Euclidean space can be directly applied to solve|NLP.
through CDP. Preliminary numerical experiments demonstrate that by dissolving the constraints
that define M, CDP gains superior computational efficiency when compared to directly applying
existing optimization approaches to solve[NLP] especially in high dimensional scenarios.

1 Introduction

In this paper, we consider the following constrained optimization problem,

ree 10) (1.1)
s.t. s(x) =0, v(x) <0,

where a subset of the equality constraints s(x) = 0 satisfies the linear independence constraint quali-
fication (LICQ). Therefore, we can reshape as the following constrained nonlinear optimization

problem (NLP),
min f(x)
st. x € M:= {x € R":c(x) =O},

(NLP)

*The Institute of Operations Research and Analytics, National University of Singapore, Singapore. (xnc@lsec.cc.ac.cn). The
research of this author is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 3 grant
call (MOE-2019-T3-1-010).

State Key Laboratory of Scientific and Engineering Computing, Academy of Mathematics and Systems Science, Chinese
Academy of Sciences, and University of Chinese Academy of Sciences, China (liuxin@lsec.cc.ac.cn). Research is supported
in part by the National Natural Science Foundation of China (No. 12125108, 11971466, 11991021), Key Research Program of
Frontier Sciences, Chinese Academy of Sciences (No. ZDBS-LY-7022).

+Department of Mathematics, and Institute of Operations Research and Analytics, National University of Singapore, Singa-
pore 119076 (mattohkc@nus.edu.sg). The research of this author is supported by the Ministry of Education, Singapore, under
its Academic Research Fund Tier 3 grant call (MOE-2019-T3-1-010).


where c(x) = 0 refers to the subset of the equality constraints s(x) = 0 in that satisfies LICQ,
and u(x) = 0 refers to all the remaining equality constraints. Throughout this paper, we make the
following assumptions on|NLP

Assumption 1.1. Default assumptions

1. f : R” > Ris locally Lipschitz continuous and Clarke regular in IR”. The definition of Clarke regular
functions can be found in Definition 2.3.4] or Definition|2.2]in this paper.

2. c: R"” > R? isa locally Lipschitz smooth mapping, i.e., the transposed Jacobian of c, denoted as J,(x),
is locally Lipschitz continuous over IR".

3. The constraints c(x) = 0 satisfies LICQ over M. That is, J-(x) is full-rank for any x € M.
4. wu: IR" + RN and v: R" > R™! are locally Lipschitz smooth mappings.
5. The feasible region o denoted as K := {x € M: u(x) =0, v(x) < 0}, is nonempty.

Optimization problems that take the form of [NLP] have wide applications in data science and
many other related areas, including computing the geometric mean over a Riemannian manifold
puzdlalaa maximum balanced cut problems (224, quadratic assignment problems {10}, clustering
problems [32], etc. In the following, we briefly present some examples of[NLP]

Example 1.2 (Riemannian center_of mass problem). Riemannian center of mass problem has important
applications in pure mathematics , data science [24 |], etc. Given the Riemannian manifold M :=
{x € R" : c(x) = 0} and the samples {s; : i € [N]} from M, the Riemannian center of mass can be expressed
as
1 y ‘
min Jo ||x—s;
Ni

xeR" (1.2)

st. c(x) =0, |lx—s*||? <1,
where s* is a prefixed point on M and r > 0 is a constant.

Example 1.3 (Minimum balanced cut for graph bisection). Given an undirected graph, the minimum cut
problem aims to separate the vertices into two clusters that have the same size, while enforcing that the number
of edges between the two clusters is as small as possible. As described in [22], such a problem can be relaxed
into the following Riemannian optimization problem with additional linear constraints,

1, fet
— Ltr (XTLX
min, — git (X"Ex)
st. Diag(XX") = Im, (1.3)

Xle= 0,

where L is the Laplacian matrix of the graph, e € IR" is the vector of 1’s, and the constraints Diag(XX') = In
defines an Oblique manifold in R"™4.

As mentioned in [ll], the equality constraints c(x) = 0 in[NLP]define an embedded submanifold
M in R". Therefore,NLP|can be regarded as a optimization problem in IR" with equality constraints
[c(x);u(x)] = 0 and inequality constraints v(x) < 0, or be regarded as a optimization problem over
the embedded submanifold M with additional constraints u(x) = 0 and v(x) < 0.

According to how the constraints c(x) = 0 are treated, there are two parallel categories of opti-
mization approaches for solvingINLP] One category regards[NLPjas a standard nonlinear constrained
optimization problem in R”, where the constraints c(x) = 0 are nested as an additional set of equality
constraints, without any special treatment of its underlying manifold structures. Then a great number
of Euclidean optimization approaches (i.e. approaches designed for constrained optimization problems
in IR") can directly be applied for solving [NLP] These approaches include augmented Lagrangian
method (ALM) ina sequential quadratic programming method (SQP) (14), interior-point method


[8], filter method [15], etc. Furthermore, benefited from the rich expertise gained over the past decades
for solving constrained optimization in IR", various efficient Euclidean solvers (i.e. solvers developed
for constrained optimization in IR”), including ALGENCAN (3, [4] and Ipopt it, are developed and
widely applied in solving[NLP]

However, as the constraints c(x) = 0 define an embedded submanifold in R”, another category
of optimization approaches regard u(x) = 0 and v(x) < 0 as additional constraints over the
embedded submanifold M, i.e.,

min f(x)

xeM (1.4)
s.t. u(x) =0, v(x) <0.

Following the well-recognized framework presented in (il, existing Riemannian optimization approaches
(i.e. optimization approaches designed for optimization over Riemannian manifolds) are developed
by extending efficient Euclidean optimization approaches from IR” to M, including Riemannian SQP
method a Riemannian augmented Lagrangian method (22) 30}, Riemannian nonsmooth penalty
method [22], Riemannian Frank-Wolfe method [34], etc. Compared to directly applying Euclidean op-
timization approaches to solve[NLP] these Riemannian optimization approaches are potentially more
efficient by identifying c(x) = 0 as a Riemannian manifold and exploit the geometrical structures of
M [22].

Although these Riemannian optimization approaches are powerful alternatives for solving (1.4),
developing these Riemannian approaches relies on geometrical materials for the underlying mani-
fold, which include Riemannian derivatives, retractions and their inverse, vector transports, etc [11/9].
Determining those geometrical materials is usually challenging, see {1} |9,|29] for instances. As a result,
those geometrical materials are well understood only for several well-known Riemannian manifolds,
which limits the applications of these existing Riemannian optimization approaches only to several
well-known manifolds.

Moreover, even if those geometrical materials for M are available, developing a Riemannian _op-
timization approach from an Euclidean optimization approach by the framework described in [1] is
still not an easy task. To transfer an Euclidean optimization approach to its Riemannian versions, one
need to replace the derivatives of f, u and v by their Riemannian derivatives, introduce retractions
to keep the iterates in M, and employ vector transports to move vectors among different tangent
spaces of M. As a result, existing Riemannian optimization approaches are limited, especially when
compared to the increasing number of Euclidean optimization approaches. Furthermore, in contrast
to various available efficient Euclidean solvers (i.e., solvers developed based on Euclidean optimiza-
tion approaches), there is no publicly released Riemannian solvers for[NLP] Considering the great
effort needed in developing Riemannian optimization approaches and their corresponding solvers,
it is challenging for these approaches to follow the progress and advances in nonconvex constrained
optimization in the Euclidean space.

In brief, directly solving [NLP]by Euclidean optimization approaches enjoys the great convenience
from various available highly efficient algorithms and solvers. On the other hand, solving
through Riemannian optimization approaches can achieve better efficiency by exploiting the struc-
ture of M, while the involved geometrical materials may impose great difficulties in developing
efficient Riemannian optimization solvers. Therefore, it is natural to consider how we can combine
the advantages from both the Euclidean optimization approaches and Riemannian optimization ap-
proaches. More precisely, we are motivated to ask the following question:

Can we directly apply Euclidean optimization solvers for while achieving better efficiency
through exploiting the structure of the embedded submanifold M defined by constraints c(x) = 0?

Very recently, shows that the following smooth optimization problem
min f(x), s. t. c(x) =0, (1.5)

xeER"
is equivalent to the unconstrained minimization of the following constraint dissolving function,

f(Alx)) +E jle(x) IP. (1.6

3


Here 6 > 0 is the penalty parameter, and the constraint dissolving mapping A should satisfy the
following assumptions.

Assumption 1.4. Blanket assumptions on A
1. A: R” > R" is locally Lipschitz smooth in IR";
2. A(x) = x holds for any x € M;

3. The Jacobian of c(A(x)) equals to 0 for any x € M. That is, J4(x)Jc(x) = 0 holds for any x € M,
where J 4(x) € IR"*" denotes the transposed Jacobian of A at x.

The construction of the constraint dissolving mapping A for various important manifolds can
be found in Section 4]. As proven in [35], and have the same stationary points in a
neighborhood of M with appropriately selected penalty parameter 6. Moreover, shows that the
construction of A can totally avoid the needs for any geometrical materials of M. Therefore, various
unconstrained optimization approaches can be directly applied to solve through (1.6).

Our motivation comes from the constraint dissolving approaches for Riemannian optimization
35]. The formulation of the constraint dissolving function in motivates us to consider replacing
f, wand v in[NLP]by their corresponding constraint dissolving functions, and remove the constraints
c(x) = 0 from[NLP]through exact penalization. Thus we arrive at the following constraint dissolving
problem (CDP):

min h(x) = f(ACx)) + § lle(2)|?

xEIR” 2
st. aij(x) == uj(A(x)) +S |Ie(x)||? =0, 1 € [Ne], (CDP)

8j(x) = 0(A(x)) + FZ Ile(X)P- <0, 7 € [NIL
where f, {t;} and {y;} are all non-negative penalty parameters. As the construction of the constraint
dissolving mapping A in|CDP]is independent of the geometrical materials of M [35], we can directly
apply various existing Euclidean solvers to[|CDP} without any computation of geometrical materials
such as retractions, vector transports, Riemannian differentials, etc. Furthermore, compared to[NLP]
[CDP]has eliminated the constraints c(x) = 0. Hence solving the resulting problem[CDP]by Euclidean
solvers can potentially become more efficient.

However, existing constraint dissolving approaches are only developed for minimizing
the objective function f over M without any additional constraints. How to establish the equivalence
between [NLP] and [CDP] especially in the presence of nonsmooth objective function and additional
constraints, remains to be worked out. More importantly, as[NLP]has different constraints as[CDP
the relationships between [NLP] and [CDP] on constraint qualifications, stationary points, and KKT
points, should be carefully analyzed.

Contribution In this paper, we propose the constraint dissolving approach for[NLP|by transform-
ing it into[CDP] where the constraints c(x) = 0 is eliminated. We first prove the equivalence of the
constraint qualifications between[NLP]and [CDP] in the sense that a broad class of constraint qualifi-
cations for[NLP]imply that all KKT points of[CDPlare its first-order stationary points. Moreover, we
prove the equivalence between[NLP]and|CDP]in the sense that they have the same stationary points
and Karush-—Kuhn-Tucker (KKT) points over the feasible region under mild conditions. Addition-
ally, we prove sharper results on the equivalence for the special case where Ng = 0 in[NLP] These
results on the equivalence between|NLPland|CDP]demonstrate that transforming [NLPlinto[CDP]pre-
serves the validity of constraint qualifications, while keeping the stationary points and KKT points
of [NLP] unchanged. Therefore, we can solve[NLP]by directly employing various existing Euclidean
approaches to solve|CDP

We perform numerical experiments on Riemannian center of mass problems and minimum bal-
anced cut problems to demonstrate the strengths of our proposed constraint dissolving approach.
As a baseline, we compare our proposed constraint dissolving approach to the above-mentioned


Euclidean optimization approaches by simply considering c(x) = 0 as an extra set of equality con-
straints. Preliminary numerical experiments illustrate that applying existing Euclidean optimization
approaches to|[CDP|can achieve better computational efficiency, especially in high dimensional cases.
These numerical experiments further demonstrate that solving [NLP] through[CDP]can enjoy similar
advantages offered by existing Riemannian optimization approaches, while avoiding the difficulties
in determining the geometrical materials of the underlying manifolds and designing Riemannian
optimization solvers.

Organization The outline of this paper is as follows. In Section 2, we fix the notations, definitions,
and constants that are necessary for the proofs in this paper. We establish the theoretical properties
of [CDP]and discuss the special case with only inequality constraints. The proofs for the theoretical
properties of[CDPlare presented in the appendix. In Section 4, we present several illustrative numer-
ical examples to show that[CDP]allows the straightforward implementations of various Euclidean
optimization approaches. We conclude the paper in the last section.

2 Notations, definitions and constants

2.1 Notations

Let range(A) be the subspace spanned by the column vectors of matrix A, while ||-||; and ||-||
denote the ¢;-norm and ¢2-norm of vectors or operators, respectively. The notations diag(A) and
Diag(x) stand for the vector formed by the diagonal entries of matrix A, and the diagonal matrix with
the entries of x € R” as its diagonal, respectively. We denote the smallest and largest eigenvalues of
A by Amin(A) and Amax(A), respectively. Besides, 7;(A) refers to the /-th largest singular value of A,
and Omin(A) refers to the smallest singular value of A. Furthermore, for any matrix A € IR’*?, we
use At to denote the pseudo-inverse of A. That it, AT € IR?*" satisfies AA'A = A, ATAAt = At,
and both AtA and AA’? are symmetric [16].

For the submanifold M, we denote 7, as the tangent space of M at x, which can be expressed
as Tx := {d € R": J-(x)'d = 0}. Moreover, Ny denotes the normal space of M at x, ie., Ny :=
{d © R": d'u =0, Vu € Tx}. From the definitions of Jz and Ny, it holds that Vy. = range(J-(x)).
Additionally, for any x € IR", we define the projection to M as proj(x,M) := argmin,< 4 ||x — y|-
It is worth mentioning that for any x € IR", the optimality condition of the above problem leads to the
fact that for any y € proj(x,M), it holds that x — y € range(J-(y)). Furthermore, dist(x, M) refers to
the distance between x and M, i.e. dist(x,M) :=infyewy |[x — y|].

The transposed Jacobian of mappings A and c are denoted as J4(x) € IR"*" and J,-(x) € R"*?,
respectively. That is, let c; and A; denotes the i-th coordinate of the mapping c and A respectively,
then J, and J,4 are defined as

Jce(x) := [Ve1(x),...,Vep(x)], and Ja(x) = [VAj(x),..., VAn(x)].

Similarly, we define J, and Jy as the transported Jacobian for u and v, respectively. Moreover, for a
given subset G of IR", we define J4(x)G := {Ja(x)d:d © G}. We set

AK(x) = AAC A(a) +++),
k times
for k > 1, and define A°(x) := x, A®(x) := an A‘ (x). Furthermore, we denote ¢(x) := f(A(x))
— +00

in the rest of this paper. Additionally, we denote the closed ball at x with radius p as By,» := {y € R":
ly — x|| < p}. Finally, for any positive integer n, we use the notation [n] := {1,2,...,n}. We denote
Ka as the feasible region of [CDP] i.e. K4 := {x € R" : a(x) = 0,0(x) < 0}, and denote F(x) as the
active index set of the inequality constraints in[NLP] i.e. F(x) := {j € [Nj] : vj(x) = 0}. Similarly, we
denote the active index set of the inequality constraints in[CDPlas F4(x) := {j € [Ny] : 6j(x) = Of.
Note that for any x € M, we have F(x) = F(x), since j(x) = v;(x) holds for any x € M and
j € [N7]. Furthermore, it is easy to verify that K C Ky.


2.2 Preliminaries
2.2.1 Subdifferential and regularity
Definition 2.1. The generalized directional derivative of f at x € IR" in the direction d, denoted by f*(x,d),

is defined as
fly + td) — Fly) (2.1)

f*(x,d) = limsup F

yx,tl0
The generalized gradient or the (Clarke) subdifferential of f at x € IR"*?, denoted by Of (x), is defined as
Of (x) := {w ER": (w,d) < f*(x,d), forall d € R"}. (2.2)

Definition 2.2. We say that f is (Clarke) regular at x € IR" if for every direction d, the one-sided directional

derivative 4
in L+H) Fe)

f'(x,d) =1 2.3)

exists and f'(x,d) = f*(x,d).

2.2.2 Constraint qualifications and optimality conditions

In this subsection, we present the definitions on the constraint qualifications for the constrained
optimization problems and (CDP), respectively. It is worth mentioning that both[NLPland[CDP]
are constrained optimization problems in IR”, hence their constraint qualifications can be defined in
the same manner. Recall that K and K.4 denote the feasible region of [NLP] and[CDP]respectively, we
have the following definitions on the constraint qualifications for and (CDP).

Definition 2.3. For any closed cone C in IR", its polar cone C° is defined as
C° := {d ER": (d,w) <0, Wwe C}.

Definition 2.4. For any x € K, the (Bouligand) tangent cone with respect to K is defined as

Tic(x) := {aeR’ Dax, EK x, te 10, 8t.d= lim a}
ko+oo tk

Similarly, for any x € Ky, the (Bouligand) tangent cone with respect to K , is defined as

Tk, (x) = {4 ER": 4x, €K, 3x, & 10,st.d= lim sah
k-+00 ty

Definition 2.5. For any x € K, the linearizing cone with respect to K is defined as
The" (x) = {d ER": (d, Vei(x)) = 0, (d, Vuj(x)) = 0, (d, Voj(x)) < 0,1 € [p],i € [Ne], j € F(x)}.
Similarly, for any x € Ky, the linearizing cone with respect to Ky is defined as
Tein(x) = {d ER": (d, Viaii(x)) = 0, (d, VO;(x)) < 0,1 € [Ne], j € Fa(x)}

From Definition [2.5] it can be shown that for any x € K, T(x) C Tji"(x) and

(Tye" (x e1Vei(x )+ AjiVuj(x) + uj V0;(x) N ;

Similarly, it can also be shown that for any x € Ky, Tx, (x) C Ten(x) and

(Ten (x) r= { y2 AiVii(x) + YO pjVG,i(x ) [AE RM, we RW yj =0Vi ¢ Falx )}.
i€ [Ng] j€(N7]


Definition 2.6. For any given x € K, we define the following constraint qualifications at x for

¢ The linear independence constraint qualification (LICQ) with respect to|NLP\holds at x if {Vcj(x) : 1 €
[p]} U{Vuj(x) : 7 € [Ne]} U{Vo;(x) : | € F(x)} isa linearly independent set in R".

¢ The Mangasarian-Fromovitz constraint qualification (MFCQ) with respect to[NLP\holds at x if {Vici (x) :
1 € [p]} U{Vuj(x) : i © [Ne] } is linearly independent and there exists d € IR" such that

(d,Vcj(x)) = 0, V1 € [p],
(d, Vuj(x)) =0, Vi € [Ne],
(d, Vuj(x)) <0, Vj € F(x).

° The GCQ with respect to[NLP]holds at x if Tx(x)° = T,li"(x)°.
Similarly, for any given x € Ky, we define the following constraint qualifications at x for

¢ The linear independence constraint qualification (LICQ) with respect to|CDP\holds at x if {Vuj(x) :i €
[Ne]} U {VG;(x) : 7 © F(x)} is a linearly independent set in IR".

¢ The Mangasarian-Fromovitz constraint qualification (MFCQ) with respect to[|CDP\holds at x if {Vii;(x) :
i € [Ng]} is linearly independent and there exists d € IR” such that

(d, Vii;(x)) = 0, Vi € [Ne],
(d,V6j(x)) <0, Vj € Fa(x).

¢ The GCQ with respect to(CDP\holds at x if Tk. ,(x)° = Teen (x)?

It is worth mentioning that the constraint qualifications in Definition |2.6] are equivalent to the
Riemannian constraint qualifications defined in i7, Definition 3.12], as discussed in |Z, Remark 3.13].
Furthermore, based on the definition of tangent cones in Definition we define the stationary
points for[NLP]and[CDPlas follows.

Definition 2.7. We say x € K is a first-order stationary point o if it satisfies

0 € Of (x) + Tx(x)°. (2.4)
Moreover, x € K-4 isa first-order stationary point o if
0 € dh(x) + Tx, (x)°. (2.5)

2.2.3 KKT conditions

In this subsection, we introduce the definition of KKT conditions and optimality conditions for

both and
Definition 2.8. For any given x € IR", the Lagrangian o is defined as

Lyip(x,A,p,) 2= f(x) +e 'e(x) +Alu(x) +p! o(x), (2.6)
whose the partial subdifferential with respect to variable x is defined as
OxLyip(x,p,A,p) = Of(x) + YE prVer(x) + YO ANU (x) + YO pjVo;(x). (2.7)
le[p] i€ [Ng] j€[Ni]

Moreover, for any given x € IR", the Lagrangian o is defined as

Lepp(x,A,p) = h(x) + Al a(x) + pl O(x), (2.8)
whose partial subdifferential with respect to variable x is defined as
i€[Ng] j€[N1]


Based on the Lagrangians of and we present the definition of KKT points as follows.
Definition 2.9. For any given x € M, we say x is a KKT point to (with multipliers p, A and y) if

0 € 0xL£nzrp(x,p,A, K),

- (2.10)
c(x) =0, u(x) =0, v(x) <0, w>0, pw’ v(x) = 0.
Definition 2.10. For any given x € IR", we say x is a KKT point to (with multipliers A and y) if

0 € 0,Lepp(x,A, HL),

ii(x) =0, 0(x) <0, n> 0, w' a(x) =0. (2.11)

2.3. Constants

In this paper, for any given x € M, we choose a positive scalar p; such that

px i= argmax {p inf {cmiJe(¥)) y € Buy} > semin( (x).
0<p<l

Then for any given x € M, based on the definition of px, we define Oy := {y € X : |ly—x|| < px}
and set several constants as follows:

© Ox, 2= Amin Je(x));
° Myf >= SuPyco, UP wear(A(y)) Ill:
© Mxu ‘= supyco, |lJu(y) I:
© Mx = supyce, |Jo(y) |;
° Mx, := supyco, llJe(y) i

© M4 != sup,co, l\a(y)|k

Poly) —JelZ)Il.

s Lyc = SUPy 2cO,y#z ~— |ly=zl 7

_ Jaty)—Jal2)I.
° Ly A — SUPy 7cO,,y#z ne

Z)]c(A(z
Lab = SUP cc yee AWAD La LACDI

Itis worth mentioning that all the aforementioned constants are independent of the penalty parameter
B. Moreover, for any given x € IR” with multipliers (A, 1), we set My ayo = May,¢ + ||A|]] Mau +
|| ||, Mx,o. Furthermore, based on these constants, we set

min J ox, —
© On = {y ER": ly —x]] Sex}:
+ Tiece {yeR": lvl <a“

© Or Uxem Ox;
e fe) = Uxem Ow

Note that ex < a onc - automatically implies that ex < 1 < Mx ,. Additionally, it is
worth vventioning ‘that ‘for any given x € M, Asoumption guarantees that ox, > O and ex > 0.
Furthermore, from the definition of O, we can conclude that Ox C Ox C Ox holds for any given
x € M,and M lies in the interior of .


3 Theoretical Properties

In this section, we present the theoretical properties of We first present the explicit expres-
sion of dh(x) in the following proposition. We omit its proof since it can be established by direct
calculation.

Proposition 3.1. For any x € R", it holds that

dh(x) = Ja(x)Of(A(x)) + Blc(x)e(x), (3.1)
Viti(x) = Ja(x)Vui(A(x)) + Tie(x)e(x), (3.2)
VGj(x) = Ja(x)Voi(A(x)) + YiJe(x)e(x). (3.3)

Moreover, for any x € M,i € [Ng] and j € [Ny], it holds that
dh(x) = Ja(xof(x), Viti(x) = Ja(x) Vui(x), and VOj(x) = Ja(x)V0;(x).

3.1 Constraint qualifications

In this subsection, we analyze the relationship on the constraint qualifications between[NLP]and
We show that LICQ and MFCQ with respect to[NLPlimply the validity of the corresponding
constraint qualifications with respect to[CDP] while GCQ with respect to[NLP]implies that the KKT
points of[CDPJare all first-order stationary points of [CDP] Fugure[I]summarizes the main results in
this subsection.

LICQ for => MFCOQ for => GCQ for

ine | lag

LICQ for — = MFCQ for —* for

Figure 1: Relationships on the constraint qualifications between and The fact that
can be regarded as a constraint qualification for is shown in Theorem[3.6] Here “Prop.” is the
abbreviation for “Proposition”.

The proofs for Proposition[3.2|— PropositionB.5Jare presented in Section[A.2]

Proposition 3.2. For any x € K, suppose the LICQ with respect to holds at x, then the LICQ with
respect to holds at x.

Proposition 3.3. For any x € K, suppose the MFCQ with respect to holds at x, then the MFCQ with
respect to holds at x.

Lemma 3.4. For any x € K, it holds that

Tee (x) = Jax) TEx), T(x)? N range(Ja(x)) = a(x) TE"(x)°, (3.4)
Tk (x) C Jax)! Tic, (x )) Tk, (*)° A range(Ja(x)) © Ja(x)Tx(x)°. (3.5)
In the rest of this subsection, we aim to illustrate that the GCQ for implies that all the

KKT points of are first-order stationary points of We first show the relationships between
Tk, (x) and Ten (x) when GCQ for|NLPlholds at x €« M.

Proposition 3.5. For any x € K, suppose GCQ for[NLP]holds at x, i.e., Tic(x)° = Tji"(x)°. Then it holds
that

Tra (x)? M range(Ja(x)) = Teil (x)° 0 range(Ja(x)). (3.6)


As mentioned in various existing works [25], GCQ is usually recognized as one of the weakest
constraint qualifications in the sense that a great number of constraint qualifications for[NLP]leads to
the validity of GCQ for[NLP] Interested readers could refer to for more details on the relationships
among various constraint qualifications. In the following theorem, we illustrate that Proposition 3.5]
ensures that KKT conditions holds at any first-order stationary point of [CDP] We present the proof
for TheoremB.6]in Section[A.2]

Theorem 3.6. For any x € K, suppose Tx, (x)° MN range(J4(x)) = Teen (x)? MN range(J4(x)), then x isa
first-order stationary point o if and only if x is a KKT point of{CDP}

3.2 Stationarity

In this subsection, we aim to analyze the relationship between[NLP]and in the aspect of their
first-order stationary points and KKT points. Figure[2]summarizes the main results of this subsection.
In addition, the proofs for Theorem[3.7] TheoremB.8]and Theorem[3.10]are presented in Section

FOSPs of|NLP FOSPs of|CDP}

KKTs of [NLP KKTs of|CDP|

Figure 2: Relationships on the first-order stationary points (FOSPs) and KKT points (KKTs) between
and Here “Theo.” is the abbreviation for “Theorem”.

Theorem 3.7. For any given x € K, it holds that
1. if x is a first-order stationary point o then x is a first-order stationary point of[NLP}
2. x isa KKT point to if and only if x is a KKT point to|CDP

Applying Euclidean optimization approaches to solve|CDP]usually yields an infeasible sequence
that converges to a KKT point of [NLP] Therefore, it is of great importance to analyze the relationship
between the KKT points of and the KKT points of The following theorem illustrates that
under mild conditions with sufficiently large 8, any KKT point y to[CDP]in QO, for some x € Misa
KKT point of [NLP] Note that the difference between the result here and that of Theorem is that
we do not assume the KKT point y € K in TheoremB.8]

Theorem 3.8. For any given x € M, suppose y € Ox is a KKT point o with multipliers A and j, then
it holds that

dist(0,0,Lcpp(y,A, 1)

Orc AL, AMy, A, (3.7)
= la (B+ OL it LEH) — = | Mle) I
TW FI) ( ic(Ne] SS je( ) Or
Furthermore, suppose
32Ly,a(My,4 +1)Mya,
Bt+ ye AjT + yi KY, = — > 3 — (3.8)
xc

i€[Nez] i€[Ni]
holds at y, then y is a KKT point of[NLP|

Remark 3.9. Theorem|3.8]provides a theoretical lower-bound for B that depends on the multipliers A and 1.
In various existing Euclidean approaches, the multipliers {A;}ie[n,] are assumed or proved to be uniformly
bounded. In these cases, we can immediately achieve a theoretical lower-bound for B.

On the other hand, when the uniform boundness of the multipliers {A;} ic [Nz] 1S not guaranteed, some
existing works can estimate an upper-bound for the multipliers Therefore, we can adaptively adjust B to

10


force it to satisfy the theoretical lower-bound in Theorem and hence ensure that the applied optimization
approach can find a KKT point of[NLP|

Furthermore, for the special cases where Ng = 0 (i.e., no additional equality constraints in|NLP), we can
prove that the theoretical lower bound for B in{CDP]is independent of the multipliers. We put the detailed
discussion in Section 3.3.

As illustrated in [35], we know that the constraint dissolving operator A can quadratically reduce
the feasibility violation of any infeasible point y € Ox, hence A®(y) is feasible. In the following theo-
rem, we prove the relationships between y and A®(y) in terms of the function values and derivatives,
which is of great importance in characterizing the properties of[CDPlat those infeasible points in Ox.

Theorem 3.10. For any given x € K.4, suppose y € Ox isa KKT point of(CDP|with multipliers A and 1, and

8M JA, (M A+I1)L b
B+ Yo A+ Yo yy = So. (3.9)

i€ [Ne] je[Ni] Ox,

Then it holds that
Lepply,A,#) = Lcpp(A(y),A, 2),

3.10
Lepply,A, bu) = L£cpp(A®(y),A, H)- eal)

3.3 Inequality Constrained Case

In this subsection, we consider the special case of NLP]where the additional constraints u(x) = 0
are absent. More precisely, we analyze the equivalence between and under the following
assumption.

Assumption 3.11. Ne = 0 in[NLP|

As the additional equality constraints u(x) = 0 are absent, the Lagrangian of is denoted
as Lcpp(x,p) for simplicity in this subsection. Moreover, we make the following condition on the
threshold value for B and {7;} for a given x € M, which is independent to the corresponding multi-

plier p.

Condition 3.12. Given any x € M, the parameters B and {7yj}j<[N,] in satisfy the following inequali-
ties,
64M, ¢( Mya +1) (Ly pb + Ox,cL
B > x,f ( x,A i x,b x,C x,A) ; taf 7
O%x,c EIN]

y

Remark 3.13. When M is compact, there exists a finite set ZC M such that M C Uxyer Ox. Therefore,
we can choose uniform positive lower bounds for oy,: and €x, while finding uniform upper bounds for all the
other aforementioned constants. Specifically, when M is compact, we can choose a uniform upper bound for
the penalty parameters B and {7;} for(CDP\under Assumption

5, 32Lx,AMxo(Mz,a +1)

(3.11)

We first study the equivalence on constraint qualifications. The following lemma illustrates that
for any x € K with 6 and {7;} satisfying Condition B.12} we can obtain sharper results on the rela-
tionship between Jj (x) and 7x, (x). The main results of this subsection is summarized in Figure]
The proof for Lemma Theorem[3.15] Theorem[3.16] and Corollary[3.17)can be found in Section

Lemma 3.14. Suppose Assumption holds, and the penalty parameters B and {7y;} in{CDP]|satisfy Condi-
tion then it holds for any x € K that

Ti(x) = Ja(x) "Tic, (x), (3.12)
Tic, (x)° Nrange(Ja(x)) = Ja(x)Tic(x)?. (3.13)

In the following theorem, we use Lemma to obtain a sharper result on the equivalence of
first-order stationary points between and in the sense that the lower bound for B and {7;}
are independent of the multiplier p.

11


FOSPs of(NLD| FOSPs of|CDP|

Theo.
KKTs of KKTs of

Figure 3: Relationships on the first-order stationary points (FOSPs) and KKT points (KKTs) between
NLP] and under Assumption [3.11] Here “Theo.” is the abbreviation for “Theorem”, while
“Coro.” is the abbreviation for “’Corollary”.

Theorem 3.15. For any given x € K, suppose Assumption[3.1 holds, and the penalty parameters B and {7;}
in satisfy Condition Then x is a first-order stationary point o if and only if x is a first-order

stationary point of{CDP|

Based on Theorem[3.15] we prove that the equivalence of KKT points in TheoremB.8]can be further
improved to the equivalence of first-order stationary points.

Theorem 3.16. For any given x € K, suppose Assumption holds, the penalty parameters B and {;} in
ICD P satisfy Condition Ify € Ox isa first-order stationary point o then y is a first-order stationary
point of(NLP}

Corollary illustrates that for any x € K, and have the same KKT points in Ox,
where the lower bound for f and {7;} are independent of the multiplier py.

Corollary 3.17. For any given x € K, suppose Assumption holds, the penalty parameters B and {y;} in
|CDP\satisfy Condition Then y € Ox is a KKT point o if and only if y is a KKT point of{NLP|

The next corollary follows directly from Theorem hence we omit its proof for simplicity.

Corollary 3.18. For any given x € Ky, suppose Assumption holds, the penalty parameters B and {~y;}
in|CDP|satisfy Condition If y € Oy isa KKT point o with multiplier j, then it holds that

Lepply, HF) = L£cpp(Aly), 2),

3.14
Lepply,t) > Lepp(A®(y), #): el)

4 Numerical Experiments

In this section, we present preliminary numerical results on applying various existing efficient
solvers to through the constraint dissolving approach

4.1 Basic settings

All the numerical experiments are performed on a server with Dual Intel(R) Xeon(R) Gold 6242R
CPU @ 3.10GHz x20 running Python 3.8 under Ubuntu 20.04. Moreover, we choose various existing
solvers that are developed based on different methods, including the interior point solvers TRCON
and Ipopt [31], the ALM-based solver ALGENCAN [3], and the solvers PSQP (http://www. cs.cas.cz/“luksan/s
and SLSQP that are developed based on sequential programming methods. We choose the initial
points to be the same for all the solvers, and terminate these solvers when the running time exceeds
1200 seconds. The implementations details for each solver are presented below.

e ALGENCAN (version 3.1.1): ALGENCAN is a FORTRAN package that employs GENCAN as
its subroutine. We call the ALGENCAN solver through its Python interface provided by the
pyOpt package |24]. We set the “epsopt = 10-6” while keeping all the other parameters as their
default values.

12


¢ IPOPT (version 3.14.5): Ipopt solver is written in C++ and we run it through its python interface
provided in Cylpopt package. We set “hessian_approximation = limited-memory” to adopt the
(L)BFGS method for an approximated Hessian. Moreover, we set “tol = 10~°” and keep all the
other parameters as their default values.

PSQP: PSQP solver is programmed in FORTRAN and we run it through the Python interface
provided by the pyOpt package. We set “TOLG = 10~®” and keep all the other parameters as
their default values.

SLSQP: SLSQP is a FORTRAN solver provided by SciPy package, together with its Python in-
terface integrated in SciPy-optimize package. We set “tol = 10-6”, “ftol = 10-5” and keep all the
other parameters as their default values.

¢ TRCON: The solver TRCON is developed purely in Python based on the trust-region method.
We set “hess = None” to employ the BFGS strategy to compute approximated Hessians for the
objective function, fix “gtol = 10~°” and keep all the other parameters as their default values.

It is worth mentioning that under our settings, all the tested solvers only utilize the first-order deriva-
tives of the objective function and constraints. Moreover, we employ the automatic differentia-
tion package autograd to compute the derivatives automat-
ically. In the numerical results, we report the function value obtained, as well as the feasibility
and stationarity of the result % obtained by these test solvers. Here the feasibility is measured by
\|2() || + |]c(€) || + |max{o(%),0}||, and the stationarity is measured by || VxLnzip(%,6,A, ji) ||, where
p,A,fi=  — argmin —|Lyrp(3,p, 4,1).
peR?,AERNE weR)!

4.2 Riemannian center of mass

In this subsection, we test the numerical performance of our constraint dissolving approach on
Example[L.2|by applying all the tested solvers to solve[CDP} and compare it with the baselines (i.e.
directly applying the tested solvers to solve[NLP). In Example[1.2] we choose the submanifold M as
the symplectic Stiefel manifold Sing := {X € R™*4 : X' QmX = Qy}. Moreover, we choose N = 1000
and randomly generate s* and {s;}1<i<j on the symplectic manifold. For the Riemannian center of
mass problem in Example[1.2] the corresponding[CDPlcan be expressed as

N
min, 3p LAG) —sil?-+ 5 lee?

xElR”

st. ||A(x) —s*|/? <1,

(4.1)

where we fix 6 = 1. Moreover, we set the initial point as s* in each test instance for all the compared
solvers.

The numerical results are presented in Table[I] From that table, we can conclude that applying
existing solvers to|CDP]is significantly more efficient than directly apply those solvers to|NLP} For
some large-scale instances, the improvement could be 10-40 times for some solvers. Moreover, we
notice that SLSQP solver encounters errors due to the ill-conditioned LSQ subproblems when it is
directly applied to[NLP] Furthermore, our results report that ALGECAN is unstable for almost all the
test instances when it is directly applied to[NLP]

4.3 Minimum balanced cut for graph bisection

In this subsection, we perform the numerical experiments on Example[1.3] where we choose the
matrix L as the Laplacian for a given graph G, i.e. L = D— A, where D € R”*" is the diagonal
matrix of the degree of the vertices, and A € IR”*” is the adjacency matrix. In our numerical exper-
iments, we randomly generate the graph G by the Erdés-Rényi model through the build-in function
G = Erdos_Renyi(m,), where any two nodes in G are connected with probability p independently.

13


Table 1: A comparison between and on Riemannian center of mass problems.

Test problems

Solvers

ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON

ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSOP
SLSQP
TRCON
ALGENCAN
IPOPT
PSOP
SLSQP
TRCON

m = 50
m = 100
(gr) =
(10, 0.01)
m = 150
m = 200
q=6
q = 10
(m,r) =
(100, 0.01)
q=14
q=18
r = 0.001
r=0.01
(m,q) =
(100, 10)
r=0.1
r=1

ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSOP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON

Function value Substationarity Feasibility CPU time (s)
CDP] = [NEP
5.44e+00 5.44e+00 4.07e-08 3.75e-07 4.77e-11 1.20e-09 0.37 3.39
5.44e+00 5.44e+00 9.95e-07 1.68e-07 1.00e-08  1.00e-06 0.61 9.77
5.44e+00 5.44e+00 7.01e-07 3.46e-07 7.42e-11 3.55e-12 0.42 1.49
5.44e+00 - 3.80e-07 - 5.87e-06 - 3.68 -
5.44e+00 5.46e+00 5.99e-07 7.06e-04 2.16e-12 1.11e-07 0.59 30.39
5.60e+00 - 3.95e-07 - 6.88e-11 - 1.03 -
5.60e+00 5.60e+00 4.88e-07 2.14e-07 1.08e-08  1.00e-06 1.02 30.11
5.60e+00 5.60e+00 8.46e-07 2.11e-07 2.20e-09 4.10e-12 0.56 4.43
5.60e+00 - 4.51e-07 - 8.22e-06 - 30.93 -
5.60e+00 5.74e+00 9.32e-07 5.81e-02 1.33e-12 6.45e-06 1.11 45.36
5.84e+00 - 5.71e-07 - 3.77e-09 - 1.13 -
5.84e+00 5.84e+00 5.48e-07 3.51e-07 9.98e-09 1.00e-06 1.57 75.25
5.84e+00 5.84e+00 5.32e-07 7.18e-07 5.30e-11 4.18e-11 1.58 10.07
5.84e+00 - 6.07e-07 - 6.21e-06 - 112.83 -
5.84e+00 6.01e+00 9.83e-07 6.73e-02 1.62e-12 1.85e-05 2.53 63.93
5.88e+00 - 4.69e-07 - 4.99e-11 - 1.47 -
5.88e+00 5.88e+00 2.64e-07 3.87e-08 1.14e-08  1.00e-06 1.91 57.91
5.88e+00  5.88e+00 6.70e-07 4.08e-07 1.44e-10 6.97e-13 2.61 17.54
5.88e+00 - 6.18e-07 - 7.40e-06 - 256.97 -
5.88e+00  6.06e+00 9.54e-07 7.12e-02 2.06e-11 2.32e-05 4.89 104.14
3.42e+00 - 1.72e-07 - 2.65e-09 - 0.60 -
3.42e+00 3.42e+00 2.42e-07 1.06e-07 1.00e-08  1.00e-06 0.65 6.33
3.42e+00 3.42e+00 2.05e-07 6.85e-07 6.24e-10 9.60e-12 0.21 0.66
3.42e+00 - 4.05e-04 - 7.90e-06 - 3.98 -
3.42e+00 3.42e+00 7.54e-07 7.46e-04 1.42e-12 1.11e-15 0.76 2.74
5.60e+00 - 3.95e-07 - 6.88e-11 - 1.03 -
5.60e+00 5.60e+00 4.88e-07 2.14e-07 1.08e-08 1.00e-06 1.02 30.11
5.60e+00 5.60e+00 8.46e-07 2.11e-07 2.20e-09 4.10e-12 0.56 4.43
5.60e+00 - 4.51e-07 - 8.22e-06 - 30.93 -
5.60e+00 5.74e+00 9.32e-07 5.81e-02 1.33e-12 6.45e-06 1.11 45.36
8.08e+00 - 4.09e-07 - 1.90e-10 - 1.82 -
8.08e+00  8.08e+00 3.42e-07 4.52e-08 1.13e-08  1.00e-06 1.92 56.56
8.08e+00  8.08e+00 7.33e-07 3.03e-07 3.18e-11 1.11e-11 1.80 18.54
8.08e+00 - 4.12e-07 - 6.21e-06 - 110.97 -
8.08e+00  8.27e+00 4.87e-07 7.78e-02 1.98e-12 4.92e-06 2.21 114.87
9.98e+00 - 3.76e-07 - 4.77e-10 - 2.04 -
9.98e+00 9.98e+00 6.74e-07 3.63e-08 1.03e-08  1.00e-06 2.33 86.40
9.98e+00 9.98e+00 8.64e-07 2.73e-07 1.59e-10 8.89e-13 2.34 52.86
9.98e+00 - 1.84e-07 - 6.28e-06 - 219.27 -
9.98e+00 1.02e+01 9.72e-07 7.83e-02 1.27e-11 4.48e-05 4.13 293.02
5.78e+00 - 4.75e-08 - 1.13e-10 - 1.78 -
5.78e+00 5.78e+00 9.99e-07 7.01e-10 1.04e-08  1.00e-05 1.68 38.36
5.78e+00 5.78e+00 9.90e-07 6.45e-07 2.81e-10 2.70e-12 1.62 8.04
5.78e+00 - 5.52e-07 - 9.97e-06 - 56.10 -
5.78e+00  5.83e+00 6.83e-07  1.11e-01 3.97e-12 2.03e-05 6.65 45.06
5.60e+00 - 3.95e-07 - 6.88e-11 - 1.03 -
5.60e+00 5.60e+00 4.88e-07 2.14e-07 1.08e-08  1.00e-06 1.02 30.11
5.60e+00 5.60e+00 8.46e-07 2.11e-07 2.20e-09 4.10e-12 0.56 4.43
5.60e+00 - 4.51e-07 - 8.22e-06 - 30.93 -
5.60e+00 5.74e+00 9.32e-07 5.81e-02 1.33e-12 6.45e-06 1.11 45.36
5.35e+00 - 1.10e-06 - 9.34e-10 - 0.52 -
5.35e+00 5.35e+00 8.12e-07 7.60e-08 1.00e-08  1.00e-07 0.57 25.64
5.35e+00 5.35e+00 3.69e-07 3.18e-07 2.00e-11 2.06e-10 0.18 2.19
5.35e+00 - 4.92e-05 - 8.65e-06 - 7.40 -
5.35e+00 5.51e+00 8.62e-07 6.38e-03 5.49e-12 1.23e-05 0.46 43.00
4.85e+00 - 6.35e-07 - 3.45e-09 - 0.82 -
4.85e+00 4.85e+00 8.07e-07 1.36e-07 9.96e-09  1.06e-08 0.94 49.84
4.85e+00 4.85e+00 9.32e-07 6.70e-07 5.93e-11 3.51e-10 0.20 2.67
4.85e+00 - 4.57e-04 - 6.61¢-06 - 6.01 -
4.85e+00 4.90e+00 6.62e-07 2.52e-02 8.24e-12 1.06e-11 0.46 11.87

14


Furthermore, we randomly generate an initial point on the Oblique manifold. In our proposed ap-
proaches, we transform into the following optimization problem with q equality constraints in
R” x q,
min — ptr (A(x) "LA(x)) + £ [[Diag(XX) — In|
XeR"™4 4. 4
st. A(X)'e=0,

(4.2)

where A(X) = 2X (Diag(XX') + I,) ~* ER" and B is fixed as 0.05 for all test instances.

The numerical results are presented in Table[2] We observe that all the solvers successfully com-
pute a solution for|NLP}and From Table [2] we can conclude that applying existing solvers to
CDPlis significantly more efficient than directly applying those solvers to[NLP] Again, the improve-
ment can be over 10 times for some of the compared solvers. Moreover, among all the solvers, it
appears that PSQP has the best overall performance. Therefore, we can conclude that[CDP] enables
the direct implementation of existing solvers while enjoying the high efficiency from utilizing the
partial manifold structures of[NLP] Furthermore, it is worth mentioning that constructing [CDPlis in-
dependent of the geometrical properties of the Riemannian manifold M. By solving [NLP] via[CDP]
we utilize the Riemannian structures of the constraints while avoiding the needs to deal with the
geometrical materials of the Riemannian manifold M.

5 Conclusion

In this paper, we focus on a general constrained optimization problem|NLP} where some of its
equality constraints define an embedded submanifold in IR”. We propose a constraint dissolving
approach for to achieve better convenience and efficiency by directly applying various of Eu-
clidean optimization solvers while exploiting the Riemannian structure inside (NLP). In our proposed
constraint dissolving approaches, solving [NLP]is transformed into solving the constraint dissolving
problem|CDP} which is a constrained optimization problem in IR” that has eliminated the manifold
constraints c(x) = 0 from[NLP] Under mild conditions, we prove the equivalence between[NLPland
CDP} in the aspect of their constraint qualifications, stationary points and KKT points. Moreover, for
the special case with no additional equality constraints except c(x) = 0 for[NLP] we prove sharper re-
sults on the equivalence between[NLP]and|CDP] Therefore, we can directly apply existing Euclidean
optimization approaches to solve [CDP]and enjoy the rich expertise gained over the past years for
solving constrained optimization in R”.

We perform preliminary numerical experiments on Riemannian center of mass problems and bal-
anced graph bisection problems, where various existing numerical solvers, including ALGENCAN,
IPOPT, PSQP, SLSQP, and TRCON are applied to solve[NLP]and[CDP] The numerical results demon-
strate that applying these solvers to[CDP]can achieve superior performance in efficiency and stability
over the approaches of directly solving Therefore, we can conclude that our constraint dis-
solving approach enjoys the great convenience in directly applying various Euclidean optimization
solvers, and its numerical performance can be boosted by exploiting the structures of M in[NLP]

References

[1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2009.

[2] Bijan Afsari. Riemannian |? center of mass: existence, uniqueness, and convexity. Proceedings of
the American Mathematical Society, 139(2):655-673, 2011.

[3] Roberto Andreani, Ernesto G Birgin, José Mario Martinez, and Maria Laura Schuverdt. On aug-
mented Lagrangian methods with general lower-level constraints. SIAM Journal on Optimization,
18(4):1286-1309, 2008.

15


Table 2: A comparison between and on minimum balanced cut problems.

Test problems

Solvers

ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON

m= 60
m = 100
(4,0) =
(2,0.1
m = 200
m = 400
q=2
et
(m,p) =
(100, 0.1)
q=6
q=8
p = 0.05
(m,q) =? =
(100, 2)
o =0.2
p =0.3

ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON
ALGENCAN
IPOPT
PSQP
SLSQP
TRCON

Function value Substationarity Feasibility CPU time (s)

CDP] [NLP
-1.62e+01  -1.62e+01 4.05e-07 5.80e-07 3.12e-09 8.06e-09 0.85 2.21
-1.62e+01 -1.62e+01 5.66e-07 5.42e-07 1.16e-10 3.52e-12 0.63 0.62
-1.62e+01 -1.62e+01 6.11e-07 5.22e-07 4.49e-11 5.39e-11 0.09 0.61
-1.62e+01 -1.62e+01 4.74e-07 2.38e-07 4.08e-11 8.68e-13 0.16 0.72
-1.62e+01 -1.62e+01 9.41e-07 8.80e-06 2.24e-12 1.38e-10 0.70 6.02
-3.43e+01 -3.43e+01 5.27e-07 = 6.25e-07. -2.81e-09 3.51e-09 1.12 6.37
-3.43e+01  -3.43e+01 9.88e-07 5.15e-07 1.95e-11 2.35e-11 0.79 7.73
-3.43e+01 -3.43e+01 7.71e-07 9.95e-07 4.44e-11 5.50e-11 0.17 2.16
-3.43e+01 -3.43e+01 6.57e-07 1.23e-07 8.26e-11 3.42e-13 0.70 2.04
-3.43e+01  -3.43e+01 9.26e-07 9.92e-07 4.93e-12 8.0le-13 0.80 17.63
-744e+01 -7.44e+01 = =1.64e-07 = 2.92e-07 = 2.09e-09-9.09e-10 5.84 34.68
-7.44e+01 -7.44e+01 6.20e-07 8.92e-07 4.74e-10 5.48e-11 1.83 85.03
-7.44e+01 -7.44e+01 9.31e-07 9.34e-07 2.31e-10 1.5le-10 0.38 24.35
-7.44e+01 -7.44e+01 9.89e-07 1.21e-07 3.30e-11 5.18e-13 7.45 23.67
-744e+01 -7.42e+01 9.96e-07 1.65e-02 1.17e-11 3.18e-04 2.21 49.28
-1.63e+02 - 7.15¢e-07 - 1.37e-10 - 60.40 -
-1.63e+02  -1.63e+02 9.17e-07 9 9.36e-07 —-7.83e-11 2.82e-10 = 8.41 739.39
-1.63e+02 -1.63e+02 8.14e-07 7.08e-07 2.12e-10 1.22e-10 1.19 251.36
-1.63e+02 -1.63e+02 5.22e-07 7.00e-08 4.45e-11 2.68e-13 75.74 181.46
-1.63e+02 -1.62e+02 7.81e-05 6.09e-03 9.61e-08 1.86e-05 6.92 136.06
-3.43e+01 = -3.43e+01 9 5.27e-07 —6.25e-07. -2.81e-09 = 3.51e-09 = 1.12 6.37
-3.43e+01 -3.43e+01 9.88e-07  5.15e-07  1.95e-11 2.35e-11 0.79 7.73
-3.43e+01 -3.43e+01 7.71e-07 9.95e-07 4.44e-11 5.50e-11 0.17 2.16
-3.43e+01 -3.43e+01 6.57e-07 1.23e-07 8.26e-11 3.42e-13 0.70 2.04
-3.43e+01  -3.43e+01 9.26e-07 9.92e-07 4.93e-12  8.0le-13 0.80 17.63
-3.60e+01  -3.60e+01 =3.52e-07  -2.61e-07 = 8.78e-09 = 3.22e-09 1.24 10.57
-3.60e+01 -3.60e+01 7.42e-07 3.82e-07 1.42e-11 7.74e-12 1.27 40.78
-3.60e+01  -3.60e+01 1.07e-06 7.60e-07 9.36e-11 1.04e-10 0.34 5.54
-3.60e+01 -3.60e+01 8.38e-07 1.20e-07 1.83e-12 9.12e-13 4.06 7.64
-3.60e+01  -3.60e+01 9.09e-07 1.78e-04 1.75e-10 2.83e-08 1.57 24.09
-3.8le+01  -3.8le+01  2.50e-07 = 1.79e-07 6.34e-10 = 2.46e-09—s 1.90 8.76
-3.81le+01  -3.8le+01 5.76e-07 7.24e-07 2.45e-10 1.83e-10 1.83 83.78
-3.81e+01 -3.8le+01 8.55e-07 9.49e-07 9.75e-11 1.47e-10 0.91 10.96
-3.8le+01 -3.81le+01 6.33e-07 7.37e-08 2.42e-11 5.82e-13 14.35 22.08
-3.81le+01 -3.8le+01 3.85e-04 4.02e-05 2.38e-08 1.47e-09 6.54 30.70
-4.10e+01 = -4.10e+01 ——7.32e-08 = 8.80e-08 = 4.17e-09_-2.07e-09 1.55 10.11
-4.10e+01 -4.10e+01 8.89e-07 6.54e-07 2.87e-11 7.69e-12 1.93 111.52
-4.10e+01 -4.10e+01 9.74e-07 8.92e-07 6Ale-11 8.06e-13 1.33 13.38
-4.10e+01 -4.10e+01 8.13e-07 7.13e-08 2.6le-11 3.78e-13 25.97 41.20
-4.10e+01 -4.10e+01 2.63e-05 1.49e-05 5.83e-11 1.75e-10 7.59 37.19
-3.28e+01  -3.30e+01 7.08e-07 8.31e-08 1.91e-09 2.44e-09 0.99 7.99
-3.28e+01 -3.27e+01 8.61e-07 6.02e-07 1.83e-09 1.13e-11 1.77 11.97
-3.28e+01  -3.30e+01 8.63e-07 8.36e-07 9.60e-11 1.08e-10 0.26 2.82
-3.28e+01  -3.30e+01 9.32e-07 1.34e-07 2.84e-12 4.58e-13 = 1.08 2.33
-3.28e+01 -3.30e+01 9.88e-07 3.30e-05 4.22e-12 1.80e-09 1.44 19.61
-3.43e+01 = -3.43e+01 9 5.27e-07 —6.25e-07 -2.81e-09 = 3.51e-09 1.12 6.37
-3.43e+01 -3.43e+01 9.88e-07 5.15e-07 1.95e-11 2.35e-11 0.79 7.73
-3.43e+01 -3.43e+01 7.71e-07 9.95e-07 4.44e-11 5.50e-11 0.17 2.16
-3.43e+01 -3.43e+01 6.57e-07 1.23e-07 8.26e-11 3.42e-13 0.70 2.04
-3.43e+01 -3.43e+01 9.26e-07 9.92e-07 4.93e-12 8.0le-13 0.80 17.63
-4.21e+01 = -4.21e+01 = 9.60e-08 3.21e-07 = 1.22e-09 = 5.36e-09 1.68 7.66
-4.21e+01 -4.21e+01 9.95e-07 9.88e-07 1.67e-09 1.55e-10 0.73 8.55
-4.21e+01 -4.21e+01 6.60e-07 7.11e-07 6.00e-11 2.82e-10 0.16 3.22
-4.21e+01 -4.21e+01 9.57e-07 1.21e-07 8.63e-12 6.58e-13 0.61 2.75
-4.21e+01 -4.20e+01 9.98e-07 2.18e-02 3.60e-12 2.89e-04 1.27 19.15
-4.37e+01 = -4.37e+01 = .2.52e-07.—-1.77e-08 = 1.46e-09—:1.98e-10 7.51 10.21
-4.37e+01 -4.37e+01 3.61e-07 4.90e-07 1.46e-11 4.18e-11 1.83 10.92
-4.37e+01 -4.37e+01 9.88e-07 64le-07 6.0le-11 3.8le-11 0.21 4.78
-4.37e+01  -4.37e+01 8.90e-07 1.30e-07 7.63e-11 6.61e-13 (0.97 3.99
-4.37e+01  -4.37e+01 6.80e-03 1.95e-03 4.21e-04 1.68e-06 3.87 19.91

16


[4] Roberto Andreani, Ernesto G Birgin, José Mario Martinez, and Maria Laura Schuverdt. Aug-
mented Lagrangian methods under the constant positive linear dependence constraint qualifi-
cation. Mathematical Programming, 111(1):5-32, 2008.

[5] Yu Bai and Song Mei. Analysis of sequential quadratic programming through the lens of Rie-
mannian optimization. arXiv preprint arXiv:1805.08756, 2018.

[6] Marcel Berger. A panoramic view of riemannian geometry. 2003.

[7] Ronny Bergmann and Roland Herzog. Intrinsic formulation of KKT conditions and constraint
qualifications on smooth manifolds. SIAM Journal on Optimization, 29(4):2423-2444, 2019.

[8] Wei Bian, Xiaojun Chen, and Yinyu Ye. Complexity analysis of interior point algorithms for
non-Lipschitz and nonconvex minimization. Mathematical Programming, 149(1-2):301-327, 2015.

[9] Nicolas Boumal. An introduction to optimization on smooth manifolds. Available online, May,
2020.

[10] Rainer E Burkard, Stefan E Karisch, and Franz Rendl. QAPLIB—a quadratic assignment problem
library. Journal of Global Optimization, 10:391-403, 1997.

[11] Richard H Byrd, Mary E Hribar, and Jorge Nocedal. An interior point algorithm for large-scale
nonlinear programming. SIAM Journal on Optimization, 9(4):877-900, 1999.

[12] Frank H Clarke. Optimization and Nonsmooth Analysis, volume 5. SIAM, 1990.
[13] Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust-region Methods. SIAM, 2000.

[14] Frank E Curtis, Tim Mitchell, and Michael L Overton. A BFGS-SQP method for nonsmooth,
nonconvex, constrained optimization and its evaluation using relative minimization profiles.
Optimization Methods and Software, 32(1):148-181, 2017.

[15] Roger Fletcher and Sven Leyffer. Nonlinear programming without a penalty function. Mathe-
matical programming, 91(2):239-269, 2002.

[16] Gene H Golub and Charles F Van Loan. Matrix computation, volume 3. JHU press, 2013.

[17] Magnus R Hestenes. Multiplier and gradient methods. Journal of Optimization Theory and Appli-
cations, 4(5):303-320, 1969.

[18] Xiaoyin Hu, Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. A constraint dissolving approach for
nonsmooth optimization over the Stiefel manifold. arXiv preprint arXiv:2205.10500, 2022.

[19] Xiaoyin Hu, Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. An improved unconstrained approach
for bilevel optimization. arXiv preprint arXiv:2208.00732, 2022.

[20] Hermann Karcher. Riemannian center of mass and mollifier smoothing. Communications on Pure
and Applied Mathematics, 30(5):509-541, 1977.

[21] Dieter Kraft et al. A software package for sequential quadratic programming. 1988.

[22] Changshuo Liu and Nicolas Boumal. Simple algorithms for optimization on Riemannian mani-
folds with constraints. Applied Mathematics & Optimization, pages 1-33, 2019.

[23] Olvi L Mangasarian. Computable numerical bounds for Lagrange multipliers of stationary
points of non-convex differentiable non-linear programs. Operations Research Letters, 4(2):47-48,
1985.

[24] Maher Moakher. Means and averaging in the group of rotations. SIAM Journal on Matrix Analysis
and Applications, 24(1):1-16, 2002.

[25] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. 1999.

17


[26] Ruben E. Perez, Peter W. Jansen, and Joaquim R. R. A. Martins. pyOpt: A Python-based object-
oriented framework for nonlinear constrained optimization. Structures and Multidisciplinary Op-
timization, 45(1):101-118, 2012. doi: 10.1007 /s00158-011-0666-3.

[27] Michael JD Powell. A method for nonlinear constraints in minimization problems. Optimization,
pages 283-298, 1969.

[28] Anton Schiela and Julian Ortiz. An SOP method for equality constrained optimization on mani-
folds. arXiv preprint arXiv:2005.06844, 2020.

[29] Nguyen Thanh Son, P-A Absil, Bin Gao, and Tatjana Stykel. Symplectic eigenvalue problem via
trace minimization and Riemannian optimization. arXiv preprint arXiv:2101.02618, 2021.

[30] Tianyun Tang and Kim-Chuan Toh. Solving graph equipartition SDPs on an algebraic variety.
arXiv preprint arXiv:2112.04256, 2021.

[31] Andreas Wachter and Lorenz T Biegler. On the implementation of an interior-point filter line-
search algorithm for large-scale nonlinear programming. Mathematical Programming, 106(1):25—
57, 2006.

[32] Shuai Wang, Tsung-Hui Chang, Ying Cui, and Jong-Shi Pang. Clustering by orthogonal NMF
model and non-convex penalty optimization. IEEE Transactions on Signal Processing, 69:5273-
5288, 2021.

[33] Ziteng Wang, Shu-Cherng Fang, and Wenxun Xing. On constraint qualifications: motivation,
design and inter-relations. Journal of Industrial & Management Optimization, 9(4):983, 2013.

[34] Melanie Weber and Suvrit Sra. Nonconvex stochastic optimization on manifolds via Riemannian
Frank-Wolfe methods. arXiv preprint arXiv:1910.04194, 2019.

[35] Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for Riemannian optimiza-
tion. Mathematics of Operations Research, in print, 2023.

A Proofs for Theoretical Results

A.1_ Preliminary lemmas
In this subsection, we present several preliminary lemmas from Section 3.1].
Lemma A.1. For any given x € M, the following inequalities hold for any y € Ox,
1 2

— llc < dist(y, M) < Cc . Al
Fie lel < alist, M) < = letw)| (At)
Lemma A.2. For any given x € M, it holds that
2(MyAa+1
[Ay yl s AE e(yyy,,— foranyy € On, (42)
Lemma A.3. For any given x € M, it holds that
4L
leAY))I SS" lle@IP, for any y € Ox. (A.3)

XC

Lemma A.4. For any given x € M, the inclusion J4(x)'d € Tx holds for any d € IR". Moreover, when
d € Ty, it holds that J4(x)'d = d.

Lemma A.5. Given any x € M, J4(x)d = 0 ifand only if d € Ny = range(J-(x)).
Lemma A.6. Given any x € M, it holds that J4(x)* = Ja(x).

18


A.2 Proofs for Section 3.1
Lemma A.7. For any x € K, it holds that
Tx (x) © Tk, (X), (A.4)
TE (x) S TEN). (A.5)
This result can be directly derived from the fact that K C Kg.
Lemma A.8. For any closed cone X%, C R", let X. = Ja(x)' X41, then
xP range(Ja(x)) = Ja(x) A. (A6)

Proof. For any d3 € Ja(x);, there exists dy € VP such that d3 = J,4(x)d2. Asa result, from the
definition of the polar cone, it holds for any d, € 4, that

0.> (do, Ja(x)"dr) = (la(e)d2,dh) = (ds,ch) (A.7)

Therefore, from the arbitrariness of dj € 1, we can conclude that d3 € XP and hence J4(x)A7 C AP.
Together with the fact that J4(x)¥y C range(J4(x)), we have that

Ja(x) Xp C XP Nrange(J4(x)). (A.8)

On the other hand, for any d3 € XPM range(J4(x)), (d3,d1) < 0 holds for any d; € 4;. Notice
that d3 € range(J,4(x)), we have that Ja(x)Ja(x) "dg = d3. Therefore, for any d; € +1, we have

0> (daydi) = (Ja(x)Ja(x)"da-di) = (Ja(x)"ds, Ja(x) "dr, (A.9)

which illustrates that J,4(x)"d3 € V9. Therefore, we conclude that

ds = Ja(x)Ja(x)"ds € Ja(x) 4p. (A.10)
Therefore, together with (A.8), we achieve 4? MN range(J4(x)) = J4(x)¥7. This completes the proof.
O

Lemma A.9. For any x € K, it holds that Tx (x) C Tx.

Proof. For any d € Tx(x), there exists a sequence {x,} C K C M and {t,} — 0 such that d =

—. From the definition of 7;, we can conclude that d € 7;, hence complete the proof. O

limg-++00

Proof for Proposition[3.2]
Proof. For any coefficients A € IRNE, and pe IRN! such that

O= )O AVG(x)+ YO AjV5;i(x), (A.11)
ie [Ne] je Fa(x)
it holds that .
0= YO AVA(x)+ Yo fij V5; (x)
i€ [Ne] i¢Fa(x)
(A.12)
= J,(x) AiVuj(x)+ YO pjVo;(x) }.
i€[Ne] j€Fa(x)

Notice that for any x € M, we have 6(x) = v(x), hence F4(x) = F(x) holds for any x € M. Then it
holds from Lemma|/A.5]that

Yo AWVuj(x) + YO pjVo;(x) € range(Jc(x)). (A.13)
i€ [Ne] j€F(x)

19


That is, there exists 6 € IR? such that

Ye aVal(x)+ YO AMui(x) + YO pjVoji(x) =0. (A.14)
le[p] i€[Ng] j€ F(x)
Since io holds at x with respect to[NLP] we have {Vuj(x) : i € [Ne]} U{Vei(x) : 1 € [p by

{Voj(x) : j € F(x)} is linearly independent set, hence we obtain that 6 = 0, A = 0 mm i=
Thatefore, we conclude that the LICQ with respect to [CDP]holds at x.

O

Proof for Proposition[3.3]
Proof. From the definition of MFCQ with respect to[NLP] it holds that {Vuj(x) : i € [Ne]} U{Veq(x) :
1 € [p]} is a linearly independent set and there exists d € IR" such that
(d, Voj(x)) < -1, Vj € F(x);
(d, Vuj(x)) =0, Vi € [Ne]; (A.15)
(d, Vcji(x)) = 0, Vi € [p]}.

Therefore, for any A € IRN£ such that Lie [Nz] AjJa(x)Vuj(x) = 0, LemmalA.5jimplies that

y" A;Vu;j(x) € range(Jc(x)).
ic [Ng]

Then by the linear independence of {Vuj(x) : i € [Ne]} U{Ve(x) : 1 € [p]} , we get A = 0. Asa
result, we can conclude that {J4(x)Vuj(x) :i€ we E|} is a linearly independent set. Moreover, (A.15)
illustrates that d € J;. Combining with Lemma[A.4] it holds for any j € [N;] that

(d, Ja(x) Voj(x)) = a(x) ‘4, Voj(x)) = (d, Voj(x)) < -1. (A.16)
Therefore, we conclude that MFCQ with respect to holds at x. oO
Proof for Lemmal3.4]

Proof. Proof for (3.4): For any d; € Ten (x), it holds from the definition of Ten (x) that

Notice that F4(x) = F(x) holds for any x € K. Then for any i € [Neg], j € F(x), we obtain

( Ja(x) "dh, Vuj(x) ) = 0, and ( Ja(x) "ch, Voj(x) ) <0. (A.18)

Sinceex € K C M, sump ones that (J4(x)'d1,Vej(x)) = 0 holds for any ! € [p].
Therefore, based on Definition [2.5 it holds that J4(x)'d, € 7/i"(x). From arbitrariness of d, €
Teen (x x), we obtain that J 4(x gna x) C Ti" (x).

On the other hand, Lemma|A.Zjillustrates that 7//!"(x) C Ten (x). Notice that 7,/!"(x) C Tx, then
it holds from Lemma[A.4]that

Tee" (x) = Jax) TEx) S Jax) TEX). (A.19)
Asa result, it holds that
Ja(x) | TE" (x) = Te"(x). (A.20)
Together with Lemma[A.8} it holds that
Tei" (x)° Mxange(Ja(x)) = Ja(x)Te"(x)°. (A.21)

20


Proof for (3.5): Lemmaf[A.Zjillustrates that T(x) C Tic, (x). Moreover, from Lemma[A.9] it holds
that Tc (x) C Tx. Therefore, from Lemma[A.4]we have that

Tic (x) = Ja(x)' Tic (x) © Jax)! Tic, (2): (A.22)

Hence (Ja(x)!Tic,(x))° C Tic(x)°. Together with Lemma[A.3] it holds that

Tk, (x)° Nrange(J4(x)) = Ja(x) (Jax) Ties) C Ja(x)Tk(x)°. (A.23)

This completes the proof. O

Proof for Proposition[3.5}
Proof. From Lemmaj3.4} it holds that

Tkca(x)° Nrange(Ja(x)) © Ja(x)Tx(x)°
= Ja(x) TE" (x)° = TeEN(x)° N range(Ja(x)) (A.24)
C Tx, (x)° Nrange(J4(x)).

Here the first inclusion follows from (8.5), the second equality is implied by @.4), and the last inclu-
sion uses the fact that Ten(x)° C Tk, (x)° holds for any x € K,4. Therefore, we obtain that

Tic, (x)° Mrange(Ja(x)) = Te’"(x)° M range(Ja(x)), (A.25)

and complete the proof. Oo

Proof for Theorem[.6]

Proof. From Definition 2.4]and Definition 2.5] it is easy to verify that Tj"(x)° C Tc, (x)° holds for
any x € Ky. Therefore, for any x € K C Kz, that is a KKT point of (CDP x is also a first-order
stationary point of[CDP]
On the other hand, for any x € K that is a first-order stationary point of it holds from
Definition[2.7]that
0. € N(x) + They)? = Ja (AF) + Tica)”.

Notice that J4(x)df (x) C range(Ja4(x)). Then it holds that

oO
M
S
>
Ras
Q
i
Rs?
+
a
>
S
°
“—"
a)
be
oa
=)
ag
£.
—
>
Re?

(x)Of(x) + (Tic, (x)° Nrange(J4(x)))
= Ja(x)of(x) + (Tanz)? n range(Ja(x))
(x)Of (x) + Ten(x)? = h(x) + TEN(x)°,

where the last equality follows from PropositionB.1] Therefore, we can conclude that x is a KKT point
of and complete the proof. oO

A.3 Proofs for Subsection 3.2
Proof for Theorem[.7]

Proof. Proof for Theorem[3.7[1)
When x is a first-order stationary point of x € K implies that c(x) = 0. Then it holds that

0 € Oh(x) + Tx, (x)° = Ja(x)of (x) + Tr, (x)°.

21


Then it holds from @.5) in LemmaB.4|that

E (Ja(x)Of(x) + Tic, (x)°) A range(Ja(x))
S Ja(x)Of (x) + (Tic, (x)° Mrange(Ja(x)))
S Ja(x)of(x) + Ja(2)Tc(x)° = Ja(x) (OF (2) + Tic (2)°).
As a result, from Lemma|A.5] there exists 6 € IR? such that 0 € df (x) + Jc(x)6 (x)°. Addition-

+ Tk
ally, Lemma [A.9illustrates that . C 7,2, which leads to the fact that J-(x)6 + Tic(x)° = Tic(x)°.
Therefore, together with Definition[2.7] we obtain that

0 € Of (x) + Tx(x)",

hence x is a first-order stationary point of [NLP]

Proof for Theorem{3.7(2)

Suppose x € K is a KKT point to[NLP] then from Definition 2.9] there exists p € IRP, A € IRNe and
HE RM such that

0 € Of(x) + y2 p1Vci(x) )+ yi Aj;Vuj(x) + ye pjV0;(x).
le[p] i€ [Ne] je F(x)

Lemma[A.4]implies that J 4(x) Vc;(x) = 0 holds for any / € [p]. Therefore, we obtain

0€ Ja(x)(Af(x )+ YE oiVei(x)+ YO AWVu(x)+ YC wjVo;(z))

le[p] ie [Ne] j€F (x)
=dh(x)+ Do AJa(x)Vuix)+ YE wjJa(x)Voi(x)
i€[Ne] jEF (x)
=dh(x)+ YO AVA(x)+ YO pjVO(x),
i€ [Ng] je F(x)

which shows that x is a KKT point to
On the other hand, when x € K is a KKT point to it holds that

Oe dh(x)+ )o AVG(x)+ YO pjV5;(x)
i€[Ne] jEF (x)

=Jalx)(Af(x) + LY AVu(x)+ YL wiVo;(x)).

i€[Ne] j€F (x)
Combining with Lemma|[A.5] it holds that
range(Jc(x)) ( ] (ar(x > AiVuj(x)+ YO HiV0;(x)) # ©.

Therefore, we can conclude that there exists 0 € IR? such that

le[p] ie [Ne] je F(x)

which implies that x is a KKT point to This completes the proof. O

Lemma A.10. For any given x € M and any y € Ox, it holds that

2LyA (2M,.,4 + 1)
Ox,c

Jay) Jay) — In) || S lle(y)|I

22


Proof. For any y € Ox, it holds that
Waly) —In)JaQy Il < ay) Jay) — Jax) I+ May) — Ja(x) )Ja(x) |] + ay) — Ja(x) |
< Ly a(2Mx,4 + 1)dist(y,M)

< AM TY) Heyyy,
Tee

Here the second inequality follows the Lipschitz continuity of J4(y) and Lemma [A.6] The last in-

equality is directly from Lemmal[A.1] Then we complete the proof. oO
Proof for Theorem[.8]

Proof. For any y € Ox, A € IRNE, and pt € IRN!, it holds from Lemma that

2Ly AM, (2M, + 1)
sup |(Jaly) ~ inal) < RAAT ey),
wea( FoA)(x) xc
Ly a(||Ally Meu) (2Mz4 +1
Vay) — in) YE V(us0 Ay(y)| < Ah MewOMea t 9) hecyyy,
ie [NE] od

Ox,c

s

A(y) —In) YO pV (a0 A)(y)

j€[Ni]

lle(y)II.

Here the last two inequalities follow from the fact that |Vu(A(y)) || < Mx and ||Vo(A(y))|| < Mx.

Next, we estimate a lower bound of ||(J4(y) — In) J-(y)c(y)|| in the followings,
| aly) = In)Jc(y)e(y) |] 2 We(yyety i) Wate e(y)c(y)||
2 “se ley) Il — a(y)JeCy) I eI = > Ile (y)|| — Ly pdist(y, M) ||c(y) | (A.26)
ze “ Ile(y) I] — Lx,vex |le(y) || = me lel

Therefore, we can conclude that for any y € O,, and any w € 0x¥Lcpp(y,A, pL),

1
I|w|| = Maa ti Jay) — In)w|

TEs (5 Se Ajit + Ye Hj) IU Jaly )—inrnett

ie [Ne] je[Ni]

daw) = Wa(feAy(yyl| _ Ua = maw) Eres AVM AY)

Myat+1 Myat+ 1
| Jay) — In Tal) Diet WV (2 A)y)|
7 Myatl
1 Ox,c 2Ly AMA (2M,,4 +1)
+ = —= B+ Ye Amt Voy) - ESS J le
Maar (s ( [Me] iT )- Top

Ox,c ; ny. 4ALy AMxA,n
= (atte y Att be nn) Sat lie(y)Il-

i€[Ne] j€[Nd] vii

Since w is arbitrary, the inequality in TheoremB.8]follows.

23


32Ly,4(M,4+1)M

Ox,c

dist(0,d.Lcpp(y,A,M)) > —*— (6+ YA +
st(0,dxLcpp(y,A,m)) 2 gay ay (6 oy T Xt) leo.

Furthermore, when 6 + Vie(n,] AiTi + Lie tn] HiT) 2 iAH we get

Thus, for any KKT point y of that satisfies y € Ox, the left-hand side of the above inequality is
0 and hence ||c(y)|| = 0. Asa result y € K. Together with TheoremB.7]we conclude that y is a KKT
point of This completes the proof.

O
Proof for Theorem|3.10

Proof. Firstly, it directly follows from mean-value theorems, Lemma[A.2]and Lemma[A.3]that

|(F(A2(y)) + ATH Ay) + HT o(A2(y))) = (FAW) +7 (AY) + #7 0(AY)))|

2M,.,, (M A+1) 8M,,, (M A+ 1)L ib
< Mya |A’(y) — Aly)|| < “#4 Ie AY) |] < A ley) P-
A X,C

Recall Lemma[A.3] we know that the inequality ||c(A(y))|| < 3 |lc(y)|| holds for any y € Q,. Asa
result,

Lepp(A(y),A,#) — Lepp(y, A, #)
< ees )) + ATu(A2(y)) + pT o(A(y))) = (FAW) AT UAW) + 1 (AY)))|

+5(8+ amt YL wx) (lleCAW)IP le?)

ie [Ne] j€(Ni]
8My.,.(My,a + 1)L
T(6+ YO Amt Dw) Ae | ey)? <
i€[Ne] j€[N1] a

Here the last inequality uses the fact that
8M au(Mx,a +1)Lyp
B+ ) AjT + ) WY = As u =

i€ [Ng] j€INi] Ox,

Then we complete the proof. Oo

A.4_ Proof for Section 3.3
Lemma A.11. For any x € M, it holds that
Jan (x) = Ja(x).

Proof. Lemma[A.6jillustrates that J4(x)* = J4(x) holds for any x € M. Therefore, for any k > 1, it
holds that

Tae (x) = Jgra(x)Ja(A® (x) = Jae (x) Ja (x) = Jaeo(x) a(x)? = Tgee(x)Ja(x) = Jara ().

Therefore, we can conclude that J 4x(x) = J4(x) holds for any k > 1. Asa result, we obtain that

Jae (x) = Ja(x),
and this completes the proof. Oo

The following proposition is a direct corollary from [35], hence we omit its proof for simplicity.

24


Proposition A.12. For any given x € M, suppose B and {;} satisfy Condition then the following
inequalities hold for any y € Ox

n( A®(y)) — nly) < —& ey) IP (A.27)
vi(A*(y)) < vy) + = [le(y)IP- (A.28)

Proof for Lemma[3.14]
Proof. From the expression of[NLPland[CDP}] it holds that K C K4, hence
T(x) © Tica (x), and Tei"(x) © TEN).
Then together with Lemma|3.4} we obtain
Tic(x) © Ja(x) Trg (x), and Te" (x) = Ja(x) TEN):

Furthermore, for any d € Ti, (x), there exists a sequence {x,} C K4 Ox converging to x and a
sequence {f,} C IR} converging to 0, such that jim “—~ = d. Then from Proposition for any
—> +00

x, and any j € [Nj], it holds that
0j(A®(x4)) = 7 A*(A(%4))) $ 27 Alea) + F lle(AGw)) IP
< vj(A(ax)) + 2 lle(x4) I? < G(x) <0,

which implies that {A®(x,)} C K. Note that we used the fact that ||c(A(xx))||_ < 4 ||e(x,)|| from
Lemma[A.3]in the second inequality.
Therefore, it holds from Lemma that

AP (xp) — AP(x)
Te) ° po tk

From the arbitrariness of d € Tx,(x), it holds that J4(x)'Tic,(x) C T(x). Then together with
Lemma[3.4] we obtain
Tx (x) = Ja(*)' Tica (2).

Furthermore, directly from Lemma we can conclude that
Trg (x)° Mrange(Ja(x)) = Jax) T(x)”,
and complete the proof. oO

= Jgx(x)"d = Ja(x)"d,

Proof for Theorem|3.15

Proof. It holds straightforwardly from Theorem[3.7] that any first-order stationary point of isa
first-order stationary point of
On the other hand, suppose x is a first-order stationary point of then it holds that
0 € Of (x) + Tx(x)°.
Asa result, it holds from Lemma that
0 € Ja(x) (AF (x) + Tc(x)")
= Ja(x)0f(x) + Tic, (x)° ON range(Ja(x))
= (dh(x) + Tc, (x)°) Nrange(J4(x))
© n(x) + Ty (x)°.
Here the second equality used the fact that c(x) = 0. Therefore, it holds that x is a first-order station-

ary point of As a result, we obtain that x € K isa first-order stationary point of[NLPlif and only
if x is a first-order stationary point of This completes the proof. O

29


Proof for Theorem|3.16

Proof. We prove this theorem by contradiction. Suppose there exists a y € QO, that is a first-order
stationary point of[CDP]with c(y) 4 0. From Definition[2.7] we first obtain that y € K4. Then we aim
to find a direction d € Ti, (y) that produces a sufficient decrease for h.

Let d := (Ja(y)' — In)Jc(y)c(y), we first prove that d € Tx, (y). For any j € [N/], it holds from
Lemma[A.10]that

(d, eine ))

= (d,Ja(y)Vo(A(y))) = Ue(ye(y), Jay) — In) Ja(y)V2j(A(y))) (A.29)
_ Hess lIe(y)|I e(ye(y) || < — ee) [-

On the other hand, it follows (A.26) that

(Jaly)" - In) Je(y)e(y), Je(ye(y) ) = —[Jelye)IP + Ue(yely), Jay) Je(ye(y))

< ~Welyde(y)IP + Iay)Je(y) ley) Il eye) (A.30)
< ~ Jely)ely)|? + 4 == \e(we(y)|I" < 5 Melyde(y) IP.
Then it holds that

(d, Va(y)) = (4, V(oj° A)(y)) +71 (aly) = In)IeCy)e(y), Je(y)eC¥))

AL, A(2My 4 +1)Mx0 :
g (ea Oe 2) yecned)iP <0.

2
xc

(A.31)

Here the last inequality follows from ConditionB.12] As a result, we can conclude that (d, V6; i(y)) <
0 holds for any 1 < j < Nj. Then from the Lipschitz smoothness of 6;, there exists a eauaiane &) > 0

such that d(y + td) < o(y) holds for any t € [0,é|. Together with the fact that y € Ky, we can
conclude that y + td € K, holds for any t € [0,2], hence d € Tx, (y).

Furthermore, since 0 € dh(y) + Tx, (y)°, there exists w € df(A(y)) such that —(Ja(y)w+ BJc(y)c(y)) €
Tk, (x)°. Since d € Ti, (y), then it holds that

O< (d,Jaly)w + Ble(y)e(y))
(i) 2Ly aM, ¢(2M, 1
g eA OAT) relic) —£ Newel?

Ox,c

2Ly AM, ¢(2My4 +1 (A.32)
< [ef Ie(ye(y) I?

2
—F ifty)e(y)IP <0,
Here (i) follows from Lemma [A.10]and (A.30). Moreover, (ii) follows from Condition B.12) which
illustrates that

_ 64M, ¢(MxA + 1) {Lg + Oxickxa) . 64M, f(M 1A + Vina, 8Ly AMy, ¢(2Mx, At 1)

A33
, a2, oF, - (A.33)

As a result, from (A.32) we get the contradiction and achieve that c(y) = 0. Thus y € K. Therefore,
from Theorem we conclude that y is a first-order stationary point of O

26


Proof for Corollary[3.17]
Proof. For any y € Qy that is a first-order stationary point of [CDP] let d := (J4(y)' — In)Jc(y)c(y),
then from (A.31) we can conclude that (d, Vd;(y)) < 0 holds for any j € [Nj]. Thus we get that
de Ten (y).

Note that y is a KKT point of then there exists fi € RM! and w € df(A(y)) such that 0 =
Jaly)w + Ble(y)e(y) + Lien, H/V9;(y)- Therefore, similar to (A.32), we have

0= (4 Jays Bela) + » ro)

j€(N7]

IA

(d,Ja(y)w + BJe(y)c(y))
a +1)
0.

F (A.34)
- é) INe(y)e(y) I"

p 2
<7 Wewe(yll s 0.
which illustrates that c(y) = 0, and hence y € K. Therefore, from Theorem[B.7)we can conclude that

y isa KKT point to On the other hand, it directly holds from Theorem[3.7|that any KKT point to
INLP]is a KKT point to This completes the proof. oO

27
