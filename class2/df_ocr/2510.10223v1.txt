arX1iv:2510.10223v1 [cs.CL] 11 Oct 2025

Under review as a conference paper at ICLR 2026

YOU ONLY NEED 4 EXTRA TOKENS:
SYNERGISTIC TEST-TIME ADAPTATION FOR LLMS

Yijie Xu!, Huizai Yao!, Zhiyu Guo!, Weiyu Guo!, Pengteng Li’,
Aiwei Liu’, Xuming Hu!, Hui Xiong!”*
'The Hong Kong University of Science and Technology (Guangzhou)

>The Hong Kong University of Science and Technology
3

Tsinghua Universit
yxu409@connect .hkust-—gz.edu.cn
ABSTRACT

Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant distribu-
tion shifts from their training data. Domain-specific fine-tuning can mitigate this
challenge but relies on high-quality labeled data that is expensive and slow to col-
lect in expertise-limited settings. We study label-free test-time adaptation for lan-
guage models and present SYTTA, an inference-time framework that adapts mod-
els on-the-fly without additional supervision. SyTTA couples two complemen-
tary uncertainty signals that arise under distribution shift: input-side perplexity,
indicating mismatch with domain-specific terminology and patterns, and output-
side predictive entropy, indicating diffuse and unstable token probabilities during
generation. Across diverse model architectures and domain-specific benchmarks,
SYTTA delivers consistent gains. Notably, on agricultural question answering,
SYTTA improves ROUGE-Lgum by over 120% on QWEN-2.5-7B with only 4
extra tokens per query. These results show that effective test-time adaptation for
language models is achievable without labeled examples, supporting deployment
in label-scarce domains. The code will be made available upon acceptance.

1 INTRODUCTION

Large language models (LLMs) have strong capabilities in reasoning, code generation, and lan-
guage understanding, and they are being deployed in specialized domains or scenarios (OpenAI

2023} [Team et al.| {2023} [Anthropic| {2024} [Guo et al.|/2025). Financial institutions use LLMs for

market analysis, healthcare providers employ them for clinical are support, and agricultural

Syeeueataate aa them for crop management advice (Wu et al.| [2023} (Singhal et al.| [2023]
. However, these models often underperform in domain- specific settings where
the a rate terminology, and knowledge needs differ from pre-training data

[2023] Singhal etal[2023) [Gu etal [2021} [Bella etal. 2024) Hu etal. 2025),

The standard responses include supervised fine-tuning (SFT) and reinforcement learning from hu-
man feedback (RLHF), which are effective when high-quality supervision is available
[2022} {Ouyang et al.|/2022). In production, however, collecting and refreshing domain-accurate data
1s costly, and specialized knowledge evolves over time, making aarp difficult. Retrieval-
augmented generation (RAG) (Lewis et al.] {2020} [Mao et al] 2021] and few-shot prompting (
let al.|/2023) mitigate the need for finetuning, but both rely on eer supervision in different forms:
RAG requires maintained corpora, while prompting depends on carefully chosen examples. These
methods alleviate but do not remove the reliance on explicit resources, motivating approaches that
adapt without external supervision.

These constraints motivate a complementary direction: adapting models at inference time without
external supervision. Humans learn a language once and later adapt to new accents or dialects
after brief exposure, without new explicit instruction, because the core vocabulary and grammar are

already in place (Clarke & Garrett] {2004 2003). Analogously, LLMs possess broad

base abilities from pre-training; they can still miss the intent of a question or fail to select the right

“Corresponding author.


Under review as a conference paper at ICLR 2026

knowledge, not because the knowledge is absent, but because query and answer distributions diverge
from pre-training. For instance, as shown in Figure [I] a query in Scottish dialect (“messages and
a piece”) is misinterpreted by the model, even though the intended meaning is “groceries and a
sandwich.” A human who already speaks English, however, can usually adapt after brief exposure to
such dialectal variations and will eventually understand the phrase correctly. This mirrors the goal of
test-time adaptation: adjusting to distribution shifts during inference without requiring new labeled
supervision. For autoregressive LLMs, distribution shift yields measurable uncertainty patterns:
domain-specific inputs trigger higher token-level perplexity, and decoding exhibits higher predictive
entropy. Treating these quantities as self-supervised signals enables per-cohert adaptation under
practical latency budgets. This converts deployment-time uncertainty into a training signal that
narrows the train—-deploy gap without labels.
@

Prior test-time adaptation for LLMs has typi- groceries > A
cally optimized a single signal. Input-side ob- sandwich “> 3
jectives reduce perplexity to better match domain

patterns 2025), yet they do not di- ty

I will buy groceries and a
sandwich for lunch. What
should I pack?

Pack a reusable bag for
the groceries and napkins
for the sandwich.

rectly control decoding behavior. Output-side en-
tropy minimization sharpens predictions
2021 2022), but naive ap-

plication to autoregressive generation can cause messages > D<]

repetition and collapse (Holtzman et al.| |2020). piece > i]

The challenge is to couple these signals so that
the model becomes more confident and more
domain-aware, while avoiding degeneration and

unnecessary computation.

I will buy the messages
and a piece for lunch.
What should I pack?

Pack your phone to send
messages and some glue
for the piece.

To this end, we propose Synergistic Test-time _. ; . .
Adaptation (SYTTA), a unified framework that Figure 1: Illustration of LLM degradation under dis-
couples input perplexity and output predictive en- inbution shift a Seotush: dialect queny ("messages
tropy for LLMs. SYTTA jointly reduces these and a piece”’) is misinterpreted as unrelated intent.
uncertainties with guardrails that prevent degenerate text, and automatically allocates optimization
effort to the dominant source of uncertainty per instance. The procedure is efficient: SYTTA adapts
with only 4—16 extra tokens per query and supports two deployment modes. The Dynamic-Ref mode
updates during generation for maximum effect, while the Static-Ref mode pre-computes signals be-
fore decoding to reduce latency. Both modes are practical for real deployments.

Our contributions are as follows:

1. We address the challenge of adapting LLMs to specialized domains under distribution shift and
introduce Synergistic Test-time Adaptation (SYTTA), a framework that jointly leverages input
perplexity and output entropy as self-supervised signals to adapt LLMs without labeled data.

2. We demonstrate consistent performance gains across domains and tasks on models spanning
multiple families and parameter scales, while requiring only a small per-query token budget.

3. We conduct extensive empirical analysis examining the effectiveness of different components
across various scenarios, providing insights into when and how test-time adaptation benefits
different types of distribution shifts.

2 RELATED WORKS

Fine-tuning and retrieval from external knowledge. Supervised fine-tuning and instruction tun-
ing improve performance for downstream tasks when high-quality labels or preferences are avail-
able (2022), and RLHF aligns models with human feedback
Retrieval-augmented methods combine parametric models with external corpora (Lewis et al.|
[Guu et al.||2020), but they introduce extra modules and costs. These approaches assume labeled data
(SFT/RLHF) or a curated, queryable corpus (RAG), and are thus not directly applicable in our test-
time setup, where only questions are given without labels or domain knowledge.

Label-free test-time adaptation. Test-time adaptation updates models during inference without
labels, aiming to mitigate performance degradation under distribution shift. In vision, entropy min-

imization (Tent) adapts classifier heads on unlabeled batches (Wang et al.||2021), with follow-ups


Under review as a conference paper at ICLR 2026

improving stability and efficiency via sample selection (Niu et al.| 2022), online adaptation
et al.| [2024), or conservative objectives (Zhang et al.| . For LLMs, test-time training with
in-context examples improves few-shot reasoning (Akyiirek et al.,|2024). Input-side updates with
perplexity objectives also yield strong gains without labels (Hu et al.| . These results highlight
the utility of both input updates and output uncertainty control under shift.

Reinforcement learning with verifiable or consistency signals. RLVR uses programmatic checks
as reliable rewards 2025). GRPO replaces the critic with group-based scoring (Shao

2024), while variants like DAPO (Yu et al.| |2025), GFPO (Shrivastava et al.| 2025),
GSPO (Zheng et al.||2025), and GVPO (Zhang et al.||2025a) address stability, efficiency, or length

control. Others explore test-time RL from consistency signals such as majority voting (Zuo et al.
2025), or simple entropy-based signals for math, code, and science tasks (Agarwal et al.| |2025).
These methods rely on self-consistency or external verifiers, which limits their use in domain-

specific or instruction tasks without reliable checkers. Our method is inspired by their stable op-
timization goals, but works without verifiers at test time.

3. PROBLEM SETUP

3.1 APPLICATION SCENARIOS

We investigate test-time adaptation for question answering under the challenging “question-only”
condition, where the model is exposed to a large set of unlabeled questions from a shifted target
distribution. The inputs are processed in batches, denoted by X = ee To adapt, the language
model may generate a short prefix for each input. Crucially, the token budget for this prefix must be
minimal to ensure that the adaptation process does not introduce significant latency, which would
diminish its practical utility in real-world applications.

Cohort-Level Adaptation. Our setting resembles a multi-tenant model-as-a-service deployment.
Before answering a batch window of target-domain questions X, the model performs a single self-
supervised adaptation pass on the corresponding unlabeled pool. After this pass, parameters are
frozen, and answers are generated for that cohort. We evaluate on this same cohort, which is a trans-
ductive test-time adaptation protocol where the unlabeled evaluation inputs are exactly those used
for adaptation, and no ground-truth answers are accessed. When switching to a different domain,
the model resets to a base snapshot, preventing cross-cohort information leakage or unintended ac-
cumulation. This workflow keeps inference lightweight while maintaining reliability across cohorts.

3.2. NOTATIONS

Let x = (21,...,@m,) denote an input question, which is a sequence of m tokens from a vocabulary
Y. The corresponding response is a token sequence y = (y1,.--,Yn) of length n. We denote the
base LLM as pg, parameterized by weights 6. The model calculates the probability of a response y
given an input x through an autoregressive factorization:

po(y |x) =] | poly | yer,2). (1)
t=1

During test-time adaptation, the model parameters are updated from 6 to 6’ based on the current
input. Inference is then performed using the adapted model, pg’ (- | -). For the adaptation step itself,
the model generates a short prefix, denoted 71.,, of length k. The value of k also represents the extra
token budget allocated for adaptation.

4 METHOD: SYTTA

Our method, SYTTA, realizes Synergistic Test-time Adaptation by coupling two complementary
signals over a shared, short prefix context (Figure [2). Input Distribution Adaptation pulls the input
side toward the target domain by lowering the question’s perplexity; Output Confidence Shaping
pushes the output side toward confident yet anchored next-token distributions. These two signals act
on the same prefix, and we coordinate them with a Dynamic Importance Weighting rule that keeps
their magnitudes comparable across instances. We elaborate on each of these components in the
following sections. Additionally, we state the use of LLMs in Appendix[A.6]


Under review as a conference paper at ICLR 2026

zs Output: G/uons hold quarks.

Po (GH |Z, Hat) + Entropy

7 i

Ltotal Gluons Atom Force Gluons Atom Force

Input Distribution Adaptation 0 — 8 —nVoLrotal Output Confidence Shaping

Figure 2: Overview of the SYTTA framework. Input Distribution Adaptation lowers input perplexity, Output
Confidence Shaping reduces output entropy, and Dynamic Importance Weighting balances the two signals. We
leverage uncertainties as self-supervised signals for test-time adaptation.

4.1 INPUT DISTRIBUTION ADAPTATION

To anchor the model in the target domain’s specific language and concepts, we first optimize its
ability to understand the incoming question x. Following recent test-time learning work (Hu et al.|
(2025), Input Distribution Adaptation minimizes prompt perplexity (equivalently NLL):

Lapa(6') = —— 23 log por(@s | e<i). (2)
To focus adaptation on challenging instances, we employ a gating mechanism where the optimiza-
tion is applied only to samples whose initial NLL under the base model pg exceeds a predefined
threshold. For these selected samples, the loss is further amplified by a factor proportional to their
NLL, promoting faster and more stable learning on difficult inputs.

4.2 OUTPUT CONFIDENCE SHAPING

While Input Distribution Adaptation reduces input perplexity, it does not ensure coherent or con-
fident generation. Models may still exhibit high predictive entropy or drift during decoding. To
complement input-side adaptation, we introduce an output-oriented objective that regularizes the
next-token distribution. Unlike many test-time procedures that update the model at every step,
which risk error propagation and added cost, SYTTA can decouple supervision from adaptation
and explicitly shape outputs using entropy and reverse Kullback—Leibler (KL) terms.

For each input x, we form a short prefix of length & and a reference distribution from the base model.

Let
(x) = ‘career. k), Static-Ref mode, 3)

GENPREFIX(pg’,x,k), Dynamic-Ref mode,

Given the generated prefix (x), we define the base-model reference logits at each step as
4" (a) =logpo(:| #,G<r(x)), t=1...,k. (4)

In the Static-Ref mode, 9(x) and {2}°!()}#_, are computed once with the frozen base model pg
and cached for the whole adaptation. In the Dynamic-Ref mode, the model updates while generating
its own short prefix; the base-model reference logits { 21°! (x) } are computed on the fly for the same
context and are not cached.

The adapted model pg conditions on («, 9(x)) with a base-model-forced forward pass to obtain
learning signals. Output Confidence Shaping then minimizes token-level predictive entropy along
the prefix and regularizes the adapted distribution toward the base model. The entropy term aggre-
gates next-token entropies,

k
Lent (6) = D> A(po(-| 2, G<t)), (5)
t=1

4


Under review as a conference paper at ICLR 2026

Algorithm 1 Training procedure of SYTTA-k with optional prefix cache

Require: dataset D, base model pg, step size 7, prefix len k, KL weight AxL, mode (Static-Ref/Dynamic-Ref )
Ensure: adapted parameters 6’
1: if mode = Static-Ref men

3: end if

4: 0° 0
)
6
7

: for s = 1toS do
Sample mini-batch X C D, |X| = B
Build tensors y € Y®, Zieh EC RBEXRXY:

Static-Ref Dynamic-Ref
¥ = (9(@))cex, y = GENPREFIX(po, X, k),
Zs = (2i%k(@))wex Zt, = LOGITS (po, X, ¥, k)
8: Run pg (X, y) // base-model-forced (Static-Ref) / generated
9: Compute losses: Lipa, Locs, LKL S R?
10: Compute weights: wipa, Wocs € R?

11: Aggregate batch loss: Lpatch = ((wiva, L1wa) + (wocs, Locs) + AKL * 1 Lx)

12: Update: 6’ + 0’ — 1 VorLoatch
13: end for
return 6’

where H(-) is the Shannon entropy. We do not commit to a particular instantiation of the entropy
computation here, leaving flexibility for implementation choices.

Test-time adaptation is known to be highly sensitive and can easily suffer from over-updating, which
leads to model collapse. To prevent drift and collapse, we add a per-token reverse KL term|Gu et al.|
(2023) against the base-model reference,

Lei (0 = Da por(- | ©, Gt) || softmax(2f*(«))) . (6)

t=1

The Output Confidence Shaping objective combines these two parts,
Locs(6') = Lenr(6") + Axi Lxr(9’), (7)

where Ax, balances confidence sharpening and proximity to the base model. The prefix length &
(typically 4-16 tokens) sets the strength of the output-side signal relative to computation. Detailed
discussions of the entropy objective design and the choice of the KL formulation are provided in

Appendix

4.3, DYNAMIC IMPORTANCE WEIGHTING

A static weighting between the Input Distribution Adaptation objective and the Output Confidence
Shaping objective is suboptimal, because their relative difficulty varies across steps and instances.
We therefore use a dynamic scheme that keeps the two contributions on a comparable scale, which
helps stabilize training. The total loss is

Lroai(O!) = why, Loa’) + wh 2s Locs(6’). (8)
Static baseline. As a point of reference, the static baseline fixes wjp,a=Wocs=1.

Dynamic Loss-Ratio Weighting. To balance the two objectives, we propose a dynamic weight-
ing scheme inspired by normalization-based methods in multi-task learning
(2019). The core idea is to adjust each objective’s weight at every step based on its current
contribution to the total loss, while enforcing stability.

First, we track the overall loss magnitude using an exponential moving average (EMA) with mo-
mentum  € [0,1), which acts as a dynamic normalizer:

£Y = Bc) 4 1 —p)(£O, + £®,). (9)


Under review as a conference paper at ICLR 2026

Using this normalizer, we compute the relative contribution of each loss, 7 = £ /(L® +) for
i € {IDA, OCS}, and normalize them to obtain preliminary weights a = 7 1; tie These

are scaled by base coefficients \;, yielding wh) =2-;- wl.

However, we also observe that Locs could always be orders of magnitude larger than Lypa, where
this ratio-based approach can cause training instability by effectively silencing one objective. To
prevent this, we introduce a bounded rebalancing mechanism. We first clip the ratio of the two
weights within a range

(t)
ae 5 ain (2. Onin; Aina) (10)

IDA

We then rescale the weights to maintain their sum, ensuring the total gradient magnitude remains
controlled:
(t) 4 ,() ) _ 2 2a
(wpa: wocs) = \itFaw That : (11)

This design keeps both objectives on a comparable scale. Clipping activates only under extreme loss
imbalance to prevent dominance, and EMA-based normalization governs weighting. As the weights
are set by forward-pass statistics rather than backpropagation, the EMA offers stability without
requiring sensitive hyperparameter tuning (e.g., temperature). We compare our scheme with the
static baseline in Section|6.4] and more details are shown in Appendix[A.4.2|

Algorithm and Complexity. We summarize Table 1: Adaptation cost during training.
the computational cost of adaptation only during

training in Table[T] comparing our approach with Metis Poreseitntl asses
several baselines. The “SYTTA (Static-Ref)’ | TENT, EATA (k+1)|D|
variant is notably efficient, requiring only a sin- TLM 2|D|

gle forward pass per sample in the dataset (|D|), | SYTTA (Dynamic-Ref) (k + 1)|D|
significantly outperforming current methods like SYTTA (Static-Ref) |D|

TLM, TENT, and EATA. We refer to our method ~

with prefix length k as SYTTA-k. The full procedure is in Algorithm [I] For each batch, adapta-
tion runs a single base-model-forced forward pass of pg over the length-& prefixes in the Static-Ref
mode, using cached base-model generation results and log-probabilities for the KL term. This re-
moves repeated decoding and avoids feedback from unstable updates. Additionally, Static-Ref better

exploits vLLM 2023) features, such as PagedAttention’s paged KV memory and con-
tinuous batching with prefix reuse, yielding faster training.

5 EXPERIMENTS
5.1 EXPERIMENTAL SETUP

Datasets. Following the experimental setup of (2025), we evaluate our method primar-
ily on the AdaptEval benchmark suite, designed to test two key capabilities: downstream domain
adaptation and instruction following. To this end, we use its two main components: DomainBench
and InstructBench. DomainBench assesses model performance on specialized knowledge

domains and comprises four datasets: Agriculture (KisanVaani| {2023), GeoSignal
2023), GenMedGPT 2023), and Wealth (Bhartil |2023), while Inst ructBench mea-

sures the ability to adhere to diverse instructions and consists of three datasets: Dolly (Conover

2023), Alpaca-GPT4 (Peng et al.|/2023), and InstructionWild 2023). Ad-
ditional details regarding each dataset are available in Appendix [A.2|

Base Models and Baselines. To validate the effectiveness and generalizability of our method,
we conduct experiments using a diverse set of state-of-the-art open-source language models and
compare against strong baselines. Our base models include the instruct version of LLAMA 3.1-
8B (AT at Metal [2024a), LLAMA 3.2-3B (AI at Metal [2024b), and two instruct models from the
Qwen series, QWEN 2.5-7B and QWEN 2.5-14B (The Qwen Team} |2024).

We compare SYTTA against several methods: the base model without adaptation, which serves as
a lower bound; TLM 2025), which adapts by optimizing input perplexity only; and two
prominent methods from computer vision, Tent (Wang et al.|/2021) and EATA 2022).


Under review as a conference paper at ICLR 2026

Table 2: Main results on DomainBench and InstructBench. ROUGE-Lsum scores (x 100 for visibility;
higher is better). For each model and dataset, the highest score is bold and the second-highest is underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Dolly Alpaca-GPT4 InstructWild Avg.
Base Model 8.34 22.02 14.13 21.45 16.48 30.68 34.41 25.61 30.23
0.98 4.59 9.32 2.33 4.30 5.66 5.72 6.41 5.93
0.39 4.89 5.49 0.03 2.70 1.05 6.83 3.55 3.81
14.23 27.56 24.29 26.73 23.20 24.77 37.66 27.66 30.03
Dynamic-Ref
LEANED ecSB! SOV TTA 19.72 26.74 17.64 29.10 23.30 32.56 40.53 34.69 35.93
SYTTA-16 18.37 27.15 17.85 28.18 22.89 30.56 39.72 32.08 34.12
Static-Ref
SyTTA-4 20.12 29.45 17.59 29.07 24.06 34.12 40.53 36.15 36.93
SYTTA-16 15.38 28.31 19.66 28.28 22.91 33.46 39.67 32.65 35.26
Base Model 8.59 y 13.53 21.65 16.51 32.90 34.40 25.67 30.99
Tent 1.16 3.79 0.74 13.22 4.73 0.45 4.84 9.78 5.02
EATA 1.52 6.43 1.86 14.60 6.10 1.75 5.89 2.53 3.39
TLM 16.33 28.85 25.71 28.95 24.96 32.36 38.41 28.88 33.22
Dynamic-Ref
LLaMa-3.1-8B SyYTTA-4 20.17 29.47 26.48 29.58 26.43 34.61 41.27 36.15 37.34
SYTTA-16 19.56 26.52 25.03 29.55 25.16 32.98 39.45 35.05 35.83
Static-Ref
SyYTTA-4 16.49 29.52 24.82 29.50 25.08 35.45 40.85 35.71 37.34
SYTTA-16 15.17 29.19 21.23 29.86 23.86 35.42 39.85 32.08 35.78
Base Model 9.43 22.03 12.51 23.88 16.96 27.05 38.17 27.77 31.00
Tent 19.64 22.15 5.31 28.59 18.92 21.47 24.38 26.93 24.26
EATA 16.30 21.24 12.83 22.57 18.23 30.57 27.01 23.81 27.13
TLM 11.23 26.21 29.67 28.13 23.81 31.05 43.08 30.76 34.96
Dynamic-Ref
OWEN 3°1B “Sy ITAA 17.68 29.42 29.74 29.69 26.63 35.69 32.95 37.32
SYTTA-16 21.14 28.81 26.83 30.25 26.76 35.93 33.67 37.58
Static-Ref
SyYTTA-4 19.40 29.67 27.00 36.51 43.40 34.07 37.99
SYTTA-16 18.31 29.79 25.87 36.27 43.04 33.72 37.68
Base Model 10.67 24.36 18.23 28.06 31.84
Tent 4.92 28.19 18.97 29.66 23.02
EATA 1.88 i 27.97 15.31 22.99 26.08 24.80
TLM 11.09 32.20 29.48 25.37 34.04 42.20 35.61
Dynamic-Ref
Qwens2.8r148 SYTTA-4 20.09 30.57 31.05 30.14 27.96 37.04 43.24 34.45 38.24
SYTTA-16 18.82 28.72 29.79 29.95 26.82 35.57 42.86 35.49 37.97
Static-Ref
SyYTTA-4 19.52 30.45 28.91 29.53 27.10 36.32 43.13 34.12 37.86
SYTTA-16 21.85 30.93 22.26 29.57 26.15 37.04 42.90 34.46 38.13

Following the adaptations in (2025), we adapt their core principle of entropy minimization
to the LLM’s output distribution and implementation details to create strong baselines.

Evaluation Metrics. We primarily use ROUGE-Leum 2004) to evaluate the quality of gener-
ated responses against the reference answers, capturing sentence-level overlap with summaries. A
discussion of alternative metrics is provided in Appendix[A.3.1]

Implementation Details. We fine-tune models using Low-Rank Adaptation (LoRA)
2021) with a rank of 8, targeting the query and value projection matrices (q_proj and v_pro}).
Our implementation is based on the LLaMA Factory framework (Zheng et al.|/2024), with inference
accelerated by the vLLM engine 2023). For reproducibility, all responses are generated
via greedy decoding. The training uses a learning rate of 1 x 10~°, one epoch, and a cosine learning
rate scheduler. Additional hyperparameters and details are provided in Appendix [A-5|

5.2 RESULTS

The main results are summarized in Table [2 showing that across all models and datasets, SYTTA
achieves clear improvements over both the base model and prior test-time adaptation methods.
Entropy-only approaches such as Tent and EATA fail to adapt autoregressive LLMs, often collapsing
performance to near-zero scores. Input-only perplexity optimization (TLM) is a stronger baseline
and can be competitive in some cases, but it shows instability, including collapse on Dolly, and
rarely delivers the best overall results. In contrast, SyYTTA combines input adaptation and output
confidence shaping under dynamic weighting, yielding consistent and often state-of-the-art improve-
ments across both DomainBench and InstructBench. The gains are particularly striking on
some datasets; for example, on the Agriculture dataset, SYTTA improves ROUGE-Lsum by
over 120% on QWEN 2.5-7B with only 4 extra tokens per query. The only notable exception is
GenMedGPT, where TLM sometimes outperforms; we attribute this to its synthetic GPT-generated
nature, whose distribution diverges from real clinical text and diminishes the value of shaping output
confidence. Overall, SYTTA significantly improves average ROUGE-Lgum, with gains of 40-60%


Under review as a conference paper at ICLR 2026

Model: LLaMA 3.2-3B Model: LLaMA 3.1-8B Model: Qwen 2.5-7B Model: Qwen 2.5-14B

ROUGE-Lsum

20

10

4 16 4 16

4 16 4 16
Generation Length Generation Length Generation Length Generation Length

9 Static-Ref E Dynamic-Ref

Figure 3: ROUGE-Lsum scores under different generation lengths (4 vs. 16) and models. Results are shown
for both Static-Ref Mand Dynamic-Ref @, with error bars indicating standard deviations.

on DomainBench and 15-22% on Inst ructBench depending on the model and variant. We
also provide additional results under more prefix generation lengths in Table[3]in the Appendix[A.3]

Trends across model families and sizes. We observe that the relative gains vary systematically
with the base model family. For the LLAMA family, larger models benefit more from SYTTA, with
the 8B variant showing larger relative improvements than the 3B variant. For the QWEN family,
the opposite trend appears: the 7B model improves more than the 14B model. We hypothesize that
this difference arises because QWEN models are more strongly post-trained and instruction-aligned,
leaving less headroom for test-time adaptation.

6 FINDINGS BASED ON SYTTA

In this section, we analyze the design choices of SYTTA by addressing research questions that are
central to understanding its performance and robustness. Specifically, we investigate:

* Q1: Is longer prefix generation always better for adaptation?

* Q2: Can the computational efficiency of the Static-Ref mode be maintained without sacrificing
performance?

* Q3: Does Kullback—Leibler (KL) divergence genuinely contribute to model stability?

* Q4: Is Dynamic Importance Weighting necessary for balancing input and output objectives?

6.1 IMPACT OF PREFIX GENERATION LENGTH (k) Ee a
We first average results across tasks and then
aggregate by model and generation length
(marginalizing over update modes) to obtain Fig-
ure Across all base models, a short prefix
(kK = 4) outperforms a longer prefix (k = 16).
The average gains range from small but consis-
tent (about +1 ROUGE-Lgym point on QWEN 2.5-
7B) to more pronounced improvements (about +6
points on LLAMA 3.2-3B). This pattern indi-
cates that most of the useful adaptation signal is
contained in the earliest few tokens, while extend-
ing the prefix primarily increases variance and
susceptibility to incidental noise without provid-
ing commensurate benefit. Consequently, k = 4
offers a better stability—efficiency trade-off and is
a robust default for adaptation. As shown in Fig. [4] the token-level response entropy on two repre-
sentative datasets follows the same pattern: a very high spike at the first few tokens, followed by a
rapid drop and a stable range for the rest of the generation. For both the domain-specific instruction-
following set, the maximum occurs around k ~ 4. This is consistent with our findings and supports
adapting on the first few high-entropy tokens (e.g., k = 4), which carry most of the useful signal. In
contrast, longer prefixes mainly add noise and can lead to overfitting with little additional benefit.

‘Average Entropy

‘Token Position in Generation

Figure 4: Average token-level response entropy
computed by averaging across all responses.


Under review as a conference paper at ICLR 2026

6.2 Static-Ref VS. Dynamic-Ref FOR DEPLOYMENT

To isolate the effect of update mode, we average results across generation lengths and compare
Static-Ref with Dynamic-Ref in Figure[3| Static-Ref is consistently more stable across models and,
on average, performs as well as or better than Dynamic-Ref. The advantage is clear in the LLAMA
family (e.g., about +5 point ROUGE-Lgym on LLAMA 3.1-8B when averaged across lengths), while
the gap is smaller in the QWEN family (e.g., less than +1 point on QWEN 2.5-7B). We attribute the
reduced gap in QWEN to stronger post-training that makes online updates less sensitive to prefix
drift. Considering stability and cost (one forward pass per sample for Static-Ref), Static-Ref is
the recommended default for practical deployment, with Dynamic-Ref reserved for scenarios that
explicitly benefit from tight coupling to the live decoding trajectory.

6.3 ROLE OF KL DIVERGENCE FOR STABILITY

KL Regularization — Static-Ref KL Regularization — Dynamic-Ref

We compare SYTTA with and without a KL term
that penalizes divergence from the base policy
during online updates. To isolate this factor, we 0

average across generation lengths and report re- 2 | |
sults by model family and update mode. In Fig.f5] “a |e | ee
(blue bars), enabling KL shows two consistent ef- . i ia — Sa
fects. First, it yields larger gains in Dynamic-Ref __,,,
than in Static-Ref. A useful view is to treat the
KL term as a trust-region: it restricts the adapta-
tion to stay close to the base model, preventing
abrupt shifts caused by transient gradients dur- |
ing decoding. This constraint is more important a. = =

for Dy namic-Ref , where the model updates with Figure 5: Ablations of KL regularization Mi and
the dynamic references and small errors can accu-  Pynamic Importance Weighting on ROUGE-Lsum
mulate; Static-Ref uses a fixed reference, so drift across models. Both absolute and relative improve-
is naturally smaller. Second, it more or less im- ments (%) are shown.

proves the average ROUGE-Lgum across models. The improvement is clearer in the LLAMA family,
while the QWEN family shows smaller but steady gains, which we attribute to stronger post-training
that already constrains the adaptation. Further details are in Appendix[A.4]

ROUGE-Lsum

rtance Weighting — Static-Ref — Dyna

ROUGE-Lsum
8 BN

6.4 NECESSITY OF DYNAMIC IMPORTANCE WEIGHTING

We ablate Dynamic Importance Weighting by comparing it to a fixed weighting while holding other
settings constant. We average across generation lengths and report by model family and update
mode. In Fig. [5] (orange bars), our scheme improves ROUGE-Lyum on most models. The net gain
is larger under Dynamic-Ref than under Static-Ref, because the evolving reference amplifies sensi-
tivity to early-token updates and DIW offsets this by rebalancing gradients on the fly. The effect is
more pronounced for the LLAMA family, while QWEN shows smaller but consistent gains, which
we attribute to stronger post-training that already reduces conflicts between objectives.

Mechanistically, DIW keeps the Input Distribution Adaptation and Output Confidence Shaping
losses on a comparable scale using EMA-normalized loss ratios with a clipped weight ratio, which
prevents one objective from dominating when their magnitudes differ by orders. In addition, it also
alleviates the inherent instability caused by the non-smooth entropy patterns of response tokens (see
Figure (4p. DIW and KL are complementary: KL limits adaptation drift, and DIW balances the two
objectives step by step. For deployment, we enable DIW with KL by default. Adding DIW adds
robustness to long generations and mixed-domain workloads without extra inference cost.

7 CONCLUSION

This study introduces Synergistic Test-time Adaptation(S YTTA), a novel label-free framework that
adapts LLMs to specialized domains or scenarios at inference time. By synergistically coupling
input perplexity and output entropy, SYTTA provides a more robust and effective solution to dis-
tribution shifts than current approaches, improving both domain awareness and generation stability.
Experiments and findings demonstrate that SyTTA can consistently improve performances across
diverse models and benchmarks, delivering substantial gains with minimal computational overhead.
This framework enhances the optimization of LLM deployment for both efficiency and reliability,
promising a practical path for adaptation in label-scarce specialized domains. Future work will focus
on extending this synergistic principle to more diverse generative tasks and deployment scenarios.


Under review as a conference paper at ICLR 2026

REFERENCES

Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effec-

tiveness of entropy minimization in Ilm reasoning, 2025. URL|https://arxiv.org/abs/
2505.15134

AI at Meta. The Llama 3.1 Herd of Models. arXiv preprint arXiv:2407.12644, 2024a.
AI at Meta. The Llama 3.2 Herd of Models. arXiv preprint arXiv:2408. 11453, 2024b.

Ekin Akyiirek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and
Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning, 2024.

URL https://arxiv.org/abs/2411.07279

Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Weizhu Chen, and
Jian-Guang Lou. Skill-based few-shot selection for in-context learning. arXiv preprint
arXiv:2305.14210, 2023.

Anthropic. The claude 3 model family: Opus, sonnet, haiku. Technical report, Anthropic, March

2024. URLihttps://www.anthropic.com/news/claude-3-family

Yarin Bar, Shalev Shaer, and Yaniv Romano. Protected test-time adaptation via online entropy
matching. In Advances in Neural Information Processing Systems, 2024.

Gabor Bella, Paula Helm, Gertraud Koch, and Fausto Giunchiglia. Tackling language modelling
bias in support of linguistic diversity. In Proceedings of the 2024 ACM Conference on Fairness,
Accountability, and Transparency, pp. 562-572, 2024.

Gaurav Bharti. wealth-alpaca_lora. |nttps://huggingface.co/datasets/gbharti/
wealth-alpaca_lora| 2023.

Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In International conference
on machine learning, pp. 794-803. PMLR, 2018.

Constance M. Clarke and Merrill F. Garrett. Rapid adaptation to foreign-accented english. The
Journal of the Acoustical Society of America, 116(6):3647—3658, 2004. doi: 10.1121/1.1815131.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
instruction-tuned Ilm, 2023. URL

daven3. Geosignal. https: //huggingface.co/datasets/daven3/geosignal) 2023.

Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann,
Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical
natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):
1-23, 2021.

Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large lan-
guage models. arXiv preprint arXiv:2306.08543, 2023.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu
Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,
Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei
Feng, Chengda Lu, ..., and Zhen Zhang. Deepseek-rl incentivizes reasoning in Ilms through
reinforcement learning. Nature, 645:633-638, 2025. doi: 10.1038/s41586-025-09422-z.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-
augmented language model pre-training. In Hal Daumé III and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research. PMLR, 2020.

10


Under review as a conference paper at ICLR 2026

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In International Conference on Learning Representations, 2020.

Chengyin Hu, Yao Qin, Zhibo Wang, Xiaoyu Li, Siliang Tang, Yueting Zhuang, et al. Test-time
learning for large language models. In International Conference on Learning Representations,
2025.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685, 2021.

KisanVaani. Agriculture-qa_ english-only. https://huggingface.co/datasets/
KisanVaani/agriculture-qa-english-only, 2023.

Matheus Thomas Kuska, Mirwaes Wahabzada, and Stefan Paulus. Ai for crop production — where
can large language models (IIms) provide substantial value? Computers and Electronics in Agri-
culture, 221:108924, 2024. doi: 10.1016/j.compag.2024.108924.

Woosuk Kwon, Zhuohan Zhu, Danyang Lee, Rockwell Stutsman, Seung-won Han, Jueun Park,
Xujing Zhang, Ion Stoica, and Joseph E. Gonzalez. vLLM: Easy, fast, and cheap LLM serving
with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles
(SOSP ’23), pp. 611-627. Association for Computing Machinery, 2023.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rocktischel, Sebastian Riedel, and Douwe
Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural
Information Processing Systems, 2020.

Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Proceedings of
the ACL-04 Workshop on Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, 2004.
Association for Computational Linguistics.

Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
1871-1880, 2019.

Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu
Chen. Generation-augmented retrieval for open-domain question answering. In Proceedings of
the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4089-
4100, 2021.

Jinjie Ni, Fuzhao Xue, Kabir Jain, Mahir Hitesh Shah, Zangwei Zheng, and Yang You. Instruc-

tion in the wild: A user-based instruction dataset. https://github.com/XueFuzhao/
InstructionWild, 2023.

Shuai Niu, Jiaolong Yang, Song Bai, Yongchao Xu, and Xiang Bai. Efficient test-time model adap-
tation without forgetting. In Proceedings of the 39th International Conference on Machine Learn-
ing, volume 162 of Proceedings of Machine Learning Research, pp. 16888-16905. PMLR, 2022.

Dennis Norris, James M. McQueen, and Anne Cutler. Perceptual learning in speech. Cognitive
Psychology, 47(2):204—238, 2003. doi: 10.1016/S0010-0285(03)00006-9.

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-
ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike,
and Ryan Lowe. Training language models to follow instructions with human feedback. In Ad-
vances in Neural Information Processing Systems (NeurIPS), 2022.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pp. 311-318, 2002. doi: 10.3115/1073083.1073135.

11


Under review as a conference paper at ICLR 2026

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning
with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

Zehong Shao, Yanzhao Zhang, Junxian He, Yongchao Zhou, et al. Deepseekmath: Pushing the

limits of mathematical reasoning in open language models, 2024. URL
org/abs/2402.03300

Vaibhav Shrivastava, Yi Zhang, Saurabh Agarwal, et al. Group filtered policy optimization for

concise reasoning, 2025. URL https: //arxiv.org/abs/2508.09726

Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan
Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul
Gamble, Chris Kelly, Nathaniel Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Agiiera
y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad
Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam,
and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620:172-180,
2023. doi: 10.1038/s41586-023-06291-2.

The Gemini Team et al. Gemini: A family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023.

The Qwen Team. Qwen2.5: A Fast and Strong Large Language Model Series. arXiv preprint
arXiv:2408.05947, 2024.

Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully
test-time adaptation by entropy minimization. In International Conference on Learning Repre-
sentations (ICLR), 2021. arXiv:2006.10726.

Rongsheng Wang. Genmedgpt-Sk-en. https://huggingface.co/datasets/
wangrongsheng/GenMedGPT-5k-—en, 2023.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Interna-
tional Conference on Learning Representations (ICLR), 2022. Published at ICLR 2022.

Xiaohan Wen, Li Chen, Bowen Yu, Jun Wang, Peng Wang, and Yue Zhang. Reinforcement learning
with verifiable rewards implicitly improves reasoning for large language models. arXiv preprint

arXiv:2506.14245, 2025. URL https: //arxiv.org/abs/2506.14245

Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab-
hanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model

for finance, 2023. URL https: //arxiv.org/abs/2303.17564) arXiv preprint.

Qiying Yu, Zheng Yuan, Yiming Wang, et al. Dapo: An open-source Ilm reinforcement learning

system at scale, 2025. URL https: //arxiv.org/abs/2503.14476

Kaichen Zhang, Yuzhong Hong, Junwei Bao, Hongfei Jiang, Yang Song, Dinggian Hong, and Hui
Xiong. Gvpo: Group variance policy optimization for large language model post-training, 2025a.

URL https://arxiv.org/abs/2504.19599

Qingyang Zhang, Yatao Bian, Xinke Kong, Peilin Zhao, and Changqing Zhang. Test-time adaption
by conservatively minimizing entropy. In International Conference on Learning Representations,
2025b. ICLR 2025 Poster.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evalu-
ating text generation with bert. In International Conference on Learning Representations (ICLR),
2020.

Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang,

Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy op-
timization, 2025. URL|https://arxiv.org/abs/2507.18071

12


Under review as a conference paper at ICLR 2026

Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and
Yongqiang Ma. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Pro-
ceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3:
System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics.

URL http://arxiv.org/abs/2403.13372

Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen
Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding,
and Bowen Zhou. Ttrl: Test-time reinforcement learning, 2025. URL

13


Under review as a conference paper at ICLR 2026

A APPENDIX

CONTENTS

A.l Details of Algorithm Design|] ................ 2.2.00... 0000005 14
A.l.1 Entropy Objective] «2. 6 bs eee eee ee eee ee He 14
A.1.2 KL Regularization Details}... 2... 0... ... 02.00.0002 e 14

Le ee 15
A.2.1 DomainBench] .......... 2... 020. eee ee ee 15
Loewe ee ewe eee ee eee 16

Lee 16
A.3.1 Other Evaluation Metrics}... 2... 0... ee ee 16

ee 7
fear samen eee eee eemeer et ssnmae 17
ove eee es 17

Ce 7

ee 7

Ce 18

A.8 Reproducibility Statement) ..... 2.2.2... . 0.2.0.0... 0020.0 0008.4 18

A.1 DETAILS OF ALGORITHM DESIGN

This section clarifies our design choices with notation consistent with Section|4] covering the entropy
objective (cumulative versus average) and why we use reverse KL divergence instead of forward KL.

A.1.1 ENTROPY OBJECTIVE

Recall that Output Confidence Shaping aggregates token-level entropies along the length-k prefix.
We consider two variants that are compatible with the notation in Section|4.2| Let

pi) = pe(-|2,ger), t=1,...,k.
The cumulative form sums the entropies over the prefix,

k
Can) = S° A(vi(:)), (12)
t=1

while the average form normalizes by k,

av: / 1 x /
Lenn (0) = ¢ >) H(ri(.))- (13)
t=1

When losses are combined with fixed coefficients, the two forms differ only by a constant fac-
tor. In our setting, however, the absolute scale interacts with Dynamic Importance Weighting (Sec-
tion (4.3). which uses forward-pass magnitudes to rebalance objectives. Empirically, the cumulative

form in equation [12] gives a stronger and more stable signal, leading to small but consistent gains

across models and datasets. We therefore use Cer in all main results.

A.1.2. KL REGULARIZATION DETAILS

Let the base-model reference distribution at step t be
Pe" (-) = softmax(zf(x)) = pol: | x, Get):

We consider two KL choices between the adapted distribution p/,(-) and the reference pi*'(-).

14


Under review as a conference paper at ICLR 2026

Forward KL (mode-covering).

pret
Lindg -> Dxx(p re | p) -> S- pit(a ) log ao. (14)

t=1aceEV

This penalizes the adapted model for missing probability mass where the reference has support,
encouraging coverage of all reference modes.

Reverse KL (mode-seeking).

al = Pw (vt lpi) = Sra) ) log e. (15)

t=1 aEV

Reverse KL has a well-known mode-seeking behavior: to reduce equation the adapted distri-
bution concentrates on high-density regions of pi“; if pi(a) > 0 while pi (a) ~ 0, the penalty
becomes very large, discouraging exploration of regions that the reference assigns negligible proba-
bility to. This property is desirable in our test-time setting, where supervision is absent and unstable
on-the-fly updates can drift. Using reverse KL, therefore, acts as a practical trust region that anchors
the adapted model to the base policy while still allowing entropy reduction on the prefix.

Choice in SYTTA. We adopt the reverse form in equation|I5]and use it in the Output Confidence
Shaping objective

Locs(0") = Lent (9!) + Ant Lei (G').
Forward KL in equation [I4]is more tolerant of spreading mass and can encourage mode coverage,
which raises entropy and reduces stability during online updates. In contrast, reverse KL provides
stronger safeguards against degenerate repetition and off-support drift, especially in Dynamic-Ref
where references evolve with the prefix. In all experiments, we use reverse KL; ablations in Ap-
pendix[6.3]show that it improves robustness and average performance.

A.2. DETAILS OF DATASETS

Here we describe the benchmarks and datasets used in our experiments, primarily derived from
AdaptEval 2025). We adopt the original data splits and preprocessing protocols estab-
lished in the benchmark.

A.2.1  DOMAINBENCH

DomainBench evaluates model adaptation in specialized domains requiring factual precision and
domain-specific reasoning. It spans Agriculture, Geography, GenMedGPT, and Wealth,
totaling over 110k examples. These datasets jointly test whether models can move beyond everyday
text and produce reliable, domain-specific responses.

Agriculture. The Agriculture dataset includes 22.6k Q&A pairs on
soil, crop growth, irrigation, fertilizer use, pest control, and weather effects. Questions are posed
in a practical, farmer-oriented style, while answers provide concise, actionable guidance. It evalu-
ates whether models can capture applied agricultural knowledge and generate context-appropriate
recommendations.

GeoSignal. The GeoSignal dataset contains 39.7k instructions spanning min-
eral classification, stratigraphic analysis, tectonic features, and geospatial terms. It blends general
tasks with domain-specific reasoning, such as relation inference and fact checking. The dataset
challenges models to handle professional geoscientific language and structured knowledge.

GenMedGPT. The GenMedGPT dataset is a synthetic medical corpus of 5.5k pa-
tient—-doctor dialogues. Patient queries describe symptoms or conditions, and responses emulate
clinical advice across diagnostics, pharmacology, and lifestyle guidance. It tests whether models
can adapt to medical discourse and emulate expert consultation.

15


Under review as a conference paper at ICLR 2026

Wealth. The Wealth dataset provides over 44k instructions on finance, covering
accounting, taxation, market analysis, and investment strategies. Prompts follow an Alpaca-style
format with short instructions and extended answers. It measures a model’s ability to reason about
financial concepts and generate coherent domain-specific responses.

A.2.2. INSTRUCTBENCH

InstructBench assesses general instruction-following ability across curated and naturally oc-
curring prompts. It combines datasets of different sizes and styles to test robustness, adaptability,
and generalization in open-ended instruction adherence.

Dolly. The Dolly dataset 2023) contains 15k human-authored instructions

spanning brainstorming, classification, QA, summarization, and information extraction. All re-
sponses are concise and practical, reflecting workplace and educational use cases. It serves as a
strong baseline for evaluating general instruction-following quality.

Alpaca-GPT4. The Alpaca-GPT4 dataset (Peng et al.||2023) consists of 52k instructions

paired with GPT-4 responses. The dataset covers explanation, summarization, multi-step reasoning,
and procedural tasks. Its detailed and fluent answers allow testing of whether models can follow
complex instructions and maintain coherence across longer generations.

InstructionWild. The InstructionWild dataset includes over 110k
real-world prompts collected from social media, open-source communities, and forums. The instruc-
tions are highly diverse, often noisy, and context-rich, ranging from casual conversational queries
to technical tasks. It provides a challenging benchmark for robustness to non-curated, long-tail
instructions.

A.3 DETAILS OF EXPERIMENTS

We also provide in Table B]a more detailed version of the main results reported in Table [2| From
these results, we observe that using a prefix generation length of k = 4 yields the best overall
performance.

ROUGE-Lgum as the main evaluation metric. ROUGE-Lem measures the longest
common subsequence (LCS) between a generated sequence and a reference summary, aggregating
precision and recall over the subsequence. Unlike other ROUGE variants, ROUGE-Lzum operates
at the sentence level, which makes it particularly suited for summarization-style evaluation, as it
rewards long, in-order matches while allowing gaps. Higher ROUGE-Lgym indicates closer preser-
vation of reference content and structure.

A.3.1 OTHER EVALUATION METRICS

BERTScore-F1. BERTScore-F1 |Zhang et al.|(2020) measures semantic similarity using contex-
tual embeddings. We compute it with the official be rt-scord|| implementation and the bert—

base-mult ilingual-cased’|model. To control length effects, both prediction and reference
are tokenized and truncated to at most 500 tokens, after which the two strings are trimmed to the
same length before scoring. We report the mean F1 across all examples. The detailed results are
shown in Table[4]

ROUGE-1. ROUGE-1 (2004) measures unigram overlap. We use rouge_score.Roug
Scorex)|with use_stemmer=True and split_summaries=True. As preprocessing, we
replace “<n>” with a space, segment the candidate into sentences with nity] and join sentences
with newlines. Scores are the Fl variant averaged over examples. The detailed results are shown in
Table [5]

https://github.com/Tiiiger/bert_score
‘https://huggingface.co/google-bert/bert-—bas
https://pypi.org/project/rouge-score/

“https://github.com/nitk/nitk

multilingual-cased

16


Under review as a conference paper at ICLR 2026

ROUGE-2. ROUGE-2 extends the overlap to bigrams, capturing short phrase consistency. We use
the same rouge_score settings as for ROUGE-1 (use_stemmer=True, split_summari
es=True) and the same sentence-level preprocessing (<n> replacement, sentence segmentation,
newline joins). We report Fl averaged over examples. The detailed results are shown in Table[6]

ROUGE-L. ROUGE-L computes the longest common subsequence overlap, rewarding in-order
matches while allowing gaps. Implementation and preprocessing follow the same protocol as above
(rouge_score with use_stemmer=True, split_summaries=True; sentence segmenta-
tion and newline joins). We report the Fl variant averaged over examples. The detailed results are
shown in Table[/]

BLEU. BLEU (2002) is based on modified n-gram precision with a brevity penalty.

We compute sentence-level BLEU using the NLTK implementation with the standard smoothing
method 4. Tokenization uses the NLTK word tokenizer, with a whitespace fallback if the tokenizer
is unavailable. We average the sentence-level scores over examples. The detailed results are shown
in Table[8]

Summary. Across the tables, we observe that SYTTA consistently achieves strong performance
across multiple metrics with different prefix generation length k. Given that ROUGE-Lgum is more
robust, we highlight in the main results (Table [2) the configurations with k = 4 and k = 16, which
stand out in TableB]

A.4 DETAILS OF FINDINGS
A.4.1 KL CONFIGURATION

In practice, a moderate KL coefficient works well. For the QWEN family with stronger post-training,
larger models benefit from a smaller KL coefficient since their lower-entropy outputs make the same
KL weight overly restrictive; for the LLAMA family, we keep a single coefficient across sizes. We
enable KL by default and tune it following these family-specific rules. Concretely, we set the KL
coefficient to 0.16 for all LLAMA models, while for QWEN-2.5, the 7B variant uses 0.16 and the
14B variant uses 0.01.

A.4.2.. HYPERPARAMETERS OF DYNAMIC IMPORTANCE WEIGHTING

We adopt a dynamic importance weighting strategy with three hyperparameters: an EMA decay
coefficient 3 (default 0.9) to smooth the total loss, a lower bound floor (default 10~3), and an upper
bound ceil (default 10°) to constrain the loss ratio and prevent extreme imbalance. These values are
fixed in all reported experiments.

A.5 DETAILS OF IMPLEMENTATION

Our experiments were conducted on high-performance servers equipped with either four or six
NVIDIA A800 GPUs (80GB memory each) or eight NVIDIA H100 GPUs (80GB memory each).
The A800 machines with four GPUs used the SXM4 version, while those with six GPUs were con-
figured with the PCIe version. All systems were built with Intel(R) Xeon(R) Platinum CPUs, 1TB
of RAM, and a software environment consisting of Python 3.11, PyTorch 2.4, and NCCL 2.21.5 to
ensure reproducibility.

A.6 THE USE OF LARGE LANGUAGE MODELS

We acknowledge the use of a Large Language Model (LLM) to assist with language editing and
polishing of this manuscript. The LLM’s role was strictly limited to improving grammar, clarity,
and phrasing. All scientific ideas, methodologies, results, and conclusions presented herein are
the original work of the authors. The authors have thoroughly reviewed all revisions and assume
complete responsibility for the entirety of the paper’s content.

17


Under review as a conference paper at ICLR 2026

A.7 ETHICS STATEMENT

We affirm compliance with the ICLR Code of Ethics. Our work studies label-free test-time adapta-
tion using publicly available benchmarks and does not involve new data collection, human subjects,
or personally identifiable information. We follow the licenses of all datasets and base models cited in
the paper. Because some tasks touch on finance, medicine, and agriculture, model outputs may carry
risk if taken as advice. Our experiments are research-only; the method is not intended for clinical or
financial decision-making without qualified human oversight. Deployments should include content
filters, disclaimers, and domain-expert review, and must comply with local laws and institutional
policies. We report no conflicts of interest or external sponsorship that could bias the results. The
computational overhead is small (4—16 extra tokens per query and one forward pass per sample in
Static-Ref), which limits environmental impact relative to standard fine-tuning.

A.8 REPRODUCIBILITY STATEMENT

We aim to make the work reproducible. The method is fully specified in Section [4] with pseudocode
in Algorithm [I] Datasets, splits, and preprocessing follow Appendix [A.2] Training and inference
settings, including LoRA configuration, learning schedules, gating, KL weighting, and decoding,
are detailed in Appendix[A.5] KL details are in Appendix [A-4]

18


Under review as a conference paper at ICLR 2026

Table 3: Detailed results on DomainBench and InstructBench. ROUGE-Lsum scores (x 100 for visibil-
ity; higher is better). For each model and dataset, the highest score is bold and the second-highest is underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Dolly Alpaca-GPT4 InstructWild Avg.
Base Model 8.34 22.02 14.13 21.45 16.48 30.68 34.41 25.61 30.23
Tent 0.98 4.59 9.32 2.33 4.30 5.66 5.72 6.41 5.93
EATA 0.39 4.89 5.49 0.03 2.70 1.05 6.83 3.55 3.81
TLM 14.23 27.56 24.29 26.73 23.20 24.77 37.66 27.66 30.03
Dynamic-Ref
SYTTA-2 18.86 28.99 24.65 29.14 25.41 29.31 40.38 32.48
ELaMA-3.2-3B 0 SvPTA-d 19.72 26.74 17.64 29.10 23.30 32.56 40.53 34.69
SYTTA-8 18.56 28.70 20.10 28.53 23.97 32.69 40.00 33.26
SYTTA-16 18.37 27.15 17.85 28.18 22.89 30.56 39.72 32.08
Static-Ref
SYTTA-2 15.18 27.48 18.39 28.80 22.46 34.12 39.88 32.23 35.49
SYTTA-4 20.12 29.45 17.59 29.07 24.06 34.12 40.53 36.15 36.93
SYTTA-8 16.95 28.21 21.01 28.30 23.62 34.08 39.05 35.10 36.08
SYTTA-16 15.38 28.31 19.66 28.28 22.91 33.46 39.67 32.65 35.26
Base Model 8.59 22.28 13.53 21.65 16.51 32.90 34.40 25.67 30.99
Tent 1.16 3.79 0.74 13.22 4.73 0.45 4.84 9.78 5.02
EATA 1.52 6.43 1.86 14.60 6.10 1.75 5.89 2.53 3.39
TLM 16.33 28.85 25.71 28.95 24.96 32.36 38.41 28.88 33.22
Dynamic-Ref
LLAMA-3.1-8B  SYETA-2 17.60 30.38 25.83 29.51 25.83 33.39 40.84 30.52 34.92
" . SYTTA-4 20.17 29.47 26.48 29.58 26.43 34.61 41.27 36.15 37.34
SYTTA-8 20.44 29.00 26.66 30.03 26.53 32.23 40.60 34.34 35.72
SYTTA-16 19.56 26.52 25.03 29.55 25.16 32.98 39.45 35.05 35.83
Static-Ref
SYTTA-2 16.40 29.04 21.97 29.83 24.31 33.47 40.54 30.67 34.90
SYTTA-4 16.49 29.52 24.82 29.50 25.08 35.45 40.85 35.71 37.34
SYTTA-8 16.56 29.87 25.30 29.24 25.24 = 35.19 39.82 33.17 36.06
SYTTA-16 15.17 29.19 21.23 29.86 23.86 35.42 39.85 32.08 35.78
Base Model 9.43 22.03 12.51 23.88 16.96 27.05 38.17 27.77 31.00
Tent 19.64 22.15 5.31 28.59 18.92 21.47 24.38 26.93 24,26
EATA 16.30 21.24 12.83 22.57 18.23 30.57 27.01 23.81 27.13
TLM 11.23 26.21 29.67 28.13 23.81 31.05 43.08 30.76 34.96
Dynamic-Ref
QWEN-2.5-7B SYTTA-2 15.22 29.55 28.96 29.12 25.71 36.07 31.67 37.02
. SYTTA-4 17.68 29.42 29.74 29.69 26.63 35.69 32.95 37.32
SYTTA-8 21.79 29.12 29.28 29.93 27.53 35.97 34.35 37.69
SYTTA-16 21.14 28.81 26.83 30.25 26.76 35.93 33.67 37.58
Static-Ref
SYTTA-2 13.47 29.75 29.48 29.08 25.45 35.46 42.90 31.31 36.56
SYTTA-4 19.40 29.37 29.56 29.67 27.00 36.51 43.40 34.07 37.99
SYTTA-8 21.19 29.58 28.90 29.82 27.37 = 36.27 42.04 33.63 37.31
SYTTA-16 18.31 29.47 25.92 29.79 25.87 36.27 43.04 33.72 37.68
Base Model 10.67 23.46 14.42 24.36 18.23 28.06 39.34 28.12 31.84
Tent 4.92 27.89 14.87 28.19 18.97 29.66 28.12 11.29 23.02
EATA 1.88 28.23 3.17 27.97 15.31 22.99 26.08 25.33 24.80
TLM 11.09 28.70 32.20 29.48 25.37 34.04 42.20 30.59 35.61
Dynamic-Ref
QweEN-2.5-14B SYTTA-2 16.37 30.52 30.45 29.88 26.80 35.60 43.37 32.86 37.28
: SYTTA-4 20.09 30.57 31.05 30.14 27.96 37.04 43.24 34.45 38.24
SYTTA-8 22.01 31.31 24.51 30.05 26.97 35.71 42.65 36.56 38.31
SYTTA-16 18.82 28.72 29.79 29.95 26.82 35.57 42.86 35.49 37.97
Static-Ref
SYTTA-2 14.24 30.14 29.73 28.65 25.69 36.88 43.17 31.58 37.21
SYTTA-4 19.52 30.45 28.91 29.53 27.10 36.32 43.13 34.12 37.86
SYTTA-8 16.34 30.91 25.51 29.86 25.65 36.28 42.62 34.52 37.81
SYTTA-16 21.85 30.93 22.26 29.57 26.15 37.04 42.90 34.46 38.13

19


Under review as a conference paper at ICLR 2026

Table 4: Detailed results on DomainBench and InstructBench. BERTScore-F1 scores (x 100 for vis-
ibility; higher is better). For each model and dataset, the highest score is bold and the second-highest is
underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Dolly Alpaca-GPT4 InstructWild Avg.
Base Model 66.66 67.72 66.74 67.75 67.22 71.74 72.11 70.18 71.34
Tent 66.62 64.49 66.78 64.43 65.58 68.04 72.82 69.75 70.20
EATA 67.44 69.45 68.01 63.21 67.03 67.80 71.92 66.50 68.74
TLM 66.28 70.17 70.95 69.30 69.17 69.87 74.32 70.61 71.60
Dynamic-Ref
SYTTA-2 70.06 70.77 70.05 70.03 71.91 74.70 72.03 72.88
LLAMA-3.2-3B SYTTA-4 69.50 69.24 70.27 69.74 72.75 74.71 72.06 73.17
. SYTTA-8 69.96 70.13 70.22 69.82 72.59 74.56 71.41 72.86
SYTTA-16 68.27 69.79 69.70 69.36 69.96 72.88 71.42 71.42
Static-Ref
SYTTA-2 66.49 70.03 69.09 70.02 68.91 74.37 71.41 73.10
SYTTA-4 70.01 70.16 68.54 70.28 69.75 74.70 72.87 73.56
SYTTA-8 68.05 69.90 69.99 70.03 69.49 74.15 72.22 73.24
SYTTA-16 66.42 69.32 69.80 69.72 68.81 72.85 71.42 72.52
Base Model 66.66 67.77 66.41 67.72 67.14 72.05 70.14 71.70
Tent 67.46 69.03 67.69 67.91 68.02 67.87 69.12 68.25
EATA 66.28 67.34 66.46 65.45 66.38 59.81 68.98 67.54
TLM 66.89 70.86 72.10 69.91 69.94 74.44 70.80 72.37
Dynamic-Ref
SYTTA-2 67.26 70.89 71.44 69.95 69.89 74.91 71.27 73.14
LLAMA-3.1-8B SyYTTA-4 70.29 70.82 72.78 69.85 70.94 74.73 72.45 73.67
° . SYTTA-8 70.35 70.95 71.73 70.22 70.82 74.76 72.03 73.26
SYTTA-16 70.19 69.03 70.92 70.09 70.06 74.08 74.00 73.03
Static-Ref
SyYTTA-2 67.00 71.02 69.72 69.80 69.38 73.32 74.52 71.30 73.05
SYTTA-4 67.28 70.32 71.29 69.88 69.69 73.99 74.48 72.94 73.81
SYTTA-8 67.22 70.75 71.78 69.95 69.93 73.95 74.37 71.45 73.26
SYTTA-16 66.83 70.30 69.97 69.72 69.21 73.74 74.37 72.24 73.45
Base Model 65.67 67.91 65.51 68.43 66.88 70.60 73.56 70.61 71.59
Tent 69.06 70.40 67.00 68.87 68.83 70.54 74.42 70.78 71.91
EATA 66.34 69.97 67.05 68.62 68.00 71.17 72.92 71.14 71.74
TLM 64.99 70.07 74.07 70.22 69.84 73.15 75.95 71.65 73.58
Dynamic-Ref
SYTTA-2 66.54 71.02 73.42 69.96 70.24 73.54 75.94 71.58 73.68
QWEN-2.5-7B SYTTA-4 69.78 71.03 73.48 70.26 71.14 74.33 75.75 71.77 73.95
. SYTTA-8 71.07 71.24 73.66 70.70 71.67 74.44 75.71 72.53 74,23
SYTTA-16 70.20 70.81 72.30 70.89 71.05 74.10 75.30 71.87 73.76
Static-Ref
SYTTA-2 65.86 71.21 73.69 69.94 70.17 74.04 75.83 71.64 73.84
SYTTA-4 68.90 70.73 73.56 70.05 70.81 74.98 75.57 72.14 74.23
SYTTA-8 70.19 71.03 73.35 70.39 71.24 75.27 75.40 72.14 74.27
SYTTA-16 69.09 70.73 72.06 70.60 70.62 74.55 75.60 72.43 74.19
Base Model 65.21 68.28 65.98 68.33 66.95 70.63 73.87 70.91 71.80
Tent 68.01 69.39 68.42 69.92 68.94 73.72 74.25 69.80 72.59
EATA 64.73 70.27 68.02 69.00 68.00 73.98 74.42 70.95 73.11
TLM 64.81 70.83 75.38 70.46 70.37 73.34 76.13 71.58 73.68
Dynamic-Ref
SyYTTA-2 66.22 71.36 74.31 70.08 70.50 73.68 71.85 73.93
QWEN-2.5-14B SYTTA-4 68.35 71.06 74.48 70.29 71.04 73.72 72.06 73.96
. SYTTA-8 71.05 71.97 72.04 70.70 71.44 = 73.50 74.49 74.64
SYTTA-16 68.86 70.69 73.91 70.56 71.00 73.98 72.31 73.92
Static-Ref
SYTTA-2 65.40 71.04 73.92 69.76 70.03 74.33 71.53 74.04
SYTTA-4 68.38 71.33 73.71 69.99 70.85 74.22 72.03 74.10
SYTTA-8 65.99 71.20 72.35 70.54 70.02 74.61 71.98 T4.17
SYTTA-16 71.37 71.72 70.87 70.10 71.01 74.56 71.96 74.12

20


Under review as a conference paper at ICLR 2026

Table 5: Detailed results on DomainBench and InstructBench. ROUGE-1 scores (x 100 for visibility;
higher is better). For each model and dataset, the highest score is bold and the second-highest is underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Alpaca-GPT4 InstructWild Avg.
Base Model 9.09 24.57 15.89 23.39 18.24 37.60 27.97 33.41
Tent 9.02 20.41 aliggcral 16.96 16.03 40.81 26.56 32.04
EATA 9.45 29.32 16.55 18.19 18.37 36.48 21.91 28.10
TLM 16.11 30.98 26.76 29.35 25.80 40.97 30.30 33.16
Dynamic-Ref
SYTTA-2 21.91 32.45 27.00 32.03 28.35 33.30 35.66 37.67
SYTTA-4 22.76 29.75 18.47 31.86 25.71 36.24 38.34 39.55
ELAMA-3.23B sy TTA-8 21.58 32.11 22.57 31.55 26.95 37.39 36.64 39.14
SYTTA-16 21.48 23.05 20.96 30.73 24.05 = 27.76 35.32 34.63
Static-Ref
SYTTA-2 17.32 21.83 31.78 25.46 38.83 43.48 35.48 39.26
SYTTA-4 23.65 17.36 31.84 26.26 = 37.60 44.08 39.83 40.51
SYTTA-8 19.59 23.36 31.08 26.33 38.40 42.49 38.72 39.87
SYTTA-16 17.64 20.98 30.35 24.64 37.76 39.13 35.31 37.40
Base Model 9.36 15.07 23.61 18.24 36.85 37.56 28.02 34.14
Tent 11.41 17.48 25.51 20.73 = 27.44 28.21 27.19 27.61
EATA 9.66 15.97 13.89 16.81 38.52 TIT 29.88 25.39
TLM 18.35 28.17 31.71 27.63 36.44 41.72 31.73 36.63
Dynamic-Ref
SYTTA-2 20.07 28.23 32.37 28.63 37.69 44.44 33.57 38.57
LLAMA-3.1-8B  SYETA-4 23.08 28.78 32.33 29.27 38.52 44.67 40.09 41.09
° . SYTTA-8 23.97 28.58 32.77 29.40 36.35 44.14 37.81 39.43
SYTTA-16 22.70 27.08 30.91 27.31 28.34 41.40 38.94 36.23
Static-Ref
SYTTA-2 18.57 33.53 24.44 32.54 27.27 38.10 44.31 33.70 38.70
SYTTA-4 16.87 32.85 27.38 32.26 27.34 39.80 44.12 39.50 41.14
SYTTA-8 18.74 33.37 27.35 32.06 27.88 40.25 43.45 36.60 40.10
SYTTA-16 16.93 32.31 24.80 32.09 26.53 40.33 42.80 35.08 39.40
Base Model 10.18 24.48 13.61 25.98 18.56 30.10 41.52 30.15 33.92
Tent 22.45 32.45 17.26 31.28 25.86 32.15 43.77 33.42 36.45
EATA 16.31 31.91 16.76 29.61 23.65 32.72 40.90 35.46 36.36
TLM 11.96 29.98 32.76 31.72 26.60 37.43 44.60 34.65 38.89
Dynamic-Ref
SYTTA-2 16.98 33.17 31.41 31.76 28.33 40.76 46.65 34.73 40.71
QWEN-2.5-7B SYTTA-4 23.94 33.02 31.97 32.41 30.33 40.75 45.05 36.22 40.67
. SYTTA-8 25.99 33.22 31.64 32.71 30.89 = 41.35 46.24 37.99 41.86
SYTTA-16 24.79 32.23 29.48 33.12 29.90 40.83 45.84 36.66 41.11
Static-Ref
SYTTA-2 14.87 33.26 32.01 31.78 27.98 40.37 46.42 40.38
SYTTA-4 22.45 32.74 32.03 32.33 29.89 41.66 45.33 37.5$ 41.52
SYTTA-8 24.72 33.11 31.33 32.57 30.43 42.08 45.61 37.02 41.57
SYTTA-16 20.85 32.96 28.61 32.53 28.74 40.50 46.15 37.10 41.25
Base Model 11.67 26.09 15.99 26.59 20.09 31.16 42.76 30.58 34.83
Tent 16.99 31.07 20.08 30.96 24.78 40.09 42.74 32.26 38.36
EATA 13.41 30.59 21.03 29.93 23.74 40.26 43.81 32.09 38.72
TLM 12:12 32.01 34.53 32.31 27.74 = 37.94 45.64 33.33 38.97
Dynamic-Ref
SYTTA-2 18.33 34.17 32.94 32.70 29.53 40.38 46.87 36.04 41.10
QweEN-2.5-14B SYTTA-4 23.26 33.38 32.91 32.43 30.50 40.87 46.69 37.98 41.85
. SYTTA-8 26.00 34.80 27.35 33.09 30.31 40.15 46.28 40.65 42.36
SYTTA-16 21.43 32.07 32.16 32.89 29.64 40.26 45.17 37.94 41.13
Static-Ref
SYTTA-2 15.91 32.27 31.29 28.28 = 41.68 46.73 34.60 41.00
SYTTA-4 22.61 31.41 32.42 30.09 41.55 46.63 37.45 41.88
SYTTA-8 18.41 28.55 32.75 28.50 41.00 46.20 37.88 41.69
SYTTA-16 25.57 25.38 32.47 29.47 41.81 45.75 36.87 A1.A7

21


Under review as a conference paper at ICLR 2026

Table 6: Detailed results on DomainBench and InstructBench. ROUGE-2 scores (x 100 for visibility;
higher is better). For each model and dataset, the highest score is bold and the second-highest is underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Dolly Alpaca-GPT4 InstructWild Avg.
Base Model 3.04 9.81 2.70 7.42 5.74 16.56 16.12 9.39 14.03
Tent 3.05 8.21 2.77 5.17 4.80 13.68 18.13 9.41 13.74
EATA 3.11 12.96 3.18 7.21 6.61 12.08 15.60 7.61 11.76
TLM 5.37 14.89 8.49 10.58 9.83 12.30 19.02 10.90 14.08
Dynamic-Ref
SYTTA-2 7.07 15.05 9.21 11.91 10.81 15.41 20.59 14.16 16.72
LLAMA-3.2-3B SYTTA-4 6.57 13.76 4.12 11.95 9.10 17.59 20.43 14.72 17.58
- . SYTTA-8 6.60 15.18 6.66 11.57 10.00 18.19 20.10 13.78 17.35
SYTTA-16 6.25 9.49 3.64 10.63 7.50 12.70 18.13 12.81 14.55
Static-Ref
SYTTA-2 5.73 13.89 4.19 11.62 8.86 19.52 20.02 12.85 17.46
SYTTA-4 7.11 14.83 3.76 11.92 9.40 18.99 20.47 15.78 18.41
SYTTA-8 6.03 14.41 7.37 11.24 9.76 18.71 19.56 14.57 17.61
SYTTA-16 5.07 13.39 3.62 10.63 8.18 18.58 17.13 12.79 16.17
Base Model 3.32 9.97 3.28 7.48 6.01 18.40 16.24 9.28 14.64
Tent 2.62 11.82 2.62 8.51 6.39 12.81 10.69 9.65 11.05
EATA 3.32 12.33 2.22 2.94 5.20 18.92 2.41 10.29 10.54
TLM 6.43 15.17 9.61 11.89 10.78 18.21 20.02 11.43 16.56
Dynamic-Ref
SYTTA-2 6.41 16.47 10.99 12.27 11.54 18.81 21.14 13.93 17.96
LLAMA-3.1-8B SyYTTA-4 7.05 15.82 11.14 12.17 11.54 18.92 21.00 14.83 18.25
- . SYTTA-8 7.26 15.31 13.22 12.48 12.07 17.24 20.57 14.26 17.36
SYTTA-16 7.26 11.82 9.36 10.75 9.80 13.44 19.14: 14.98 15.84
Static-Ref
SYTTA-2 6.08 15.64 7.33 12.11 10.29 18.99 20.60 11.94 17.18
SYTTA-4 5.73 16.05 9.03 11.99 10.70 20.73 20.72 15.44 18.96
SYTTA-8 6.09 15.43 10.92 11.91 11.09 19.87 19.92 13.60 17-79
SYTTA-16 5.65 14.61 7.23 11.71 9.80 20.71 19.52 12.96 17.73
Base Model 3.63 9.50 3.50 8.67 6.33 14.33 18.64 10.11 14.36
Tent 7.32 15.15 4.24 11.96 9.67 16.17 21.51 12.13 16.60
EATA 5.32 15.09 4.10 11.60 9.03 16.50 19.68 13.21 16.46
TLM 4.20 13.23 14.85 11.79 11.02 19.27 22.34 12.60 18.07
Dynamic-Ref
SYTTA-2 6.07 15.85 13.67 12.04 11.91 21.93 23.12 12.78 19.28
QWEN-2.5-7B SyYTTA-4 cera 15.83 14.10 12.64 12.57 21.55 22.95 13.40 19.30
. SYTTA-8 8.57 15.85 14.18 13.21 12.95 22.29 23.00 14.86 20.05
SYTTA-16 8.05 15.28 11.38 13.02 11.93 21.61 22.19 14.00 19.27
Static-Ref
SYTTA-2 5.31 15.87 14.27 11.83 11.82 21.38 22.89 12.54 18.92
SyTTA-4 7.29 15.54 14.01 12.21 22.79 14.29 19.87
SYTTA-8 8.10 15.62 13.61 12.79 22.67 22.54 14.03 19.75
SYTTA-16 6.86 15.84 10.86 12.54 21.83 22.21 14.15 19.39
Base Model 4.14 10.10 3.84 8.69 6.69 14.71 19.56 10.30 14.86
Tent 5.64 14.21 4.42 11.20 8.87 21.38 20.17 11.03 17.53
EATA 4.54 14.71 4.64 10.31 8.55 21.59 20.71 11.34 17.88
TLM 4.31 14.40 16.75 12.07 11.88 19.53 23.08 12.06 18.22
Dynamic-Ref
SYTTA-2 6.38 16.53 14.76 12.35 12.50 21.63 23.38 13.21 19.41
QWEN-2.5-14B SYTTA-4 7.68 16.25 14.79 12.01 12.68 20.66 23.36 13.97 19.33
. SYTTA-8 8.38 16.36 9.43 12.58 11.69 21.05 23.09 16.29 20.14
SYTTA-16 6.98 15.31 14.98 12.39 12.42 21.59 22.84 14.27 19.56
Static-Ref
SYTTA-2 5.46 15.78 14.06 11.45 11.69 = 21.77 23.21 12.42
SYTTA-4 7.52 15.79 13.29 12.04 12.16 22.48 23.12 13.90
SYTTA-8 6.02 16.22 9.65 12.51 11.10 = 22.22 23.12 14.09
SYTTA-16 8.19 15.37 7.19 12.01 10.69 22.22 22.39 13.66

22


Under review as a conference paper at ICLR 2026

Table 7: Detailed results on DomainBench and InstructBench. ROUGE-L scores (x 100 for visibility;
higher is better). For each model and dataset, the highest score is bold and the second-highest is underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Dolly Alpaca-GPT4 InstructWild Avg.
Base Model 6.63 17.20 10.37 15.16 12.34 26.02 24.93 16.97 22.64
Tent 6.64 15.18 11.26 11.10 11.04 = 22.64 28.63 16.81 22.69
EATA 7.00 21.78 11.61 15.40 13.95 20.20 24.25 15.20 19.88
TLM 12.22 23.48 19.68 20.30 18.92 20.88 29.59 19.16 23.21
Dynamic-Ref
SYTTA-2 16.82 24.84 19.91 22.37 20.99 31.06 24.08 26.59
SYTTA-4 17.48 22.19 13.58 22.39 18.91 31.02 25.70 27.89
ELAMA-3.23B sy TTA-8 16.35 24.96 16.25 22.35 19.98 30.88 23.83 27.79
SYTTA-16 16.07 17.13 13.14 21.89 17.06 28.63 22.49 24.14
Static-Ref
SYTTA-2 13.07 23.16 13.74 22.02 18.00 30.27 30.56 22.09 27.64
SYTTA-4 18.51 24.41 13.22 22.40 19.64 29.22 31.07 26.97 29.09
SYTTA-8 14.59 23.81 16.99 21.61 19.25 29.84 30.17 24.96 28.32
SYTTA-16 13.34 22.86 13.14 20.88 17.56 29.39 27.81 22.48 26.56
Base Model 6.84 17.38 9.87 15.23 12.33 28.07 24.86 16.94 23.29
Tent 10.18 21.11 11.79 16.68 14.94 21.41 18.00 18.64 19.35
EATA 7.01 21.35 10.91 8.73 12.00 30.31 7.55 18.72 18.86
TLM 14.00 24.72 21.51 22.25 20.62 28.47 30.73 19.81 26.34
Dynamic-Ref
SYTTA-2 16.24 26.72 22.49 23.35 22.20 28.52 31.74 22.59 27.62
LLAMA-3.1-8B  SYETA-4 17.34 26.33 22.95 22.51 22.28 30.31 32.00 25.22 29.18
° . SYTTA-8 18.21 25.33 23.99 23.49 22.76 27.65 31.11 24.75 27.84
SYTTA-16 17.26 21.11 21.79 21.19 20.34 = 21.62 29.87 26.31 25.94
Static-Ref
SYTTA-2 13.97 26.22 16.45 22.68 19.83 28.91 31.22 20.66 26.93
SYTTA-4 12.60 25.86 19.39 22:41. 19.99 32.15 31.58 26.29 30.00
SYTTA-8 14.00 25.22 19.71 22.50 20.36 = 30.68 30.62 23.16 28.16
SYTTA-16 12.81 25.15 16.77 22.10 19.21 31.25 30.30 23.07 28.21
Base Model 7.31 16.79 8.56 16.73 12.35 22.09 27.73 17.83 22.55
Tent 17.85 25.05 10.70 22.23 18.96 25.24 32.28 20.83 26.12
EATA 14.22 24.45 10.39 21.23 17.57 25.39 30.04 22.70 26.04
TLM 8.63 22.23 26.20 21.66 19.68 28.60 32.76 21.37 27.58
Dynamic-Ref
SYTTA-2 12.68 25.66 24.89 22.10 21.33 32.01 33.78 21.64 29.14
QWEN-2.5-7B SYTTA-4 18.17 25.80 25.34 23.05 23.09 32.09 Q 22.70 29.45
. SYTTA-8 19.78 25.95 25.67 23.88 23.82 33.16 24.78 30.57
SYTTA-16 18.91 25.00 21.73 23.64 22.32 32.20 24.03 29.81
Static-Ref
SYTTA-2 11.03 25.73 25.56 21.86 21.05 31.51 33.63 21.41 28.85
SYTTA-4 17.11 25.11 25.25 22.39 22.47 33.27 33.50 24.00 30.26
SYTTA-8 19.06 25.56 25.02 23.16 23.20 33.63 33.32 23.43 30.12
SYTTA-16 15.70 26.42 20.54 23.03 21.42 = 32.56 32.86 23.66 29.69
Base Model 8.35 17.94 9.88 16.95 13.28 22.84 28.66 18.01 23.17
Tent 12.38 24.21 12.82 21.30 17.68 32.76 30.24 19.27 27.42
EATA 9.67 24.83 12.65 19.69 16.71 R 30.63 19.36 27.50
TLM 8.74 23.71 29.17 22.01 20.91 33.22 20.32 27.50
Dynamic-Ref
SYTTA-2 13.44 26.25 26.69 22.68 22.26 = 31.52 33.65 22.25 29.14
QweEN-2.5-14B SYTTA-4 17.52 25.75 27.06 22.13 23.12 31.41 33.73 23.77 29.64
. SYTTA-8 19.78 27.07 19.73 23.00 22.39 33.38 27.11 30.56
SYTTA-16 15.73 24.81 26.57 22.63 22.43 32.98 24.11 29.87
Static-Ref
SYTTA-2 11.47 25.74 25.79 21.04 21.01 32.33 33.47 21.08 28.96
SYTTA-4 16.97 26.00 25.27 22.32 22.64 32.97 33.44 23.24 29.88
SYTTA-8 26.50 20.69 22.86 20.86 32.85 33.47 23.76 30.02
SYTTA-16 26.58 LQ 22.25 21.41 32.49 22.91 29.39

23


Under review as a conference paper at ICLR 2026

Table 8: Detailed results on DomainBench and InstructBench. BLEU scores (x100 for visibility;
higher is better). For each model and dataset, the highest score is bold and the second-highest is underlined.

DomainBench InstructBench
Model Method Agriculture GeoSignal GenMedGPT Wealth Avg. Dolly Alpaca-GPT4 InstructWild Avg.
Base Model 1.00 A471 1.65 3.11 2.62 7.64 8.68 3.35 6.55
Tent 1.04 4.24 1.59 2.15 2.26 7.10 10.55 3.66 7.10
EATA 1.08 6.78 2.01 2.99 3.21 6.20 8.73 2.92 5.95
TLM 2.24 7.96 5.21 5.33 5.18 6.31 10.72 4.50 7.18
Dynamic-Ref
SyYTTA-2 8.34 5.39 6.42 5.87 8.35 12.47 7.08 9.30
SYTTA-4 TAB 2.50 6.39 4.90 9.52 12.68 7.86 10.02
ELAMA-3.2-3B svt Ag 8.11 2.74 6.34 5.08 9.64 12.23 7.38 9.75
SYTTA-16 3.12 4.76 2.40 5.60 3.97 6.84 10.55 5.83 7.74
Static-Ref
SYTTA-2 2.44 7.61 2.52 6.25 4.70 10.37 11.96 6.22 9.51
SyTTA-4 3.58 8.12 2.51 6.36 5.14 10.00 12.72 8.47 10.40
SYTTA-8 2.81 7.80 3.44 6.08 5.03 10.07 11.65 7.88 9.86
SYTTA-16 2.26 7.A3 2.40 5.57 4Al 9.79 10.12 5.83 8.58
Base Model 1.10 4.69 1.54 3.10 2.61 9.31 8.70 3.30 7.10
Tent 1.37 6.45 2.16 3.78 3.44 TAZ 5.39 4.37 5.63
EATA 1.06 6.20 legal 1.12 2.52 10.04 1.12 4.10 5.09
TLM 2.78 8.08 6.27 6.21 5.83 9.48 10.57 4.81 8.29
Dynamic-Ref
SyYTTA-2 2.96 8.99 6.98 6.74 6.42 10.27 12.87 5.55 9.56
LLAMA-3.1-8B SYTTA-4 3.52 8.63 7.57 6.65 6.59 10.04 12.95 8.51 10.50
SYTTA-8 3.59 8.27 5.48 7.13 6.12 9.43 12.84 7.56 9.94
SYTTA-16 3.63 6.45 6.17 5.75 5.50 TAZ 11.04 8.21 8.79
Static-Ref
SYTTA-2 2.65 8.35 3.78 6.66 5.36 10.69 12.58 5.24 9.50
SyTTA-4 2.44 8.20 5.57 6.46 5.67 12.01 12.84 8.19 11.01
SYTTA-8 2:71 8.05 6.93 6.53 6.05 10.88 12.31 6.76 9.98
SYTTA-16 2.49 7.85 3.97 6.23 5.14 10.24 11.92 6.64 9.60
Base Model 1.22 4A8 1.23 3.80 2.68 6.62 10.48 3.75 6.95
Tent 3.54 8.01 1.79 6.17 4.88 8.57 12.92 5.49 8.99
EATA 2.32 8.11 1.70 5.90 4.51 8.74 10.85 6.27 8.62
TLM 1.47 7.25 9.98 6.29 6.25 10.70 13.14 6.07 9.97
Dynamic-Ref
SyYTTA-2 2.59 8.66 9.24 6.47 6.74 11.62 14.39 6.21 10.74
QweN-2.5-7B SYTTA-4 3.66 8.68 9.69 6.84 7.22 12.42 13.77 6.84 11.01
” SYTTA-8 4.35 8.82 9.58 7.27 7.51 12.33 7.75 11.48
SYTTA-16 3.94 8.29 TAZ 7.27 6.73 12.16 21. 10.98
Static-Ref
SyTTA-2 2.12 8.88 9.68 6.37 6.76 11.70 14.32 6.12 10.71
SyYTTA-4 3.39 8.53 9.34 6.56 6.96 12.92 13.92 7.46 11.43
SYTTA-8 4.01 8.86, 9.28 6.95 7.27 13.04 13.84 7.35 11.41
SYTTA-16 3.09 8.51 6.99 6.95 6.38 12.20 14.27 TAT 11.31
Base Model 1.46 4.99 1.49 3.92 2.97 6.77 11.37 3.99 7.38
Tent 2.27 7.67 1.81 6.22 449 11.85 12.11 4.47 9.47
EATA 1.63 7.24 1.95 5.45 4.07 11.71 13.07 5.08 9.95
TLM 1.46 8.10 11.10 6.44 6.77 10.63 14.12 5.50 10.08
Dynamic-Ref
SYTTA-2 2.58 9.41 9.96 6.71 7.16 12.07 14.95 6.55 11.19
QWEN-2.5-14B SyTTA-4 3.40 9.39 9.86 6.51 7.29 = 11.53 14.89 7.26 11.23
” SYTTA-8 4.00 9.04 6.07 6.96 6.52 11.87 14.63 8.78 11.76
SYTTA-16 3.12 8.25 9.58 6.79 6.94 11.71 13.86 TAB8 11.02
Static-Ref
SyTTA-2 2.12 8.71 9.37 6.03 6.56 12.55 14.81 5.82 11.06
SyYTTA-4 3.50 8.79 8.91 6.54 6.94 12.28 14.83 7.08 11.40
SYTTA-8 2.41 9.22 6.03 6.93 6.15 12.18 14.34 7.33 11.28
SYTTA-16 4.24 9.62 4,23 6.46 6.14 12.13 14.05 7.11 11.10

24
