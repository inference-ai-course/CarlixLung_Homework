arXiv:2210.13865vl1 [cs.CL] 25 Oct 2022

Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for
Misinformation

Max Glockner', Yufang Hou’, Iryna Gurevych!
' Ubiquitous Knowledge Processing Lab (UKP Lab),

Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt

2 IBM Research Europe, Ireland
www.ukp. tu-darmstadt.de, yhou@ie. ibm. com

Abstract

Misinformation emerges in times of uncer-
tainty when credible information is lim-
ited. This is challenging for NLP-based
fact-checking as it relies on counter-evidence,
which may not yet be available. Despite in-
creasing interest in automatic fact-checking, it
is still unclear if automated approaches can
realistically refute harmful real-world misin-
formation. Here, we contrast and compare
NLP fact-checking with how professional fact-
checkers combat misinformation in the ab-
sence of counter-evidence. In our analysis, we
show that, by design, existing NLP task def-
initions for fact-checking cannot refute mis-
information as professional fact-checkers do
for the majority of claims. We then define
two requirements that the evidence in datasets
must fulfill for realistic fact-checking: It must
be (1) sufficient to refute the claim and (2)
not leaked from existing fact-checking articles.
We survey existing fact-checking datasets and
find that all of them fail to satisfy both cri-
teria. Finally, we perform experiments to
demonstrate that models trained on a large-
scale fact-checking dataset rely on leaked ev-
idence, which makes them unsuitable in real-
world scenarios. ‘Taken together, we show
that current NLP fact-checking cannot realis-
tically combat real-world misinformation be-
cause it depends on unrealistic assumptions
about counter-evidence in the data!.

1 Introduction

According to van der Linden (2022), misinforma-
tion is “false or misleading information masquerad-
ing as legitimate news, regardless of intent’. Mis-
information is dangerous as it can directly impact
human behavior and have harmful real-world con-
sequences such as the Pizzagate shooting (Fisher
et al., 2016), interfering in the 2016 democratic US

'Code provided at https://github.com/UKPLab/
emn1p2022-missing-counter-evidence

Facebook posts
stated on December 19, 2021 in a Facebook post:
"Half a million sharks could be
killed to make the COVID-19
vaccine."

I

99

Many vaccines use
squalene harvested
from sharks

Only few (not yet
approved) COVID
vaccines use squalene

Figure 1: A false claim from PolitiFact. It is unlikely to
find counter-evidence. Fact-checkers refute the claim
by disproving why it was made.

election (Bovet and Makse, 2019), or the promo-
tion of false COVID-19 cures (Aghababaeian et al.,
2020). Surging misinformation during the COVID-
19 pandemic, coined “infodemic” by WHO (Zaro-
costas, 2020), exemplifies the danger coming from
misinformation. To combat misinformation, jour-
nalists from fact-checking organizations (e.g., Poli-
tiFact or Snopes) conduct a laborious manual effort
to verify claims based on possible harms and their
prominence (Arnold, 2020). However, manual fact-
checking cannot keep pace with the rate at which
misinformation is posted and circulated. Auto-
matic fact-checking has gained significant attention
within the NLP community in recent years, with the
goal of developing tools to assist fact-checkers in
combating misinformation. For the past few years,
NLP researchers have created a wide range of fact-
checking datasets with claims from fact-checking
organization websites (Vlachos and Riedel, 2014;
Wang, 2017; Augenstein et al., 2019; Hanselowski
et al., 2019; Ostrowski et al., 2021; Gupta and
Srikumar, 2021; Khan et al., 2022). The fundamen-
tal goal of fact-checking is, given a claim made


by a claimant, to find a collection of evidence and
provide a verdict about the claim’s veracity based
on the evidence. The underlying technique used by
fact-checkers, and journalists in general, to assess
the veracity of a claim is called verification (Silver-
man, 2016). In a comprehensive survey, Guo et al.
(2022) proposed an NLP fact-checking framework
(FCNLP) that aggregates existing (sub)tasks and
approaches of automated fact-checking. FCNLP
reflects current research trends on automatic fact-
checking in NLP and divides the aforementioned
process into evidence retrieval, verdict prediction,
and justification production.

In this paper, we focus on harmful misinfor-
mation claims that satisfied the professional fact-
checkers’ selection criteria and refer to them as
real-world misinformation. Our goal is to answer
the following research question: Can evidence-
based NLP fact-checking approaches in FCNLP
refute novel real-world misinformation? Fc-
NLP assumes a system has access to counter-
evidence (e.g., through information retrieval) to re-
fute a claim. Consider the false claim “Telemundo
is an English-language television network” from
FEVER (Thorne et al., 2018): A system following
FCNLP must find counter-evidence contradicting
the claim (i.e., Telemundo is a Spanish company)
to refute the claim. This may require more com-
plex reasoning over multiple documents. We con-
trast this example to the real-world false claim that
“Half a million sharks could be killed to make the
COVID-19 vaccine” (Figure 1). If true, credible
sources would likely report this incident, providing
supporting evidence. As it is not, before being fact-
checked, there is no refuting evidence stating that
COVID-19 vaccine production will not kill sharks.
Only after guaranteeing that the claim relies on the
false premise of COVID-19 vaccines using squa-
lene (harvested from sharks), it can be refuted. Af-
ter the claim’s verification, fact-checkers publish
reports explaining the verdict and thereby produce
counter-evidence. Relying on counter-evidence
leaked from such reports is unrealistic if a system
is to be applied to new claims.

In this work, we identify gaps between current
research on FCNLP and the verification process of
professional fact-checkers. Via analysis from dif-
ferent perspectives, we argue that the assumption
of the existence of counter-evidence in FCNLP is
unrealistic and does not reflect real-world require-
ments. We hope our analysis sheds light on future

Verdict
60 — False
—— Mixed
— True

a
o

Percentage (%)

nN
Oo

=
Oo

2008

2010 2012 2014

Year

2016 2018 2020

Figure 2: Ratio of verdicts per year (PolitiFact).

research directions in automatic fact-checking. In
summary, our major contributions are:

¢ We identify two criteria from the journalistic
verification process, which allow overcoming
the reliance on counter-evidence (Section 2).

¢ We show that FCNLP is incapable of satis-
fying these criteria, preventing the success-
ful verification of most misinformation claims
from the journalistic perspective (Section 3).

¢ We identify two evidence criteria (sufficient &
unleaked) for realistic fact-checking. We find
that all existing datasets in FCNLP containing
real-world misinformation violate at least one
criterion (Section 4) and are hence unrealistic.

¢ We semi-automatically analyze MULTIFC, a
large-scale fact-checking dataset to support
our findings, and show that models trained
on claims from PolitiFact and Snopes (via
MULTIFC) rely on leaked evidence.

2 How Humans Fact-check

To motivate our distinct focus on misinforma-
tion, we investigate what claims professional fact-
checkers verify. We crawl 20,274 fact-checked
claims from PolitiFact 7 ranging from 2007-2021.
Figure 2 shows the ratio of different verdicts > per
year. After 2016, fact-checkers increasingly select
false claims as important for fact-checking. In 2021
less than 10% of the selected claims were correct.

Some claims can be refuted via counter-evidence
(as required by FCNLP). For example, official

“https ://www.politifact.com/

3We conservatively group verdicts “pants on fire” and
“false” to False, “mostly false” and “half true” to Mixed and
“mostly true” and “true” to True.


Claim Based Upon

(1) _ If you were forced to use a Sharpie to fill out your ballot, that is voter fraud.

(2) The Biden administration will begin "spying" on bank and cash app accounts starting 2022.
(3) Barcelona terrorist is cousins with former President Barack Obama.

(4) The Democratic health care plan is a government takeover of our health programs.

(5) People in Holland protests against of COVID-19 measures.

false assumption
tax legislation
satire article
health care plan
protests event

Table 1: Example misinformation claims for source guarantee.

statistics can contradict the false claim about the
USS. that “In the 1980s, the lowest income people
had the biggest gains’. If the evidence makes it
impossible for the claim to be true (e.g., because
of mutually exclusive statistics) we refer to the evi-
dence as global counter-evidence. Global counter-
evidence attacks the textual claim itself without
relying on reasoning and sources behind it. In
contrast, to refute the claim that “COVID-19 vac-
cines may kill sharks” (Figure 1), fact-checkers
did not rely on global counter-evidence specifically
proofing that sharks will not be killed to produce
COVID-19 vaccines. Neither is it plausible that
such counter-evidence exists. Here, the counter-
evidence is bound to the claim’s underlying (false)
reasoning. The claim is only refuted because it
follows the false assumption, not because it was
disproved. The absence of global counter-evidence
is not an exceptional problem for this specific claim
but is common among misinformation: Misinfor-
mation surges when the high demand for infor-
mation cannot be met with a sufficient supply of
credible answers (Silverman, 2014; FullFact, 2020).
Non-credible and possibly false and harmful infor-
mation fill these deficits of credible information
(Golebiewski and Boyd, 2019; Shane and Noel,
2020). The very existence of misinformation often
builds on the absence of credible counter-evidence,
which in turn, is essential for FCNLP.

Professional fact-checkers refute misinformation
even if no global counter-evidence exists, e.g., by
rebutting underlying assumptions (Figure 1). Ta-
ble 1 shows a few false claims built on top of vari-
ous resources: (1) relies on a false assumption that
sharpies invalidate election ballots, (2 & 4) misin-
terpret official documents or laws, (3) is based on
non-credible sources, and (5) changes a topic of a
specific event from “gas extraction” to “COVID-19
measures’. Fact-checkers use the reasoning for
the claim to consider evidence that is, or refers to,
the claimant’s source: the original tax legislation
(2), or alternate (correct) descriptions of protests
against gas extraction (5). Here, the content of the

evidence alone is often insufficient. The assertion
that the claimant’s source and the used counter-
evidence are identical, or refer to the same event
is crucial to refute the claim: Claim (2) is refuted
because the tax legislation it relies upon does not
support the “spying” claim. However, the docu-
ment does not specifically refute the claim, and
without knowing that the claimant relied on it, it
becomes useless as counter-evidence. Similarly,
the correct narrative of protests against gas extrac-
tion is only mutually exclusive to the false claim
(5) of protests against COVID-19 measures when
assuring both refer to the identical incident. For
similar reasons, the co-reference assumption is crit-
ical to the task definition of SNLI (Bowman et al.,
2015). After this assertion, mutual exclusiveness
is not required to refute the claim: It is sufficient
if the claim is not entailed (i.e. incorrectly derived
or relies on unverifiable speculations) or based on
invalid sources (such as satire) to refute it. Based
on these observations we identify two criteria to
refute claims if no global-counter evidence exists.
We validate their relevance in Section 3:

¢ Source Guarantee: The guarantee that iden-
tified evidence either constitutes or refers to
the claimant’s reason for the claim.

* Context Availability: We broadly consider
context as the claim’s original environment,
which allows us to unambiguously compre-
hend the claim, and trace the claim and its
sources across multiple platforms if required.
It is a logical precondition for the source guar-
antee.

Both criteria are challenging for computers but nat-
urally satisfied by human fact-checkers. Buttry
(2014) defines the question “How do you know
that?” to be at the heart of verification. After se-
lecting a claim, finding provenance and sourcing
are the first steps in journalistic verification. Prove-
nance provides crucial information about context
and motivation (Urbani, 2020). Journalists must
then identify solid sources to compare the claim


with (Silverman, 2014; Borel, 2016). Ideally, the
claimant provides sources, which must be included
and assessed in the verification process. During
verification, journalists rely, if possible, on relevant
primary sources, such as uninterpreted and original
legislation documents (for claim 2, Table 1). Fact-
checking organisations see sourcing as one of the
most important parts of their work (Arnold, 2020).

3 Can FCNLP Help Human Verification?

In this section, we first analyze human verification
strategies based on an analysis of 100 misinforma-
tion claims. We then contrast human verification
strategies with FCNLP.

3.1 Human Verification Strategies

We manually analyze 100 misinformation claims*

from two well-known fact-checking organizations:
PolitiFact and Snopes. We randomly choose 50
misinformation claims from each website which
contains 25 claims from MULTIFC (a large NLP
fact-checking dataset with real-world claims before
2019) and 25 claims from 2020/2021. We extract
the URL for each claim and analyze its verifica-
tion strategy based on the entire fact-checking arti-
cle. Claims that require the identification of scam
webpages, imposter messages, or multi-modal rea-
soning® such as detecting misrepresented, mis-
captioned or manipulated images (Zlatkova et al.,
2019) were marked as not applicable to FCNLP
by nature. In the first round of analysis, we assess
whether humans relied on the source guarantee to
refute the claim. Each claim (and its verification)
is unique and can be refuted using different strate-
gies. In the second round of analysis we identify
the primary strategy to refute the claim and verify
that it is based on the source guarantee. This led us
to identify 4 primary human-verification strategies:

1. Global counter-evidence (GCE): Counter-
evidence via arbitrarily complex reasoning but
without the source guarantee.

2. Local counter-evidence (LCE): Evidence re-
quires the source guarantee to refute the (rea-
soning behind) the claim.

“Claims are from the following categories: “pants on fire”,
“false” and “mostly false’.

>If a claim can be expressed in text and verified without
multi-modal reasoning we consider the verbalized variant of
the claim and do not discard it.

Sre. Strategy | MULTIFC 20/21 All %
yes LCE 19 16 35 46.7
yes NCS 9 5 14 18.7
no GCE 10 10 20 26.7
no NEA 1 4 5 6.7
no other 0 1 1 1.3
yes all 28 21 49 65.3
no all 11 15 26 34.7
all all 39 36 75 ~=100.0

Table 2: Strategies used to refute 75 of 100 misinforma-
tion claims with and without source guarantee (Sre.).

3. Non-credible source (NCS): Evidence re-
quires the source guarantee to refute the claim
based on non-credible sources (e.g. satire).

4. No evidence assertion (NEA): The claim is
refuted as no (trusted) evidence supports it.

We discard 25 non-applicable claims and show
the results of the remaining 75 claims in Table 2.
Please refer to Appendix A for more analysis de-
tails and examples. In some cases, the selection of
one strategy is ambiguous if multiple strategies are
applied. In a pilot study to analyze human verifica-
tion strategies, two co-authors agreed on 9/10 ap-
plicable misinformation claims. In general, about
two-thirds of the claims were refuted by relying on
the source guarantee. In 20 cases fact-checkers re-
futed the claim by finding global counter-evidence.
In one case (other), fact-checkers relied entirely on
expert statements. In general, experts supported
the fact-checkers in identifying and discussing ev-
idence, or strengthened their argument via state-
ments but did not affect the underlying verification
strategy.

3.2. NLP Fact Verification

Focusing on evidence-based approaches. Ap-
proaches in FCNLP estimate the claim’s veracity
based on surface cues within the claim (Rashkin
et al., 2017; Patwa et al., 2021), assisted with meta-
data (Wang, 2017; Cui and Lee, 2020; Li et al.,
2020; Dadgar and Ghatee, 2021), or using evidence
documents. Here, the system uses the stance of
the evidence towards the claim to predict the ver-
dict. Verdict labels are often non-binary and in-
clude a neutral stance (Thorne et al., 2018), or fine-
grained veracity labels from fact-checking organi-
zations (Augenstein et al., 2019). Evidence-based
approaches either rely on unverified documents or
user comments (Ferreira and Vlachos, 2016; Zu-
biaga et al., 2016; Pomerleau and Rao, 2017), or


assume access to a presumed trusted knowledge
base such as Wikipedia (Thorne et al., 2018), scien-
tific publications (Wadden et al., 2020), or search
engine results (Augenstein et al., 2019). In this pa-
per, we focus on trusted evidence-based verification
approaches which can deal with the truth changing
over time (Schuster et al., 2019). More importantly,
they are the most representative of professional fact
verification. Effectively debunking misinformation
requires stating the corrected fact and explaining
the myth’s fallacy (Lewandowsky et al., 2020), both
of which require trusted evidence.

Global counter-evidence assumption in FCNLP.
In FCNLP, evidence retrieval-based approaches
assume that the semantic content of a claim is
sufficient to find relevant (counter-) evidence in a
trusted knowledge base (Thorne et al., 2018; Jiang
et al., 2020; Wadden et al., 2020; Aly et al., 2021).
This becomes problematic for misinformation that
requires the source guarantee to refute the claim.
By nature, in this case, the claim and evidence
content are distinct and not entailing. Content can-
not assert that two different narratives describe the
same protests (e.g., Claim 5 in Table 1), or that
a non-entailing fact (squalene is harvested from
sharks) serves as a basis for the false claim (e.g.,
Figure 1). The consequence is a circular reasoning
problem: Knowing that a claim is false is a precon-
dition to establishing the source guarantee, which
in turn is needed to refute the claim. To escape
this cycle, one must (a) provide the source guar-
antee by other means than content (e.g., context),
or (b) find evidence that refutes the claim without
the source guarantee (global counter-evidence). By
relying only on the content of the claim, FCNLP
cannot provide the source guarantee and is limited
to global counter-evidence, which only accounts
for 20% of misinformation claims analyzed in the
previous section.

Current FCNLP fails to provide source guaran-
tees. We note that providing the source guarantee
goes beyond entity disambiguation, as required in
FEVER (Thorne et al., 2018). The self-contained
context within claims in FEVER is typically suffi-
cient to disambiguate named entities if required.°
After disambiguation, the retrieved evidence serves
as global counter-evidence.

®Tn the claim “Poseidon grossed $181,674,817 at the world-
wide box office on a budget of $160 million’ it is clear that
“Poseidon” refers to the film, not an ancient god. (FEVER)

Recent approaches further add context snippets
from Wikipedia (Sathe et al., 2020) or dialogues
(Gupta et al., 2022) to resolve ambiguities and can-
not provide the source guarantee to break the circu-
lar reasoning problem. These snippets differ from
the context used by professional fact-checkers who
often need to trace claims and their sources across
different platforms. Recently, Thorne et al. (2021)
annotate more realistic claims w.r.t. multiple evi-
dence passages. They found supporting and refut-
ing passages for the same claim, which prevents
the prediction of an overall verdict. Some works
collect evidence for the respective claims by identi-
fying scenarios where the claimant’s source is nat-
urally provided: such as a strictly moderated forum
(Saakyan et al., 2021), scientific publications (Wad-
den et al., 2020), or Wikipedia references (Sathe
et al., 2020). However, such source evidence is
only collected for true claims. Adhering to the
global counter-evidence assumptions of previous
work, false claims in these works are generated
artificially and do not reflect real-world misinfor-
mation.

3.3. Human and NLP Comparison

Our analysis (Table 2) finds fact-checkers only
refuted 26% of false claims with global counter-
evidence. In all other cases, fact-checkers relied on
source guarantees (LCE, NCS) or asserted that no
supporting evidence exists (NEA). The verification
strategy is not evident given the claim alone but
dependent on existing evidence. The claim that
“President Barack Obama’s policies have forced
many parts of the country to experience rolling
blackouts” is refuted via global counter-evidence
(that rolling blackouts had natural causes). The
claim that “90% of rural women and 55% of all
women are illiterate in Morocco” seems verifiable
via official statistics. Yet, no comparable statistics
exist and the claim is refuted due to relying on a
decade-old USAID request report.

We further analyze claims refuted via global
counter-evidence, that FCNLP, in theory, can re-
fute. Some claims only require shallow reasoning
as directly contradicting evidence naturally exists:
A transcript of an interview in which Ron DeSantis
was asked about the coronavirus can easily refute
the claim “Ron DeSantis was never asked about
coronavirus’. Another case is when information
about the claim’s veracity already exists, e.g., be-
cause those affected by the myth already corrected


Claims Evidence Ey.
Dataset Source False Claims | Unleaked Sufficient | Ann.

1 SCIFACT (Wadden et al., 2020) Scientific generated n/a v v
2 COvID-FACT (Saakyan et al., 2021) Reddit generated n/a v v
3. WIKIFACTCHECK (Sathe et al., 2020) Wikipedia generated n/a v v
4 Fm? (Eisenschlos et al., 2021) Game generated n/a v v
5 Thorne et al. (2021) User Queries = paraphrased n/a v v
6 ~~ FAVIQ (Park et al., 2022) User Queries = paraphrased n/a v no
7 LIARPLUS (Wang, 2017; Alhindi et al., 2018) FC Article real-world no v v
8 | POLITIHOP (Ostrowski et al., 2021) FC Article real-world no v v
9  CLIMATEFEVER (Diggelmann et al., 2020) Web real-world v no v
10 HEALTHVER (Sarrouti et al., 2021) Web real-world v no v
11. UKP-SNOoPEs (Hanselowski et al., 2019) FC Article real-world v no v
12. PUBHEALTH (Kotonya and Toni, 2020b) FC Article real-world v no no
13. WATCLAIMCHECK (Khan et al., 2022) FC Article real-world v no no
14.‘ Baly et al. (2018) FC Article real-world no no v
15 MULTIFC (Augenstein et al., 2019) FC Article real-world no no no
16 X-FAcT (Gupta and Srikumar, 2021) FC Article real-world no no no

Table 3: Overview of NLP fact-checking datasets as realistic test-beds to combat real-world misinformation. We
indicate whether humans annotated the stance between claim and evidence (Ev. Ann.)

the claim. Most claims require complex reasoning
like legal text understanding or comparing and de-
riving statistics. Some claims require the definition
of some terms first, to make them verifiable. Col-
lecting all required global counter-evidence often
requires aggregating and comparing different infor-
mation, possibly under time constraints. Consider
the false claim that “J//egal immigration wasn’t a
subject that was on anybody’s mind until [Trump]
brought it up at [his] announcement’: To refute
this claim, one must first determine when Trump
announced his run for the presidency, then count
and compare how often “illegal immigration” was
mentioned before and after the announcement.’

4 NLP Fact-Checking Datasets

Based on our observations in Section 3 and FC-
NLP’s reliance on global counter-evidence, we hy-
pothesize that evidence in existing fact-checking
datasets does not fully satisfy real-world demands.
We, hence, investigate how FCNLP’s assumptions
affect fact-checking datasets and if they constitute
realistic test beds for real-world misinformation.
For real-world scenarios, datasets must contain real-
world misinformation claims and realistic counter-
evidence. For evidence, we define the following
two requirements:

¢ Sufficient: Evidence must be sufficient to jus-
tify the verdict from a human perspective.

TWe assume disambiguation requires no source guarantees,
and that basic context (date, location, claimant) is known.

¢ Unleaked: Evidence must not contain infor-
mation that only existed after the claim was
verified.

The issue of leaked evidence was also mentioned
very recently by Khan et al. (2022). Unlike us, they
did not comprehensively analyze existing datasets,
evaluate the impact on trained systems (Section 5),
or consider the complementary criterion of suffi-
cient (counter-) evidence. Relying on leaked evi-
dence is related to the important yet different task
of detecting already-verified claims (Shaar et al.,
2020), but is unrealistic for novel claims.

We survey NLP fact-checking datasets with nat-
ural input claims® that assume access to trusted
evidence. Table 3 summarizes our survey results.
Datasets 1-6 contain no real-world misinforma-
tion: False claims are derived from true real-world
claims (1-3) or within a gamified setting (4), ensur-
ing that counter-evidence exists. Other works (5 &
6) reformulate real-world user queries, which are
linked to Wikipedia articles as (counter-) evidence.

We find that no dataset with real-world misin-
formation (7-16) satisfies both evidence criteria.
We identify four categories: First, datasets that
consider (parts of) a fact-checking article as ev-
idence contain sufficient, yet leaked evidence (7
& 8). Second, annotators estimate claim veracity
based on evidence such as Wikipedia or scientific
publications. The authors find that evidence of-

5See the survey from Guo et al. (2022) for datasets with
natural and artificial claims.


Detected via Claim & Evidence

phrase & Claim: Google Earth Finds SOS From Woman Stranded on Deserted Island

snippet URL __ Evidence Snippet: The Truth: The story is a hoax. ... GOOGLE EARTH FINDS WOMAN TRAPPED ON
DESERTED ISLAND FOR 7 YEARS ... other end “How did you find me” to which they replied “Some kid
from Minnesota found your SOS sign on Google Earth”; From Truth Or Fiction

phrase Claim: Country music singer Merle Haggard left his entire estate to an LGBT group.

Evidence Snippet: Discover ideas about Country Singers. Fake news reports that recently-deceased country
music legend Merle Haggard left his entire estate to an LGBT group; From Pinterest

Table 4: Examples from MULTIFC of leaked evidence detected via the snippet URL (linking to a fact-checking

article) or a phrase of the evidence snippet.

ten only covers parts of these realistic, complex
claims, which yield low annotator agreement (9),
or a weakened task definition for stances (10).

Third is to rely on the same evidence as fact-
checkers, termed premise evidence by Khan et al.
(2022). Here, only UKP-SNOPES (11) provides ev-
idence annotations. Hanselowski et al. (2019) col-
lect and annotate original evidence snippets from
the fact-checking article. They find the stance of-
ten conflicts with the verdict: Though most claims
are false, the majority of evidence is supporting.
In 45.5% of cases, annotators found no stance for
the professionally selected evidence snippets even
though professional fact-checkers considered these
snippets important to be included in the article. Due
to conflicting and unexplained evidence snippets,
we rate this insufficient to predict the correct verdict.
The human verification process (Section 2) guides
the creation of the fact-checking article and can
serve as a possible explanation for these problems.
Articles link to the claim’s context and possibly
other similar claims (likely supporting). Often (e.g.
during COVID-19 (Simon et al., 2020)), claims
are not completely fabricated. Fact-checkers iden-
tify documents and their interdependence when
investigating the claimant’s reasoning for the claim
(likely not refuting). Documents used to disprove
the claimant’s reasoning may have no or little rele-
vance to the original claim (as in Figure 1). Each
step is non-trivial and may rely on numerous docu-
ments (or expert statements). Relying on premise
evidence without considering the verification pro-
cess and how these documents relate, is insufficient.
Both other datasets (12 & 13) in this category pro-
vide no annotations and are limited to freely avail-
able evidence documents (as opposed to paywalled
web pages or e-mails).

Fourth is using a search engine during dataset
construction to expand the accessible knowledge.
Even when excluding search results that point to

the claim’s fact-checking article, leaked evidence
persists: Different organizations may verify the
same claims, or disseminate the fact-checkers ver-
ification. Only Baly et al. (2018) provide stance
annotation for Arabic claim and evidence pairs.
For false claims, they found that only a few doc-
uments disagree, and more agree, with the claim.
A possible explanation is that misinformation of-
ten emerges when trusted information or counter-
evidence is scarce. Fact-checking articles fill this
deficit. Partially excluding them during dataset gen-
eration reduces the found counter-evidence. Lack-
ing counter-evidence is not a problem of the dataset
generation, but the underlying nature of misinfor-
mation, and should be considered by the task def-
inition. We rate evidence in this category (14-16)
leaked and insufficient, and back it up in Section 5.

5 A Case Study of Leaked Evidence

We view MULTIFC (Augenstein et al., 2019), the
largest dataset of its group, as an instantiation of
FCNLP applied to the real world: It contains real-
world claims and professionally assessed verdicts
as labels from 26 fact-checking organizations (like
Snopes or PolitiFact). The authors use the Google
search engine to expand evidence retrieval to the
real world during dataset construction. We abstract
from the fact that MULTIFC only provides incom-
plete evidence snippets and consider (if possible)
the underlying article in its entirety.

5.1 Quantification of Leaked Evidence

We focus our analysis on 16,244 misinformation
claims that we identify via misinformation labels
(listed in Appendix B.1). To quantify how many
claims in MULTIFC contain leaked evidence, we
consider all evidence snippets stemming from a
fact-checking article, or discussing the veracity of
a claim, as leaked. Table 4 shows examples of
leaked evidence that strongly indicates the claim’s


Leaked Claims (Number) Claims (%)
Url 8,999 55.6%
Phrase 9,656 59.7%
Url or Phrase 11,267 69.7%

Table 5: Absolute and relative number of automatically
identified leaked evidence of MULTIFC misinforma-
tion.

Claim has leaked evidence

Categories All Leaked Unleaked
Any 100 32 68
No Stance 37 0 37

No Refuting 63 0 63
Original & Refuting | 15 10 5

Table 6: Manual analysis of 100 claims without auto-
matically identified leaked evidence.

verdict. The first snippet comes directly from a fact-
checking organization (Truth Or Fiction’). Only
identifying leaked evidence that directly comes
from fact-checking organizations is insufficient:
After the publication of the verification report, its
content is disseminated via other publishers (such
as Pinterest in the second example). We identify
leaked evidence snippets using patterns for their
source URLs or contained phrases. A complete list
of all used patterns is given in Appendix B.2). This
requires the evidence to be relevant. Irrelevant arti-
cles are insufficient, albeit not leaking. To this end,
we manually analyze 100 claims with 230 automat-
ically found leaking evidence snippets. We confirm
that 83.9% of the snippets are leaked (details in Ap-
pendix B.3). 97/100 of the selected claims contain
at least one leaked evidence snippet.

Table 5 lists the number of claims with leaked ev-
idence identified by the pattern-based approach. It
detected leaked evidence for 69.7% of misinforma-
tion claims. In addition, we manually analyze evi-
dence of 100 misinformation claims for which this
approach found no leaked evidence. Misinforma-
tion verification often requires multiple evidence
documents, rendering a single sufficient evidence
snippet unrealistic. We follow Sarrouti et al. (2021)
and test if a snippet supports or refutes parts of
the claim. Table 6 shows that approximately one-
third of the claims contain further leaked evidence.
15 claims have unleaked refuting evidence. In 10
cases this evidence is overshadowed via leaked ev-
idence for the same claim. Most analyzed claims

https ://www.truthorfiction.com/

Input All Leaked Unleaked A
Snopes

Samples 1,014 482 532

Sn.-Text | 29.4/60.4 26.3/66.3 30.1/55.0 = +11.3
Sn.-Title | 27.3/57.8  23.8/64.6 28.2/51.5 +13.1
Snippets | 30.5/60.5  28.7/67.6 30.2/53.7. —_-+13.9
Full 32.7/62.7 30.6/68.7 = 33.0/57.2. +:11.5

PolitiFact

Samples 2,717 2,111 606

Sn.-Text | 35.5/34.5 38.0/37.2  24.1/24.5 +12.7
Sn.-Title | 48.0/47.4 55.1/54.6 21.1/21.6 +33.0
Snippets | 52.0/51.3 59.7/59.2 22.1/23.0 = +36.2
Full 52.6/51.9  59.7/59.3  25.6/25.9 +33.4

Table 7: Fl-macro/micro scores and difference in F1-
micro (A) averaged over 3 runs. Inputs are: only snip-
pet texts, titles, entire snippets or claim and snippets
(full).

only have non-refuting evidence. Similar to Baly
et al. (2018), we found supporting evidence for 40
misinformation claims; for 35 of these claims, the
evidence was misinformation and thus supported
the claim; for the remaining five claims, the claim
became accurate, and the evidence became avail-
able at a date later than the claim’s creation.

5.2 Impact on Trained Systems

Hansen et al. (2021) found that models in MUL-
TIFC can predict the correct verdict based on the
evidence snippets alone. To test if leaked evi-
dence can serve as an explanation, we fine-tune
BERT (Devlin et al., 2019) (bert-base-uncased) to
predict the veracity label of a claim given the evi-
dence snippets with and without a claim. As input
to BERT, we separate the claim (when used) from
the evidence snippets using a [SEP] token and pre-
dict the veracity label based on a linear layer on
top of a preceding [CLS] token (Training details
in Appendix C.1.). When each evidence snippet
is represented via its content only this performs
on par with the specialized model introduced by
Hansen et al. (2021). We additionally find that the
snippet’s title carries much signal, and adding it to
the input improves the overall performance on Poli-
tiFact. Snippets are concatenated (separated by “;”
in the order provided by MULTIFC and truncated
after 512 tokens. We experiment on the train-, dev-
and test-splits Hansen et al. (2021) extracted from
MULTIFC on claims from Snopes and PolitiFact.
We test four types of input: only evidence (only
title, only text, both), or the complete sample of


claim and evidence. For the evaluation (Table 7),
we split the test data based on whether a claim
contains automatically identified leaked evidence.
On Snopes, the macro-F1 is higher on the un-
leaked than on the leaked subset. Upon closer
inspection, we find that the label distribution on
Snopes is heavily skewed towards “false”, which
worsens on the leaked subset. Models seem to rely
on patterns of leaked evidence to predict the major-
ity label “false” (see Appendix C.2). On the leaked
subset, this comes at the cost of incorrect predic-
tions for all other labels, yielding a lower Fl-macro.
On the larger PolitiFact subset, labels are not much
skewed towards a single majority label. Across all
experiments, the performance gap signals the re-
liance on leaked evidence. We confirm the impact
of leaked evidence for both datasets by evaluat-
ing the model on the same instances with leaked
or unleaked evidence, to avoid the different label
distribution distorting the results (Appendix C.3).

6 Related Work

Combat Misinformation After Its Verification.
The identified limitations of the previous studies on
NLP fact-checking datasets described in Section 4
do not devalue the surveyed datasets and we view
them as highly important and useful contributions.
These limitations are tied to our specific research
question to refute novel real-world misinformation.
We strongly build on these previous works and
view them as crucial starting points to fact-check
real-world misinformation. Existing fact-checking
articles are highly valuable and automatic methods
should utilize them to detect and combat misin-
formation. Automatic methods specifically using
these resources detect misinformation by matching
claims with known misconceptions (Hossain et al.,
2020; Weinzierl and Harabagiu, 2022) or already
verified claims (Vo and Lee, 2020; Shaar et al.,
2020; Martin et al., 2022; Hardalov et al., 2022b).

Surveys on Automatic Fact-Checking. Recent
work surveyed (aspects of) automated fact-
checking and related tasks, including explainability
(Kotonya and Toni, 2020a), stance classification
(Kiictik and Can, 2020; Hardalov et al., 2022a), pro-
paganda detection (Da San Martino et al., 2021), ru-
mor detection on social media (Zubiaga et al., 2018;
Islam et al., 2020), fake-news detection (Oshikawa
et al., 2020; Zhou and Zafarani, 2020), and auto-
mated fact-checking (Thorne and Vlachos, 2018).
We refer interested readers to these papers.

Guo et al. (2022) surveyed the state of automatic
fact-checking. Based on their work, we zoom in on
real-world misinformation to investigate the gap be-
tween professional fact-checkers and FCNLP. Re-
cently, Nakov et al. (2021) surveyed tasks to assist
humans during the verification. Our work differs in
that we focus solely on the automatic verification
approach of misinformation. They argue for the
need for automatic tools to support humans during
verification. Similarly, Graves (2018) interviewed
expert fact-checkers and computer scientists and
conclude, that automatic fact-checking cannot repli-
cate professional fact-checkers in the foreseeable
future. Our results confirm the challenging na-
ture of misinformation but also outline why current
models have unrealistic expectations, and how hu-
mans overcome these problems. We believe this to
be important as real-world misinformation is well
within the scope of current NLP research.

Towards Human Verification. A possible path
forward is to align automatic verification with jour-
nalistic verification: Use the claimant’s reason-
ing to find evidence and verify the claim. This
relies on the complex task of finding the correct
sources (Arnold, 2020). A fruitful but understud-
ied direction may be automated provenance detec-
tion (Zhang et al., 2020, 2021). Building systems
that can provide source guarantees paves the way
for reasoning tasks, such as the detection of log-
ical fallacies (Jin et al., 2022), implicit warrants
(Habernal et al., 2018), or propaganda techniques
(Da San Martino et al., 2019; Huang et al., 2022).
Integrating sufficient context into datasets is non-
trivial and may require tracing a claim and its
source across multiple platforms. Existing liter-
ature shows the heterogeneity of misinformation
(Borel, 2016; Wardle et al., 2017; Cook, 2020) and
can help to identify small, focused problems that
can realistically be translated into NLP. Approaches
from computer vision focus on misinformation-
specific approaches to detect manipulated or mis-
represented images (Zlatkova et al., 2019; Ab-
delnabi et al., 2022; Musi and Rocci, 2022).

7 Conclusion

In this work, we contrasted NLP fact-checking ap-
proaches with how professional fact-checkers com-
bat misinformation. We identified that reliance
on counter-evidence hinders current fact-checking
systems to refute real-world misinformation. Us-
ing MULTIFC we find that most evidence is in-


sufficient, or leaked and exploited by trained mod-
els. Moving forward, we suggest to align NLP
approaches with the human verification process,
and task definitions with smaller and well-defined
verification strategies.

Limitations

The scope of this study is restricted to misinfor-
mation claims, and their representation as textual
statements, that professional fact-checking organi-
zations selected as important to verify. This only
represents a fraction of all existing misinforma-
tion (Vinhas and Bastos, 2022). Our findings can-
not be generalized to other types of misinforma-
tion. Process definitions for claim selection and
verification differ amongst fact-checkers (Arnold,
2020). The assessed claims for the analysis and
experiments are biased to the claim selection cri-
teria, including the domain, language, and geo-
graphical biases of Snopes and PolitiFact. Even
fact-checkers cannot fully eliminate subjectivity.
Nieminen and Sankari (2021) find 11% PolitiFact’s
verified claims uncheckable. We consider the fact-
checkers assessment as the gold standard and ad-
here to the introduced subjectivity. PolitiFact and
Snopes verify claims from English-speaking coun-
tries with rich resources and trusted government
documents. Fact-checking organizations may rely
on different strategies, adapted to different scenar-
ios such as different topics, dissemination of mis-
information, or trust and availability of official in-
formation.!° The quantification of leaked evidence
is bound to the time-frame, fact-checking organiza-
tions, and found evidence of MULTIFC. We did not
investigate the influence of different factors such as
the fact-checkers language, domain, or popularity,
nor did we evaluate different evidence collection
strategies. The same restrictions apply to the ex-
perimental results. Further, following Hansen et al.
(2021) we only consider labels on a veracity scale
for the experiments (e.g. excluding “misleading”’).

Ethics Statement

In this work we only consider publicly available
data as provided by fact-checking organizations or
MULTIFC, but do not publish it. We do not use any
personal data. We note that creating more realistic
datasets (including realistic context), as suggested

https: //www.poynter.org/fact-checking/2019/

by us, induces ethical challenges as it requires per-
sonalized data (e.g. from Twitter or Facebook). We
consider this study’s goal to reduce harmful mis-
information by aligning automatic methods with
best-practices from professional fact-checkers as
ethically correct. However, even if successfully
developed, fact-checking systems are inevitably
imperfect. Malicious actors may design claims that
exploit the system’s weakness to predict the oppo-
site verdict, giving legitimacy to false claims, or
discrediting correct claims. Further, malicious ac-
tors may develop fact-checking systems under their
control. When extended with triggers enabling
backdoor attacks (Chen et al., 2021) to control the
outcome, these systems can serve as powerful tools
to decide what seems true or false.

Acknowledgements

We thank the anonymous reviewers for their valu-
able feedback. We further thank Luke Bates, Tim
Baumgartner and Ilia Kuznetsov for their feedback
on this work. This research work has been funded
by the German Federal Ministry of Education and
Research and the Hessian Ministry of Higher Edu-
cation, Research, Science and the Arts within their
joint support of the National Research Center for
Applied Cybersecurity ATHENE and by the Euro-
pean Regional Development Fund (ERDF) and the
Hessian State Chancellery — Hessian Minister of
Digital Strategy and Development under the pro-
motional reference 20005482 (TexPrax).

References

Sahar Abdelnabi, Rakibul Hasan, and Mario Fritz.
2022. Open-Domain, Content-based, Multi-modal
Fact-checking of Out-of-Context Images via Online
Resources. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition,

pages 14940-14949.

Hamidreza Aghababaeian, Lara Hamdanieh, and Ab-
bas Ostadtaghizadeh. 2020. Alcohol intake in an at-
tempt to fight COVID-19: A medical myth in Iran.
Alcohol, 88:29-32.

Tariq Alhindi, Savvas Petridis, and Smaranda Mure-
san. 2018. Where is your evidence: Improving fact-
checking by justification modeling. In Proceedings
of the First Workshop on Fact Extraction and VER-
ification (FEVER), pages 85-90, Brussels, Belgium.
Association for Computational Linguistics.

Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,
James Thorne, Andreas Vlachos, Christos

heres-how-fact-checking-is-developing-across-africa/ Christodoulopoulos, Oana Cocarascu, and Arpit


Mittal. 2021. FEVEROUS: Fact Extraction and
VERification Over Unstructured and Structured
information. In Thirty-fifth Conference on Neural
Information Processing Systems Datasets and
Benchmarks Track (Round 1).

Phoebe Arnold. 2020. The challenges of online fact
checking: how technology can (and can’t) help.
Technical report, FullFact.

Isabelle Augenstein, Christina Lioma, Dongsheng
Wang, Lucas Chaves Lima, Casper Hansen, Chris-
tian Hansen, and Jakob Grue Simonsen. 2019.
MultiFC: A Real-World Multi-Domain Dataset for
Evidence-Based Fact Checking of Claims. In Pro-
ceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 4685-4697,
Hong Kong, China. Association for Computational
Linguistics.

Ramy Baly, Mitra Mohtarami, James Glass, Lluis
Marquez, Alessandro Moschitti, and Preslav Nakov.
2018. Integrating Stance Detection and Fact Check-
ing in a Unified Corpus. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 21-27, New Orleans, Louisiana. Association
for Computational Linguistics.

Brooke Borel. 2016. The Chicago guide to fact-
checking. University of Chicago Press.

Alexandre Bovet and Hernan A Makse. 2019.  In-
fluence of fake news in Twitter during the 2016
US presidential election. Nature communications,
10(1):1-14.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632-642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Steve Buttry. 2014. Verification fundamentals: Rules
to live by. Verification Handbook: A Definitive
Guide to Verifying Digital Content for Emergency
Coverage, pages 15-23.

Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael
Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu,
and Yang Zhang. 2021. BadNL: Backdoor Attacks
against NLP Models with Semantic-preserving Im-
provements. In Annual Computer Security Applica-
tions Conference, pages 554-569.

John Cook. 2020. Deconstructing Climate Science De-
nial. In Edward Elgar Research Handbook in Com-
municating Climate Change. Edward Elgar Publish-
ing.

Limeng Cui and Dongwon Lee. 2020. CoAID:
COVID-19 Healthcare Misinformation Dataset.
arXiv preprint arXiv:2006.00885.

Giovanni Da San Martino, Stefano Cresci, Alberto
Barron-Cedefio, Seunghak Yu, Roberto Di Pietro,
and Preslav Nakov. 2021. A survey on computa-
tional propaganda detection. In Proceedings of the
Twenty-Ninth International Conference on Interna-
tional Joint Conferences on Artificial Intelligence,

pages 4826-4832.

Giovanni Da San Martino, Seunghak Yu, Alberto
Barrén-Cedefio, Rostislav Petrov, and Preslav
Nakov. 2019. Fine-Grained Analysis of Propaganda
in News Article. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 5636-5646, Hong Kong, China. As-
sociation for Computational Linguistics.

Sajad Dadgar and Mehdi Ghatee. 2021. Checkovid:
A COVID-19 misinformation detection system on
Twitter using network and content mining perspec-
tives. arXiv preprint arXiv:2107.09768.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leippold.
2020. CLIMATE-FEVER: A Dataset for Verifica-
tion of Real-World Climate Claims. In Tackling Cli-
mate Change with Machine Learning workshop at
NeurIPS.

Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian,
Benjamin Borschinger, and Jordan Boyd-Graber.
2021. Fool me twice: Entailment from Wikipedia
gamification. In Proceedings of the 2021 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 352-365, Online. Asso-
ciation for Computational Linguistics.

William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1163-1168, San Diego, California. Associa-
tion for Computational Linguistics.

Marc Fisher, John Woodrow Cox, and Peter Hermann.
2016. Pizzagate: From rumor, to hashtag, to gunfire
in DC. Washington Post, 6:8410-8415.


FullFact. 2020. Framework for information incidents.
Technical report, FullFact.

Michael Golebiewski and Danah Boyd. 2019. Data
voids: Where missing data can easily be exploited.
Technical report, Data & Society Research Institute.

Lucas Graves. 2018. Understanding the Promise and
Limits of Automated Fact-Checking. In Reuters In-
stitute for the Study of Journalism (Reuters Institute
for the Study of Journalism Factsheets). Reuters In-
stitute for the Study of Journalism.

Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-
chos. 2022. A Survey on Automated Fact-Checking.
Transactions of the Association for Computational
Linguistics, 10:178—206.

Ashim Gupta and Vivek Srikumar. 2021. X-Fact:
A New Benchmark Dataset for Multilingual Fact
Checking. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 2: Short Papers),
pages 675-682, Online. Association for Computa-
tional Linguistics.

Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and
Caiming Xiong. 2022. DialFact: A Benchmark for
Fact-Checking in Dialogue. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
3785-3801, Dublin, Ireland. Association for Com-
putational Linguistics.

Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,
and Benno Stein. 2018. The Argument Reasoning
Comprehension Task: Identification and Reconstruc-
tion of Implicit Warrants. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume I (Long Pa-
pers), pages 1930-1940, New Orleans, Louisiana.
Association for Computational Linguistics.

Andreas Hanselowski, Christian Stab, Claudia Schulz,
Zile Li, and Iryna Gurevych. 2019. A Richly An-
notated Corpus for Different Tasks in Automated
Fact-Checking. In Proceedings of the 23rd Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 493-503, Hong Kong, China. Asso-
ciation for Computational Linguistics.

Casper Hansen, Christian Hansen, and _ Lucas
Chaves Lima. 2021. Automatic Fake News
Detection: Are Models Learning to Reason? In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), pages
80-86, Online. Association for Computational
Linguistics.

Momchil Hardalov, Arnav Arora, Preslav Nakov, and
Isabelle Augenstein. 2022a. A Survey on Stance De-
tection for Mis- and Disinformation Identification.

In Findings of the Association for Computational
Linguistics: NAACL 2022, pages 1259-1277, Seat-
tle, United States. Association for Computational
Linguistics.

Momchil Hardalov, Anton Chernyavskiy, Ivan Koy-
chev, Dmitry Ilvovsky, and Preslav Nakov. 2022b.
CrowdChecked: Detecting Previously Fact-Checked
Claims in Social Media. In Proceedings of the 2nd
Conference of the Asia-Pacific Chapter of the Associ-
ation for Computational Linguistics and the 12th In-
ternational Joint Conference on Natural Language
Processing, page (to appear), online. Association for
Computational Linguistics.

Tamanna Hossain, Robert L. Logan IV, Arjuna Ugarte,
Yoshitomo Matsubara, Sean Young, and Sameer
Singh. 2020. COVIDLies: Detecting COVID-19
Misinformation on Social Media. In Proceedings
of the Ist Workshop on NLP for COVID-19 (Part 2)
at EMNLP 2020, Online. Association for Computa-
tional Linguistics.

Kung-Hsiang Huang, Kathleen McKeown, Preslav
Nakov, Yejin Choi, and Heng Ji. 2022. Faking Fake
News for Real Fake News Detection: Propaganda-
loaded Training Data Generation. arXiv preprint
arXiv:2203.05386.

Md Rafiqul Islam, Shaowu Liu, Xianzhi Wang, and
Guandong Xu. 2020. Deep learning for misinforma-
tion detection on online social networks: a survey
and new perspectives. Social Network Analysis and
Mining, 10(1):1-20.

Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles
Dognin, Maneesh Singh, and Mohit Bansal. 2020.
HoVer: A Dataset for Many-Hop Fact Extraction
And Claim Verification. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020,
pages 3441-3460, Online. Association for Computa-
tional Linguistics.

Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xi-
aoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya
Sachan, Rada Mihalcea, and Bernhard Schdélkopf.
2022. Logical Fallacy Detection. arXiv preprint
arXiv:2202.13758.

Kashif Khan, Ruizhe Wang, and Pascal Poupart. 2022.
WatClaimCheck: A new dataset for claim entailment
and inference. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1293-1304,
Dublin, Ireland. Association for Computational Lin-
guistics.

Neema Kotonya and Francesca Toni. 2020a. Explain-
able Automated Fact-Checking: A Survey. In
Proceedings of the 28th International Conference
on Computational Linguistics, pages 5430-5443,
Barcelona, Spain (Online). International Committee
on Computational Linguistics.


Neema Kotonya and Francesca Toni. 2020b. Explain-
able Automated Fact-Checking for Public Health
Claims. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7740-7754, Online. Associa-
tion for Computational Linguistics.

Dilek Kiiciik and Fazli Can. 2020. Stance Detection: A
Survey. ACM Computing Surveys (CSUR), 53(1).

Stephan Lewandowsky, John Cook, Ullrich Ecker, Do-
lores Albarracin, Michelle Amazeen, P. Kendou,
D. Lombardi, E. Newman, G. Pennycook, E. Porter,
D. Rand, D. Rapp, J. Reifler, J. Roozenbeek,
P. Schmid, C. Seifert, G. Sinatra, B. Swire-
Thompson, S. van der Linden, E. Vraga, T. Wood,
and M. Zaragoza. 2020. The Debunking Handbook
2020. OpenBU.

Yichuan Li, Bohan Jiang, Kai Shu, and Huan Liu. 2020.
Toward A Multilingual and Multimodal Data Repos-
itory for COVID-19 Disinformation. In 2020 IEEE
International Conference on Big Data (Big Data),
pages 4325-4330.

Alejandro Martin, Javier Huertas-Tato, Alvaro Huertas-
Garcia, Guillermo Villar-Rodriguez, and David Ca-
macho. 2022. FacTeR-Check: Semi-automated
fact-checking through semantic similarity and natu-
ral language inference. Knowledge-Based Systems,
251:109265.

Elena Musi and Andrea Rocci. 2022. Staying Up to
Date with Fact and Reason Checking: An Argumen-
tative Analysis of Outdated News. In Steve Oswald,
Marcin Lewinski, Sara Greco, and Serena Villata,
editors, The Pandemic of Argumentation, pages 311-—
330. Springer International Publishing, Cham.

Preslav Nakov, David Corney, Maram Hasanain,
Firoj Alam, Tamer Elsayed, Alberto Barr6én-
Cedefio, Paolo Papotti, Shaden Shaar, and Giovanni
Da San Martino. 2021. Automated Fact-Checking
for Assisting Human Fact-Checkers. In Proceedings
of the Thirtieth International Joint Conference on
Artificial Intelligence, IJCAI-21, pages 4551-4558.
International Joint Conferences on Artificial Intelli-
gence Organization.

Sakari Nieminen and Valtteri Sankari. 2021. Check-
ing PolitiFact’s Fact-Checks. Journalism Studies,
22(3):358-378.

Ray Oshikawa, Jing Qian, and William Yang Wang.
2020. A Survey on Natural Language Processing
for Fake News Detection. In Proceedings of the
12th Language Resources and Evaluation Confer-
ence, pages 6086-6093, Marseille, France. Euro-
pean Language Resources Association.

Wojciech Ostrowski, Arnav Arora, Pepa Atanasova,
and Isabelle Augenstein. 2021. Multi-Hop Fact
Checking of Political Claims. In Proceedings of the
Thirtieth International Joint Conference on Artifi-
cial Intelligence, IJCAI-21, pages 3892-3898. Inter-
national Joint Conferences on Artificial Intelligence
Organization.

Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2022. FaVIQ:
FAct Verification from Information-seeking Ques-
tions. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume I: Long Papers), pages 5154-5166, Dublin, Ire-
land. Association for Computational Linguistics.

Parth Patwa, Shivam Sharma, Srinivas Pykl, Vineeth
Guptha, Gitanjali Kumari, Md Shad Akhtar, Asif Ek-
bal, Amitava Das, and Tanmoy Chakraborty. 2021.
Fighting an infodemic: Covid-19 fake news dataset.
In International Workshop on Combating Online
Hostile Posts in Regional Languages during Emer-
gency Situation, pages 21-29. Springer.

Dean Pomerleau and Delip Rao. 2017. Fake News
Challenge. http://www. fakenewschallenge.
org/.

Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana
Volkova, and Yejin Choi. 2017. Truth of Varying
Shades: Analyzing Language in Fake News and Po-
litical Fact-Checking. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2931-2937, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda
Muresan. 2021. COVID-Fact: Fact Extraction and
Verification of Real-World Claims on COVID-19
Pandemic. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 2116-2129, Online. Association for Computa-
tional Linguistics.

Mourad Sarrouti, Asma Ben Abacha, Yassine Mra-
bet, and Dina Demner-Fushman. 2021. Evidence-
based Fact-Checking of Health-related Claims. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 3499-3512, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.

Aalok Sathe, Salar Ather, Tuan Manh Le, Nathan Perry,
and Joonsuk Park. 2020. Automated fact-checking
of claims from Wikipedia. In Proceedings of the
12th Language Resources and Evaluation Confer-
ence, pages 6874-6882, Marseille, France. Euro-
pean Language Resources Association.

Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel
Roberto Filizzola Ortiz, Enrico Santus, and Regina
Barzilay. 2019. Towards Debiasing Fact Verification
Models. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3419-3425, Hong Kong, China. Association for
Computational Linguistics.

Shaden Shaar, Nikolay Babulkov, Giovanni
Da San Martino, and Preslav Nakov. 2020. That is


a Known Lie: Detecting Previously Fact-Checked
Claims. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguis-
tics, pages 3607-3618, Online. Association for
Computational Linguistics.

Tommy Shane and Pedro Noel. 2020. Data deficits:
why we need to monitor the demand and supply
of information in real time. Technical report, First
Draft.

Craig Silverman. 2014. Verification handbook: An ul-
timate guideline on digital age sourcing for emer-
gency coverage. European Journalism Centre.

Craig Silverman. 2016. Verification handbook: Addi-
tional Materials. European Journalism Centre.

Felix Simon, Philip N. Howard, and Rasmus Kleis Niel-
son. 2020. Types, sources, and claims of COVID-19
misinformation. Technical report, Reuters Institute
for the Study of Journalism.

James Thorne, Max Glockner, Gisela Vallejo, Andreas
Vlachos, and Iryna Gurevych. 2021. Evidence-
based Verification for Real World Information
Needs. arXiv preprint arXiv:2104.00640.

James Thorne and Andreas Vlachos. 2018. Automated
Fact Checking: Task Formulations, Methods and Fu-
ture Directions. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3346-3359, Santa Fe, New Mexico, USA. As-
sociation for Computational Linguistics.

James Thorne, Andreas _-Viachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a Large-scale Dataset for Fact Extraction
and VERification. In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume I (Long
Papers), pages 809-819, New Orleans, Louisiana.
Association for Computational Linguistics.

Shaydanay Urbani. 2020. Verifying Online Informa-
tion. Technical report, First Draft.

Sander van der Linden. 2022. Misinformation: suscep-
tibility, spread, and interventions to immunize the
public. Nature Medicine, 28(3):460-467.

Otavio Vinhas and Marco Bastos. 2022. Fact-
Checking Misinformation: Eight Notes on Consen-
sus Reality. Journalism Studies, 23(4):448-468.

Andreas Vlachos and Sebastian Riedel. 2014. Fact
Checking: Task definition and dataset construction.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, pages 18-22, Baltimore, MD, USA. Associa-
tion for Computational Linguistics.

Nguyen Vo and Kyumin Lee. 2020. Where Are the
Facts? Searching for Fact-checked Information to

Alleviate the Spread of Fake News. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
7717-7731, Online. Association for Computational
Linguistics.

David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu
Wang, Madeleine van Zuylen, Arman Cohan, and
Hannaneh Hajishirzi. 2020. Fact or Fiction: Verify-
ing Scientific Claims. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 7534-7550, On-
line. Association for Computational Linguistics.

William Yang Wang. 2017. “Liar, Liar Pants on Fire”:
A New Benchmark Dataset for Fake News Detection.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 422-426, Vancouver, Canada.
Association for Computational Linguistics.

Claire Wardle et al. 2017. Fake news. it’s complicated.
Technical report, First Draft.

Maxwell Weinzierl and Sanda Harabagiu. 2022. Vac-
cineLies: A natural language resource for learning
to recognize misinformation about the COVID-19
and HPV vaccines. In Proceedings of the Thir-
teenth Language Resources and Evaluation Confer-
ence, pages 6967-6975, Marseille, France. Euro-
pean Language Resources Association.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-Art Natural Language Process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations, pages 38-45, Online. Asso-
ciation for Computational Linguistics.

John Zarocostas. 2020. How to fight an infodemic. The
Lancet, 395(10225):676.

Yi Zhang, Zachary Ives, and Dan Roth. 2020. “Who
said it, and Why?” Provenance for Natural Lan-
guage Claims. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 4416-4426, Online. Association for
Computational Linguistics.

Yi Zhang, Zachary Ives, and Dan Roth. 2021. What
is Your Article Based On? Inferring Fine-grained
Provenance. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 5894-5903, Online. Association for
Computational Linguistics.


Xinyi Zhou and Reza Zafarani. 2020. A survey of fake
news: Fundamental theories, detection methods, and
opportunities. ACM Computing Surveys (CSUR),
53(5):1-40.

Dimitrina Zlatkova, Preslav Nakov, and Ivan Koychev.
2019. Fact-Checking Meets Fauxtography: Verify-
ing Claims About Images. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 2099-2108, Hong Kong,
China. Association for Computational Linguistics.

Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva,
Maria Liakata, and Rob Procter. 2018. Detection
and Resolution of Rumours in Social Media: A Sur-
vey. ACM Computing Surveys (CSUR), 51(2).

Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob
Procter, and Michal Lukasik. 2016. Stance Classifi-
cation in Rumours as a Sequential Task Exploiting
the Tree Structure of Social Media Conversations.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 2438-2448, Osaka, Japan.
The COLING 2016 Organizing Committee.

A Human Misinformation Verification
Examples

Table 8 shows further examples for the reasoning
over global counter-evidence, or when source guar-
antees are required. Further, we list examples we
considered inapplicable to FCNLP by nature and
ergo excluded. Cases include imposter content (1),
fake web pages (2), and claims that are about multi-
modal content (3) or require reasoning over multi-
modal sources during verification (4). We show
further examples of complex and different reason-
ing via global counter-evidence. This includes:
reasoning over scientific documents (5); aggregat-
ing counter-examples (6); aggregating distinct do-
nations (7); finding and contextualizing multiple
events, including deportation and imprisonment of
a murderer, and aligning these events with the po-
litical leadership in the U.S. (8); Claims (9-11) re-
quire source guarantees to resources like a specific
document (9), event (10), or organization (11).

B_ Leaked Evidence Analysis

B.1 Misinformation Labels

We consider all claims rated with strongly-leaning
false verdicts and other verdicts that fall into the
misinformation category such as “misleading” as
misinformation. Remaining claims are either true

(e.g. “verified”, “mostly true”), mixed (e.g. “half-
true’, “outdated”) or not clearly applicable to mis-
information (e.g. “opinion!”, “scam’’, “full flop’).
We provide all considered labels within MULTIFC
below.

¢ ABC: in-the-red

Africa Check: incorrect, misleading
¢« BOOM Live: rating: false

¢ Check Your Fact: verdict: false

Climate Feedback: incrorrect, misleading

FactScan: factscan score: false, factscan score: mislead-
ing

Factly: false

FactCheckNI: conclusion: false

FactCheck.org: false, distorts the facts, misleading,
spins the facts, not the whole story, cherry picks

Gossip Cop: 0, 1, 2,3

Hoax Slayer: fake news
¢ Huffington Post CA: a lot of baloney

¢ MPR News: false, misleading

Observatory: mostly_false

¢ Pandora: mostly false, false, pants on fire!

PesaCheck: false

PolitiFact: mostly false, false, pants on fire!, fiction

Radio NZ: fiction

Snopes: false, mostly false, miscaptioned, misattributed
¢ The Ferret: mostly false, false
¢ The Journal: we rate this claim false

¢ Truth Or Fiction: fiction!, mostly fiction!, incorrect
attribution!, misleading!, inaccurate attribution!

¢ VERA Files: fake, misleading, false

Voice of San Diego: determination: false, determina-
tion: huckster propaganda, determination: barely true,
determination: misleading

Washington Post: 4 pinnochios, false, not the whole
story, needs context


# Year Misinformation Claim Strategy

(1) 2020 Tennis star Serena Williams posted a message on social media that began, “I’m sick of COVID-19. n/a
I’m sick of black vs. white. I’m sick of Republicans vs. Democrats.”

(2) 2010 You can look up anyone’s driver’s license for free through the ’National Motor Vehicle Licence n/a
Organization’ web site.

(3) 2016 A photograph shows a newly hatched baby dragon. n/a

(4) 2018 Couple Arrested For Selling ‘Golden Tickets To Heaven. n/a

(5) 2021 There is no added safety to the public if you’re vaccinated. GCE

(6) 2011 Limiting labor negotiations to only wages is how it is for the most part in the private sector. GCE

(7) 2018 Kathy Manning gave nearly $1 million to liberals GCE

(8) 2018 Democrats let him (cop killer Luis Bracamontes) into our country, and Democrats let him stay. GCE

(9) 2014 A list reproduces Saul Alinsky’s rules for “How to Create a Social State.". LCE

(10) 2014 Greg Abbott and his surrogates have referred to women who have been the victims of rape or LCE
incest as though somehow what they are confronting is a minor issue.

(11) 2021 During protests over the police in-custody death of George Floyd in the summer of 2020, Kamala LCE

Harris donated money to a Minnesota nonprofit that helped protesters who were arrested get out
of jail and break more laws.

Table 8: Example claims from Snopes and PolitiFact that professional fact-checkers refuted with global counter-
evidence (GCE), local counter-evidence (LCE) via the source guarantee, or marked as not-applicable (n/a) for
FCNLP.

Organization Template

Africa Check africacheck.org/reports

AFP Fact Check factcheck.afp.com

Check Your Fact checkyourfact.com

Climate Feedback climatefeedback.org/claimreview

Fact or Fiction radionz.co.nz/programmes/election17-fact-or-fiction
FactCheck.org factcheck.org

FactCheckNI factcheckni.org

FACTLY factly.in

FactsCan factscan.ca

Full Fact fullfact.org

Gossip Cop gossipcop.com

Health Feedback healthfeedback.org/claimreview

Hoax Slayer hoax-slayer.net

Lead Stories FactChecker | hoax-alert.leadstories.com

PesaCheck pesacheck.org

PolitiFact politifact.com

Snopes snopes.com

Truth or Fiction truthorfiction.com

Washington Post washingtonpost.com/news/fact-checker

Table 9: Used Templates to automatically identify leaked snippets via the URLs.



Regular Expression

’“false:’

*politifact’

* snopes’

’ debunk’

’real story behind’

’\bfake\b’

’\bhoax\b’

*\bfalsely\b’

’\brumors?\b’

*\bmyths?\b’

’\bnot real news\b’

’\bunfounded\b’

*factL -]check’

Example

FALSE: Map Shows Results of the 2012 Presidential Election If Only ... A map doesn’t show the
results of the 2012 election if only people who pay ... FALSE: Map Shows Results of the 2012
Presidential Election If Only Taxpayers Had ... is a map of how the Electoral College vote would
look like if ONLY those who ...

PolitiFact: Testing Kathleen Vinehout claim on Scott Walker, new car ... Dec 20, 2013 ... We
check a claim by state Sen. Kathleen Vinehout that Gov. Scott Walker bought "80 new, brand new
vehicles" that "we probably don’t need.".

Real History Blog: The ACLU has NOT filed suit to have all military ... Feb 10, 2010 ... The
ACLU has never filed such a suit, says the ACLU. Says Snopes, if ... and another suit to end
prayer from the military completely. They’re ...

Debunked: Did ’The Simpsons’ predict President Donald Trump’s ... Feb 9, 2017 .... "The
Simpsons’ has predicted a number of world events and an internet rumor said the show predicted
the death of Donald Trump. Veuer’s Nick ...

The real story behind the statistic Trump just used to attack Obamacare; Jun 13, 2017... ...
tweeted that 2 million people “just dropped out of ObamaCare.” 2 million more people just
dropped out of ObamaCare. It is in a death spiral.

Trump "moron" Harley-Davidson CEO quote: Fake.Jun 27, 2018 ... The CEO of Harley-Davidson
Did Not Call Donald Trump a “Moron” ... Harley Davidson CEO Matthew S Levatich says: "Our
decision to move ...

Eddie Murphy - latest news, breaking stories and comment - The ... All the latest breaking news
on Eddie Murphy. Browse The ... Paul Walker tragedy sparks Eddie Murphy Twitter death hoax -
News - Final film of the Twilight ...

CNN helpfully fact-checks Donald Trump’s tweet about its “way down”; Jun 27, 2017 ... Trump
tweeted that “Fake News CNN” had its “Ratings way down!” which he said was due to the
network being “caught falsely pushing their ...

Did the Obama White House ban Christmas Nativity scenes ... Nov 21, 2018 ... Contrary to "War
on Christmas" rumors, the Obama White House did not ban Nativity scenes from the premises:

Did Coca-Cola Contain Coke? Here’s What History Says; Since I was a little girl, ’'ve heard the
myth that Coca-Cola used to actually contain cocaine. However, how credible is this rumor? I set
out to find if there was any ...

NOT REAL NEWS: A look at what didn’t happen this week; Aug 11, 2017 ... NOT REAL:
John McCain Says He ’ Accidentally’ Voted No On Healthcare Repeal ... last month that sank a
GOP effort to repeal the Affordable Care Act, ... story purportedly showing the fake senator in
handcuffs is actually a ...

No, 15,000 people did not vote for Harambe in 2016 | PunditFact; Nov 22, 2016 ... Harambe
received 15,000 votes in the presidential election. ... Rumors that 15,000 people voted for the
dead gorilla Harambe are unfounded.

Fact-checking an immigration meme that’s been circulating for more ...Jul 5, 2018 ... "More than
66% of ALL births in California are to illegals on Medi-Cal" ... According to Medi-Cal, 50.4
percent of the state’s births that year were ...

Table 10: Used regular expressions to automatically identify leaked evidence snippets.


B.2 Automatic Identification of Leaked
Evidence

Table 9 shows the URLs used to automatically de-
termine leaked evidence. We consider an evidence
snippet as leaked if any URLs of Table 9 is a sub-
string of the snippet’s URL. We exclude URLs if
they may also cover URLs to news articles. Fur-
ther, we consider an evidence snippet as leaked,
if its lowercased title or text matches any of the
regular expressions in Table 10. We identified two
commonly made errors:

¢ Different Claim: The approach considered
evidence as leaked if it is not relevant to the
exact same claim, but connected to the same
incident (“President Obama pushed through
the stimulus based on the promise of keeping
unemployment under § percent.” and “The
president promised that if he spent money on
a stimulus program that unemployment would
go to 5.7 percent or 6 percent. Those were his
words’), or thematically related (“Cadbury
chocolate eggs are infected with HIV-positive
blood” and “HIV & AIDS infected oranges
coming from Libya’).

Discussing Fake News or Fact-Checking:
The approach selects snippets, that discuss
fact-checking or fake news from a different
perspective, not as a result of verification.
This includes opinions or reports complain-
ing about “fake news” being spread, or about
the fact-checking process.

B.3. Manual Guidelines

To determine the stance of evidence snippets to a
claim, or whether it is leaked or not, we proceed in
the following order: We first read the original fact-
checking article, to fully comprehend the claim
and how fact-checkers refuted it. If the title or
text of the evidence snippets provides sufficient
information, we decide based on the snippet alone.
If we cannot make a clear decision based on the
snippet, we consider the original web page. This
may be required, as evidence snippets often contain
incomplete sentences.

B.3.1_ Leaked Evidence Snippets

We consider evidence snippets as leaked if (a) they
constitute information that relies on the verification
of the same claim, or (b) provides originally un-
known information from the claim’s future. When

relying on content from the claim’s verification,
we do not require the information to contradict the
claim from a human perspective. This often oc-
curs when different pages (such as overview pages)
reference the fact-checking article. Such a page
may be a clear indication of the verdict in some
cases (e.g. if titled “All False Claims by Person A’).
In other cases, different interpretations are valid:
The statement “We previously fact-checked simi-
lar claims that ..”’ may be seen as neutral or as a
give-away that similar claims were refuted. Further,
humans cannot judge whether models may rely on
latent patterns. An overview page titled “All claims
from Person A’ may be sufficient for the model if it
learned that most claims by Person A are false. To
remove this ambiguity, we consider any mention /
or information taken form the claim’s verification
as leaked. We do not strictly consider all evidence
that appeared after the verification as leaked. Not
all evidence published after the claim’s verification,
is based not based on the verification. If not, we
verify whether it relies on new information that pre-
viously did not exist or whether the truth changed.
Consider the claim “Khloe Kardashian did give
birth over easter.” refuted on April 5, 2018. Ev-
idence about her actual birth on April 12, 2018,
does not rely on a previous verification but is still
considered leaked (new information available). In
other cases (“Coca-Cola’s "Share A Coke" cam-
paign includes a bottle for the KKK.”, March 2,
2016) we consider evidence from March 30, 2016,
as unleaked: It correctly reports about the same
incident the claim refers to without any mention of
the false claim, or its verification.

B.3.2. Stance of Evidence Snippets

For most claims, it is unrealistic to assume a sin-
gle evidence snippet can refute them entirely. We
follow Sarrouti et al. (2021) to allow evidence to
support or refute parts of the claim only. We sepa-
rately mark supporting evidence from the claim’s
future, as the claim’s veracity may have changed.
We consider correctly identified counter-evidence
as refuting the claim, even when it requires the
source guarantee.

C_ Experiments on MULTIFC

C.1_ Training details

For our experiments we use bert-base-uncased
as provided by Wolf et al. (2020). We represent
each evidence snippet e as the title, the text body,


or the concatenation of both (depending on the ex-
periment). We concatenate all evidence snippets
e;, separated by a semicolon (e); €93 ...3 En). We
input the concatenation of the claim c and the con-
catenated evidence, separated by [SEP] token, and
truncated after 512 tokens: [CLS] c [SEP] e1;
€23 ...5 €n LSEP]. We predict the label via a linear
layer on the [CLS] token. We train all models for
5 epochs with a learning rate of 2e-5 and a batch
size of 16. We select the model with the highest
F1 score on the development dataset, evaluated af-
ter each epoch. We keep the default parameters
for all other values. We always train and evaluate
three models using the seeds (1, 2,3). We did not
fine-tune any hyperparameter. We provide code for
reproduction. We run our experiments on a DGX
A100.

C.2 Performance per Label

We show the F1 score for each label and both
datasets in Table 11 (Snopes) and Table 12 (Politi-
Fact). The dataset of Snopes is highly imbalanced
towards the majority class “false”. The class im-
balance is amplified within the leaked subset. The
evidence-only model benefits from leaked evidence
for (leaning) false claims. The performance drops
on samples with evidence exhibiting leaked char-
acteristics when the gold veracity tends towards
true. The performance on the unleaked subset is
slightly more balanced when comparing different
labels. The majority label “false” is still dominat-
ing. Yet, the more balanced predictions across all
labels yield a higher Fl-macro score. On PolitiFact,
a majority of claims across all labels contain leaked
evidence. Models learn to exploit it for all labels.
The best performance gain can be seen over claims
misinformation claims. Yet, the more balanced pre-
dictions across all labels yield a higher Fl-macro
score.

C.3. Evaluation on Identical Claims with
Different Evidence

To avoid the label distribution to distort the im-
pact of leaked evidence, we evaluate the trained
model only on claims that contain leaked and un-
leaked evidence snippets. We separately measure
the performance of the evidence-only models on
this subset, when only concatenating leaked evi-
dence to the claim, and when only concatenating
unleaked evidence to the same claim. The overall
metrics on Snopes (Table 13) do not change much.
The performance on the individual labels differs

(even not always for the better) when comparing
the prediction with leaked or unleaked evidence.
We find that leaked evidence snippets are strong
indicators for the model to predict the label “false”.
This comes at the cost of a lower recall across all
other labels. On PolitiFact evidence snippets show
great improvements and double almost every met-
ric (Table 14).

C.4 Comparison with a Claim-Only Baseline

We show the detailed results of the claim-only base-
line for all claims that contain leaked and unleaked
evidence snippets in Table 15. All these samples
are considered leaked, as they contain (amongst
others) leaked evidence. The results align with
our previous observations: Models trained on the
Snopes subset rely on the majority class “false”,
yielding an overall high performance. We com-
pare the results of the claim-only model (which
by default cannot benefit from leaked evidence)
with the results of the evidence-only model (com-
pare Table 13) on the same subset. The claim-
only model outperforms the evidence-only model
on unleaked evidence. When the evidence-only
model sees leaked evidence, it either performs on
par (Fl-micro, accuracy) or worse (Fl-macro) than
the claim-only model. Here, the claim-only model
is slightly less biased towards predicting “false”,
which improves the performance on the remain-
ing labels. Both results indicate the reliance of
the model on leaked evidence (even if not for the
better). On the PolitiFact subset, the claim-only
baseline performs well behind the evidence-only
baseline (compare Table 14).


Leaked Unleaked Difference
Gold Label | Ev.-Only Full # Pairs | Ev.-Only Full # Pairs | AEv.-Only A Full
true 33.3412 33.6 +39 39 45.0 +08 44.9 +17 93 -11.6 +20 -11.3 +25
mostly true 0.0 +0.0 0.0 +0.0 10 0.0 +0.0 0.0 +0.0 19 0.0 +0.0 0.0 +0.0
mixture 18.0438 22.4 +30 44 28.6 423 32.8 +10 81 -10.6 +5.9 -10.4 +25
mostly false 6.2 +5.5 10.8 +4.0 36 4.4 +39 9.4 47.2 40 +1.9 +83 41.5 +41
false 85.8 +04 86.2 +03 353 73.2 +409 77.7 +20 299 $12.5 411 $8.5 +17

Table 11: Fl-score on Snopes (via MULITFC) using the evidence-only model using solely evidence (title & text)
snippets, and the full model. We report the Fl-score for each label on both splits (leaked and unleaked evidence),
and their difference.

Leaked Unleaked Difference

Gold Label Ey.-Only Full # Pairs | Ev.-Only Full # Pairs | A Ev.-Only A Full

True 57.9412 58.3 410 288 29.0414 27.3 437 113 +29.0 +1.0 +31.0 +37
Mostly True 59.0 +07 58.1 +02 387 20.5419 25.8 +24 123 $38.5 42.6 +32.3 42.5
Half-True 58.3413 58.4 +03 405 28.6424 30.9 +26 132 +29.8 43.6 +27.5 +23
Mostly False 56.2 +12 56.8 +04 364 18.2462 17.1 +07 97 +38.0 47.1 +39.7 +03
False 58.4409 58.8411 419 19.0430 26.4 +09 102 +39.5 +28 +32.4 +09
Pants on Fire! | 68.1 +06 68.0 +04 248 17.6437 25.9 +22 39 +50.5 43.1 +42.2 +19

Table 12: Fl-score on PolitiFact (via MULITFC) using the evidence-only model using solely evidence (title & text)
snippets, and the full model. We report the Fl-score for each label on both splits (leaked and unleaked evidence)

and their difference.

Leaked Unleaked
Gold Label | Precision Recall 1 | Precision Recall F1 | Support
true 31.5 14.0 19.0 17.0 33.3, 22.5 38
mostly true 0.0 0.0 0.0 0.0 0.0 0.0 10
mixture 28.1 23.5 25.4 19.8 37.1 25.8 4d
mostly false 0.0 0.0 0.0 5.9 2.8 3.8 36
false 773 93.55 84.6 82.4 73.9 77.9 353
Accuracy 71.9 60.5 481
F1-micro 66.0 61.6 481
F1-macro 25.8 26.0 481

Table 13: Precision recall and F1 of the evidence-only BERT model based on all claims of the Snopes dataset that

contain leaked and unleaked evidence snippets.

Leaked Unleaked
Gold Label Precision Recall F1 | Precision Recall F1 | Support
true 58.3 53.9 56.0 21.1 23.8 22.3 288
mostly true 61.4 54.5 57.0 26.7 26.6 26.5 385
half-true 46.7 61.6 52.8 24.2 32.3 27.6 404
mostly false 59.1 55.4 57.1 21.8 18.4 19.7 360
false 60.9 58.1 59.4 26.4 28.5 27.4 419
pants on fire! 75.0 63.0 68.5 57.4 22.3 32.1 247
Accuracy 57.6 25.8 2103
F1-micro 57.9 25.8 2103
F1-macro 58.5 25.9 2103

Table 14: Precision recall and F1 of the evidence-only BERT model based on all claims of the PolitiFact dataset
that contain leaked and unleaked evidence snippets.



Snopes PolitiFact

Gold Label Precision Recall Fl Support | Precision Recall F1 Support
true 41.8 33.3 36.9 38 25.6 25.6 25.5 288
mostly true 0.0 0.0 0.0 10 29.8 34.1 31.7 385
half-true / mixture 22.6 28.0 24.9 44 26.7 35.1 30.3 404
mostly false 73 1.9 2.9 36 23.0 21.5 22.2 360
false 80.7 88.7 84.5 353 28.2 23.9 25.9 419
pants on fire! - - - - 54.7 33.3 41.3 247
Accuracy 70.4 481 28.8 2103
F1-micro 67.4 481 28.9 2103
F1-macro 29.8 481 29.5 2103

Table 15: Precision recall and F1 of the claim-only BERT model based on all claims containing leaked and unleaked
evidence.
