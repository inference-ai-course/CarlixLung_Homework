2510.10453v1 [cs.CL] 12 Oct 2025

arXiv

JOURNAL OF I4Tgx CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

End-to-end Speech Recognition with similar length
speech and text

Peng Fan, Wenping Wang, Fei Deng

Abstract—The mismatch of speech length and text length poses
a challenge in automatic speech recognition (ASR). In previous
research, various approaches have been employed to align text
with speech, including the utilization of Connectionist Temporal
Classification (CTC). In earlier work, a key frame mechanism
(KFDS) was introduced, utilizing intermediate CTC outputs to
guide downsampling and preserve keyframes, but traditional
methods (CTC) failed to align speech and text appropriately
when downsampling speech to a text-similar length. In this paper,
we focus on speech recognition in those cases where the length
of speech aligns closely with that of the corresponding text. To
address this issue, we introduce two methods for alignment: a)
Time Independence Loss (TIL) and b) Aligned Cross Entropy
(AXE) Loss, which is based on edit distance. To enhance the
information on keyframes, we incorporate frame fusion by
applying weights and summing the keyframe with its context
2 frames. Experimental results on AISHELL-1 and AISHELL-2
dataset subsets show that the proposed methods outperform the
previous work and achieve a reduction of at least 86% in the
number of frames.

Index Terms—speech recognition, end-to-end, self-attention,
frame reduction

I. INTRODUCTION

Recently, the Conformer-based end-to-end automatic speech
recognition (ASR) model has been the most popular way in
the speech recognition research community. According to the
decoder’s type, end-to-end ASR model can be divided into
Connectionist Temporal Classification (CTC) based model,
attention-based Encoder-Decoder (AED) model, and RNN-
Transducer (RNN-T) based model [1]-[6].

In the field of speech processing, the challenge of speech
length exceeding text length is a noteworthy concern, par-
ticularly in tasks related to speech translation and speech
recognition. To address this issue, several methods have been
developed. For example, text length can be extended to match
the length of the speech, as demonstrated in GMM-HMM.
Also, alignment of speech and text can be achieved by
introducing blank labels through CTC and RNN-T. Another
approach is to integrate the speech features of the encoder
into the encoder’s autoregressive decoding process via AED,
without the need for explicit alignment [1], [3], [4], [7].
These solutions partially solve the problem of speech text
mismatch, but the length of speech remains considerable, as

Corresponding author: Fei Deng.

Peng Fan and Fei Deng are with the College of Computer Science
and Cyber Security, Chengdu University of Technology, Chengdu 610059,
China.(e-mail: fanpeng2023 @hotmail.com; dengfei@cdut.edu.cn).

Wenping Wang is with National Key Laboratory of Fundamental Sci-
ence on Synthetic Vision, Sichuan University, Chengdu 610065, China.
(email:sonography @aliyun.com)

does the computational workload. During speech recognition
process, speech is usually downsampled by 3-4 times the
length, but the speech length is still much longer than the text
length. In addition, previous work used CTC outputs to guide
downsampling and skip the blank frame for decoder [8]-[10].

Recently, some researchers employs an intermediate CTC
[11] to downsampling the speech feature in encoder, key
frame-based downsampling (KFDS) [12] and Skipformer [13],
akin to the approach introduced by Wang et al. [8] for
accelerating the process, incorporating a CTC output guide
for the downsampling task. Meng’s work uses intermediate
CTC and CTC Spike Reduction methods to guide attention
masks and reduce redundant peaks, thereby increasing model
efficiency [14].

In this study, our work is based on KFDS. We only retain
key frames and downsample them to a length similar to the
text. However, a new problem arises, that is, CTC loss cannot
be applied if the speech length is similar to the text length.
Therefore, we introduce the Length Similarity Loss (LSL)
to address this issue. LSL comprises two implementation
methods: one is the Time Independence Loss (TIL), which
removes temporal information from both the input and output.
The other is the Aligned Cross-Entropy (AXE) loss, which
relies on edit distance alignment to synchronize the input and
output before computing the cross-entropy loss. Our work is
akin to CIF [15] in terms of achieving alignment between input
and output. However, a notable difference is that CIF does not
incorporate speech downsampling.

In this work, we focus on ASR with the aim of downsam-
pling speech to a length similar to the corresponding text. Ex-
perimental results on the AISHELL-1 dataset and AISHELL-2
dataset subsets demonstrate the effectiveness of our proposed
methods, achieving a Character Error Rate (CER) comparable
to the baseline. Furthermore, our approach performs compa-
rably to previous methods that utilized CTC loss. Meantime,
substantial reduction in computational complexity is achieved,
while reducing the number of frames by at least 86%.

II. METHOD

In this paper, our end-to-end ASR model comprises
Conformer-based encoder layers and Transformer-based De-
coder layers. Specifically, the vanilla AED joint CTC loss
model, shown in Fig. 1(a), incorporates the CTC loss function
during training to acquire speech-to-text alignment informa-
tion, thereby enhancing the performance of the AED model.
The final objective loss function is defined as follows:

L= 0 * Lote + 1 * Lee. (1)


JOURNAL OF I4Tgx CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

n t
Ly CE Loss
CE Loss
CE Loss
A Transformer Length Similar
Ly Decoder Loss
Transformer Transformer ' '
Decoder Decoder ' ' Conformer
i ' Encoder 2 — Se ea
Conformer Encoder CTC ' ‘ 4 x i
Encoder 2 Loss ) 3 Bs a ~~ | Remove blank frames [ | 1 | a
x : t . & duplicate frames A 4
| Conformer | Encoder CTC. ALAA AMY | '
Encoder Loss Conformer ‘ iq
A Conformer intermediate CTC Eoeauen | Key frame
Encoder 1 ' select
Subsampling Subsampling Subsampling Intermediate CTC : :
Fbank feature DOU00000000 4
Fbank feature [a senes eine a Vo eererrefrrireccreccicsecc
\ i 5 - ] Key frame [ Blank frame
bs Input Speec'!
Input Speech Input Speech Bt

(a) The vanila model

(b) The model with intermediate CTC

(c) The model with key frame based downsampling

Fig. 1. The overall architecture of the vanilla Conformer-based AED model (a), the AED model with intermediate CTC (b), and the proposed speech length

is similar to the text model(KFDS-based mechanism downsampling) (c).

Fig. 1(b) illustrates the AED model with intermediate CTC,
which enhances model performance. The final objective loss
for this model is defined as follows:

(2)

As shown in Fig. 1, different from the vanilla conformer-
based AED model, our proposed model incorporates a key
frame-based downsampling module (the rose red block in
Fig. 1(c)). Furthermore, the proposed LSL replaces CTC loss
for computing the encoder’s final output and target text. The
proposed model’s final objective loss is defined as:

L= Qo * Comer ete + ay * Lorca + a2 * Le.

(3)

Here, we set the parameters as follows: a9,Q1,Qa@2 =
0.2,0.1,0.7, employing the intermediate CTC loss to predict
key frames and provide time information after encoder 1,
LSL is uesd for handling the output after downsampling from
encoder 2, and CE loss is utilized for the attention-based
decoder output.

L= Qo * Linter—cte + ay * Lis + a2 * Lee.

A. Key frame downsampling

We employ KFDS to downsample the length of speech
to be similar to the text and subsequently explore speech
recognition in this context. We will briefly introduce the key
frame-based self-attention (KFSA) and the down-sampling
process based on the key frame mechanism (KFDS). As shown
in the dashed box at the bottom right of Fig. 1, the key
frame mechanism selects key frame using the non-blank frame
sequence generated by the intermediate CTC loss to remove
duplicates and blank frames [8], [11].

The KFSA mechanism utilizes previously generated
keyframes to reduce the self-attention mechanism module.
The KFDS process, as illustrated in Fig. 1, involves down-
sampling frames guided by key frames and preserving the
frames corresponding to these key frames.

B. Length Similar Loss (LSL)

However, in prior research, the final output of the encoder
was determined using the CTC loss. When we use the KFDS
mechanism to downsample speech to a length similar to the
text, the speech and text are mapped to the same feature space,
and the previous method (CTC) is not suitable for calculating
losses.

This subsection, we introduce the LSL to deal with it. We
introduce two LSL functions to address this issue from two
distinct angles. Firstly, we employ a time information removal
method (TIL) to compute the loss between the model’s pre-
dictions and the ground truth values. Secondly, we align these
elements by adjusting the distance before calculating the CE
loss (AXE loss).

1) Time independence loss (TIL): In this work, after frame-
downsampling by the KFDS mechanism, the length of the out-
put of the encoder is similar to the target text. However, their
corresponding relationship cannot be established directly. The
calculation of loss is prevented due to the absence of chrono-
logical order and one-to-one correspondence. Consequently,
the temporal information was removed, and all information
sets were consolidated into a single output denoted as Y’.

Note that, despite the removal of timing information at the
end, it has already been acquired by the inter-CTC model in
the sixth layer, ensuring that the final output will not be out
of order.

Let Y be a target sequence of L tokens {y1, yo,..., yz}, and
P be the model predictions, a sequence of T' token probability
distributions {pi,p2,...,pr}, v is the vocabulary size. We
ignored the time information of the target Y and predicted
P. VY onehot — {yonehot ygrenet _.. ,ygrehoty correspond Y
one-hot vector of v dimension. The time independence loss is
defined by Eq.6.

L

y= S- yorchot

i=l

(4)


JOURNAL OF I4Tgx CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

T
=> op (5)
7.
y’
TIL=S_-Y'log a (6)

2) Aligned cross-entropy loss (AXE): The AXE loss was
initially proposed to solve the problem of excessive penalty
for the change of output word order when using CE loss in
machine translation [16]. The final prediction sequence of our
encoder is very close to the target sequence in length, but there
may be insertion and deletion errors, which is unsuitable for
directly using CE loss, so the AXE loss function is introduced
to calculate the loss. Our goal is to find a monotonic alignment
between Y and P that will minimize the CE loss, and thus
focusing the penalty on lexical errors (predicting the wrong
token) rather than positional errors (predicting the right token
in the wrong place). We define an alignment a to be a
function that maps target positions to prediction positions, i.e.
{1,...,L} > {1,..., 7}. The AXE loss can be depicted with
the following Eq.7 [16].

AXE(Yi,...,¥z,Pi,...,Pr) =

min AXE(Y,...,Yz,P1,...,Pr]a)=

min log Pay
tiny, (-2 py &

— > log Px (é mn)
ke{1...T}\{a(1),...a(L)}

s.t. 1 < a(1) < a(2) <

(7)

| A
4

C. Frame fusion

In this paper, we use the keyframe mechanism for down-
sampling. Benefiting from the two proposed loss calculation
methods, we can achieve extreme downsampling of speech
similar in length to the text. If only keyframes are used, a
lot of useful information will be discarded. Although CTC
has peaks, its adjacent frames also contain a large amount of
information. Therefore, we used frame fusion to preserve more
information and employed two methods for frame fusion.

Multiply8Sum
Softmax | : |

Fig. 2. Attention-based frame fusion.

Firstly, frame fusion was achieved through attention-based
frame fusion without adding new parameters. As shown in
Fig.2, the rose red bar represents the key frame, the orange
bar represents the left frame of the keyframe, and the light

green bar represents the right frame of the key frame. This
process is shown in the following Eq. 8 and Eq. 9.

S = Softmax(hy_i:t+;) (8)
tj
O= S © Sx: (he) (9)
k=t—i

Here, ¢ is key frame, S represents the weight of frames from
t—itot+ J, with 7 equal to 1 and 7 equal to 1, while O
denotes the fusioned 3-frame.

As shown in Fig.3, we also used concatenate-based frame
fusion, concatenated the keyframe sequence with the keyframe
context 2 frames on the channel dimension, and then reduced
it to the original dimension by using a linear layer.

A

al Concatenate
& Fully connected layer

1 I |

Fig. 3. Concatenate-based frame fusion.

D. Three-step trainning

Conformer with - Conformer with . Conformer with
intermedia CTC ; KFDS([-1, +1] + k)/“—+ KFDS(k)

Fig. 4. Three-step training.

To facilitate model training convergence, a three-stage
training process is proposed. The training process of our
proposed model is depicted in Fig.4. In the first stage, a base
Conformer model is trained to utilize intermediate CTC loss
to obtain peak information. Subsequently, in the second stage,
the KFDS method with key frame and context 2 frames ([-
1,+1]+K), initializes the model using the Stage 1 model init
for downsampling speech length. In the third stage, to achieve
further downsampling, LS loss is adopted, and the model is
initialized using the Stage 2 model.

II. EXPERIMENTS
A. Corpus and Experimental Configurations

In this subsection, we verify our proposed KFDS-based
Conformer encoder-decoder network on the open-source
datasets: AISHELL-1 [17] and AISHELL-2 [18] subsets.
In all our experiments, 80-dimensional log Mel-filter bank
(Fbank) features are extracted from a 25ms window with a
10ms frame shift. SpecAugment is used as acoustic feature
augmentation [19]. To conduct modeling on the AiSHELL-1,
a vocabulary consisting of 5234 labels that incorporate Chinese
characters and other special characters is employed.


JOURNAL OF I4Tgx CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

We adopt hybrid CTC and attention-based auto encoder-
decoder (AED) architecture following Wenet recipe [20]. For
the encoder, there are 12 Conformer blocks. For each block,
the convolutional kernel size is 31, the number of attention
heads is 4, and the hidden dimensions of the attention and FFN
layer are 256 and 2048 respectively. The decoder consists of 6
Transformer blocks, where each block has 4 attention heads.

As for training details, we follow the training recipes
provided by Wenet. ! It will be easy for others to reproduce our
experiments. Note that, in order to obtain a better initial inter-
mediate CTC guidance, KFSA and KFDS are introduced after
the first NV normal training epochs. N is 40 for AISHELL-1
experiments.

B. Overall Results

TABLE I
THE OVERALL RESULT ON AISHELL-1 WITH CER.

Models Condition Test Drop ratio
Conformer [20] All 4.75 /

BO (baseline) All 4.58 /
Efficient Conformer [21] All 464 /

El (KFSA) K 4.58 /

E2 (KFDS) [-l, +1] +K 452 65%
E3 (KFDS with TIL) K 4.65 87%
E4 (KFDS with AXE) K 449 87%
E5 (KFDS without Loss) K 89.83 87%

1) Results On Aishell-1: TABLE I shows the overall char-
acter error rate (CER) on the AISHELL-1 test set. During
decoding, CTC prefix beam search is used to generate N-best
candidates first and then rescored using a Transformer decoder.
We only report the final results here after the Transformer
decoder rescore. In Table I, the result of the vanilla Conformer
model was from [20]. However, there is no intermediate CTC
during model training in [20]. For a fair comparison, we add
intermediate CTC loss during training of BO model, which is
our baseline model with 4.58% CER. We also compare our
methods with Efficient Conformer [21], which downsampled
feature sequences uniformly.

Model E1 is trained using the previous KFSA mechanism
with “+ K’”, which means all other key frames are used during
attention calculation. El is trained using key frames only
and obtains 4.58% 9.CER, which also proves that key frames
contain more helpful information and are crucial for attention
mechanisms. E2 is trained using the KFDS mechanism with
local temporal context widths | ({-1, +1] + K). Compared with
the KFSA-based models El, the KFDS-based models obtain
lower, 4.52% CER and less computational complexity. El and
E2 are both the previous key frame mechanism models.

Models E3 and E4 are trained using the specified KFDS
mechanism, which uses only keyframes to guide the down-
sampling of speech frames. Both E3 and E4 adopt extreme
downsampling, which reduces the speech frame to the length
of the deduplicated non-blank label sequence predicted by
the intermediate CTC loss. The E3 and E4 obtain 4.65%
and 4.49% CER respectively. E3’s performance surpasses the

'The recipe on AISHELL-1 is publicly available in https://github.com/
wenet-e2e/wenet/tree/main/examples/aishell/s0/conf;

vanilla Conformer and is comparable with BO and Efficient
Conformer. However, it performed wose compared to El using
the KFSA mechanism and E2 using 3 frames of KFDS. This
is likely because this method ignores the order information of
the text, which is important in speech recognition. However,
this method is also a tradeoff one for calculating the loss when
the length of a speech frame sequence is close to that of a text
sequence.

Especially, E4 obtained 4.49% CER, which surpasses the
previous best results with 0.03% absolute CER. This is a very
interesting result. When using the KFDS mechanism, E4 with
less information actually performs better than E2, indicating
that the key frame mechanism has indeed learned enough
key information. Meanwhile, compared to E3, the absolute
CER decreased by 0.16%, indicating that the time information
in the speech frame is very important for predicting the
final text sequence. ES employed an extreme downsampling
strategy without computing the loss for the output of the
second encoder. Consequently, it achieved a CER of 89.83%,
leading to performance degradation. This underscores the
crucial necessity of calculating the loss for the encoder 2
output. Additionally, our approach achieves an 87% reduction
in frames on the AISHELL-1 dataset, surpassing the previous
reduction of 65%.

It is worth mentioning that considering that the model is not
easy to converge after extreme downsampling, we use E2’s
final average of 30 epochs model as model init to facilitate
better training of our model.

TABLE II
THE OVERALL RESULT ON AISHELL-2 SUBSETS WITH CER.

Models Condition Test Drop ratio
Conformer [20] All 8.40 /

BO (baseline) All 8.30 /

Efficient Conformer [21] All 8.42 /

EI (KFSA) K 8.21 = /

E2 (KFDS) [-l, +1] +K 818 58%

E3 (KFDS with TIL) K 8.76 86%

E4 (KFDS with AXE) K 8.43 86%

E5 (KFDS without Loss) K 87.52 86%

2) Results On Aishell-2 subsets: To verify the effectiveness
of our proposed method, we conducted experiments on subsets
of the AISHELL-2 dataset. TABLE II presents the overall
CER on the AISHELL-2 test set. The baseline BO model with
intermediate CTC loss achieves a CER of 8.30%, while the
vanilla Conformer has a CER of 8.40%, demonstrating that
the intermediate CTC loss method enhances model perfor-
mance. In addition, the Efficiency Conformer model achieves
a CER of 8.42%, performing slightly worse than the first two
baseline models but with reduced computational complexity.
The KFSA model reaches a CER of 8.21%, and the KFDS
model achieves 8.18%, demonstrating the effectiveness of the
keyframe mechanism. For the proposed method in this study,
TIL yields a CER of 8.76%, the lowest performance among all
models, attributed to information loss; temporal information is
removed after feature downsampling. The AXE loss model,
with a CER of 8.43%, performs worse than both the BO
baseline model and the Efficiency Conformer but achieves
substantial downsampling by discarding 86% of the frames.


JOURNAL OF I4Tgx CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Lastly, when the loss is not computed after the second en-
coder that performs downsampling, the model’s CER increases
significantly to 87.52%, indicating a substantial degradation
in performance. This aligns with the results observed on the
AISHELL-1 dataset, underscoring the importance of calculat-
ing the loss at this stage.

3) Frame fusion: We first explored whether frame fusion
is necessary for our experiment and which fusion method
is optimal. Note that all models in this experiment were
trained through pseudo-one-hot with KL loss. As shown in
TABLE II, E6 is a KFDS that only uses keyframes without
any frame fusion, but it achieved the worst result 4.81%, which
is worse than the E2 of the original KFDS mechanism and
even worse than the performance of the three baseline models.
By fusing 3 frames into 1 frame, E7 achieved a poor CER
effect of 4.71% using an additional network for frame fusion,
compared to 4.65% using weighted summation for E8. Finally,
we attempted to fuse 5 frames into 1 frame and obtained a
CER of 4.74% using a weighted sum method, indicating that
the effect of fusing more frames would also degrade, possibly
due to the inclusion of information unrelated to the current
keyframe.

TABLE III
INVESTIGATE THE DIFFERENT FRAME FUSION METHODS
Models Fusion method Test
E6 No fusion 4.81
E7 Concatenate-based 471

E8 Attention-based(3 frames) 4.65
E9 Attention-based(5 frames) 4.74

IV. CONCLUSION

Matching the length of a speech with the length of its target
text presents a significant challenge in the field of ASR. In this
paper, we investigate ASR within the context of downsampling
speech to match the length of the text using KFDS. In response
to this challenge, we introduce two novel alignment tech-
niques: a)TIL and b) AXE Loss. To enhance the content of key
frames, we implement a frame fusion approach, incorporating
weights and summing the key frame with its contextual two
frames. Our proposed methodology has exhibited results that
superior to the baseline in extensive experiments conducted on
the AISHELL-1 dataset. Furthermore, our approach achieves a
substantial reduction of at least 87% in the number of frames.
Further experiments on a subset of AISHELL-2 reinforce the
effectiveness and robustness of our proposed method.

REFERENCES

[1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A
neural network for large vocabulary conversational speech recognition,”
in 2016 IEEE international conference on acoustics, speech and signal
processing (ICASSP). TEEE, 2016, pp. 4960-4964.

[2] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence

sequence-to-sequence model for speech recognition,’ in 20/8 IEEE

international conference on acoustics, speech and signal processing

(ICASSP). TJEEE, 2018, pp. 5884-5888.

[3] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, “Connection-

ist temporal classification: labelling unsegmented sequence data with

recurrent neural networks,” in Proceedings of the 23rd international

conference on Machine learning, 2006, pp. 369-376.

A. Graves, “Sequence transduction with recurrent neural networks,”

arXiv preprint arXiv:1211.3711, 2012.

Y. Zhang, S. Sun, and L. Ma, “Tiny transducer: A highly-efficient

speech recognition model on edge devices,” in ICASSP 2021-2021 IEEE

International Conference on Acoustics, Speech and Signal Processing

(ICASSP). JEEE, 2021, pp. 6024-6028.

A. Graves and N. Jaitly, “Towards end-to-end speech recognition with

recurrent neural networks,’ in International conference on machine

learning. PMLR, 2014, pp. 1764-1772.

M. Aymen, A. Abdelaziz, S. Halim, and H. Maaref, “Hidden markov

models for automatic speech recognition,” in 20/1 International confer-

ence on communications, computing and control applications (CCCA).

IEEE, 2011, pp. 1-6.

[8] Y. Wang, Z. Chen, C. Zheng, Y. Zhang, W. Han, and P. Haghani, “Ac-

celerating rnn-t training and inference using ctc guidance,” in ICASSP

2023-2023 IEEE International Conference on Acoustics, Speech and

Signal Processing (ICASSP). TEEE, 2023, pp. 1-5.

Y. Yang, X. Yang, L. Guo, Z. Yao, W. Kang, F. Kuang, L. Lin, X. Chen,

and D. Povey, “Blank-regularized ctc for frame skipping in neural

transducer,” arXiv preprint arXiv:2305.11558, 2023.

10] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, “Fsr: Accelerating

the inference process of transducer-based models by applying fast-skip

regularization,” arXiv preprint arXiv:2104.02882, 2021.

11] J. Lee and S. Watanabe, “Intermediate loss regularization for ctc-

based speech recognition,” in ICASSP 2021] - 2021 IEEE International

Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021,

pp. 6224-6228.

12] P. Fan, C. Shan, S. Sun, Q. Yang, and J. Zhang, “Key frame mechanism

for efficient conformer based end-to-end speech recognition,’ [EEE

Signal Processing Letters, vol. 30, pp. 1612-1616, 2023.

13] W. Zhu, S. Sun, C. Shan, P. Fan, and Q. Yang, “Skipformer: A skip-

and-recover strategy for efficient speech recognition,” in 2024 IEEE

International Conference on Multimedia and Expo (ICME), 2024, pp.

1-6.

14] Q. Meng, M. Liu, K. Huang, K. Wei, L. Xie, Z. Quan, W. Deng, Q. Lu,

N. Jiang, and G. Zhao, “Seq-former: A context-enhanced and efficient

automatic speech recognition framework,” in Proc. Interspeech 2024,

2024, pp. 212-216.

15] L. Dong and B. Xu, “Cif: Continuous integrate-and-fire for end-to-

end speech recognition,’ in ICASSP 2020-2020 IEEE International

Conference on Acoustics, Speech and Signal Processing (ICASSP).

IEEE, 2020, pp. 6079-6083.

16] M. Ghazvininejad, V. Karpukhin, L. Zettlemoyer, and O. Levy, “Aligned

cross entropy for non-autoregressive machine translation,” in Jnterna-

tional Conference on Machine Learning. PMLR, 2020, pp. 3515-3523.

17] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-source
mandarin speech corpus and a speech recognition baseline,’ in 20/7
20th conference of the oriental chapter of the international coordinating
committee on speech databases and speech I/O systems and assessment
(O-COCOSDA). TYEEE, 2017, pp. 1-5.

18] J. Du, X. Na, X. Liu, and H. Bu, “Aishell-2: Transforming mandarin asr

research into industrial scale,’ arXiv preprint arXiv: 1808.10583, 2018.

19] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,

and Q. V. Le, “SpecAugment: A Simple Data Augmentation Method

for Automatic Speech Recognition,” in Proc. Interspeech 2019, 2019,

pp. 2613-2617.

20] B. Zhang, D. Wu, Z. Peng, X. Song, Z. Yao, H. Lv, L. Xie, C. Yang,

F. Pan, and J. Niu, “Wenet 2.0: More productive end-to-end speech

recognition toolkit,” arXiv preprint arXiv:2203.15455, 2022.

21] M. Burchi and V. Vielzeuf, “Efficient conformer: Progressive down-
sampling and grouped attention for automatic speech recognition,” in
2021 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU). YEEE, 2021, pp. 8-15.

