arXiv:2011.04372v1 [cs.CL] 9 Nov 2020

UiO $ University of Oslo

Farhad Nooralahzadeh

Low-Resource Adaptation of
Neural NLP Models

Thesis submitted for the degree of Philosophiae Doctor

Department of Informatics
Faculty of Mathematics and Natural Sciences

2020



© Farhad Nooralahzadeh, 2020

Series of dissertations submitted to the
Faculty of Mathematics and Natural Sciences, University of Oslo
No. 2299

ISSN 1501-7710

All rights reserved. No part of this publication may be
reproduced or transmitted, in any form or by any means, without permission.

Cover: Hanne Baadsgaard Utigard.
Print production: Reprosentralen, University of Oslo.


In memory of my father,
I miss you everyday! You always encouraged me.




Abstract

Real-world applications of natural language processing (NLP) are challenging.
NLP models rely heavily on supervised machine learning and require large
amounts of annotated data. These resources are often based on language data
available in large quantities, such as English newswire. However, in real-world
applications of NLP, the textual resources vary across several dimensions, such
as language, dialect, topic, and genre. It is challenging to find annotated data
of sufficient amount and quality. The objective of this thesis is to investigate
methods for dealing with such low-resource scenarios in information extraction
and natural language understanding. To this end, we study distant supervision
and sequential transfer learning in various low-resource settings. We develop and
adapt neural NLP models to explore a number of research questions concerning
NLP tasks with minimal or no training data. We first make use of sequential
transfer learning in order to induce non-contextualized word embeddings to
capture domain-specific semantics and benefit downstream tasks in NLP. We
subsequently enhance these embeddings using a domain-specific knowledge
resource and present a benchmark dataset for intrinsic and extrinsic evaluation
of domain embeddings. In the information extraction field, we propose a hybrid
model that combines a reinforcement learning algorithm with partial annotation
learning to clean the noisy, distantly supervised data for low-resource named
entity recognition in different domains and languages. In the next step following
entity detection in the information extraction pipeline, we design a neural
architecture with syntactic input representation to alleviate domain impact in low-
resource relation extraction. Finally, we introduce a cross-lingual meta-learning
framework that provides further improvements in low-resource cross-lingual
natural language understanding tasks in various settings and languages.




Acknowledgments

This Ph.D. thesis and research project have been materialized with the support
and sincere encouragement and guidance of several people who have inspired me
to continue my path and settle for nothing less than the best. Thus, I would
like to express my utmost gratitude to these people for their support.

First and foremost, I would like to express my sincere gratitude to my
supervisors Lilja @vrelid and Jan Tore Lgnning, for the continuous support
of my Ph.D. study and related research, for their patience, motivations, and
immense knowledge. Their guidance helped me in all the time of research and
writing of this thesis. I could not have imagined having better supervisors and
mentors for my Ph.D. study.

Besides my supervisors, I would like to thank the SIRIUS management team.
I am especially grateful for the hard work and support of Arild Waaler, David
Cameron, Ingrid Chieh Yu, and Lise Reang, as well as Tom Erling Henriksen for
his advice during and after the mentoring program.

I am thankful to Mara Abel, and her team at the Federal University of
Rio Grande do Sul (UFRGS) for hosting me and many fruitful discussion on
Ontology during my research visit in Brazil. Obrigado a todos.

I also would like to say a special thanks to Isabelle Augenstein, Johannes
Bjerva, and CopeNLU research team who have hosted, supported, and encouraged
me during my second research visit at the University of Copenhagen.

From the bottom of my heart, I would like to say big thank you for all the
SIRIUS and Language Technology Group, past and present members, for the
feedback and support on different occasions and for creating a friendly work and
research environment. Tusen takk.

Last but not least, I would like to thank my family. Finishing this work
would not have been possible without your love and support. I love you all.

¢ Farhad Nooralahzadeh
Oslo, November 2020




Contents

Abstract
Acknowledgments
Contents

List of Figures

List of Tables

1 Introduction
1.1 Motivation... 2...
1,2 Research Questions .... 2.2... 0. eee es

1.3 Structure of the Thesis .................00484
1.4 Publications .. 2... 2...

2 Background

Di Dimensions of textual variation ...............
2.2 Deep Neural Networks in NLP ................
2.3 Distant Supervision ..........0.. 00000000 0
2.4 ‘Transfer Learning. . i: i iit bt bee te HH ww
2.5 Sequential Transfer Learning ...............0..
2.6 Information Extraction ...............0000.

2.7 Natural Language Understanding ..............

3 Evaluation of Domain-specific Word Embeddings

Ball Introduction s.4.5 5533443333332 mee wee wo
3.2 Related Work .........0.0. 0000000 eee
3.3 Intrinsic Evaluation Setup ..............0.00.
3.4 Intrinsic Evaluation Experiments ..............

3.5 Manual Analysis. 2.266 ee eee tt tte eee ES
3.6 Embedding Enrichment Using a Knowledge Resource . . .

3.7 Extrinsic Evaluation ............... 2.0000.

3.8 BUMMATY «ae e ey ee ek eed EEE EER MOEA M ewe
4 Named Entity Recognition in Low-Resource Domains

4.1 Introduction .. 2... 2... 2 ee

4.2 Background ... 2... 0.0... 0 ee
4.3 Low-Resource NER ... 2... 0.0.0.0. ee ee ee
4.4 Meodeless eww RR 228844 R FRE BREE eB eS

oO RNH

vii


Contents

4.5 Experiments .. 2... 0. 2. ee
4.6 Performance Comparison ............0..00004
4.7 Size of Gold Dataset .................00.0.
4.8 Summary... 2. ee
5 Low-Resource Relation Extraction
5.1 Introduction ..::es8e eee eee ewe RE REE REG
5.2 Previous Work... 2.2... 0. ee
5.3 SemEval 2018 Task 7... .......0...2...0 0000.4
5.4 Evaluation Metrics ........0..0. 0000000004
5.5 System Design... ss uw wee we ww eee eee
5.6 Initial Experiments ................200040.4
5.7 Syntactic Dependency Representations ...........
5.8 Error Analysis... 2... 2 ee
5.9 SUMMSTY sss see Pw EMM REE eee eee EE EES
6 Natural Language Understanding in Low-Resource Genres
and Languages
6.1 Introduction .. 2... 2. ee
6.2 Natural Language Understanding (NLU) ..........
6.3 NLU Models 2:2 s.em BB BR Gee HH EEE EE ED
6.4 Model-Agnostic Meta-Learning (MAML) ..........
6.5 Related Work . 2... ...0.0 0.000002 eee ee eee
6.6 Cross-Lingual Meta-Learning.................
6.7 Experiments :::: se ewww wR HR RE ee Ee
6.8 Discussion and Analysis... ............-.0004
6.9 Summary... 2. ee
7 Conclusion and Future work
7.1 Proposed Methods and Findings ...............
7.2 Contributions .. 2... 0... ee
7.3 Future directions .........0.. 00000022 e eae
Bibliography

viii

115
115
117
120
123
127
129
129
141
144

147
147
151
152

155


List of Figures

21

2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
2.10
2.11
2.12
2.13
2.14
2.15

3.1
3.2
3.3
3.4
3.5

3.6
3.7

3.8

3.9

4.1
4.2
4.3

4.5
4.6

4.7

Structure of a feed-forward neural networks model (Goodfellow
et aly 20UB) coo eT TTR wR Mw OO
CNN architecture for a sentence classification task ........
Recurrent Neural Networks (RNNs) architecture .........
Bidirectional RNNs Model ................0000.
LSTM module structure at time stept ..............
GRU module structure at time stept ...............
The representation of the sequence to sequence (seq2seq) model
The attention model... ............ 00.000 000.%
The encoder-decoder model with Transformer architecture

A taxonomy for transfer learning in NLP .............
Word2vec Architectures... 0.2 ee
Comparison of the model architectures in BERT, GPT and ELMo
Adaptation stage using GPT .................04.
Sequential transfer learning using BERT .............
Fine-tuning in BERT for various NLP tasks... .........

Term structure in the slb glossary... 2... ........00.0.,
Distribution of the sources in the domain corpora. ........
Annotation interface (1): login and annotation guideline.

Annotation interface (2): disciplines and years of experience. . .
Annotation interface (3): target and prediction word pairs and
relations: owewacae tte t dade eee ee HHH ee
Agreements /Disagreements categories among annotators. ... .
Leveraging structure in external lexical resources to produce
semantic landmark (Pilehvar and Collier, 2017). .........
Graph with links between related words showing the observed
(grey) and the inferred (white) word vectors. ...........
Term structure in the Geoscience Vocabularies (GeoSci) data set.

General Architecture in the Deep Neural NER... ........
Architecture of BiLSTM-CRF framework... ...........
Character- and word-level BiLSTM encoding of NER model.

NER+PA+RL architecture... 2.2... ee
Annotation of the distantly labeled example in Partial-CRF.

Performance of the different configuration of our model trained
on various sizes of annotated dataset in the BC5CDR.......
Test F1 score vs. the number of distantly supervised sentences in
the BC5CDR dataset... 2... 0.000.000.0000 0000.

11
12
14
14
15
16
17
18
19
21
25
25
26
27
28

40
42
51
51

52
53

57

59
60

71
72
74
80
81

91

91


List of Figures

4.8

4.9

5.1

5.2

6.1
6.2
6.3

6.4
6.5

6.6

6.7

6.8

6.9

6.10

Performance of the different configuration of our model trained
on various sizes of annotated dataset in the LaptopReview. .. .
Test F1 score vs. the number of distantly supervised sentences in
the LaptopReview dataset. ...............20000.

Model architecture with two channels for an example shortest
dependency path, .::: eee meeeeeeeeee bab teta as
Dependency representations for the example sentence. ......

QA instances in the MLQA dataset... ............2..,
Architecture of the ESIM model... .............--5
Modified Masked Language Model and Translation Language
Model in XLM........0.0.00 0.000. ee ee ee
Model Agnostic Meta-Learning (MAML) in supervised learning.
Example of applying Model Agnostic Meta-Learning (MAML) in
supervised learning. .. 2.6. ee ee
Differences in performance in terms of accuracy scores on the test

set for zero-shot X-MAML on XNLI using the Multi- BERT model.

Differences in performance in terms of accuracy scores on the test
set for the zero-shot case using training (without meta-learning)
on XNLI with the Multi- BERT model. ...........2.2..
Differences in performance in terms of accuracy scores on the test
set for zero-shot X-MAML on XNLI using the En-BERT (English)

Differences in performance in terms of accuracy scores on the test
set for few-shot X-MAML on XNLI using the Multi- BERT model
The mutual gains among English (en), German (de), Russian (ru),

and Chinese (zh) languages in zero-shot X-MAML with Multi-BERT.

125

134

137

138

140

144


List of Tables

3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8

3.9

3.10
3.11

3.12
3.13

3.14
3.15
3.16
3.17

4.1

4.2
4.3
4.4
4.5

5.1

5.2
5.3
5.4

5.5
5.6
5.7

Absolute intrinsic evaluation datasets................
Extrinsic evaluation tasks and datasets... .........000.
N-grams & Part of speech tags in the slb glossary. ........
Sources of the Oil and Gas corpus... ............0.0.-
Evaluation results for different architectures. ...........
Evaluation results for different vector size (default=100).
Evaluation results for different context window size (default=5).
Evaluation results for different number of negative samples
(defamlt=5)i0 cco oa Gece ao PTT ERD So
Evaluation results for different value for frequency cut off
(default=5).. 0...
General domain and domain-specific embedding models... . . .
Results from the intrinsic comparative evaluation of general
domain and domain-specific embedding models. .........
Manual analysis results for the 10-most-similar words. ......
N-grams and Relations in the Geoscience Vocabularies (GeoSci)
datasel: swzaeeuwe BRR Rd eT ERT RTE RRR ww SD
Evaluation over the GeoSci knowledge resource. .........
Evaluation in the slb data set (learning data set). ........
Classification data set... 2...
Results of the classification task with various configurations.

Example sentence from CoNLLO03 in BIO and BIOES annotation
schemes. 2.0...
Overview of datasets in our experiments. .............

Result with different setting of the distantly supervised NER model.

NER models comparison. ..........0.. 0000p eee
Unsupervised NER Performance Comparison............

Semantic relation typology and the coarse relations in SemEval
BOIS ‘Task 7: sa caw e Baba eT aR RRR ww
Number of instances for each relation in the final dataset... ..

F1.(avg. in 5-fold) scores for different model setting during training.

F1.(avg. in 5-fold) scores for different dependency representation
during training, cae es bat eee ae RRR ww SD
Official evaluation results of the submitted runs on the test set.

Results on SemEval Task 2018 Task 7 (Gabor et al., 2018)... .
Effect of using the shortest dependency path on each relation type
in Sub-task Lidlwecca ce cee t ate e ee eR HHH

35

Al
42
45
46
46

AT

AT
48

49
55

61
62
62
63
64

70
84
87

89

101
104

105
106
106

108

xi


List of Tables

5.8

5.9

5.10

5.11

5.12

6.1
6.2
6.3

6.4

6.5

6.6

6.7

6.8

6.9

6.10

7.1

xii

Hyper parameter optimization results for each model with different

representation. i sass bev w HH wwe eee Eee Eas 109
Performance of each model with optimized hyper parameters for
different representation... 2... 2.0.0.2... 000000 eae 109
Effect of using the different parser representation on each relation
type in sub-task 1.1.2... 0.202200. 0222 ee ee ee 111

The examples for which the CoNLL/SB-based models in the
classification sub-task correctly predict the relation type in 5-fold

trials, whereas the UD based model has an incorrect prediction. 112
The examples based on the CoNLL-, SB- and UD-based models

in the extraction sub-task. ..........0.00 2.052 eee 112
Statistics for the MultiNLI corpus by genre. ........... 118
Examples from the MultiNLI corpus. ............... 118
Examples (premise and hypothesis) from various languages and

genres from the XNLI corpus (Conneau et al., 2018). ...... 119

Overview of the number of QA instances in the development and
test portions of the MLQA dataset across the different languages. 119

Test accuracies with different settings of MAML on MultiNLI. . 132
Accuracy results on the XNLI test set for zero- and few-shot
X-MAML. Columns indicate the target languages. ........ 133

The performance in terms of average test accuracy for the zero-
shot setting over 10 runs of X-MAML on the XNLI dataset using
Multi-BERT.......... PWN M EW eee Eee EE 135
The performance in terms of a average test accuracy for the zero-
shot setting over 10 runs of X-MAML on the XNLI dataset using
En-BERT. .......... Le 139
The performance in terms ats average test accuracy for the few-
shot setting over 10 runs of X-MAML on the XNLI dataset using

Multi-BERT. .. 2... .0..0.20.020.0. 0.200000 0000000. 141
F1 scores (average over 10 runs) on the MLQA test set using

zero-shot X-MAML. ... 2... 0.000000 2 eee ee 142
Overview of the research questions and related chapters. .... 148


Chapter 1
Introduction

1.1. Motivation

There is a growing interest in real-world applications of natural language
processing (NLP) for extracting, summarizing, and analyzing textual data.
While NLP methods have led to many breakthroughs in practical applications,
most notably perhaps in machine translation, question answering, and natural
language inference, it is still challenging to use NLP in many real-world scenarios.
Since NLP relies heavily on supervised machine learning, the modeling of most
NLP tasks requires large amounts of annotated data. These resources are often
based on language data available in large quantities, such as English newswire.
However, in NLP’s real-world applications, the textual resources may vary across
several dimensions, such as language, dialect, topic, genre, etc. Considering
the cross-product of these dimensions, it is difficult to find annotated data of
sufficient amount and quality that spans all possible combinations and assists
current advanced NLP techniques (Plank, 2016).

In general, NLP application scenarios, can be classified into three categories
according to their data resources (Duong, 2017): (i) High- or Rich-resource
settings, where a large amount of annotated data is available; (ii) Low-resource or
Resource-poor ones, where there is limited annotated data; and (iii) Zero-resource
settings, where there is no annotated data available in the target context. Off-the-
shelf resource-intensive NLP techniques tend to perform poorly where annotated
data are not readily available (i-e., low-resource and zero-resource settings). An
immediate solution is to create annotated data representative of new target
scenarios. However, collecting and annotating corpora for each new variety
requires experts and is usually expensive. Therefore, it is necessary to find
techniques that can relieve the problem of creating training sets. Our primary
motivation in this thesis is based on the following argument in Plank (2016):

"If we embrace the variety of this heterogeneous data by combining it
with proper algorithms, in addition to including text covariates/latent
factors, we will not only produce more robust models, but will also
enable adaptive language technology capable of addressing natural
language variation."

NLP for low-resource settings has recently received much attention, with
dedicated workshops on the topic (Haffari et al., 2018; Cherry et al., 2019).
In general, most previous work associates the low resource property with the
language dimension (King, 2015; Tsvetkov, 2016; Duong, 2017; Kann et al., 2019).
In this work, we follow Plank (2016) and consider the low-resource setting as
fundamentally multi-dimensional, spanning over all kinds of variability within


1. Introduction

natural language, e.g., language, dialect, domain, genre. Therefore, the scope of
this thesis is broader, and we explore how to adapt and improve the performance
of NLP algorithms in a number of different low-resource settings, spanning
across different domains, genres and languages and dealing with a number of
central NLP tasks. We here make a distinction between domain, genre, and
language. We call the variety aspect domain when the source dataset defers in
terms of topic (chapters 3, 4, and 5). The term Topic is the general subject of
a document and ranging from very broad to more detailed such as oil and gas,
biomedical, and e-commerce. Furthermore, we use the term genre, where the
source dataset covers non-topical text properties such as function, style, and
text type in Chapter 6.

A number of approaches have been proposed to address the challenge of
low-resource scenarios. They have significantly improved upon the state-of-
the-art on a wide range of NLP tasks for various settings. In this thesis, we
make use of adaptation techniques that fall into the following main paradigms:
(i) Distant Supervision: A supervised learning paradigm where the training data
is not manually annotated, but automatically generated using knowledge bases
(KBs) and heuristics (Mintz et al., 2009) (ii) Transfer Learning: Techniques for
leveraging data from additional domains, tasks or languages to train a model
with better generalization properties (Ruder et al., 2019).

Real-world applications of NLP typically incorporate a number of more
specialized, task-specific systems, e.g., pre-processing, various types of syntactic
or semantic analysis, inference, etc. Here we focus mainly on NLP tasks from
the areas of Information Extraction (specifically Named Entity Recognition and
Relation Extraction) and Natural Language Understanding (more specifically
Natural Language Inference and Question-Answering).

Before we delve into the theoretical and experimental study of our work, in
the next few pages, we present our research questions and highlight some of our
main contributions and, finally, provide a more detailed outline of the thesis.

1.2 Research Questions

At a high level of abstraction, we attempt to answer the following main research
questions in this thesis:

RQI. What is the impact of different input representations in neural low-resource
NLP?

The vector representations of tokens instantiate the distributional hypothesis by
learning representations of the meaning of words, called embeddings, directly from
text corpora. These representations are crucial elements in the performance of
downstream NLP systems and underlie the more powerful and more recent
contextualized word representations. We here study input representations
trained on data from specific domains using sequential transfer learning of
word embeddings. Concretely, we attempt to answer the following research
questions:


Research Questions

(i) Can word embedding models capture domain-specific semantic relations
even when trained with a considerably smaller corpus size?

(ii) Are domain-specific input representations beneficial in downstream NIP
tasks?

In order to address these questions, we study input representations trained on
data from a low resource domain (Oil and Gas). We conduct intrinsic and
extrinsic evaluations of both general and domain-specific embeddings. Further,
we investigate the effect of domain-specific word embeddings in the input layer of
a downstream sentence classification task in the same domain. Domain-specific
embeddings are further studied in the context of the relation extraction task on
data from an unrelated genre and domain: scientific literature from the NLP
domain.

In many NLP tasks, syntactic information is viewed as useful, and a variety
of new approaches incorporate syntactic information in their underlying models.
Within the context of this thesis, we hypothesize that syntax may provide a level
of abstraction that can be beneficial when there is little available labeled data.
We pursue this line of research particularly for low-resource relation extraction,
and we look at the following question:

(iii) What is the impact of syntactic dependency representations in low-resource
neural relation extraction?

We design a neural architecture over dependency paths combined with domain-
specific word embeddings to extract and classify semantic relations in a
low-resource domain. We explore the use of different syntactic dependency
representations in a neural model and compare various dependency schemes. We
further compare with a syntax-agnostic approach and perform an error analysis
to gain a better understanding of the results.

RQ Il. How can we incorporate domain knowledge in low-resource NLP?

Technical domains often have knowledge resources that encode domain knowledge
in a structured format. There is currently a line of research that tries to
incorporate this knowledge encoded in domain resources in NLP systems. The
domain knowledge can be leveraged either to provide weak supervision or to
include additional information not available in text corpora to improve the
model’s performance. Here, we explore this line of research in low-resource
scenarios by addressing the following questions:

(i) How can we take advantage of existing domain-specific knowledge resources
to enhance our models?

We investigate the impact of domain knowledge resources in enhancing
embedding models. We augment the domain-specific model by providing
vector representations for infrequent and unseen technical terms using a domain
knowledge resource and evaluate its impact by intrinsic and extrinsic evaluations.


1. Introduction

Given the availability of domain-specific knowledge resources, distant
supervision can be applied to generate automatically labeled training data
in low-resource domains. In this thesis, we explore the use of distant supervision
for low-resource Named Entity Recognition (NER) in various domains and
languages. We here address the following question:

(ii) How can we address the problem of low-resource NER using distantly
supervised data?

The outcome of distant supervision, however, is often noisy. To address this
issue, we explore the following research question:

(iii) How can we exploit a reinforcement learning approach to improve NER in
low-resource scenarios?

We present a system which addresses the problem of noisy, distantly supervised
data using reinforcement learning and partial annotation learning.

RQIIl. How can we address the challenges of low-resource scenarios using transfer
learning techniques?

Transfer learning has yielded significant improvements in various NLP tasks.
The most dominant practice of transfer learning is to pre-train embedding
representations on a large unlabeled text corpus and then to transfer these
representations to a supervised target task using labeled data. We explore this
idea, namely sequential transfer learning of word embeddings, in the first research
question (i.e., RQ I above).

Further, we consider the transfer of models between two linguistic variants
such as genre and language, when little (i-e., low-resource) or no data (i.e.,
zero-resource) is available for a target genre or language. We study this
challenging setup in two natural language understanding tasks using meta-
learning. Accordingly, we investigate the following research questions:

(i) Can meta-learning assist us in coping with low-resource settings in natural
language understanding (NLU) tasks?

(ii) What is the impact of meta-learning on the performance of pre-trained
language models in cross-lingual NLU tasks?

We here explore the use of meta-learning to perform the zero-shot and few-
shot cross-lingual and cross-genre transfer in two different natural language
understanding tasks: natural language inference and question answering.

1.3 Structure of the Thesis

This thesis is a collection of case studies with the unifying objective of addressing
low-resource settings in NLP and is structured as follows:


Structure of the Thesis

Chapter 2: Background

This chapter contains the background that is necessary to understand the
contributions of the thesis as a whole. It gives an overview of a particular
family of machine learning models that will be employed in the thesis, Deep
Neural Networks (DNNs). In this chapter, we describe two paradigms that have
been proposed to address low-resource NLP: Distant supervision and Transfer
learning. The general NLP areas of Information Extraction (IE) and Natural
Language Understanding (NLU) are briefly described in this chapter, whereas
details regarding specific NLP tasks are delegated to subsequent chapters.

Chapter 3: Evaluation of Domain-specific Word Embeddings

In this chapter, we study input representations trained on data from a low
resource domain (Oil and Gas) using sequential transfer learning of word
embeddings. We conduct intrinsic and extrinsic evaluations of both general
and domain-specific embeddings. We further adapt embedding enhancement
methods to provide vector representations for infrequent and unseen terms.

Chapter 4: Named Entity Recognition in Low-Resource Domains

In this chapter, we explore the use of distant supervision for Named En-
tity Recognition (NER) in four low-resource scenarios. We apply distant
supervision and present a system that addresses the problem of noisy, distantly
supervised data in two ways. We study a reinforcement learning strategy with a
neural network policy to identify false positive instances at the sentence level.
We further adopt a technique of incomplete annotation to address the false
negative cases. Finally, we evaluate the proposed hybrid model on various
benchmark datasets.

Chapter 5: Low-Resource Relation Extraction

In this chapter, we focus on relation extraction in a low resource setting,
namely scientific papers in NLP. We study the effect of varying input repre-
sentations to a neural architecture, specifically Convolutional Neural Networks
(CNN), to extract and classify semantic relations between entities in scientific
papers. We investigate the effect of transfer learning using domain-specific word
embeddings in the input layer and go on to provide an in-depth investigation of
the influence of different syntactic dependency representations, which are used to
produce dependency paths between the entities in the input to the system. We
further compare our syntax-informed approach with a syntax-agnostic approach.
In order to gain a better understanding of the results, we perform manual error
analysis.


1. Introduction

Chapter 6: Natural Language Understanding in Low-Resource Genres
and Languages

In this chapter, we consider the transfer of models along two dimensions
of variation, namely genre and language, when little or no data is available for
a target genre or language, i.e. low-resource and zero-resource settings. We
explore meta-learning to address this challenging setup, where, in addition to
training a source model, another model learns to select which training instances
are the most beneficial. We experiment using standard supervised, zero-shot
cross-lingual, as well as few-shot cross-genre and cross-lingual settings for differ-
ent natural language understanding tasks (natural language inference, question
answering). We make use of an extensive experimental setup to investigate the
effect of meta-learning in various low-resource scenarios. We apply our proposed
cross-lingual meta-learning framework on various pre-trained language models
for zero-shot and few-shot natural language inference and question answering
tasks. We further conduct a comprehensive analysis to investigate the impact of
typological sharing between languages in our framework.

Chapter 7: Conclusion and Future work

In this chapter, we describe our proposed methods and findings. Our main
contributions are summarized, and we provide an outlook into future directions
in this chapter.

1.4 Publications

The part of the work presented in this thesis has been presented in the following
scientific articles:

1. Nooralahzadeh, Farhad; @vrelid, Lilja and Lgnning, Jan Tore (2018).
"Evaluation of Domain-specific Word Embeddings using Knowledge
Resources." In: Proceedings of the Eleventh International Conference
on Language Resources and Evaluation, European Language Resources
Association (ELRA).

2. Nooralahzadeh, Farhad; @vrelid, Lilja and Lenning, Jan Tore (2018).
"SIRIUS-LTG-UiO at SemEval-2018 Task 7: Convolutional Neural Net-
works with Shortest Dependency Paths for Semantic Relation Extraction
and Classification in Scientific Papers." In: Proceedings of the 12th Interna-
tional Workshop on Semantic Evaluation. Association for Computational
Linguistics.

3. Nooralahzadeh, Farhad and Qvrelid, Lilja (2018). "Syntactic Dependency
Representations in Neural Relation Classification." In: Proceedings of the
Workshop on the Relevance of Linguistic Structure in Neural Architectures
for NLP. Association for Computational Linguistics.


Publications

4. Nooralahzadeh, Farhad; Lénning, Jan Tore and QOvrelid, Lilja (2019).
"Reinforcement-based denoising of distantly supervised NER with partial
annotation." In: Proceedings of the 2nd Workshop on Deep Learning
Approaches for Low-Resource NLP (DeepLo 2019). Association for
Computational Linguistics.

The following preprint is also discussed:

1. Nooralahzadeh, Farhad; Bekoulis, Giannis; Bjerva, Johannes; and
Augenstein, Isabelle (2020). "Zero-shot cross-lingual transfer with meta
learning." In: CoRR vol. abs/2003.02739.

Finally, while not directly related, the following article has also been completed
over the course of the PhD:

1. Nooralahzadeh, Farhad and @vrelid, Lilja (2018). "SIRIUS-LTG: An Entity
Linking Approach to Fact Extraction and Verification." In: Proceedings
of the First Workshop on Fact Extraction and VERification (FEVER).
Association for Computational Linguistics.




Chapter 2
Background

This chapter contains the background that is necessary to understand the
contributions of the thesis as a whole. We start by briefly discussing variation
in textual data which is central to the thesis (Section 2.1). After that, we
give an overview of a particular family of machine learning models that will be
employed in the thesis, Deep Neural Networks (Section 2.2). We subsequently
describe two machine learning paradigms that have been proposed to address
low-resource NLP: Distant supervision in Section 2.3 and Transfer learning in
Section 2.4. The general NLP areas of Information Extraction (IE) and Natural
Language Understanding (NLU) are briefly described in Section 2.6 and 2.7,
respectively. Whereas details regarding specific NLP tasks are delegated to
subsequent chapters.

2.1 Dimensions of textual variation

The notion of domain is frequently used in low-resource NLP, although there is
little common ground in what constitutes a domain (Plank, 2011). The term is
usually used to refer to textual data of the same topic, genre, or source under
the assumption that this common denominator will have some systematic impact
on the vocabulary or linguistic aspects of the text. Various definitions of the
term domain have been presented in previous research such as Lee (2002); Finkel
and Manning (2009); Plank (2011); van der Wees et al. (2015); Plank (2016)
and Aharoni and Goldberg (2020). Lee (2002) notes that the terms genre,
register, teat type, domain, sub-language, and style are often used differently
in various communities or even interchangeably. Finkel and Manning (2009)
further describe the meaning of domain as "It may refer to a topical domain
or to distinctions that linguists might term mode (speech versus writing) or
register (formal written prose versus SMS communications)". It is defined in
Plank (2011) as a collection of texts from a certain coherent sort of discourse,
and Aharoni and Goldberg (2020) define domains by implicit clusters of sentence
representations provided by pre-trained language models (see Section 2.5.1.2).
However, Plank (2016) argues that there are numerous other factors that should
be taken into consideration, e.g., demographic factors, communicational purpose,
sentence type, style, technology/medium, language, etc. She proposes to see
a domain as a variety in a large dimensional variety space. In this view, most
textual datasets are sub-spaces of this variety space. The dimensions in this
space are fuzzy aspects such as language, dialect, topic, genre, social factors
(age, gender, personality, etc.), including yet unknown aspects. "A domain
forms a region in this space, with some members more prototypical than others"
(Plank, 2016).


2. Background

In this thesis and as noted already in Chapter 1, we focus in particular on
the textual varieties of domain, genre, and language. We call the variety aspect
domain when datasets differ in terms of topic. Furthermore, we use the term
genre where dataset differences are characterized by non-topical text properties
such as function, style, and text type.

2.2 Deep Neural Networks in NLP

Natural Language Processing (NLP) involves the engineering of computational
models and processes to solve practical problems in understanding human
languages. Processing natural language text encompasses a number of syntactic,
semantic, and discourse-level tasks (e.g., word segmentation, part-of-speech
tagging, phrase chunking, parsing, word sense disambiguation, named entity
recognition, semantic role labeling, semantic parsing, anaphora resolution).
For a long time, NLP systems were based on traditional machine learning
approaches, centered around algorithms such as Perceptrons, linear Support
Vector Machines (SVM), and Logistic Regression trained on sparse hand-crafted
features (Goldberg, 2017). These methods are known to have some challenges.
Recently, the re-emergence of artificial neural networks (ANNs), also known
as deep neural networks (DNNs), provides a way to develop highly automatic
features and representations to handle complex interpretation tasks. These
approaches, with the pioneering work of Collobert et al. (2011), have yielded
impressive results for many different NLP tasks. In general, a neural network
with many hidden layers is often referred to as a deep learning model. In the
following sections, we will briefly introduce several DNN models that have been
widely employed in NLP and that are central also in this thesis.

2.2.1. Deep Feed-Forward Networks

Deep Feed-Forward Networks, also known as Feed-Forward Neural Networks
(FFNNs) or Multi-layer Perceptrons (MLPs), are the simplified version of DNNs
(Goodfellow et al., 2016). They are the foundation of most deep learning models
and consist of many layers, with the first layer taking the input and last layer
providing outputs. The layers in the middle are known as hidden layers, and
capture relations between the input and output. Figure 2.1 shows a typical
structure of a feed-forward neural networks model.

The hidden layer is used to transform the input layer values into values in a
higher-dimensional space so that we can learn more features automatically from
the input. The transformation is done by a collection of perceptron nodes in the
hidden layer using a non-linear function, known as an activation function. In
such a model, information constantly flows from one layer to the next (i.e., input
layer —> hidden layers—>+ output payer). Training of the FFNNs model in
supervised learning is done by two steps: (i) Forward propagation of information
from the input to the output layer through hidden layers and compute a loss
function (i.e., training errors), and (ii) Backward propagation of loss from the

10


Deep Neural Networks in NLP

Output
Layer

Layer

1-st hidden k-th hidden
Layer Layer

Figure 2.1: Structure of a feed-forward neural networks model (Goodfellow
et al., 2016)

output to the input layer. The aim of backward propagation is to minimize the
training error by measuring the margin of error of the output and then adjust the
network parameters accordingly. We repeat both forward- and back-propagation
to predict an output until the parameters of the model are calibrated.

2.2.2 Convolutional Neural Networks

Convolutional Neural Networks (CNNs) (LeCun and Bengio, 1998; Krizhevsky
et al., 2012) are a specialized kind of deep neural networks where convolution
operations, derived from mathematics and signal processing, are applied to
capture indicative local patterns from the data. The resulting local aspects are
most informative for the prediction task at hand. In the field of NLP, The CNN’s
feature functions (i.e., the convolution filters) are applied to extract high-level
features from adjacent words or n-grams regardless of their position, while taking
local ordering patterns into account (Goldberg, 2017). The CNN architecture
consists of multiple convolutions and pooling layers. The convolution layers aim
to extract useful local features from the input, which results in multiple feature
maps. Then, a pooling layer is applied to one or multiple convolution layers to
reduce the spatial size of feature maps. In the end, usually, a fully connected
layer outputs the probability distribution over each target class.

Figure 2.2 presents the CNN architecture applied to a sentence classification

11


2. Background

A activation function

senor 1-max softmax function
joolin regularization
a: 8 in this layer

3 region sizes: (2,3,4) 2 feature
Sentence matrix 2 filters for each region maps for 6 univariate
75 size each vectors
totally 6 filters region size concatenated

together to forma
single feature
vector

d=5

like

this
movie

very
much

Figure 2.2: CNN architecture for a sentence classification task (Zhang and
Wallace, 2017)

task and proposed by Kim (2014). For each sentence, words are represented as a
vector in the input layer. Word vectors can be initialized randomly or fetched
from pre-trained embeddings (see Section 2.5.1). The filter layer in Figure 2.2
includes three filter region sizes: 2, 3, and 4, each of which has two filters.
This layer performs convolutions on the sentence matrix and generates (variable-
length) feature maps. Subsequently, the 1-max pooling function performs pooling
over each map (i.e., the largest number from each feature map is extracted).
Thus a uni-variate feature vector is generated from all six maps, and a feature
vector is formed by connecting these six features. Finally, the fully connected
softmax layer receives this feature vector as input and uses it to classify the
sentence as a binary classification task to output two possible output states
(Zhang and Wallace, 2017). We could provide different characteristics or views

12


Deep Neural Networks in NLP

of a sentence in the input layer, referred to as a channel. For instance, in the
sentence classification task, one channel will be the sequence of words, while
another channel is the sequence of corresponding POS tags. It is common to
apply a different set of filters to each channel, and then combine the multiple
representations of input into a single vector.

The explained architecture in Figure 2.2 is just an example of CNNs (albeit
a widely used architecture), and there are various designs where a different
set of layers (i.e., filter, pooling, and fully connected layer) along with various
hyper-parameters such as filter size, number of feature maps, activation function,
pooling strategy, are employed.

CNN models have been effectively applied to position-invariant contextual
features in various NLP tasks such as sentence and document classification.
However, they have some challenges in maintaining sequential order and modeling
long-distance dependencies, which is essential for many NLP tasks (Young et
al., 2018). Recurrent Neural Networks and its variants are introduced as a
suitable solution for such types of tasks, and we will now turn to these models
in the next section.

2.2.3 Recurrent Neural Networks

Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986; Elman, 1990) are
designed to process sequential information. The model is recurrent since it
performs the same task for each input sequence element, such that the current
step’s output is conditioned on the previous step. As shown in Figure 2.3, the
input sequence is typically represented by a fixed-size vector of tokens and is
passed sequentially (one by one) to a recurrent unit. The main power of the
RNN is the ability to memorize the outputs of previous computation steps and
utilize them in the current computation. This capability made the RNNs a
preferred neural architecture in solving sequential NLP tasks such as language
modeling, machine translation, named entity recognition, textual similarity, and
text generation.

Some extensions of RNNs are introduced, such as Bidirectional RNN (Bi-
RNNs) (Schuster and Paliwal, 1997), which can be seen as stacking two RNNs
on top of each other, one going forward, the other one going backward over the
sequence input (Figure 2.4). The Bi-RNNs are based on the idea that the output
at a specific time step depends on the previous elements in the sequence, as well
as future elements.

For a long sequence length, however, the vanilla RNN models suffer from the
vanishing gradient problem (Hochreiter and Schmidhuber, 1997). This means
that the gradient shrinks as it back propagates through time. If a gradient value
becomes extremely small, it does not contribute to the learning process. The
vanishing gradient causes the model to ignore long-term dependencies and, hence,
hardly learn the dependencies between temporally distant sequences. In other
words, the RNNs tend to focus on short term dependencies, which are often not
desired. This limitation is mitigated by alternative network architectures like

13


2. Background

Cell output, Cell output,,, Cell output, Cell output,,,
Sequence embedding
eee eee)
RNN RNN Cell RNN Ce | RNN Cell RNN Ce
Cell Timestep t Timestep t + 1 Timestep t + 2 Timestep t + 3
(IT) een (IT) (IT) een
Xt Xt+1 Xt42 Xt43

Figure 2.3: (left) Folded RNN with an input sequence and feedback loop. (right)
Unfolded version of RNN through the time steps. The same RNN cell is applied
in different time steps to the words in the sequence example. (Pilehvar and
Camacho-Collados, 2020)

/ / i fi
h t+3 he h t+2 Ress h t+1 Re+2 h t he+3
eeeleee (YT ITT) 000/666) eee! )

Figure 2.4: Bidirectional RNNs Model (Pilehvar and Camacho-Collados, 2020)

long short-term memory and gated recurrent unit networks, which are the most
widely used RNN variants in NLP applications.

Long Short-Term Memory Networks (LSTMs) LSTMs (Hochreiter and
Schmidhuber, 1997; Gers et al., 2000) are explicitly designed to cope with
the vanishing gradients problem using a gating mechanism. In the vanilla RNNs,
the repeating modules (i.e., the rectangle boxes in Figures 2.3 and 2.4) have a
straightforward design, such as a single non-linearity. The structure of LSTMs

14


Deep Neural Networks in NLP

Figure 2.5: LSTM module structure at time step t (Pilehvar and Camacho-
Collados, 2020)

is not fundamentally different from the structure of RNNs, but the repeating
module has a different setting. As shown in Figure 2.5, the repeating cell consists
of four neural layers: the input gate, forget gate, cell state, and the output gate.

These layers are calculated according to the following formula (Gold-
berg, 2017):

ig = 0 (a, W™ + Py_1W"™)

fe = o(a, WF + hy WwW")

Op = 0(a,W*? + hy_-1W"*)

C;, = tanh(x,W*° + hy 1W"°)

C; = fr@ C1 +i OG,
he =A 8 tanh(C;)

Where ® is element-wise multiplication, h; is the hidden state in time-step t
and 2, f,o are the input, forget and output gates, respectively. In the first step
of the LSTM block, we decide what information should be retained or thrown
away from the cell state (i.e., forget gate ). For example, in the language model,
the cell state might decide to remember the singular or plural information of
the present subject in order to predict the correct verb tense in the next related
states. While, if it sees a new subject, it will forget the information about the
old subject. C;, is called the candidate cell state and is computed based on the
current input and the previous cell state. C; is the internal memory of the unit.
It is a combination of the previous memory C;_, multiplied by the forget gate,
and the newly computed cell state ts, multiplied by the input gate. Given the
memory C;, the output hidden state hy is computed by multiplying the memory

15


2. Background

Ae-4 @__—— he

Zt
h

Xt

Figure 2.6: GRU module structure at time step t (Pilehvar and Camacho-
Collados, 2020)

with the output gate (i.e., a filtered version of the cell state). Intuitively, in the
LSTM cell, we compute the C and h at time t and output them to the next cell.

Gated Recurrent Unit Networks (GRUs) The GRUs, introduced by Cho et
al. (2014), similar to LSTMs, follow the design of RNNs; however, each repeating
block has a slightly simpler variant of the LSTM. GRU combines the forget gate
and input gate into a single update gate. It incorporates two gates, the reset
gate and the update gate, and manages the flow of information similar to LSTM

without a memory unit (Figure 2.6). The formulation of the GRU module is as
follows (Goldberg, 2017):

Zp = o(a,W* + he-1W"*)

r, = 0(a,.W?" + y_1W""])

fy = tanh(a,W*" + (rp @ hy_1)W"]
he = (1 — 2%) @ he_it, % @ hy

(2.2)

Because of this update, the final model is more straightforward than the standard
LSTM and is also widely used in NLP.

2.2.4 Attention Mechanisms and Transformer

One of the common applications of RNN architectures is in sequence-to-sequence
(seq2seq) models. Seq2seq models are DNNs that have been successfully

16


Deep Neural Networks in NLP

X Y¥ Z <eos>

A B Cc D <eos> X ¥. Z

Figure 2.7: The representation of the sequence to sequence (seq2seq) model -
translating an input sequence A B C D into a target sequence X Y Z. Here,
<eos> indicates the end of a sequence. The blue boxes at the left show the
encoder, and the red boxes construct the decoder (Luong et al., 2015)

employed in NLP tasks like machine translation between multiple languages, text
summarization, and language generation. The seq2seq model tries to transfer an
input sequence to a new output sequence where the length of input and output
may vary. The seq2seq model (Figure 2.7) normally has an encoder-decoder
architecture, composed of:

e An encoder: It compiles the incoming sequence and captures the
information into a context vector (i.e., sentence embedding vector) of
a fixed length. This representation is assumed to be a good summary of
the meaning of the whole source sequence.

e A decoder: It is initialized with the context vector to produce the output
sequence. The early implementation of the seq2seq model used the last
state of the encoder network as the initial decoder state.

Both the encoder and decoder are RNNs with LSTM or GRU units. The naive
seq2seq model works fine for short sequences. However, in a long sequence,
it becomes problematic when the encoder compresses the entire input into a
fixed-sized context vector and transmits it into the decoder as the contextual
information of the input. This problem is addressed by attention mechanisms
proposed by Bahdanau et al. (2015) and Luong et al. (2015).

The attention mechanism (Figure 2.8) allows the decoder to refer back to
the input sequence. Specifically, during decoding, it gives importance to specific
parts of the input sequence instead of the entire sequence. Therefore, in the
attention, all the intermediate outputs from the encoder state are considered,
and we utilize them to generate the context vector from all states. It allows the

17


2. Background

Figure 2.8: The attention model proposed by Bahdanau et al. (2015). The
decoder trying to generate the target word y, given a result of attention and
encoder over the source sequence X1,X2...X 7

model to focus on essential elements by giving weight to each element in the
sequence.

Different ways of constructing attention mechanisms have been introduced,
including global and local attention (Luong et al., 2015) and self-attention
(Vaswani et al., 2017). Self-attention suggests implementing attention to words
in the same sequence. For instance, while encoding a word in an input sentence,
self-attention enables the encoder to look at other words in the input for clues that
can further lead to a better encoding for the word. During decoding to produce
a resulting sentence, it makes sense to provide appropriate attention to words
that have already been produced. This type of attention mechanism has become
widely used in a state-of-the-art encoder-decoder model called transformer.

The transformer model, shown in Figure 2.9, has many stacked layers in both
encoder and decoder components. It considers self-attention in the encoder and
decoder modules, as well as cross-attention between them. The proposed model is
based entirely on an attention mechanism to capture the global relations between
input and output, without including RNNs and CNNs. It incorporates other
techniques in its encoder and decoder components such as residual connections
(He et al., 2016), layer normalization (Ba et al., 2016), dropouts and positional
encodings. The transformer becomes an essential component of pre-trained
language models such as BERT and GPT (see Section 2.5.1.2).

18


Distant Supervision

Output
Probabilities

Add & Norm
Feed
Forward
Add & Norm

Multi-Head
Attention

Add & Norm
Masked
Multi-Head
Attention
a a,

Add & Norm

Nx

Add & Norm
Multi-Head
Attention

Positional @ @

Positional
Encoding @ EY Encoding
Input Output
Embedding Embedding

Inputs Outputs
(shifted right)

Figure 2.9: The encoder-decoder model with Transformer architecture (Vaswani
et al., 2017).

2.3 Distant Supervision

Distant supervision has been proposed to deal with the lack of sufficient labeled
data for training supervised machine learning methods by exploiting existing
knowledge resources. Craven and Kumlien (1999) initiated this idea as a weak
supervision method to populate a knowledge base in the biomedical domain.
Subsequently, Mintz et al. (2009) generalized the initial idea for the relation
extraction task and formulated the distant supervision assumption as follow:

"If two entities participate in a relation, any sentence that contains
those two entities might express that relation."(Mintz et al., 2009, p.
1006)

The term distant is used by assuming that no explicit labeled data is provided,
however knowledge resources (e.g., Wikipedia, Freebase) are available for
automated labeling of training instances in text corpora. Distant supervision
can be formally defined as follows (Smirnova and Cudré-Mauroux, 2018):

"Given a text corpus C and a knowledge base K, distant supervision
assigns relations from K to sentences from C. More specifically, the

19


2. Background

idea is to first collect those sentences from the corpus C that contain
entity pair (e1,e2) where both e; and e2 exist in the knowledge base
K. If there exists one triple (e1,7,e2) in the knowledge base, then
the distant supervision set a label to the sentence as an instance of
relation r." (Smirnova and Cudré-Mauroux, 2018, p. 106:4)

For example, the following sentence contains the entity pair (Steven Spielberg,
Saving Private Ryan)

(2.3) [Steven Spielberg]’s film [Saving Private Ryan] is loosely based on the
brothers’ story.

Assuming the triple (Steven Spielberg, is director of, Saving Private Ryan) exists
in the knowledge base, the textual sentence is labeled with the is director of
relation label. It can be used as training data for subsequent relation extraction.

Distant supervision has been successfully applied to tasks like relation
extraction (Riedel et al., 2010; Augenstein et al., 2014) and entity recognition
(Fries et al., 2017; Shang et al., 2018b; Yang et al., 2018). We will return to this
topic in Chapter 4 where we explore the use of distant supervision for named
entity recognition in low-resource scenarios.

2.4 Transfer Learning

Humankind can learn new tasks faster and more efficiently if he/she has prior
experience with similar tasks. For example, people who know how to ride
a bike will likely manage to ride a motorcycle with little or no training. In
short, we learn how to learn across tasks. This statement brings the following
question: Is it possible to design a machine learning model with similar properties,
learning new tasks by leveraging prior knowledge gained in other learning
processes? Recently, this question is answered by transfer learning (Pan and
Yang, 2010). Transfer learning makes use of knowledge acquired while solving
one problem or more than one problem and applies it to a different but related
problem/s. It refers to a set of methods that extend the learning mechanism
by leveraging data from additional languages, domains, or tasks to train a
model with better generalization properties. Transfer learning has yielded to
a significant improvement in various NLP tasks (Chen and Moschitti, 2019;
Devlin et al., 2019; Howard and Ruder, 2018; Peters et al., 2018) and this is due
to the fact that NLP tasks share common knowledge about language, such as
linguistic representation and structural similarity (Ruder et al., 2019). Moreover,
languages have common typological features such as phonological, grammatical,
and lexical properties (Dryer and Haspelmath, 2013).

Based on different scenarios that are mostly encountered in NLP systems,
Pan and Yang (2010) and Ruder et al. (2019) proposed a taxonomy for transfer
learning for the NLP field (Figure 2.10). Following the notation of Pan and
Yang (2010) and Ruder et al. (2019), transfer learning is defined as follows:

20


Transfer Learning

Domain adaptation

Transductive

Transfer learning

Cross-lingual

learning

Multi-task
learning

yseq owes

Transfer
Learning

syseq
quezessta

Inductive
Transfer learning

&
8s
2 Sequential
Transfer learning
ea

Figure 2.10: A taxonomy for transfer learning in NLP (Ruder et al., 2019)

"Given a settings S = {D,7} where, D is a dataset that contains
a feature space ¥ = {x1,...,%n} with a marginal probability
distribution P(4’) over the feature space. On the other hand, a
task T = {V,P(V),P(Y|¥}) consists of a label space Y, a prior
distribution P(Y), and a conditional probability distribution P()|4)
which is usually learned using the training data consisting pairs of
x; € X and y; € Y. Having a source setting S, including D, and
a corresponding source task 7;, as well as a target setting S; with
D, and target task 7;, the aim of transfer learning is to perform the
target task in order to learn the target P:(4|4;) in D; using the
information provided by the elements in the source setting where
D, #D, or T; 4 Tt. Typically, it is assumed that the target setting
is either in low-resource or zero-resource mode."

According to the proposed taxonomy and notation, transfer learning involves
the following scenarios:

21


2. Background

Transductive transfer learning: The source and target tasks are the same,
whereas the source and target dataset vary. If the marginal probability
distribution of source and target dataset are different P,(¥V;) 4 P:(4;), when the
datasets come from different domains or genres, the scenario is known as domain
adaptation. If there is a discrepancy in the feature spaces of the source and target
datasets ¥, #4 %;, for example, when the datasets are in two various languages,
we refer to cross-lingual learning scenario in NLP. Cross-lingual learning can be
viewed as an extreme case of adaptation (Plank, 2016).

Inductive transfer learning: The source and target tasks are different 7, 4 71,
regardless of whether the source and target datasets are the same or not. In this
case, if the tasks are learned simultaneously, the scenario is known as multi-task
learning, while sequential transfer learning will be used if the learning process is
performed sequentially. We want to stress that if in this category the variation
on the source and target dataset is considered, both scenarios (i.e., multi-task
learning and sequential transfer learning) can be applied to domain adaptation
(i.e., Ps(¥s5) A Pi(4)) and cross-lingual learning (i.e., V7; A 1).
In the context of this thesis, we focus on sequential transfer learning.

2.5 Sequential Transfer Learning

Sequential transfer learning is defined as a setting where a learning process is
carried out in sequence (Ruder et al., 2019). It can be useful when (i) the target
task is in a low- or zero-resource setting, (ii) the source task is in a high-resource
setting, and (iii) the objective is the adaptation of many target tasks. This
learning approach consists of the two following steps (Ruder et al., 2019):

1. Pre-training: The general representation of the model is learnt on the
source language, task, or domain.

2. Adaptation: The learned knowledge is transferred and adjusted to target
languages, tasks, or domains. In other words, it involves copying the
weights from a pre-trained network and tuning them on the targets.

The most dominant practice of sequential transfer learning is to pre-train
embedding representations on a large unlabeled text corpus and then to transfer
these representations to a supervised target task using labeled data. In the
following section, we will give an overview of the pre-trained representations that
are employed in this thesis.

2.5.1 Embedding Representations

The distributed vector representations of tokens, called word embeddings, are
an essential component of neural methods for downstream NLP tasks. Word
embeddings are vectors based on the distributional hypothesis meaning that

22


Sequential Transfer Learning

words appearing in a similar context have a similar meaning. In other words, they
have learned representations of text where words with the same meaning have
a similar representation. Learning useful word representations in a supervised
setting with limited data is often difficult. Therefore, many unsupervised learning
approaches have been proposed to take advantage of large amounts of unlabeled
data that are readily available. It results in more useful word embeddings
(Pennington et al., 2014; Mikolov et al., 2013a). However, the differences in
the meaning of a word in varying contexts are lost when it is associated with a
single representation. Static pre-trained embeddings are limited in two respects
(Pilehvar and Camacho-Collados, 2020): (i) the role of context is ignored, and
(ii) by providing the individual word vector representation, it is problematic
to capture higher-order semantic phenomena, such as compositionality and
long-term dependencies. To alleviate the limitations of static word embeddings
and to deal with varying word context, pre-trained language models (Peters et
al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019; Conneau and
Lample, 2019) are proposed and create context-sensitive word representations.
The success of these approaches suggests that these representations capture
highly transferable and task-agnostic properties of languages.

2.5.1.1. Static Word Embeddings

Word2vec, proposed by Mikolov et al. (2013a), is one of the most popular
approaches in learning word representations from text inputs. It can effectively
capture the semantics of words and is straightforwardly transferred into other
downstream tasks. The proposed method consists of a single layer architecture
based on the inner product between word vectors based on two different learning
approaches as follows:

Continuous Bag-Of-Words (CBOW): Learns the embeddings by estimating
the conditional probability of a particular word based on its context (i.e.,
surrounding words within a specified window size). Specifically, given a sequence
of words, the model receives as input a window of C context words and predicts
the target word w; by minimizing the following objective (Rong, 2014):

IC|

k= S “log P(wiwre,---, Wri, Wi41s +++, We4e) (2.4)
t=1

wraps

and
exp(u] Ue)

V
> | exp(ul ve)
where V is the vocabulary size, ve is the sum of the embeddings vector of the

context words wy_-c,..-, Wt-1, Wi+1,---, We+c, and u is the embeddings vector
of the target word.

P(w,|we_c,---, Wt-1, We; +++, Wetec) => (2.5)

23


2. Background

Skip-gram: Learns by predicting the surrounding words (context) given a
current word. In other words, it minimizes the following objective (Rong, 2014):

IC|

t=1 —C<j<Cij#0

and
exp(Uf, ; Ue)
Dr exp} ur)
where u and v are the current and context word embeddings, respectively.
Figure 2.11 depicts these two approaches of the word2vec model where the
window size C = 2. In order to train the word2vec model, we provide many word-
context pairs where the window size parameter characterizes the context, and the
weights learned by the models make up the actual word vector representations.
While the objective of both word2vec models is computationally expensive, the
negative-sampling approach is presented as a more efficient way of deriving word
embeddings. It means that for each positive pair (i.e., output word and context
word), it samples k words w, from the vocabulary and add it as a negative
example (wo, w,) to the Wneg. The number of negative samples k is a parameter
of the algorithm. To this effect, the following simplified training objective is
capable of producing high-quality word embeddings:

E = logo(ut,v) — S- log o(uj,,v) (2.8)

7 eC Wreg

P(we4j|we) = (2.7)

where uy, is the embeddings of output word (i.e., the positive sample),
a is the sigmoid function, Whe, is the set of negative samples and v in
CBOW it is the mean of the embeddings vector of the context words
Wt-C, +++, Wt-1, Wt+1,---,Wt+c, Whereas, in the skip-gram model it is the input
word embeddings.

2.5.1.2 Contextualized Word Representations

Before pre-trained language models, context-independent or static vectors as
described in the previous section, were generally used for transfer learning in NLP
tasks. A common practice was employing them as a look-up table to structure
the input layer of deep neural models where it results in high training efforts
to learn a target task. Considering the limitations of static word embeddings,
recent work presents context-sensitive word representations using neural language
models with two different transfer strategies (Devlin et al., 2019). The feature-
based approach, such as ELMo (Embeddings from Language Models proposed
by Peters et al. (2018)), uses task-specific architectures that include the pre-
trained representations as additional features. In contrast, in the fine-tuning
strategy such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), the
underlying network structure can be leveraged in the learning of a target task
with simply fine-tuning all pre-trained parameters. These pre-trained language

24


Sequential Transfer Learning

INPUT PROJECTION OUTPUT INPUT PROJECTION OUTPUT

w(t-2)

\

w(t+1) w(t+1)

w(t+2) w(t+2)

CBOW Skip-gram

Figure 2.11: Word2vec Architectures (Mikolov et al., 2013a)

OpenAl GPT

Figure 2.12: Comparison of the model architectures in BERT, GPT and ELMo
(Devlin et al., 2019). It can be seen that BERT is deeply bidirectional, GPT is
unidirectional and ELMo is shallow bidirectional.

models show that despite being trained with only a language modeling task, they
provide highly transferable and task-agnostic features of the language (Liu et
al., 2019a). ELMo creates contextualized representations derived from a 2-layer
bidirectional LSTM. It is trained with a coupled language model (LM) objective
on a large text corpus (Devlin et al., 2019). In contrast, BERT (Bidirectional
Encoder Representations from Transformers) and GPT (Generative Pre-trained
Transformer) are bi-directional and uni-directional language models, respectively,
based on the transformer architecture (Vaswani et al., 2017). They create a
contextualized representation of each token by attending to different parts of the
input sentence. Unlike GPT, BERT integrates the concept of masked language
model in the pre-training phase, where the goal is to predict randomly masked

25


2. Background

Classification Start Text Extract iH Transformer

Entailment stat | Premise | Delim | Hypothesis | Extract |}>| Transformer ++} Linear
yp Transforme!

stat | Texta | beim | Text2 | extract |}+| Transformer

Similarity

Start Text 2 Delim Text 1 Extract ‘| Transformer

stat | Context | Deim | Answer 1 | Extract ||+{ Transformer

Multiple Choice | Start Context Delim | Answer 2 | Extract ||») Transformer |

Start Context Delim | Answer N | Extract |/+| Transformer ||

Figure 2.13: Adaptation stage using GPT. (left) Transformer architecture. (right)
Fine-tuning on different tasks. (Radford et al., 2018)

tokens given their captured context from both directions. It is also trained on
a neat sentence prediction task that further boosts the model’s performance.
Figure 2.12 shows the differences in pre-training model architectures. It can
be seen that BERT uses a bidirectional Transformer, while GPT employs a
left-to-right Transformer. ELMo uses the concatenation of independently trained
left-to-right, and right-to-left LSTMs to generate features for downstream tasks.
Among them, only BERT representations are jointly conditioned on both left
and right context in all layers and it is deeply bidirectional (Devlin et al., 2019).
In the following section, we will discuss how GPT and BERT models are
employed in the second step of sequential transfer learning, namely adaptation.

2.5.2 Adaptation

Adaptation is the second stage of sequential transfer learning, in which the
representation is transferred to a new task. In prior works, only input word
embeddings are transferred to the down-stream task, however with GPT and
BERT, all parameters are transferred to initialize end-task model parameters. It
involves updating the pre-trained representations (i.e., Fine-tuning).

Figure 2.13 depicts the adaptation stage using GPT and several input
transformations to handle the inputs for different types of tasks. It can be seen
that each task has a specific input transformation, and the discriminative fine-
tuning step passes inputs through the GPT pre-trained model (i.e., transformer)
to obtain the intermediate output. Then the transformer output is fed into an
added softmax based linear output layer. BERT’s fine-tuning is similar to GPT,
and for each task, it processes the task-specific inputs and outputs with the
pre-trained BERT and performs fine-tuning of all the parameters end-to-end

26


Information Extraction

Starv/End ma

Masked Sentence A Masked Sentence B Question Paragraph
a t=
Unlabeled Sentence A and B Pair Question Answer Pair

Pre-training Fine-Tuning

Figure 2.14: Sequential transfer learning using BERT. (left) Pre-training stage
using transformer, (right) Fine-tuning stage where the same pre-trained model
parameters are used to initialize the BERT model for various NLP tasks. During
fine-tuning, all parameters are updated. By excluding the output layers, the
same architectures are used in both stages of sequential transfer leaning through
BERT. (Devlin et al., 2019).

(Figure 2.14). It uses a sentence separator ([SEP]) and classifier token ([CLS]),
in which their embeddings are learned during pre-training. For GPT, in contrast,
these tokens are only introduced at fine-tuning time (Devlin et al., 2019). At
the output level, as shown in Figure 2.15, for token level tasks such as sequence
labeling and question answering, the token representations are fed into the final
layer. For classification tasks (e.g., entailment and sentiment analysis), on the
other hand, the representation of the classifier token ([CLS]) is fed into the
output layer.

2.6 Information Extraction

Information extraction (IE) is the process of extracting desired knowledge in
terms of names, entities, events, properties, and relations from a semi-structured
or unstructured text by transforming them into a structured format. The
structure is usually represented in the form of <subject, predicate, object> or
< entity,, relation, entity2 > triplets, known as facts. In this process, first, we
have to define what constitutes a subject and object, then which type of relations
should be considered. There are two paradigms of information extraction that
have emerged recently (Nakashole, 2012):

e Schema-based IE: In this approach, the process of extracting information
from various information sources is guided by an ontology. The ontology
typically consists of two kinds of information views; those that make

27


2. Background

iu ce C

ot See

Sentence 1 Sentence 2 Single Sentence
(a) Sentence Pair Classification Tasks: (b) Single Sentence Classification Tasks:
MNLI, QQP, QNLI, STS-B, MRPC, SST-2, CoLA
RTE, SWAG
Start/End Span

BERT

Question Paragraph Single Sentence
(c) Question Answering Tasks: (d) Single Sentence Tagging Tasks:
SQuAD v1.1 CoNLL-2003 NER

Figure 2.15: Fine-tuning in BERT for various NLP tasks (Devlin et al., 2019).

28

up the assertion-view and those that make up the taxonomy-view. The
taxonomy-view represents a topology or taxonomy of the domain at hand
and includes the definition of the concepts, attributes, and their inter-
relationships. The assertion-view describes the attributes of instances
(or individuals), the roles between instances, and other assertions about
instances regarding their concept membership within the taxonomy-view.
The concepts and relations in the ontology are generally hand-specified
by either developers of the ontology or by domain experts. Therefore
the major flaw in schema-based IE is the limited number of classes and
relations that can be populated from sentences.

Schema-free IE: This approach to IE aims to extract assertions from a
large volume of textual data, avoiding the restriction to a pre-specified


Information Extraction

vocabulary. It extracts all relations by learning a set of lexico-syntactic
patterns in a supervised or unsupervised manner. While the schema-free
IE answers the recall issue, it is highly susceptible to noise, due to the lack
of tightly enforced semantics on relations and entities (Nakashole, 2012).

Traditionally, the process of information extraction can be divided into a series
of tasks. It typically begins with lexical analysis like assigning part-of-speech and
features to words and phrases through morphological analysis and dictionary.
Then it continues by entity recognition to identify names and instances of
particular concepts of interest. The syntactic analysis comes along in most
solutions to identify the noun, verb phrases, and dependency structure. Finally,
relation extraction and classification tasks are applied to construct the facts of
interests. We can consider discourse analysis as a complementary step in the flow
which resolves relations of co-reference and draws inferences from the document’s
explicitly stated facts. Traditional IE systems are often based on a pipeline
architecture where the tasks have been done sequentially using different task-
specific patterns. The patterns are obtained using various statistical analysis
and pattern recognition methods. Currently, most IE systems employ end-
to-end neural networks by learning deep neural networks that map directly
from the input to the output data naturally consumed and produced in IE
tasks (e.g., Xu et al. (2015a), Gupta et al. (2016), Zheng et al. (2017), Zeng
et al. (2018) and Cui et al. (2018)). Even though most IE neural systems work
end-to-end, there has been interest in incorporating various linguistic categories
(PoS-tags, dependencies) into them to improve the performance, such as Chiu
and Nichols (2016), Xu et al. (2015b) and Nooralahzadeh et al. (2018).
Information Extraction has been explored under different areas such as Fact
& Relation extraction, Knowledge base population and Unseen entity extraction.

e Fact & Relation extraction. This process generates structured data in
the form of entity-relation triplets from natural language text documents.
The conventional approach to fact extraction is to use pattern-based
extraction and employ consistency constraint reasoning to provide proper
facts. In schema-based IE approaches such as BOA (Gerber and Ngonga
Ngomo, 2011), PORSPERA (Nakashole et al., 2011) , DARE (Xu et
al., 2010), the patterns emerge by starting with a few seed facts from a
knowledge base to bootstrap the extraction process. For example, BOA
relies on distant supervision and existing facts from a knowledge base,
in particular DBpedia. It applies a recursive procedure, starting with
extracting triples from linked data, then extracting natural language
patterns from sentences and constructing the facts in triplet format (i-e.,
RDF triples). In schema-free IE approaches (e.g., TextRunner (Yates et
al., 2007), ReVerb (Fader et al., 2011; Mausam et al., 2012)) these patterns
are constructed by leveraging linguistic structure, through syntactic and
lexical rules. For instance, ReVerb extracts relations based on simple
linguistic patterns (i.e., in terms of PoS-tags and noun phrase chunks). It
extracts the facts by assuming that every relational phrase must be either

29


2. Background

a verb and a verb followed immediately by a preposition (e.g., located in),
or a verb followed by nouns, adjectives, or adverbs ending in a preposition
(e.g., has an atomic weight of) (Fader et al., 2011). ReVerb first looks for a
matching relational phrase and then finds the arguments (i.e., entity; and
entity) of the relationship. Although these systems have been widely used
in a variety of fact and relation extraction approaches, most of them were
built on hand-crafted patterns from syntactic parsing, which causes errors
in propagation and compounding at each stage. To alleviate extraction
errors, various deep neural-based approaches have recently been proposed
(Stanovsky et al., 2018; Cui et al., 2018; Jiang et al., 2019; Zhang et
al., 2017). For instance, Cui et al. (2018) applied a seq2seq framework to
provide a schema-free IE system.

e Knowledge Base Population (KBP). Knowledge Base Population
(KBP) is the task of taking an incomplete knowledge base, and a large
corpus of text, and completing the incomplete elements of the knowledge
base. That is, the model has to interpret the text and get the desired
information out of it. Therefore in this task, we assume that we have prior
but incomplete knowledge about the subject, and our aim is discovering its
properties. Recent works like Lin et al. (2015) and Socher et al. (2013a),
exploit knowledge embeddings to infer new relational facts.

e Unseen entity extraction. The previous areas rely on a common
assumption in schema-based IE, namely the existence of a knowledge base
that contains all entities and their types. However, during the extraction of
facts from highly dynamic sources such as news, social media, and technical
documents, new entities emerge that are not in the reference knowledge
base. This area covers the problem of out-of knowledge base entities.
Conventional named entity recognition tools have coarse-grained types and
only deal with a limited set of entities such as a person, organization, and
company. However, fine-grained methods (Ma et al., 2016; Mai et al., 2018;
Dogan et al., 2019) consider up to 200 types. In contrast, the proposed
tools in this area, like PEARL (Nakashole et al., 2013) and FINET (Del
Corro et al., 2015), deal with thousands of types. They are semi-supervised
systems that leverage a repository of many relational patterns. Subjects
and objects of each pattern carry the type information. They categorize
entity mentions by the most likely type according to the pattern repository.
The type system is based on a partial or the entire WordNet (Miller, 1995)
hierarchy.

In this thesis, we study two essential tasks in the area of Information Extraction,
namely Named Entity Recognition (Chapter 4) and Relation Extraction (Chapter

30


Natural Language Understanding

2.7 Natural Language Understanding

Understanding of natural language is an essential and general goal of NLP.
Natural Language Understanding (NLU) comprises a wide range of diverse tasks,
including, but not limited to, natural language inference, question answering,
sentiment analysis, semantic similarity assessment, and document classification.
In this thesis, we explore two central NLU tasks, including natural language
inference and question answering (Chapter 6). We provide a brief description
of these tasks in the following sections but go into details on related work in
Chapter 6.

2.7.1 Natural Language Inference (NLI)

NLI is the task of predicting whether a hypothesis sentence is true (entailment),
false (contradiction), or undetermined (neutral) given a premise sentence. NLI
systems need some semantic understanding and models trained on entailment
data can be applied to many other NLP tasks such as text summarization,
paraphrase detection, and machine translation. The task of NLI, also known as
textual entailment, is well-positioned to serve as a benchmark task for research
on NLU (Williams et al., 2018).

2.7.2 Question Answering (QA)

The task of QA is often designed in the context of a reading comprehension task.
This machine reading problem is formulated as extractive question answering,
in which the answer is drawn from the original text (Eisenstein, 2019). In this
context, given a context and a question, the QA task aims to identify the span
answering the question in the context.

31




Chapter 3

Evaluation of Domain-specific
Word Embeddings

In this chapter, we study input representations trained on data from a low
resource domain (Oil and Gas) using sequential transfer learning of word
embeddings (See Section 2.5 in Chapter 2). We conduct intrinsic and extrinsic
evaluations of both general and domain-specific embeddings. We observe
that constructing domain-specific word embeddings is worthwhile even with
a considerably smaller corpus size. Although the intrinsic evaluation shows low
performance in synonymy detection, an in-depth error analysis reveals the ability
of these models to discover additional semantic relations such as hyponymy,
co-hyponymy, and relatedness in the target domain. Extrinsic evaluation of
the embedding models is provided by a domain-specific sentence classification
task, which we solve using a convolutional neural network. We further adapt
embedding enhancement methods to provide vector representations for infrequent
and unseen terms. Experiments show that the adapted technique can provide
improvements both in intrinsic and extrinsic evaluation.

3.1. Introduction

Domain-specific, technical vocabulary presents a challenge to NLP applications.
Recently, word embedding models (See Section 3.3.2 in Chapter 2) have been
shown to capture a range of semantic relations relevant to the interpretation
of lexical items (Mikolov et al., 2013b) and furthermore provide useful input
representations and transferable knowledge for a range of downstream tasks
(Collobert et al., 2011). The majority of work dealing with intrinsic evaluation
of word embeddings has focused on general domain embeddings and semantic
relations between common and generic terms. However, it has been shown
that embeddings differ from one domain to another due to lexical and semantic
variation (Hamilton et al., 2016; Bollegala et al., 2015). Domain-specific terms
are challenging for general domain embeddings since there are few statistical
clues in the underlying corpora for these items (Bollegala et al., 2015; Pilehvar
and Collier, 2016b). On the other hand, domain knowledge resources, where
the meanings of words are represented by defining the various relationships
among those words, provide valuable prior knowledge for many NLP tools. Many
works show that the encoded knowledge available in lexical resources can be
exploited to improve the semantic coherence or coverage of existing word vector
representations (Faruqui et al., 2015; Pilehvar and Collier, 2017).

The following research questions related to the domain-specific data and
model are investigated in this chapter:

33


3. Evaluation of Domain-specific Word Embeddings

RQ 3.1. Can word embedding models capture domain-specific semantic relations
even when trained with a considerably smaller corpus size?

RQ 3.2. How can we take advantage of existing domain-specific knowledge
resources to enhance the resulting models?

To answer these research questions, we train domain-specific embeddings
and conduct a comprehensive study including a wide range of evaluation criteria
against terminological resources, contrasting several general and domain-specific
embedding models. We augment the domain-specific embeddings using a domain
knowledge resource. We further adapt embedding enhancement methods to
provide vector representations for infrequent and unseen terms by investigating
the works of Pilehvar and Collier (2017) and Faruqui et al. (2015). We then go on
to examine the contribution of these models in the performance of a downstream
classification task.

3.2 Related Work

Despite the pervasive use of word embeddings in language technology, there is
no agreement in the community on the best ways to evaluate these semantic
representations of language’. There exist a variety of benchmarks that are widely
employed to assess the quality of word representations and to compare different
distributional semantic models. Existing evaluation methods can largely be
separated into two categories: intrinsic evaluation and extrinsic evaluation.

3.2.1. Intrinsic Evaluation

Intrinsic evaluation methods attempt to directly quantify how well various
kinds of linguistic regularities can be detected with a model-independent of its
downstream applications (Baroni et al., 2014; Schnabel et al., 2015). Existing
schemes in intrinsic evaluation fall into two major scenarios (Schnabel et al., 2015):
Absolute intrinsic evaluation and Comparative intrinsic evaluation, which will
be described in the following.

Absolute intrinsic evaluation: This type of evaluation directly tests for
syntactic or semantic relationships between words (Schnabel et al., 2015) and
analyzes the generic quality of embeddings (Yaghoobzadeh and Schiitze, 2016).
Since it is computationally inexpensive and leads to fast prototyping and
development of vector models, it has been the topic of many evaluation challenges.
Several datasets have been developed to this end. Table 3.1 shows a compilation
of datasets employed for intrinsic evaluation of word embeddings, organized by
the semantic relation. It involves tasks such as the following (Baroni et al., 2014;
Schnabel et al., 2015):

l1RepEval @ACL 2016, 2017, and 2019: Workshop on Evaluating Vector Space
Representations for NLP

34


Related Work

‘SjOSBIVP UWOTYEN[VAD IISULIJUL oINJOSGY :T°E qe

(9T0Z) ‘Te 99 Aoyoasy, Sain yes} Gy UUM SpIoOM. 1 80 XMPEIN-NAS VOO-OGAO
(ST0Z) ‘Te 3 AOPoAST, Somnjzeoy Tf UM Spl1om VM Gy xT} I-NAS
(910Z) TSIAeN pte sopeTog-oypeuren Sp10M [+8 JO 79 888 WOTOEIOp TONG 10 ou1YO)
(G10Z) ‘Te 3 Peqeuyos SpIoM [+E JO OOT UOISNLIYUT
(910Z) ‘Te Jo BAoYprpy yy suorsonb ASoTeuR 766 SLVa
(eET0Z) “Te 3 AOTOLN suorsonb ASopeue yy 6 WHSNV
° Asopeuy
(eET0Z) “Te 3 AOTONLN suorysonb ASopeue yf GOT NASNV
(VET0Z) ‘Te 3 AOTOYTAL suorjsonb ASoreue sy, GGT NV
(866T) ‘Te 3 ORY sured q1oA-unoU (OT aVeDIN JouOIOJOIg [RMOT0T9S
(2002) 9ped sured q1oA-UNOU [TZ dn
(OL0Z) UST pue Tuoreg sol108eye9 QT ‘s}doou09 Eg ONLLLVA covmmeetenery
(800Z) ‘Te jo Tuoreg sol10807e9 g ‘s}do.u09 Py ITISSH =
(900Z) qoreynUy sol108eye0 Tz ‘s}daou0d ZOP dv
(2661) steujng pue JonepueyT (spioM f) suoTysonb odfoyp-1y[NUT 0g THAOL uoTpojoq wWAUOUAG
(st0Z) ‘Te 9° HH sired prom 666 666-X0TUHIS
(FI0Z) ‘Te Jo Toyeg sired p10M PPT qioA,
(€10Z) ‘Te yo Suon'T sired p10oMm PE0z PIOMA TRY
(ZLOZ) ‘Te 4 TMI sued prom 00E NaI
(ZL0Z) ‘Te yo IweTey sired prom TL) TLL LI
(TL0Z) ‘Te 4 Aysurpey sired prom 1.8z 186A LN 58%
(600Z) ‘Te 39 os sired pom €0Z wWIS-SM eee es
(6002) ‘Te 99 OLS syed pros 7G PuU-SM
(900Z) Stomog pur Suez sired pIOM (ET O€T-dA
(1002) ‘Te 4 Wloyspoy UL syed prom eG¢ EGE-WISPION,
(1661) sePteyD pure JOT sired p10a 0¢ 0€-DIN
(G96T) YSnouspooy pur uleysuoqny sired prom Gg OY

CRIBS |

‘JUL yoseyeq

oureu yosezeq

4SeL,

35


3. Evaluation of Domain-specific Word Embeddings

36

Semantic Relatedness: Given a ground truth of human assigned
proximity scores to word pairs such as money-dollar ® 8.42, tiger:mammal
= 6.85, the evaluation task aims to find the degree of correlation between
the scores provided by the model and the human rating as a performance
of the model. The cosine similarity of the corresponding vectors for word
pairs provided by the model should be highly correlated with the gold
standard (measured by Spearman or Pearson correlation).

Categorization: Given a set of words, the system needs to group them
into different semantic categories (e.g., helicopters and motorcycles should
go to the vehicle class, dogs and elephants into the mammal class). By
applying a clustering method to the corresponding vectors of all words in
a dataset, the model’s performance is calculated concerning the purity of
the outcome clusters concerning the human-judgment labels.

Synonym Detection: The ability of the embedding model to find the
correct synonym for a word is assessed. For example, for the target word
levied the list of options to choose are imposed (correct one), believed,
requested and correlated. For each target word, its cosine similarities with
synonym candidates are calculated, and the one with the highest score is
selected. The model performance is the accuracy of the model prediction.

Selectional Preference: The goal is to label a noun as a subject or
object for a specific verb (e.g., people received a high average score as a
subject of to eat, and a low score as an object of the same verb). Baroni
et al. (2014) and Schnabel et al. (2015) followed the procedure of Baroni
and Lenci (2010) to perform this task. First, for each verb in the dataset,
the 20 nouns which are most strongly associated as subject or object are
selected, then a prototype vector is calculated as the average of these nouns
is calculated. Therefore we will have a subject and object type prototype
vector for each verb. The performance of the model will be the correlation
degree (spearman) of the averaged human ratings for each type and the
cosine scores between the target nouns vectors and the relevant prototype
vectors of the verbs.

Analogy: The analogy task asks the model to detect whether two pairs
of words stand in the same relation. These relations fall into different
types of linguistic relations, such as morphological and semantic relations.
Having two word pairs A:B::C:D where the D is missing, the goal is to
find the missing word in the relation: A is to B as C is to D, in which
C, D are related by the same relation as A, B . For example, France :
Paris :: Germany : Berlin. Following the procedure proposed by Mikolov
et al. (2013a), the first term vector in the first pair is subtracted from the
second term vector, then the test term is added to the result (B-A+C).
Afterward, the nearest neighbor to the final vector is requested from the
model. The performance of the model is measured as the proportion of
the questions where the nearest neighbor suggested by the model is the
correct answer (accuracy).


Related Work

e Coherence / Outlier Detection: The goal is to assess whether the
neighbor words in the embedding semantic space are mutually related.
Therefore a good model should provide coherent neighborhoods for a
target word. To tackle this task, groups of coherent words and intruder
words are introduced, and the model should be able to spot the word
that is an outlier and does not belong to the group of neighbor words.
For example, among the following words: (a) finally (b) eventually (c)
immediately (d) put, the query word is option (a), intruder is (d). Schnabel
et al. (2015) presented this intrinsic evaluation as an intrusion task and
evaluated the performance of the models by the precision metric. On
the other hand, in Camacho-Collados and Navigli (2016), the task was
introduced as outlier detection and solved as a clustering problem, in which
each group of coherent words are clustered based on a compactness score
and the intruder words are ranked by their positions (8 outlier positions:
the 1st position has the lowest dissimilarity to the cluster and the 8th
position has the highest dissimilarity). The model quality is measured by
outlier position percentage and accuracy of the outlier detection.

e QVEC and QVEC-CCA : The basic hypothesis of QVEC is that
dimensions in distributional vectors encode the linguistic features of words.
It measures the quality of a model by how well the embedding correlates
with a matrix of features from manually crafted lexical resources. For
example, the target word fish is assigned to the following senses along
with scores: animal: 0.684, food: 0.157, competition: 0.0526, contact:
0.105. Tsvetkov et al. (2015) introduced QVEC as a measure to quantify
the linguistic regularities of an embedding model. For target words that
are in the embedding model, it obtains an alignment between the word
vector dimensions and the linguistic dimension in which it maximizes the
correlation (Pearson correlation) between the aligned dimensions of the two
matrices. The higher the correlation, the more salient the linguistic feature
of the dimension. The QVEC-CCA (Tsvetkov et al., 2016) followed the same
idea as QVEC. However, to measure the correlation between the embedding
matrix and the linguistic matrix, it employs canonical correlation analysis
(CCA (Hardoon et al., 2004)). CCA generates two basic vectors for the
embedding and feature metrics such that the projections of these two
metrics onto their basic vectors have a maximum correlation.

Comparative intrinsic evaluation: This type of evaluation is based on direct
feedback from the user on the model outcome using a crowd-sourcing environment
(e.g., Amazon Mechanical Turk). For each target word, each embedding model
is questioned to provide the nearest neighbors at ranks k € {1,5,50}. Then
the human annotators select the most similar answer, and the model that has
the majority votes is considered to be the winner. The dataset is called Query
Words and includes 100 queries (Schnabel et al., 2015).

37


3. Evaluation of Domain-specific Word Embeddings

Task Data Set Dataset info. Evaluation
(Train/Dev/Test) Ref.
POS Tagging Penn Treebank 958K, 34K, 58K Ghannay et al. (2016)
(Marcus et al., 1993) Chiu et al. (2016b)
Nayak et al. (2016)
Chunking CoNLL 2000 191K, 21K, 47K — Schnabel et al. (2015)
(Tjong Kim Sang and Buchholz, 2000) Chiu et al. (2016b)
Ghannay et al. (2016)
Nayak et al. (2016)
Named CoNLL2003 205K, 52K, 47K Ghannay et al. (2016)
Entity Recognition (Tjong Kim Sang and De Meulder, 2003) Chiu et al. (2016b)
Nayak et al. (2016)
Sentiment Analysis Stanford Sentiment Treebank (Socher et al., 2013b) 8.5K, 1.1K, 2.2K Nayak et al. (2016)
Movie Reviews (aclimdb) (Maas et al., 2011) 25K, -, 25K Schnabel et al. (2015)
Question Classifica- TREC (Li and Roth, 2002) 15.5k, -, 500 Nayak et al. (2016)
tion
Natural Language PPDB:Eng (Ganitkevitch et al., 2013a) 221.4M Nayak et al. (2016)
Inference

Table 3.2: Extrinsic evaluation tasks and datasets.

3.2.2 Extrinsic Evaluation

Given the widespread use of word embeddings as input representations in neural
NLP systems, the quality of a word vector may also be assessed by performance
in downstream tasks. This is done by measuring changes in performance metrics
specific to the tasks by extrinsic evaluation. The downstream language technology
tasks on which the quality of a word embedding has been examined, fall into
syntactic (e.g., POS tagging, Chunking) and semantic (e.g., Named Entity
Recognition, Sentiment Analysis) categories. However, by the definition of
extrinsic evaluation, any downstream task could be considered as an evaluation
method. Various downstream tasks and the related resources that are commonly
used in the extrinsic evaluations of word embeddings are as follows (Table 3.2):

Part-Of-Speech (POS) Tagging ‘To identify the morpho-syntactic label of
each word in the sentences. The evaluations are performed on the standard Penn
treebank dataset (Marcus et al., 1993), using the neural method suggested by
Collobert et al. (2011) .

Chunking The chunking is a syntactic sequence labeling task where the goal
is to locate phrases in the text. The pre-trained embedding models are used as
an input for a noun phrase chunking task similar to those employed by Turian
et al. (2010) and Collobert et al. (2011) using the dataset of CONLL-2000 shared
task (Tjong Kim Sang and Buchholz, 2000).

Named Entity Recognition NER systems perform the task of detecting named
entities in text (e.g., persons, locations and organization) as a sequence prediction

38


Intrinsic Evaluation Setup

task. The system proposed by Collobert et al. (2011) is evaluated on the
CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003) using different
embedding models.

Sentiment Analysis The task of sentiment analysis aims to classify the polarity
of a given text as positive, negative, or neutral. Several datasets like user reviews
(Maas et al., 2011) and Stanford Sentiment Treebank (Socher et al., 2013b) are
used for downstream evaluation of this task.

Question Classification Question classification refers to the task of mapping
a given question to one of k classes such as Person, Quantity, Duration, Location,
and so on. For example, given the question “Who was the first woman killed in
the Vietnam War?”, we would like to know that the target of this question is
a person. A hierarchical classifier is used to classify questions into fine-grained
classes described in Li and Roth (2002).

Natural Language Inference (NLI) The NLI task is used to examine the
ability of embedding models in propagating lexical relations (Nayak et al., 2016).
NLI aims to infer the logical relationship, typically entailment or contradiction,
between the given hypothesis and premise sentences. The task is performed using
the dataset presented in Ganitkevitch et al. (2013a).

In the following sections, we will describe how we evaluate domain-specific
word embedding models using both intrinsic and extrinsic evaluation schemes.

3.3 Intrinsic Evaluation Setup

Intrinsic evaluation of word embeddings has two main requirements. First, we
require a query inventory as a gold standard, and second, a word embedding
model that has been trained on a specific corpus. In this section, we describe
how we build a domain-specific query inventory for the Oil and Gas domain
by exploiting a domain-specific knowledge resource. Then, the domain-specific
corpus and the training of the embedding models will be described. We then go
on to clarify the evaluation methodology.

3.3.1 Domain-specific query inventory

As an intrinsic evaluation, we would like to assess the quality of representations
independently of a specific NLP task. Currently, this type of evaluation is
mostly done by testing the overall distance/similarity of words in the embedding
space, i.e., it is based on viewing word representations as points and then
computing full-space similarity. The assumption is that the high dimensional
space is smooth and similar words are close to each other (Yaghoobzadeh
and Schiitze, 2016). Computational models that could capture similarity as
distinct from the association have many applications in language technology
such as ontology and dictionary creation, language correction tools, and machine

39


3. Evaluation of Domain-specific Word Embeddings

caprock English | Espafiol

1. n. [Geology]

A relatively impermeable rock, commonly shale, anhydrite or salt, that forms a
barrier or seal above and around reservoir rock so that fluids cannot migrate
beyond the reservoir. It is often found atop a salt dome. The permeability of a
caprock capable of retaining fluids through geologic time is ~ 10-8-10°%
darcies.

Synonyms: seal
Alternate Form: cap rock

See: halite, migration, petroleum system Q | View image

Schematic diagram of cap rock over
reservoir

1of1

Figure 3.1: Term structure in the slb glossary.

translation. As described in detail above, for the general domain, there exists
a wide range of gold standard resources for evaluating distributional semantic
models in their ability to capture semantic relations of different types, for
instance, Simlex-999 (Hill et al., 2015). WordSim-353 (Agirre et al., 2009) and
MEN (Bruni et al., 2012) (See Table 3.1).

However, evaluating domain-specific embeddings by applying these gold
standards will not provide an adequate picture of their quality since they do
not share a common vocabulary and word meanings. The domain of oil and
gas has received little previous work in NLP and there are no readily available
resources. For this reason, we create a domain-specific gold standard using the
Schlumberger oilfield glossary (slb).? The slb is a reference that defines major
oilfield activities and has been created by technical experts.

Figure 3.1 shows the structure of the term caprock in this glossary. Terms
are described by their part of speech, their discipline (e.g., Geology, Well
Completions), as well as a textual definition. Terms are linked to other terms in
the glossary using semantic and lexical relations such as Synonyms, Antonyms,
and Alternate forms. It provides a network of related terms that can be navigated
through the glossary. In our example entry for instance, we find that the
synonyms and alternate forms of caprock are seel and cap rock, respectively. (see
Figure 3.1). Finally, if the term has an image that clarifies its definition, it will
appear in the image section next to the definition part. All these elements in the

?http:/www.glossary.oilfield.slb.com/

40


Intrinsic Evaluation Setup

n-gram He Noun Verb Adj. Adv. Pre-position

unigram 1499 1261 98 189 1 2
bigram 2569 2505 35 36 1 1
trigram 660 644 13 4 0 0
>3 158 149 3 6 0 0
All 4886 4559 138 246 2 3

Table 3.3: N-grams & Part of speech tags in the slb glossary.

term’s structure are located in the following tables inside a relational database:

definitions: All the main terms of the glossary are located in this table. They
are defined by id, name, definition, term_type (verb, noun, adjective,
adverb, preposition and transitive verb) as part of speech tags, postdate,
and lastupdate. It is possible that one term has more than one definition if
it has different part of speech tags, or if it is assigned to different disciplines
or both. For example, the term dry gas has been assigned to the Geology
and Well Completions disciplines and has different definitions in each
discipline.

disciplines: There are 20 main categories that describe disciplines in the
glossary (e.g., Drilling, Geology, Geophysics and Well Completion). Each
term in the definitions table is assigned to one or more disciplines and its
definition varies based on the assigned discipline.

images: To illustrate and clarify many definitions, high-quality and full-color
photographs are assigned to the definition by image_name, image_caption
and image_url.

links: The inter-glossary relations among the terms in the glossary are specified
in this table. These relations are characterized with type taken from
the following set: Synonym, Antonym, Alternative form and See. These
relations form the basis of our intrinsic evaluation dataset.

We construct a domain query inventory by extracting all terms and their inter-
glossary relations from the relational database. The terms are converted to
lowercase and assigned n-gram type (i.e., a contiguous sequence of a word,
unigram where n=1, bigram where n=2, trigram where n=3 and >3 where n>3).
The glossary consists of 4,886 terms. Table 3.3 shows the distributions of terms
in the glossary concerning their n-gram type and part of speech tags (one term
may be assigned to more than one tag).

Following the symmetric nature of the Synonym, Antonym, and Alternative
form relations, we infer a relationship if it is missing between terms. The
final query inventory contains 878 synonym pairs, 28/4 antonym pairs and 934

41


3. Evaluation of Domain-specific Word Embeddings

Source Abbr. Description Docs’ Sentences
American Association of Petroleum Geologist AAPG Scientific articles 3,382 72,243
C&C Reservoirs-Digital Analogs CCR Field evaluation reports 1,140 244,017
Elsevier ELS Scientific articles, magazines 40,757 7,703,447
Geological Society, London Memoirs GSL Scientific articles 152 32,352
Norwegian Petroleum Directory NPD Norwegian Field info 514 49,426
Tellus TELLUS Basin info 1,478 179,450
Total 47,423 8,280,935

Table 3.4: Sources of the Oil and Gas corpus.

Figure 3.2: Distribution of the sources in the domain corpora.

alternative form pairs. We observe that the majority of terms in the query
inventory are multi-word units (70%) and nouns (72%). This indicates that a
large portion of the domain-specific vocabulary that we want to capture in our
model consists of multi-word entities. Thus we need to take this into account
during the training of embeddings.

3.3.2 Training of Word Embeddings

In order to train domain-specific embeddings, we need a domain-specific corpus.
Therefore, we compile a corpus consisting of technical reports and scientific
articles in the Oil and Gas domain °. Table 3.4 shows detailed information about
these sources. As we can see, the corpus covers several different genres with a
majority taken from the genre of scientific articles. The corpus contains 47, 423
documents and 8, 280,935 sentences.

As can be seen from Table 3.4, the corpus contains different types of
documents from various sources. Figure 3.2 depicts the distribution of sources
in the domain corpora. We observe that most of the documents belong to
Elsevier, and the other sources cover a small proportion of the documents. The

3SIRIUS partners provided the sources for the corpus.

42


Intrinsic Evaluation Setup

domain-specific dataset is preprocessed using the following steps:

1. Tokenization and lemmatization using Stanford-CoreNLP (Manning et
al., 2014). English stop words and sentences with less than three words
are removed from the corpus.

2. Shuffling: we randomly shuffle the text in the dataset. During the training
of embedding models, the learning rate is linearly dropped as training
progresses, text appearing early has a more significant effect on the model.
Shuffling makes the effect of all text almost equivalent (Chiu et al., 2016a).

For training of the word embeddings, we exploit the available word2vec (see
Section 3.3.2 in Chapter 2) implementation gensim (Rehtiek and Sojka, 2010).
The elements that impact the performance of the model are the input corpus,
model architecture, and the hyper-parameters. In many articles (Fares et
al., 2017; Camacho-Collados and Pilehvar, 2018) lemmatizing, case-folding, and
shuffling input during training the word2vec are recommended; we carried out
our experiments with these settings as detailed above.

We employ the phrase model of gensim, which automatically detects common
phrases (multi-word expressions). The phrases are collocations (frequently co-
occurring tokens), and we consider bigrams and trigrams in this extraction
process. The phrase model has two main parameters:

1. min.count: During training, all words and phrases with total count lower
than this number are ignored.

2. threshold: Represents a threshold for forming the phrases (higher means
fewer phrases).

A phrase of words a and 0 is extracted if :

(count(a,b) — min.count) * N

hreshol wl
count(a) * count(b) saree et)

Here N is the total vocabulary size and count(a, 6) is the total number of times
where word a and b co-occur as a multi-word expression in the corpus. We set
the min.count equal to 5 and the threshold equal to 200 and 100 for bigrams
and trigrams, respectively (these values were determined empirically). We
further proceed with the domain-specific model generation by creating two sets
of embeddings, employing both the CBOW and the Skip-gram architectures
(see Section 3.3.2 in Chapter 2) with default settings. In the initial evaluation
step, we compare the outcomes of these two models to determine the better
architecture.

Embedding models consist of several parameters that can be tuned. We now
go on to compare different settings for the hyperparameters, while keeping all
other settings constant. It has been claimed in previous works that optimizations
of hyper parameters and certain system choices constitute the leading causes

43


3. Evaluation of Domain-specific Word Embeddings

of differences in performance rather than the algorithms themselves (Levy et
al., 2015). Here we investigate the impact of various system design choices in
the evaluation of domain-specific embeddings across the following parameters 4:

e Vector size (dim): The dimensionality of the learned dense vector is deter-
mined by the vector size parameter. (dim € 50, 100, 200, 300, 400, 500, 600)

e Context window size (win): The range of words included in the context of
a target word is determined by the window size parameter. For instance, a
size of 3, takes three words before and after a target word and injects into
the training model as context words. (win € 2,3,5, 10, 15, 20).

e Negative sampling size (neg): The idea of word2vec is to maximize the
similarity between the word vectors, which appear close together (in the
context of each other), and minimize the similarity of words that do not.
However, this process includes an expensive computation to calculate the
similarity between the target word and all other context words in the corpus.
Negative sampling is one of the ways of addressing this problem, by simply
selecting select a couple of contexts at random and calculating the similarity
of target word to randomly chosen negative words. (neg € 3,5, 10,15) (See
section 3.3.2 in Chapter 2).

e Frequency cut off (min.count): Words with a total frequency lower than
the min.count will be ignored from the corpus. This results in fewer words
in the vocabulary of the model. (min.count € 2,3,5, 10).

e n-most-similar: The parameter n for top n-most-similar as output is fixed
at the value 5 (the maximum number of terms that are involved in each
relation set in the query inventory).

We evaluate these different system design settings based on our intrinsic
benchmark. We train different embedding models by varying values of one hyper-
parameter and keeping others as default. After that, we perform evaluations
over the domain-specific query inventory.

3.3.3 Evaluation measures

For evaluation, we assume that for each term in the inventory, an embedding
model should be able to propose similar words which are related semantically as
either synonym, alternative form or antonym. We will measure this by looking
at a target word and its relation set in the query inventory, for instance, its
synonyms and top n-most similar words predicted by the embeddings model. If
the synonym of the target word is in the n-most-similar words, we will count it
as a true positive. Otherwise, the target word and its synonym will be considered
as a false negative, and the target word and the predicted word by the model
will be counted as a false positive. Since these relations are symmetric, the pairs
(t;,t;) and (t;,t;) are considered equivalent in the evaluation. We calculate the

4Default values are in bold.

44


Intrinsic Evaluation Experiments

Model Synonymy | Antonymy | Alt. form
A R P|A R P|A R P

Skip-gram 98 80 2.2 | 464 41.3 93 | 121 104 24
CBOW 12.7 10.2 2.7 | 55.3 49.2 11.1] 12.8 11.0 2.6

Table 3.5: Evaluation results for different architectures.

accuracy (A) as the number of target words for which the model provides at least
one correct prediction, the recall (R) as the number of correctly predicted word
pairs over all word pairs (i.e., the sum of true positives and false negatives) and
precision (P) as the number of correctly predicted word pairs over all predicted
word pairs (i.e., the sum of true positives and false positives) for each relation
category.

3.4 Intrinsic Evaluation Experiments

In the following, we present experiments that evaluate the domain-specific word
embedding models intrinsically. We first present tuning experiments and then
present an experimental comparison between domain-specific and general domain
embedding models.

3.4.1. Model architecture: Skip-gram vs. CBOW

First, we compare the models obtained using the different word2vec architectures
(CBOW and Skip-gram) with default values for hyper-parameters i.e. dim = 100,
win = 5, min.count = 5 and neg = 5. Table 3.5 presents the results for the
two architectures broken down by semantic relation from the query inventory.
The results show that the embedding models have higher scores for antonymy
prediction than synonymy, see Table 3.5. This result is consistent with previous
studies such as Plas and Tiedemann (2006) and Leeuwenberg et al. (2016) in
which they reported that using distributional similarity some word categories
like antonyms, (co)hyponyms or hypernyms show up more often than synonyms.
In general, we find that the CBOW based model shows better results than the
Skip-gram in all semantic relation tasks.

3.4.2 Hyper-parameter tuning

We go on to explore the impact of each hyper-parameter on the detection of
semantic relations. We observe that the performance of the embedding models
can be notably improved over the default hyper-parameters, but like the findings
in other studies (Gladkova et al., 2016; Chiu et al., 2016a), the effects of different
configurations are diverse and sometimes they are contradictory. For example,

45


3. Evaluation of Domain-specific Word Embeddings

dim Synonymy Antonymy Alt. form
A R P A R P A R P
50 12.7 10.2 2.7 | 48.2 429 96 ) 114 98 2.8

100 12.7 10.2 2.7 | 55.4 49.2 11.1) 12.9 110 2.6
200 14.7 12.4 33 | 554 49.2 11.1] 143 123 2.9
300 15.7 13.1 3.5 | 55.4 49.2 11.1] 136 11.7 2.7
400 15.7 13.1 3.5 | 57.1 50.8 11.4) 13.6 11.7 2.7
500 14.7 12.4 3.3 | 53.6 47.6 10.7] 15.0 12.9 3.0
600 14.7 12.4 3.3 | 51.8 46.0 104/129 11.0 2.6
700 14.7) 12.4 3.3 | 53.6 47.6 10.7] 13.6 11.7 2.7

Table 3.6: Evaluation results for different vector size (default=100).

win Synonymy Antonymy Alt. form

A R P A R P A R P
2 12.7 10.2 2.7 | 55.4 49.2 11.1] 13.6 12.3 2.9
3 13.7 12.4 3.3 | 48.2 429 96 |) 114 98 2.8
5 12.7. 10.2 2.7 | 55.4 49.2 11.1) 12.9 11.0 2.6
10 13.1 10.9 2.9 | 53.6 47.6 10.7] 13.6 12.3 2.9
15 12.7 10.2 2.7 | 67.1 50.8 11.4] 12.9 11.0 2.6
20 12.7 10.2 2.7 | 53.6 47.6 10.7) 12.1 10.4 2.4

Table 3.7: Evaluation results for different context window size (default=5).

different relation categories benefit from different context window sizes, and we
find that the model with larger context windows tends to capture the antonymy
relation while a model with smaller windows, better captures synonymy relation
of the words. We also observe that negative sampling and frequency cut-off
parameters have different impacts on the three relation categories.

Vector size (dim) The effect of vector size on the trained models is quite similar
in all tasks (Table 3.6). We observe a large improvement in all evaluations when
the dimensionality is increased. However, the improvement peaks at 400 for the
synonymy and antonymy predictions and 500 for alternative form.

Context window size (win) Table 3.7 depicts the impact of window size per
evaluation task. We find that the embedding model can benefit from low window
size (w=3) for the synonymy task while in antonymy and alternative form tasks
the model performance fluctuates between lower and higher window sizes.

46


Intrinsic Evaluation Experiments

neg Synonymy Antonymy Alt. form

A R P A R P A R P
3 12.7 10.2 2.7 | 53.6 47.6 10.7] 12.1 104 2.4
5 12.7 10.2 2.7 | 55.4 49.2 11.1] 12.9 11.0 26
10 12.7. 10.2 2.7 | 55.4 49.2 11.1] 13.0 11.7 2.7
15 12.7 10.2 2.7 | 51.8 46.0 10.4 | 13.6 12.3 2.9

Table 3.8: Evaluation results for different number of negative samples
(default=5).

min.count Synonymy Antonymy Alt. form

A R P A R P A R P

2 124 99 2.7 | 544 48.4 10.9) 13.0 11.8 2.7
3 126 10.1 2.7 | 56.1 50.0 11.2} 13.2 12.0 2.8
5 12.7 10.2 2.7 | 554 49.2 11.1) 12.9 11.0 2.6
10 13.1 10.4 2.8 | 54.7 483 109] 13.0 118 2.7

Table 3.9: Evaluation results for different value for frequency cut off (default=5).

Negative sampling (neg) Unlike the practical recommendation in Levy et
al. (2015) who state that the skip-gram model prefers many negative samples, the
CBOW model shows the contradictory result with respect to this parameter in
our evaluation benchmarks. Table 3.8 presents that the results remain constant
regardless of the negative sampling number in the synonym prediction task. On
the other hand, we find that the performance correlates with an increase of this
parameter in alternative form detection. For the antonym task, performance
reaches a peak on neg equal to 5 and 10 before dropping.

Frequency cut off (min.count) The impact of excluding words that are less
frequent in response to variation of the min.count parameter is summarized
in Table 3.9. This parameter shows a different impact compared to the other
parameters. While, ignoring more words has a positive effect in synonymy
detection, improvement halts at min.count = 3 for antonymy and alternative
form relations.

Since the context window size (win), negative sampling (neg) and frequency
cut off (min.count) parameters showed inconsistent results among the relations,
we selected the CBOW model with vector size (dim) equal to 400 and we fixed
the other parameters to their defaults i.e. win = 5, min.count = 5 and neg = 5.
This configuration, hereinafter referred to as OILGAS.d400, showed the overall
best performance during the tuning experiments.

47


3. Evaluation of Domain-specific Word Embeddings

Model Coverage dim
Google News 26% (100B, 3M) 300
Wiki+Giga 23% (6B, 400K) 300
O1LGaAs.d400 31% (108M, 330K) 400
enwiki 29% (1.8B, 2M) 400
enwiki+OILGAs | 31% (1.9B, 2.3M) 400

Table 3.10: General domain and domain-specific embedding models.

3.4.3 Comparative evaluation

In order to compare the domain-specific embeddings with general domain em-
beddings, we select two widely used pre-trained embedding models: Wiki+Giga
© and GoogleNews © to see how they perform in our evaluation benchmark. The
pre-trained models were chosen to have similar settings to our models. The input
data in the Wiki+Giga has been tokenized and lowercased with the Stanford
tokenizer, whereas the GoogleNews model is trained on a part of the Google News
dataset and it contains both words and phrases. The phrases are obtained using
the same approach, as described in Section 3.3.2. The words are not lemmatized
in both models, and GoogleNews also contains capitalized words.

The results of the comparative evaluation of the domain-specific and pre-
trained models are summarized in Table 3.11. Since the words in the vocabularies
of both pre-trained models are not in lemma form, we consider the surface form
of terms for the evaluation. We also report the proportion of query terms that
are covered by the vocabulary of each model as coverage. We find that despite
the large input and vocabulary size in both GoogleNews and Wiki+Giga models,
they have less coverage than the domain-specific model. We further observe that
(see Tables 3.10and 3.11) despite the considerably smaller training data set, the
O1LGAs.d400 performs better across all the tasks.

It is clear that, this comparison is somewhat unfair due to differences in pre-
processing and hyperparameter tuning. Therefore, to investigate the impact of
these differences, we apply the same pre-processing steps and hyperparameters to
train the CBOW model over the English Wikipedia dump (20 September 2016),
here dubbed enwiki. Furthermore, we conduct a similar experiment with a data
set consisting of both the general and domain-specific corpora (enwiki+OILGASs).
However, these approaches do not show further improvements in our evaluation
benchmark, as reported in Table 3.11. Surprisingly, the mixing of Wikipedia and
OILGAS does not increase the coverage rate. It can be attributed to the fact that
the phrase extraction method (Section 3.3.2) is not able to capture the missing
multi-word expressions. In many cases in the mixed corpus (enwiki+OILGAs)

5https://nlp.stanford.edu/projects/glove/
6 https://code.google.com/archive/p/word2vec/

48


Manual Analysis

Model Synonymy Antonymy Alt. form

A R P A R P A R P
Google News 90 7.0 1.8 | 51.2 3870 81 | 41 16 0.4
Wiki+Giga 40 3.2 0.8 | 40.4 43.8 102] 18 3.7 0.8

O1LGas.d400 15.7 13.1 3.5 | 57.1 50.8 11.4) 13.6 11.7 2.7

enwiki 8.2 6.7 1.8 | 39.1 33.3 7.5 | 83 81 1.9
enwiki+OILGAs | 11.1 7.8 2.1 | 55.3 47.7 10.7] 86 89 2.0

Table 3.11: Results from the intrinsic comparative evaluation of general domain
and domain-specific embedding models.

the relative increase in the frequency of tokens individually is higher than the
relative increase of co-occurring tokens (e.g., the relative increase of the word
"source" and the word "rock" in the enwiki#OILGAS is larger than the relative
increase of the word "source rock" compared to the OILGAS corpus).

3.5 Manual Analysis

The results in Section 3.4.3 show that the domain-specific model provides better
results than general domain models for a domain-specific benchmark. However,
we also observe that performance is low for all three tasks, in particular for
the synonymy detection task. In this section, we explore the reasons behind
these low scores and gain insight into the domain-specific model predictions, in
particular the synonymy detection, through an in-depth error analysis.

As noted above, the primary cause of low performance is due to out of
vocabulary (OOV) terms in the query inventory. As shown in Table 3.10 the
model vocabulary contains only 31% of the evaluation dataset. We find that
the majority of terms that participate in synonymy relations are not included in
the word embeddings model, this is in particular the case for multiword items.
The majority of these terms either do not occur or have a frequency lower than
the cut off threshold in the domain dataset. Excluding the OOV terms from
the evaluation tasks has some impact on the model performance for synonymy
detection, recall (R) is 29%, and precision (P) is 6.5%. Still, these scores are
low, we therefore examine the model predictions closer.

We randomly choose 100 terms from the reference inventory, which are also
in the model vocabulary, and we manually categorize their 10-most-similar words
provided in the word embeddings. The manual analysis was performed by two
domain experts as well as the author.

In this section, we are inspired by the work of Leeuwenberg et al. (2016),
where the authors categorized the result of embeddings for a synonym extraction
task in the following categories (The categories with * are added by us).

49


3. Evaluation of Domain-specific Word Embeddings

e Spelling Variant: The prediction is an abbreviation or there are
differences between prediction and target word because of hyphenation.

e Alternative or derived form: The prediction is an alternative or derived
form of the target word.

e Reference-Synonyms: The prediction is a synonym of the target word
in the oilfield glossary.

e Human-judged Synonyms: The prediction is judged as true by the
expert (but is not present in the glossary).

e Antonyms«: The prediction is an antonym of a target term.

e Hypernyms: The prediction is a more general category of the target
term.

e Hyponyms: The prediction is a more specific type of the target term.

e Co-Hyponyms: The prediction and target term share a common
hypernym.

e Holonyms* The prediction denotes a whole whose part is denoted by the
target term.

e Meronyms«: The prediction is a part of the target term.
e Related: The prediction is semantically related to target.

e Unrelated/Unknown: The prediction and target terms are semantically
unrelated.

3.5.1. Annotation tools

For manual analysis, the domain experts are asked to categorize the randomly
selected terms (Section 3.5) and their 10-most-similar words provided by the
domain-specific embedding model. We implement a live web application 7 using
React.js and the Firebase platform. Figures 3.3, 3.4 and 3.5 depict screenshots
of the annotation environment. The annotator can log in to the annotation
environment using the first screen (Figure 3.3). Then, the annotator selects
the discipline that he/she wants to annotate (Figure 3.4). In the annotation
interface (Figure 3.5), we provide the target words according to their discipline
in the Schlumberger oilfield glossary. Each annotation page contains a target
word at the top and the 10-most-similar words provided by the model in the
grid (predicted words). The annotators should consider a target word and a
predicted word as a pair and select one of the target categories. Each category
is denoted by its abbreviation in the page (e.g., "Synonyms" as "SYN"). In
each annotation page, there are 10-word pairs. Using "Next" and "Back", the
annotators can navigate through the pages. In the end, they will find a "Submit"

Thttp://obscure-tor-63439.herokuapp.com/

50


Manual Analysis

Survey App

Guideline
Please read the following instructions to select a category for each word pair in the next slides.

Each slide contains a target word at the top and the 10-most-similar words provided by the model in the grid (predicted words). You should consider
a target word and a predicted word as a pair and select one of the following categories. Each category is denoted by Its abbreviation in the slide. For
example for “Synonyms” you will have "SYN".

In each slide there are 10 word pairs. Using "Next" and "Back" you can navigate if you have chosen a category for all pairs in the slide. At the end you
will find a "Submit" button to send the results.

The categories are describes below. In all the following the examples the first word in a pair is the target word and the second one is the predicted
word.

1. Spelling Variant (SPL): The prediction is an abbreviation or there are differences between prediction and target word because of hyphenation.
Example: commander:cmdr, bioscience: bio-science

2. Alternative or derived form (ALT): inflections or derivations. The prediction is alternative or derived form of the target word. Example: verb.+er (
provide:provider), Comparative degree ( strong:stronger), verb.+ation (continue:continuation), plural (student:students)

3. Synonym (SYN): The prediction is a synonym of the target word. Example: sofa:couch, plan:scheme

4. Antonym (ANT): The prediction is an antonym of a target term. Example: survival:death, cheap:expensive

5. Hypernym (HPR): The prediction is a more general category of the target term. Example: red:color, apple:fruit

6. Hyponym (HPO): The prediction is a more specific type of the target term. Example: color:red, fruit:apple

7. Co-Hyponym (CHP): The prediction and target term share a common hypemym. Example: red:white, apple:pear

8. Holonym (HOL): The prediction denotes a whole whose part is denoted by the target term. Example: player:team, engine:car
9. Meronym (MRN): The prediction Is a part of the target term. Example: team:player, car:engine

10. Related(REL): Prediction and target term are semantically related. Example: goal:football, icecream:spoon

11. Unrelated (UNR): The association between prediction and target term is unknown, they are semantically unrelated

Login

Login with Google account below.

Figure 3.3: Annotation interface (1): login and annotation guideline.

@) Discipline Selection

© All Disciplines

© Drilling and Well

© Geology
Geophysics

© IOR

How many years experience do you have in this area?

00 1-40 4-80 8+

Figure 3.4: Annotation interface (2): disciplines
and years of experience.

51


3. Evaluation of Domain-specific Word Embeddings

ANT: Antonyms

The prediction is an antonym of a target term.

Example: survival:death, cheap expensive

SPL ALT SYN ANT HPR HPO CHP

tluvio-dettaic

delta

tan-detta

fluvia-dettaic

fluviodeltaic

tluviat-dominated delta

fandelta

tan-dettaic

delta-plain

parahe

MAN

REL

UNR

21

22

Figure 3.5: Annotation interface (3): target and prediction word pairs and relations.

23

24

Logout

52


Manual Analysis

Blank; 3

SPL.1
Blank; 3

HOL; 1

ALT; 13,

ALT ANT CHP HPO HPR REL SPL SYN UNR MRN

Figure 3.6: Agreements/Disagreements categories among annotators. Each
column shows the number of pairs in which there is an agreement among the
annotators. It also shows the category that has been assigned by one of the
annotators in case of disagreement. (e.g., in alternative or derived form (ALT),
all annotators agree on 13 instances , while there is a disagreement in one instance
and one of the annotators selects the spelling variant (SPL).

button to send the results. Using this tool, the annotators assign each pair, i.e.,
target and predicted word to one of the categories. We also allow annotators to
leave empty the assignment if they do not have sufficient knowledge about the
relation between the terms. In order to encourage inter-annotator consistency,
we provide a general annotation guideline (Figure 3.3).

3.5.2 Results and discussion

We receive annotation results for 240-word pairs (24 target words) from the
domain experts in the geology discipline. We report the inter-annotator
agreement as a proportion of agreement. For this task, we do not use the

53

m™ Blank
= MRN
mHOL
mUNR
mSYN
mSPL
mREL
HPR



3. Evaluation of Domain-specific Word Embeddings

Kappa statistic as the chance of agreement for the task is very low due to
implicit similarity among the categories. We observe that in 175 cases, we have
a majority agreement among three annotators. Moreover, all the annotators
agree in 72 cases. By this observation, the inter-annotator agreement for the
task is 58 = 0.729.

To explore the disagreements of annotators in more detail, we extract the
instances for which the two annotators determine the same category, whereas
the third annotator selects another category, and we report it as a confusion
matrix.

Figure 3.6 shows the resulting confusion matrix. In general, we find that the
annotators disagree in terms of similar categories. For example the hyponyms
are annotated as meronyms, related, synonyms and co-hyponyms. The categories
with the highest number of disagreements are hyponyms, hypernyms, related
and synonyms. The annotators agree mostly on alternative or derived form and
unrelated categories. For further analysis, we consider the majority vote among
the annotators as a final category for the word pairs.

Table 3.12 shows the result of this analysis. We count the number of each
category that has been considered as an error in the synonym extraction task.
It means that a word is predicted as 10-most-similar words (i.e., 1°*:10°") by the
model and is considered as a false positive. However, the prediction and target
word are assigned to a specific category during the manual annotation process.

In general, the result of this analysis shows that the model predictions
are semantically meaningful in a majority of cases, and all categories except
the Unrelated/Unknown represent some type of morphosyntactic or semantic
relation between terms. Less than 20% of the errors are assigned to the
Unrelated/Unknown category. This reveals that if we consider the count of
human-judged synonyms as true positives, the actual scores for precision and
recall will be considerably higher than those reported in the evaluation section.
Moreover, the embeddings model proposes more synonyms that are not in the
reference, even though the reference is provided by the manual procedure. The
most frequent error type falls in the related category.

The hyponym and co-hyponym relations are another frequent error type that
were also reported in previous studies (Plas and Tiedemann, 2006; Leeuwenberg
et al., 2016). The morphosyntactic type of relations such as Alternative forms,
spelling variant cover another type of errors. The error analysis further reveals
several meaningful relation types such as Hypernyms, Meronyms, and Holonyms
that are useful in many downstream applications.

3.6 Embedding Enrichment Using a Knowledge Resource

Even though the word embeddings capture important semantic relations in
the domain, the first experiment shows that the domain-specific technical
vocabulary has many elements that are generally disregarded by the distributional
representation techniques. In other words, they still fall short of providing
domain-specific representations for many terms. These approaches rely on the

54


Embedding Enrichment Using a Knowledge Resource

Category Example [target prediction] 15*:10'"(%)
1. Spelling Variant borehole + bore-hole 2A.
2. Alternative or derived form acidizing— acidization 3.2
3. Reference-Synonyms filter cake + mud cake 2.8
4. Human-judged Synonyms seismometer — seismograph 8.4
5. Antonyms transgressive —> regressive 0.9
6. Hypernyms acidizing — stimulation 1.3
7. Hyponyms EOR — In-situ combustion 9.3
8. Co-Hyponyms EOR — MEOR 13.1
9. Holonyms shoe— wellbore Ll
10. Meronyms rig > wellhead 2.8
11. Related Kirchhoff migration — NMO correlation 35.2
12. Unrelated/Unknown backflow — sediment-laden 19.5

Table 3.12: Manual analysis results for the 10-most-similar words.

statistics derived from textual input; therefore, they are incapable of providing
representations for words that are not seen frequently in the training process.
Furthermore, they do not include the valuable information that is accommodated
in domain knowledge resources such as semantic lexicons and glossaries. In this
section, we address these issues by applying a set of techniques that exploit prior
domain knowledge in enhancing the embedding models and induce representations
for OOV terms. We then go on to evaluate the impact of the refinement methods
over an unseen terminological resource.

3.6.1 Related Work

Although word embedding techniques have drawn significant interest in the field,
they are not well equipped to deal with unseen and infrequent words, nor do they
consider word relations found in knowledge resources. Improving the quality of
embedding models has been an active area of research for the past few years.
Based on the way they view the problem, these techniques can be classified
into two main branches: (1) Improving word vectors using lexical resources, (2)
Learning representations for rare words.

In the first line of techniques, researchers work on leveraging semantic
knowledge resources such as WordNet (Miller, 1995), PPPD (Ganitkevitch
et al., 2013b), and FrameNet (Baker et al., 1998) as a relational semantics to
improve the outcome of distributional semantics. Such semantic knowledge can
provide the sense of the word, its relation with other words, such as synonyms,
antonyms, hypernymy, and meronymy. The question is how to design new
distributional semantic algorithms to leverage relational semantics and generate
high-quality word embeddings given the availability of morphological, syntactic,
and semantic knowledge. In Yu and Dredze (2014), and Fried and Duh (2015),
it was shown that the quality of word vectors could be improved by using

55


3. Evaluation of Domain-specific Word Embeddings

semantic knowledge from lexicons. In both works, they propose a new training
objective that incorporates the word2vec language model objective and prior
knowledge from semantic resources. These models use constraints among words
as a regularization term on the learning objective during training. However,
the proposed models are built on a specific distributional semantic technique
ie., word2vec. Similarly, Faruqui et al. (2015) and Pilehvar and Collier (2016a)
proposed a refinement method as a post-processing step which exploits knowledge
from the semantic network to apply to existing pre-trained embeddings. These
approaches are general in that they can be readily applied to any set of word
representations and any semantic network and they are not limited to particular
methods for constructing vectors.

The Zipfian distribution (Zipf, 1972) is a characteristic of words in natural
language, where some words are frequent, but most are rare. Learning
representations for words in the "long tail" of this distribution is challenging
for word embedding techniques since their learning methods require many
occurrences of each word to generalize well. The second branch of related
work includes methods that produce representations for words that were not
encountered frequently during the training of the embeddings models. Botha
and Blunsom (2014) and Soricut and Och (2015) have proposed methods that
incorporate morphological information into word representations. They focus on
morphologically complex rare words and tried to derive representations of words
from morphemes using different composition functions. In another framework
for the training of word embeddings, known as fasttert, Bojanowski et al. (2017)
improved the representation of words by taking into account subword information.
The proposed approach incorporates character n-grams into the skip-gram model.
It can construct a vector for a word from its character n-grams, even if a word
does not appear in the training corpus. However, these techniques are incapable
of deriving representations for words for which no sub-word information might
be available in the training corpus, such as infrequent domain-specific terms.
To expand the vocabulary of the embedding model not only for morphological
variation but also for unseen and infrequent domain-specific words, the approach
proposed by Pilehvar and Collier (2017) exploits the knowledge encoded in
lexical resources and induce vector representation for terms which either have
low frequencies or are non-existent in domain corpora. Here, we employ the
techniques of Pilehvar and Collier (2017) and Faruqui et al. (2015), since they
can be applied to vectors obtained from any word vector training method as a
post-processing step. In the following sections, these approaches are described
further, and their impacts in our study are subsequently evaluated.

3.6.2 Embeddings for infrequent terms

To induce embeddings for rare and unseen words, Pilehvar and Collier (2017)
recommend a technique that expands the vocabulary of pre-trained embedding
models. The technique leverages knowledge encoded in external lexical resources
that provides better coverage of rare words or comprises domain-specific
terminologies. It assumes that there is a pre-trained word embedding model W

56


Embedding Enrichment Using a Knowledge Resource

ae
¥ v . kart
ailaicrisouy.) \.pbeieecttey) |
? ma. gas
‘a piKe
(acts) vehicle bicycle
(ineurrentasoe aaiicaretitd eeeety Neeacae
\acommmance.J truck
¥ ot van
Ss
\anirecteslweticie) | ster!
aanfc aici al
(aad

Figure 3.7: Leveraging structure in external lexical resources to produce semantic
landmark (Pilehvar and Collier, 2017).

and a lexical resource S = (V, /) in a graph structure, where V is a set of nodes
that correspond to words and F is a set of semantic relations among the words.
To produce an embedding for an infrequent word w,. that does not exist in the
vocabulary of W, but is covered by S, the following phases are implemented:

I. Word Semantic Landmarks Extraction A set of landmarks for word w, are
the most semantically similar words, which can be extracted from S. As shown
in Figure 3.7, it is achieved by viewing S as a semantic network and analyzing
its structure. The semantic landmarks for word w, are extracted by using
the Personalized PageRank (Haveliwala, 2003) algorithm, here dubbed PPR.
The PPR provides promising results in many NLP tasks such as Word Sense
Disambiguation (Agirre and Soroa, 2009) , Named entity linking (Nooralahzadeh
et al., 2016) and Word sense similarity (Pilehvar and Navigli, 2015). The PPR is
a modified version of PageRank (Brin and Page, 1998) algorithm. PageRank is
a method for ranking the nodes in a graph according to their relative structural
importance. The main idea of PageRank is that whenever a link from node 7 to
node j exists in a graph, a vote from node 7 to node j is produced, and hence
the rank of node 7 increases. Besides, the strength of the vote from i to j also
depends on the rank of node i: the more important node 7 is, the more strength
its votes will have. Moreover, PageRank can also be viewed as the result of a
random walk process, where the final rank of node 7 represents the probability
of a random walk over the graph ending in node i, for sufficiently many steps.
Let S be a graph derived from external lexical resources, with n nodes
{wi,...,Wn} and d; be the outdegree of node i; let Prxn be a transition
probability matrix, where P;,; denotes the probability of shifting from node 7 to
node j. The P;,; is equal to the inverse of d; if there is a semantic link from 7

57


3. Evaluation of Domain-specific Word Embeddings

to j and zero otherwise. Therefore, to calculate PageRank vector «7 over S for
each node 7 , the power method can be used as follows:
tO? = ag*-DT P+ (1—a)u (3.2)

In equation 3.2, v is the n x 1 column vector in which the prior importance of
each node is assigned to the cell corresponding to each node and a is the damping
factor, a scalar value between 0 and 1. In the traditional PageRank, the vector v
is a stochastic normalized vector whose element values are all 4 , thus assigning
equal probabilities to all nodes in the graph in case of random jumps. However,
in PPR, a modified version of v is used. PageRank is calculated by applying an
iterative algorithm that computes the equation successively until convergence
below a given threshold is achieved, or, more typically, until a fixed number
of iterations are executed. Once 2” is calculated for a word i in the semantic
lexicon, we can obtain a list of most similar words to the w,, i.e., semantic
landmarks for word i, by sorting the x? according to their probabilities.

ll. Embedding Induction: Let L; be a sorted list of semantic landmarks for
word 7 and q(x) be an embedding of word 2 in the space of W. The induced
embedding for w; in the same semantic space can be provided using the following
equation:

La

q(wi) = +r aoe 5.) (3.3)

where 1; is the j*" word in L; and q(w;) is the observed embedding of word w;
in W . Here the intuition is to calculate a new embedding for w; by the weighted
average of its semantic landmarks. The exponential weighting provides more
importance to the top words in the semantic landmarks since these are more
representative for word i. The parameter @ adjusts the contribution of the initial
embeddings q(w;) in the final embeddings. To induce embeddings for unseen
words, @ is set to zero.

In another work, Faruqui et al. (2015) proposed the retrofitting method as a
post-processing step to apply to existing pre-trained embeddings. The goal is to
refine word vector representations to capture relatedness suggested by semantic
lexicons while preserving their similarity to the corresponding embeddings. The
objective of the retrofitting method is to minimize the following:

= ailla — al? + SO Bisllas — al? (3.4)

i=l G,j)eE

where g € Q is the observed vector representation for each term in the semantic
lexicon and q € Q@ is the corresponding retrofitted vector.

The method aims to learn @ such that the q is close to its counterparts in Q
(i.e., @) and to adjacent nodes in the semantic lexicon under a distance metric.
Figure 3.8 depicts an example graph with such connections; white nodes are

58


Embedding Enrichment Using a Knowledge Resource

Te?

( Aiton A) } ( a }

acne Nie

Ts orrec “<} —— = oe Ns y

Figure 3.8: Graph with links between related words showing the observed (grey)
and the inferred (white) word vectors. (Faruqui et al., 2015).

labeled with the retrofitted vectors g, shaded nodes are labeled with the observed
ones q. EF is the set of relations among the terms in the semantic lexicon. a and
8 correspond to the relative weights of the relation type. Since V is convex in
Q, an efficient iterative updating method is used to solve the objective function.
Retrofitted embeddings @ are initialized to be equal to the observed ones Q.
Then by taking the first derivative of UV with respect to q; the following online
update is used for ten iterations to reach convergence:

a= pGaer isd + di
.=
Uy agyer Cig + %

The formula in the Eq. 3.5 computes a new embedding for a term 7, which is
in the pre-trained model and has relations of interest in the semantic lexicon,
whereas its neighbors should be part of the pre-trained model. To provide an
embedding for OOV words, we extend @ in each iteration by adding the terms
that are in the semantic lexicon and connect to the terms that are in Q via
relations of interest. Since there is no initial vector for these type of words in
the observed model, a is set to zero, and the online update formula for the OOV
terms will be as follows:

(3.5)

iG )EE Big dy

(3.6)
5G, j)EE Bij

i=

3.6.3 Additional domain-specific query inventory

We use another domain-related glossary to perform a quantitative comparison of
domain-specific word embeddings before and after the inducing and retrofitting
process. We create a test query inventory using the same approach as explained

59


3. Evaluation of Domain-specific Word Embeddings

carbonate mud

http://resource.geoscimi.org/classifier/cgi/lithology/carbonate_mud

alt label marl bP)
broader carbonate sediment
mud size sediment
definition Carbonate sediment composed of less than 25 percent clasts i=]
that have a maximum diameter more than 2 mm, and the ratio
of sand size to mud size clasts is less than one.
source follow pattern used for clastic sand and mud categories, based fal
on SLTTs 2004
type Resource
Concept
in scheme simple lithology

is primary topic of carbonate mud
narrower carbonate ooze

Figure 3.9: Term structure in the Geoscience Vocabularies (GeoSci) data set.

in Section 3.3.1 over the Geoscience Vocabularies data set °, here dubbed GeoSci.
GeoSci was developed by the UGS (CGI Commission for the Management and
Application of Geoscience Information). GeoSci covers the domain of geology
and describes geological features, geological time, mineral occurrences, and
mining-related features. Figure 3.9 shows the structure of the term carbonate
mud in this domain-specific resource. Each term is defined as a resource under
geosciml name-space with a specific URI. Resources are linked to other resources
with syntactically and semantically aligned relations such as Abbreviated label,
Synonym '°, Broader and Narrower.

We construct a test query inventory by extracting all terms and their inter-
glossary relations from the RDF files. Table 3.13 shows the distributions of terms
in the glossary with respect to their n-gram and participated relations. The test
set consists of 1,753 terms. It contains 196 synonym pairs, 1,639 broader pairs,
1,584 narrower pairs and 986 abbreviated label pairs. Like the slb glossary, the
majority of terms are multi-word units (63%).

Shttp://resource.geosciml.org/
°Uniform Resource Identifier
10GeoSic vocabulary specifies this relation as Alternative label.

60


Embedding Enrichment Using a Knowledge Resource

n-grams # Synonymy Broader Narrower’ Abbr. label

unigram 651 110 588 387 730
bigram 598 45 550 737 201
trigram 328 28 326 305 47
>3 176 13 175 155 8

All 1753 196 1639 1584 986

Table 3.13: N-grams and Relations in the Geoscience Vocabularies (GeoSci) data
set.

3.6.4 Experiments and Evaluation

We use the structure of the slb glossary as prior domain knowledge to enrich the
OILGAS.d400 embeddings model that was selected as the best embedding model
following our experiments in Section 3.4. To enrich the pre-trained embeddings,
we employ the two techniques that are described in Section 3.6.2 as follows:

Inducing embeddings for unseen words: We create the semantic graph
G = (V, F) using the sib glossary, where V is the set of slb terms and F is the
set of edges that denote semantic relationships i.e, synonymy, antonymy and
alternative form among terms in V. For each term in V, we extract semantic
landmarks using the PPR algorithm by using Eq. 3.2. We set the damping
factor a to its default value (0.85), and the personalize vector v is a one-hot
initialization vector in which 1.0 is assigned to the cell corresponding to the
term. We choose top n = 10 most similar words according to their probability
in «) as a semantic landmark for each word. We induce new embeddings for
observed terms in the OILGAS.d400 embedding model by exploiting the Eq. 3.3
and setting @ to 1.0 and for OOV to zero (".induced#10").

Retrofitting Word Vectors to Semantic Lexicons Experiments in Faruqui
et al. (2015) showed that including all semantic relations in the retrofitting
process has a better impact than having only one of them. We, therefore,
consider connections of a word to its synonyms, alternative forms, and antonyms.
Moreover, similar to the origin, all a; are set to 1 and §;; to be degree(i)~!. The
Eq. 3.6 is used to retrofit the OILGAS.d400 model by employing the structure
of the semantic lexicon (".retrofitted"). To induce word vectors for OOV terms,
we carry out the retrofitting process with Eq. 3.6 ("-retrofitted+OOV'").

Table 3.14 shows the performance of the model in the test dataset as well as the
induced and retrofitted models with two different configurations. We observe that
the inducing algorithm has negative impacts on the synonymy and abbreviated
label relation. It seems that with a small semantic lexicon, the inducing algorithm
provides noisy semantic landmarks for each term, which leads the induced
embeddings vector to be close to non-related terms. However, the retrofitting

61


3. Evaluation of Domain-specific Word Embeddings

Model Synonymy Narrower Broader Abbr.
label

A RP A R P A RP A RP

OILGAS 25.5 15.5 5.1 | 12.5 5.0 2.7) 47 44 09 | 24 2.3 0.5
OILGAS.induced#10 11.0 6.7 2.2) 125 50 2.7 | 47 44 0.9 | 2.0 20 0.4
OmGas. retrofitted 27.4 16.7 5.5) 12.5 5.0 2.7) 47 44 09 | 2.2 2.2 0.4

OILGAs.retrofitted+OOV | 30.2 18.4 6.1] 12.5 5.0 2.7 | 4.7 44 09 | 24 2.3 0.5

Table 3.14: Evaluation over the GeoSci knowledge resource.

Model | Synonymy | Antonymy | Alt. form
| A R P| A R P| A R P
OILGAS. retrofitted 59.0 47.3 12.4 | 87.3 82.2 18.5) 32.6 29.1 6.8

OmLGas.retrofitted+OOV | 94.6 90.7 25.3 | 98.5 97.6 24.1 | 96.2 94.7 21.7

Table 3.15: Evaluation in the slb data set (learning data set).

process provides an improvement in the synonymy relation. The improvement is
highest when we consider the adapted version (retrofitted +OOV). Interestingly
the inducing and retrofitted models have no impact in the narrower and broader
relationships. This can be attributed to the fact that the employed semantic
lexicon does not include these kinds of associations to lead the retrofitting process.
In the abbreviated label relation, there is a slightly negative effect when we apply
the original retrofitting process. Expectedly, the applied retrofitting process
encourages the terms in semantic lexicons to have similar vector representations
with respect to their semantic and morphosyntactic relations (i.e., synonymy,
antonymy and Alternative forms). The improvement is biggest for retrofitted
models when they are assessed in the query inventory generated from the input
semantic lexicons. Covering OOV terms by using Eq.3.6, empowers the model
to provide high performance in each relation benchmark (Table 3.15).

3.7 Extrinsic Evaluation

While the intrinsic evaluation attempts to interpret the encoding content of an
embedding model in terms of lexical-semantic relations, extrinsic evaluation
investigates the contribution of an embedding model to the performance of a
specific downstream task (Section 3.2.2). In this section, we investigate the
influence of our domain-specific model in a domain related classification task.

3.7.1. Classification Data Set

The task of the exploration department in the Oil and Gas industry is to find
exploitable deposits of hydrocarbons (oil or gas). Geo-scientists in the exploration

62


Extrinsic Evaluation

Property Label # Sentences
Lithology_ Main Lu 1,193
DepEnv_ Main Dy 483
Facies F 387
DepEnv_Sub Ds 298
Lithology_RockType Lr 191
BasinType B 49
DepEnv_ General De 38

Table 3.16: Classification data set.

department model the subsurface geography by classifying rock layers according
to multiple stratigraphic hierarchies using information from a wide range of
different sources. The quality of the analysis depends on the availability and
the ease of access to the relevant data. Previous technical studies, reports, and
surveys are crucial resources in this process.

We were granted access to a dataset of 1,348 sentences from exploration
textual documents, which are then manually labeled with various geological type
properties by domain experts. Example 3.7 shows a sentence from the data set
along with its assigned set of properties.

(3.7) Submarine fans and deltaic/estuarine facies of the San Juan Formation
were deposited during the Maastrichtian regression, which gave way during
the Paleocene-Eocene to black marine shales and carbonates of the Vidono
Formation and the shelfal and pro-delta shales of the Caratas Formation.

Properties: Lithology_RockType (Lr), Lithology_Main (Laz), DepEnv_Sub
(Ds), DepEnv_General (Dg)

The resulting data set contains 1,348 sentences in which experts assigned
each sentence to 7 different properties. The sentences are pre-processed using the
same approach, as described in Section 3.3.2. Table 3.16 depicts the properties
and number of sentences for each. It is clear that the data set is unbalanced
regarding the properties, and the downstream task is a multi-label classification
task.

3.7.2 Multi-label Classification Model

We use a slight variant of the Convolutional Neural Networks (CNNs) architecture
(see Section 2.2.2 in Chapter 2) that is proposed by Kim (2014) for sentence
classification tasks. We keep the value of hyperparameters equal to the ones
that are reported in the original work, however we update the dimension of the
embedding layer according to the dimension of the domain-specific embedding

63


3. Evaluation of Domain-specific Word Embeddings

Model Ds Ly B De F Dy Lr
Fl Fl Fl Fil Fil Fl Fil
CNN.rand 28.9 | 90.6 | 0.0 | 0.0 63.0 | 57.1 | 68.2
CNN.domain 51.1 | 91.4 | 23.9 | 11.3 | 71.1 | 66.3 | 65.8
CNN. multi.rand 38.0 | 91.4 | 7.3 | 5.0 63.9 | 58.6 | 69.9
CNN. multi.enwiki 43.9 | 90.5 | 11.3 | 0.0 | 61.1 | 61.4 | 57.8
CNN. multi.domain 56.2 | 92.2 | 33.8) 15.0) 71.7 | 69.4 | 72.5
CNN. multi.retrofitted+OOV 64.0 | 91.3 | 11.3 | 0.0 67.6 | 68.8 | 72.2
CNN.multi.domain &retrofitted+OOV | 68.2} 92.8) 32.0 | 9.4 73.4| 73.5) 71.1
CNN.multi.retrofitted+OOV&domain | 53.4 | 92.6 | 20.9 | 10.0 | 71.8 | 67.0 | 70.7

Table 3.17: Results of the classification task with various configurations.

model. Furthermore, since the architecture aims to assign a single label to
each sentence, we update the activation function to sigmoid at the output
layer. The sigmoid is a non-linear function, is defined as o(a) = ie and
produces a probability for each of the potential properties. During training,
these probabilities are used to compute the error, while during testing, we round

each of the probabilities to 0 or 1 depending upon a set threshold (0.5).

3.7.3 Extrinsic evaluation experiments

Like Kim (2014), we run experiments with several variants of the model to
investigate the importance of domain-specific input as follows:

e CNN.rand: As a baseline model, all words in the embedding layer are
randomly initialized and updated in the training process.

e CNN.domain: the embedding layer is initialized with a domain-specific
model and fine-tuned for the target task.

e CNN.multi.rand: There are two embedding layers as a ’channel’ in the
CNN architecture. Both channels are initialized randomly, and only one of
them is updated during training while the other remains static.

e CNN.multi.domain: Same as before, but the channels are initialized
with domain-specific vectors.

e CNN.multi.enwiki: The channels consider the general domain word
vectors from section 3.4.3 using the English Wikipedia data.

To deal with the effects of an unbalanced dataset and guarantee that each fold in
5-fold cross-validation will have the proportion of the same classes during training
and testing, we apply the stratification of multi-label data proposed by Sechidis

64


Summary

et al. (2011). The stratification is a sampling method that takes into account
the existence of disjoint labels within a dataset and provides samples where the
proportion of these labels is maintained. Sechidis et al. (2011) introduce an
iterative stratification method that distributes samples based on how desirable a
given label is in each fold, tackling the problem of lack of rare label evidence in
folds.

Results of the classification task with various CNN configurations are
presented in the first section of Table 3.17. In general, the multi-channel
model performs better than the single-channel setting. The results suggest that
having a significant amount of sentences per property helps the CNN model to
classify better. The baseline model does not perform well on its own. The use
of the pre-trained embeddings model helps the model in property assignment.
Particularly, domain-specific embeddings provide higher performance gain in
the task-at-hand when it is used in both channels. We further investigate the
influence of the refined word embedding models in our classification task.

e CNN. multi.retrofitted+OOV: We used the retrofitted domain embed-
dings including the OOV vectors for two channels. One channel is static
and the other is non-static.

e CNN. multi.domain&retrofitted+OOV: First channel is initialized
with original domain-specific embeddings with static mode and the second
makes use of the retrofitted embeddings with a non-static mode.

e CNN.multi.retrofitted+OOV&domain: Same as previous setting,
but the channels swap their input.

In these experiments, because of having many multi-words as OOV terms in
the model, we replaced tokens in the sentences with their bigram and trigram
forms if their combination occurs in the model vocabulary (e.g., fracture porosity
is replaced by fracture_porosity as an input unit). The experiment (second
section of Table 3.17) shows that the enhanced embedding models provide better
input representations for classes with a sufficient number of instances.

3.8 Summary

This chapter contains research and experiments on the evaluation of word
embedding models trained on a low-resource domain, namely the Oil and Gas
domain. The first research question is whether constructing domain-specific
word embeddings is beneficial even with limited input data. This question is
answered by conducting intrinsic and extrinsic evaluations of both general and
domain-specific embeddings. The empirical evaluation shows that even though
the distributional models have low performance in domain-specific synonymy
detection, an in-depth manual error analysis reveals the striking ability of the
embedding models to discover other semantic relations such as (co)hyponymy,
hypernymy, and relatedness. Furthermore, we observe that domain-specific

65


3. Evaluation of Domain-specific Word Embeddings

trained embeddings perform better compare to the general domain embeddings
trained on much larger input data.

The second research question investigates the impact of existing domain
knowledge resource on enhancing the embedding models. We augment the
domain-specific model by providing vector representations for infrequent and
unseen technical terms using a domain knowledge resource. Experiments show
the importance of dealing with rare words in an embedding model in both
intrinsic and extrinsic evaluation.

To summarize, we make the following contributions in this chapter:

(i) We create a domain-specific evaluation dataset: a corpus and a query
inventory for the oil and gas domain,

(ii) We train and release the domain-specific embeddings for the oil and gas

domain !',

(iii) We conduct intrinsic evaluation including a manual analysis by domain
experts,

(iv) We inject domain knowledge into domain embeddings and show that it
produces advancements in the intrinsic and extrinsic evaluation.

11Link to the domain-specific model: http://vectors.nlpl.eu/repository/1 1/75.zip.

66


Chapter 4

Named Entity Recognition in
Low-Resource Domains

Named Entity Recognition (NER) is an important task in the information
extraction pipeline as stated in Section 2.6 of Chapter 2. Existing NER systems
rely on large amounts of human-labeled data for supervision. However, obtaining
large-scale annotated data in low-resource scenarios is challenging, particularly in
specific domains like health-care, e-commerce, and so on. Given the availability
of domain specific knowledge resources (e.g., ontologies, dictionaries), distant
supervision has become a solution to generate automatically labeled training
data to reduce human effort, as explained in Section 2.3 of Chapter 2. The
outcome of distant supervision for NER is often noisy however. False positive
and false negative instances are the main issues that reduce performance on this
kind of auto-generated data.

In this chapter, we explore the use of distant supervision for NER in four
low-resource scenarios. We present a system which addresses the problem of
noisy data in two ways. We study a reinforcement learning strategy with a
neural network policy to identify false positive instances at the sentence level.
We further adopt a technique of incomplete annotation to address the false
negative cases. The proposed hybrid model achieves competitive performance
on benchmark datasets.

4.1. Introduction

Named Entity Recognition (NER) is one of the primary tasks in information
extraction pipelines. Traditional studies apply statistical techniques such as
Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using
large amounts of features and extra resources (Ratinov and Roth, 2009; Passos
et al., 2014). In recent years, deep learning approaches achieve state-of-the-art
results in the task without any feature engineering (Ma and Hovy, 2016; Lample
et al., 2016). Most of these works assume that there is a certain amount of
annotated sentences in the training phase. However, the availability of large
amounts of labeled data is problematic, particularly in specific domains. In the
low-resource setting, where the amount of data and the knowledge of the domain
are insufficient for traditional approaches, distant supervision (see Section 2.3
in Chapter 2) is proposed by Mintz et al. (2009) to address the challenge of
obtaining training data for new domains using existing knowledge resources
(dictionaries, ontologies). It has previously been successfully applied to tasks
like relation extraction (Riedel et al., 2010; Augenstein et al., 2014) and entity
recognition (Fries et al., 2017; Shang et al., 2018b; Yang et al., 2018). To create

67


4. Named Entity Recognition in Low-Resource Domains

training data in a NER task, it identifies entity mentions if they exist in the
knowledge base (e.g., domain-specific dictionary, glossary, ontology) and assigns
the corresponding type according to the knowledge base.

However, distant supervision approaches encounter two main limitations.
First, due to limited coverage of the knowledge resources, unmatched tokens
result in False Negatives (FNs). Second, since simple string matching is employed
to detect entity mentions, ambiguity in the knowledge resource may lead to False
Positives (FPs). For the FN problem, Tsuboi et al. (2008) incorporate partial
annotations into CRFs and propose a parameter estimation method for CRFs
using partially annotated corpora (here-in after referred to as Partial-CRF).
In order to reduce the negative impact of FPs for relation extraction, Qin et
al. (2018) propose a deep reinforcement learning (RL) agent where the agent’s
goal is to decide whether to remove or keep the distantly supervised instance.

In this chapter, we combine the Partial-CRF approach with the RL approach
to clean the noisy, distantly supervised data for NER. More specifically, we
explore the following research questions:

RQ 4.1. How can we address the problem of low-resource NER using distantly
supervised data?

RQ 4.2. How can we exploit a reinforcement learning approach to improve NER
in low-resource scenarios?

RQ 4.3. Is the proposed solution beneficial for different low-resource scenarios?

4.2 Background

In the following sections, we will describe the background of NER and models
that have been proposed for the NER task.

4.2.1. Named Entity Recognition

The term Named Entity was introduced in 1996 at the 6th Message Understanding
Conference (MUC), as unique identifiers of entities (organizations, persons,
locations), times (dates, times), and quantities (monetary values, percentages)
(Chinchor, 1998). Most of the annotation datasets for the NER task contain
these types of named entities, though with important variations. Example 4.1
shows an example of named entities in the widely used CoNLL shared task
(Tjong Kim Sang and De Meulder, 2003) dataset.

4.1) Adams and Platt are both injured and will miss England ’s
a
PER PER LOC

At first glance, it seems that only proper names (e.g., Adams, Platt, England)
can be considered as named entities. However, depending on the application,
it can be useful to recognize some other linguistic categories as named entities
such as pronouns (e.g., it, who, she, he) or nominal mentions (e.g., the girl,
mother, the company). Furthermore, the definition can vary depending on the

68


Background

genre and domain (e.g., Health care, Technical review, e-commerce). As an
example, 4.2 depicts another type of named entities in the bio-medical domain
from BioCreative V CDR task corpus (Li et al., 2016). Here we see that domain
specific tags, such as Chemical and Disease, are used to label named entity
mentions.

(4.2) Selegiline-induced postural hypotension in Parkinson’ s disease
Chemical Disease Disease

The NER task, or the more general entity mention detection task, is the
first step and an essential component of the information extraction pipeline.
It involves detecting the boundaries of the phrases that correspond to entities
and determining their entity types. Intuitively, given a sentence of words
W : wiwe...Wn, NER assigns a sequence of tags y : y1Y2.--Yn from a predefined
set of categories y; € ®,|®| = k.

A single named entity can span several tokens within a sentence. Therefore,
sentences are usually represented in specific sequence labeling schemes in NER
datasets. The two most popular ones are the following schemes:

e BIO: It stands for Beginning, Inside and Outside (of a text segment). If
the token is the beginning of a named entity, it will be labeled as B-<type
of NE>, if it is inside a named entity, but not the first token within the
token, it will be labeled I-<type of NE>, and if it is not a part of a named
entity, it will be labeled as O.

e BIOES: Similar but more detailed than BIO, BIOES encodes the
beginning, the inside, and last token of multi-token chunks while
differentiating them from unit-length chunks. It encodes a singleton entity
as S-<type of NE> and explicitly marks the end token of multi-word
named entities as E-<type of NE>.

Table 4.1 shows an example sentence that is annotated in both of the BIO and
BIOES labeling schemes.

4.2.2 Neural NER models

Recently, deep neural models have been employed in the NER task and reached
state-of-the-art results on many NER datasets. They benefit from continuous
vector representations and semantic composition through nonlinear processing
to discover useful representations and underlying factors from input data (see
Section 2.2 in Chapter 2). Existing models are composed of multiple processing
layers to learn representations of data with multiple levels of abstraction. Their
architecture (Figure 4.1) usually consists of three main components (Li et
al., 2020) as follows:

e Distributed representations for input: Distributed representations,
as described in Section 2.5.1 of Chapter 2, present each word or character

69


4. Named Entity Recognition in Low-Resource Domains

Label

Token BIO BIOES
Adams B-PER S-PER
and O O
Platt B-PER S-PER
are O O
both O O
injured O O
and O O
will O O
miss O O
England B-LOC S-LOC
’s O O
opening O O
World B-MISC B-MISC
Cup I-MISC E-MISC
qualifier O O
against O O
Moldova B-LOC S-LOC
on O O
Sunday O O

O O

Table 4.1: Example sentence from CoNLL03 in BIO and BIOES annotation
schemes.

by a low dimensional dense vector where each dimension comprises a latent
feature. Word-level distributed representations are considered inputs to
the NER models. This representation is typically pre-trained over a
large collection of text through unsupervised algorithms and captures
the syntactic and semantic properties of its elements. The input layer
can be either frozen or fine-tuned during the training phase. The pre-
trained word embeddings that are used widely in English NER models are
Google Word2vec !, Stanford GloVe 7, Facebook fastText ? and SENA
4° Several NER models incorporate character-based word representations
besides the word representations as an input layer. This representation
is learned using an end-to-end neural model. It enables the NER model

70

lhttps://code.google.com/archive/p/word2vec/
?http://nlp.stanford.edu/projects/glove/
3https://fasttext.cc/docs/en/english-vectors.html
*https://ronan.collobert.com/senna/


Background

B-PER O B-PER O O fe) Oo Oo O B-LOC 0 ...
Adams and Platt are both injured and will miss England 's ...

(3] Tag decoder
Softmax, CRF, RNN, Point network,...

8 Context encoder

CNN, RNN, Language model, Transformer,...

f
Q Distributed representations for input

Pre-trained word embedding, Character-level
embedding, POS tag, Gazetteer.,...

Adams and Platt are both injured and will miss England 's ...

Deep Learning Based NER

Figure 4.1: General Architecture in the Deep Neural NER in the BIO labeling
(Li et al., 2020).

to learn the representations for unseen words and to share information
of morpheme-level regularities. In addition to word- and character-level
representations, some NER models include other syntactical, context,
morphological, and lexical features into the input layer such as POS tags,
word shape, dependency roles, word positions, and gazetteers. However,
incorporating these types of features may affect the generality of the NER
models.

e Context encoder: The second module of neural NER models is devoted
to capturing the contextual dependency from the input representations.
The widely-used contextual encoders are Convolutional Neural Networks
(CNNs) and Recurrent Neural Networks (RNNs) (see Section 2.2 of Chapter
2 for more details). Lately, pre-trained language models, as explained in
Section 2.5.1.2 of Chapter 2, such as ELMO (Peters et al., 2018) and BERT
(Devlin et al., 2019), are used as context encoders and provide a pre-trained
deep representation model from unlabeled text. It is empirically verified
that the pre-trained model can be fine-tuned with one additional layer for
various downstream tasks, including NER, and enhance their performance.

e Tag decoder: As a last part of the NER model, it takes the output of

71


4. Named Entity Recognition in Low-Resource Domains

Tag Decoder Conditional Random Field (CRF)

forward
BiLSTM
—
backward
4 4 4 4
Lookup | Distributed representations for input |
f f f f
Sequence | Sequence of tokens |

Figure 4.2: Architecture of BiLSTM-CRF framework.

the context encoder and predicts a sequence of labels corresponding to the
input sequence. A Conditional Random Fields (CRFs) framework (Lafferty
et al., 2001) is the most common choice for the tag decoder step. Most of
the state-of-the-art NER models employ CRFs to capture inter-dependency
among the labels and show that CRFs can provide higher tagging accuracy
in general. A Multi-Layer Perceptron and Softmax based decoder is another
type of design choice for the tag decoder step in some NER models. It
casts the sequence labeling task as a multi-class classification problem.
In this arrangement, the label for each token is independently predicted
without taking into account the adjacent label.

4.2.3. BiLSTM-CRF framework

BiLSTM-CRF (Figure 4.2) is a commonly used neural framework that is exploited
for NER. In the following, we introduce the components of the BiLSTM-CRF
architecture employed in our work.

BiLSTMs Encoder. Bidirectional Recurrent Neural Networks (Bi-RNNs)
(Schuster and Paliwal, 1997) combines an RNN network (see Section 2.2.3
in Chapter 2) which moves forward through time, beginning from the start of
the sequence representation, along with another RNN that moves backward,
starting from the end of the sequence and is trained using all available input
information in the past and future of a specific time frame.

The BiLSTM context encoder employs a Long Short Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997) instead of RNN. LSTM, as described in

72


Background

Section 2.2.3 in Chapter 2, is a special kind of RNN, capable of learning long-
term dependencies. This network architecture can efficiently solve the long-term
dependencies problem by introducing a gating mechanism and a memory cell.

Character- and World level encoding. The character-level BiLSTM networks
process characters of word input and learn character-level features while
training. Learning character-level embeddings has the advantage of learning
representations specific to the task and domain at hand. It has been shown
that this type of representation is useful for morphologically rich languages,
and handles the out-of-vocabulary problem for some downstream tasks such as
part-of-speech tagging and language modeling (Ling et al., 2015). The randomly
initialized embedding vector corresponding to each character in the input word
is passed through the BiLSTM network in a forward and backward fashion.
The forward and backward outputs from the BiLSTM are concatenated to
form a character-level encoding for each word. As shown in Figure 4.3, this
character-level encoding is then concatenated with word embeddings from a
word embeddings lookup-table and given to another BiLSTM network as a final
context encoder layer. Let X : x1x%2...%, be a word-level input representation ,
where x; is the embedding vector for the 74, word and its character-level input
is C': co,—€1,1€1,2€1,3C1,— - ++ Ci,j »-.n,—, Where c;,; is the dense vector of the jrn
character in word w; and c;,_ is the representation for a space character after
w,;. The first BiLSTM encoder layer will receive a dense vector corresponding to
each character. Formally, a LSTM cell will compute the current hidden state h,
based on the current vector c;, the previous hidden state hy; and the previous
cell state s¢_1, however we will only consider h; at word boundaries, namely
space characters or c;,— (see the implementation of LSTM in Section 2.2.3 of
Chapter 2). If Rey, is the output of the forward character-level LSTM at ¢;,—
and , he, is the output of the backward character-level LSTM at c¢;,_ , the
character-level encoding result for the i;, word is:

>

hf = he,

4 a,

_ Whey (4.3)

Subsequently, the character-level encoding output for 24; word is concatenated
to its word embedding vector and is fed into the second BiLSTMs network. h;
as the output of the forward word-level LSTM at the 74, word and the output
of backward word-level LSTM h; are concatenated and provide the word-level
encoding representation h; for a word 2:

CRF In sequence labeling tasks, the dependencies between adjacent labels
should be taken into account. Particularly in NER, the characteristics of tagging
schemes (e.g. BIO, BIOES) impose various hard constraints such as (Lample
et al., 2016):

73


4. Named Entity Recognition in Low-Resource Domains

B-PER

CRF

C) Concatinate
<> us™™ ot

e,
BS ws
P

fees) Embeddings nl

Loo olonoy 1,
a oe we we
Il a t tt

Figure 4.3: Character- and word-level BiLSTM encoding of NER model in the
BIO labeling (Liu et al., 2018a).

e The first word in a sentence should be annotated with a label that begins
with B- or O, not I- in the BIO scheme.

e The label ORG cannot come after B-LOC or any other tag that is not
LOC

e The possible tag that can take place after B-ORG is either -ORG or O in
the BIO scheme.

In order to guarantee that the output label sequence is valid, we jointly decode a
chain of labels. CRF, as a discriminative type of sequence-based model, considers
the dependencies between labels in neighborhoods and defines a conditional
probability distribution over a label sequence. It learns the dependency among
labels automatically based on the annotated samples during the training process.
The CRF layer comes on top of the last layer (i.e, word-level BiLSTM) to
model the dependencies across output tags and locate the best tag sequence by
maximizing the log-probability in the following equation:

es(W.y)
log (p(y|W)) = log Fy ora) (4.5)
where h h
0) = Ps + Toes (4.6)
w=1 q=1

74


Background

Where W : wyw2... Wr, is an input sequence, y : y1Y2---Yn is an annotated tag
sequence and Y is all possible tag sequences of the sentence. CRF takes P as
an emission score matrix which is a k x n output tensor of a linear encoder
applied to the last BiLSTM layer where P;,; corresponds to the score of the
j'” tag of the i” word in a sentence. T is a (k + 2) x (k +2) transition tensor
which represents the transition probability from the i‘” tag to the j” tag. Two
additional tags <BOS> and <EOS> are added at the start and end of a sequence,
respectively. The transition matrix is randomly initialized and is learned by
the CRF during the training phase. For training, we encourage the model to
produce a valid sequence of output labels by defining the loss function as the
negative log likelihood of the current sequence tag y:

£ = —log(p(y|W)) = log( D7) e*) — s(W.y) (4.7)
y'EY
While testing or decoding, the goal is to determine the best label sequence yx
that maximizes the likelihood conditioned on the input sentence W and learnt
model parameters 9 (e.g, T and P) :

y* = arg max p(y|W; O) (4.8)
yeY

Since we model only bi-gram interactions (i.e, two adjacent labels) in the CRF
model, both Eq. 4.7 and Eq. 4.8 can be computed by adopting the Viterbi
algorithm.

4.2.4 Reinforcement Learning

Reinforcement learning (RL) differs from supervised and unsupervised learning
in that the goal is to learn a set of actions without relying on a labeled training
dataset to maximize a predefined reward function. The learning process is based
on the finite Markov Decision Process (MDP) framework where the RL model
consists of various key elements: In a given state (s,) of a stochastic environment,
an agent as a learner and decision maker tries to find an optimal action (a;) in
order to maximize the expected rewards (r;), by following a policy (7). More
specifically, at each time step t = {0,1,2,...,T}, the agent receives a state
8; € S as representation of the environment, and following the policy, the agent
performs an action (a; € A). The RL model aims to maximize the following
objective function (Sutton et al., 1998):

T
max By ~r9( eal [So r( (sz, at)] (4.9)
t

Where r(s;, a;) is a reward in time step t, 7 is a sequence of states, actions,
and rewards known as a trajectory, and 7(T) is the joint probability of a sequence
of actions that can be formulated in MDP as:

to(T) = 7o($1,01,---,$87, 47) = p(s1 I az|S+)p(S441|Se, At) (4.10)

75


4. Named Entity Recognition in Low-Resource Domains

T(a;|Sz) is a policy that tells the agent how to act from a particular state, and
p(s+41|Sz, 4+) known as the model in RL, is a transition function that predicts
the next state after taking action.

The most successful RL techniques employ a neural network in conjunction
with RL. Neural models enable the RL model to deal with unstructured
environments, learn complex functions, solve complicated problems in an end-to-
end fashion, or predict actions in unseen states. Policy Gradient introduced by
Sutton et al. (1999) is one of the RL algorithms that focuses on the policy. The
policy is learned by directly differentiating the objective function in Eq. 4.9 as
follows:

ua

J(0) = Elpaites (rT) yon St, At | = ye eC [r(7)] = [ ror(ar

t

Since:
VoFla) = fle) I = F(a)Valoe f(a)
Then:
VoJ (6 ) = | Voro(r)r( T)dt = [rove log m9(T)r(r)dr =

Ew (7) [Vo log m9(T)r(7)
Considering Eq. 4.10:

TF
log 9(7) = log p(s1) + So log T9(az|8z) + log p(se41|82, ae)

t=1
T T
Val (@) = Bp scmgtr) (> Vo log 79 (az|S:) (ds St, At )
t=1 t=1
Since:
z 1
J(0) = Ey nrg(r) ia St, At } x Woe da Msists tit)
t it
Then:
a
i=1 \t=1 t=1

Eq. 4.11 computes how likely the trajectory is under the current policy. If
the results of the trajectory lead to a high positive reward, it will increase
the likelihood. On the other hand, it will decrease the likelihood of a policy
if it outputs a high negative reward. In short, keep what has positive effects
and throw out what does not. REINFORCE (Williams, 1992) is known as the

76


Low-Resource NER

Algorithm 1: REINFORCE (Williams, 1992).

1 Initialize 6 at random
2 for Generate {r'}, following 7 do
3 fort =1toT—1do

4 VoI(8) © D2; (XE; Vo log m6 (ai,2|8i,4)) (24 r( Si, @i,t))
5 0+ 0+aVoJ(9)
6 Return 0

Monte-Carlo policy gradient, which uses Monte Carlo rollout to compute the
rewards (see Algorithm 1). The agent collects a trajectory 7 of one episode
using its current policy and updates the policy parameter using the 7. Since one
full trajectory must be completed to construct a sample space, REINFORCE
updates the policy network parameters (weights) in the direction of the gradient
(see line 5 of Algorithm 1).

4.3 Low-Resource NER

The task of NER has been widely studied in the last decade and is usually
formulated as a sequence labeling problem. Using neural techniques, many
studies report state-of-the-art results on this task (Lample et al., 2016; Ma and
Hovy, 2016). These studies utilize character and/or word embeddings to encode
sentence-level features automatically. Recently, the use of contextualized word
representation (Peters et al., 2018; Akbik et al., 2018) significantly improves the
state-of-the-art results in many sequence labeling tasks and specifically also in
the NER benchmark.

In the supervised paradigm, NER suffers from a lack of large-scale labeled
training data when moving to a new domain or new language. To alleviate the
reliance on human annotated data, distant supervision is proposed by Mintz et
al. (2009), to generate annotated data by heuristically aligning text to an existing
domain-specific knowledge resource. It is widely used for relation extraction
(Mintz et al., 2009; Riedel et al., 2010; Augenstein et al., 2014) and lately it
has attracted attention also for NER (Ren et al., 2015; Fries et al., 2017; Shang
et al., 2018b; Yang et al., 2018). In this section, we look more closely at previous
works that utilize the data generated by distant supervision in relation extraction
and NER tasks and address the challenges of noisy generated data.

Feng et al. (2018) propose a model for sentence level relation classification in
noisy sentences that are collected via distant supervision. The model contains a
relation classifier and an instance selector. The instance selector filters out the
low-quality sentences by using reinforcement learning and provides the selected
sentences for the relation classifier. They adopt REINFORCE in the instance

77


4. Named Entity Recognition in Low-Resource Domains

filtering step. The relation classifier predicts the relation at the sentence level and
produces rewards as a weak supervision signal to the filtering module. The two
modules are trained jointly to optimize their objective functions. In the relation
classification module, a convolutional architecture determines the relation class
for entity pairs in a given sentence. The instance selector is the agent that follows
a feed-forward neural policy network to distill the training data for the relation
classifier. At the same time, it refine its policy function using the feedback
from the relation classifier. The reward is calculated based on the prediction
probabilities in the CNN model when the selection of all training sentences is
finished.

Qin et al. (2018) also explore deep reinforcement learning as a false positive
removal tool for distantly supervised relation extraction. The policy-based
agents are learned for each relation type, and they aim to determine and remove
the false positive cases from auto-generated labeled data. In contrast to Feng
et al. (2018), the reward is intuitively reflected by the performance change of the
relation classifier. They design the policy agent in a supervised fashion and use
a pre-trained policy network in the RL module. Here, we adapt their approach
to the NER task. Unlike Qin et al. (2018), we learn the policy agent in an
unsupervised manner, where the parameters are learned by interaction with the
environment.

Yang et al. (2018) make use of reinforcement learning to tackle false positives
in distantly supervised NER. Similar to our work, Yang et al. (2018) address
the noisy automatic annotation in NER, by using partial annotation learning
and reinforcement learning. However, unlike our approach, they train the NER
model and reinforcement learning model jointly, calculating the reward based
on the loss of the NER model. In contrast, we employ the RL module as a
pre-processing /filtering step, incorporating the previous state to satisfy a Markov
decision process (MDP). Yang et al. (2018) evaluate only on a Chinese dataset,
whereas we apply our model also to English datasets. Furthermore, after running
their code °, we observe that to reach the reported results in their paper on the e-
commerce dataset, the model needs more than 500 epochs and the reinforcement
learning component removes all the distantly annotated sentences after some
epochs. This means that after some epochs, the code in reality only applies the
baseline NER model on the annotation dataset and ignores the RL module since
there are no distantly annotated sentences. Their two datasets are included in
our experiment in order to compare to their results.

Shang et al. (2018b) present the AutoNER model, which employs a new type
of tagging scheme (dubbed Tie or Break) rather than the common ones (i.e., BIO,
BIOES). The model does not have a final CRF layer but still achieves state-ofthe-
art unsupervised F'l scores on several benchmark datasets. Instead of predicting
the label of each token, they propose predicting whether two adjacent tokens
are tied (i.e., Tie) in the same entity mention or not (i.e., Break). They find
that even when the boundaries of an entity mention are mismatched by distant
supervision, most of its inner ties are not affected, and thus more robust to noise.

>https://github.com/rainarch/DSNER

78


Model

Algorithm 2: Overall Training Procedure NER+PA+RL.

Input: Human Annotated (A) + Distantly Labeled Data (D)
1 Pre-train NER with Partial-CRF (NER+PA) on A+D
2 Apply RL on D
3 Train NER+PA using A + cleaned D

Accordingly, they design a neural architecture (AutoNER), that identifies all
possible entity spans by detecting such ties and then predicts the entity type
for each span. Crucially, they employ a set of high-quality phrases in distant
supervision, using a phrase mining technique, AutoPhrase (Shang et al., 2018a),
to reduce the false-negative labels. The AutoPhrase framework leverages available
high-quality phrases in general knowledge bases such as Wikipedia and Freebase
for distant supervision to avoid additional manual labeling effort. Therefore, it
independently creates samples of positive labels from general knowledge and
negative labels from the given domain corpora and trains several classifiers. Then,
it aggregates the predictions of the classifiers to reduce the noise from negative
labels. In the first phase, AutoPhrase establishes the set of phrase candidates
that contains all n-grams considering a threshold based on the raw frequency of
the n-grams. Given a phrase candidate, the quality of the phrase is estimated by
some statistical features such as point-wise mutual information, point-wise KL
divergence, and inverse document frequency. Finally, it finds a complete semantic
unit in some given context by using part-of-speech-guided phrasal segmentation.
AutoPhrase can support any languages as long as a general knowledge base of
the language is available, while benefiting from, but not requiring a POS tagger
(Shang et al., 2018a).

4.4 Model

In this section, we present the proposed model, which copes with the problems
in distantly supervised NER. We implement Partial-CRF together with a
performance-driven, policy-based reinforcement learning method to detect FNs
and FPs in distantly supervised NER. We here combine techniques that have
been shown to be useful in previous work (Yang et al., 2018). In our architecture,
as shown in Figure 4.4, we first apply partial annotation learning (PA) using
the annotation dataset (A) and distantly labeled data (D). Then, we apply
reinforcement learning (RL) to clean FPs from the noisy dataset (D). Our RL
agent is rewarded based on the change in the NER’s performance and is modeled
as a Markov Decision Process (MDP).

Algorithm 2 describes the overall training procedure for our model, and in
the following sections, we detail the various components of our model.

79


4. Named Entity Recognition in Low-Resource Domains

NER+PA

ER performance (F1

Policy Network

Tokens Sequence Selected Instances

'
'
'
1
‘
'
'
'
‘
‘
'
'
'
'
'
'
‘
'
'
'
'
'
'
‘
'
'
'
‘
'
'
'
‘
'
‘
'
'
‘
‘
‘
'
'
‘

Figure 4.4: NER+PA+RL architecture.

4.4.1. Baseline NER model

Our baseline model is a BILSTM-CRF architecture (Lample et al., 2016; Habibi
et al., 2017), which is described in Section 4.2.3. The first layer takes character
embeddings for each word sequence and then merges the output vector with the
word embedding vector to feed into a second BiLSTM layer. We modify the top
element (CRF layer) of the baseline model as follows.

4.4.2 Partial-CRF layer (PA)

As mentioned above, FN instances constitute a common problem in distantly
annotated datasets. It is caused by limited coverage of the knowledge base
resource when some of the entity mentions are not found in the resource and
followingly labeled as non-entities (’O’). We follow Tsuboi et al. (2008) and
treat the result of distant supervision as a partially annotated dataset where
non-entity text spans are annotated as any possible tag. Figure 4.5 illustrates the
annotation of distantly supervised examples using the BIOES labeling scheme
that we employ.

80


Model

B-Disease_|B-Disease |B-Disease_|B-Disease [B-Disease B-Disease |B-Disease_|B-Disease_|B-Disease__|B-Disease
|-Disease |-Disease |-Disease [Disease |-Disease |-Disease |-Disease |-Disease |-Disease |-Disease
E-Disease _|E-Disease__|E-Disease__|E-Disease__|E-Disease E-Disease__|E-Disease__|E-Disease__[E-Disease _|E-Disease
S-Disease__|S-Disease__|S-Disease__|S-Disease _|S-Disease S-Disease__|S-Disease_|S-Disease__|S-Disease__|S-Disease
B-Chemical_|B-Chemical |B-Chemical |B-Chemical_ |B-Chemical B-Chemical |B-Chemical ]B-Chemical |B-Chemical_|B-Chemical
|-Chemical_|I-Chemical_||-Chemical_|I-Chemical__|I-Chemical [-Chemical_|I-Chemical_||-Chemical_|I-Chemical_||-Chemical
E-Chemical_|E-Chemical_|£-Chemical |E-Chemical_ |E-Chemical E-Chemical_|E-Chemical |E-Chemical |E-Chemical |E-Chemical
S-Chemical_|S-Chemical_|S-Chemical_|S-Chemical_|S-Chemical S-Chemical_|S-Chemical_|S-Chemical_|S-Chemical_|S-Chemical

ie} ie) fe} ie) ie) B-Disease | E-Disease ie) fe) ie) ie) ie) S-Chemical

leprosy | developed a Heinz body hemolytic | anemia while taking a dose of dapsone

Figure 4.5: Annotation of the distantly labeled example in Partial-CRF based

on the BIOES labeling. The words with green tags are found in the dictionary

and assigned to the corresponding entity types, and the ones that are not found
in the dictionary are assigned to all possible tags (yellow).

Let Yz, denote all the possible tag sequences for a distantly supervised sentence
W. Then, the conditional probability of the subset Yr; given W is:

p(YL|W) = S° vlylW) (4.12)
yeYL

Extending the original equation of the CRF layer (Eq. 4.5) provides the log-
probability for the distantly supervised instance:

ti
yey: es(Woy")

diyey eWay!) °

Using partial annotation learning, non-entity text spans are annotated as any
possible tag. It gives a chance for non-entity text spans to be considered and
scored properly in the updated version of CRF (Partial-CRF) and become a
part of the most optimal tag sequence.

log(p(Yz|W)) = log (4.13)

4.4.3. Reinforcement Learning (RL)

The RL agent is designed to determine whether the distantly supervised instance
is a true positive or not. There are two main components in RL: (i) the
environment, and (ii) the policy-based agent. Following Qin et al. (2018), we
model the environment as a Markov Decision Process (MDP), where we add
information from the previous state to the current state. The policy-based agent
is formulated based on the Policy Gradient Algorithm (Sutton et al., 1999), as
explained in Section 4.2.4, where we update the policy model by computing the
reward after finishing the selection process for the whole training set. Algorithm
3 presents additional details of the RL strategy in our NER model. The following
subsections describe the elements of the RL agent.

81


4. Named Entity Recognition in Low-Resource Domains

Algorithm 3: Reinforcement learning Algorithm to clean FPs on D.
Input: Training dataset (At;ain) + Distantly Labeled Data (D) ,
Pre-train NER+PA on Atrain + D, Validation dataset (Aya)
1 Initialize 6 in policy network
2 Initialize s* as all-zero vector with the same dimension of s;,
3 for epochi=0— N do
4 for instance d; € D do

5 Provide s; using NER+PA model 8; =concatenation(s;, s*)

6 Randomly sample a; ~ 7(a;9,8;); compute p; = 1(a; 6, 5;), save
(aj, D5)

% if a; == 0 then

8 | save 5; into U;

9 Recompute the s* as an average of V5; € WV;

10 D,; = D— (Vd;; j € Y;)

11 Train NER+PA on Ajrain + D;

12 Calculate F? on Ay) and save F} and WU;
i3 | r1,=Fi- Fit

15 Update Policy network (Eq. 4.14)

16 Update D = D— (Vd;; j € Un)
17 Re-train NER+PA on A+ D

4.4.3.1. State

The RL agent interacts with the environment to decide about instances at the
sentence level. A central component of the environment is the current and
previous state in the selection process. The state S$; in step 7 represents the
current instances as well as their label sequences. Following Yang et al. (2018)
the state vector 5; includes:

e The vector representation of instances before the Partial-CRF layer, where
we concatenate the outputs of the first and last nodes in the BiLSTM layer
of the base NER model.

e The label sequence scores calculated by the linear encoder before the
Partial-CRF model. (i.e, Pj; in Eq. 4.6).

If a word is annotated with a specific label, the score will be the corresponding
value of the label. Otherwise, the score will be the mean of all possible word
labels in the linear encoder. These two vectors are concatenated to represent the
current state. To satisfy the MDP, the average vector of the removed instances

82


Experiments

in the earlier step 7 — 1 is concatenated to the current state and represents the
state for the RL agent.

4.4.3.2 Reward

The NER model will achieve improved performance if the RL agent filters out
the FP instances from the noisy dataset. Accordingly, the RL agent will receive
a positive reward; otherwise, the agent will receive a negative reward. Following
Qin et al. (2018), we model the reward as a change of the NER performance;
particularly, we adapt the Fl score to calculate the reward as the difference
between F'l scores of the adjacent epochs (i.e., 7; = Fi — F/~').

4.4.3.3. Policy Network

The policy network 1(a,;;;, 8;) is a feed forward network with two fully-connected
hidden layers. It receives the state vector for each distantly supervised instance
and then determines whether the instance is a false positive or not. The 7 as a
classifier with parameter @ decides an action a; € {1,0} for each s; € S;. The
loss function for the policy network is formulated based on the policy gradient
method and the REINFORCE algorithm (Section 4.2.4). Since we calculate the
reward as a difference between F'l scores in two contiguous epochs, the agent will
be compensated for a set of actions that has a direct impact on the performance
of the NER model in the current epoch. In other words, the different parts of
the removed instances in each epoch are the reason for the change in F'l scores.
Accordingly, the policy will update using the following gradient:

6=0+a(\V/ S- log 1(a;|S534)r;

0 ajz,87EQ

(4.14)
+VV So log a(aj|5;;9)(—ri)]
0 a;,8;E€Q4-1
According to Qin et al. (2018), assuming W; is removed in epoch i :
( v) (4.15)

Qy-1 = Via — (UN Y;_-1)

This means that if there is an increase in F, at the current epoch 72, we will
assign a positive reward to the instances that have been removed in epoch 7 and
not in epoch 7 — 1 and negative reward to the instances that have been removed
in epoch 7 — 1 and not in the current epoch.

4.5 Experiments

We perform experiments on four benchmark datasets to compare our method to
similar techniques and investigate the impact of the number of available annotated
sentences for our approach. In this section, we describe the experimental setup
and various components of the model.

83


4. Named Entity Recognition in Low-Resource Domains

Size Dictionary

Name Domain Entity Types (Train/Dev./'Test) Size 7 Raw Sent.
BC5CDR Bio-Medical Disease, Chemical 4,560/4,581/4,797 322,882 20,217
LaptopReview Technical Reviews Aspect terms 2,445 /609/800 13,457 15,000

E Brand, Product
EC ~Commerce Model, Material 1,200/400/800 927 2,500

(Chinese) of cca

Specification

NEWS news (Chinese) Person 3,000/3,328/3,186 71,664 3,722

Table 4.2: Overview of datasets in our experiments.

4.5.1 Datasets

Our approach requires an annotated dataset, a knowledge resource , and a corpus
of raw text. We rely on the resources used by Shang et al. (2018b) and Yang
et al. (2018) for English and Chinese, respectively, as well as their train-test
splits. As is shown in Table 4.2, these datasets are from several different domains
(biomedical, e-commerce, technical reviews, and news) as well as two different
languages. For all datasets, the distant supervision is performed on the raw
data to create a distantly annotated dataset using the knowledge resource (i.e.,
dictionary). The annotation is based on the BIOES labeling scheme. Below we
briefly describe the datasets.

BC5CDR. This dataset is from BioCreative V Chemical Disease Relation task
(Li et al., 2016) and contains 12,852 Disease and 15,935 Chemical entity mentions
in 1,500 articles. Example 4.16 shows an annotated sentence in this dataset
with the BIOES tags. The BC5CDR is already partitioned randomly into a
training, a development and a test set (500 articles each). The related dictionary
is constructed from the MeSH database® and the CTD chemical and Disease”
vocabularies and contains 322,882 Disease and Chemical entities. As raw text, we
use a corpus consisting of 20,217 sentences that is provided in Shang et al. (2018b)
and extracted from PubMed papers.

(4.16) Selegiline - induced postural hypotension in Parkinson ”
S-Chemical O O B-Disease E-Disease O- B-Disease I-Disease
8 disease

I-Disease E-Disease O

LaptopReview. The LaptopReview dataset contains laptop aspect terms taken
from the SemEval 2014 Challenge, Task 4 Subtask 1 (Pontiki et al., 2014). The
3,845 review sentences are annotated with 3,012 AspectTerm mentions (e.g., disk

Shttps://www.nlm.nih.gov/mesh/download_mesh.html
Thttp://ctdbase.org/downloads/

84


Experiments

drive). We extract 15,000 sentences from the Amazon laptop review dataset §
as raw text. Wang et al. (2011) designed this dataset for aspect-based sentiment
analysis. Thanks to Shang et al. (2018b), they provide a dictionary of 13,457
computer terms crawled from a public website °. An example sentence from the
training in the BIOES tags is shown in example 4.17.

(4.17) I love the operating system and the preloaded
oO O @) B-AspectTerm E-AspectTerm O Oo B-AspectTerm
software

E-AspectTerm O

EC. The EC dataset is a Chinese dataset from the e-commerce domain. We
choose this dataset in order to compare our results to the approach by Yang
et al. (2018). There are 5 entity types: Brand, Product, Model, Material
and Specification on user queries. An example sentence of the EC dataset is
represented in example 4.18. This corpus contains 1,200 training instances, 400
in development set, and 800 in the test set. Yang et al. (2018) provide a small
dictionary of 927 entries and 2,500 sentences as raw text.

O O O O O B-Product J-Product E-Product O
‘T want to buy a gaming computer’

(4.18)

NEWS. The NEWS dataset is another Chinese dataset from the news domain
and is annotated with Person type (PER) and provided by Yang et al. (2018),
as shown by the sentence taken from this dataset in example 4.19. The NEWS
dataset contains 3,000 sentences for training, 3,328 for development, and 3,186
for testing. Yang et al. (2018) apply distant supervision to raw data, and obtain
3,722 annotated sentences. The dataset and raw text are taken from the MSRA
corpus (Levow, 2006).

(4.19)

AA = Wil , F iA vie a
B-PER I-PER  E-PER O B-PER I-PER E-PER O
‘Committee members Wu Changzhen and Luo Hanxian ...

>

oO ot

4.5.2 Pre-trained Embeddings

The pre-trained embeddings have been used as initialization for the embedding
layer of the LSTM layers of the BiLSTM model described in Section 4.4.
Standard pre-trained GloVe 100-dimensional word vectors are employed for the
LaptopReview dataset. In our experiments on the EC dataset, we use the 100-
dimensional Chinese character embeddings provided by Yang et al. (2018), which
is trained on one million sentences of user-generated text. For the biomedical

Shttp://times.cs.uiuc.edu/~wang296/Data/
°https:/www.computerhope.com/jargon.htm

85


4. Named Entity Recognition in Low-Resource Domains

dataset, we use pre-trained 200-dimensional word vectors trained on PubMed
abstracts, all PubMed Central (PMC) articles, and English Wikipedia (Pyysalo
et al., 2013). We here employ an embedding model that is domain-specific since
we observe that this type of model provides an improvement in our previously
studied domain-specific downstream task (see Chapter 3). In addition, Wang
et al. (2019c) show that the domain-specific embeddings are beneficial for tagging
performance on the BC5CDR dataset.

4.5.3 Evaluation

We report the performance of the model on the test set as the micro-averaged
precision, recall, and F'l score. According to CoNLL-2003 (Tjong Kim Sang
and De Meulder, 2003), a predicted entity is counted as a true positive if both
the entity boundary and entity type is the same as the ground-truth (i.e., exact
match). To alleviate the randomness of the scores, the mean of five different
runs are reported.

4.5.4 Model Variants

We use slightly different variants of our model for English and Chinese. For
English we follow Liu et al. (2018b) in leveraging a language model to extract
character-level knowledge. We keep the parameters in the model the same as in
the original work. In order to compare to state-of-the-art models, we follow the
same approach during training (i.e., by merging the training and development
data as a training set in BC5CDR and randomly selecting 20% from the training
set as the development set in LaptopReview). For the Chinese EC dataset, we
only use character-based LSTM and CRF layers and discard the word-based
LSTM and language model. For a fair comparison, the model parameters are
set to be the same as in Yang et al. (2018), as well as the batch size, optimizer,
and learning rate for RL module. We use 100 epochs in RL and initialize the
average vector of the removed sentences as an all-zero vector.

4.5.5 High-Quality Phrases

Considering all non-entity spans (i.e., O’ type) as a potential entity, provides
noise in the Partial-CRF process. To address this issue, we use a set of quality
multi-word and single-word phrases, provided by Shang et al. (2018b) and
obtained using their AutoPhrase method (Shang et al., 2018a). Note that this
resource is available only for the English datasets; therefore, it is not included in
the experiments on the Chinese datasets. When using these phrases, we assign
all possible tags only for the token spans that are matched with this extended
list. In our model, we treat high-quality phrases as potential entities, and we
assign all possible entity types in the annotation of distantly supervised sentences.
For example, in Figure 4.5, we could only find the word ‘leprosy’ in this list,
therefore, in annotation, we assign all possible tags to this token, and the other
non-entity tokens remain as ’O’

86


Experiments

Model Variant Data Pr. Re. Fl

NER+PA oe 85.82 88.58 87.18
NER+PAQ S 91.28 87.07 89.13
NER+PA+RL _ 87.00 89.04 88.01
NER+PA+RLO 4 | 92.05 87.91 89.93
NER+PA az | 61.00 70.80 65.53
NER+PA® SE | 66.36 66.06 66.21
NER+PA+RL #6 | 80.47 73.70 76.94
NER+PA+RL® N= | 81.07 74.01 77.38

Table 4.3: Result with different setting of the distantly supervised NER model.
& indicates that we use the list of high-quality phrases along with the dictionary
to annotate raw text. The PA and RL denote the use of partial annotation
learning and reinforcement-based components, respectively.

Model Data Pr. Re. Fl
Liu et al. (2018b) * aia 88.84 85.16 86.96
Wang et al. (2019c) 5 89.10 88.47 88.78
Beltagy et al. (2019)** 3 - - 88.94
NER+PA+RL (This work) aa 92.05 87.91 89.93
Winner system in Pontiki et Be 84.80 66.51 74.55
al. (2014) ae

NER+PA+RL (This work) Se 81.07 74.01 77.38
Yang et al. (2018) O 61.57 61.33 61.45
NER+PA+RL (This work) oa 61.86 65.36 63.56
Yang et al. (2018) S 81.63 76.95 79.22
NER+PA+RL (This work) B 80.20 79.88 80.04

Table 4.4: NER models comparison. *: is the base NER model in our approach
and results are reported by Wang et al. (2019c). **: They use Pretrained
Contextualized Embeddings for Scientific Text (SciBERT) with an in-domain
vocabulary (ScITVOCAB) in Ma and Hovy (2016) for NER.

87


4. Named Entity Recognition in Low-Resource Domains

4.6 Performance Comparison

We investigate the impact of the different components of the model (Table 4.3) in
the two English datasets via ablation experiments, where we contrast the use of
partial annotation learning (PA) (see Section 4.4.2) and the reinforcement-based
component (RL) (see Section 4.4.3), with and without the high-quality phrases
(the high-quality phrases (@) are available only for the English datasets).

The experiments confirm the efficiency of the PA and RL modules in resolving
FN and FP issues in the distantly labeled datasets. We observe that compared
with NER+PA+RL, NER+PA+RL© obtains absolute improvements of +1.92
and +0.44 F1 points on the BC5CDR and LaptopReview datasets, respectively.
Overall, our final system (NER+PA+RL®) achieves an improvement of +2.75
and +11.85 F1 on the BC5CDR and LaptopReview respectively over the baseline
system NER+PA. The results also corroborate Shang et al. (2018b), showing
that incorporating high-quality phrases always leads to a boost in precision and,
subsequently, F1 scores.

Table 4.4 depicts the comparison of our model to the previous NER models.
We observe that our final system, the NER+PA+RL model with high-quality
phrases, achieves higher F'l scores on the different datasets compared to the other
models. In order to compare to the RL based approach in Yang et al. (2018),
we run the model without high-quality phrases on the Chinese EC and NEWS
dataset. Our design provides higher F1 scores than Yang et al. (2018), where
it boosts the reported F'1 score with +2.11 and +0.82 points on the EC and
NEWS datasets, respectively. These experiments show that the different design
of the RL module leads to improved results.

Following this work, there are some new studies on the BC5CDR and
LaptopReview datasets such as Beltagy et al. (2019) and Liu et al. (2020).
These approaches generally rely on the use of large, pre-trained language models.
Beltagy et al. (2019) achieve 90.01% F1 score in the BC5CDR by fine-tuning
ScIBERT and Liu et al. (2020) report 82.80% F1 score in the LaptopReview
using ELMO (Peters et al., 2018) trained on the corresponding dataset.

4.7 Size of Gold Dataset

In all the previous experiments, we take advantage of the availability of an
annotated dataset. However, one of the challenges in domain-specific NER is the
availability of gold supervision data. We here examine the performance of our
proposed model under settings using different sizes of human-annotated data. In
order to conduct this examination with our final method, we focus on the English
datasets because of the availability of high-quality phrases. We proportionally
select sub-samples x% € [2, 10, 20,30, 40, 50,60, 70,80, 100] from the training
data of the BC5CDR and LaptopReview (with random sampling). Figures 4.6
and 4.8 show the performances of the models trained on the selected sentences.
The X-axis is the corresponding proportions (x%) of the human-annotated
dataset, while the Y-axis is the F1 scores on the testing set. We observe that

88


Size of Gold Dataset

Method Data | Pr. Re. Fl

93.93 58.35 71.98
84.98 83.49 84.23
Shang et al. (2018b) 88.96 81.00 84.80
NER+PA+RL& 88.73 = 77.51 82.74

Dictionary Match 90.68 44.65 59.84
Giannakopoulos et al. (2017) 74.51 3141 44.37
Shang et al. (2018b) 72.27 59.79 65.44
NER+PA+RL& 68.63 56.88 62.21

Dictionary Match
Fries et al. (2017)

BC5CDR

Laptop
Review

Table 4.5: Unsupervised NER Performance Comparison. The proposed method
is trained only on distantly labeled data.

the performance of all models (including baselines) improves as more training
instances become available. However, as shown in Figures 4.6 and 4.8, the final
method (NER+PA+RLS&) achieves a performance of 83.18 and 63.50 with only
2% of the annotated dataset in the BC5CDR and LaptopReview, respectively.
Whereas the base NER. model requires almost 45% of the ground truth sentences
to reach the same performance. This indicates that with a small set of human
annotated data, our model can deliver a relatively good performance.

We further carry out experiments on the BC5CDR and LaptopReview
test sets, where our model is trained exclusively on distantly annotated data.
We report the outcome together with the scores of the other state-of-the-
art unsupervised methods in Table 4.5, where we also compare to simple
dictionary matching. It is clear that the model of Shang et al. (2018b)
(AutoNER) is still the best performing NER method on the BC5CDR and
LaptopReview datasets in an unsupervised setup. However, if we compare
the performance of our model (NER+PA+RL6& in Figure 4.6) with AutoNER
trained with both gold training and distantly labeled sentences in the BC5CDR
dataset (i-e., AutoNER-GOLD+DistantSupervision in Figure 4.7 taken from
Shang et al. (2018b)), we observe that our method provides slightly higher
performance (F'1 score) compared to the AutoNER system !° in a similar
training scenario (i.e., training with both human annotated and distantly
labeled sentences). Furthermore, comparing the performance of our model
on the LaptopReview dataset (NER+PA+RL® in Figure 4.8) with AutoNER
(i.e., AutoNER-GOLD+DistantSupervision in Figure 4.9 taken from Shang et
al. (2018b)) shows that both systems have quite similar results (i.e., F1 scores)
on this dataset. It is also worth noting that the approach proposed by Fries
et al. (2017) utilizes extra human effort to design regular expressions and requires

10The absolute F1 score is not reported in the original work. Therefore, we compare our
result with the corresponding F1 in Figure 4.7.

89


4. Named Entity Recognition in Low-Resource Domains

specialized hand-tuning.

4.8 Summary

This chapter presents an approach to alleviate the critical shortcoming of auto-
generated data in low-resource NER. We propose a performance-driven, policy-
based reinforcement learning module that removes the sentences with FPs,
whereas the adapted Partial-CRF layer deals with FNs. We examine the impact
of each component in ablation experiments. We also found that the proposed
model and methodology lead to competitive results on four benchmark datasets
from different domains and languages in a supervised setting.
To summarize, the contribution of our model is three-fold. Concretely, we:

(i) combine the Partial-CRF approach with performance-driven, policy-based
reinforcement learning to clean the noisy, distantly supervised data for
low-resource NER in a pre-processing step;

(ii) formulate the reward function in RL based on the change in the performance
of the NER module where the policy of RL is trained in an unsupervised
manner by interaction with the environment;

(iii) show that our approach can boost the neural NER system’s performance
on four datasets from different domains and for two different languages
(English and Chinese).

90


Summary

BC5CDR
90
85
&
8 80
aa
Cc
fo}
ct
“75
i ---- NER
Ui
' — NER+PA
70 | —-— NER+PA
H voles NER+PA+RL
' —— NER+PA+RLO
60 70 80 90 100

653 10 20 30 40 50
x% of training samples

Figure 4.6: Performance of the different configuration of our model trained on
various sizes of annotated dataset in the BC5CDR. F1 Score on Test vs, the

percentage of human annotated sentences.

me
©
r)

Test Fl Scores
lo)
N
ul

0.70 — AutoNER-Gold+DistantSupervision
---- Supervised Benchmark
— AutoNER-DistantSupervision

10000 15000 20000
# of Distantly Labeled Training Sentences

0 5000

Figure 4.7: Test F1 score vs. the number of distantly supervised sentences
in the BC5CDR dataset. The supervised benchmarks with 86.96 F1 score, is
LM-LSTM-CRF (Liu et al., 2018b) trained with all human-annotate sentences
(NER in our experiment). AutoNER-DistantSupervision is the AutoNER model
(Shang et al., 2018b) trained on the selected sentences from distantly labeled

data. AutoNER-Gold+DistantSupervision is the AutoNER model trained on
both human-annotated and selected distantly labeled sentences.

91


4. Named Entity Recognition in Low-Resource Domains

LaptopReview

80
70
+ 60
o
Be
Cc
fe)
ct
50
---- NER
— NER+PA
40 —-— NER+PAQ
votes NER+PA+RL
—— NER+PA+RLO
30

2 10 20 30 40 50 60 70 80 90 100
x% of training samples

Figure 4.8: Performance of the different configuration of our model trained on
various sizes of annotated dataset in the LaptopReview. F1 Score on Test vs,
the percentage of human annotated sentences.

0.8
0.7
0.6

0.5

Test Fl Scores

0.4

— AutoNER-Gold+DistantSupervision
---- Supervised Benchmark
— AutoNER-DistantSupervision

0.3

0 1000 2000 3000 4000
# of Distantly Labeled Training Sentences

Figure 4.9: Test F1 score vs. the number of distantly supervised sentences in
the LaptopReview dataset. The supervised benchmarks with the scores of 74.55
is the score of the winner system in the SemEval2014 Challenge Task 4 Subtask
1 (Pontiki et al., 2014). AutoNER-DistantSupervision is the AutoNER model
(Shang et al., 2018b) trained on the selected sentences from distantly labeled
data. AutoNER-Gold+DistantSupervision is the AutoNER model trained on
both human-annotated and selected distantly labeled sentences.

92


Chapter 5
Low-Resource Relation Extraction

Relation extraction is the next step following entity detection in the information
extraction pipeline, where semantic relationships are extracted from an input text.
Extracted relationships usually occur between two or more named entities (see
Section 4.2 in Chapter 4) and are classified based on a set of predefined semantic
categories. Relation extraction allows us to acquire structured knowledge from
unstructured text as explained in Section 2.6 of Chapter 2. In this chapter,
we focus on relation extraction in a low resource setting, namely the genre
and domain of scientific papers in NLP. We study the effect of varying input
representations to a neural architecture, specifically CNN (see Section 2.2.2 of
Chapter 2), to extract and classify semantic relations between entities in scientific
papers. We investigate the effect of transfer learning using domain-specific word
embeddings in the input layer and go on to provide an in-depth investigation of
the influence of different syntactic dependency representations which are used to
produce dependency paths between the entities in the input to the system. We
compare the widely used CoNLL, Stanford Basic, and Universal Dependencies
schemes and further compare them with a syntax-agnostic approach. In order
to gain a better understanding of the results, we perform manual error analysis.

5.1 Introduction

Over the past years, natural language technology has been used increasingly in
computational research for humanities and sciences. It provides an intelligent
way for search engines to access scientific literature, and it enables the search
engines to respond to complex queries such as Which papers address a problem
using a specific method, or What materials or resources have been utilized for
a specific problem in the articles? One of the critical elements of this type of
technology is relation extraction and classification.

The neural advances in the NLP field, as explained in Section 2.2 of Chapter
2, challenge long-held assumptions regarding system architectures. The classical
NLP systems, where components of increasing complexity are combined in a
pipeline architecture, are being challenged by end-to-end architectures trained on
distributed word representations to directly produce different types of analyses
traditionally assigned to downstream tasks. Syntactic parsing has been viewed
as a crucial component for many tasks aimed at extracting various aspects of
meaning from text, but recent work challenges many of these assumptions. For
the task of semantic role labeling, for instance, systems that make little or no
use of syntactic information, have achieved state-of-the-art results (Marcheggiani
et al., 2017). For tasks where syntactic information is still viewed as useful, a
variety of new methods for the incorporation of syntactic information have been

93


5. Low-Resource Relation Extraction

employed, such as recursive models over parse trees (Ebrahimi and Dou, 2015;
Socher et al., 2013c), tree-structured attention mechanisms (Kokkinos and
Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various
types of syntactically aware input representations, such as embeddings over
syntactic dependency paths (Xu et al., 2015b).

In this chapter, we continue this line of work and present a system based on
a CNN architecture over the shortest dependency paths combined with domain-
specific word embeddings to extract and classify semantic relations in scientific
papers. We investigate the use of different syntactic dependency representations
in a neural relation classification task and compare the widely used CoNLL,
Stanford Basic, and Universal Dependencies schemes. We further compare
with a syntax-agnostic approach and perform an error analysis to gain a better
understanding of the results. Accordingly, the contributions of this chapter lie
in investigating the following research questions:

RQ 5.1. Are domain-specific input representations beneficial for relation extrac-
tion task?

RQ 5.2. What is the impact of syntactic dependency representations in low-
resource neural relation extraction?

RQ 5.3. Which kind of syntactic dependency representation is most beneficial
for neural relation extraction and classification?

5.2 Previous Work

Relation extraction and classification can be defined as follows: given a sentence
where entities are manually annotated, we aim to identify the pairs of entities that
are instances of the semantic relations of interest and classify them based on a
pre-defined set of relation types. Different approaches have been applied to solve
the task of relation extraction and classification in previous work. The traditional
studies mainly focus on feature-based methods. Almost all systems submitted to
SemEval 2010 task 8 | (Hendrickx et al., 2010), used either Maximum Entropy
or Support Vector Machine classifiers. These systems made use of contextual,
lexical, and syntactic features combined with richer linguistic and background
knowledge such as WordNet and FrameNet (Hendrickx et al., 2010; Rink and
Harabagiu, 2010).

The re-emergence of neural networks provides a way to develop highly
automatic features and representations to handle complex interpretation tasks.
These approaches have yielded impressive results for many different NLP tasks.
In the relation classification task, the use of deep neural networks has been
investigated in several studies (Socher et al., 2012; Lin et al., 2016; Zhou et
al., 2016). There are three widely used deep neural networks (DNNs) architectures
(see Section 2.2 of Chapter 2 for more details) used for relation extraction:

1SemEval 2010 task 8: Multi-Way Classification of Semantic Relations Between Pairs of
Nominals

94


SemEval 2018 Task 7

Convolutional neural networks (CNNs), Recurrent neural networks (RNNs), and
hybrid models which combine these two types of models. Recently, pre-trained
language model, as explained in Section 2.5.1.2 of Chapter 2, such as BERT
(Devlin et al., 2019) is used for several tasks in scientific text. For example
ScIBERT (Beltagy et al., 2019) leverages BERT on a large multi-domain corpus
of scientific publications to improve performance on downstream scientific NLP
tasks. Subsequently, Wang et al. (2019a) and Jiang et al. (2020) employed BERT
and SCIBERT for the scientific relation classification task.

In previous work, CNNs have been effectively applied to extract lexical and
sentence level features for relation classification (Zhang and Wang, 2015; Lee
et al., 2017; Nguyen and Grishman, 2015). Sentences or the context between
two target entities are used as input for the CNNs. Such representations suffer
from irrelevant sub-sequences or clauses when target entities occur far from each
other, or there are other target entities in the same sentence. To avoid negative
effects from irrelevant chunks or clauses and capture the relation between two
entities, the researchers proposed methods that can embed syntactic tree features
within a neural architecture. The shortest dependency path (sdp) between two
entities is frequently used for relation classification. The sdp between two
entities in the dependency graph captures a condensed representation of the
information required to assert a relationship between two entities (Bunescu and
Mooney, 2005). Xu et al. (2015a), Liu et al. (2015) and Xu et al. (2015b) employ
DNNs such as CNNs and RNNs to learn more robust and effective relation
representations from the sdp between two entities. Their experiments on the
SemEval 2010 relation classification data set show that sdp can be a valuable
resource for relation classification by covering highly relevant information of
target relations.

Dependency representations have by now become widely used representations
for syntactic analysis, often motivated by their usefulness in downstream
applications. There is currently a wide range of different types of dependency
representations in use, which vary mainly in terms of choices concerning syntactic
head status. Some previous studies have examined the effects of these choices in
various downstream applications (Miyao et al., 2008; Elming et al., 2013). Most
recently, two Shared Tasks on Extrinsic Parser Evaluation (Oepen et al., 2017;
Fares et al., 2018) were aimed at providing better estimates of the relative
utility of different types of dependency representations and syntactic parsers for
downstream applications. However, the downstream systems in this previous
work have been limited to traditional (non-neural) systems, and there is still a
need for a better understanding of the contribution of syntactic information in
neural downstream systems.

5.3 SemEval 2018 Task 7

In this chapter, we employ the data sets released for the SemEval 2018 task 7
(Gabor et al., 2018), which encode relation instances between scientific concepts.
The relations belong to a set of semantic categories that are related to the science

95


5. Low-Resource Relation Extraction

domain, and their instances are frequently used in abstracts and introductions of
scientific articles. The shared task provides systematic evaluation steps that are
essential for complete information extraction from scientific text. The concepts
represent domain entities specific to the scientific discipline of Natural Language
Processing (NLP). The task consists of three sub-tasks, where the first two
sub-tasks are dedicated to the classification of relation instances, and the last
one is devoted to the full task of extracting the relation instances, as well as
classifying them. Our system participated in this task and ranked third (out of
28) participants in the overall evaluation.

The data that is provided in the task is extracted from the abstract section of
scientific papers from the ACL Anthology corpus (Gabor et al., 2016). Each sub-
task makes available 350 annotated abstracts and 150 abstracts as training
and test data, respectively. The training and test data sets contain pre-
annotated domain entities. Furthermore, the relation instances along with
their directionality, are provided in both the training and the test data sets of the
classification sub-tasks. The test data provided for the extraction sub-task, on
the other hand, does not contain the relation instances. Below, we will describe
the sub-tasks in more detail.

5.3.1. Relation classification scenario

The task of relation classification on this data set is to predict the semantic
relation between a given pair of entities within the abstract of a scientific
paper. The semantic relation set contains five asymmetric relations (USAGE,
RESULT, MODEL-FEATURE, PART_WHOLE, TOPIC) and one symmetric
relation (COMPARE). Each abstract in the training dataset contains pairs
of entities that are assigned to one of these six relations. Table 5.1 shows the
semantic relation typology of the six major relation types and their definitions,
along with some example entity pairs.

There are two sub-tasks in this relation classification scenario: classification
on clean data and classification on noisy data.

Sub-task 1.1: Relation classification on clean data: The entities and
corresponding relations have been manually annotated in the training and
test dataset. The test dataset contains the unlabeled relation instances, and the
task is to predict the label for each entity pair. For example in the text snippet
in example 5.1, the relation instance holds between two entity identifiers, i.e.,
(P05-1057.8, P05-1057.4) and the relation label should be predicted as : USAGE
(P05-1057.8, P05-1057.4).

(5.1) <entity id=’P05-1057.3’> All knowledge sources </entity>
are treated as <entity id=’P05-1057.4’> feature functions </entity>

Sub-task 1.2: Relation classification on noisy data: In sub-task 1.2 the
entities have been automatically annotated based on a combination of terminology
extraction (Saffron Knowledge Extraction Framework (Bordea et al., 2013)) and

96


SemEval 2018 Task 7

‘((810Z) ‘Te Je Joqey :eornog)
ssoooid UoryeyouUR UI posh ore yey} SoUO pouTeIs IOUY v WOIZ SUOTYLTEI VsIeOD oY} pue ABoTOdAy UOTYRIAI IYURUIAG :T°C BIqRT,

prepureys ‘4[nso1 yuoutIodxe ‘yNsel :ZNYY 0} poreduioo yuouIodxo ‘ynsoel : TONY oreduio09
‘AYIYUO Ioyjoue 0} poreduiod st AWN UY = NOSTNVdINOO

yooods - yoreesol uouswmousyd :ZOYyy Be jo siskveue : OU Apnys
poyjeut - 1aded Repl Ue :ZONYyp syuosoid zoyjyne/soded : :;OYV esodoid
O10} SIT YIM YIOM OYIJUOLOS B SoyepaI AIOBO4VO SIU_T, JIdOL

4X0} - suoIssoidxo yxoquod :ZNYp Ul punoj uouowouoyd v ‘AqYUS : :TOYV uousutousyd
UIVULOp - dSpoT MOU eyep JO puly :ZNYYP Wo popoesyxo UOTeULIOJUL :THNYV ao1noseyep

sydoouo9 - ASoTO}UO

eByep :ZNYV oomosos/osequyep :[HYV
“drysuorjepor opoym-y7red & UI ore SoryTyUT

jo posodutoo
ATOHM LYVd

Sp1IOM - SOLI0807e9
sourere}yn - Wore oIdI04UT
S}JUON}IYJSUOd - Iop1o

AYO :ZNYY Ue OF pozeposse UOT}eULIOFUI-eyour/3e} :THNYV
AYLJWO PoATosqo :ZHNYP ue jo uoryeyuosaidos yourysqe :THNYV
AYLYUO NYP Poasosqo ue Jo SorsoyoeV1ey Poarlosqo :[HNYUV
‘AVIYUS IOYJOUR JO [OPOUL JORIYSG IO OIYSLIOpORIeYS OIYATeUR B ST AQIS UY

3ey
Jopout
reyp
TAGCOW

soueuLojiod - rosred
souequos - AJINSIqure
eouRULIOJIed - 1ap.10

ynsel :-ZDYyy poyjour/yuoutttodxo :
yseq/ppy :ZOYUV e Ul ule_qoid v st uouswWoUDYd :TOYy
synsel :ZONYV eyep jo Aysodoid oytoeds : Nyy

‘TOUV

‘q[Nsol B Spelt 10 syooHe AYYUO UY

spjors
wotqoid
spooye
LINSdad

souoques - osred
esourder - ur1oysAs JJ
sutsied - yoroidde
Jopouw - yovoidde

eyep :ZOYyp uo pousojsod ysey
eqyep “NOUV 0} pordde poyjout

Ysed :GOYYP woysds‘poyyour

poyjou IoyJO :ZONYP uloysAs ‘poyyour
“SUOT}LTOI OSeSsN Aq PoYUI] oe VAVp pur ‘syse} ‘sporty

‘TOU
‘TOU
‘TOU
‘TOU

eyep uo ysey
eyep uo posn
ysey IO} posn
Aq posn
AOVSN

ajdwuexy

uoryeue[dxy

add, UOTPETEYy

mR
o>)


5. Low-Resource Relation Extraction

available ontological resources such as WordNet (Miller, 1995) and BabelNet
(Navigli and Ponzetto, 2012). Therefore, the dataset contains a fair amount
of noise (verbs, irrelevant words). The terms include high-level terms (e.g.
algorithm, paper, method) and are not always full NPs. The example sentence
in 5.2 shows an instance from the test dataset for this sub-task, where we
observe the noisy entity assignments which incorrectly predicts an entity label
for challenging.

(5.2) Morphological <entity id=’N06-1042.14’> ambiguity </entity>
(e.g. lives = live+s or life+s) is a <entity id=’N06-1042.15’> challenging
</entity> <entity id=’N06-1042.16’> problem </entity> for agglutinative
<entity id=’N06-1042.17’> languages </entity>

The relation instance in example 5.2 is (N06-1042.14, N06-1042.17) and the task
is to predict the label as: MODEL-FEATURE (N06-1042.14, NO6-1042.17).

5.3.2 Relation extraction and classification scenario

Given an abstract and pre-annotated entities, the goal of the extraction and
classification task is: 1) To find entity pairs that are in a relation 2) To predict the
relation label (as in the classification sub-tasks) and its direction. The training
data contains manually annotated entities and labeled semantic relations that
hold between these along with the directionality of the relation. The dataset is
identical to the one provided for sub-task 1.1. In the test dataset, only abstracts
and annotated entities are given, and participants are asked to predict the entity
pairs, their relation types, and the direction of the relations. For instance, in
the following sentence in example 5.3, the entity pairs (H01-1001.5, H01-1001.7,
REVERSE) and (H01-1001.9, H01-1001.10) should be identified and classified
with the USAGE label.

(5.3) Traditional <entity id=’H01-1001.5’> information retrieval </entity> use a
<entity id=’H01-1001.6’> histogram </entity> of <entity id=’H01-1001.7’>
keywords </entity> as the <entity id=’H01-1001.8’> document representation
</entity> but <entity id=’H01-1001.9’>oral communication</entity> may
offer additional <entity id=’H01-1001.10’> indices </entity> such as the time
and place of the rejoinder and the attendance.

5.4 Evaluation Metrics

Following the SemEval 2018 task 7 (Gabor et al., 2018), each sub-task is evaluated
differently. For sub-tasks 1.1 and 1.2, which are classification tasks, the following
evaluation metrics are used:

e Relation class-wise: Precision, recall, and Fl-measure (S=1) for each
semantic relation label.

e Global: Macro-average and Micro-average F1 scores evaluated for every
distinct relation label.

98


System Design

For sub-task 2, evaluation is conducted for each step (i-e., Extraction and
Classification). The quality of the extraction step is evaluated based on the
standard measures of Precision, Recall, and F1, where the label and directionality
of the relations are ignored in the calculation. In the classification step, the
same evaluation metrics as sub-task 1.1 and 1.2 are employed. However, only
correctly connected entities with correct directions (when relevant) and labels
are considered as a correct instance. Here, we report the official scores of each
experiment’s task, i.e., for the classification tasks, we report Macro-average F1,
and we report the F1 score for the extraction task.

5.5 System Design

In this section, we describe the various components of our system. We introduce
the input data’s specifics in terms of pre-processing, label encoding, and word
embeddings and further introduce the architecture of our CNN system for relation
extraction and classification.

5.5.1 Dataset preparation

For each relation instance, in the training data set, the sentence containing the
participant entities is considered as a text representation of the relation instance.
Therefore, if two relations appear in one sentence, they will have the same text
representation. Sentence and token boundaries are detected using the Stanford
CoreNLP tool (Manning et al., 2014). Since most of the entities are multi-word
units, in order to obtain a precise dependency path between entities, we replace
the participant entities in the relation instance with their codes prior to parsing.
The example sentence in 5.4 below is thus transformed to (5.5).?

(5.4) All knowledge sources are treated as feature functions.
(5.5) All P05_1057_3 are treated as P05_1057_4.

Given an encoded sentence, we obtain the shortest dependency path connecting
two target entities for each relation instance using a syntactic parser, see below.

For syntactic parsing, we employ the parser described in Bohnet and
Nivre (2012), a transition-based parser that performs joint PoS-tagging and
parsing. We train the parser on the standard training sections 02-21 of the Wall
Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The
constituency-based treebank is converted to dependencies using two different
conversion tools: (i) the pennconverter software? (Johansson and Nugues, 2007),
which produces the CoNLL dependencies’, and (ii) the Stanford parser using

?Preliminary results showed that this replacement technique improved results for relation
extraction classification.

$http://nlp.cs.|th.se/software/treebank-converter/

4The pennconverter tool is run using the rightBranching=false flag.

99


5. Low-Resource Relation Extraction

either the option to produce basic dependencies ° or its default option which is
Universal Dependencies v1.3°. The parser achieves a labeled accuracy score of
91.23 when trained on the CoNLLO8 representation, 91.31 for the Stanford basic
model, and 90.81 for the UD representation, when evaluated against the standard
evaluation set (section 23) of the WSJ. We acknowledge that these results are
not strictly speaking state-of-the-art parse results for English. However, the
parser is straightforward to use and re-train with the different dependency
representations. We also compare to another widely used parser, namely the
pre-trained parsing model for English included in the Stanford CoreNLP toolkit
(Manning et al., 2014), which outputs Universal Dependencies only. However, it
was clearly outperformed by our version of the Bohnet and Nivre (2012) parser
in the initial development experiments.

Based on the dependency graphs output by the parser, we extract the shortest
dependency path connecting two entities. The path records the direction of arc
traversal using left and right arrows (i.e., + and —) as well as the dependency
relation of the traversed arcs and the predicates involved, following Xu et
al. (2015a). The entity codes in the final path are replaced with the corresponding
word tokens at the end of the pre-processing step. For the sentence in (5.4) and
the two entities knowledge sources and feature functions we thus extract the
path in (5.6) below.

(5.6) knowledge sources + SBJ + are + VC — treated + ADV > as ~ PMOD >
feature functions

Since the related entity pairs and the relation types are provided for the full
dataset, we extend the dataset for sub-task 1.1 and 2 by extracting the related
entities and their corresponding sdp from the sub-task 1.2 dataset. In order to
train a model for sub-task 2, we also augment the dataset by extracting NONE
relation instances (see Section 5.5.2), from the corresponding dataset. Table 5.2
shows the number of instances for each relation class. As we can see, the class
distribution is clearly unbalanced.

5.5.2 Label encoding

The classification sub-tasks contain five asymmetric and one symmetric classes
(see Section 5.3.1). The relation instances, along with their directionality, are
provided in both the training and the test data sets. For these sub-tasks, we
therefore use the same labels in our system. For sub-task 2, which combines the
extraction and classification tasks, however, we construct an extra set of relation
types. First, we collect every pair of entities within a single sentence that are
not included in the annotated relation set. To minimize the noise, we retain
only the entity pairs which are not further away than 6 tokens. From these

>The Stanford parser is run using the -basic flag to produce the basic version of Stanford
dependencies.

6Note, however, that the Stanford converter does not produce UD PoS-tags, but outputs
native PTB tags.

100


System Design

Sub-task Reverse
Relation 1.1&2 1.2 False True Total
USAGE 483 464 615 332 947
MODEL-FEATURE 326 172 346 152 498
RESULT 72 121 135 58 193
TOPIC 18 240 235 23 258
PART WHOLE 233 192 273 152 425
COMPARE 95 Al 136 - 136

NONE 2315 - 2315 - 2315

Table 5.2: Number of instances for each relation in the final dataset.

entity pairs, we generate negative instances with the NONE class and extract
the corresponding sdp. Second, to preserve the directionality in the asymmetric
relations, we add the — symbol to the instances with reverse directionality
(e.g., USAGE(el,e2,REVERSE) becomes ~USAGE(e1,e2)). The final label set for
sub-task 2 thus consists of 12 relations.

5.5.3 Word embeddings

In our system, following the sequential transfer learning of word embeddings (see
Section 2.5.1 in Chapter 2), two different sets of pre-trained word embeddings are
used for initialization. One is the 300-d pre-trained embeddings provided by the
NLPL repository ‘(Fares et al., 2017), trained on English Wikipedia data with
word2vec (Mikolov et al., 2013a), here dubbed wiki-w2v. In Chapter 3, we saw
that domain-specific embeddings perform better compared to the general domain
embeddings trained on much larger input data. Therefore, we train a second set
of domain-specific embeddings on the ACL Anthology corpus. We obtain the
XML versions of 22,878 articles from ACL Anthology °. After extracting the
raw texts, for training of the 300-d word embeddings (acl-w2v), we exploit the
available word2vec (Mikolov et al., 2013a) implementation gensim (Rehtiek and
Sojka, 2010) for training.

5.5.4 Classification Model

At the time of the SemEval task, the dominant approaches in relation extraction
using the shortest dependency path as an input representation generally involve
a CNN architecture. We design our system based on a CNN architecture similar
to the one used for sentence classification by Kim (2014) (see Section 2.2.2 in

Thttp://vectors.nlpl.eu/repository/
Shttps://acl-arc.comp.nus.edu.sg/

101


5. Low-Resource Relation Extraction

All km@wieadge"Seurees are treated as Feature’ Finetions

|e dependency path between two entities

knowledge sources « SBJ « are + VC > treated = ADV = as + PMOD = feature functions

look-up

Figure 5.1: Model architecture with two channels for an example shortest
dependency path.

Chapter 2 and also employed for sentence classification in Chapter 3). Figure
5.1 provides an overview of the proposed model. It consists of 4 main layers as
follows:

1. Look-up Table and Embedding layer: In the first step, the model

102

takes the shortest dependency path (i.e., the words, dependency edge
directions, and dependency labels) between entity pairs as input and maps
it into a feature vector using a look-up table operation. Each element of the
dependency path (i.e., word, dependency label, and arrow) is transformed
into an embedding layer by looking up the embedding matrix M ¢ R®®Y,
where d is the dimension of the CNN embedding layer, and V is the size of
the vocabulary. Each column in the embedding matrix can be initialized
randomly or with pre-trained embeddings. The dependency labels and edge
directions are always initialized randomly and fine-tuned during model
training.

Convolutional Layer: The next layer performs convolutions with ReLU
activation over the embeddings using multiple filter sizes, and extracts
feature maps.

Max pooling Layer: By applying the maz operator, the most effective
local features are generated from each feature map.

Fully connected Layer: Finally, the higher-level syntactic features are
fed to a fully connected softmax layer, which outputs the probability
distribution over each relation.


Initial Experiments

5.6 Initial Experiments

In an initial round of experimentation, we assess the influence of different word
embedding models for our task. Specifically, we contrast the use of general
domain embeddings with domain-specific word embeddings. We also assess
the use of a two-channel architecture (see Section 2.2.2 in Chapter 2) for the
incorporation of pre-trained word embeddings in our model.

5.6.1 Model settings

In the initial experiments, we keep the value of the model hyper-parameters
equal to the ones that are reported by Kim (2014), i.e., 128 filters for each
window size, a dropout rate of p = 0.5 and [2 regularization of 3. To deal with
the effects of class imbalance, we weight the cost by the ratio of class instances.
Thus each observation receives a weight, depending on the class it belongs to.
The effect of the minority class observations is thereby increased simply by a
higher weight of these instances and is decreased for majority class observations.
Furthermore, to guarantee that each fold in n-fold cross-validation will have the
same distribution of classes during training, development, and test, we apply
the stratification technique proposed by Sechidis et al. (2011). We use the
development set to detect when overfitting starts during the training of our
model; using early stopping, training is then stopped before convergence to avoid
overfitting (Prechelt, 2012). As described above, the official evaluation metric
is the macro-averaged F1-score. Therefore we implement early-stopping with
patience= 20 (i.e., the number of epochs to wait before early stop if no progress
on the development set) based on the macro-F 1 score in the development set.

5.6.2 Model variants

We run experiments with several variants of the model. In particular, we
here contrast the use of pre-trained (general vs. domain-specific) and randomly
initialized word embeddings in the input layer, and the use of one or two channels
(see Section 2.2.2 in Chapter 2). Specifically, we compare the following model
variants:

e cnn.rand: A baseline model, where all elements in the embedding layer
are randomly initialized and updated in the training process.

e cnn.wiki-w2v: The embedding layer is initialized with the pre-trained
Wikipedia word embeddings and fine-tuned for the target task.

e cnn.acl-w2v: The embedding layer is initialized with the pre-trained ACL
Anthology word embeddings and fine-tuned for the target task.

e cnn.multi.rand: There are two embedding layers as a ’channel’ in the
CNN architecture. Both channels are initialized randomly, and only one of
them is updated during training while the other remains static.

103


5. Low-Resource Relation Extraction

Sub-task

Model 11 1.2 2

Ext. Class.
cnn.rand 68.86 73.47 72.33 54.62
cnn. wiki-w2v 70.14 74.20 72.50 54.20
cnn.acl-w2v 72.74 75.69 72.74 57.56
cnn.multi.rand 68.30 74.11 (2256 55.16
cnn.multi.wiki-w2v 68.07 75.01 72.59 55.30
cnn.multi.acl-w2v 72.85 75.83 72.63 55.45
cnn.multi.wiki-w2v.rand 69.85 75.58 72.70 56.69
cnn.multi.acl-w2v.rand 73.06 76.36 72.05 56.99

Table 5.3: F1.(avg. in 5-fold) scores for different model setting during training.

e cnn.multi.wiki-w2v: Same as before, but the channels are initialized
with Wikipedia embedding vectors.

e cnn.multi.acl-w2v: The two channels are initialized with ACL embed-
ding vectors.

e cnn.multi.wiki-w2v.rand: First, the channel is initialized with
Wikipedia embeddings in static mode and the second initialized randomly
with a non-static mode.

e cnn.multi.acl-w2v.rand: Same as previous setting, but the first channel
makes use of ACL embeddings.

5.6.3 Results

During development, we first investigated the performance of different model
variants (see Section 5.6.1) using the Universal Dependency representation output
by the Stanford CoreNLP toolkit; by running 5-fold cross-validation. The data
set is split into five folds. In the first iteration, the first fold is used to test the
model, and the rest is used to train the model (i.e., three folds for training and
one fold for development set to perform early stopping). In the second iteration,
the second fold is used as the testing set, while the rest serve as the training
set. This process is repeated until each fold of the five folds has been used
as the testing set. The experiments (Table 5.3) show that the multi-channel
mode performs better only in the classification sub-tasks compared to the single-
channel setting. The use of the pre-trained embeddings helps the model in class
assignments. Notably, the domain-specific embeddings (i.e., acl-w2v) provide
higher performance gains when used in the model.

104


Initial Experiments

Model/Sub-task

Representation cnn.multi.acl-w2v.rand = cnn.acl-w2v
1.1 1.2 8
Ext. Class.
Stanford Basic 74.16 77.70 72.91 58.11
CoNLLO08 72.65 76.83 74.26 60.31
UD v1.3 69.55 76.60 71.09 54.53
UD (Stanford CoreNLP) 73.06 76.36 72.74 57.56

Table 5.4: Fl.(avg. in 5-fold) scores for different dependency representation
during training.

Further, we experiment with the selected configuration for each task using
different dependency representations to produce the shortest paths between
entities. Table 5.4 presents the F1l-score of each dependency representation for
each sub-task via 5-fold cross-validation on the training data. In the evaluation
period, we re-run 5-fold cross-validation using the selected model for each sub-
task. However, in this setting we use four folds as training and one fold as a
development set, and we apply the output model to the evaluation dataset. The
results indicate that the Stanford Basic scheme performs best in the classification
subtask, whereas the CoNLL representation provides the highest result in the
full extraction task.

The comparison of different syntactic representations is potentially problem-
atic; however, given that the default hyper-parameters may favor one of the
representations simply by chance. Ideally, the hyper-parameters should be tuned
for each dependency representation in turn to enable a fair comparison. In the
next sections, we apply Bayesian Optimization to tune our hyper-parameters and
provide an analysis of the influence of syntax and various syntactic dependency
representations in our system.

5.6.4 Participating systems and results in SemEval 2018

The SemEval 2018 task attracted 32 participants. The subtask 1.1, subtask
1.2, and subtask 2 received around 28, 19, and 11 participants, respectively.
The DNNs methods, including CNNs and LSTMs were widely used by the
participating teams. Only five teams applied non-neural approaches such as
Support Vector Machines (SVM) (Gabor et al., 2018).

We select the first (1st) and second (2nd) best performing models on the
development datasets as well as the majority vote (mv) of 5 models for the final
submission. The overall results of our system, as evaluated on the SemEval 2018
shared task dataset are shown in Table 5.5.

105


5. Low-Resource Relation Extraction

1st 2nd mv
Sub-task Ext. Class. Ext. Class. Ext. Class.
1.1 - 72.1 - 74.7 - 76.7
1.2 = 83.2 - 82.9 = 80.1

2 37.4 33.6 36.5 28.8 35.6 28.3

Table 5.5: Official evaluation results of the submitted runs on the test set.

1.1 1.2 ee
Ext. Class.
Baseline (Gabor et al., 2018) 34.4 53.5 26.8 12.6
ETH-DS3Lab (Rotsztejn et al., 2018) 81.7 90.4 48.8 49.3
UWNLP (Luan et al., 2018) 78.9 - 50 39.1
Talla (Pratap et al., 2018) 74.2 848 - -
SIRIUS-LTG-UiO (Our system) 76.7 83.2 374 33.6
Entity-Aware BERT,, (Wang et al., 2019a) 81.4 - - -
MRC-ScIBERT (Jiang et al., 2020) 80.5 - - -

Table 5.6: Results on SemEval Task 2018 Task 7 (Gabor et al., 2018).

Our system ranks third in all three sub-tasks of the shared task (Gabor
et al., 2018). We compare our system to the baseline and the winning systems in
Table 5.6. We also report the most recent works on the SemEval 2018 dataset,
Wang et al. (2019a) and Jiang et al. (2020), where the pre-trained transformers
such as BERT (Devlin et al., 2019) and ScrBERT (Beltagy et al., 2019) have
been exploited in the relation classification task.

5.7 Syntactic Dependency Representations

In this section, we further examine the use of syntactic representations as input
to our neural relation classification system. We hypothesize that the shortest
dependency path as a syntactic input representation provides an abstraction that
is somehow domain independent. Therefore, the use of this type of structure
may lessen domain effects in low-resource settings. We quantify the influence of
syntactic information by comparing to a syntax-agnostic approach and further
compare different syntactic dependency representations that are used to generate
embeddings over dependency paths.

106


Syntactic Dependency Representations

All knowledge sources are treated as feature functions
(a) CoNLL Dependencies

det

| eae

All knowledge sources are treated as feature functions
(b) SB: Stanford Basic Dependencies

nmod

det nsubjpass
[ [

All knowledge sources are treated as feature functions

(c) UD: Universal Dependencies

Figure 5.2: Dependency representations for the example sentence.

5.7.1 Dependency representations

Figure 5.2 illustrates the three different dependency representations we compare:
the so-called CoNLL-style dependencies (Johansson and Nugues, 2007) which
were used for the 2007, 2008, and 2009 shared tasks of the Conference on
Natural Language Learning (CoNLL), the Stanford ‘basic’ dependencies (SB)
(de Marneffe et al., 2006) and the Universal Dependencies (v1.3) (UD; McDonald
et al. (2013); de Marneffe et al. (2014); Nivre et al. (2016)). We see that the
analyses differ both in terms of their choices of heads vs. dependents and the
inventory of dependency types. Where CoNLL analyses tend to view functional
words as heads (e.g., the auxiliary verb are), the Stanford scheme capitalizes
more on content words as heads (e.g., the main verb treated). UD takes the
tendency to select contentful heads one step further, analyzing the prepositional
complement functions as a head, with the preposition as itself as a dependent
case marker. This is in contrast to the CoNLL and Stanford scheme, where the
preposition is head.

107


5. Low-Resource Relation Extraction

best F1 (in 5-fold)

Relation without sdp with sdp Diff.
USAGE 60.34 80.24 + 19.90
MODEL-FEATURE 48.89 70.00 + 21.11
PART_WHOLE 29.51 70.27 +40.76
TOPIC 45.80 91.26 +45.46
RESULT 54.35 81.58  +27.23
COMPARE 20.00 61.82 + 41.82
macro-averaged 50.10 76.10  +26.00

Table 5.7: Effect of using the shortest dependency path on each relation type in
sub-task 1.1 (see Section 5.3.1).

5.7.2 Experiments

We run all the experiments with the multi-channel setting described above ° in
which the first channel is initialized with the pre-trained ACL embeddings in
static mode (i.e., it is not updated during training) and the second channel is
initialized randomly and is fine-tuned during training (non-static mode). The
macro F1-score is measured by 5-fold cross-validation and, once again, to deal
with the effects of class imbalance, we weight the cost by the ratio of class
instances; thus, each observation receives a weight, depending on the class it
belongs to.

5.7.3 Assessing the effect of syntactic information

To evaluate the effects of syntactic information in general for the relation
classification task, we compare the model’s performance with and without the
dependency paths. In the syntax-agnostic setup, a sentence that contains the
participant entities is used as input for the CNN. In addition to the word
embeddings, to specify the position of each entity pair, we also use position
embeddings for all words in the sentence. The position embeddings encode
the relative distances of each word to the entity mentions. We here keep the
value of hyper-parameters equal to the ones used in the initial experiments.
To provide the shortest dependency path (sdp) for the syntax-aware version
we compare to, we use our parser with Stanford dependencies, as described
above. Table 5.7 shows the effect of using syntactic information through the
shortest dependency path for each relation type. We find that the effect of
syntactic structure varies between the different relation types. However, the sdp

9 As we recall, our initial rounds of experiments show that the multi-channel model works
better than the single-channel model.

108


Syntactic Dependency Representations

Hyper parameters (optimal)

Sub-task Repr. Filter Feature Activation L2 Reg. Learning Dropout
size maps func. rate Prob.
CoNLL 4-5 1000 Softplus  1.15e+01 = 1.13e-03 1
1.1 SB 4-5 806 Sigmoid _—8.13e-02 1.79e-03 0.87
UD v1.3 5 716 Softplus  1.66e+00 9.63e-04 1
CoNLL 3-4-5 667 ReLU 4.96e+00 — 1.26e-03 0.88
2 SB 6-7 339 Sigmoid 1.00e-04 6.96e-04 0.48
UD v1.3 3-4-5 549 Iden 5.22e-01 5.09e-04 0.81
Default values 3-4-5 128 ReLU 3 le-3 0.5

Table 5.8: Hyper parameter optimization results for each model with different
representation. The max pooling strategy consistently performs better in all
model variations. We also report the default value for each hyper parameter.

F1.(avg. in 5-fold)
Sub-task Repr. Default Optimized

CoNLL 72.65 74.49
1.1 SB 74,16 75.05
UD v1.3 69.55 69.57
CoNLL 60.31 60,54
2 SB 58.11 61.18
UDvi.3 54.53 56.80

Table 5.9: Performance of each model with optimized hyper parameters for
different representation. In optimized column we evaluate each model with the
optimal value for each hyper parameter, given in Table 5.8. In default column,
the default value for each hyper parameter is used.

information has a clear positive impact on all the relation types, ranging from
improvements of 20 to 45 percentage points depending on the specific relation.
This can be attributed to the fact that the context-based representations suffer
from irrelevant sub-sequences or clauses when target entities occur far from each
other, or there are other target entities in the same sentence. The sdp between
two entities in the dependency graph captures a condensed representation of the
information required to assert a relationship between two entities (Bunescu and
Mooney, 2005).

109


5. Low-Resource Relation Extraction

5.7.4 Comparison of different dependency representations

To assess model performance with various syntactic dependency representations,
we create a sdp for each training example using the different parse models and
exploit them as input to both the relation extraction and classification model.
With the use of default parameters, there is a risk that these favor one of the
representations simply by chance. In order to perform a fair comparison between
the different dependency representations, we make use of Bayesian optimization
(Brochu et al., 2010) in order to locate optimal hyper-parameters for each of the
dependency representations. We construct a Bayesian optimization procedure
using a Gaussian process with 100 iterations and Expected Improvement (EI) for
its acquisition functions. We set the objective function to maximize the macro
F1 score over 5-fold cross-validation on the training set. Here we investigate the
impact of various system design choices with the following parameters!?:

e Filter region size: € {3, 4, 5, 6, 7, 8, 9, 3-4, 4-5, 5-6, 6-7, 7-8, 8-9, 3-4-5,
4-5-6, 5-6-7, 6-7-8, 7-8-9}

e Number of feature maps for each filter region size: € {10 : 1000}
e Activation function: € {Sigmoid, ReLU,Tanh, Softplus, Iden}.
e Pooling strategy: € {mazx, avg}.

e L2 regularization: € {le—4:le+ 2}.

e Learning rate: © {le—6: le— 2}.

© Dropout probability 1+: € {0.1 : 1}.

Table 5.8 presents the optimal values for each configuration using different
dependency representations. We see that the optimized parameter settings vary
for the different representations, showing the importance of tuning for these
types of comparisons. The results (Table 5.9) furthermore show that the sdps
based on the Stanford Basic (SB) representation provide the best performance
for both subtasks following hyperparameter tuning, followed by the CoNLLO8
representation. We also observe that for the extraction subtask (subtask 2), the
best representation changes following tuning of the system. It can be seen that
the results for the UD representation are consistently quite a bit lower than the
two others. This is perhaps somewhat surprising given the fact that downstream
usefulness is one of the motivations behind this dependency framework.

5.8 Error Analysis

Our results show that the best performing dependency framework in our system
is the Stanford Basic scheme and furthermore that the widely used Universal

10Default values are {3-4-5, 128, ReLU, max, 3, le-3, 0.5}
11The probability that each element is kept, in which 1 implies that none of the nodes are
dropped out

110


Error Analysis

best F1 (in 5-fold)

Relation Freq. CoNLL SB UD
USAGE 947 76.84 82.39 77.56
MODEL-FEATURE — 498 68.27 68.54 66.36
PART_WHOLE 425 75.32 71.28 67.11
TOPIC 258 89.32 90.57 87.62
RESULT 193 82.35 81.69 82.86
COMPARE 136 66.67 66.67 54.24
macro-averaged 76.94 77.57 72.83

Table 5.10: Effect of using the different parser representation on each relation
type in sub-task 1.1 (see Section 5.3.1).

Dependencies scheme consistently provides somewhat lower results in both
relation classification and full extraction. To gain a better understanding of the
reasons behind these differences in performance, we perform error analysis.

Table 5.10 presents the effect of each parser representation in the classification
task, broken down by relation type. Firstly, we note that in general, the results
differ between the different relation types, where the TOPIC relation has the
highest score (90.57 F1 with the SB representation). In contrast, the most
infrequent COMPARE relation has the lowest F-score (66.67 F1 with SB). We
further observe that the UD-based model falls behind the others on most of the
relation types (i.e., COMPARE, MODEL-FEATURE, PART_WHOLE, TOPICS).
To explore these differences in more detail, we manually inspect the instances
for which the CoNLL/SB-based models correctly predict the relation type in
5-fold trials, whereas the UD-based model has an incorrect prediction.

Table 5.11 shows some of these examples for the classification sub-task,
marking the entities and the gold class of each instance and also showing the sdp
from each representation. We observe that the UD paths are generally shorter.
A striking similarity between most of the instances is the fact that one of the
entities resides within a prepositional phrase. Whereas the SB and CoNLL paths
explicitly represent the preposition in the path, the UD representation does not.
Clearly, the difference between, for instance, the USAGE and PART_WHOLE
relation may be indicated by the presence of a specific preposition (X for Y vs.
X of Y). This is also interesting since this particular syntactic choice has been
shown in previous work to have a negative effect on intrinsic parsing results for
English (Schwartz et al., 2012).

We go on to examine the errors in the full extraction sub-task (task 2, see
Section 5.3.2) where the system trained using UD-based paths has an incorrect
prediction, whereas the two other systems (CoNLL-based and SB-based) do not.
We note that here as well the exclusion of the preposition from the path in the

111


Low-Resource Relation Extraction

5.

Sentence 1 This indicates that there is no need to add | punctuation | in transcribing |SuOkS0 @osor) simply in order to help parsers. class: PART_WHOLE

CoNLL punctuation + obj add — adv in + pmod -> transcribing — obj — spoken corpora

SB punctuation dobj add prep in pcomp — transcribing —+ dobj > spoken corpora

UD v1.3 punctuation «+ dobj add advcl transcribing —+ dobj > spoken corpora

Sentence 2 In the process we also provide a [Genel ceitiien of | parsing | motivated by an informal notion due to Lang . class: MODEL-FEATURE
CoNLL formal definition nmod of + pmod parsing

SB formal definition + prep — of + pobj > parsing

UD v1.3 formal definition + nmod — parsing

Sentence 3 This paper describes a practical HE@S tea tart, for automatic evaluation of (@Wessere egos] MP6 yee) in spoken dialogue. class: USAGE
CoNLL ""black-box" methodology nmod for + pmod evaluation nmod of + pmod — question-answering NL systems

SB "black-box" methodology + prep — for — pobj — evaluation + prep — of — pobj — question-answering NL systems

UD v1.3 "black-box" methodology —+ nmod -—> evaluation + nmod —> question-answering NL systems

Table 5.11: The examples for which the CoNLL/SB-based models in the classification sub-task correctly predict the relation
type in 5-fold trials, whereas the UD based model has an incorrect prediction.

Sentence 4 However , for Sebi Genres) which use more /inleae ce seventeen ersesla4) , for example tag and ccg , tagging accuracy is much lower . class: USAGE
CoNLL fine-grained grammatical categories <~ prd + use + nmod ¢ grammar formalisms

SB fine-grained grammatical categories <~ dobj use < remod < grammar formalisms

UD v1.3 fine-grained grammatical categories <- advmod < use > nmod — grammar formalisms

Sentence 5 We consider the case of multi-document summarization , where the input | documents | are in | Arabic | , and the output summary is in English . class: MODEL-FEATURE
CoNLL Arabic pmod in prd are — sbj documents

SB Arabic pobj in prep are nsubj documents

UD v1.3 Arabic + nsubj + documents

Table 5.12: The examples for which the CoNLL/SB-based models in the extraction sub-task correctly predict the relation type
in 5-fold trials, whereas the UD based model has an incorrect prediction.

112


Summary

UD representation is problematic. A high proportion of the errors contain an
entity that resides within a prepositional phrase, as exemplified by the sentences
in Table 5.11. We also observe some parse errors in the UD parse. Sentence 4
in Table 5.12, for instance, gives an example where the UD parser incorrectly
assigns the embedded verb (use) of a relative clause status as a main verb
with two modifier dependents, rather than recognizing that the relative clause
(which use more fine-grained grammatical categories) depends on the entity
fine-grained grammatical categories. We also find another difference between the
representations which shows up in the errors, namely the combination of the
aforementioned UD treatment of prepositions as dependent case markers and
the copula construction, e.g., for Sentence 5 in Table 5.12, where the CoNLL
and SB parsers assign head status to the copula verb are in combination with
the PP complement in Arabic, whereas the UD parser assigns head status to
Arabic of which the documents is a subject dependent.

5.9 Summary

This chapter presents a CNN model over the shortest dependency paths
between entity pairs for relation extraction and classification in scientific
text. We examine several variants of this architecture for the proposed
model. The experiments demonstrate the effectiveness of domain-specific word
embeddings for all sub-tasks as well as sensitivity to the specific dependency
representation employed in the input layer. We compared three widely
used dependency representations (CoNLL, Stanford Basic, and Universal
Dependencies) and find that representation matters and that certain choices have
clear consequences in downstream processing. Our experiments also underline
the importance of performing hyperparameter tuning when comparing different
input representations.
To summarize, the following contributions are made in this chapter:

(i) We evaluate the effect of syntax in a neural relation extraction and
classification system,

(ii) We study the impact of domain-specific embeddings,
(iii) We assess the effect of varying the syntactic input representations, and

(iv) We perform a manual error analysis that helps understand the most
important aspects of syntactic representation for these tasks.

113




Chapter 6

Natural Language Understanding
in Low-Resource Genres and
Languages

Learning what to share between tasks (i.e., transfer learning as explained in 2.3
of Chapter 2) has been a topic of great importance recently, as strategic sharing
of knowledge has been shown to improve the performance of downstream tasks.
In multilingual applications, sharing knowledge between languages is important
when considering that most languages in the world suffer from being under-
resourced. In this chapter, consider the transfer of models along two dimensions
of variation (see Section 2.4 in chapter 2), namely genre and language, when little
or no data is available for a target genre or language. These scenarios are known
as low-resource and zero-resource settings. We show that this challenging setup
can be approached using meta-learning, where, in addition to training a source
model, another model learns to select which training instances are the most
beneficial. We experiment using standard supervised, zero-shot cross-lingual,
as well as few-shot cross-genre and cross-lingual settings for different natural
language understanding tasks (natural language inference, question answering).
Our extensive experimental setup demonstrates the consistent effectiveness of
meta-learning in various low-resource scenarios. We improve the performance of
pre-trained language models for zero-shot and few-shot NLI and QA tasks on
two NLI datasets (i-e., MultiNLI and XNLI), and on the MLQA dataset. We
further conduct a comprehensive analysis, which indicates that the correlation
of typological features between languages can further explain when parameters
sharing learned via meta-learning is beneficial.

6.1 Introduction

There are more than 7000 languages spoken in the world, over 90 of which have
more than 10 million native speakers each (Eberhard et al., 2019). Despite this,
very few languages have proper linguistic resources when it comes to natural
language understanding tasks. Although there is a growing awareness in the field,
as evidenced by the release of datasets such as XNLI (Conneau et al., 2018), most
NLP research still only considers English (Bender, 2019). While one solution
to this issue is to collect annotated data for all languages, this process is both
too time consuming and expensive to be feasible. Additionally, it is not trivial
to train a model for a task in a particular language (e.g., English) and apply it
directly to another language where only limited training data is available (i.e.,
low-resource languages). Therefore, it is essential to investigate strategies that

115


6. Natural Language Understanding in Low-Resource Genres and Languages

allow us to use a large amount of training data available for English for the
benefit of other languages.

Meta-learning has recently been shown to be beneficial for several machine
learning tasks (Koch et al., 2015; Vinyals et al., 2016; Santoro et al., 2016; Finn
et al., 2017; Ravi and Larochelle, 2017; Nichol et al., 2018). In the case of
NLP, recent work has also shown the benefits of this sharing between tasks
and domains (Dou et al., 2019; Gu et al., 2018; Qian and Yu, 2019). Although
meta-learning for cross-lingual transfer has been investigated in the context of
machine translation (Gu et al., 2018), in this chapter, we attempt to study the
meta-learning effect for natural language understanding tasks. We investigate
cross-lingual meta-learning using two challenging evaluation setups, namely:

(i) Few-shot learning: where only a limited amount of training data is available
for the target domain or genre.

(ii) Zero-shot learning: where no training data is available for the target
domain or genre.

Specifically, in Section 6.7, we evaluate the performance of our model on two
natural language understanding tasks, as follows:

e Natural Language Inference (NLI) by experimenting on the MultiNLI
(cross-genre setup) and the XNLI (cross-lingual setup) datasets (Conneau
et al., 2018).

e Question Answering (QA) on the MLQA as a multilingual question
answering dataset (Lewis et al., 2020).

Accordingly, we investigate the following research questions in this chapter:

RQ 6.1. Can meta-learning assist us in coping with low-resource settings in
natural language understanding (NLU) tasks?

RQ 6.2. What is the impact of meta-learning on the performance of pre-trained
language models such as BERT (Devlin et al., 2019), XLM (Conneau and
Lample, 2019) and XLM-RoBERTa (Conneau et al., 2020) in cross-lingual NLU
tasks 1?

RQ 6.3. Can meta-learning provide a model- and task-agnostic framework in
low-resource NLU tasks?

RQ 6.4. Are typological commonalities among languages beneficial for the
performance of cross-lingual meta-learning?

lAt the time of writing, these are the top performing models in cross-lingual NLU
benchmarks.

116


Natural Language Understanding (NLU)

6.2 Natural Language Understanding (NLU)

Understanding of natural language is an essential and challenging goal of
NLP. Natural language understanding comprises a wide range of diverse tasks,
including, but not limited to, natural language inference, question answering,
sentiment analysis, semantic similarity assessment, and document classification.
In this thesis, we explore the use of transfer learning by leveraging meta-learning
to perform various NLU tasks, including natural language inference and question
answering. We provide a brief description of these tasks in the following sections.

6.2.1 Natural Language Inference (NLI)

NLI is the task of predicting whether a hypothesis sentence is true (entailment),
false (contradiction), or undetermined (neutral) given a premise sentence. NLI
systems need some semantic understanding and models trained on entailment
data can be applied to many other NLP tasks such as text summarization,
paraphrase detection, and machine translation. The task of NLI, also known as
textual entailment, is well-positioned to serve as a benchmark task for research
on NLU (Williams et al., 2018). Here, we present some of the datasets that have
been provided for NLI tasks and are exploited in this chapter.

MultiNLI The Multi-Genre Natural Language Inference (MultiNLI) corpus has
433k sentence pairs annotated with textual entailment information (Williams
et al., 2018). It covers a range of different genres of spoken and written text and
offers an explicit setting for cross-genre evaluation. The NLI premise sentences
are derived from 10 different resources to cover a maximally broad range of
genres of American English, such as: FACETOFACE, TELEPHONE, VERBATIM,
STATE, GOVERNMENT, FICTION, LETTERS, 9/11, TRAVEL and OuP. All of the
genres appear in the test and development sets, but only five are included in the
training set (See Table 6.1, which presents the statistics for the MultiNLI dataset
by genre). Table 6.2 depicts randomly chosen examples from the MultiNLI
dataset, shown with their genre labels, and both the selected gold labels.

XNLI The Cross-lingual Natural Language Inference (XNLI) dataset (Conneau
et al., 2018) consists of 5000 test data and 2500 development hypothesis-premise
pairs with their textual entailment labels for English. The pairs are annotated
and translated, by employing professional translators, into 14 languages French
(fr), Spanish (es), German (de), Greek (el), Bulgarian (bg), Russian (ru), Turkish
(tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), Swahili
(sw) and Urdu (ur). XNLI provides a multilingual benchmark to evaluate how
to perform inference in low-resource languages such as Swahili or Urdu, in
which only training data for the high-resource language English is available from
MultiNLI. Some examples from the XNLI corpus are shown in Table 6.3.

117


6. Natural Language Understanding in Low-Resource Genres and Languages

#tExamples
Genre Train Dev. Test
FICTION 77,348 2,000 2,000
GOVERNMENT 77,350 2,000 2,000
SLATE 77,306 2,000 2,000
‘TELEPHONE 83,348 2,000 2,000
TRAVEL 77,350 2,000 2,000
9/11 0 2,000 2,000
FACE-TO-FACE 0 2,000 2,000
LETTERS 0 2,000 2,000
OUP 0 2,000 2,000
VERBATIM 0 2,000 2,000

MultiNLI Overall 392,702 20,000 20,000

Table 6.1: Statistics for the MultiNLI corpus by genre. The first five genres
represent in the training, development and test sets, and the remaining five
represent in the development and test set (Williams et al., 2018).

Premise Label Hypothesis

FICTION

The Old One always comforted Ca’daan, neutral Ca’daan knew the Old One
except today. very well.

LETTERS

Your gift is appreciated by each and neutral Hundreds of students
every student who will benefit from will benefit from your
your generosity. generosity.

‘TELEPHONE

yes now you know if if everybody contradiction August is a black out
like in August when everybody’s on month for vacations in the
vacation or something we can dress a company.

little more casual or

9/11
At the other end of Pennsylvania entailment People formed a line at the
Avenue, people began to line up for end of Pennsylvania Avenue.

a White House tour.

Table 6.2: Examples from the MultiNLI corpus, shown with their genre and
selected gold labels (Williams et al., 2018).

118


Natural Language Understanding (NLU)

Language Premise / Hypothesis Genre Label

‘You don’t have to stay there.

English (en) You ear leave Face-To-Face Entailment
La figure 4 montre la courbe d’offre des services de partage de travaux. F
French (fi . er t Entailment
ench (ft) Les services de partage de travaux ont une offre variable. Government ramen
Y se estremecié con el recuerdo.
Spanish (es ‘ —_ 7 re Ficti Entailment
panish, (eg) El pensamiento sobre el acontecimiento hizo su estremecimiento. tenon. ramen!
Wahrend der Depression war es die armste Gegend, kurz vor dem Hungertod. ss
German (de) Die Weltwirtschaftskrise dauerte mehr als zehn Jahre an. Travel Neutral
oe Ni silaha ya plastiki ya moja kwa moja inayopiga risasi. =
Swahili (sw Pe ae Teleph Neutral
web (Bw) Inadumu zaidi kuliko silaha ya chuma. empnone ae
. Ui mer 3aHmMaemca 3TUM yoke Ha WporaxKeHun 85 ser. we
Russian (ru) Mig HONE HASSE OH TAT SARTRGARRGH. Letters Contradiction
Chinese (oh) LEBER, SEERA BD FPS E97 BA A Slate Contradiction
: KEATS AA GLY IT .
Arabic (ar) . a fret ee an | a ieuest eo 4 Nine-Eleven Contradiction

Table 6.3: Examples (premise and hypothesis) from various languages and genres
from the XNLI corpus (Conneau et al., 2018).

During what time period did the Angles migrate to Great Britain?

What are the names given to the campuses on the east side of the
land the university sits on?

The name "England" is derived from the Old English name Englaland [...] The
Angles were one of the Germanic tribes that settled in Great Britain during the
Early Middle Ages. [...] The Welsh name for the English language is "Saesneg"

WaAhrend welcher Zeitperiode migrierten die Angeln nach
GroBbritannien?

The campus is in the residential area of Westwood [...] The campus is informally
divided into North Campus and South Campus, which are both on the eastern
half of the university's land. [...] The campus includes [...] a mix of architectural
styles.

~Cuales son los nombres dados a los campus ubicados en el lado
este del recinto donde se encuentra la universidad?

Der Name England leitet sich vom altenglischen Wort Engaland [...] Die Angein
waren ein germanischer Stamm, der das Land im Friihmitte/alter besiedelte.
[...] ein Verweis auf die weiBen Klippen von Dover.

Sain! ity yl) Ses) sala ia daa Gl

El campus incluye [...] una mezcla de estilos arquitectonicos. Informalmente
esta dividido en Campus Norte y Campus Sur, ambos localizados en la parte
este del terreno que posee la universidad. [...] El Campus Sur esta enfocado en
la ciencias fisicas [...] y el Centro Médico Ronald Reagan de UCLA.

FAS SREP RAR MRAtA?

Regal 4 las lS ya lad" a 5 Englatand Saal y AS Say "ai Gans is Ally
Legh Gopal Vale ay [oa] oploew all y gered! Ligh DAS | fled pho pli 5) a pall SL Ge
Jusyt

Trong khoang thdi gian nao ngudi Angles di cu dén Anh?

Tén goi cua Anh trong tiéng Viét bat ngudn tir tiéng Trung. [...] Nguoi Angle la
mét trong nhirng bé toc German dinh cu tai Anh trong Tho? dau Trung C6. [...]
duéng nhu no lién quan toi phong tuc goi nguéi German tai Anh la Angli
Saxones hay Anh - Sachsen.

BORARTENMDA PILATE, AAMAS AS SR Ee
TRAZRRAHPL, WAULAURZSHHCRAHS, RPHARRA
pa as Library) LASHES BAR EGR. [...] XN RES
Brahm,

faeafterrera oret era &, Fares VT fern H ar afew wy ger ATH fee aT 8?

wa 1919 4 apitvery 4 area ater after elon, aa ged are sured eft...) ofa
aritrenftes wa A sat Wea silz ataroit Y feonfsia @, ait eat fereaferenrera Bt
Hatfaara, wforeter farsa, aut caer S daiftla aa sie quenite Afswat Hex fia a1

Figure 6.1: QA instances in the MLQA dataset. Answers shown as highlighted
spans in contexts. Contexts shorten for clarity with "[...]" (Lewis et al., 2020).

Dataset | English (en) Arabic (ar) German (de) Spanish (es) Hindi (hi) Vietnamese (vi) | Chinese (zh)
Dev 1148 517 512 500 507 511 507
Test 11590 5335 4517 5253 4918 5495 4918

Table 6.4: Overview of the number of QA instances in the development and test
portions of the MLQA dataset across the different languages.

119


6. Natural Language Understanding in Low-Resource Genres and Languages

6.2.2 Question Answering (QA)

The task of QA is often designed in the context of a reading comprehension task.
This machine reading problem is formulated as extractive question answering,
in which the answer is drawn from the original text (Eisenstein, 2019). In this
context, given a context and a question, the QA task aims to identify the span
answering the question in the context. We study the QA task using the following
two datasets:

SQuAD Stanford Question Answering Dataset (SQUAD v1.1), provided by
Rajpurkar et al. (2016), is a reading comprehension dataset and contains 107,785
question-answer pairs obtained from 536 English Wikipedia articles.

MLQA Lewis et al. (2020) introduce a Multilingual Question Answering dataset
(called MLQA) containing QA instances in 7 languages, namely English (en),
Arabic (ar), German (de), Spanish (es), Hindi (hi), Vietnamese (vi) and Simplified
Chinese (zh). Figure 6.1 shows some examples from the MLQA dataset. MLQA
is split into development and test splits, with detailed numbers in Table 6.4.
Recently, it has been used in many benchmarks for the evaluation of cross-lingual
transfer learning, e.g., Hu et al. (2020a) and Liang et al. (2020).

6.3 NLU Models

We perform experiments on a variety of models that have been proposed for NLU
tasks, including Enhanced Sequential Inference Model (ESIM), Bidirectional
Encoder Representations from Transformers (BERT), Cross-Lingual Language
Model (XLM) and XLM on RoBERTa (XLM-RoBERTa). These models have
become competitive baselines on NLI and QA tasks. In the following sections,
we will briefly describe the models.

ESIM Enhanced Sequential Inference Model (ESIM), proposed by Chen et
al. (2017), is commonly used for textual entailment problems. ESIM employs
LSTMs with attention to create a rich representation, capturing the relationship
between premise and hypothesis sentences. It introduces local inference modeling,
which models the inference relationship between premise and hypothesis after
the two fragments have been aligned locally. Figure 6.2 shows the architecture of
the ESIM model. It consists of three layers. The input encoding layer, which is
the first layer, uses BiLSTM to provide a contextual representation of each word
element in the input premise and hypothesis. Then, the local inference modeling
collects information to perform local inference between words and phrases in the
second layer. This layer computes a form of soft attention computed between the
words in the two sentences to model their interactions. The softmaz function is
applied to transfer the attention weights computed between each word in the
premise and the hypothesis to a probability distribution. The inference between
sentence pairs is modeled by concatenation of the encoded and conditioned

120


NLU Models

b=]

‘= Iv, avg’ Va, max’ Vb, avg’ Yb, max!

Inference
Composition

Local
Inference
Modeling

Input
Encoding

Figure 6.2: Architecture of the ESIM model (Chen et al., 2017).

representations of words, their difference, and component-wise product. The last
layer is devoted to inference composition to perform composition and aggregation
over local inference output and to make the global judgment. Since the previous
layer introduces a lot of new dimensions, the outputs from inference modeling are
first passed through a mapping function F' consisting of a simple feed-forward
layer with ReLU activation to control the model’s complexity. Then, the second
BiLSTM layer provides two new vectors. To merge these two vectors, average
and max pooling operations are applied, and the results are concatenated in a
final representation to predict the probabilities of the classes associated to the
input sentences. The prediction step contains a two-layer perceptron G with
tanh and softmaz activation functions.

121


6. Natural Language Understanding in Low-Resource Genres and Languages

Transformer
hp * *# ft KF FR A RA
=. HRB ee eee
= MMMM eee
Transformer
SS, Sn, SS
=. HRB ee eee

Figure 6.3: Modified Masked Language Model and Translation Language Model
in XLM (Conneau and Lample, 2019).

BERT The Bidirectional Encoder Representations from Transformers (BERT)
(Devlin et al., 2019) is designed to pretrain deep bidirectional representations
from unlabeled text by jointly conditioning on both left and right context in all
layers (see Section 2.5.1.2 in Chapter 2). In our study, we employ the original
English BERT model (En-BERT) and Multilingual BERT (Multi-BERT) models.
Like the original English BERT model, Multi-BERT is a 12 layer transformer,
but instead of being trained only on monolingual English data with an English-
derived vocabulary, it is trained on the Wikipedia pages of 104 languages with a
shared word piece vocabulary.

XLM XLM, proposed by Conneau and Lample (2019), uses a similar pre-
training objective as Multi-BERT with a larger model, a more extensive shared
vocabulary, and leverages both monolingual and parallel data. XLM modifies
BERT in the following way: First, instead of using word or characters as the
input of the model, it uses Byte-Pair Encoding (BPE), introduced by Sennrich
et al. (2016), that splits the input into the most common sub-words across all
languages, thereby increasing the shared vocabulary between languages. This
setting is denoted as Masked Language Modeling (MLM). Second, the Translation
Language Modeling (TLM) modifies the BERT architecture as follows: (i) It
extends the masked language model to pairs of parallel sentences. Unlike BERT,
each training sample consists of the same text in two languages. Therefore, the
model can use the context from one language to predict tokens in the other,
as different words are randomly masked words in each language, and (ii) The

122


Model-Agnostic Meta-Learning (MAML)

model is also informed about the language ID and the order of the tokens (i.e.,
the Positional Encoding) in each language as input metadata. It helps the
model learn the relationship between related tokens in different languages. The
complete XLM model is trained by both MLM and TLM and alternating between
them (Figure 6.3). We make use of a variant of the XLM-15 that is trained with
MLM + TLM on the 15 XNLI languages.

XLM-RoBERTa (XML-R) Robustly Optimized BERT Pre-training Approach
(RoBERTa) (Liu et al., 2019b) has the same architecture as BERT, but uses
SentencePiece (Kudo and Richardson, 2018) as a tokenizer. It modifies key
hyperparameters, removing the next-sentence pretraining objective and training
with much larger mini-batches and learning rates. XLM-RoBERTa (XML-R) is
a RoBERTa-version of XLM trained based on a much larger multilingual corpus
(i.e., more than two terabytes of publicly available CommonCraw1 data in 100
different languages) and has become the new state-of-the-art on cross-lingual
benchmarks (Hu et al., 2020a). The biggest update that XLM-R offers over the
original is a significantly increased amount of training data. XLM-Roase with
125M parameters and XLM-Rjarge with 355M parameters are the variations of
XLM-R that are trained on 2.5 TB of CommonCrawl data in 100 languages and
have been used in our work.

6.4 Model-Agnostic Meta-Learning (MAML)

Meta-learning, or learning to learn, can be seen as an instance of sequential
transfer learning (see Section 2.5 of Chapter 2). It trains a high-level model
sequentially based on the sub-models that are typically optimized (Ruder et
al., 2019). Meta-learning tries to tackle the problem of fast adaptation to a
handful of new training data instances. It discovers the structure among multiple
tasks such that learning new tasks can be done quickly. In NLP, this has
been done by repeatedly simulating the learning process on low-resource tasks
using various high-resource tasks (Gu et al., 2018). There are several ways of
performing meta-learning:

(i) Metric-based: It aims to learn similarities between feature representations
of instances from different training sets given a similarity metric. The
idea is to learn a metric space and then use it to compare low-resource
testing to high-resource training samples. The representative works in this
category include Siamese Network (Koch et al., 2015), Matching Network
(Vinyals et al., 2016), and Relation Network (Sung et al., 2018).

(ii) Model-based: The idea is to use an additional meta-learner to learn and
to update the original learner with a few training examples. The focus
has been on adapting models that learn fast (e.g., memory networks) for
meta-learning (Santoro et al., 2016). Ravi and Larochelle (2017) introduce
an LSTM-based meta-learner to learn the optimization algorithm used to
train the original network.

123


6. Natural Language Understanding in Low-Resource Genres and Languages

M: Model

T; €p(7): Task

a,8: Step size

6: Initial parameter

oF : Optimal parameter in task i, fast weight

; outer-loopfy TT ee eee eee eee 1
8 ee SS eee me eee ee eee meee rere "
- T, :
1 8H s ’ "
1 3! 8 ry)
rhe th =O-aV gly ,(Mg)| 4
§t 1
' i nt um
1 Cn ee Beene ene nee Ly
' 1 1 it
1oore '
uv 3 1
31 s Lr My) r
1 PUSS 1
_— m
12, 1
' 34
' 1
' 1
1 oy :
ee ene Foon.
1 1
' Meta-Learned 0 '
I occ ;

Figure 6.4: Model Agnostic Meta-Learning (MAML) in supervised learning.

(iii) Optimization-based: The optimization algorithm itself is designed in a
way that favors fast adaption (Finn et al., 2017; Nichol et al., 2018).
The optimization-based methods introduce no additional architectures nor
parameters. They can find good initialization parameters of the model
using a small training set and adapt to new tasks quickly.

In this chapter, we focus on optimization-based methods due to their
superiority in several tasks (e.g., computer vision (Finn et al., 2017)) over
the above-mentioned meta-learning architectures. They achieved stat-of-the-art
performance by directly optimizing the gradient towards a proper parameters
initialization and fine-tuning on low-resource scenarios, We investigate the idea
of meta-learning for transferring knowledge in a cross-genre and cross-lingual
setting for natural language understanding, particularly for NLI and QA tasks.
Specifically, we exploit the usage of Model Agnostic Meta-Learning (MAML),
which uses gradient descent and achieves a good generalization for a variety
of tasks (Finn et al., 2017). MAML can quickly adapt to new target tasks by
using only a few instances at test time, assuming that these new target tasks
are drawn from the same distribution.

Formally, MAML (see Figure 6.4) is applied in supervised learning step-by-
step as follows (Finn et al., 2017):

124


Model-Agnostic Meta-Learning (MAML)

Task 1 Task 2 _ New Task |
{cat, lamb, pig} {dog, shark, lion} {duck, dolphin, hen}

Esta i Jee a Se
Train
* gard i ¥
chm Gee ate

~ 98° SEB £68
u

casket AB
my iP
. nd

¥

a" PTT
Tes pe *
ee) ee

Lr 1 (Mp, ) Crit, )

Train

Define the gonoodness of a function M: L(M)=));L7;

Find the best M: F*= argmingL(™)

Figure 6.5: Example of applying Model Agnostic Meta-Learning (MAML) in
supervised learning.

1. Let us assumes that there is a model M with parameters @ and a distribution
p(T) over tasks.

2. We sample a batch of tasks 7; from the distribution p(7). Let us say we
sample n tasks as {71,..., Tn}.

3. In the inner loop, for each task 7; in tasks 7, we prepare the support set
(i.e., D'"”) and query set (i.e., D¥°**). The parameters 6 is updated using
one or a few iterations of gradient descent steps on the training examples
in the support set (ie., D’"“”) of task 7;. For example, for one gradient
update,

6; =0—aVoLr, (Mo) (6.1)

125


6. Natural Language Understanding in Low-Resource Genres and Languages

where a is the step size, the Mg is the learned model from the neural
network and £7, is the loss on the specific task 7j.

4. The parameters of the model @ are trained to optimize the performance
of Mg on the unseen test examples (i-e., Di**’) across tasks p(T). The
meta-learning objective is:

min S> L7,(Mo,)=min S> Lr (Mpaveenit)) (6-2)
Ti~P(T) Ti~p(T)

The MAML algorithm aims to optimize the model parameters via a small
number of gradient steps on a new task, which we refer to as the meta-
update. The meta-update across all involved tasks is performed for the 6
parameters of the model using stochastic gradient descent (SGD) as:

9<-0—BVo S> Lr,(Mor) (6.3)
Ti~p(T)

where (@ is the meta-update step size.
5. We repeat step 2 to 4 for N number of iterations as outer-loop.

6. The final parameters @ is the optimal parameters that can be used to
initialize the model M in a new task.

The meta-update step in MAML (Eq. 6.3) involves a gradient through a gradient
which can be both computationally and memory intensive. A modified version of
MAML ignores the second derivative (Finn et al., 2017), resulting in a simplified
and cheaper implementation, known as First-Order MAML (FOMAML):

0-0-8 S> VoLr;(Mor) (6.4)
Ti~v(T)

We further illustrate MAML with a simple example, shown in Figure 6.5. We
here have two image classification tasks: One training task (Task 1), to label
images as a cat, lamb, or pig, and another task, to label images as a dog, shark,
or lion. The aim is to train a neural network model M towards parameters that
can adapt quickly and with few examples to a novel classification task (i.e.,
to label images as a duck, dolphin, or hen). First, we randomly initialize our
model parameters 6. We train our model on Task 1 using the train set and
minimize the loss using gradient descent and find the optimal parameters 6).
(see Eq. 6.1). Similarly, for Task 2, we start with a randomly initialized model
parameters # and minimize the loss by finding the optimal parameters bo. using
gradient descent. In the next step, we perform meta-optimization in each task’s
test set by minimizing the loss in the test set. We calculate the losses (i.e.,
Ly, (Mg,’) and L7,(M,,) ) by taking the gradient with respect to our optimal

parameters calculated in the previous step Oy. and 0. Then, we update the
original parameters 6 using the test sets of Tasks 1 and 2 (see Eq. 6.3). During

126


Related Work

meta-training, the MAML learns the optimal initialization parameters that allow
the model M to adapt quickly and efficiently to a new few-shot task with new,
unseen classes (i.e., Task 3).

6.5 Related Work

This chapter’s primary motivation is the low availability of labeled training
datasets for most of the different text genres and languages. To alleviate this
issue, several methods, including so-called few-shot learning approaches, have
been proposed. Few-shot learning methods have initially been introduced within
the area of image classification (Vinyals et al., 2016; Ravi and Larochelle, 2017;
Finn et al., 2017), but have recently also been applied to NLP tasks such as
relation extraction (Han et al., 2018), text classification (Yu et al., 2018) and
machine translation (Gu et al., 2018). Specifically, in NLP, these few-shot
learning approaches include either: (i) the transformation of the problem into
a different task (e.g., relation extraction is transformed to question answering
(Abdou et al., 2019; Levy et al., 2017)); or (ii) meta-learning (Andrychowicz
et al., 2016; Finn et al., 2017).

6.5.1 Meta-Learning

Meta-learning has recently received much attention from the NLP community. It
has been applied to the task of machine translation (Gu et al., 2018), where they
propose to use meta-learning for improving the machine translation performance
for low-resource languages by learning to adapt to target languages based on
multilingual high-resource languages. They show that the use of meta-learning
significantly outperforms the multilingual, transfer learning-based approach
proposed by Zoph et al. (2016) and enables them to train a competitive neural
machine translation system with only a fraction of training examples. However,
in the proposed framework, they include 18 high-resource languages as auxiliary
languages and five diverse low-resource languages as target languages. In this
chapter, we assume access to only English as a high-resource language.

For the task of dialogue generation, Qian and Yu (2019) address domain
adaptation using meta-learning. They introduce an end-to-end trainable dialog
system that learns from multiple resource-rich tasks and is adapted to new
domains with minimal training samples using meta-learning. Model-agnostic
meta-learning (MAML) (Finn et al., 2017) is applied to the dialog domain and
adapts a dialog system model using multiple resource-rich single domain dialog
datasets. They show that the meta-learning enables the model to learn general
features across multiple tasks and is capable of learning a competitive dialog
system on a new domain with only a few training examples in an efficient manner.

Dou et al. (2019) explore model-agnostic meta-learning (MAML) and variants
thereof for low-resource NLU tasks in the GLUE dataset (Wang et al., 2018).
They consider different high-resource NLU tasks such as MultiNLI (Williams
et al., 2018) and QNLI (Rajpurkar et al., 2016) as auxiliary tasks to learn

127


6. Natural Language Understanding in Low-Resource Genres and Languages

meta-parameters using MAML. Then, they fine-tune the low-resource tasks
using the adapted parameters from the meta-learning phase. They demonstrate
the effectiveness of model-agnostic meta-learning in NLU tasks and show that
the learned representations can be adapted to new tasks effectively.

Obamuyide and Vlachos (2019) show that framing relation classification as
an instance of meta-learning improves the performance of supervised relation
classification models, even with limited supervision at training time. They apply
model-agnostic meta-learning to explicitly learn model parameters initialization
for enhanced predictive performance across all relations with limited supervision
in relation classification.

Recently, model-agnostic meta-learning has been applied to the task of
Natural Language Generation (NLG) (Mi et al., 2019). They formulate the
problem from a meta-learning perspective and propose a generalized optimization-
based approach. They show that the meta-learning based approach significantly
outperforms other training procedures since it adapts fast and well to new
low-resource settings.

All the works mentioned above on meta-learning in NLP assume that there
are multiple high-resource tasks or languages, which are then adapted to new
target tasks or languages with a handful of training samples. However, in a
cross-lingual NLI and QA setting, the available high-resource language is usually
only English.

6.5.2 Cross-Lingual NLU

Cross-lingual learning has a fairly short history in NLP, and has mainly been
restricted to traditional NLP tasks, such as PoS tagging and parsing. In contrast
to these tasks, which have seen much cross-lingual attention (Plank et al., 2016;
Bjerva, 2017; de Lhoneux et al., 2018), there has been relatively little work on
cross-lingual NLU, partly due to a lack of benchmark datasets. Existing work
has mainly been focused on NLI (Agié and Schluter, 2018; Conneau et al., 2018),
and to a lesser degree on RE (Faruqui and Kumar, 2015; Verga et al., 2016)
and QA (Lewis et al., 2020; Abdou et al., 2019). Previous research generally
reports that cross-lingual learning is challenging and that it is hard to beat a
machine translation baseline (e.g., Conneau et al. (2018)). Such a baseline is
(for instance) suggested by Faruqui and Kumar (2015), where the text in the
target language is automatically translated to English. For many language pairs,
a machine translation model may be available, which can be used to obtain data
in the target language. To evaluate the impact of using such data, in much of
previous research work, the English training data is translated into the target
language using a machine translation system. Then, the model is fine-tuned on
the translated data and evaluated on the test set of target languages and reported
as a TRANSLATE-TRAIN baseline. Alternatively, after fine-tuning the model
on the English training data, a TRANSLATE-TEST baseline is introduced by
evaluating the model on the test data that is translated from the target language
to English using the machine translation system.

128


Cross-Lingual Meta-Learning

In this chapter, we show that our meta-learning based framework can achieve
competitive performance compared to a machine translation baseline (for XNLI),
and propose a method that requires no training instances for the target task in
the target language.

6.6 Cross-Lingual Meta-Learning

The underlying idea of using MAML in NLP tasks (Gu et al., 2018; Dou et
al., 2019; Qian and Yu, 2019) is to employ a set of high-resource auxiliary
tasks or languages to find an optimal initialization from which learning a target
task or language can be done using only a small number of training instances.
In a cross-lingual setting (i.e., XNLI, MLQA), where only an English dataset
is available as a high-resource language, and a small number of instances are
available for other languages, the training procedure for MAML requires some
non-trivial changes. For this purpose, we introduce a cross-lingual meta-learning
framework (X-MAML), which uses the following training steps (a more formal
description of the proposed model X-MAML is given in Algorithm 4):

1. Pre-training on the high-resource language h (i.e., English): Given all the
training samples in the high-resource language h, we first train the model
M on h to initialize the model parameters 0.

2. Meta-learning using low-resource languages L: This step consists of choosing
one or more auxiliary languages A from the low-resource set L. Using the
development set of each auxiliary language in A, we construct a randomly
sampled batch of tasks 7;. Then, we update the model parameters using
K data points of J; (D{"®’") by one gradient descent step (see Eq. (6.1)).
After this step, we can calculate the loss value using Q examples (D{°*‘)
in each task. It should be noted that the K data points used for training
(D‘"“") are different from the Q data points used for constructing D‘***.
We sum up the loss values from all tasks to minimize the meta-objective
function and to perform a meta-update using Eq. (6.3). This step is
performed in multiple iterations.

3. Zero-shot or few-shot learning on the target languages {L \ A}: In the
last step of X-MAML, we first initialize the model parameters with those
learned during meta-learning. We then continue by evaluating the model
on the test set of the target languages (i.e., zero-shot learning) or fine-
tuning the model parameters with standard supervised learning using the
development set of target languages and evaluate on the test set (i.e.,
few-shot learning).

6.7 Experiments

In this section, we address our research questions (i.e., RQ 6.1 and RQ 6.2
are explored across Sections 6.7.2, 6.7.3 and 6.7.4, and RQ 6.3 is addressed in

129


6. Natural Language Understanding in Low-Resource Genres and Languages

Algorithm 4: X-MAML.
Input: high-resource language h, set of low-resource languages L,
Model M, step size a and learning rate 6
1 Pre-train M on h and provide initial model parameters 0
2 Select one or more languages from L as a set of auxiliary languages (A)
3 while not done do

4 for 1 € Ado
5 Sample batches of tasks J; using the development set of the
auxiliary language |
6 for each J; do
7 Sample K data-points to form Di” = {(X*,Y*)}#, ET;
8 Sample Q data-points to form Dies! = {(X4, Y)}2, € J, for
meta-update
9 Compute VoL7, (Ma) on Dire”
10 Compute adapted parameters with gradient descent:
6° = 0 — aVoL-r, (Mo)
11 Compute £7; (My’) using Dies!
12 Update 6 + 6 — BV >=; Lr; (My)

13 Perform either (i) zero-shot or (ii) few-shot learning on {L \ A} using
meta-learned parameters 6

Section 6.7.4) by conducting a set of experiments. We perform experiments
on the MultiNLI, XNLI, and MLQA datasets using different NLU models, as
explained in Section 6.3. We report results for few-shot as well as zero-shot
cross-genre and cross-lingual learning. To examine the model- and task-agnostic
features of X-MAML, we conduct experiments with various models for both
tasks.

6.7.1. Experimental Setup:

We implement X-MAML using the higher library.? We use the Adam optimizer
(Kingma and Ba, 2014) with a batch size of 32 for both zero-shot and few-shot
learning. We fix the step size a and learning rate 6 to le — 4 and le — 5,
respectively. We experimented using [10, 20, 30,50, 100, 200, 300] meta-learning
iterations in X-MAML. However, 100 iterations led to the best results in our
experiments. We sample two sets of 16 data points from the batch to construct
Dire” and D*** (ie., The sample sizes K and Q in X-MAML are equal to
16 for each dataset). We report results for each experiment by averaging the
performance over ten different runs (i.e., various random seeds). An evaluation of

?https://github.com/facebookresearch/higher

130


Experiments

NLI benchmarks is performed reporting accuracy on the respective test sets. For
the evaluation of the QA dataset, we use the F, score following the multilingual
evaluation script available with the MLQA data. °.

Baselines: In order to evaluate the impact of meta-learning on various
scenarios, we create our baseline for each scenario. We create: (i) zero-shot
baselines: directly evaluate the model on the test set of the target languages and
genres (for each task), and (ii) few-shot baselines: fine-tune the model on the
development set and evaluate on the test set of the low-resource languages and
genres.

6.7.2. Few-Shot Cross-Genre NLI

To verify our learning routine more generally and to address the research question
RQ 6.1, we define 7; as an NLI task in each genre. We exploit MAML, in its
original setting (see Section 6.4), to investigate whether meta-learning encourages
the model to learn a good initialization for all target genres, which can then be
fine-tuned with limited supervision for each genre’s development instances (2000
examples) to achieve a good performance on its test set. In MultiNLI, which
is a cross-genre dataset, we employ the Enhanced Sequential Inference Model
(ESIM), as explained in Section 6.3. We train ESIM on the MultiNLI training set
to provide initial model parameters 6. We evaluate the pre-trained model on the
English test set of XNLI (since the MultiNLI test set is not publicly available) to
set the baseline for this scenario. Since MultiNLI is already split into genres, we
use each genre as a task within MAML. We then include either the training set
(5 genres) or the development set (10 genres) during meta-learning (similar to
Step 2 in Section 6.6). In the last phase (similar to Step 3 in X-MAML), we first
initialize the model parameters with those learned by MAML. We then continue
to fine-tune the model using the development set of MultiNLI and report the
accuracy on the English test set of XNLI. We proportionally select sub-samples
x = (1%, 2%, 3%, 5%, 10%, 20%, 50%, 100%] from the training data (with random
sampling). The results obtained by training on the corresponding proportions
(%%) of the MultiNLI dataset using ESIM (as the learner model M) are shown in
Table 6.5.

We observe that for both settings (i.e., MAML on training (5 tasks) and on
development (10 tasks)), the performances of all models (including baselines)
improve as more training instances become available. However, as demonstrated
by our experimental study, the effectiveness of MAML is larger when only limited
training data is available (improving by 12% in accuracy when 2% of the data is
available on the development set).

https://github.com/facebookresearch/MLQA

131


6. Natural Language Understanding in Low-Resource Genres and Languages

Baseline MAML

x% Trea, Te

1 38.60 49.78 50.92
2 37.80 48.58 50.66
3 47.09 51.40 52.85
E) 49.88 52.22 51.40
10 51.02 52.51 53.95
20 59.14 61.38 58.16
50 63.37 63.85 61.74
100 64.35 64.99 64.61

Table 6.5: Test accuracies with different settings of MAML on MultiNLI. x%:
the percentage of training samples. Baseline: The test accuracy of trained
ESIM using x% of training data. MAML: The test accuracy of ESIM after
meta-learning, where 7r;ain: 5 tasks are defined in MAML using the training
set, and Tpey: 10 tasks are included in MAML using the development set. Bold
font indicates best results for the various proportions of the used training data.

6.7.3 Zero- and Few-Shot Cross-Lingual NLI

We now aim to answer the questions RQ 6.1 and RQ 6.2 and proceed to investigate
zero- and few-shot X-MAML for the cross-lingual NLI task. In XNLI, which
is a cross-lingual dataset, we employ the PyTorch version of BERT + (Devlin
et al., 2019) as the underlying model M (see Section 6.3). However, since our
proposed meta-learning method is model-agnostic, it can easily be extended to
any other architecture. Following our X-MAML framework, we first study the
impact of meta-learning with one low-resource language to serve as an auxiliary
language. We evaluate the performance of a cross-lingual NLI model on the set
of languages provided in the XNLI dataset. In the following sections, we study
the effect of X-MAML on the performance of the En-BERT and Multi-BERT
models (see Section 6.3) for the cross-lingual NLI task.

6.7.3.1 Zero-Shot Learning

In this set of experiments, we employ the proposed framework (i.e., X-MAML)
within a zero-shot setup, in which we do not fine-tune after the meta-learning
step.

Zero-shot X-MAML with Multi-BERT As the first training step (i.e., pre-
training on a high-resource language, see Step 1 in Section 6.6 for more

*https://github.com/huggingface/transformers

132


Experiments

“eyep poyE[sues} oy} SUIsSN pouNnj-ouy St Jopour oy} pue osensury] josrey oy} 07 poyesuesy SI eyep Sutures} Yssuq
ayy ‘(610% “ozporq pue NA\) NIVUL-ALVISNVUL UT “Ustsugq wo pouny-ouy st Popout oy} Woy} pur YsI[suy 0} poyzestrery
SI Byep so} oSeNSurLT yoSiey oud ‘(610% “TR 99 UAC) LSAL-ALLWISNViLL UL XxX osensury] yo8re} yoro Jo Aowmodoe 4so4 oy
SUIAOIGUIL UL TI VIA-X 10} sosensury Areyrxne pelyoueq ysour oy} ore (2) ‘T/) ‘oSenSurl yosrey yore soy oSensury ArelIxne
auo Suisn Aq oouvuLlojiod (XW) UMUITxeUT pue (HAV) osesoae oy} ylodor OsTe oA “AOVINOIV OBVIOAV OSTM-MOI SO}ROIPUL
UUINTOS SAV oLJ, ‘“osenSury joS1e} ay} JO Yos 4S0} oY} UO oyeNTeAd pu jos JUoUdOTIASp oY UO Jopouw oy} ouNy-ouy om ‘SUTUTLET
joYys-Mo} (IT) 1oJ pu fos<ensury 4o81e} oy} JO Jos 4So} oY} UO Jopoul pouTer}-o1d oY} oyeNTRAS OM ‘SUTUTROT JOYS-O.10zZ (1) IOJ [epou
ouTpeseq TYAA THIN Mo Joy “sfepou TYAATMHNIN osTe ere (610%) ezporq pue nay pue (6T0Z) ‘Te 39 UAeq Jo sfepout oy],

‘soseNSUR] JOSIEY OY} OYROIPUL SUTINJOD “TTVIN-X JOYS-Mof PUB -O10Z OJ 49S 4809 [TNX oY} UO synsor AoVMIIV :9°9 BIBL,

OTL 9°09 G9 eg9 Z OL ZE9 819 8°02 9°02 ev. VSL Teh 8 UL G’sL 6°92 TG8 (610Z) ozporq pure nA,
(NIVUL-ALVTISNVELL) Supusos, yo aynjsunsy aunyoopy
- 129 - - TOL - - VOL - - - - PrL = PL - vis (610z) ‘Te 3 UIAeq
(LSALALV TSNVAL) 1894 10 aqvjsunay aunyooyy
6902 | 69°79 FE6S GLso G8'PL Goo L8EL V89 6999 GTtL LLL 9902 LOPL 992 L6GL TL%s Xe (7G
(natua) (agian) (aatms) = (ygiun) = (xanga) (na‘6q) — (vy‘6q) = (ms‘sa) — (ug’'op) = (yz‘un) — (maims) (aya) = (yaina) = (ypina) = (nutun) | (bun) “enn omy) TINVIN-X
- £9'€9 - VOI 66°EL GTI9 see L419 6879 TETL L2LTL 8669 PeeL PEGL TESL POs X4> Oe
TOOL | 189 €osS LOPO9 I2rL PLTI9 CGEL G6L9 Pr GVIL P6TL €00L 9FEL 8T9L GEGL 6&8 XVIV
& 9969 GOSS E879 CGP L919 THeL G49 979 B8GTL O8TL L669 PEEL 909L POGL GoC8 DAV
(bun) “env 2uQ) TWWIN-X
s969 | TZ€9 G8l4o L479 86eL B8TTI9 eel LEL9 S879 P80L O9TL PS69 GoeL 6L'SL 6E°SL V6T8 (ourjoseq MO) LYAG-HIMW
Burusna) OYS-Nay
86°89| €6c9 cog egeod eTeL 49°79 E9CL 6TL9 L279 PHIL cytL 9169 SVEL LEGL 69'SL 69°28 xe (By
(2p) (4872) (mitan) —(a’nt) (an'6q) (6p) (412) (mstan) (6q'ap) — (na‘ay) — (metan) (ye'6q) (apy) (49%) (@p'4y) | (Suny “anv omy) TNVIN-X
a TGT9 = GT LP = 690L 9669 L1¢9TL 9899 e609 P69 9889 S619 69TL I8fL LIPVL 888 XM
eel19 | IST9 9L6h 0069 ITIL 6899 G8 TL so99 O@T9 «=66T0L «6S 69) = G6L9 STL «=6—LOGL «= GPL =~ 60GB XVIV
- v66S LOLP 1909 600L 807 G60L eeS9 IP09 0689 6889 9TL19 OOTL eFPL 98EL 69TS DAV
(‘6unj ‘env 2uQ) TWVIN-X
€eo9 ) OLS B8olp 998 0689 9FZS cZTOL e979 PO6S FEL9 C819 EL69 PL69 GEL GPEL 9ET8 (oureseq MO) TMAC-HIMIN
£99 08S v0S 0°09 £69 gcc o'69 679 919 0°69 6°89 v99 TTL er sel TGS (6102) ozporq pure nA,
- GE'8g - - 8'e9 - - v9 - - - - GOL PL - ris (610z) ‘Te 3 UIAoq
dafsuni, JOnbuL]-sso19 JoYs-o19Z

Bae am ms 1 yz uy IA re ay ni 3q ic) op so y ua

133


6. Natural Language Understanding in Low-Resource Genres and Languages

Target Languages
ru hi fr es en el de bg ar

th sw

tr

bg de el en es fr hi ru sw th tr ur
Auxiliary Languages

Figure 6.6: Differences in performance in terms of accuracy scores on the test set
for zero-shot X-MAML on XNLI using the Multi-BERT model. Rows correspond
to target and columns to auxiliary languages used in X-MAML. Numbers on
the off-diagonal indicate performance differences between X-MAML and the
baseline model in the same row. The coloring scheme indicates the differences in
performance (e.g., blue for large improvement).

information) in X-MAML for XNLI, we fine-tune Multi-BERT on the MultiNLI
dataset (English) to obtain the initial model parameters 6 for each experiment.
We go on to apply the second and third steps of X-MAML in the zero-shot
scenario. We report the impact of meta-learning for each target language as a
difference in accuracy with and without meta-learning on top of the baseline
model (Multi-BERT) on the test set (Figure 6.6). Each column corresponds
to the performance of Multi-BERT after meta-learning with a single auxiliary
language, and evaluation on the target language of the XNLI test set. In general,
we observe that our zero-shot approach with X-MAML outperforms the baseline
model without MAML and results reported by Devlin et al. (2019). This way,
we improve the performance of Multi-BERT in zero-shot cross-lingual NLI. We

134


Experiments

Auxiliary language baseline
ar bg de el en es fr hi ru sw th tr ur vi zh

ar - 65.76 65.48 66.05 64.41 65.27 65.24 65.86 65.31 63.66 65.25 65.58 65.56 65.84 65.32 64.63
bg 68.36 - 68.79 68.39 67.95 68.45 68.80 68.86 69.41 66.10 67.62 67.95 68.63 68.67 69.45 67.82
de 70.88 71.46 - 71.26 71.09 71.12 71.11 71.59 71.83 68.65 70.29 70.37 71.42 71.15 71.83 69.74
el 67.53 67.58 67.25 - 66.11 67.13 67.39 67.95 67.71 65.11 67.12 67.15 67.69 67.19 67.34 65.73
en 81.68 81.79 82.02 81.77  - 81.88 81.91 81.88 82.03 80.44 81.18 81.43 81.80 81.73 82.09 81.36
es 74.48 74.51 74.63 74.58 74.4 - 74.95 74.81 74.63 72.66 73.91 74.12 74.51 74.71 75.07 73.85
fr 74.13 74.02 74.22 74.11 73.75 74.18 - 74.17 74.34 71.87 73.04 73.41 74.15 74.21 74.42 73.45
hi 60.75 61.59 60.84 60.61 59.31 60.18 60.66 - 61.75 57.10 59.39 60.47 62.20 60.76 61.56 58.56
ru 68.78 69.47 69.47 68.93 68.64 68.89 69.25 69.44 - 66.11 68.18 68.72 69.52 69.02 70.19 67.94
sw 48.71 48.53 47.36 49.13 46.70 48.43 47.81 47.11 47.28 - 49.20 49.76 46.61 48.43 46.50 47.58
th 54.65 55.39 53.80 54.98 51.14 54.09 54.15 55.26 53.82 52.90 - 55.24 53.79 54.99 52.85 52.46
tr 60.94 61.20 60.22 61.09 58.66 60.60 60.32 60.93 60.29 59.98 60.53 - 60.82 60.68 59.47 59.04
ur 60.30 60.87 60.34 60.20 58.82 59.81 60.12 61.51 61.02 56.37 59.38 60.02 - 59.87 60.46 58.70
vi 71.27 71.56 71.32 71.14 70.35 71.22 71.42 71.57 71.73 68.11 69.87 70.53 71.43  - 71.82 70.12

zh 70.24 70.68 70.65 70.12 69.91 70.29 70.47 70.59 71.11 67.47 69.33 69.50 70.29 70.54 - 68.90

Table 6.7: The performance in terms of average test accuracy for the zero-
shot setting over 10 runs of X-MAML on the XNLI dataset using Multi-
BERT (multilingual BERT), as base model. Each column corresponds to
the performance of the Multi- BERT system after meta-learning with a single
auxiliary language, and evaluation on the target language of the XNLI test set.
The auxiliary language is not included during the evaluation phase. Results of
the Multi: BERT model without X-MAML (baseline) are also reported.

observe the largest difference in performance when transferring from Urdu (ur)
as an auxiliary language to Hindi (hi) as a target (e.g., +3.6% in accuracy). We
also detect strong gains when transferring from Urdu (ur), Russian (ru), and
Bulgarian (bg) as auxiliary languages in X-MAML.

Furthermore, Hindi (hi) is the most effective auxiliary language and provides
the highest average accuracy in the zero-shot setting. Table 6.7 shows the average
accuracy over ten runs of X-MAML on the XNLI dataset using Multi-BERT as
the base model. Each column corresponds to the performance of the Multi- BERT
system after meta-learning with a single auxiliary language, and evaluation on
the target language of the XNLI test set.

We hypothesize that the degree of typological commonalities among the
languages affects (i.e., positive or negative) on the performance of X-MAML
and will return to this below. It can be observed that the proposed learning
approach provides positive impacts across most of the target languages. However,
including Swahili (sw) as an auxiliary language in X-MAML is not beneficial for
the performance on the other target languages.

In Table 6.6, we include the original baseline performances reported in Devlin
et al. (2019)° and Wu and Dredze (2019). We report the average and maximum
performance by using one auxiliary language for each target language. We also
report the performance of X-MAML by also using Hindi (which is the most

5https://github.com/google-research/bert/blob/master/multilingual.md

135


6. Natural Language Understanding in Low-Resource Genres and Languages

effective auxiliary language for the zero-shot setting, as shown in Figure 6.6).
We Once again, suspect that this may be because of the typological similarities
between Hindi (hi) and other languages.

Now we conduct the zero-shot X-MAML using two auxiliary languages (see
Step 3 in Section 6.6). The results (Table 6.6) show that X-MAML using two
auxiliary languages obtains the largest benefit in the zero-shot experiments.
It improves our internal Multi- BERT baseline by +3.65% points in terms of
average accuracy © on the zero-shot scenario. We report the most beneficial pair
of auxiliary languages for the zero-shot X-MAML in improving the test accuracy
of each target language in Table 6.6.

We further experiment with regular training of the model using an auxiliary
language, instead of performing meta-learning (step 2 in Section 6.6), followed
by zero-shot learning on the target languages. In other words, we apply all steps
of X-MAML as explained in Section 6.6, however instead of step 2, we perform
regular supervised learning using the development set of the auxiliary language.
We evaluate the final model on the test sets of the target languages. From
this experiment, we observe that meta-learning has a strongly positive effect on
predictive performance (see Figure 6.7). Comparing the results in Figure 6.6
and Figure 6.7 shows that we have similar trends of the improvements, however
using meta-learning boost performance on all languages in the XNLI dataset up
to 3.6%, while the largest improvement without meta-learning is 2.5%.

Zero-shot X-MAML with En-BERT Similar to the previous section, as the
first training step (i.e., pre-training on a high-resource language, see Step 1
in Section 6.6 for more information) in X-MAML for XNLI, we fine-tune En-
BERT on the MultiNLI dataset (English) to obtain the initial model parameters
9 for each experiment. Then, we apply the second and third steps of X-MAML
in the zero-shot scenario. Figure 6.8 and Table 6.8 depict the results of this
experiment.

We observe an improvement in accuracy by performing X-MAML on cross-
lingual NLI using En-BERT (see Figure 6.8). We further note that English as
an auxiliary shows negative impact (i.e., decreasing performance) in most of the
cases. In the reverse setting, using any other language as an auxiliary does not
lead to improvement on the English test dataset. The experiments show that
the target languages such as Spanish (es), French (fr) and German (de) obtain
the largest gains (i.e., improvements up to +9.3% points in terms of average
accuracy), while languages such as Thai (th), Swahili (sw) and Vietnamese (vi)
get the lowest gains in X-MAML on the cross-lingual NLI using En-BERT. This
can possibly be attributed to the fact that the performance of En-BERT depends
directly on word piece overlap, as denoted by Pires et al. (2019). For the exact
accuracy scores, we refer to Table 6.8.

6We consider only the best auxiliary languages for each target language, and then take the
average.

136


Experiments

fr es en el de bg ar

Target Languages
ru hi

th sw

tr

ur

h vi

0.79 |e
N

ar bg de el en es fr hi ru sw th tr ur vi zh
Auxiliary Languages

Figure 6.7: Differences in performance in terms of accuracy scores on the test
set for the zero-shot case using training (without meta-learning) on XNLI with
the Multi-BERT model. Rows correspond to target and columns to auxiliary
languages used in X-MAML. Numbers on the off-diagonal indicate performance
differences between training on the auxiliary languages (without meta-learning)
and the baseline model in the same row. The coloring scheme indicates the
differences in performance (e.g., blue for large improvement).

6.7.3.2 Few-Shot Learning

For few-shot learning, following the steps in X-MAML, we perform fine-tuning on
the development set (2.5k instances) of the target languages, and then evaluate
on the test set (Step 3 in Section 6.6). We employ Multi-BERT as the underlying
model M in this scenario. Detailed ablation results are presented in Table 6.9
and Figure 6.9.

Overall, these results demonstrate that we have a positive impact on most of
the low-resource target languages. However, the improvements in the few-shot
X-MAML are lower compared to the zero-shot setting (i.e., improvements up
to +0.61% points in terms of average accuracy for few-shot X-MAML on XNLI

137


6. Natural Language Understanding in Low-Resource Genres and Languages

ca 2 a
ES a

fr es en el de bg ar

Target Languages

th sw ru hi

f ls Ba i I
ar bg de el en es fr hi ru sw th tr ur vi
Auxiliary Languages

Figure 6.8: Differences in performance in terms of accuracy scores on the test
set for zero-shot X-MAML on XNLI using the En-BERT (English) model. Rows
correspond to target and columns to auxiliary languages used in X-MAML.
Numbers on the off-diagonal indicate performance differences between X-MAML
and the baseline model in the same row. The coloring scheme indicates the
differences in performance (e.g., blue for large improvement).

using the Multi- BERT model). Target languages such as Hindi (hi), Russian (ru),
Thai (th), Arabic (ar) and Greek (el) benefit from X-MAML with Multi-BERT.
At the same time, the few-shot X-MAML with Multi-BERT provides negative
impacts for French (fr), Turkish (tr) and Urdu (ur) as target languages.

In Table 6.6, we compare X-MAML results with one and two auxiliary
languages to the external and internal baselines. We detect that using two
auxiliary languages in the meta-learning step (Step 2 in Section Section 6.6)
leads to similar conclusions as before (i.e., using two auxiliary languages leads
the largest benefits in the few-shot X-MAML with Multi-BERT).

In contrast to the zero-shot X-MAML with Multi-BERT, we observe that
Swahili (sw) acts as the overall most effective auxiliary language for meta-learning

138


Experiments

Auxiliary lang. baseline
ar bg de el en es fr hi ru sw th tr ur vi zh

39.09 37.32 40.90 34.48 36.49 36.65 39.24 39.10 38.09 35.48 38.36 39.79 37.46 37.03 34.47

ar a
bg 42.33

- 38.29 41.92 35.17 37.55 37.58 40.04 38.93 38.32 36.37 38.72 40.90 37.81 37.41 35.23
de 41.88 42.77  - 41.59 37.68 46.41 46.43 40.90 42.70 44.89 39.42 45.70 41.05 45.03 40.30 38.52
el 40.08 38.50 38.70 - 35.18 37.65 37.80 40.15 38.72 39.42 35.91 39.82 41.06 38.73 37.99 35.15
en 81.95 81.87 81.89 82.22 - 82.12 82.05 82.23 81.88 82.23 82.52 82.01 82.03 82.29 82.27 83.45

es 47.41 47.64 53.24 46.59 42.81
fr 45.55 46.40 49.81 44.81 40.0
hi 39.61 38.91 36.91 39.32 34.4 -
ru 41.87 38.73 39.10 41.98 35.05 38.02 38.13 40.73 36.11 39.51 41.12 38.51 37.69 35.09

6 53.18 45.79 47.56 51.
8
6
5 -
sw 39.05 37.55 40.07 39.00 36.41 40.33 39.85 38.45 37.57 — - 37.26 42.01 38.82 42.70 38.72 37.96
3
4
6
3
8

3

0 44.69 52.04 46.30 50.83 45.87 43.95
44.14 46.30 48.05 42.13 48.54 44.24 48.67 43.58 41.04

4

9

49.89

36.87 36.78 39.08 37.14 35.88 37.15 39.98 37.20 37.40 34.69

th 36.16 35.41 36.46 36.17 35.63 36.43 36.32 35.64 35.43 36.7. 36.91 36.05 36.63 36.67 35.73
tr 39.33 37.62 41.44 39.42 37.3

4s
42.07 41.26 38.83 37.63 44.12 38.23 -
ur 36.85 38.46 36.27 39.55 34. 35.72 35.63 39.09 38.64 36.80 35.33 36.94 =
vi 41.85 39.35 42.97 41.62 38.53 43.85 42.52 40.53 39.38 45.46 39.89 45.11 41.63 -
zh 37.21 36.09 37.18 36.68 34.48 36.33 36.55 35.25 36.16 37.73 36.64 37.70 35.99 37.66 - 34.63

38.97 43.42 39.86 38.84
36.91 36.85 33.93

Table 6.8: The performance in terms of average test accuracy for the zero-
shot setting over 10 runs of X-MAML on the XNLI dataset using En-BERT
(monolingual), as base model. Each column corresponds to the performance of
the En-BERT system after meta-learning with a single auxiliary language, and
evaluation on the target language of the XNLI test set. The auxiliary language
is not included during the evaluation phase. Results of the En-BERT model
without X-MAML (baseline) are also reported.

with Multi-BERT in the few-shot learning setting (see results in the few-shot
learning section in Table 6.6).

Moving on, in Table 6.6 we report results from Devlin et al. (2019) that use
machine translation at test time (TRANSLATE-TEST) and results from Wu
and Dredze (2019) that use machine translation at training time (TRANSLATE-
TRAIN), where they have been shown to be strong baselines in previous work.
In TRANSLATE-TRAIN, the English training data is translated to the target
language, and the model is fine-tuned using the translated data. While in
TRANSLATE-TEST, the target language test data is translated into English
sand then the model is fine-tuned on the translated version.

Note that, using X-MAML, we can alleviate the machine translation step
(TRANSLATE-TEST) from the target language into English. The results in
Table 6.6 also indicate that X-MAML boosts Multi-BERT performance on XNLI.
It is worthwhile mentioning that Multi-BERT in the TRANSLATE-TRAIN
setup outperforms our few-shot X-MAML. However, we only use 2k development
examples from the target languages, whereas work mentioned above, 433k
translated sentences are used for fine-tuning.

6.7.4 Zero-Shot Cross-Lingual QA

Here, we attempt to answer our research questions RQ 6.2 and 6.3 in the cross-
lingual QA. To understand whether our framework is model- and task- agnostic

139


6. Natural Language Understanding in Low-Resource Genres and Languages

0.60

1 oo = on oo
To

0.45

= EE Bg

; 0.30
n O
Et
oD
§= 0.15
2 3
“3

sg 0.00

-0.15

ar bg de el en es fr hi ru sw th tr ur vi zh
Auxiliary Languages

Figure 6.9: Differences in performance in terms of accuracy scores on the test set
for few-shot X-MAML on XNLI using the Multi- BERT model. Rows correspond
to target and columns to auxiliary languages used in X-MAML. Numbers on
the off-diagonal indicate performance differences between X-MAML and the
baseline model in the same row. The coloring scheme indicates the differences in
performance (e.g., blue for large improvement).

and can apply to other tasks and models besides NLI and BERT, we conduct
additional experiments for the question answering task. We investigate the
impact of X-MAML on other pre-trained language models, namely XLM and
XLM-RoBERTa (XLM-R) (see Section 6.3). We use these models as the base
model M in X-MAML for our QA experiments. We employ the XLM-15 version
of XLM, XLM-Roase and XLM-Riarge versions of XLM-R (see Section 6.3). The
SQuAD v1.1 training data (see Section 6.2.2) is used in the pre-training step
of X-MAML (see Step 1 in Section 6.6). We use the cross-lingual development
and test splits provided in the MLQA dataset (Table 6.4) for meta-learning and
evaluation steps, respectively. We use a similar approach for cross-lingual QA
on the MLQA dataset.

Table 6.10 shows the results of zero-shot X-MAML for the MLQA dataset. We

140


Discussion and Analysis

Auxiliary language baseline
ar bg de el en es fr hi ru sw th tr ur vi zh

ar - 67.84 67.73 67.85 67.62 67.84 67.80 67.81 67.85 67.87 67.86 67.83 67.71 67.89 67.95 67.37
bg 71.79 - 71.76 71.80 71.72 71.77 71.80 71.74 71.94 71.77 71.78 71.78 71.77 71.79 71.92 71.60
de 73.36 73.23. - 73.37 73.30 73.30 73.33 73.46 73.27 73.34 73.38 73.32 73.37 73.34 73.43 73.25
el 69.95 69.98 69.97 - 69.94 69.99 69.91 69.93 69.95 69.98 70.03 70.02 69.90 69.95 70.03 69.54
en 82.24 82.21 82.13 82.22 - 82.15 82.27 82.26 82.24 82.24 82.19 82.39 82.25 82.14 82.20 81.94
es 76.07 76.12 76.14 76.02 76.06 - 76.18 76.14 76.10 75.94 76.03 75.91 76.10 76.00 76.09 75.79
fr 75.32 75.23 75.16 75.24 75.23 75.18 - 75.19 75.22 75.31 75.28 75.19 75.28 75.19 75.28 75.39
hi 64.95 64.82 64.78 64.89 64.64 64.63 64.90 - 64.87 64.94 64.73 64.84 64.79 64.97 64.83 64.37
ru 71.19 71.27 71.17 71.33 71.19 71.19 71.33 71.28 - 71.31 71.34 71.45 71.18 71.29 71.38 70.84
sw 58.14 58.23 57.95 57.99 57.53 57.97 57.94 58.10 58.04 - 58.00 58.22 58.08 58.01 58.09 57.82
th 61.59 61.64 61.57 61.71 61.40 61.51 61.51 61.68 61.54 61.50 - 61.58 61.41 61.56 61.74 61.18
tr 64.74 64.79 64.69 64.82 64.59 64.82 64.76 64.83 64.70 64.89 64.92 - 64.74 64.73 64.66 64.85
ur 63.67 63.58 63.69 63.63 63.55 63.63 63.68 63.61 63.72 63.63 63.72 63.81 - 63.67 63.60 63.7
vi 73.51 73.52 73.46 73.35 73.36 73.29 73.39 73.31 73.51 73.38 73.39 73.41 73.42 - 73.41 73.23
zh 74.04 73.97 74.02 74.02 73.74 74.01 74.02 74.10 74.11 73.99 74.01 74.21 74.06 73.95 - 73.93

Table 6.9: The performance in terms of average test accuracy for the few-
shot setting over 10 runs of X-MAML on the XNLI dataset using Multi-
BERT (multilingual BERT), as base model. Each column corresponds to
the performance of the Multi-BERT system after meta-learning with a single
auxiliary language, and evaluation on the target language of the XNLI test set.
The auxiliary language is not included during the evaluation phase. Results of
the Multi: BERT model without X-MAML (baseline) are also reported.

compare our results on the MLQA dataset to those reported in two benchmark
papers, Hu et al. (2020a) and Liang et al. (2020). We also report our own
baseline for the task. The baselines are provided by training each base model on
the SQuAD v1.1 train set (see Step 1 in Section 6.6) and evaluating on the test
set of MLQA. In Table 6.10, we consider only the best auxiliary languages for
each target language, and then compute the average F; score.

We observe that all of the target languages benefit from at least one of
the auxiliary languages by adapting the models using X-MAML, highlighting
the benefits of this method. We find that: (i) our zero-shot results with
X-MAML improve on those without meta-learning (i.e., baselines); (ii) performing
X-MAML with two auxiliary languages provides the largest gains for the models in
cross-lingual QA. Overall, zero-shot learning models with X-MAML outperform
both internal and external baselines. The improvement is +1.04%, +0.89% and
+1.47% in average F score compared to XLM-15, XLM-Royase and XLM-Ryarge,
respectively.

6.8 Discussion and Analysis

Somewhat surprisingly, we find that cross-lingual transfer with meta-learning
yields improved results even when languages strongly differ (i.e., in terms of
language family) from one another. For instance, for zero-shot meta-learning on
XNLI, we observe gains for almost all auxiliary languages, except for Swahili (sw).

141


6. Natural Language Understanding in Low-Resource Genres and Languages

Model en ar de es hi vi zh avg
Our baseline 69.80 48.95 52.64 58.15 46.67 48.46 42.64 | 52.47
=| | (One auz. lang.) 69.39 48.45 53.04 57.68 46.90 49.79 44.36 | -
ais : : 52.80
e a lo XxX ar hi es en zh zh hi
a (Two aux. lang.) 68.88 49.76 53.18 58.00 48.43 50.86 45.44 53.51
*% | (li,le) 3X (es,ar) (vi,zh) (vi,zh) (en,zh) (vi,zh) (hi,zh)  (es,hi) .
3 Liang et al. (2020) | 80.1 56.4 62.1 67.9 60.5 67.1 61.4 65.1
8 Our baseline 80.38 57.23 63.08 67.91 61.46 67.14 62.73 | 65.70
Gi S| (One aux. lang.) | 80.19 57.97 63.57 67.46 61.70 67.97 64.01 66.12
a a 1+ xX vt hi ar vi vt hi hi :
ca
= | (Two aux. lang. 80.31 58.14 64.07 68.08 62.67 68.82 64.06
Y 66.59
| (hl) > X (ar,vi) (hi,vi) (ar,hi) (ar,hi) (es,ar) (ar,hi) (ar,hi) ,
» Hu et al. (2020a) 83.5 66.6 70.1 74.1 70.6 7A 62.1 71.6
2 Our baseline 83.95 66.09 70.62 74.59 70.64 74.13 69.80 | 72.83
e)3 (One aux. lang.) 84.31 66.61 70.84 74.32 70.94 74.84 70.74
2/5 ' | 73.93
hs a lax ar hi ar hi vt ar hi
et a (Two aux. lang.) | 84.60 66.95 71.00 74.62 70.93 74.73 70.29 74.30
KI (li,le) > X (hi,vi) (hi,vi) (ar,vi) (envi) (ar,vi) (es,hi) (en,vi) .

Table 6.10: F1 scores (average over 10 runs) on the MLQA test set using
zero-shot X-MAML. Columns indicate the target languages. The avg column
indicates row-wise average F1 score. We also report the most beneficial auxiliary
language/s for X-MAML in improving the test F1 of each target language.

This indicates that the meta-parameters learned with X-MAML are sufficiently
language agnostic, as we otherwise would not expect to see any benefits in
transferring from, e.g., Russian (ru) to Hindi (hi) (one of the strongest results
in Figure 6.6). This is dependent on having access to a pre-trained multilingual
model such as BERT; however, using monolingual BERT (En-BERT) yields
overwhelmingly positive gains in some target/auxiliary settings (see results
in Figure 6.8). For few-shot learning when we only have access to a handful
of training instances, our findings are similar, as almost all combinations of
auxiliary and target languages lead to improvements when using Multi-BERT
(Figure 6.9). Therefore, we try to shed light on the behavior of our proposed
model and answer our last research question (i.e., RQ 6.4) in the following
section.

6.8.1 Typological Correlations

In order to better explain our results for cross-lingual zero-shot and few-
shot learning, we investigate typological features, and their overlap between
target and auxiliary languages. We evaluate on the World Atlas of Language
Structure (WALS, Dryer and Haspelmath (2013)), which is the largest openly

142


Discussion and Analysis

available typological database. It comprises approximately 200 linguistic features
with annotations for more than 2500 languages, which have been made by
expert typologists through the study of grammars and field work. We draw
inspiration from previous works (Bjerva and Augenstein, 2018a; Bjerva and
Augenstein, 2018b) that attempt to predict typological features based on language
representations learned under various NLP tasks. Similarly, we experiment with
the two following conditions:

(i) We attempt to predict typological features based on the mutual gain/loss
in performance using X-MAML.

(ii) We investigate whether sharing between two typologically similar languages
is beneficial for performance using X-MAML.

We train one simple logistic regression classifier per condition above, for each
WALS feature. In the first condition (i), the task is to predict the exact WALS
feature value of a language, given the change in accuracy in combination with
other languages. In the second condition (ii), the task is to predict whether a
main and auxiliary language have the same WALS feature value, given the change
in accuracy when the two languages are used in X-MAML. We compare with
two simple baselines, one based on always predicting the most frequent feature
value in the training set, and one based on predicting feature values with respect
to the distribution of feature values in the training set. We then investigate
whether any features could be consistently predicted above baseline levels, given
different test-training splits. We apply a simple paired t-test to compare our
model predictions to the baselines. As we are running a large number of tests
(one per WALS feature), we apply Bonferroni correction, changing our cut-off
p-value from p = 0.05 to p = 0.00025.

We first investigate a few-shot X-MAML, when using Multi-BERT, as
reported in Table 6.9. We find that languages sharing the feature value for
WALS feature 67A The Future Tense are beneficial to each other. This feature
encodes whether or not a language has an inflectional marking of the future
tense, and can be considered to be a morphosyntactic feature. We next look
at zero-shot X-MAML with Multi-BERT, as reported in Table 6.7. For this
case, we find that languages sharing a feature value for the WALS feature 25A
Locus of Marking: Whole-language Typology typically help each other. This
feature describes whether the morphosyntactic marking in a language is on the
syntactic heads or dependents of a phrase. For example English (en), German
(de), Russian (ru), and Chinese (zh) are ‘dependent-marking’ in this feature.
Moreover, if we look at the results in Figure 6.6, they have the largest mutual
gains from each other during the zero-shot X-MAML, as shown in Figure 6.10. In
both cases, we thus find that languages with similar morphosyntactic properties
can be beneficial to one another when using X-MAML.

143


6. Natural Language Understanding in Low-Resource Genres and Languages

‘ {
jen

7) ' ' ' '
® ' ' ' ' '
D ' ' 1
orc , 5 ‘

= 1 1
© ' ' '

aa ‘ : i

o ‘ 1 q '

-®

BOOS sce

‘de: el'en:es fr hi: ru:sw th tr ur wi: zh:

Auxiliary Languages

oo
ar

Figure 6.10: The mutual gains among English (en), German (de), Russian
(ru), and Chinese (zh) languages in zero-shot X-MAML with Multi-BERT.
Rows correspond to target and columns to auxiliary languages used in X-
MAML. Numbers indicate performance differences between X-MAML and the
baseline model in the same row. The coloring scheme indicates the differences in
performance (e.g., blue for large improvement).

6.9 Summary

In this chapter, we show that meta-learning allows us to leverage training data
from auxiliary languages and genres, to perform the zero-shot and few-shot
cross-lingual and cross-genre transfer. We achieve competitive performance
compared to a machine translation baseline (for XNLI), and propose a method
that requires no training instances for the target task in the target language.
Experiments with different models show that our method is model agnostic,
and can be used to extend any pre-existing model. We evaluated this on two
challenging NLU tasks (NLI and QA), and on a total of 15 languages. We can
improve the performance of strong baseline models for (i) zero-shot XNLI, and
(ii) zero-shot QA on the MLQA dataset. Furthermore, we show in a typological
analysis that languages which share certain morphosyntactic features tend to
benefit from this type of transfer.

To summarize, the contribution of this chapter (detailed in Section 6.6) is
four-fold. Concretely, we:

(i) exploit the use of meta-learning methods for two different natural language
understanding tasks (i.e., NLI, QA);

(ii) evaluate the performance of the proposed architecture on various scenarios:
cross-genre, cross-lingual, standard supervised, and zero-shot, across a total

144


Summary

(iii

)

WN

of 15 languages (i.e., 15 languages in XNLI and 7 languages in MLQA);

observe consistent improvements of our cross-lingual meta-learning ar-
chitecture (X-MAML) over the previous models on various cross-lingual
benchmarks (i.e., improving the Multilingual BERT model by +3.65%
and +1.04% points in terms of average accuracy on zero-shot and few-shot
XNLI, respectively, and boosting the XLM-Riarge by +1.47% in terms of
average F, score on zero-shot QA);

perform an error analysis, which reveals that typological commonalities
between languages can partially explain the cross-lingual trends.

145




Chapter 7
Conclusion and Future work

This thesis investigates methods for dealing with low-resource scenarios in
information extraction and natural language understanding tasks. To this end,
we study distant supervision and sequential transfer learning in various low-
resource settings. We develop and analyze models to explore three essential
questions concerning NLP tasks with minimal or no training data which cut
across several of the chapters in this thesis (see Table 7.1 for an overview that
maps the general research questions to individual chapters and sub-questions):

RQI. What is the impact of different input representations in neural low-resource
NLP?

RQ Il. How can we incorporate domain knowledge in low-resource NLP?

RQ Ill. How can we address challenges of low-resource scenarios using transfer
learning techniques?

During the course of this thesis we have made contributions in low-resource NLP
in four different areas: domain-specific embeddings (Chapter 3), named entity
recognition (Chapter 4), relation extraction and classification (Chapter 5), and
cross-genre and cross-lingual natural language understanding (Chapter 6). In
the following, we describe our proposed methods and findings (Section 7.1). Our
main contributions are summarized in Section 7.2, and we provide an outlook
into future directions in Section 7.3.

7.1. Proposed Methods and Findings

Previous research shows that automatically learning transferable representations
in terms of word embeddings boosts NLP models’ performance in various down-
stream tasks. The following research question addresses a central challenge that
needs to be answered for embeddings in a technical domain:

e RQ 3.1. Can word embedding models capture domain-specific semantic
relations even when trained with a considerably smaller corpus size?

To answer this question, we here focus on a new and relatively unexplored
technical domain: the oil and gas domain, train domain-specific embeddings
on this technical low-resource domain. We further construct a domain-specific
evaluation dataset, including a corpus and a query inventory for the oil and gas
domain (Section 3.3.1). We evaluate, in Sections 3.4 and 3.7, the effectiveness of
domain-specific models using intrinsic and extrinsic evaluations. In Section 3.4,
empirical intrinsic evaluations reveal that domain-specific trained embeddings
perform better than general domain embeddings trained on much larger input

147


7. Conclusion and Future work

Question Chapter | Sub-Question
RQ I: What is the impact of differ-| Chapter 3 RQ 3.1
eens in neural RQ 5.1
° Chapter 5 RQ 5.2
RQ 5.3
RQ II: How can we incorporate Chapter 3 RQ 3.2

domain knowledge in low-resource
RQ 4.1
?

NLP! Chapter 4 RQ 4.2
RQ II: How can we address chal-| Chapter 3 RQ 3.1
lenges of low-resource scenarios us- RQ 51
ing transfer learning techniques? Chapter 5 RQ 5.9
RQ 6.1
Chapter 6 RQ 6.2

Table 7.1: Overview of the research questions and related chapters.

data. Furthermore, in Section 3.5, the in-depth manual analysis shows the
ability of the domain model to discover semantic relations such as (co)hyponymy,
hypernymy, and relatedness, giving insight into these models beyond the intrinsic
evaluation dataset.

In our target domain, in addition to text, there exists a domain-specific
knowledge resource (i.e., Schlumberger oilfield glossary) created by domain
experts to facilitate information processing. We here pose the following research
question:

e RQ 3.2. How can we take advantage of existing domain-specific knowledge
resources to enhance the resulting models?

We enhance, in Section 3.6, the domain embeddings by incorporating domain
knowledge from the oilfield glossary and constructing embedding representations
for infrequent technical terms. We find that the domain embeddings and their
enhanced versions can be useful resources to support a downstream domain-
specific NLP task (Section 3.7.3). The results on a multi-label domain-specific
sentence classification task show that the enhanced domain embeddings provide
higher performance and aid the model in label assignment.

NER is a central task in NLP and one that often requires domain-specific,
annotated data. In Chapter 4, we focus on the named entity recognition task in
several low-resource domains. We here pose the following research questions:

e RQ 4.1. How can we address the problem of low-resource NER using
distantly supervised data?

148


Proposed Methods and Findings

e RQ 4.2. How can we exploit a reinforcement learning approach to improve
NER in low-resource scenarios?

We introduce a framework to address the common challenges of distantly
supervised datasets for low-resource NER. The main concerns in distantly
supervised NER are false positive and false negative instances. Our framework
combines a neural NER model with a partial-CRF layer and a policy-based
reinforcement learning component (Section 4.4). The partial-CRF component
(PA in Section 4.4.2) is designed to deal with the false negatives, while the
reinforcement-based module (RL in Section 4.4.3) handles the false positives
instances. We quantify, in Section 4.6, the impact of each component in our
proposed framework. We further in Section 4.7, investigate the performance
of our model under settings using different sizes of human-annotated data.
The ablation studies determine the efficiency of the partial-CRF and policy
reinforcement modules in fixing the problems in the distantly annotated NER
datasets. Overall, our final system, a combination of NER, PA, and RL, achieves
an improvement of +2.75 and +11.85 F1 on the BC5CDR and LaptopReview
respectively over the baseline system. Furthermore, we observe that our model
can deliver relatively good performance with a small set of gold data. Our final
method achieves a performance of 83.18 and 63.50 with only 2% of the annotated
dataset in the BC5CDR and LaptopReview domains, respectively. In contrast,
the base NER, model requires almost 45% of the ground truth sentences to reach
the same performance.
In Section 4.6, we aim to answer:

e RQ 4.3. Is the proposed solution beneficial for different low-resource
scenarios?

We evaluate our model across four diverse datasets from different domains (i.e.,
biomedical, e-commerce, technical reviews, and news) and languages (English
and Chinese). Experimental results show that our approach can boost the
performance of the neural NER system in resource-poor settings and achieve
higher F1 scores on the different datasets compared to previous work +.
Another central IE task is relation extraction. In Chapter 5, we introduce
an adapted neural framework incorporating domain-specific embeddings and
syntactic structure to address low-resource relation extraction tasks in the
SemEval 2018 task 7. Our framework is based on a CNN architecture over
the shortest dependency paths between entity pairs for relation extraction and
classification in scientific text (Section 5.5). The framework leverages knowledge
from both domain-specific embeddings and syntactic representations to help the
low-resource relation extraction task. It ranks third in all three sub-tasks of the
SemEval 2018 task. With this, we attempt to answer the following questions:

e RQ 5.1. Are domain-specific input representations beneficial for relation
extraction task?

1At the time of publishing the results were state-of-the-art.

149


7. Conclusion and Future work

e RQ 5.2. What is the impact of syntactic dependency representations in
low-resource neural relation extraction?

We first, in Section 5.6.1, investigate the utility of domain-specific word
embeddings to our neural relation extraction model. The sensitivity analysis
study confirms, what we already found in Chapter 3, the positive impact of
domain-specific embeddings by providing higher performance gains when used
in our model. By inspecting the performance of the model with and without the
dependency paths in Section 5.7.3, we affirm the influence of syntactic structure
compared to a syntax-agnostic approach in this setting. We find that the effect
of syntactic structure varies between different relation types. However, the
syntactic representation has a clear positive impact on all the relation types,
ranging from improvements of 20 to 45 percentage points depending on the
specific relation. We further ask the following question:

e RQ 5.3 Which kind of syntactic dependency representation is most
beneficial for neural relation extraction and classification?

Thus, in Section 5.7.4, we examine the influence of incorporating various
dependency representations in our neural model. We contrast the use of three
input representations for our relation extraction model employing the widely used
CoNLL, Stanford Basic (SB), and Universal Dependencies (UD) schemes. We
compare the effectiveness of specific inputs to our neural relation extraction model
by inspecting the effect of various syntactic representations. Furthermore, we
observe that the widely used Universal Dependencies scheme consistently provides
somewhat lower results in both relation classification and extraction tasks. We,
therefore, opted for manual inspection of a set of incorrect predictions provided
by the Universal Dependencies-based model, which are correctly predicted by
the two other systems (CoNLL and Stanford Basic -based models). Overall, our
results and analysis show that the particular choice of syntactic representation
has clear consequences in downstream processing. We observe that the UD paths
are generally shorter, and the entities often reside within a prepositional phrase.
Whereas the SB and CoNLL paths explicitly represent the preposition in the
path, the UD representation does not. We note that the system benefits from
the explicit inclusion of prepositions in the path, and that the UD treatment of
prepositions as dependent case markers, as well as the copula construction, is
problematic in our system design.

There are several other dimensions of variation for natural language texts,
as discussed in Section 2.1 in Chapter 2. In Chapter 6 we go on to study low-
resource settings in cross-genre and cross-lingual natural language understanding
tasks. We explore the use of meta-learning by leveraging training data from an
auxiliary genre or language, to perform the zero-shot and few-shot cross-lingual
and cross-genre transfer in two different natural language understanding (NLU)
tasks: natural language inference (NLI) and question answering (QA). We here
attempt to answer the following questions:

e RQ 6.1. Can meta-learning assist us in coping with low-resource settings
in natural language understanding (NLU) tasks?

150


Contributions

e RQ 6.2. What is the impact of meta-learning on the performance of
pre-trained language models such as BERT, XLM, and XLM-RoBERTa in
cross-lingual NLU tasks?

e RQ 6.3. Can meta-learning provide a model- and task-agnostic framework
in low-resource NLU tasks?

We propose, in Section 6.6, a cross-lingual meta-learning framework for low-
resource NLU tasks. We evaluate our framework on various scenarios, including
cross-genre and cross-lingual NLI in zero- and few-shot settings across 15
languages (Section 6.7.3). We further, in Section 6.7.4, investigate the model- and
task-agnostic properties of our proposed framework by conducting experiments
for the cross-lingual QA task. The experiments show that our cross-lingual
meta-learning architecture (X-MAML) consistently improves the strong baseline
models. It improves the multilingual BERT by +3.65 and +1.04 percentage
points in terms of average accuracy on zero-shot and few-shot XNLI, respectively.
Furthermore, it boosts the XLM-RoBERTa by +1.47 percentage points in terms
of the average F1 score on zero-shot QA. In Section 6.8, we aim to answer:

e RQ 6.4. Are typological commonalities among languages beneficial for the
performance of cross-lingual meta-learning?

Thus, we conduct an error analysis to explore the impact of typological sharing
between languages in our framework. We evaluate on the World Atlas of
Language Structure (WALS) as the largest openly available typological database.
We attempt to predict typological features based on the mutual gain/loss in
performance using our meta-learning framework. We further investigate whether
the target and auxiliary languages have the same WALS feature value, given
the change in accuracy when the two languages are used in cross-lingual meta-
learning. This indicates that languages with similar morphosyntactic properties
can be beneficial to one another in our meta-learning framework. For instance,
we observe that languages sharing a feature value for the WALS feature 25A
Locus of Marking: Whole-language Typology typically help each other in zero-shot
cross-lingual meta-learning with Multi-BERT.

7.2 Contributions

We here summarize the main contributions of the thesis:

(i) Make use of sequential transfer learning in terms of non-contextualized word
embeddings to address the problem of low-resource domains in downstream
tasks in NLP (see Chapters 3,4 and 5).

(ii) Enhance domain-specific embeddings using a domain-specific knowledge

resource and present a benchmark dataset for intrinsic and extrinsic
evaluation of domain embeddings (Chapter 3).

151


7. Conclusion and Future work

(iii) Propose a hybrid model that combines a reinforcement learning algorithm
with partial annotation learning to clean the noisy, distantly supervised
data for low-resource NER in different domains and languages (see Chapter

(iv) Design a neural architecture with syntactic input representation to alleviate
domain impact in low-resource relation extraction (see Chapter 5).

(v) Introduce a cross-lingual meta-learning framework that provides further
improvements in low-resource cross-lingual NLU tasks in various settings
and languages (see Chapter 6).

7.3 Future directions

Even though the proposed methods achieve competitive performance compared
to previous work in the respective low-resource NLP tasks, there are several
potential avenues for future research. In the following, we will look into some of
the future research directions that can alleviate some of the limitations of the
proposed methods and low-resource NLP in general.

Sequential transfer learning through pre-trained word embeddings has brought
significant improvements for many low-resource NLP tasks. The pre-trained
word embeddings that we employed in chapters 3,4 and 5 provide a single static
representation for each word and have limitations that are already discussed
in Section 2.5.1 of Chapter 2. The immediate idea for improving the proposed
models in this thesis is to exploit the use of contextualized embeddings such as
BERT, ELMo, and GPT. Some domain-specific versions of BERT are available,
which are trained or fine-tuned on in-domain text, including SciBERT (Beltagy et
al., 2019), BioBERT (Lee et al., 2020) and ClinicalBERT (Alsentzer et al., 2019)
and can be used in low-resource NER and relation extraction tasks on some of
the target domains. However, there is still a need to train the contextualized
embedding models in other domains. This remains a challenge since it requires
large amounts of training data.

Even though the contextualized embeddings handle rare words implicitly
using techniques such as byte-pair encoding and WordPiece embeddings, they still
struggle with small corpora and with providing good representations for unseen
words (Schick and Schiitze, 2020). In chapter 3, we incorporate a knowledge
resource to augment the trained non-contextual embeddings by providing vector
representations for infrequent and unseen technical terms. However, the proposed
solution is limited in two respects: (i) the target word must appear in the
knowledge resource, and (ii) its neighbors must be part of the vocabulary of the
embeddings model. One way to overcome this limitation and improve embeddings
of uncommon words is to jointly incorporate surface-form and context information
directly from the textual content as described in Schick and Schiitze (2019b)
and Schick and Schiitze (2019a). The former combines an embedding based
on n-grams with an embedding obtained from averaging over all context words.
Whereas, the latter introduces an attentive mimicking model that computes an

152


Future directions

embedding by giving access not only to a word’s surface form, but also to all
available contexts. The attentive mimicking model learns to attend to the most
informative and reliable contexts.

The pre-trained language models can be further enhanced by leveraging
knowledge accumulated by humans in terms of knowledge resources such as
WordNet (Miller, 1995), ConceptNet (Speer et al., 2017), FrameNet (Baker
et al., 1998), DBpedia (Lehmann et al., 2015). Work on incorporating knowledge
resources into pre-trained language models has shown some promise on several
NLP tasks (Zhang et al., 2020; Peters et al., 2019; Wang et al., 2019b; Zhang
et al., 2019). It would also be interesting to investigate the impact of jointly
applying both of these research directions, i.e., surface-context information and
knowledge-representations, on pre-trained language models.

A limitation of work in this thesis is the use of conventional neural
architectures such as CNNs and BiLSTM in low-resource named entity recognition
and relation extraction tasks, respectively. Transformer-based models such as
BERT, GPT, XLM, and XLM-RoBERTa, which are proposed as one system for
all tasks, might be more appropriate on low-resource NLP settings. Adapting the
transformer-based model (see Section 2.5.2 in Chapter 2) by the task specific fine-
tuning, mitigates the need for having task-specific models and it transfers a pre-
trained language model directly to a target task through minimal modifications,
usually by modifying the last layer. (Pilehvar and Camacho-Collados, 2020).

For low-resource NER, we envision numerous directions for future research.
For instance, we deal with false positive instances at the sentence level via a
reinforcement model. However, our method still has some challenges, and the
false positive problem is still a bottleneck for the performance. We want to
modify our approach to treat false positives at the entity type level, rather than
treating these at the sentence level. Moreover, we can expand our work to other
types of reinforcement learning techniques such as imitation learning. It has
been shown that the algorithmic expert in imitation learning allows direct policy
learning. At the same time, the learned policies transfer successfully between
domains and languages, improving the performance of low-resource NLP tasks
(Du and Ji, 2019; Liu et al., 2018c).

Another limitation is the use of supervised learning algorithms throughout.
The current neural models in chapters 4 and 5 require a set of training examples
to provide good generalization. Future work can be to extend the study to
improve the performance of the models in an unsupervised fashion.

We believe that there is room for further improvement in low-resource relation
extraction, as presented in Chapter 5. A limitation of this work is that we cannot
say that syntactic representations are more helpful in a resource-poor setting
than in a resource-rich. This is an interesting future direction. Another possible
area of improvement would be to extend the study to neural dependency parsers.
Graph-based neural dependency parser has been shown to provide more accurate
parses (Dozat et al., 2017; Kiperwasser and Goldberg, 2016; Song et al., 2019).
Moreover, we can study the problem of relation extraction in resource-poor
settings by open information extraction (Open IE) techniques. Although the
idea of Open IE has been investigated in many recent works (Cui et al., 2018;

153


7. Conclusion and Future work

Stanovsky and Dagan, 2016; Gao et al., 2020; Wu et al., 2019; Han et al., 2019;
Hu et al., 2020b), there are still a lot of open research questions. Most Open IE
approaches focus on the English language and general domains, leaving aside
other settings. The applicability and transferability of previously proposed Open
IE approaches to other languages and domains will be an interesting direction
for future work.

In this thesis, we study the impact of typology sharing among languages in
our cross-lingual meta-learning framework. It would be interesting to investigate
how NLP and linguistic typology can interact and benefit from each other in
low-resource scenarios and extend our work to other cross-lingual NLP tasks
and more languages.

Overall, the real world applications of NLP models are still challenging, and
our contribution has been a step on the way, but there is more to do. We hope
that our research in this thesis serves as a stepping stone for future research and
inspires others to study open research questions in the area of low-resource NLP.

154


Bibliography

Abdou, M., Sas, C., Aralikatte, R., Augenstein, I., and Sogaard, A. (2019).
“X-WikiRE: A Large, Multilingual Resource for Relation Extraction as
Machine Comprehension”. In: Proceedings of the 2nd Workshop on Deep
Learning Approaches for Low-Resource NLP (DeepLo 2019). Association for
Computational Linguistics, pp. 265-274.

Agié, Z. and Schluter, N. (2018). “Baselines and Test Data for Cross-Lingual
Inference”. In: Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018). European Language
Resources Association (ELRA).

Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pagca, M., and Soroa, A. (2009).
“A Study on Similarity and Relatedness Using Distributional and WordNet-
based Approaches”. In: Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chapter of the Association
for Computational Linguistics. Association for Computational Linguistics,
pp. 19-27.

Agirre, E. and Soroa, A. (2009). “Personalizing PageRank for Word Sense
Disambiguation”. In: Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguistics. Association for
Computational Linguistics, pp. 33-41.

Aharoni, R. and Goldberg, Y. (2020). “Unsupervised Domain Clusters in
Pretrained Language Models”. In: Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics. Association for Computational
Linguistics.

Akbik, A., Blythe, D., and Vollgraf, R. (2018). “Contextual String Embeddings
for Sequence Labeling”. In: Proceedings of the 27th International Conference
on Computational Linguistics. Association for Computational Linguistics,
pp. 1638-1649.

Almuhareb, A. (2006). “Attributes in lexical acquisition”. PhD thesis. University
of Essex, Colchester, UK.

Alsentzer, E., Murphy, J., Boag, W., Weng, W.-H., Jindi, D., Naumann, T., and
McDermott, M. (2019). “Publicly Available Clinical BERT Embeddings”.
In: Proceedings of the 2nd Clinical Natural Language Processing Workshop.
Association for Computational Linguistics, pp. 72-78.

Andrychowicz, M., Denil, M., Colmenarejo, S. G., Hoffman, M. W., Pfau, D.,
Schaul, T., Shillingford, B., and Freitas, N. de (2016). “Learning to Learn
by Gradient Descent by Gradient Descent”. In: Proceedings of the 80th
International Conference on Neural Information Processing Systems. Curran

Associates Inc., pp. 3988-3996.

155


Bibliography

Augenstein, I., Maynard, D., and Ciravegna, F. (2014). “Relation Extraction
from the Web Using Distant Supervision”. In: Knowledge Engineering and
Knowledge Management. Springer International Publishing, pp. 26-41.

Ba, L. J., Kiros, J. R., and Hinton, G. E. (2016). “Layer Normalization”. In:
CoRR vol. abs/1607.06450.

Bahdanau, D., Cho, K., and Bengio, Y. (2015). “Neural Machine Translation by
Jointly Learning to Align and Translate”. In: 3rd International Conference
on Learning Representations, ICLR 2015, Conference Track Proceedings.

Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). “The Berkeley
FrameNet Project”. In: Proceedings of the 17th International Conference
on Computational Linguistics - Volume 1. Association for Computational
Linguistics, pp. 86-90.

Baker, S., Reichart, R., and Korhonen, A. (2014). “An Unsupervised Model for
Instance Level Subcategorization Acquisition”. In: Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP).
Association for Computational Linguistics, pp. 278-289.

Baroni, M., Dinu, G., and Kruszewski, G. (2014). “Don’t count, predict! A
systematic comparison of context-counting vs. context-predicting semantic
vectors”. In: Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics. Association for Computational Linguistics,
pp. 238-247.

Baroni, M., Evert, S., and Lenci, A. (2008). “Lexical Semantics Bridging the gap
between semantic theory and computational simulations”. In: Proceedings of
20th European Summer School in Logic, Language and Information (ESSLLI
2008).

Baroni, M. and Lenci, A. (2010). “Distributional Memory: A General Framework
for Corpus-Based Semantics”. In: Computational Linguistics vol. 36, no. 4,
pp. 673-721.

Beltagy, I., Lo, K., and Cohan, A. (2019). “SciBERT: A Pretrained Language
Model for Scientific Text”. In: Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Association for Computational Linguistics, pp. 3615-3620.

Bender, E. M. (2019). The# BenderRule: On Naming the Languages We Study
and Why It Matters. https://thegradient.pub/the-benderrule-on-naming-the-
languages-we-study-and-why-it-matters/.

Bjerva, J. (2017). “One Model to Rule them all — Multitask and Multilingual
Modelling for Lexical Analysis”. PhD thesis. University of Groningen.

Bjerva, J. and Augenstein, I. (2018a). “From Phonology to Syntax: Unsupervised
Linguistic Typology at Different Levels with Language Embeddings”. In:
Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers). Association for Computational Linguistics, pp. 907—
916.

Bjerva, J. and Augenstein, I. (2018b). “Tracking Typological Traits of Uralic
Languages in Distributed Language Representations”. In: Proceedings of

156


Bibliography

the Fourth International Workshop on Computational Linguistics of Uralic
Languages. Association for Computational Linguistics, pp. 76-86.

Bohnet, B. and Nivre, J. (2012). “A Transition-Based System for Joint
Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing”.
In: Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning.
Association for Computational Linguistics, pp. 1455-1465.

Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). “Enriching Word
Vectors with Subword Information”. In: Transactions of the Association for
Computational Linguistics vol. 5, pp. 135-146.

Bollegala, D., Maehara, T., and Kawarabayashi, K.-i. (2015). “Learning Word
Representations from Relational Graphs”. In: Proceedings of 58rd Annual
Meeting of the Association for Computational Linguistics, and the 7th
International Joint Conference on Natural Language Processing of the Asian
Federation of Natural Language Processing. Association for Computational
Linguistics, pp. 730-740.

Bordea, G., Buitelaar, P., and Polajnar, T. (2013). “Domain-independent term
extraction through domain modelling”. In: The 10th international conference
on terminology and artificial intelligence. TIA 2013.

Botha, J. A. and Blunsom, P. (2014). “Compositional Morphology for Word
Representations and Language Modelling”. In: Proceedings of the 31st
International Conference on International Conference on Machine Learning
- Volume 32. JMLR.org, pp. 1899-1907.

Brin, S. and Page, L. (1998). “The anatomy of a large-scale hypertextual Web
search engine”. In: Computer Networks and ISDN Systems vol. 30, no. 1,
pp. 107-117.

Brochu, E., Cora, V. M., and Freitas, N. de (2010). “A Tutorial on Bayesian
Optimization of Expensive Cost Functions, with Application to Active
User Modeling and Hierarchical Reinforcement Learning”. In: CoRR
vol. abs/1012.2599.

Bruni, E., Boleda, G., Baroni, M., and Tran, N.-K. (2012). “Distributional
Semantics in Technicolor”. In: Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long Papers - Volume 1.
Association for Computational Linguistics, pp. 136-145.

Bunescu, R. C. and Mooney, R. J. (2005). “A Shortest Path Dependency
Kernel for Relation Extraction”. In: Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pp. 724-731.

Camacho-Collados, J. and Navigli, R. (2016). “Find the word that does not belong:
A Framework for an Intrinsic Evaluation of Word Vector Representations”. In:
Proceedings of the 1st Workshop on Evaluating Vector-Space Representations
for NLP. Association for Computational Linguistics, pp. 43-50.

Camacho-Collados, J. and Pilehvar, M. T. (2018). “On the Role of Text
Preprocessing in Neural Network Architectures: An Evaluation Study on Text
Categorization and Sentiment Analysis”. In: Proceedings of the 2018 EMNLP

157


Bibliography

Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for
NEP. Association for Computational Linguistics, pp. 40-46.

Chen, L. and Moschitti, A. (2019). “Transfer Learning for Sequence Labeling
Using Source Model and Target Data”. In: Proceedings of the AAAI
Conference on Artificial Intelligence. Vol. 33. Association for the Advancement
of Artificial Intelligence (AAAT) Press, pp. 6260-6267.

Chen, Q., Zhu, X., Ling, Z.-H., Wei, S., Jiang, H., and Inkpen, D. (2017).
“Enhanced LSTM for Natural Language Inference”. In: Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Linguistics, pp. 1657-1668.

Cherry, C., Durrett, G., Foster, G., Haffari, R., Khadivi, S., Peng, N., Ren,
X., and Swayamdipta, S. (2019). Proceedings of the 2nd Workshop on Deep
Learning Approaches for Low-Resource NLP (DeepLo 2019). Association for
Computational Linguistics, Hong Kong, China.

Chinchor, N. A. (1998). “Proceedings of the Seventh Message Understanding
Conference (MUC-7) Named Entity Task Definition”. In: Proceedings of the
Seventh Message Understanding Conference (MUC-7), 21 pages.

Chiu, B., Crichton, G., Korhonen, A., and Pyysalo, S. (2016a). “How to
Train good Word Embeddings for Biomedical NLP”. In: Proceedings of
the 15th Workshop on Biomedical Natural Language Processing. Association
for Computational Linguistics, pp. 166-174.

Chiu, B., Korhonen, A., and Pyysalo, S. (2016b). “Intrinsic Evaluation of Word
Vectors Fails to Predict Extrinsic Performance”. In: Proceedings of the 1st
Workshop on Evaluating Vector-Space Representations for NLP. Association
for Computational Linguistics, pp. 1-6.

Chiu, J. P. and Nichols, E. (2016). “Named Entity Recognition with Bidirectional
LSTM-CNNs”. In: Transactions of the Association for Computational
Linguistics vol. 4, pp. 357-370.

Cho, K., Merriénboer, B. van, Bahdanau, D., and Bengio, Y. (2014). “On the
Properties of Neural Machine Translation: Encoder—Decoder Approaches”. In:
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure
in Statistical Translation. Association for Computational Linguistics, pp. 103-
111.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa,
P. P. (2011). “Natural Language Processing (almost) from Scratch”. In:
Journal of Machine Learning Research vol. 12, pp. 2493-2537.

Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzman, F.,
Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). “Unsupervised
cross-lingual representation learning at scale”. In: Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics.

Conneau, A. and Lample, G. (2019). “Cross-lingual Language Model Pretraining”.
In: Advances in Neural Information Processing Systems 82. Curran Associates,
Inc., pp. 7059-7069.

Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S., Schwenk,
H., and Stoyanoyv, V. (2018). “XNLI: Evaluating Cross-lingual Sentence

158


Bibliography

Representations”. In: Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing. Association for Computational
Linguistics, pp. 2475-2485.

Craven, M. and Kumlien, J. (1999). “Constructing Biological Knowledge Bases
by Extracting Information from Text Sources”. In: Proceedings of the
Seventh International Conference on Intelligent Systems for Molecular Biology.
Association for the Advancement of Artificial Intelligence (AAAT) Press,
pp. 77-86.

Cui, L., Wei, F., and Zhou, M. (2018). “Neural Open Information Extraction”. In:
Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). Association for Computational
Linguistics, pp. 407-413.

de Lhoneux, M., Bjerva, J., Augenstein, I., and Sggaard, A. (2018). “Parameter
sharing between dependency parsers for related languages”. In: Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pp. 4992-4997.

de Marneffe, M.-C., Dozat, T., Silveira, N., Haverinen, K., Ginter, F., Nivre,
J., and Manning, C. D. (2014). “Universal Stanford dependencies: A cross-
linguistic typology”. In: Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC’14). European Language
Resources Association (ELRA), pp. 4585-4592.

de Marneffe, M.-C., MacCartney, B., and Manning, C. D. (2006). “Generating
Typed Dependency Parses from Phrase Structure Parses”. In: Proceedings of
the Fifth International Conference on Language Resources and Evaluation
(LREC’06). European Language Resources Association (ELRA).

Del Corro, L., Abujabal, A., Gemulla, R., and Weikum, G. (2015). “FINET:
Context-Aware Fine-Grained Named Entity Typing”. In: Proceedings of
the 2015 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pp. 868-878.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). “BERT: Pre-
training of Deep Bidirectional Transformers for Language Understanding”.
In: Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers). Association for Computational Linguistics,
pp. 4171-4186.

Dogan, C., Dutra, A., Gara, A., Gemma, A., Shi, L., Sigamani, M., and Walters, E.
(2019). “Fine-Grained Named Entity Recognition using ELMo and Wikidata”.
In: CoRR vol. abs/1904.10503.

Dou, Z.-Y., Yu, K., and Anastasopoulos, A. (2019). “Investigating Meta-Learning
Algorithms for Low-Resource Natural Language Understanding Tasks”. In:
Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP). Association for Computational Linguistics,
pp. 1192-1197.

Dozat, T., Qi, P., and Manning, C. D. (2017). “Stanford’s Graph-based Neural
Dependency Parser at the CoNLL 2017 Shared Task”. In: Proceedings of the

159


Bibliography

CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal
Dependencies. Association for Computational Linguistics, pp. 20-30.

WALS Online (2013). Max Planck Institute for Evolutionary Anthropology,
Leipzig.

Du, W. and Ji, Y. (2019). “An Empirical Comparison on Imitation Learning
and Reinforcement Learning for Paraphrase Generation”. In: Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, pp. 6012-—
6018.

Duong, L. (2017). “Natural language processing for resource-poor languages”.
PhD thesis. University of Melbourne.

Eberhard, D. M., Simons, G. F., and Fennig, C. D. (2019). Ethnologue: Languages
of the World. https:/www.ethnologue.com/statistics/size.

Ebrahimi, J. and Dou, D. (2015). “Chain Based RNN for Relation Classification”.
In: Proceedings of the 2015 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies.
Association for Computational Linguistics, pp. 1244-1249.

Eisenstein, J. (2019). Introduction to Natural Language Processing. MIT Press,
Cambridge, MA, United States.

Elman, J. L. (1990). “Finding Structure in Time.” In: Cognitive Science vol. 14,
no. 2, pp. 179-211.

Elming, J., Johannsen, A., Klerke, S., Lapponi, E., Martinez Alonso, H., and
Segaard, A. (2013). “Down-stream effects of tree-to-dependency conversions”.
In: Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies.
Association for Computational Linguistics, pp. 617-626.

Fader, A., Soderland, S., and Etzioni, O. (2011). “Identifying Relations for Open
Information Extraction”. In: Proceedings of the Conference on Empirical
Methods in Natural Language Processing. Association for Computational
Linguistics, pp. 1535-1545.

Fares, M., Kutuzov, A., Oepen, S., and Velldal, E. (2017). “Word vectors, reuse,
and replicability: Towards a community repository of large-text resources”.
In: Proceedings of the 21st Nordic Conference on Computational Linguistics.
Association for Computational Linguistics, pp. 271-276.

Fares, M., Oepen, S., Ovrelid, L., Bjorne, J., and Johansson, R. (2018). “The 2018
Shared Task on Extrinsic Parser Evaluation: On the Downstream Utility of
English Universal Dependency Parsers”. In: Proceedings of the CoNLL 2018
Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.
Association for Computational Linguistics, pp. 22-33.

Faruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E. H., and Smith,
N. A. (2015). “Retrofitting Word Vectors to Semantic Lexicons”. In: The
2015 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. Association for
Computational Linguistics, pp. 1606-1615.

160


Bibliography

Faruqui, M. and Kumar, S. (2015). “Multilingual Open Relation Extraction
Using Cross-lingual Projection”. In: Proceedings of the 2015 Conference of the
North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Linguistics,
pp. 1351-1356.

Feng, J., Huang, M., Zhao, L., Yang, Y., and Zhu, X. (2018). “Reinforcement
learning for relation classification from noisy data”. In: Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence. Association for
the Advancement of Artificial Intelligence (AAAI) Press.

Finkel, J. R. and Manning, C. D. (2009). “Hierarchical Bayesian Domain
Adaptation”. In: Proceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of the Association
for Computational Linguistics. Association for Computational Linguistics,
pp. 602-610.

Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G.,
and Ruppin, E. (2001). “Placing Search in Context: The Concept Revisited”.
In: Proceedings of the 10th International Conference on World Wide Web.
Association for Computing Machinery, pp. 406-414.

Finn, C., Abbeel, P., and Levine, S. (2017). “Model-Agnostic Meta-Learning for
Fast Adaptation of Deep Networks”. In: Proceedings of the 34th International
Conference on Machine Learning, ICML 2017. Vol. 70. PMLR, pp. 1126-1135.

Fried, D. and Duh, K. (2015). “Incorporating Both Distributional and Relational
Semantics in Word Representations”. In: 3rd International Conference on
Learning Representations, ICLR, Workshop Track Proceedings.

Fries, J., Wu, S., Ratner, A., and Ré, C. (2017). “SwellShark: A Generative
Model for Biomedical Named Entity Recognition without Labeled Data”. In:
CoRR vol. abs/1704.06360.

Gabor, K., Buscaldi, D., Schumann, A.-K., QasemiZadeh, B., Zargayouna, H., and
Charnois, T. (2018). “SemEval-2018 Task 7: Semantic Relation Extraction and
Classification in Scientific Papers”. In: Proceedings of The 12th International
Workshop on Semantic Evaluation. Association for Computational Linguistics,
pp. 679-688.

Gabor, K., Zargayouna, H., Buscaldi, D., Tellier, I., and Charnois, T. (2016).
“Semantic Annotation of the ACL Anthology Corpus for the Automatic
Analysis of Scientific Literature”. In: Proceedings of the Tenth International
Conference on Language Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA).

Ganitkevitch, J., Van Durme, B., and Callison-Burch, C. (2013a). “PPDB: The
Paraphrase Database”. In: Proceedings of NAACL-HLT. Association for
Computational Linguistics, pp. 758-764.

Ganitkevitch, J., Van Durme, B., and Callison-Burch, C. (2013b). “PPDB: The
Paraphrase Database”. In: Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics, pp. 758-
764.

161


Bibliography

Gao, T., Han, X., Xie, R., Liu, Z., Lin, F., Lin, L., and Sun, M. (2020). “Neural
Snowball for Few-Shot Relation Learning”. In: Proceedings the Thirty-fourth
AAAI Conference on Artificial Intelligence. Association for the Advancement
of Artificial Intelligence (AAAT) Press.

Gerber, D. and Ngonga Ngomo, A.-C. (2011). “Bootstrapping the Linked Data
Web”. In: 1st Workshop on Web Scale Knowledge Extraction @ ISWC 2011.

Gers, F. A., Schmidhuber, J. A., and Cummins, F. A. (2000). “Learning to
Forget: Continual Prediction with LSTM”. In: Neural Computation vol. 12,
no. 10, pp. 2451-2471.

Ghannay, S., Favre, B., Estéve, Y., and Camelin, N. (2016). “Word Embedding
Evaluation and Combination”. In: Proceedings of the Tenth International
Conference on Language Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA).

Giannakopoulos, A., Musat, C., Hossmann, A., and Baeriswyl, M. (2017).
“Unsupervised Aspect Term Extraction with B-LSTM & CRF using
Automatically Labelled Datasets”. In: Proceedings of the 8th Workshop
on Computational Approaches to Subjectivity, Sentiment and Social Media
Analysis. Association for Computational Linguistics, pp. 180-188.

Gladkova, A., Drozd, A., and Matsuoka, S. (2016). “Analogy-based detection of
morphological and semantic relations with word embeddings: what works and
what doesn’t.” In: Proceedings of the NAACL Student Research Workshop.
Association for Computational Linguistics, pp. 8-15.

Goldberg, Y. (2017). Neural Network Methods in Natural Language Processing.
Morgan & Claypool Publishers, San Rafael, CA.

Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press,
Cambridge, MA, United States.

Gu, J., Wang, Y., Chen, Y., Li, V. O. K., and Cho, K. (2018). “Meta-Learning
for Low-Resource Neural Machine Translation”. In: Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, pp. 3622-3631.

Gupta, P., Schiitze, H., and Andrassy, B. (2016). “Table Filling Multi-Task
Recurrent Neural Network for Joint Entity and Relation Extraction”.
In: Proceedings of COLING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers. The COLING 2016 Organizing
Committee, pp. 2537-2547.

Habibi, M., Weber, L., Neves, M., Wiegandt, D. L., and Leser, U. (2017).
“Deep learning with word embeddings improves biomedical named entity
recognition”. In: Bioinformatics vol. 33, no. 14, pp. i37—i48.

Haffari, R., Cherry, C., Foster, G., Khadivi, S., and Salehi, B. (2018). Proceedings
of the Workshop on Deep Learning Approaches for Low-Resource NLP.
Association for Computational Linguistics, Melbourne, Australia.

Halawi, G., Dror, G., Gabrilovich, E., and Koren, Y. (2012). “Large-Scale
Learning of Word Relatedness with Constraints”. In: Proceedings of the 18th
ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. Association for Computing Machinery, pp. 1406-1414.

162


Bibliography

Hamilton, W. L., Clark, K., Leskovec, J., and Jurafsky, D. (2016). “Inducing
Domain-Specific Sentiment Lexicons from Unlabeled Corpora”. In: Proceed-
ings of the 2016 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, pp. 595-605.

Han, X., Gao, T., Yao, Y., Ye, D., Liu, Z., and Sun, M. (2019). “OpenNRE: An
Open and Extensible Toolkit for Neural Relation Extraction”. In: Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP): System Demonstrations. Association for Computational
Linguistics, pp. 169-174.

Han, X., Zhu, H., Yu, P., Wang, Z., Yao, Y., Liu, Z., and Sun, M. (2018).
“FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset
with State-of-the-Art Evaluation”. In: Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, Brussels, Belgium,
October 81 - November 4, 2018. Association for Computational Linguistics,
pp. 4803-4809.

Hardoon, D. R., Szedmak, S. R., and Shawe-taylor, J. R. (2004). “Canonical
Correlation Analysis: An Overview with Application to Learning Methods”.
In: Neural Computation vol. 16, no. 12, pp. 2639-2664.

Haveliwala, T. H. (2003). “Topic-sensitive PageRank: a context-sensitive ranking
algorithm for Web search”. In: IEEE Transactions on Knowledge and Data
Engineering vol. 15, no. 4, pp. 784-796.

He, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep Residual Learning for
Image Recognition”. In: 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE Computer Society, pp. 770-778.

Hendrickx, I., Kim, 5. N., Kozareva, Z., Nakov, P., Séaghdha, D. O., Padé, S.,
Pennacchiotti, M., Romano, L., and Szpakowicz, S. (2010). “SemEval-2010
Task 8: Multi-way Classification of Semantic Relations Between Pairs of
Nominals”. In: Proceedings of the 5th International Workshop on Semantic
Evaluation. Association for Computational Linguistics, pp. 33-38.

Hill, F., Reichart, R., and Korhonen, A. (2015). “SimLex-999: Evaluating
Semantic Models With (Genuine) Similarity Estimation”. In: Computational
Linguistics vol. 41, no. 4, pp. 665-695.

Hochreiter, S. and Schmidhuber, J. (1997). “Long Short-Term Memory”. In:
Neural Computation vol. 9, no. 8, pp. 1735-1780.

Howard, J. and Ruder, S. (2018). “Universal Language Model Fine-tuning for Text
Classification”. In: Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, pp. 328-339.

Hu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. (2020a).
“XTREME: A massively multilingual multi-task benchmark for evaluating
cross-lingual generalization”. In: CoRR vol. abs/2003.11080.

Hu, X., Wen, L., Xu, Y., Zhang, C., and Yu, P. S. (2020b). “SelfORE: Self-
supervised Relational Feature Learning for Open Relation Extraction”. In:
CoRR vol. abs/2004.02438.

163


Bibliography

Jiang, M., D’Souza, J., Auer, S., and Downie, J. S. (2020). “Improving Scholarly
Knowledge Representation: Evaluating BERT-based Models for Scientific
Relation Classification”. In: CoRR vol. abs/2004.06153.

Jiang, Z., Yin, P., and Neubig, G. (2019). “Improving Open Information
Extraction via Iterative Rank-Aware Learning”. In: Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, pp. 5295-5300.

Johansson, R. and Nugues, P. (2007). “Extended Constituent-to-Dependency
Conversion for English”. In: Proceedings of the 16th Nordic Conference of
Computational Linguistics (NODALIDA 2007). University of Tartu, Estonia,
pp. 105-112.

Kann, K., Cho, K., and Bowman, S. R. (2019). “Towards Realistic Practices
In Low-Resource Natural Language Processing: The Development Set”. In:
Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP). Association for Computational Linguistics,
pp. 3342-3349.

Kim, Y. (2014). “Convolutional Neural Networks for Sentence Classification”.
In: Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics, pp. 1746—
1751.

King, B. P. (2015). “Practical Natural Language Processing for Low-Resource
Languages”. PhD thesis. University of Melbourne.

Kingma, D. P. and Ba, J. (2014). Adam: A Method for Stochastic Optimization.

Kiperwasser, E. and Goldberg, Y. (2016). “Simple and Accurate Dependency
Parsing Using Bidirectional LSTM Feature Representations”. In: Transactions
of the Association for Computational Linguistics vol. 4, pp. 313-327.

Koch, G., Zemel, R., and Salakhutdinov, R. (2015). “Siamese neural networks for
one-shot image recognition”. In: Deep Learning Workshop at the International
Conference on Machine Learning (ICML). Vol. 2.

Kokkinos, F. and Potamianos, A. (2017). “Structural Attention Neural Networks
for improved sentiment analysis”. In: Proceedings of the 15th Conference of the
European Chapter of the Association for Computational Linguistics, EACL
2017, Volume 2: Short Papers. Association for Computational Linguistics,
pp. 586-591.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). “ImageNet Classifica-
tion with Deep Convolutional Neural Networks”. In: Advances in Neural
Information Processing Systems 25. Curran Associates, Inc., pp. 1097-1105.

Kudo, T. and Richardson, J. (2018). “SentencePiece: A simple and language
independent subword tokenizer and detokenizer for Neural Text Processing”.
In: Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations. Association for Computational
Linguistics, pp. 66-71.

Lafferty, J. D., McCallum, A., and Pereira, F. C. N. (2001). “Conditional Random
Fields: Probabilistic Models for Segmenting and Labeling Sequence Data”. In:

164


Bibliography

Proceedings of the Eighteenth International Conference on Machine Learning.
Morgan Kaufmann Publishers Inc., pp. 282—289.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C.
(2016). “Neural Architectures for Named Entity Recognition”. In: Proceedings
of the 2016 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies. Association
for Computational Linguistics, pp. 260-270.

Landauer, T. K. and Dutnais, $. T. (1997). “A solution to Plato’s problem: The
latent semantic analysis theory of acquisition, induction, and representation of
knowledge”. In: PSYCHOLOGICAL REVIEW vol. 104, no. 2, pp. 211-240.

LeCun, Y. and Bengio, Y. (1998). “Convolutional Networks for Images, Speech,
and Time Series”. In: The Handbook of Brain Theory and Neural Networks.
MIT Press, pp. 255-258.

Lee, D. (2002). “Genres, registers, text types, domains and styles: clarifying the
concepts and navigating a path through the BNC jungle”. In: Teaching and
Learning by Doing Corpus Analysis. Brill Rodopi, pp. 245-292.

Lee, J. Y., Dernoncourt, F., and Szolovits, P. (2017). “MIT at SemEval-2017 Task
10: Relation Extraction with Convolutional Neural Networks”. In: Proceedings
of the 11th International Workshop on Semantic Evaluation (SemEval-2017).
Association for Computational Linguistics, pp. 978-984.

Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., and Kang, J. (2020).
“BioBERT: a pre-trained biomedical language representation model for
biomedical text mining”. In: Bioinformatics vol. 36, no. 4, pp. 1234-1240.

Leeuwenberg, A., Vela, M., Dehdari, J., and Genabith, J. van (2016). “A
Minimally Supervised Approach for Synonym Extraction with Word
Embeddings”. In: The Prague Bulletin of Mathematical Linguistics vol. 105,
no. 1, pp. 111-142.

Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P. N.,
Hellmann, S., Morsey, M., Kleef, P. van, Auer, S., and Bizer, C. (2015).
“DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from
Wikipedia”. In: Semantic Web Journal vol. 6, no. 2, pp. 167-195.

Levow, G.-A. (2006). “The Third International Chinese Language Processing
Bakeoff: Word Segmentation and Named Entity Recognition”. In: Proceedings
of the Fifth SIGHAN Workshop on Chinese Language Processing. Association
for Computational Linguistics, pp. 108-117.

Levy, O., Goldberg, Y., and Dagan, I. (2015). “Improving Distributional
Similarity with Lessons Learned from Word Embeddings”. In: Transactions
of the Association for Computational Linguistics vol. 3, pp. 211-225.

Levy, O., Seo, M., Choi, E., and Zettlemoyer, L. (2017). “Zero-Shot Relation
Extraction via Reading Comprehension”. In: Proceedings of the 21st
Conference on Computational Natural Language Learning (CoNLL 2017).
Association for Computational Linguistics, pp. 333-342.

Lewis, P., Oguz, B., Rinott, R., Riedel, S., and Schwenk, H. (2020). “MLQA:
Evaluating Cross-lingual Extractive Question Answering”. In: Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics.

165


Bibliography

Li, J., Sun, Y., Johnson, R. J., Sciaky, D., Wei, C.-H., Leaman, R., Davis, A. P.,
Mattingly, C. J., Wiegers, T. C., and Lu, Z. (2016). “BioCreative V CDR
task corpus: a resource for chemical disease relation extraction”. In: Database
vol. 2016.

Li, J., Sun, A., Han, J., and Li, C. (2020). “A Survey on Deep Learning for
Named Entity Recognition”. In: IEEE Transactions on Knowledge and Data
Engineering, pp. 1-1.

Li, X. and Roth, D. (2002). “Learning Question Classifiers”. In: Proceedings of
the 19th International Conference on Computational Linguistics - Volume 1.
Association for Computational Linguistics, pp. 1-7.

Liang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L.,
Jiang, D., Cao, G., et al. (2020). “XGLUE: A New Benchmark Dataset
for Cross-lingual Pre-training, Understanding and Generation”. In: CoRR
vol. abs/2004.01401.

Lin, Y., Liu, Z., Sun, M., Liu, Y., and Zhu, X. (2015). “Learning Entity and
Relation Embeddings for Knowledge Graph Completion”. In: Proceedings of
the Twenty-Ninth AAAI Conference on Artificial Intelligence. Association
for the Advancement of Artificial Intelligence (AAAT) Press, pp. 2181-2187.

Lin, Y., Shen, S., Liu, Z., Luan, H., and Sun, M. (2016). “Neural Relation
Extraction with Selective Attention over Instances”. In: Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Computational Linguistics,
pp. 2124-2133.

Ling, W., Dyer, C., Black, A. W., Trancoso, I., Fermandez, R., Amir, S., Marujo,
L., and Lwiés, T. (2015). “Finding Function in Form: Compositional Character
Models for Open Vocabulary Word Representation”. In: Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pp. 1520-1530.

Liu, L., Ren, X., Shang, J., Gu, X., Peng, J., and Han, J. (2018a). “Efficient
Contextualized Representation: Language Model Pruning for Sequence
Labeling”. In: Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics,
pp. 1215-1225.

Liu, L., Shang, J., Ren, X., Xu, F. F., Gui, H., Peng, J., and Han, J. (2018b).
“Empower sequence labeling with task-aware neural language model”. In:
Thirty-Second AAAI Conference on Artificial Intelligence. Association for
the Advancement of Artificial Intelligence (AAAI) Press.

Liu, M., Buntine, W., and Haffari, G. (2018c). “Learning How to Actively Learn:
A Deep Imitation Learning Approach”. In: Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, pp. 1874-1883.

Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E., and Smith, N. A. (2019a).
“Linguistic Knowledge and Transferability of Contextual Representations”.
In: Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,

166


Bibliography

Volume 1 (Long and Short Papers). Association for Computational Linguistics,
pp. 1073-1094.

Liu, S., Sun, Y., Li, B., Wang, W., and Zhao, X. (2020). “HAMNER: Headword
Amplified Multi-span Distantly Supervised Method for Domain Specific
Named Entity Recognition”. In: Proceedings of the 34th AAAI Conference
on Artificial Intelligence. Association for the Advancement of Artificial
Intelligence (AAAI) Presss.

Liu, Y., Wei, F., Li, S., Ji, H., Zhou, M., and Wang, H. (2015). “A Dependency-
Based Neural Network for Relation Classification”. In: Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing (Volume 2:
Short Papers). Association for Computational Linguistics, pp. 285-290.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
M., Zettlemoyer, L., and Stoyanov, V. (2019b). “RoBERTa: A Robustly
Optimized BERT Pretraining Approach”. In: CoRR vol. abs/1907.11692.

Luan, Y., Ostendorf, M., and Hajishirzi, H. (2018). “The UWNLP system at
SemEval-2018 Task 7: Neural Relation Extraction Model with Selectively
Incorporated Concept Embeddings”. In: Proceedings of The 12th International
Workshop on Semantic Evaluation. Association for Computational Linguistics,
pp. 788-792.

Luong, T., Pham, H., and Manning, C. D. (2015). “Effective Approaches to
Attention-based Neural Machine Translation”. In: Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, pp. 1412-1421.

Luong, T., Socher, R., and Manning, C. (2013). “Better Word Representations
with Recursive Neural Networks for Morphology”. In: Proceedings of the
Seventeenth Conference on Computational Natural Language Learning.
Association for Computational Linguistics, pp. 104-113.

Ma, X. and Hovy, E. (2016). “End-to-end Sequence Labeling via Bi-directional
LSTM-CNNs-CRF”. In: Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, pp. 1064-1074.

Ma, Y., Cambria, E., and Gao, S. (2016). “Label Embedding for Zero-shot Fine-
grained Named Entity Typing”. In: Proceedings of the 26th International
Conference on Computational Linguistics: Technical Papers. Association for
Computational Linguistics, pp. 171-180.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C.
(2011). “Learning Word Vectors for Sentiment Analysis”. In: Proceedings of
the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Linguistics,
pp. 142-150.

Mai, K., Pham, T.-H., Nguyen, M. T., Nguyen, T. D., Bollegala, D., Sasano,
R., and Sekine, S. (2018). “An Empirical Study on Fine-Grained Named
Entity Recognition”. In: Proceedings of the 27th International Conference
on Computational Linguistics. Association for Computational Linguistics,
pp. 711-722.

167


Bibliography

Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J. R., Bethard, S., and McClosky,
D. (2014). “The Stanford CoreNLP Natural Language Processing Toolkit”. In:
Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics. Association for Computer Linguistics, pp. 55-60.

Marcheggiani, D., Frolov, A., and Titov, I. (2017). “A Simple and Accurate
Syntax-Agnostic Neural Model for Dependency-based Semantic Role La-
beling”. In: Proceedings of the 21st Conference on Computational Natural
Language Learning. Association for Computational Linguistics, pp. 411-420.

Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). “Building a
Large Annotated Corpus of English: The Penn Treebank”. In: Computational
Linguistics vol. 19, no. 2, pp. 313-330.

Mausam, Schmitz, M., Bart, R., Soderland, S., and Etzioni, O. (2012). “Open
Language Learning for Information Extraction”. In: Proceedings of the 2012
Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning. Association for Computational
Linguistics, pp. 523-534.

McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D.,
Ganchey, K., Hall, K., Petrov, S., Zhang, H., Tackstrém, O., Bedini, C.,
Bertomeu Castelld, N., and Lee, J. (2013). “Universal Dependency Annotation
for Multilingual Parsing”. In: Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers).
Association for Computational Linguistics, pp. 92-97.

McRae, K., Spivey-Knowlton, M. J., and Tanenhaus, M. K. (1998). “Modeling
the Influence of Thematic Fit (and Other Constraints) in On-line Sentence
Comprehension”. In: Journal of Memory and Language vol. 38, no. 3, pp. 283-
312.

Mi, F., Huang, M., Zhang, J., and Faltings, B. (2019). “Meta-Learning for Low-
resource Natural Language Generation in Task-oriented Dialogue Systems”.
In: Proceedings of the Twenty-Eighth International Joint Conference on
Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial
Intelligence Organization, pp. 3151-3157.

Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). “Efficient Estimation
of Word Representations in Vector Space”. In: 1st International Conference
on Learning Representations, ICLR 2018, Workshop Track Proceedings.

Mikolov, T., Yih, W.-t., and Zweig, G. (2013b). “Linguistic Regularities in Con-
tinuous Space Word Representations”. In: Human Language Technologies:
Conference of the North American Chapter of the Association of Compu-
tational Linguistics. Association for Computational Linguistics, pp. 746-
751.

Miller, G. A. and Charles, W. G. (1991). “Contextual correlates of semantic
similarity”. In: Language and Cognitive Processes vol. 6, no. 1, pp. 1-28.
Miller, G. A. (1995). “WordNet: A Lexical Database for English”. In: COMMU-

NICATIONS OF THE ACM vol. 38, no. 11, pp. 39-41.

Mintz, M., Bills, S., Snow, R., and Jurafsky, D. (2009). “Distant Supervision
for Relation Extraction Without Labeled Data”. In: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the 4th International

168


Bibliography

Joint Conference on Natural Language Processing of the AFNLP: Volume 2 -
Volume 2. Association for Computational Linguistics, pp. 1003-1011.

Miyao, Y., Setre, R., Sagae, K., Matsuzaki, T., and Tsujii, J. (2008). “Task-
oriented Evaluation of Syntactic Parsers and Their Representations”. In:
Proceedings of the 46th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies. Association for Computational
Linguistics, pp. 46-54.

Nakashole, N. (2012). “Automatic Extraction of Facts, Relations, and Entities
for Web-Scale Knowledge Base Population”. PhD thesis. Universitat des
Saarlandes.

Nakashole, N., Theobald, M., and Weikum, G. (2011). “Scalable Knowledge
Harvesting with High Precision and High Recall”. In: Proceedings of the
Fourth ACM International Conference on Web Search and Data Mining.
Association for Computing Machinery, pp. 227-236.

Nakashole, N., Tylenda, T., and Weikum, G. (2013). “Fine-grained Semantic
Typing of Emerging Entities”. In: Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, pp. 1488-1497.

Navigli, R. and Ponzetto, S. P. (2012). “BabelNet: The Automatic Construction,
Evaluation and Application of a Wide-Coverage Multilingual Semantic
Network”. In: Artificial Intelligence vol. 193, pp. 217-250.

Nayak, N., Angeli, G., and Manning, C. D. (2016). “Evaluating Word Embeddings
Using a Representative Suite of Practical Tasks”. In: Proceedings of the 1st
Workshop on Evaluating Vector-Space Representations for NLP. Association
for Computational Linguistics, pp. 19-23.

Nguyen, T. H. and Grishman, R. (2015). “Relation Extraction: Perspective from
Convolutional Neural Networks”. In: Proceedings of the 1st Workshop on
Vector Space Modeling for NLP. Association for Computational Linguistics,
pp. 39-48.

Nichol, A., Achiam, J., and Schulman, J. (2018). “On First-Order Meta-Learning
Algorithms”. In: CoRR vol. abs/1803.02999.

Nivre, J., Marneffe, M.-C. de, Ginter, F., Goldberg, Y., Haji¢, J.. Manning, C. D.,
McDonald, R., Petrov, S., Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016). “Universal Dependencies v1: A Multilingual Treebank Collection”. In:
Proceedings of the Tenth International Conference on Language Resources and
Evaluation (LREC’16). European Language Resources Association (ELRA),
pp. 1659-1666.

Nooralahzadeh, F., Lopez, C., Cabrio, E., Gandon, F., and Segond, F. (2016).
“Adapting Semantic Spreading Activation to Entity Linking in Text”. In:
Natural Language Processing and Information Systems. Springer International
Publishing, pp. 74-90.

Nooralahzadeh, F., Ovrelid, L., and Lgnning, J. T. (2018). “SIRIUS-LTG-UiO
at SemEval-2018 Task 7: Convolutional Neural Networks with Shortest
Dependency Paths for Semantic Relation Extraction and Classification in
Scientific Papers”. In: Proceedings of The 12th International Workshop on
Semantic Evaluation. Association for Computational Linguistics, pp. 805-810.

169


Bibliography

Obamuyide, A. and Vlachos, A. (2019). “Model-Agnostic Meta-Learning for
Relation Classification with Limited Supervision”. In: Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, pp. 5873-5879.

Oepen, S., @vrelid, L., Bjérne, J., Johansson, R., Lapponi, E., Ginter, F., and
Velldal, E. (2017). “The 2017 Shared Task on Extrinsic Parser Evaluation.
Towards a Reusable Community Infrastructure”. In: Proceedings of the 2017
Shared Task on Extrinsic Parser Evaluation at the Fourth International
Conference on Dependency Linguistics and the 15th International Conference
on Parsing Technologies. Association for Computational Linguistics (ACL),
pp. 1-12.

Padé, U. (2007). “The integration of syntax and semantic plausibility in a wide-
coverage model of human sentence processing”. PhD thesis. Universitat des
Saarlandes.

Pan, S. and Yang, Q. (2010). “A Survey on Transfer Learning”. In: [EEE
Transactions on Knowledge and Data Engineering vol. 22, no. 10, pp. 1345—
1359.

Passos, A., Kumar, V., and McCallum, A. (2014). “Lexicon Infused Phrase
Embeddings for Named Entity Resolution”. In: Proceedings of the Eighteenth
Conference on Computational Natural Language Learning. Association for
Computational Linguistics, pp. 78-86.

Pennington, J., Socher, R., and Manning, C. (2014). “Glove: Global Vectors
for Word Representation”. In: Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP). Association
for Computational Linguistics, pp. 1532-1543.

Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and
Zettlemoyer, L. (2018). “Deep Contextualized Word Representations”. In:
Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers). Association for Computational Linguistics, pp. 2227—
2237.

Peters, M. E., Neumann, M., Logan, R., Schwartz, R., Joshi, V., Singh,
S., and Smith, N. A. (2019). “Knowledge Enhanced Contextual Word
Representations”. In: Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP). Association
for Computational Linguistics, pp. 43-54.

Pilehvar, M. T. and Camacho-Collados, J. (2020). Embeddings in Natural
Language Processing. Morgan & Claypool Publishers, San Rafael, CA.

Pilehvar, M. T. and Collier, N. (2016a). “De-Conflated Semantic Representations”.
In: Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics, pp. 1680-
1690.

Pilehvar, M. T. and Collier, N. (2016b). “Improved Semantic Representation
for Domain-Specific Entities”. In: Proceedings of the 15th Workshop on

170


Bibliography

Biomedical Natural Language Processing. Association for Computational
Linguistics, pp. 12-16.

Pilehvar, M. T. and Collier, N. (2017). “Inducing Embeddings for Rare and
Unseen Words by Leveraging Lexical Resources”. In: Proceedings of the 15th
Conference of the European Chapter of the Association for Computational Lin-
guistics: Volume 2, Short Papers. Association for Computational Linguistics,
pp. 388-393.

Pilehvar, M. T. and Navigli, R. (2015). “From Senses to Texts: An all-in-one graph-
based approach for measuring semantic similarity”. In: Artificial Intelligence
vol. 228, no. C, pp. 95-128.

Pires, T., Schlinger, E., and Garrette, D. (2019). “How Multilingual is
Multilingual BERT?” In: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. Association for Computational
Linguistics, pp. 4996-5001.

Plank, B. (2011). “Domain Adaptation for Parsing”. Ph.D. thesis. University of
Groningen.

Plank, B. (2016). “What to do about non-standard (or non-canonical) language
in NLP”. In: CoRR vol. abs/1608.07836.

Plank, B., Sdgaard, A., and Goldberg, Y. (2016). “Multilingual Part-of-Speech
Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary
Loss”. In: Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, Volume 2: Short Papers. The
Association for Computer Linguistics.

Plas, L. van der and Tiedemann, J. (2006). “Finding Synonyms Using Automatic
Word Alignment and Measures of Distributional Similarity”. In: 2/st
International Conference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics, pp. 866-873.

Pontiki, M., Galanis, D., Pavlopoulos, J., Papageorgiou, H., Androutsopoulos, L.,
and Manandhar, S. (2014). “SemEval-2014 Task 4: Aspect Based Sentiment
Analysis”. In: Proceedings of the 8th International Workshop on Semantic
Evaluation (SemEval 2014). Association for Computational Linguistics,
pp. 27-35.

Pratap, B., Shank, D., Ositelu, O., and Galbraith, B. (2018). “Talla at SemEval-
2018 Task 7: Hybrid Loss Optimization for Relation Classification using
Convolutional Neural Networks”. In: Proceedings of The 12th International
Workshop on Semantic Evaluation. Association for Computational Linguistics,
pp. 863-867.

Prechelt, L. (2012). “Early Stopping — But When?” In: Neural Networks: Tricks
of the Trade: Second Edition. Springer Berlin Heidelberg, pp. 53-67.

Pyysalo, S., Ginter, F., Moen, H., Salakoski, T., and Ananiadou, S. (2013).
“Distributional Semantics Resources for Biomedical Text Processing”. In:
Proceedings of LBM 2013, pp. 39-44.

Qian, K. and Yu, Z. (2019). “Domain Adaptive Dialog Generation via Meta
Learning”. In: Proceedings of the 57th Annual Meeting of the Association

171


Bibliography

for Computational Linguistics. Association for Computational Linguistics,
pp. 2639-2649.

Qin, P., Xu, W., and Wang, W. Y. (2018). “Robust Distant Supervision Relation
Extraction via Deep Reinforcement Learning”. In: Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Linguistics, pp. 2137-2147.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). “Improving
language understanding with unsupervised learning”. In: Technical report,
OpenAL.

Radinsky, K., Agichtein, E., Gabrilovich, E., and Markovitch, S. (2011). “A
Word at a Time: Computing Word Relatedness Using Temporal Semantic
Analysis”. In: Proceedings of the 20th International Conference on World
Wide Web. Association for Computing Machinery, pp. 337-346.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). “SQuAD: 100,000+
Questions for Machine Comprehension of Text”. In: Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, pp. 2383-2392.

Ratinov, L. and Roth, D. (2009). “Design Challenges and Misconceptions in
Named Entity Recognition”. In: Proceedings of the Thirteenth Conference on
Computational Natural Language Learning. Association for Computational
Linguistics, pp. 147-155.

Ravi, S. and Larochelle, H. (2017). “Optimization as a Model for Few-Shot
Learning”. In: 5th International Conference on Learning Representations,
ICLR 2017, Conference Track Proceedings. OpenReview.net.

Rehtitek, R. and Sojka, P. (2010). “Software Framework for Topic Modelling with
Large Corpora”. In: Proceedings of LREC 2010 workshop New Challenges
for NLP Frameworks. European Language Resources Association (ELRA),
pp. 46-50.

Ren, X., El-Kishky, A., Wang, C., Tao, F., Voss, C. R., and Han, J. (2015).
“ClusType: Effective Entity Recognition and Typing by Relation Phrase-
Based Clustering.” In: International Conference on Knowledge Discovery €&
Data Mining. Association for Computing Machinery, pp. 995-1004.

Riedel, S., Yao, L., and McCallum, A. (2010). “Modeling Relations and Their
Mentions Without Labeled Text”. In: Proceedings of the 2010 European
Conference on Machine Learning and Knowledge Discovery in Databases:
Part III. Springer-Verlag, pp. 148-163.

Rink, B. and Harabagiu, S. (2010). “UTD: Classifying Semantic Relations by
Combining Lexical and Semantic Resources”. In: Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation. Association for Computational
Linguistics, pp. 256-259.

Rong, X. (2014). “word2vec Parameter Learning Explained”. In: CoRR
vol. abs/1411.2738.

Rotsztejn, J., Hollenstein, N., and Zhang, C. (2018). “ETH-DS3Lab at SemEval-
2018 Task 7: Effectively Combining Recurrent and Convolutional Neural
Networks for Relation Classification and Extraction”. In: Proceedings of

172


Bibliography

The 12th International Workshop on Semantic Evaluation. Association for
Computational Linguistics, pp. 689-696.

Rubenstein, H. and Goodenough, J. B. (1965). “Contextual Correlates of
Synonymy”. In: COMMUNICATIONS OF THE ACM vol. 8, no. 10, pp. 627—
633.

Ruder, S., Peters, M. E., Swayamdipta, S., and Wolf, T. (2019). “Transfer
Learning in Natural Language Processing”. In: Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational
Linguistics: Tutorials. Association for Computational Linguistics, pp. 15-18.

Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). “Learning Internal
Representations by Error Propagation”. In: Parallel Distributed Processing.
MIT Press, pp. 318-362.

Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. P. (2016).
“Meta-Learning with Memory-Augmented Neural Networks”. In: Proceedings
of the 38nd International Conference on Machine Learning, ICML 2016.
Vol. 48. JMLR.org, pp. 1842-1850.

Schick, T. and Schiitze, H. (2019a). “Attentive Mimicking: Better Word
Embeddings by Attending to Informative Contexts”. In: Proceedings of
the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers). Association for Computational Linguistics, pp. 489-494.

Schick, T. and Schiitze, H. (2019b). “Learning Semantic Representations for
Novel Words: Leveraging Both Form and Context”. In: The Thirty-Third
AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First
Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The
Ninth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAT 2019. Association for the Advancement of Artificial Intelligence (AAAT)
Press, pp. 6965-6973.

Schick, T. and Schiitze, H. (2020). “BERTRAM: Improved Word Embeddings
Have Big Impact on Contextualized Model Performance”. In: Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics.

Schnabel, T., Labutov, I., Mimno, D. M., and Joachims, T. (2015). “Evaluation
methods for unsupervised word embeddings”. In: Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, pp. 298-307.

Schuster, M. and Paliwal, K. (1997). “Bidirectional Recurrent Neural Networks”.
In: IEEE Transactions on Signal Processing vol. 45, no. 11, pp. 2673-2681.

Schwartz, R., Abend, O., and Rappoport, A. (2012). “Learnability-Based
Syntactic Annotation Design”. In: Proceedings of the 24th International
Conference on Computational Linguistics (Coling 2012). The COLING 2012
Organizing Committee, pp. 2405-2422.

Sechidis, K., Tsoumakas, G., and Vlahavas, I. (2011). “On the Stratification
of Multi-label Data”. In: Proceedings of the 2011 European Conference on
Machine Learning and Knowledge Discovery in Databases - Volume Part III.
Springer-Verlag, pp. 145-158.

173


Bibliography

Sennrich, R., Haddow, B., and Birch, A. (2016). “Neural Machine Translation
of Rare Words with Subword Units”. In: Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, pp. 1715-1725.

Shang, J., Liu, J., Jiang, M., Ren, X., Voss, C. R., and Han, J. (2018a). “Auto-
mated Phrase Mining from Massive Text Corpora”. In: [IEEE Transactions
on Knowledge and Data Engineering vol. 30, no. 10, pp. 1825-1837.

Shang, J., Liu, L., Gu, X., Ren, X., Ren, T., and Han, J. (2018b). “Learning
Named Entity Tagger using Domain-Specific Dictionary”. In: Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pp. 2054-2064.

Smirnova, A. and Cudré-Mauroux, P. (2018). “Relation Extraction Using Distant
Supervision: A Survey”. In: ACM Computing Survey vol. 51, no. 5, pp. 1-85.

Socher, R., Chen, D., Manning, C. D., and Ng, A. (2013a). “Reasoning With
Neural Tensor Networks for Knowledge Base Completion”. In: Advances in
Neural Information Processing Systems 26. Curran Associates, Inc., pp. 926—
934.

Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012). “Semantic
Compositionality Through Recursive Matrix-vector Spaces”. In: Proceedings
of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning. Association for
Computational Linguistics, pp. 1201-1211.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts,
C. (2013b). “Recursive Deep Models for Semantic Compositionality Over a
Sentiment Treebank”. In: Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing. Association for Computational
Linguistics, pp. 1631-1642.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts,
C. (2013c). “Recursive Deep Models for Semantic Compositionality Over a
Sentiment Treebank”. In: Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing. Association for Computational
Linguistics, pp. 1631-1642.

Song, L., Zhang, Y., Gildea, D., Yu, M., Wang, Z., and Su, J. (2019). “Leveraging
Dependency Forest for Neural Medical Relation Extraction”. In: Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, pp. 208-218.

Soricut, R. and Och, F. (2015). “Unsupervised Morphology Induction Using Word
Embeddings”. In: Proceedings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies. Association for Computational Linguistics, pp. 1627-1637.

Speer, R., Chin, J., and Havasi, C. (2017). “ConceptNet 5.5: An Open
Multilingual Graph of General Knowledge”. In: Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence. Association for the Advancement
of Artificial Intelligence (AAAT) Press, pp. 4444-4451.

174


Bibliography

Stanovsky, G. and Dagan, I. (2016). “Creating a Large Benchmark for Open
Information Extraction”. In: Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Association for Computational
Linguistics, pp. 2300-2305.

Stanovsky, G., Michael, J., Zettlemoyer, L., and Dagan, I. (2018). “Supervised
Open Information Extraction”. In: Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers). Association for
Computational Linguistics, pp. 885-895.

Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P. H. S., and Hospedales, T. M.
(2018). “Learning to Compare: Relation Network for Few-Shot Learning”. In:
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.
IEEE, pp. 1199-1208.

Sutton, R., Barto, A. G., Bach, F., et al. (1998). “Reinforcement learning: An
introduction”. In: MIT press.

Sutton, R., McAllester, D., Singh, S., and Mansour, Y. (1999). “Policy Gradient
Methods for Reinforcement Learning with Function Approximation”. In:
Proceedings of the 12th International Conference on Neural Information
Processing Systems. MIT Press, pp. 1057-1063.

Tjong Kim Sang, E. F. and Buchholz, S. (2000). “Introduction to the CoNLL-2000
Shared Task: Chunking”. In: Proceedings of the 2nd Workshop on Learning
Language in Logic and the 4th Conference on Computational Natural Language
Learning - Volume 7. Association for Computational Linguistics, pp. 127-132.

Tjong Kim Sang, E. F. and De Meulder, F. (2003). “Introduction to the CoNLL-
2003 Shared Task: Language-Independent Named Entity Recognition”. In:
Proceedings of the Seventh Conference on Natural Language Learning at
HLT-NAACL 2008. Association for Computational Linguistics, pp. 142-147.

Tsuboi, Y., Kashima, H., Mori, S., Oda, H., and Matsumoto, Y. (2008). “Training
Conditional Random Fields Using Incomplete Annotations”. In: Proceedings
of the 22nd International Conference on Computational Linguistics (Coling
2008). Coling 2008 Organizing Committee, pp. 897-904.

Tsvetkov, Y. (2016). “Linguistic Knowledge in Data-Driven Natural Language
Processing”. PhD thesis. Carnegie Mellon University.

Tsvetkov, Y., Faruqui, M., and Dyer, C. (2016). “Correlation-based Intrinsic
Evaluation of Word Vector Representations”. In: Proceedings of the 1st
Workshop on Evaluating Vector-Space Representations for NLP. Association
for Computational Linguistics, pp. 111-115.

Tsvetkov, Y., Faruqui, M., Ling, W., Lample, G., and Dyer, C. (2015). “Evaluation
of Word Vector Representations by Subspace Alignment”. In: Proceedings of
the 2015 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pp. 2049-2054.

Turian, J., Ratinov, L., and Bengio, Y. (2010). “Word Representations: A
Simple and General Method for Semi-supervised Learning”. In: Proceedings
of the 48th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, pp. 384-394.

175


Bibliography

van der Wees, M., Bisazza, A., Weerkamp, W., and Monz, C. (2015). “What’s
in a Domain? Analyzing Genre and Topic Differences in Statistical Machine
Translation”. In: Proceedings of the 58rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short Papers). Association for
Computational Linguistics, pp. 560-566.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
Kaiser, L., and Polosukhin, I. (2017). “Attention Is All You Need”. In: CoRR
vol. abs/1706.03762.

Verga, P., Belanger, D., Strubell, E., Roth, B., and McCallum, A. (2016).
“Multilingual Relation Extraction using Compositional Universal Schema”.
In: Proceedings of the 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies.
Association for Computational Linguistics, pp. 886-896.

Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D.
(2016). “Matching Networks for One Shot Learning”. In: Advances in
Neural Information Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, pp. 3630-3638.

Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, 5S. R.
(2018). “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding”. In: CoRR vol. abs/1804.07461.

Wang, H., Tan, M., Yu, M., Chang, S., Wang, D., Xu, K., Guo, X., and Potdar,
S. (2019a). “Extracting Multiple-Relations in One-Pass with Pre-Trained
Transformers”. In: Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics. Association for Computational Linguistics,
pp. 1371-1377.

Wang, H., Lu, Y., and Zhai, C. (2011). “Latent Aspect Rating Analysis
Without Aspect Keyword Supervision”. In: Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
Association for Computing Machinery, pp. 618-626.

Wang, X., Gao, T., Zhu, Z., Liu, Z., Li, J., and Tang, J. (2019b). “KEPLER:
A Unified Model for Knowledge Embedding and Pre-trained Language
Representation”. In: CoRR vol. abs/1911.06136.

Wang, X., Zhang, Y., Ren, X., Zhang, Y., Zitnik, M., Shang, J., Langlotz, C.,
and Han, J. (2019c). “Cross-type biomedical named entity recognition with
deep multi-task learning”. In: Bioinformatics vol. 35, no. 10, pp. 1745-1752.

Williams, A., Nangia, N., and Bowman, S. (2018). “A Broad-Coverage Challenge
Corpus for Sentence Understanding through Inference”. In: Proceedings of
the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long
Papers). Association for Computational Linguistics, pp. 1112-1122.

Williams, R. J. (1992). “Simple statistical gradient-following algorithms for
connectionist reinforcement learning”. In: Machine Learning vol. 8, no. 3,
pp. 229-256.

Wu, R., Yao, Y., Han, X., Xie, R., Liu, Z., Lin, F., Lin, L., and Sun, M.
(2019). “Open Relation Extraction: Relational Knowledge Transfer from

176


Bibliography

Supervised Data to Unsupervised Data”. In: Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP). Association for Computational Linguistics, pp. 219-228.

Wu, S. and Dredze, M. (2019). “Beto, Bentz, Becas: The Surprising Cross-Lingual
Effectiveness of BERT”. In: Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP). Association
for Computational Linguistics, pp. 833-844.

Wu, S., Zhang, D., Yang, N., Li, M., and Zhou, M. (2017). “Sequence-to-
Dependency Neural Machine Translation”. In: Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, pp. 698-707.

Xu, F., Uszkoreit, H., Krause, S., and Li, H. (2010). “Boosting Relation
Extraction with Limited Closed-World Knowledge”. In: Proceedings of the
23rd International Conference on Computational Linguistics: Posters. Coling
2010 Organizing Committee, pp. 1354-1362.

Xu, K., Feng, Y., Huang, S., and Zhao, D. (2015a). “Semantic Relation
Classification via Convolutional Neural Networks with Simple Negative
Sampling”. In: Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics,
pp. 536-540.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., and Jin, Z. (2015b). “Classifying
Relations via Long Short Term Memory Networks along Shortest Dependency
Paths”. In: Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing. Association for Computational Linguistics,
pp. 1785-1794.

Yaghoobzadeh, Y. and Schiitze, H. (2016). “Intrinsic Subspace Evaluation of
Word Embedding Representations”. In: Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, pp. 236-246.

Yang, D. and Powers, D. M. W. (2006). “Verb Similarity on the Taxonomy
of Wordnet”. In: In the 3rd International WordNet Conference (GWC-06).
Masaryk University.

Yang, Y., Chen, W., Li, Z., He, Z., and Zhang, M. (2018). “Distantly Supervised
NER with Partial Annotation Learning and Reinforcement Learning”.
In: Proceedings of the 27th International Conference on Computational
Linguistics. Association for Computational Linguistics, pp. 2159-2169.

Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., and Le,
Q. V. (2019). “XLNet: Generalized Autoregressive Pretraining for Language
Understanding”. In: Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS
2019. Curran Associates, Inc., pp. 5754-5764.

Yates, A., Banko, M., Broadhead, M., Cafarella, M., Etzioni, O., and Soderland,
S. (2007). “TextRunner: Open Information Extraction on the Web”. In:
Proceedings of Human Language Technologies: The Annual Conference of the

177


Bibliography

North American Chapter of the Association for Computational Linguistics
(NAACL-HLT). Association for Computational Linguistics, pp. 25-26.

Young, T., Hazarika, D., Poria, S., and Cambria, E. (2018). “Recent trends in
deep learning based natural language processing”. In: IEEE Computational
intelligenCe magazine vol. 13, no. 3, pp. 55-75.

Yu, M. and Dredze, M. (2014). “Improving Lexical Embeddings with Semantic
Knowledge”. In: Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers). Association for
Computational Linguistics, pp. 545-550.

Yu, M., Guo, X., Yi, J., Chang, S., Potdar, S., Cheng, Y., Tesauro, G., Wang, H.,
and Zhou, B. (2018). “Diverse Few-Shot Text Classification with Multiple
Metrics”. In: Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers). Association for Computational
Linguistics, pp. 1206-1215.

Zeng, X., Zeng, D., He, S., Liu, K., and Zhao, J. (2018). “Extracting Relational
Facts by an End-to-End Neural Model with Copy Mechanism”. In: Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Computational Linguistics, pp. 506-
514.

Zhang, D. and Wang, D. (2015). “Relation Classification via Recurrent Neural
Network”. In: CoRR vol. abs/1508.01006.

Zhang, S., Duh, K., and Van Durme, B. (2017). “MT/IE: Cross-lingual Open
Information Extraction with Neural Sequence-to-Sequence Models”. In:
Proceedings of the 15th Conference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Papers. Association for
Computational Linguistics, pp. 64-70.

Zhang, Y. and Wallace, B. (2017). “A Sensitivity Analysis of (and Practitioners’
Guide to) Convolutional Neural Networks for Sentence Classification”. In:
Proceedings of the Eighth International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Asian Federation of Natural Language
Processing, pp. 253-263.

Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., and Liu, Q. (2019).
“ERNIE: Enhanced Language Representation with Informative Entities”. In:
Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, pp. 1441-1451.

Zhang, Z., Wu, Y., Zhao, H., Li, Z., Zhang, S., Zhou, X., and Zhou, X. (2020).
“Semantics-Aware BERT for Language Understanding”. In: The Thirty-Fourth
AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Conference, [AAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAT 2020. Association for the Advancement of Artificial Intelligence (AA AT)
Press, pp. 9628-9635.

Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., and Xu, B. (2017).
“Joint Extraction of Entities and Relations Based on a Novel Tagging
Scheme”. In: Proceedings of the 55th Annual Meeting of the Association

178


Bibliography

for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, pp. 1227-1236.

Zhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., and Xu, B. (2016).
“Attention-Based Bidirectional Long Short-Term Memory Networks for
Relation Classification”. In: Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics. Association for Computational
Linguistics.

Zipf, G. (1972). Human Behavior and the Principle of Least Effort: An
Introduction to Human Ecology. Hafner.

Zoph, B., Yuret, D., May, J., and Knight, K. (2016). “Transfer Learning for
Low-Resource Neural Machine Translation”. In: Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, pp. 1568-1575.

179
