2111.08408v1 [cs.CL] 16 Nov 2021

arXiv

STAMP 4 NLP — An Agile Framework for Rapid
Quality-Driven NLP Applications Development

Philipp Kohl’, Oliver Schmidts!, Lars Kléser!, Henri Werth!, Bodo Kraft',
and Albert Ziindorf?

' FH Aachen - University of Applied Sciences, 52428 Jiilich, Germany
? University of Kassel, 34127 Kassel, Germany
{p.kohl,schmidts,kloeser,werth,kraft}@fh-aachen.de
zuendorf@uni-kassel.de

Abstract. The progress in natural language processing (NLP) research
over the last years, offers novel business opportunities for companies, as
automated user interaction or improved data analysis. Building sophisti-
cated NLP applications requires dealing with modern machine learning
(ML) technologies, which impedes enterprises from establishing success-
ful NLP projects. Our experience in applied NLP research projects shows
that the continuous integration of research prototypes in production-like
environments with quality assurance builds trust in the software and
shows convenience and usefulness regarding the business goal. We intro-
duce STAMP 4 NLP as an iterative and incremental process model for
developing NLP applications. With STAMP 4 NLP, we merge software
engineering principles with best practices from data science. Instantiating
our process model allows efficiently creating prototypes by utilizing tem-
plates, conventions, and implementations, enabling developers and data
scientists to focus on the business goals. Due to our iterative-incremental
approach, businesses can deploy an enhanced version of the prototype to
their software environment after every iteration, maximizing potential
business value and trust early and avoiding the cost of successful yet
never deployed experiments.

Keywords: Natural Language Processing - Process Model - Machine
Learning - Best Practices - Avoiding Pitfalls - Quality Assurance.

1 Introduction

The field of artificial intelligence in general and natural language processing as
one of its sub-fields offers tremendous novel business opportunities in a steadily
growing market [13]. Recent progress in NLP research shows the potential for
business applications, leading to a demand for more advanced NLP applications

The state-of-the-art in NLP differs from research to industrial domains. Be-
sides the progress in research, the application of ML-based NLP in many en-
terprises is severely limited [6]. The black-box behavior of ML models, missing


2 P. Kohl et al.

know-how, complex technological landscape, and the decision on an appropri-
ate tool stack discourage enterprises from implementing NLP approaches [1,10].
They discard promising projects due to the combination of high and uncertain
effort estimation [23].

Many ML projects fail because of exceeding budgets, deadlines or they do
not meet the business requirements [11]. The late integration of several projects
can lead to a services shutdown [27,17]. We minimize the risk of these situations
with agile methodology to handle the uncertainties and generate business value
and feedback on the application as early as possible. This increases the quality
and trust in the software for all involved stakeholders [5].

We propose a new process model adjusted for developing NLP applications:
Standardized Modeling Process for Natural Language Processing (STAMP 4
NLP).

With STAMP 4 NLP, we merge software engineering principles with best
practices from data science to improve and accelerate the development cycle and
integrate prototypes with every iteration into a test or production environment.
STAMP 4 NLP provides a transparent development process, including roles,
tasks, artifacts, and best practices.

Our main contributions? are:

— A novel process model for developing NLP applications, with formally spec-
ified roles, activities, and artifacts focusing on quality, and early business
value.

— Usage of predefined environment and software templates based on prior ex-
periences for accelerating the development start.

2 Related Work

Knowledge Discovery in Databases (KDD) [9] represents one of the first process
models for data mining. It offers a generic guided process of the technical tasks
to reveal patterns in data and building knowledge.

Cross Industry Standard Process for Data Mining (CRISP-DM) [4] also con-
siders business requirements and models the application development process in
an applied context in contrast to KDD. Modern ML process models originate
from CRISP-DM. It consists of six stages: Business Understanding, Data Under-
standing, Data Preparation, Modeling, Evaluation, and Deployment. Depending
on the stage’s results, it allows transitions to previous stages.

CRISP-ML(Q) [24] extends CRISP-DM for machine learning and explicitly
considers the differences between data mining (revealing patterns in data) and
machine learning (training and inference). Studer et al. focus on quality assur-
ance on every specific task. CRISP-ML(Q) merges Business Understanding and
Data Understanding into a single stage and adds the Monitoring and Mainte-
nance stage, addressing particular challenges of machine learning applications
not considered by CRISP-DM.

3 https: //github.com/philipp-kohl/stamp4nlp


STAMP 4 NLP —- Agile NLP Application Development 3

While CRISP-DM and CRISP-ML(Q) mainly focus on application creation,
Weber et al. [28] introduced an approach with defined transitions between model
development and model operation. Thus, they cover the whole model lifecycle
from planning over production until retirement. Their process does not explicitly
consider business requirements.

Similar to KDD, Amershi et al. [2] focuses on the modern, mainly technical
process for developing a machine learning model but also incorporates funda-
mental operational analytics with transitions to previous stages.

In contrast to the mentioned process models, we focus on NLP. We treat
manual data annotations and annotation guidelines as central project artifacts.
Our approach is especially suitable for supervised NLP tasks as information ex-
traction. Further, our focus is on strong quality assurance with different levels
of applied tests. We leverage approaches and best practices from software engi-
neering, combining them with machine learning approaches, such as supporting
versioning of code, data, models, and tracking experiment results [16].

Agile software development [5] uses iterations and increments instead of tran-
sitions between process stages. This leads to a stronger focus on running software
during the development process. The incremental aspect allows isolated investi-
gation of the experimental effects and the usage of continuous integration and
delivery (CI/CD) [8]. Developers receive feedback, and stakeholders gain value
and trust in the application with every deployed increment. In comparison to [24]
with task-agnostic quality assurance, we measure technical and business-oriented
metrics on a higher level after every iteration [18,19,22].

Inspired by Spring [14], maven archetypes [25] and NLPf [20], we deliver
STAMP 4 NLP with a framework, which supports the developer with a prede-
fined development environment, code for well-known standard tasks [12,26] for
creating a rapid prototype as basline or proof of concept, which the developer
enhances over iterations and increments. STAMP 4 NLP decreases the risk of
not deployed valuable experiments and failing projects.

3 Process Model

STAMP 4 NLP is an instantiable, iterative, and incremental process model for
developing natural language processing applications with a focus on quality,
business value, and simplified prototyping.

STAMP 4 NLP uses agile methodology [5] by establishing software in incre-
ments developers enhance in various iterations (c.f. Figure 1). The Evolution
Loop works in sprints (e.g., 2-4 weeks) to create a new release candidate that
the developers integrate into the customer’s test or production environment to
gain trust in the software and receive feedback for further improvements to fit
the customer’s needs [8]. The workflow is as follows: the developers start with
the Evolution Loop to define or refine the requirements, followed by multiple
iterations in the Development Loop (hours to days) to create a model that sat-
isfies the specified requirements. Deployment and testing are also a part of the
inner loop. Once the software fulfills them, the developers leave the inner loop


4 P. Kohl et al.

and proceed to the outer loop to integrate and monitor the release candidate.
Depending on the monitoring results, they trigger a new iteration.

The process model
provides particular levels
of transparency. At first,
every stakeholder can fol-
low the complex process,
even if they are unfa-
miliar with NLP. They
can also track the or-
der of tasks by con-
sulting the documenta-
tion. Thereby the respon-
sibilities are clearly de-

fined, and every involved Fig. 1. STAMP 4 NLP consists of five subprocesses ar-
party knows their tasks. yaned in two nested loops: Development Loop (green +
Clearly defined responsi- solid) for developing the NLP application and model.
bilities are helpful to pre- Evolution Loop (blue + dashed) surrounds the Devel-
vent conflicts and create opment Loop and covers mainly the interaction with the
rational processes that customer: creation and refinement of the project goals
the users can monitor With associated requirements and integration and mon-
and optimize. We use itoring of the application. Note: The solid and dashed

connectors have the same meaning of sequence flow but
the MEDIATION [1] ap help distinguish the loops in black and white print.

Seicnceea seed

No

Refinement needed?

proach to provide the ap-
plication state transpar-
ent via a dashboard with project specific business metrics for all stakeholders.

The development environment equips the user with a standard set of tools
for rapidly building a proof of concept while maintaining the flexibility to let the
user choose other preferred libraries. Creating a project instance provides folder
structure, development environment, documentation, and boilerplate code (e.g.,
REST-Service with predefined endpoints, etc.). The standard folder facilitates
the automatic loading and storing of data, models, and results to their desti-
nation by convention. Furthermore, no settling-in period for already STAMP-
involved members; they know where to find code, documentation, experiments,
and data.

ML metrics serve as a common benchmark for different models. High or low
metrics show the averaged performance but do not allow making conclusions
which use-cases a model cannot handle appropriately. Model interpretability is
a current research subject [1]. To minimize the interpretability gap in machine
learning and to receive more feedback on a low application level, we incorporated
CheckList [18] for creating behavioral tests aiming for specific capabilities the
NLP application should cover. Besides CheckList, we use MEDIATION [19] for
testing the NLP application on a business level: the developers and stakeholders
define test cases strongly related to their intended use cases in the form of
annotated documents. Thus, this additional test set serves as an indicator of


STAMP 4 NLP —- Agile NLP Application Development 5

the business readiness of the NLP application. In combination with the CI/CD
approach, we receive this feedback for every iteration and increment. Involving
testers or real user groups into the increment testing generates feedback for
business and practical usage.

STAMP 4 NLP supports the user to keep the reproducibility of experiments
and models as high as possible. We incorporated parts of Pineau’s reproducibility
list [16] into the process by documentation and tools supporting versioning of
code, data, models, and tracking experiment’s results.

In the following, we give a short description of each subprocess with its
primary artifacts. We show exemplarily a detailed BPMN diagram of the Domain
Adoption and Customization in Figure 2. Our GitHub repository provides the
other subprocess diagrams and the detailed description of each task and artifact.

3.1 Goal Specification

Description: The Goal Specification aims to establish a common business un-
derstanding. The stakeholders define and refine the business goals, and their as-
sociated technical, machine learning, business, and MEDIATION requirements
and update the documentation accordingly. It includes an evaluation of all data
sources and the data provision for the data scientists. This stage involves all
currently relevant stakeholders to minimize the bias and possibly wrong model
assumptions.

Artifacts: The primary artifacts are the refined and reviewed requirements, test
cases, and access to all mandatory data sources.

3.2. Domain Discovery and Data Selection

Description: Data scientists and domain experts prepare the annotation pro-
cess (c.f. Figure 2). They identify NLP tasks and corresponding annotation
schemas helpful to fulfill the business goals. Additionally, data scientists in-
clude domain knowledge from experts to steadily improve annotation guidelines
and collect and evaluate data samples for the annotation process and necessary
metadata.

Artifacts: The primary artifacts are the annotation guidelines, the new cor-
pus versions prepared for annotation, and documentation about licenses, data
protection, and data security.

3.3. Domain Adoption and Customization

Description: This subprocess (c.f. Figure 2) includes data annotation and
model training. The annotation process setup involves planning and, if necessary
a domain training for annotators. The annotated texts build a new corpus ver-
sion, which is used for training a new model. To minimize the annotation effort,
we want to stop further annotating when noticing the resulting model’s metrics
stagnate. We incorporated the continuous integration and delivery approach of


6 P. Kohl et al.

Domain Adoption & Customization

°} Data Scientist Domain Expert Annotator Documentation Data
Annotation Process ial a
SSS
Annotation Guidelines :
a
SS
Knowledge for Training Dataset
annotation present? ee
Evaluation Dataset
Test Dataset
a ee
Data Augmentation
Strategies Ot
——————

Augmented Training
Dataset
>

Evaluation Dataset
Model rs
|

Test Dataset

Metric Report ell

Fig. 2. Showing the task order of the subprocess Domain Adoption and Customiza-
tion as BPMN Diagram with the involved roles displayed as swimlanes. Additional
information is provided by Documentation and Data lane. Create new corpus version,
Training, and Apply Data Augmentation are automatic tasks, which run without hu-
man interaction. Training requires the execution of one command as trigger.

Schreiber et al. [21]. Thus, annotators receive feedback after each annotation
session. The feedback can motivate to continue or stop annotating because the
further annotations do not impact the model’s performance remarkably.

Artifacts: The primary artifacts are the new corpus version, the new model,
and metric reports about the model’s performance.

3.4 Application Engineering

Description: Software developers package the model and all necessary depen-
dencies. A CI/CD pipeline deploys the application in a test or production-like
environment and runs evaluations to ensure the quality gates (software quality,
machine learning metrics MEDIATION and behavioral tests via CheckList [18])
defined in the goal specification. The software package is versioned to provide
transparency, whether the made modifications improved the previous version and
for fallback solutions. Depending on the evaluation results, the application stays
in the Development Loop or transitions into the Evolution Loop for integrating
the software into the customer’s application landscape (c.f. Figure 1).
Artifacts: The primary artifacts are the software package and a quality assur-
ance report.


STAMP 4 NLP —- Agile NLP Application Development 7

3.5 Customer Integration and Evaluation

Description: Software developers integrate the packaged NLP application in
the customer’s application landscape. On the customer side, a monitoring ser-
vice checks the model’s performance. The resulting reports build the basis for a
refinement of the quality gates comparable to [22,19] ensuring the fulfillment of
the business requirements during the production phase.

Artifacts: An operation manual documents the deployment, and an integra-
tion plan explains the integration in the customer application. The performance
reports support recommendations and business decisions.

4 Project Template

STAMP 4 NLP facilitates focusing on the project-specific challenges such as
analyzing the data and the domain, annotating, experimenting with different
concepts, and deep learning architectures. To decrease the overhead data scien-
tists face while starting a new project, we offer a template with development
environment, folder structure*, tools, code and process documentation. Further-
more, the framework can generate customizable implementations for specific
NLP tasks (e.g., named entity recognition (NER) [12], or text classification [26])
into the project, helping the developers implement a first baseline or proof of
concept.

The template serves the paradigm convention over configuration [3]. There-
fore the template comes with, but is not limited to a standard set of tools
and libraries. If the user stays with the standard, no additional configuration
is needed. But the user has the opportunity to use additional libraries or tools,
resulting in extra configuration. The basic configuration provides, for exam-
ple, the library spacy® to create prototypes quickly. Depending on the business
goal, it is necessary to preserve more control over used architecture and training
routines. Therefore the developer can exchange the conventional added spacy
module with PyTorch® or similar frameworks. The same applies to the folder
structure, environment, and infrastructure. We recommend to start with the
standard configuration and specialize on demand.

5 Example

This section demonstrates a simplified STAMP 4 NLP usage over a few itera-
tions to show the intuition behind the process model. We focus on a real-world
project we performed with our business partners: The profile extraction from
social media messages of an advertising group conversation. We use named en-
tity recognition (NER) as a standard NLP task, for which we can use existing

4 similar to https: //drivendata. github. io/cookiecutter-data-science/
5 nttps://spacy.io/
° https: //pytorch.org/


8 P. Kohl et al.

approaches. NER describes the task of finding domain-relevant terms in doc-
uments: e.g., persons, brands, products, and their prices. On top of that, we
implement a business layer to aggregate the named entities to a profile.

First iteration — Requirements Analysis and Dry Run: The first iter-
ation focuses on the requirements analysis and the infrastructure test run (also
called dry run). Instantiating the process model provides a development envi-
ronment including a prototypical web-service, a pre-configured CI/CD pipeline,
and prepared documentation. We define and document the NER as the applied
NLP task. Based on the documentation, we invoke the framework for generating
a reference implementation for NER as a first baseline. Among others, we define
the corresponding machine learning metrics we want to achieve with the NLP
application. We skip the most tasks of all other subprocesses for this iteration
since its the iteration’s goal to ensure the infrastructure: training a model, em-
bed the model into a software package, deploy into a test environment, model
evaluation, publish results via a dashboard. The scores do not matter at this
stage.

Second iteration — Baseline:
The second iteration focuses on cre-
ating a first baseline. This iteration
covers the Development Loop ex- i
clusively. We prepare the annota- ~~ eee ——
tion process by defining the annota- * ; :
tion guidelines, deciding on a suit- _<“ gl
able corpus format, and transfer- .
ring the data to the corpus format
in Domain Discovery and Data Se-
lection. Domain Adoption and Cus-
tomization mainly focuses on anno-
tating a subset of the data in this
iteration to create the first baseline
with the standard implementation.
We embed the model into our soft-
ware architecture and add a busi-
ness layer to combine the named
entities to a profile. Our CI/CD
approach packages and deploys the

NER Scores for Profile Extraction == Fl-series == P-series == R-series
5

65 ~*~

60 4

Fig. 3. An excerpt of a Grafana Dashboard
visualizes the F1, precision, and recall score
for the NER component over several iterations
of annotating and architecture decision. Preci-
sion measures the correctness of positive clas-
sifications (here for a named entity) by penal-
izing false positives. Recall shows if our model
has found all named entities by penalizing false

software into the test environment,
where the framework performs de-
tailed quality assurance. The re-
sults are published on a dashboard
(c.f. Figure 3).

negatives. F1 is the harmonic mean of preci-
sion and recall. It is commonly used for model
evaluation and comparison.

Additional iterations — Beat the baseline: Further iterations focus iso-
lated on specific aspects to enhance the baseline. We incorporate the aspects as
isolated as possible to ensure the cause-effect relationships:


STAMP 4 NLP —- Agile NLP Application Development 9

Annotations Annotating new data points, improve existing annotations, de-
cide to incorporate new labels, enhance annotation guideline based on gained
experience.

Architectural Decisions Use different common neural network components
or pretrained contextualizes embedding layers [15,7].

If the application fulfills the requirements, we have a possible release candidate
and exit the Development Loop and continue the Evolution Loop. If the results
do not show the expected behavior or do not fulfill the business need, we begin
a new iteration to investigate the cause and start new experiments improving
the application’s quality.

6 Limitations and Drawbacks

Our proposed process model has a strong focus on supervised NLP. We have
many subprocesses with corresponding roles and artifacts for problems that in-
clude the manual annotation of corpora. They may be unbeneficial for unsu-
pervised tasks. We further define clear responsibilities and processes but assume
that a practical application involves loose compliance to those in some cases. The
process needs to be adapted to each project individually. A significant benefit of
the instantiable framework is a low application barrier. The resulting standard-
ized configuration and black box code can lead to laborious error detection or
adaptions when moving too far from these conventions.

7 Summary

We introduced STAMP 4 NLP a novel instantiable and iterative-incremental
process model to develop NLP applications. It supports developers to create
valuable and deployable increments rapidly, results in earlier feedback, and im-
proves quality and trust in the application for all stakeholders.

Our approach equips the user with templates, development environment, and
documentation to reduce the starting and integration overhead. That minimizes
implementation barriers, avoids common pitfalls, and sets the focus on the busi-
ness goal. Thus, STAMP 4 NLP reduces the risk of failing projects.

In the future, we plan to create a benchmark project for different groups
to work on: some groups work with the STAMP 4 NLP and others work from
scratch. Thus, we want to measure various project milestones, key performance
indicators and observe the challenges the different teams face and pitfalls they
could avoid. Furthermore, we want to use it for educational purposes to set the
focus appropriately with incremental depth increase. We want to improve the
development of generic NLP tasks, including unsupervised problem settings.

References

1. Adadi, A., Berrada, M.: Peeking inside the black-box: A survey on ex-
plainable artificial intelligence (xai). IEEE Access 6, 52138-52160 (2018).
https: //doi.org/10.1109/ACCESS.2018.2870052


10

10.

11.

12.

13.

14.

15.

16.

P. Kohl et al.

. Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan,

N., Nushi, B., Zimmermann, T.: Software engineering for machine learning: A
case study. In: Proceedings of the 41st International Conference on Software En-
gineering: Software Engineering in Practice. p. 291-300. ICSE-SEIP °19, IEEE
Press (2019). https://doi.org/10.1109/ICSE-SEIP.2019.00042, https: //doi.org/
10.1109/ICSE-SEIP.2019.00042

Baechle, M., Kirchberg, P.: Ruby on rails. Software, IEEE 24, 105 — 108 (12 2007).
https://doi.org/10.1109/MS.2007.176

Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C.,
Wirth, R.: CRISP-DM 1.0 step-by-step data mining guide. SPSS (01 2000)
Cohn, M.: Succeeding with Agile: Software Development Using Scrum. Pearson
Education Limited (2009), ISBN: 978-0-32166-056-5

Costello, K.: Gartner survey shows 37 percent of or-
ganizations have implemented ai in some form (Jan
2019), https: //www.gartner.com/en/newsroom/press-releases/

2019-01-21-gartner-survey-shows-37-percent-of-organizations-have,
accessed on 24.05.2021

Devlin, J.. Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapo-
lis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https: //www.
aclweb.org/anthology/N19-1423

Duvall, P., Matyas, S.M., Glover, A.: Continuous Integration: Improving Software
Quality and Reducing Risk. Addison-Wesley Professional (2007), ISBN: 0-32133-
638-0

Fayyad, U., Piatetsky-Shapiro, G., Smyth, P.: From data mining to knowledge
discovery in databases. AI magazine 17(3), 37 (1996)

Goasduff, L.: 3 barriers to ai adoption (Sep 2019), https://www.gartner.com/
smarterwithgartner/3-barriers-to-ai-adoption/, accessed on 24.05.2021
Jyoti, R., Shirer, M.: Idc survey finds artificial intelligence adoption being driven by
improved customer experience, greater employee efficiency, and accelerated innova-
tion (Jun 2020), https: //www.idc.com/getdoc. jsp?containerId=prUS46534820,
accessed on 24.05.2021

Li, J., Sun, A., Han, J., Li, C.: A survey on deep learning for named entity recog-
nition. IEEE Transactions on Knowledge and Data Engineering pp. 1-1 (2020).
https: //doi.org/10.1109/TKDE.2020.2981314

Liu, Se Global natural language processing market 2017-
2025 (Jun 2020), https: //www.statista.com/statistics/607891/
worldwide-natural-language-processing-market-revenues/, accessed on
24.05.2021

Mane, D., Chitnis, K., Ojha, N.: The spring framework: An open source java plat-
form for developing robust java applications. In: International Journal of Innovative
Technology and Exploring Engineering (IJITEE) (2013)

Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-
moyer, L.: Deep contextualized word representations. CoRR abs/1802.05365
(2018), http: //arxiv.org/abs/1802.05365

Pineau, J., Vincent-Lamarre, P., Sinha, K., Lariviére, V., Beygelzimer, A., d’Alché
Buc, F., Fox, E., Larochelle, H.: Improving reproducibility in machine learn-


17.

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.

28.

STAMP 4 NLP —- Agile NLP Application Development 11

ing research (a report from the neurips 2019 reproducibility program) (2020),
arXiv:2003.12206. Version 3.

Reuters: Amazon ditched ai recruiting tool that favored men for techni-
cal jobs (Oct 2018), https://www.theguardian.com/technology/2018/oct/10/
amazon-hiring-ai-gender-bias-recruiting-engine, accessed on 24.05.2021
Ribeiro, M.T., Wu, T., Guestrin, C., Singh, S.: Beyond accuracy: Behavioral testing
of NLP models with CheckList. In: Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics. pp. 4902-4912. Association for Com-
putational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-
main.442, https: //www.aclweb.org/anthology/2020.acl-main. 442

Schreiber, M., Kraft, B., Ziindorf, A.: Metrics driven research collaboration: Focus-
ing on common project goals continuously. In: 2017 IEEE/ACM 4th International
Workshop on Software Engineering Research and Industrial Practice (SER IP).
pp. 41-47 (2017). https: //doi.org/10.1109/SER-IP.2017..6

Schreiber, M.: Towards Effective Natural Language Application Development.
dissertation, University of Kassel (2019). https://doi.org/doi:10.17170/kobra-
20190529539

Schreiber, M., Kraft, B., Ziindorf, A.: Cost-efficient quality assurance of
natural language processing tools through continuous monitoring with con-
tinuous integration. In: Proceedings of the 3rd International Workshop on
Software Engineering Research and Industrial Practice. p. 46-52. SER&IP
16, Association for Computing Machinery, New York, NY, USA (2016).
https://doi.org/10.1145/2897022.2897029, https://doi.org/10.1145/2897022.
2897029

Sildatke, M., Karwanni, H., Kraft, B., Schmidts, O., Ziindorf, A.: Automated
software quality monitoring in research collaboration projects. In: Proceedings of
the IEEE/ACM 42nd International Conference on Software Engineering Work-
shops. p. 603-610. ICSEW’20, Association for Computing Machinery, New York,
NY, USA (2020). https://doi.org/10.1145/3387940.3391478, https://doi.org/
10.1145/3387940 .3391478

Staff, V.: Why do 87% of data science projects never make it
into production? (Jul 2019), https://venturebeat .com/2019/07/19/
why-do-87-of-data-science-projects-never-make-it-into-production/,
accessed on 24.05.2021

Studer, S., Bui, T.B., Drescher, C., Hanuschkin, A., Winkler, L., Peters, S., Mueller,
K.R.: Towards crisp-ml(q): A machine learning process model with quality assur-
ance methodology (2020), arXiv:2003.05155.

Varanasi, B., Belida, S.: Maven archetypes. In: Introducing Maven. Apress, Berke-
ley, CA (2014). https://doi.org/10.1007/978-1-4842-0841-0_6

Vijayan, V.K., Bindu, K.R., Parameswaran, L.: A comprehensive study of
text classification algorithms. In: 2017 International Conference on Advances in
Computing, Communications and Informatics (ICACCI). pp. 1109-1113 (2017).
https: //doi.org/10.1109/ICACCI.2017.8125990

Vincent, J.: Twitter taught microsoft’s ai chatbot to be a racist asshole in
less than a day (Mar 2016), https://www.theverge.com/2016/3/24/11297050/
tay-microsoft-chatbot-racist, accessed on 24.05.2021

Weber., C., Hirmer., P., Reimann., P., Schwarz., H.: A new process model
for the comprehensive management of machine learning models. In: Pro-
ceedings of the 21st International Conference on Enterprise Information
Systems - Volume 1: ICEIS,. pp. 415-422. INSTICC, SciTePress (2019).
https: //doi.org/10.5220/0007725304150422
