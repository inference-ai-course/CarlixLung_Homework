arX1v:2112.05780v1 [cs.CL] 7 Dec 2021

A Scoping Review of Publicly Available Language Tasks in Clinical Natural
Language Processing

Yanjun Gao, PhD!,; Dmitriy Dligach, PhD’; Leslie Christensen, MA-LIS*; Samuel Tesch’;
Ryan Laffin’; Dongfang Xu, PhD*, Timothy Miller, PhD*; Ozlem Uzuner, PhD”;
Matthew M. Churpek MD, MPH, PhD’; Majid Afshar, MD, MSCR!

1 ICU Data Science Lab, School of Medicine and Public Health,
University of Wisconsin, Madison, WI
{ygao, mchurpek, mafshar} @medicine.wisc.edu
2Department of Computer Science, Loyola University Chicago, Chicago, IL
ddligach @luc.edu
3 School of Medicine and Public Health, University of Wisconsin, Madison, WI
{leslie.christensen, sgtesch,rlaffin } @ wisc.edu
4 Boston Childrens Hospital, Harvard University, Boston, MA
{Dongfang.Xu, Timothy.Miller} @childrens.harvard.edu
5 Department of Information Sciences and Technology, George Mason University,
Fairfax, VA ouzuner@gmu.edu

ABSTRACT
Objective

To provide a scoping review of papers on clinical natural language processing (NLP) tasks that use publicly available
electronic health record data from a cohort of patients.

Materials and Methods

Our method followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.
We searched six databases, from biomedical research (e.g. Embase (Scopus)) to computer science literature database
(e.g. Association for Computational Linguistics (ACL)). A round of title/abstract screening and full-text screening
were conducted by two reviewers.

Results

A total of 35 papers with 47 clinical NLP tasks met inclusion criteria between 2007 and 2021. We categorized the tasks
by the type of NLP problems, including name entity recognition, entity linking, natural language inference, and other
NLP tasks. Some tasks were introduced with a topic of clinical decision support applications, such as substance abuse,
phenotyping, cohort selection for clinical trial. We also summarized the tasks by publication and dataset information.

Discussion

The breadth of clinical NLP tasks is still growing as the field of NLP evolves with advancements in language systems.
However, gaps exist in divergent interests between general domain NLP community and clinical informatics commu-
nity, and in generalizability of the data sources. We also identified issues in data selection and preparation including
the lack of time-sensitive data, and invalidity of problem size and evaluation.

Conclusion

The existing clinical NLP tasks cover a wide range of topics and the field will continue to grow and attract more
researchers from both general NLP domain and the clinical informatics communities. We encourage future work on


proposing tasks/shared tasks that incorporate multi-disciplinary collaboration, reporting transparency, and standard-
ization in data preparation.

INTRODUCTION

Since the inception of the first Integrating Biology and the Bedside (i2b2) shared task in 2006, currently known as
the National Natural Language Processing (NLP) Clinical Challenge (n2c2), the field of clinical NLP has advanced
in clinical applications that rely on text from the electronic health record (EHR). Tasks with publicly available data
(e.g. shared tasks) provide a new avenue for advancing the state-of-the-art using publicly available datasets in a sector
that is otherwise heavily regulated and protected from sharing patient data. In an editorial approximately a decade
ago, Chapman et al.|[I] identified the major barriers for clinical NLP developments where shared tasks may provide a
solution. At the time, some of the challenges were lack of data resources including annotation tools, benchmarking
and standardized metrics, reproducibility, collaboration between the general NLP communities and health research
communities, and the need for user-centered development.

Over the past decade, strides have been made with an increasing number and heterogeneity in clinical NLP tasks, and
many organizers are leveraging publicly available EHR notes like the Medical Information Mart for Intensive Care
(MIMC) [2]. MIMIC along with clinical notes from other health systems has overcome privacy and regulatory hurdles
to enable the growth of language tasks to address important clinical problems with NLP solutions. The benefits of
publicly available language tasks have become apparent with an opportunity for both clinical informatics and general
domain NLP communities to tackle problems together and develop systems that may translate into applied tools in
health systems. The body of language tasks continues to enable the growth with complex information extraction tasks
ranging from early diagnoses (e.g. substance abuse detection, phenotyping [34{5]) to clinical language understanding
(e.g. natural language inference (6) [7)).

However, several challenges remain as transparency in the methods, clinical motivation, and standardization across
annotation techniques and sample size determination remain highly variable. Our objective is to review the papers
on clinical NLP tasks that use publicly available EHR data from a cohort of patients. We aim to examine the
progress over the years and describe both barriers that we have overcome as well as challenges that remain in advancing
clinical NLP. This scoping review will serve as a resource for organizers and participants in the clinical NLP domain
to easily retrieve details on publicly available clinical tasks as well as identify gaps and opportunities for future tasks.
As a resource to the community, we provide a listing of all the publicly available tasks from this review for easy

referencd!]

METHODS

The methods to conduct this scoping review adhered to standards described in the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses guidelines for Scoping Reviews (PRISMA-ScR). The review format was
adapted to a population, intervention, comparator, and outcome (PICO) framework to establish the inclusion criteria
for articles. [9] The population consisted of a new language task for clinical NLP using publicly available EHRs from
a cohort of patients. The intervention includes expert annotations to build a labelled corpus of data. The comparator
is a test dataset with an evaluation metric, and the outcome is a challenge or published model that represents a state-
of-the-art (SOTA) for the task.

Literature Search

In adherence with the PICO framework, the librarian (LC) performed a full, systematic review of the literature be-
tween January 1985 and September 2021. The search combined controlled vocabulary and title/abstract terms related
to the shared language tasks in clinical NLP with a focus on publicly available datasets. The search was developed
in PubMed, tested against a set of exemplar articles, and then translated into the following databases: (1) Embase
(Scopus); (2) The Association for Computing Machinery (ACM) Guide to Computing Literature (ACM Digital Li-

‘A full list can be found at/https://git.doit.wisc.edu/Y GAO/public-available-clinical-nlp-tasks



brary); (3) Science Citation Index Expanded (Web of Science); (4) Conference Proceedings Citation Index-Science
(Web of Science); and (5) Emerging Sources Citation Index (Web of Science). The metadata from the Association for
Computational Linguistics (ACL) Anthology was downloaded separately and searched based on the database search
strategies. The search strategies were peer reviewed by two University of Wisconsin (UW)-Madison Science and En-
gineering librarians. All searches were performed on September 8, 2021 except for ACL, which was on September 1,
2021. No publication type, language, or date filters were applied. Results were downloaded to a citation management
software (EndNote x9, Clarivate Analytics, Philadelphia, PA) and underwent manual de-duplication by the librarian.
Unique records were uploaded to Rayyan screening platform for independent review. The full query with search
terms and Boolean operations for each database is detailed in Appendix A.

Study Selection

Study selection criteria were established a priori and included the following: (1) publicly available train/test dataset
from a cohort of patients; (2) NLP task; (3) novel benchmark metric; (4) models that were built and tested to establish
state-of-the-art results for the novel benchmark metric; and (5) English-language research articles and tasks. Articles
were excluded if the tasks were focused on the biomedical domain (genomics data, non-patient data, data from clinical
research databases including PubMed articles), subject-matter specific tasks without publicly available data, pre-prints
or non peer-reviewed, and individual use-case systems not designed as a shared task. Multiple papers shared a data
challenge with multiple tracks. For example, the 2014 i2b2/UTHealth shared task had two tracks, protected health
information (PHI) de-identification and temporal identification of risk factors for heart disease. We analyzed each
track as its own task, and some tasks consisted of multiple subtasks. If the subtask focused on a distinct clinical
problem, then we also considered each subtask as its own task. We excluded subtasks when the data were not clinical
text and it was not related to clinical NLP.

Review of titles and abstracts for inclusion into full-text article review was performed by researchers with expertise
in NLP and clinical informatics (YG and MA). The first 400 titles/abstracts were reviewed by the two reviewers in a
blinded fashion and a Cohens Kappa score for inter-annotator agreement was above 0.80. The subsequent papers were
divided and reviewed by each reviewer independently. Any disagreements or indeterminate decisions were resolved
through discussion and consensus.

Data Synthesis and Summarization

Among the papers included in the scoping review, characteristics of the shared tasks were described and the data corpus
metrics were summarized into Tables. The following characteristics were provided: (1) publication date and location;
(2) the type of NLP task and data source; (3) level of annotation; (4) participant details; (5) data corpus details; and
(6) evaluation metrics. Depending on where the task was published, we categorized each task as originating from
the Clinical Informatics or general domain NLP community. No critical appraisal of the literature was performed
because of the heterogeneity in tasks and evaluation metrics. We followed the guideline and checklist from the 2018
PRISMA-ScR (Appendix B).|8])

RESULTS

Search Results

Our search results identified 4,489 abstracts for review after de-duplication. After the first review phase with ti-
tle/abstract screening, 99 papers met inclusion criteria for full-text review. During the full-text review phase, 68 papers
were excluded and the most common reason for exclusion was not having publicly available data (n=24). During the
full-text review, another five papers were identified that were not part of the original query results. Thirty-five papers
spanning 47 clinical NLP tasks between 2007 and 2021 were ultimately included for analysis. Figure[I|illustrates the
selection process and results. All of the included papers were published in peer-reviewed clinical informatics (CI) and
general domain NLP journals and conference proceedings.


a a
ACM
PubMed —_e Digital as of Pp colt
(Scopus) Library cience nthology
f 7254 Papers
y Deduplication

f 4489 Papers

Met inclusion criteria for Full-Text Review

99 Full text
reviews

fF

Excluded: Data not publicly available (n=24);
No evaluation or baseline models (n=14); Not
about a task/shared task (n=10); Not about
Clinical NLP (n=9); Non-English (n=1);
Duplicate content (n=10)

Included: Manual identification (n=4)

35 Papers
Selected

Figure 1: PRISMA Diagram of our paper review process.
General Characteristics of Included Papers

The majority of tasks appeared in CI journals with the most frequently occurring in the Journal of the American
Medical Informatics Association (JAMIA; n=11) {11H21], the Journal of Biomedical Informatics (JBI; n=5) [22/26],
and the Journal of Medical Informatics (JMIR; n=2) [27]. The remaining papers were distributed across other
health/clinical informatics journals and conference proceedings, including Artificial Intelligence in Medicine [B],
Journal of Biomedical Semantics [28], Drug Safety [29], and conference proceedings from American Medical In-
formatics Association Symposium (AMIA, n=2) and World Congress on Medical and Health Informatics
(MEDINFO) [32]. In the general domain NLP community, the major proceedings included the Association of Com-
putational Linguistics (ACL, n=2) [33] 34], International Conference on Language Resources and Evaluation (LREC,
n=2) [4][35], and Empirical Methods in Natural Language Processing (EMNLP, n=2) [77] [36], International Conference
of the Cross-Language Evaluation Forum for European Language (or known as Conferences and Labs of Evaluation
Forum, CLEF, n=3) (37439], and International World Wide Web Conference (WWW) [40]. Some tasks were also pub-
lished in workshops such as the International Workshop on Semantic Evaluation (SemEval, n=2) [41] [42], Biomedical
Natural Language Processing Workshop (BioNLP, n=2) [6] [43], and Workshop on Natural Language Processing for
Medical Conversation [44]. One paper publishes in the journal of Language Resources and Evaluation [45].

The peer review process and target audiences between CI and NLP publications are considerably different. For this
reason, where the task is published indicates an interest rising from CI and the general NLP communities. Several
differences exist in the types of tasks shared between these two communities. Figure[illustrates the type of tasks and
counts between 2007 and 2021 from the two communities. Overall, 28 of the tasks were published by the CI com-
munity, and 19 tasks were published by the general domain NLP community. The earliest shared task was published
in a CI journal in 2007 (i2b2 Protected Health Information (PHI) De-Identification [11]), showing a longer history in
developing clinical NLP tasks among the CI community. Six years later, the NLP community published its first clinical
NLP task in the 2013 CLEF eHealth Task 2 Disorder Mention [38]. Interests from the general domain NLP community
have been increasing across the years, representing the majority of shared tasks in 2021 (Summarization [43], Action
Item Extraction [33], Assertion Detection [44]).


@ Tasks in Clinical Informatics Journals/Conferences

2007
2008
2010
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021

NER
0 1 2

DocClass

Coref, IE
NER, IE
NER, EL
NER, SR

2007

2008

IE(n=3), 2010

SentClass, 2012

2013

2014

2015

2016

NER(n=2), DocClass, 2017
DocClass, 2018

IE, STS, 2019

DocClass, EL 2020
IE, NER 2021

3 4

NER: Name Entity Recognition
DocClass: Document Classification
IE: Information Extraction
SentClass: Sentence Classification
STS: Semantic Textual Similarity

Coref: Coreference Resolution
EL: Entity Linking

SR: Speech Recognition

LN: Lexicon Normalization
Summ: Summarization

® Tasks in NLP Conferences/Journals

NER(n=2),
Parsing(n=2)

MRC,

QA, NLI
DocClass,
Text2SQL SentClass
(n=2),
Summ
1 2 3 4

Parsing: Syntactic Parsing

MRC: Machine Reading Comprehension
QA: Question Answering

NLI: Natural Language Inference
Text2SQL: Text to SQL

Figure 2: Types of tasks and community interests across years.

Name Entity Recognition (NER) represents nearly a quarter of all tasks (23.40%, n=11) with 28.57% (n=8) and
15.79% (n=3) in CI and general domain NLP papers [4] [38] [45], respectively. Other tasks
that occurred frequently in CI were Information Extraction (IE; n=7) (15}{17] [20] 32) [37] [42], Document Classification
(DocClass; n=5) [26]. In the general domain NLP community, the types of tasks were distributed
relatively evenly across Entity Linking (EL; n=2) [38] 41], Syntactic Parsing (Parsing; n=2) (4\/45], Natural Language
Inference (NLI; n=2) [(6][7]], and Sentence Classification (SentClass; n=2) (31) (33). Tasks required text understanding
and generation are proposed by general NLP community, such as Machine Reading Comprehension (MRC) [34],
Summarization (Summ) and Question Answering (QA) [36].

Descriptions of Included Tasks and Data

The characteristics of the tasks are shown in Table[I] We found that 38% of the NLP tasks are introduced with an
impact on clinical decision making. Most of the clinical impacts are under the category of NER task, introducing
detection and identification of various medical conditions [18], substance abuse [3], medical risk factors [22], medical
events [17] [20], and PHI de-identification [25]. Phenotyping introduced a corpus annotated with NER
without identifying a specific clinical purpose. DocClass is the second most frequent NLP task after NER (12.76%,
n=6), covering clinical decision support applications from symptom severity [24], obesity and comorbidity phenotyp-
ing [13], smoking status [12], to cohort selection for clinical trials [21]. Inconsistencies in defining NLP tasks occurred
with two papers that described phenotyping as an NER task and others described it as a document classi-
fication task. Tasks without specific clinical applications were NLI [(6][7], MRC [84], QA (B6], Summ [43], STS [27],
Coref [16], Parsing [4] [45], Text2SQL and SR [39]. Most of these tasks were introduced by the general domain
NLP community, except STS and Coref [16].

The data sources used to build the corpora were frequently derived from single health systems. Among them, the
most frequent was from MIMIC [2], which is from a large tertiary academic center in Boston and represented 31.91%
(n=15) of the tasks [6] [44]. Other urban and academic health systems
also contributed by releasing their data in a de-identified format including the following: Partners HealthCare (PHC,
n=8) (1 1414] [17] {19} (24) (25) [34]; Beth Israel Deaconess Medical Center (BIDMC, n=7) [15] {17|[19] 34]; University of
Pittsburgh Medical Center (UPMC, n=5) [[15)[16]34]]; University of Texas Health System (UTHealth, n=4) [15] [161/34];


Mayo Clinic (Mayo, n=3) [42]; and University of Washington Harborview Medical Center (UW Harborview,
n=3) [4] [26]. All these data sources represent single centers that are tertiary academic medical centers.

Several papers remain general in describing the note types as “EMR” or “EHR” without further specifying the type of
note (e.g., progress note, discharge summary, radiology report, etc.). For those papers, we denoted the type as “clinical
notes” (n=14) [21}23] [25] [27] [29] [32] [34] [41] [42] [45]. Other papers gave clear specifications and discharge summaries
were the most frequent note type (36.71%. n=17) [44], followed by radiology
reports (14.89%, n=7) (28][31)[37)[43]. Other note types included history and physical admission (H&P), daily progress
notes, electrocardiogram, echocardiogram, pathology reports, and psychiatric evaluation records [5}[7} {24} (28) (35) [37].
Text2SQL and SR had note types that were different from all other tasks. Text2SQL included structured data because
their goal was to convert the tabular data into SQL language [40]. SR included audio records from nursing handover
sessions with the intent of developing written text from spoken language [39].

We found that more than a half of the tasks used data annotated at the lexical level (51.06%, n=24)
[45]. For these tasks, lexical features served as the basis for assigning labels. Tasks like
NLI [6] {7], STS [27], Parsing and SentClass had annotations at the sentence level. Document
level annotation were created for tasks in DocClass [12] [13] [21] [24] [26] 35), MRC [B4], QA and Summ [43].

Descriptions of Task Participation, Data Size, and Evaluation

Details on participants in the shared tasks along with data metrics are shown in Table 22} however, some papers did
not report the participants. Among all tasks where participation information is available, the number of participants
ranged from 5 teams to 35 teams. Summ was only hosted once but published in 2021 as the task with the greatest
number of teams (n=35) submitting their systems [43]. STS, published in 2020, is the second most popular task and
attracted 33 teams [27]. DocClass ranked the third in number of team participants, with an average of 29.33 teams
across three shared tasks [12] {13]/21] [24].

Sample sizes across the labels were highly variable and ranged from a few hundred manually annotated labels to semi-
automated methods that produced several-fold more labels. None of the papers justified their sample size and they
were simply reported as convenience samples. Further, not all tasks reported their data splits in the papers [4] [17] BT].
For example, we only found one SentClass task with data split information readily available [33]. The units of dataset
size were also heterogeneous, and sometimes not consistent with the annotation. For instance, annotation for EL tasks
were created at lexicon level, yet few papers reported the size regarding number of words and tags [19]. The biggest
corpora task was QA, using the emrQA dataset [36]. Their annotation was generated semi-automatically on all i2b2
data. The emrQA was also used in an MRC task with a subtle difference in data split [34]. NLI also had large sets of
sentence pairs, ranging between 11,000 and 14,000 for the train set, and 405 to 1,400 for the test set (6) [7].

Accuracy and F1 were the two most frequent evaluation metrics. Most tasks used one of the two metrics focused on
evaluating if predicted labels were correct against a gold standard, such as EL, NER, etc. Most tasks in DocClass ap-
plied the F1 score to evaluate the predicted labels, with the exception of which reported the Mean Absolute Error.
Some tasks used metrics that were more common in general NLP domain, such as ROUGE and BERTScore
for summarization evaluation [43]; Pearson Correlation for STS task [27]. Tasks in parsing used F1 as well as Un-
labeled Attachment Score (UAS) and Labeled Attachment Score (LAS), two metrics that evaluated if the predicted
parsing tags were correct against human-generated parsed labels [4] {45}.

Table 1: Overview of tasks and datasets, including the data source and types, and annotation units

Impact on Clinical Data Note Annotation
De a Decision Making Source Type Unit
Entity General Non-specific MIMIC(n=3), VA Salt discharge summaries, Lexicon
Linking Lake City Health progress notes
Natural Language General Non-specific [6][7] MIMIC(n=2) history and Sentence



Inference physical admission Pairs
Lexical Normalization General Non-specific MIMIC discharge summaries, Lexicon
electrocardiogram
echocardiogram,
radiology report
Machine Reading General Non-specific PHC"| UPM' clinical note: Document
Comprehension UTHealth, BIDMq)|
Question Answering General Non-specific PHC, BIDMC, MIMIC clinical notes Document
Summarization General Non-specific MIMIC radiology reports Document
Name Entity Phenotyping UMass Memorial discharge summaries Lexicon
Recognition Disorder Detection [18] [38] Health, PHC(n=2), (n=2), radiology
Risk Factor Identification |22) (22), MTSample, BIDMC, reports, history and
PHI De-identification [11][23][25] | MIMIC, UW physical admission,
Substance Abuse Detection Harboview, Mov | clinical notes (n=6)
UK National Healt
General Non-specific UTHealth(n=2)
Information Extraction Disorder Mention PHC(n=2), BIDMC(n=2) | discharge summaries Lexicon
General Non-specific UPMC(n=2), MIMIC (n=6), radiology
(Time) (n=2), Mayo, report, electrocar-
(Concept) UK National Health diogram, clinical notes
Semantic Textual General Non-specific Mayo clinical notes Sentence
Similarity Pairs
Co-reference Resolution | General Non-specific PHC, BIDMC, discharge summaries Lexicon
UPMC
Syntactic Parsing General Non-specific UK National Health, radiology report, Sentence
UW HarborView clinical notes
Sentence Action Item Extraction MIMIC(n=2), discharge summaries Sentence
Classification General Non-specific Source Not Specified (n=2),
(Assertion)
(Negation)
Document Symptom Severity PHC(n=3), MIMIC discharge summaries, Document
Classification Prediction (n=2), UTHealth, (n=4), progress
Phenotyping UW Harborview, notes, clinical notes,
Cohort Selection for Clinical Harvard Medical psychiatric evaluation
Trial School records
Obesity Classification
Smoking Status
Classification
Others (Text2SQL, General Non-Specific MIMIC, NCTAY] nursing handover Various
Speech Recognition) data, structured
data in EHR
Table 2: Overview of tasks, average number of participants across years, years range for publications and evaluation metrics
Avg. Number of | Publication Data Split Range Evaluation
AEP ae Participant¢'| Year: Training Test Metric
Entity Linking 25.33 (n=4) — 2014-2020 50-199 Notes 50-133 Notes Acc.

2Partners HealthCare, rebranded as MassGeneralBrigham

3University of Pittsburgh M

edical Center

4We use “clinical notes” to mark the data type when the paper did not specify the note type.
>Beth Israel Deaconess Medical Center

©Mayo Clinic

7Nursing Care Team Assistant
8We count the number of participants on team-basis with n representing the number of papers when the task is a shared task or open data

challenge.

°We report the range of years for the included papers


Natural Language 17 (n=1) 2018-2019 11k-14k 405-1.4k Acc.
Inference Pairs Pairs
Lexical Normalization 5 (n=1) 2016 199 Notes 99 Notes Acc.
Machine Reading NA 2020 91k 9.9k Exact Match, Fl
Comprehension Queries Queries
Question Answering NA 2018-2021 658k-1M Pairs | 188k-296K Pairs Acc.
Summarization 35 (n=1) 2021 91k 600 ROUGE, HOLMS,
Notes Notes BERTScore, CheXBert
Name Entity 15.57 (n=7) 2007-2021 99-3.1k 117-896 Fl, Acc.
Recognition Notes Notes
Information Extraction 17.43 (n=7) 2011-2019 300-876 Notes 100-574 Notes Acc., Fl
Semantic Textual 33 (n=1) 2020 1.6k Pairs 412 Pairs Pearson Correlation
Similarity
Co-reference Resolution | 20 (n=1) 2012 590 Notes 388 Notes Fl
Syntactic Parsing NA 2016 NA NA F1, UAS, LAq"]
Sentence NA 2011-2021 518 Notes 100 Notes Fl
Classification
Document 29.33 (n=3) 2008-2020 202-11k Notes | 86-8k Notes F1, Inversed
Classification Normalized Macro-avg
Mean Absolute
Error (MAE)
Others (Text2SQL, 11.67 (n=2) 2013-2020 Text2SQL: 37k | Text2SQL: 4k Error Rate Percentage,
Speech Recognition) Records Records Acc.
SR: 100 Cases SR: 100 Cases

DISCUSSION

Lessons Learned from Past Community Interests and Efforts

Our scoping review identified a total of 35 papers spanning multiple NLP tasks across both clinical informatics and
general domain NLP communities. Among the oldest and most frequent tasks across both communities were NER
tasks, which require systems built at the lexicon level. A shift from lexicon level tasks (NER, EL, etc) to document
level tasks (MRC, QA, Summ,etc) was observed across the years with a growing interest for language understanding
and text generation problems. As the NLP field continues to evolve since the introduction of transformers and
capacity to build large pre-trained neural language systems [49] [50], the breadth of tasks are expected to grow. In more
recent years, the general domain NLP field has contributed natural language understanding and generation tasks
[53], but the CI domain remains largely focused in NER and document classification [24]. This may
represent a divergence in interest and lessons learned from the general domain community but can also inform how
the clinical NLP domain can expand its scope in tasks and potentially lead to more innovative solutions in clinical
applications.

The first publicly available task appeared in a CI journal by clinical NLP experts working at health systems affiliated
with academia [11]. Several reasons for the earlier appearance by the CI community may include the difficulties in
extracting clinical notes and privacy laws protecting patient data for sharing, which requires individuals with direct
access to the EHR. The CI community of NLP experts brings together similar computational linguistic knowledge but
they collaborate with healthcare providers to tackle the linguistic challenges in EHR data with a better understanding
of the medical terms and clinical problems. This is also reflected in the longer history of shared tasks with a focus
into a clinical problem (e.g., deidentification [11] [23] [25], clinical trial recruitment [21], etc). Tasks organizers from
the general domain NLP community focused more on fundamental tasks like Parsing [45], NLI (6) [7]. Most re-
searchers in the general domain NLP community derived from a non-clinical computational background with different
motivations to develop cutting-edge technologies with less exposure to the clinical needs of health systems. Majority

!OUAS: Unlabeled Attachment Score, LAS: Labeled Attachment Score


of papers from the general domain NLP community were not directly associated with clinical applications. Rather, the
motivations were to build a foundation for better clinical text understanding, and ultimately facilitate model develop-
ment for downstream tasks and clinical applications. To further advance the field of clinical NLP, both communities
may benefit from a foundational NLP task directed by computational linguistic experts underpinning a major clinical
challenge to ultimately improve clinical decision support or health outcomes.

Another major gap identified from this scoping review were the limitations in generalizability of the data sources.
All the data are relatively homogeneous, deriving from mainly large, urban tertiary academic centers and mainly
from single centers with a biased representation of the US population. Further, the notes derived from academic
centers also contain a large proportion of notes in the EHR documented by trainees and may not be representative of
community hospitals and health systems without trainees and the additional note types derived from them. The current
environment with HIPAA privacy laws and resources for an Enterprise Data Warehouse largely limit the availability of
data to centers. Currently, centers with informatics and computational expertise and resources support the data needed
for public tasks and these remain limited to well-resourced academic centers. Moving forward, a concerted effort
should be placed into sourcing data across geographic regions and hospital types that serve a diversity of patients and
encompass various documentation practices.

Selection of Notes and Benchmarks

The discharge summary is the most frequent note type and typically the most detailed about the hospital events and final
diagnoses and treatments provided. While these may be useful for accomplishing certain NLP tasks, their clinical ap-
plication in real-time remain limited. Augmented intelligence via clinical decision supports systems frequently ingest
data as events happen or use note types with time-sensitive appearance or repeated measures. Discharge summaries
are typically the last documentation to resolve what happened during a hospital stay and may not be useful for aug-
mented decision making. Other note types such as radiology and emergency department notes that are time-sensitive
or daily progress notes that track disease and treatment plans each day are potentially more useful for real-time NLP
applications, which is a goal for many researchers in the field.

F1 scores and overall accuracy are the most frequently used evaluation metrics, but they are only one component in
reliability and validity testing. The extent to which a system measures what it is intended to measure requires multiple
validity metrics. Criterion validity metrics with accuracy and correlation scores against reference standards are the
de facto standard in tasks. However, construct and content validity are also important. Construct validity is needed
when no universally accepted criterion exists to support the concept (or construct) being measured. This may require
human evaluation to provide more than just frequentist statistics and better report benchmarks for natural language
understanding and generation tasks. Content validity (or face validity), the extent to which the lexicons identified
by the system are representative of the concepts the system seeks to measure require more sophisticated approaches
that can evaluate semantics and word order like the BERTScore [47]. In the clinical domain, meeting all the validity
metrics may still not be enough. Pragmatic testing through clinical applications with practice simulations that examine
the system’s effectiveness should also be considered in future tasks.

Issues in Task/Data Preparation

Introducing, preparing and releasing data for a new task requires complex thoughts and actions, yet details on data
preparation are often neglected. From the included papers, we identify some issues above that may help to improve
future task presentation. Additional issues were inconsistencies reporting data split, and a small amount of papers
presented results from pre-trained models without explaining the training set which hinders reproducibility. We also
found that the data split sizes reported for most papers did not match with the annotation units. The inconsistency
between annotation units and data split unit obfuscates the real problem size. Finally, none of the papers reported how
they determined the minimum size of annotations needed to adequately train a model. Recall that even within the same
type of tasks, the data size could range substantially from hundreds to thousands (e.g. DocClass). Although it is widely
known that annotations are limited to the resources (time, budget, etc), not knowing the minimum sample size raises
a crucial question about result reliability issue: will the model performance trained on this dataset be robust enough


and trust-worthy? Models developed for tasks like NLI, MRC and QA are data hungry and should be determined a
priori, as these tasks require deeper understanding in semantics and relations. We believe by addressing these issues,
researchers could make stronger contributions to Clinical NLP.

CONCLUSION

The interests in introducing and participating in clinical NLP tasks are growing with more tasks surfacing each year.
The breadth of tasks is also growing with topics varying from tasks with specific clinical applications to those facil-
itating clinical language understanding and reasoning. It is no doubt that the field will continue to grow and attract
more researchers from both general NLP domain and the clinical informatics community. We encourage future work
on proposing tasks/shared tasks to overcome barriers in community collaboration, reporting transparency, and consis-
tency of data preparation.

ACKNOWLEDGING We thank David Bloom (UW-Madison) and Anne Glorioso (UW-Madison) for their review
of the search query as computer science librarians.

References

1. Chapman WW, Nadkarni PM, Hirschman L, D’avolio LW, Savova GK, Uzuner O. Overcoming barriers to NLP
for clinical text: the role of shared tasks and the need for additional creative solutions. BMJ Group BMA House,
Tavistock Square, London, WC1H 9JR; 2011.

2. Johnson AE, Pollard TJ, Shen L, Li-Wei HL, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible critical
care database. Scientific data. 2016;3(1):1-9.

3. Yetisgen M, Vanderwende L. Automatic identification of substance abuse from social history in clinical text. In:
Conference on Artificial Intelligence in Medicine in Europe. Springer; 2017. p. 171-81.

4. Klassen P, Xia F, Yetisgen-Yildiz M. Annotating and detecting medical events in clinical notes. In: Proceedings
of the Tenth International Conference on Language Resources and Evaluation (LREC’ 16); 2016. p. 3417-21.

5. Shen F, Liu S, Fu S, Wang Y, Henry S, Uzuner O, et al. Family History Extraction From Synthetic Clinical Narra-
tives Using Natural Language Processing: Overview and Evaluation of a Challenge Data Set and Solutions for the
2019 National NLP Clinical Challenges (n2c2)/Open Health Natural Language Processing (OHNLP) Competition
[Journal Article]. JMIR Med Inform. 2021;9(1):e24008.

6. Abacha AB, Shivade C, Demner-Fushman D. Overview of the mediga 2019 shared task on textual inference,
question entailment and question answering. In: Proceedings of the 18th BioNLP Workshop and Shared Task;
2019. p. 370-9.

7. Romanov A, Shivade C. Lessons from Natural Language Inference in the Clinical Domain. In: Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing; 2018. p. 1586-96.

8. Tricco AC, Lillie E, Zarin W, O’Brien KK, Colquhoun H, Levac D, et al. PRISMA extension for scoping reviews
(PRISMA-ScR): checklist and explanation. Annals of internal medicine. 2018;169(7):467-73.

9. Schardt C, Adams MB, Owens T, Keitz S, Fontelo P. Utilization of the PICO framework to improve searching
PubMed for clinical questions. BMC medical informatics and decision making. 2007;7(1):1-6.

10. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyana web and mobile app for systematic reviews.
Systematic reviews. 2016;5(1):1-10.

11. Uzuner O, Luo Y, Szolovits P. Evaluating the state-of-the-art in automatic de-identification. Journal of the
American Medical Informatics Association. 2007;14(5):550-63.

12. Uzuner O, Goldstein I, Luo Y, Kohane I. Identifying patient smoking status from medical discharge records [Jour-
nal Article]. JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION. 2008;15(1):14-
24. Available from: https://www.ncbi.nlm.nih. gov/pmce/articles/PMC2274873/pdt/14.S 106750270700269 | main.

13. Uzuner O. Recognizing obesity and comorbidities in sparse data [Journal Article]. Journal of the American Med-
ical Informatics Association. 2009;16(BMJ Group BMA House, Tavistock Square, London, WC1H 9JR):561-70.

14. Uzuner O, Solti I, Cadag E. Extracting medication information from clinical text [Journal Article]. JOURNAL
OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION. 2010;17(5):514-8. Available from:
//www.ncebi.nim.nih.gov/pmc/articles/PMC2995677/pdf/amiajn13947.pd


15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

Uzuner , South BR, Shen S, DuVall SL. 2010 i12b2/VA challenge on concepts, assertions, and relations in clinical
text [Journal Article]. Journal of the American Medical Informatics Association. 2011;18(5):552-6. Available
from: |https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3 168320/pdf/amiajnl-201 1-000203.pdf
Uzuner O, Bodnari A, Shen S, Forbush T, Pestian J, South BR. Evaluating the state of the art in coreference
resolution for electronic medical records [Journal Article]. J Am Med Inform Assoc. 2012;19(5):786-91. Available
from: |https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3422835/pdf/amiajnl-201 1-000784. pdf
Sun W, Rumshisky A, Uzuner O. Evaluating temporal relations in clinical text: 2012 i2b2 Challenge [Journal
Article]. Journal of the American Medical Informatics Association. 2013;20(5):806-13. Available from:
//www.ncbi.nlm.nih.gov/pmc/articles/PMC3756273/pdf/amiajnl-2013-001628.pd

Pradhan S, Elhadad N, South BR, Martinez D, Christensen L, Vogel A, et al. Evaluating the state of the art in
disorder recognition and normalization of the clinical narrative [Journal Article]. JOURNAL OF THE AMERI-
CAN MEDICAL INFORMATICS ASSOCIATION. 2015;22(1):143-54. Available from:
Henry S, Wang Y, Shen F, Uzuner O. The 2019 national natural language processing (NLP) clinical challenges
(n2c2)/Open health NLP (OHNLP) shared task on clinical concept normalization for clinical records [Journal
Article]. Journal of the American Medical Informatics Association. 2020;27(10):1529-37. Available from:
//www.ncbi.nim.nih.gov/pmc/articles/PMC7647359/pdf/ocaa106.pdf

Henry S, Buchan K, Filannino M, Stubbs A, Uzuner O. 2018 n2c2 shared task on adverse drug events and
medication extraction in electronic health records. Journal of the American Medical Informatics Association.
2020;27(1):3-12.

Stubbs A, Filannino M, Soysal E, Henry S, Uzuner O. Cohort selection for clinical trials: n2c2 2018 shared
task track 1 [Journal Article]. JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION.
2019;26(11):1163-71. Availabe from:
Stubbs A, Kotfila C, Xu H, Uzuner O. Identifying risk factors for heart disease over time: Overview of
2014 i2b2/UTHealth shared task Track 2 [Journal Article]. JOURNAL OF BIOMEDICAL INFORMATICS.

2015;58:S67-77. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978 189/pdf/nihms806521.

Stubbs A, Uzuner O. Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth
corpus [Journal Article]. JOURNAL OF BIOMEDICAL INFORMATICS. 2015;58:S20-9. Available from:
//www.ncebi.nlm.nih.gov/pmc/articles/PMC4978 1 70/pdf/nihms806528.pd

Filannino M, Stubbs A, Uzuner O. Symptom severity prediction from neuropsychiatric clinical records: Overview
of 2016 CEGS N-GRID shared tasks Track 2 [Journal Article]. JOURNAL OF BIOMEDICAL INFORMATICS.
2017;75:S62-70.

Stubbs A, Filannino M, Uzuner O. De-identification of psychiatric intake records: Overview of 2016 CEGS N-
GRID shared tasks Track 1 [Journal Article]. JOURNAL OF BIOMEDICAL INFORMATICS. 2017;75:S4-S18.
Lybarger K, Ostendorf M, Yetisgen M. Annotating social determinants of health using active learning, and char-
acterizing determinants using neural event extraction [Journal Article]. J Biomed Inform. 2021;113:103631.
Wang Y, Fu S, Shen F, Henry S, Uzuner O, Liu H. The 2019 n2c2/OHNLP Track on Clinical Semantic Textual
Similarity: Overview [Journal Article]. JMIR Med Inform. 2020;8(11):e23375.

Mowery DL, South BR, Christensen L, Leng J, Peltonen LM, Salanter S, et al. Normalizing acronyms and
abbreviations to aid patient understanding of clinical texts: ShARe/CLEF eHealth Challenge 2013, Task 2 [Journal

Article]. Journal of biomedical semantics. 2016;7:43. Available from: |https://www.ncbi.nlm.nih.gov/pmc/articles/
PMC4930590/pdf/13326_2016_Article_84.pd

Jagannatha A, Liu EF, Liu W, Yu H. Overview of the First Natural Language Processing Challenge for Extract-
ing Medication, Indication, and Adverse Drug Events from Electronic Health Record Notes (MADE 1.0) [Jour-

nal Article]. Drug Safety. 2019;42(1):99-111. Available from: https://link.springer.com/content/pdf/10.1007/
$40264-018-0762-z.pd

Uzuner O. Second i2b2 workshop on natural language processing challenges for clinical records. In: AMIA...
Annual Symposium proceedings. AMIA Symposium; 2008. p. 1252-3.

Peng Y, Wang X, Lu L, Bagheri M, Summers R, Lu Z. NegBio: a high-performance tool for negation and
uncertainty detection in radiology reports [Journal Article]. AMIA Jt Summits Transl Sci Proc. 2018;2017:188-



32.

33.

34.

35.

36.

37.

38.

39.

40.

41.

42.
43.

45.

46.

47.

48.

49.

50.

96. Available from: |https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5961822/pdf/2839 173.pdf

Viani N, Kam J, Yin L, Verma S, Stewart R, Patel R, et al. Annotating Temporal Relations to Determine the Onset
of Psychosis Symptoms. In: MedInfo; 2019. p. 418-22.

Mullenbach J, Pruksachatkun Y, Adler S, Seale J, Swartz J, McKelvey G, et al. CLIP: A Dataset for Extracting
Action Items for Physicians from Hospital Discharge Notes. In: Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Online: Association for Computational Linguistics; 2021. p. 1365-78.
Available from:

Yue X, Gutierrez BJ, Sun H, Assoc Computat L. Clinical Reading Comprehension: A Thorough Analysis of the
emrQA Dataset; 2020.

Moseley ET, Wu JT, Welt J, Foote J, Tyler PD, Grant DW, et al. A Corpus for Detecting High-Context Medical
Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients. In: Proceedings of
the 12th Language Resources and Evaluation Conference. Marseille, France: European Language Resources
Association; 2020. p. 1362-7. Available from:

Pampari A, Raghavan P, Liang J, Peng J. emrQA: A Large Corpus for Question Answering on Electronic Medical
Records. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing; 2018.
p. 2357-68.

Kelly L, Goeuriot L, Suominen H, Schreck T, Leroy G, Mowery DL, et al. Overview of the share/clef ehealth eval-
uation lab 2014. In: International Conference of the Cross-Language Evaluation Forum for European Languages.
Springer; 2014. p. 172-91.

Suominen H, Salantera S$, Velupillai S, Chapman WW, Savova G, Elhadad N, et al. Overview of the ShARe/CLEF
eHealth evaluation lab 2013. In: International Conference of the Cross-Language Evaluation Forum for European
Languages. Springer; 2013. p. 212-31.

Goeuriot L, Kelly L, Suominen H, Hanlen L, Neveol A, Grouin C, et al. Overview of the CLEF eHealth Evaluation
Lab 2015. vol. 9283; 2015.

Wang P, Shi T, Reddy CD, Assoc Comp M. Text-to-SQL Generation for Question Answering on Electronic
Medical Records; 2020.

Pradhan S, Chapman W, Man S, Savova G. Semeval-2014 task 7: Analysis of clinical text. In: Proc. of the 8th
International Workshop on Semantic Evaluation (SemEval 2014. Citeseer; 2014. .

Bethard S, Savova G, Palmer M, et al. SemEval-2017 Task 12. Clinical TempEval. 2017.

Abacha AB, Mrabet Y, Zhang Y, Shivade C, Langlotz C, Demner-Fushman D. Overview of the mediga 2021
shared task on summarization in the medical domain. In: Proceedings of the 20th Workshop on Biomedical
Language Processing; 2021. p. 74-85.

. van Aken B, Trajanovska I, Siu A, Mayrdorfer M, Budde K, Léser A. Assertion Detection in Clinical Notes: Med-

ical Language Models to the Rescue? In: Proceedings of the Second Workshop on Natural Language Processing
for Medical Conversations; 2021. p. 35-40.

Savkov A, Carroll J, Koeling R, Cassell J. Annotating patient clinical records with syntactic chunks and named
entities: the Harvey Corpus [Journal Article]. ates Resour Eval. 2016;50(3):523548. Available from:

Lin CY. Rouge: A package for automatic evaluation of summaries. In: Text summarization branches out; 2004.
p. 74-81.

Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. BERTScore: Evaluating Text Generation with BERT. In:
International Conference on Learning Representations; 2019. .

Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Advances
in neural information processing systems; 2017. p. 5998-6008.

Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers); 2019. p. 4171-
86.

Beltagy I, Lo K, Cohan A. SciBERT: A Pretrained Language Model for Scientific Text. In: Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-


51.
D2:

33%

ence on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for Computational
Linguistics; 2019. p. 3615-20. Available from:

Radford A, Narasimhan K. Improving Language Understanding by Generative Pre-Training; 2018. .

Roberts A, Raffel C, Shazeer N. How Much Knowledge Can You Pack into the Parameters of a Language Model?
In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP); 2020.
p. 5418-26.

Zhang J, Zhao Y, Saleh M, Liu P. Pegasus: Pre-training with extracted gap-sentences for abstractive summariza-
tion. In: International Conference on Machine Learning. PMLR; 2020. p. 11328-39.
