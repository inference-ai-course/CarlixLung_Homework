2310.01603v1 [cs.CL] 2 Oct 2023

arXiv

A Review of Digital Learning Environments for Teaching Natural Language

Processing in K-12 Education

XIAOYI TIAN, University of Florida, USA
KRISTY ELIZABETH BOYER, University of Florida, USA

Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence
(AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering
their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a
comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning
tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in
educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state
of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore

more effective and inclusive strategies for integrating NLP into K-12 educational contexts.

Additional Key Words and Phrases: literature review, natural language processing, artificial intelligence, learning environment, k-12,
children

ACM Reference Format:
Xiaoyi Tian and Kristy Elizabeth Boyer. 2023. A Review of Digital Learning Environments for Teaching Natural Language Processing
in K-12 Education. 1, 1 (October 2023), 24 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION

Natural Language Processing (NLP) is integrated into our everyday lives ubiquitously, with applications including spell
checkers, machine translation, recommendation systems and conversational agents. To ensure that citizens, including
young people, become responsible users and creators of intelligent solutions, there has been increasing attention
towards incorporating artificial intelligence (AI) curricula, including NLP, into 21st century computing education
[11, 46, 54]. Children are growing up with NLP-powered applications, making them ideal learning platforms. For
example, conversational agents have been demonstrated to enhance engagement in reading [57], foster language
learning [16, 56] and promote story comprehension and engagement [55].

Despite the prevalence of NLP in young people’s lives, there is limited research on teaching them NLP concepts.
Traditionally, AI and NLP concepts have been taught primarily in higher education [3, 32, 40]. However, research
indicates that children are capable of grasping AI and NLP concepts from a young age [10, 20]. Exposure to such
knowledge is crucial for young people to understand how language is processed and generated, how and where NLP
systems can be applied, and the social, economical and ethical challenges arising from AI and NLP use [31]. Fostering
Al literacy can inspire more students to consider careers in AI and NLP, laying a strong foundation for their higher
education and professional lives [28].

There have been efforts to establish frameworks for AI and NLP education at the K-12 level. Touretzky et al. [46]
founded the AI4K12 initiative, which proposed five K-12 AI guidelines for what AI concepts should every child know:

Authors’ addresses: Xiaoyi Tian, tianx@ufl.edu, University of Florida, USA; Kristy Elizabeth Boyer, keboyer@ufl.edu, University of Florida, USA.

© 2018


2 Xiaoyi Tian and Kristy Elizabeth Boyer

1) the perception and interpretation of complex information by AI systems, 2) their representation and reasoning
capabilities, 3) the ability of AI systems to learn and improve, 4) their capacity to make natural interaction with humans,
and 5) the broader societal implications and ethical considerations of AI. These “big ideas” provide a comprehensive yet
succinct roadmap to navigate the landscape of AI education.

In this landscape, NLP emerges as a crucial element in AI education due to its role in facilitating machines’ un-
derstanding, interpretation and generation of human language [21, 47]. The AI4K12 big ideas highlight many NLP
tasks and applications for children to grasp. According to Touretzky et al. [46], students should understand basic NLP
concepts such as speech recognition (big idea #1), word embeddings (big idea #2), parsing (big idea #3), text generation
and sentiment analysis (big idea #4), as well as ethical concerns related to NLP applications (big idea #5). To transform
students from AI consumers into AI creators, it is essential for learning environments to enable students to have
authentic, hands-on learning experiences [34, 42]. Such experiences can be facilitated through relatable NLP tasks that
simulate real-world applications, such as creating personalized chatbots and exploring sentiment analysis models.

Learning NLP through developing real-world NLP-powered applications is challenging for novice learners, as many
NLP tasks involve complex algorithms and require a high level of programming expertise. Common NLP tasks include
sentiment analysis, text classification, topic modeling, machine translation, speech recognition, text-to-speech, part-
of-speech tagging, and question answering [22]. Typically, these tasks are executed using text-based programming
languages (e.g., Python) and on platforms that require substantial computing resources (e.g., Tensorflow). Text-based
programming interfaces and resource intensity might not align with the developmental capabilities of children or the
resources of learning contexts, posing potential barriers to accessibility and comprehension [14]. Thus, there is a critical
need for intuitive and age-appropriate tools for introducing NLP concepts to younger learners.

Some digital learning environments aim to democratize AI by allowing novice users to create programs without
extensive programming skills or substantial computing resources [62]. These learning environments, such as Teachable
Machine [8], AISpace2 [58] and SmileyCluster [53] help reduce users’ cognitive load and allow users to focus on the
core AI concepts without having to deal with syntax issues or external libraries, making them useful tools for K-12
learners.

Inspired by the success of these digital learning environments in teaching general AI concepts, similar platforms
have been developed for teaching NLP. These specialized tools typically support at least one NLP task, such as text
classification or sentiment analysis, providing an intuitive introduction to the field. They may encompass a series of
pipeline components, including data collection and preprocessing, model building, performance evaluation, and model
exporting and deployment. Importantly, the capability for model deployment connects user-created NLP models to
real-world applications, infusing a sense of relevance into K-12 computing education and enhancing student engagement
[54].

To date, no literature review focuses on digital learning environments for teaching NLP in K-12 education. The most
relevant review, presented by Gresse von Wangenheim et al. [18], examines tools for teaching machine learning in
K-12. However, their work concentrates on tools that support development of ML models in general, with the majority
centering image recognition models. It does not address NLP-specific tasks such as dialogue systems and speech
synthesis. Furthermore, their review provides details on the tools’ development, but the report on the empirical findings
is relatively superficial in multiple aspects, including lack of information on learner backgrounds, assessments, and
study outcomes. Other reviews such as Lockwood and Mooney [33] and Taslibeyaz et al. [43] are centered around tools
and trends in teaching computational thinking, rather than AI or NLP. Valko et al. [48] review AI teaching tools without
specifically targeting the K-12 context. Reviews on teaching AI in K-12, such as Lee and Moon [29] and Garcia et al.


A Review of Digital Learning Environments for Teaching NLP in K-12 3

[13], examine features of several learning tools. Meanwhile, Giannakos et al. [15] provides a comprehensive summary
of games designed for teaching machine learning, and Zhou et al. [59] highlights 49 Al-education works based on the
Al literacy framework proposed by Long and Magerko [34]. Given the rapid advancement of NLP applications and the
current gap in literature, this review is poised to provide a comprehensive examination of the learning tools that are
designed for teaching NLP in K-12.

In this paper, we present a review of existing digital learning environments for teaching NLP in K-12. The purpose of
this review is to explore available digital learning tools for teaching NLP and to understand how these environments
support specific NLP tasks, NLP procedures, and their explainability. We will also describe how the tools and corre-
sponding pedagogical activities are evaluated and provide a case study of a popular learning environment. The the
contributions of this review can help educators and practitioners choose the most appropriate tool for their teaching
needs and guide researchers in developing more effective and accessible NLP learning tools and activities. This review

aims to answer the following research questions:

(1) What digital learning environments are available for NLP learning in K-12 education?
(2) What NLP learning tasks do these tools support, and how do they support them?

(3) How have researchers evaluated these tools in educational contexts?

In the remainder of this article, we will first describe the literature search procedure (section 2) which identifies
11 learning environments across 21 articles. Next, in section 3, we will present the results, organized according to
each research question. This will entail a detailed description of the systems’ characteristics and their respective
implementation strategies, followed by findings from the evaluation studies. To demonstrate how a particular system is
developed and evaluated, section 4 will present a case study that offers practical insights. Lastly, in section 5, we will
pinpoint research gaps derived from the outcomes of this literature review and discuss their implications for K-12 NLP

education.

2 LITERATURE SEARCH

The objective of this study is to investigate the state of the art in digital learning environments for learning NLP in the
context of K-12. We aim to characterize and compare the implementation and evaluation of these tools to identify gaps
and potential opportunities for future research.

The literature review was conducted using a non-systematic approach to search and review existing literature.
Based on the research questions, we identified two main term-categories to include in the literature search: discipline
(NLP-related) and target population (K-12). After performing multiple iterations of searches, we derived a list of
relevant synonyms for each category. The search terms were applied over both the titles as well as the abstracts of the
publications. In the literature search, we used the combined keywords from the two categories (Table 1). The complete
search string was (“AI Learning” AND “AI Education” AND “AI literacy” AND “NLP” AND “natural language processing”
AND “linguistics” AND “conversational AI” AND “dialog* system” AND “chatbot”) OR (“K12” AND “middle school”
AND “high school” AND “elementary” AND “primary school” AND “secondary education” AND “youth” AND “kid”
AND “child*”). The actual string varied based on the restrictions of each database.

2.1 Sources

We searched the main digital databases and libraries in the field of computing, including ACM Digital Library, IEEE

Xplore, ScienceDirect and Google Scholar. In addition, we also used Google search to account for the possibility that


4 Xiaoyi Tian and Kristy Elizabeth Boyer

Table 1. Keyword list for literature search

Category (connected | Search terms (connected using “or” logic)
using “and” logic)
Discipline Al Learning, AI Education, AI literacy, NLP/natural language processing, linguistics,
conversational AI, dialog(ue) system, chatbot

Target population K12, middle school, high school, elementary/primary school, secondary education,
youth, kid, child*

some educational tools may not yet have been published in scientific databases [39]. Since research on the topic of
NLP education is fairly new, recent works are most often published in niche conferences and workshops, including
AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI), Special Interest Group on Computer
Science Education Technical Symposium (SIGCSE), Interaction Design and Children (IDC), ACM CHI Conference on
Human Factors in Computing Systems (CHI), Computer-Supported Cooperative Work (CSCW), IEEE Symposium on
Visual Languages and Human-Centric Computing (VL/HCC). We performed additional searches in these proceedings to

minimize the risk of omitting relevant works.

2.2 Selection criteria

Based on the goal of this literature review, following the criteria described in Tatar and Eseryel [44], the selection

criteria for this literature review are shown in Table 2.

Table 2. Selection criteria for NLP learning tools for K-12

Inclusion criteria Exclusion criteria

K12 education (kindergarten through the 12th | Other stages of education such as pre-university

grade) level, college, and graduate level

Empirical studies Theoretical studies

Involves tools or technologies Studies that do not involve digital tools (e.g.,
curriculum design, unplugged activities only,
workshop design)

Report at least one form of assessment (e.g., | Studies that do not provide assessment”
learning outcomes, engagement, perception)*
English publications Non-English publications

Note. Criteria marked with an asterisk (*) were used for selection of papers answering RQ3 only

The initial literature search was conducted in March 2022. A second search was conducted in February 2023 to
include any additional papers published between March 2022 and February 2023. The searched papers were screened
by scrutinizing their titles and abstracts to determine their eligibility based on the selection criteria. Because this
field is still new, some learning environments are still works-in-progress and thus lack a published system evaluation.
However, it is still important to include these tools for the completeness of this review. For tools that only include
system implementation but not user evaluation, we have considered them to answer RQ1 and RQ2, but not RQ3.
Through backwards and forwards snowball sampling, this review finally yielded 21 publications describing 11 learning
environments. Their formats varied widely, including websites, theses, and conference proceedings. The distribution of

publications by type is shown in Figure 1.


A Review of Digital Learning Environments for Teaching NLP in K-12 5

Quantity of relevant system articles by type

Conference proceeding Fe
Thesis DL

Preprint i

Journal paper So

Workshop paper fo
Website ia

[) 1 2 3

T T T r T T r
5 6 7 8 9 10 11

4
Quantity of papers

Fig. 1. Distribution of types of the relevant reviewed publications

3 RESULTS

3.1

What digital learning environments are available for NLP learning in K-12 education?

We identified 11 digital learning environments developed for NLP learning in K-12 education. These learning envi-

ronments and corresponding evaluation studies are found in 21 publications (some systems involve multiple studies

published as different papers). Below we will briefly introduce each learning tool, its background, functionalities, and

the learning topics covered by the tool.

(1) NLP4ALL [4, 21]: Developed at Aarhus University, Denmark, NLP4All is a natural language processing tool

(3

7

ae

that facilitates the analysis of tweets from political parties, helping students with no prior ML experience to
explore differences and similarities between policy views and communication styles.

Cognimates [9]: Initially created by a student at the MIT Media Lab, Cognimates is intended for children aged
6-14. This platform introduces machine learning and image/text classification concepts. The block programming
platform "Codelab" further expands learning opportunities with block sections about feelings, Twitter, text
training, image training, and translation.

eCraft2Learn Unified User Interface [1, 2, 23, 24]: This interface, including the Snap! blocks, is part of the
broad eCraft2Learn project, funded by the European Union and led by the University of Eastern Finland. This
platform offers introduction to robotics, DIY electronics, visual programming, 3D modelling, and 3D printing to
learners above age of 7. It facilitates hands-on learning experiences and deepens understanding of technology
and fabrication. Besides the eCraft2Learn interface, this project also provides a comprehensive teacher guide,

student worksheet, activities and information about the hardware on the project website!.

(4) Machine Learning for Kids (ML4Kids) [26]: Dale Lane, a developer at IBM, initially developed this tool as a

personal project. ML4Kids introduces AI and ML concepts to children at all ages. It offers a learning environment
where children can learn the basics of ML and develop their own machine learning models for recognizing text,
images, numbers, and sounds. The site also contains a collage of tutorials of machine learning concepts (e.g.,
How to use confidence scores), step-by-step guides of different activities, as well as visualizations of user-created

models that scaffold the deep learning processes.

Thttps://project.ecraft2learn.eu/


6 Xiaoyi Tian and Kristy Elizabeth Boyer

(5) Teachable Machine (TM) [8, 45]: Google’s Creative Lab created Teachable Machine as a web-based interface
for users of all ages. It empowers users to train their own image, audio, or pose-based ML classification models
without coding. The tool offers a practical introduction to machine learning, image/pose/audio classification,
and deep learning.

(6

wm

LearningML [13, 41]: LearningML is developed by a group of educational technologists in Spain. It is a platform
that simplifies the process of building supervised machine learning models and learning about ML classification.
It creates an interactive and educational environment that encourages students between 10-16 years old to
explore machine learning.

(7

~—

Convo [60, 61]: Two graduate students at MIT developed Convo for middle school students. As a conversational
programming agent, Convo enables students to create deep learning-based conversational AI agents. It provides
a learning environment that explores Al-driven communication systems and their applications.

(8) Zhorai [31]: Created by researchers at both Harvard Graduate School of Design and MIT, Zhorai is a conversa-
tional agent that teaches AI/ML concepts through interactive dialogue for young users (aged 8-11). It focuses on
representation and reasoning, learning, and the social impact of AI.

(9) ConvoBlocks [49-52]: ConvoBlocks is a block-based programming interface developed by MIT for learners
between the ages of 11-18. It offers a hands-on experience in training, transfer learning, large language models,
intents, societal impact and ethics, speech synthesis, and speech recognition.

(10) Interactive Word Embeddings (IWE) [5]: IWE was designed in a collaborative effort of University of Maryland
and Carnegie Mellon University for high school and college students. This tool allows students to build their
understanding of preprocessing in NLP by visually exploring word embeddings, relationships, analogies, and
semantics in vector space.

(11) Build-a-Bot [38]: Developed by researchers at MIT, Build-a-Bot is an open-source tool designed for classroom

environments. It introduces students to the NLP pipeline, which includes data collection and labeling, data

augmentation, keyword filtering, intent recognition, and question answering, serving as a valuable resource for

teaching AI concepts.

A summary of these learning environments is presented in Table 3 and screenshots of some exemplar learning
systems (ML4Kids, TM, Cognimates, IWE) are shown in Figure 2 (top left, top right, bottom left, and bottom right,
respectively). Most of these tools are available as web applications, which makes them easily accessible to anyone with
an internet connection. However, some tools (e.g., Convo) are not publicly available, which limits their accessibility.
Of the above tools that are publicly available, most are available for free, but some are restricted to registered users
(e.g., NLP4All, eCraft2Learn) or require an API key to access them (e.g., Cognimates). 7 out of 11 tools allow users
to deploy the artifact (a working application or a trained model) to an external site. Three tools do not offer external
integration and one tool does not mention integration. Regarding language support, six tools only support English,
while five support at least one more language in addition to English. Among those five tools that support more than

one language, three tools offer multiple (10+) choices.

3.2. What NLP learning tasks do these tools support and how do they support them?

In this section, we will explore the array of NLP tasks supported by these educational tools, considering their handling
of various data types, input and output modalities, methods of NLP evaluation, and the level of explainability in these

tools. A summary of the technical implementation of the tools is in Table 4.


A Review of Digital Learning Environments for Teaching NLP in K-12 7

= Teachable Machine

Background Noise ©

: BERBER = — 7
Overt

> SSS =
— ne IDB Word Embedding Demo iaeceia. exoorimens
rd in — Tn 7
nn “ SUA MINUIAA LNT A
— a
S of wn i

Ry (eat oer toe

Fig. 2. Screenshots of example learning environments supporting NLP tasks. Top left: Machine Learning for Kids; Top right: Teachable
Machine; Bottom left: Cognimates; Bottom right: Interactive Word Embeddings

Table 3. Basic information for all tools

Name Website Integration Language
NLP4ALL http://86.52.121.12:5000/ no external integration | English, Danish
Cognimates | cognimates.me/home Codelab (a scratch fork) | Multiple
languages
eCraft2Learn | https://ecraft2learn.github.io/uui/index.html Snap! English,
Indonesian
ML4Kids https://machinelearningforkids.co.uk/ Scratch, App Inventor, | Multiple
Python
™ https://teachablemachine.withgoogle.com/ TensorFlow.js or English
TensorFlow Lite
LearningML | learningml.org Scratch; or export as English,
Tensorflow.js Spanish
Convo not available - English
Zhorai zhorai.csail.mit.edu no integration English
ConvoBlocks | http://alexa.appinventor.mit.edu/ Alexa-enabled devices | Multiple
IWE https://www.cs.cmu.edu/~dst/ no integration English
WordEmbeddingDemo/
Build-a-Bot | not available PySimpleGUI, English
chatbotAl library

As shown in Figure 3, text classification is the NLP task most commonly supported by these learning tools, with a
total of 5 tools supporting it. Following text classification, speech recognition and intent recognition are supported by
3 tools each. Meanwhile, speech synthesis and entity recognition are supported by 2 tools, while semantic parsing,

question answering, dialogue system, and text preprocessing are each supported by only 1 tool.


8 Xiaoyi Tian and Kristy Elizabeth Boyer
Table 4. Technical implementation summary for all tools
Tool Supported NLP Backend NLP evaluation metrics
tasks
NLP4ALL text classification Naive Bayesian Classifier classification confusion matrix
Cognimates | text classification, uClassify for text; clarifAI for | classification result with confidence score
sentiment detection, | image based on user-input data
dialogue
eCraft2Learn | speech synthesis, Pretrained cloud models, None
speech recognition built-in browser support, IBM
Watson
ML4Kids text classification IBM cloud Classification result with confidence score
based on user-input data
Teachable speech recognition TensorFlow.js for building Training accuracy and loss function
Machine custom machine learning
models
LearningML | text classification Tensorflow.js; uses Artificial | Beginner mode: shows degree of
Neural Network for ML; with | confidence for one data entry using natural
additional transfer learning language. Advanced mode: shows the
technique numerical probability of a label, expressed
as a percentage, as well as a confusion
matrix
Convo intent recognition, Rasa None
entity recognition
Zhorai speech recognition, Web speech API; NLTK and | Similarity ranking for classification results.
semantic parsing, text | Stanford CoreNLP toolkits; Word visualization for NLU.
classification, speech | D3.js
synthesis
ConvoBlocks | dialogue, intent Amazon Web Services; App No specific evaluation metrics. Students
recognition, entity Inventor cloud can test the agent and see the intent
recognition recognition result.
Interactive text preprocessing Word2vec, with word vectors | None
Word trained on a mix of Wikipedia
Embeddings text and news stories [36]
(IWE)
Build-a-Bot | question answering, | BERT for intent recognition; | No evaluations. The tool itself provides a
intent recognition DistiIBERT and T5 for sample dataset about earth science for
question answering teachers and students to explore.

In the context of NLP education, illustrating the entire NLP process for learners is crucial, as it provides a compre-

hensive understanding of the inner workings of AI and how different components contribute to the overall outcome.

Similar to the machine learning process, a typical NLP/ML system follows a four-step process: 1) generating training

data, 2) building the model, 3) evaluating the model, and 4) exporting and deploying [18]. Based on these general steps,

we found that these 11 tools offer a range of capabilities for training and deploying NLP models. With the exception of

one system (IWE [5]), all the identified systems support users generating their own training data (step 1), allowing

them either to collect and upload the text data by themselves or to manually label system-provided data. Only four

systems (i.e., ConvoBlocks, LearningML, ML4Kids, Teachable Machine) support all four processes involved in building

an NLP model.



A Review of Digital Learning Environments for Teaching NLP in K-12 9

NLP tasks supported by the learning tools

text classification
intent recognition
speech recognition
dialogue

entity recognition
speech synthesis
question answering
text preprocessing

semantic parsing

) 1 2 3 4 5
Frequency of supported NLP tasks

Fig. 3. Frequency of supported NLP tasks. Many tools support more than one NLP task.

Cognimates [9] supports three of the generalized NLP steps, except for training (the interface was not specific
about the process of training). Similarly, Zhorai [31] allows users to add their own training data, adjust training
parameters, and explore existing models using mind map visualization. NLP4AlIl [4] supports two modes: text tagging
and robot training, allowing users to either manually tag individual tweets or utilize the classifier to classify tweets
and experiment the results by specifying different word features. eCraft2Learn [23] uses Snap! [24], a block-based
programming environment that is enhanced with AI modules. eCraft2Learn provides options for customizing speech
output, including tone, pitch, volume, rate, and language. Other tools which focus on conversational AI (Convo [61],
Build-a-Bot [38], and ConvoBlocks [50]) enable users to create intents by generating training phrases and entities; then,
each of these three systems support some combination of connecting the intents with procedures, data augmentation
and keyword filtering, and deploying the system to an external device such as Alexa.

Next, to explore how these educational tools support different NLP tasks, we will discuss the different types of
natural language data these tools accommodate, as well as the input and output modalities. Investigating the diversity
of data modalities can help better understand the tools’ adaptivity in different learning contexts and capacity for serving
for diverse learners. In terms of data types (Figure 4), textual data is the most common in NLP tasks: ten tools involve
textual data and only four tools involve audio data.

As for the data input modalities (Figure 5), text via keyboard is the most popular modality, with eight systems
involving this modality. However, speech also appears to be a popular data input modality, with a total of seven systems
involving it. This may reflect a design choice to tailor these systems to young users’ developmental level and abilities.
Younger children may have limited typing skills and attention spans and may prefer more interactive input modalities
such as speech. Allowing children to input data using their speech without typing can lower their frustration and be
more engaging. There are two systems that provide existing data for the users to tinker with [4, 5]. NLP4All provides

4500 tweets for students to explore the differences and similarities between different political parties; IWE contains


10 Xiaoyi Tian and Kristy Elizabeth Boyer

Supported Data Types

text

audio

image

pose

numbers

ie} 2 4 6 8 10
Frequency

Fig. 4. Frequency of supported data types. Some learning tools support multiple data types.

word vectors trained on existing large datasets so that young users can explore the semantic relationships between

different words in a 3-D space.

Data Input Modalities

keyboard, speech

keyboard

system provided data
speech

keyboard, speech, file upload

file upload, speech

°

1 3 4

2
Frequency of tools

Fig. 5. Data input modalities supported by the learning tools

Figure 6 shows the output modalities offered by the systems. Because the primary NLP task these tools support is text
classification, the results of text classification are the most common data output modality. Among the five systems which
provide classification results, four also indicate some kind of confidence score for the prediction, showing how confident
the system is that it has correctly classified the input [19]. Because this is an important concept in machine learning, a
few systems offer adaptivity or additional scaffolding regarding the confidence level concept. For example, ML4Kids
offers a series of tutorials explaining the ML concepts, including what is a confidence score and how to use it effectively.

LearningML offers adaptivity for scaffolding this concept. For beginners, the system indicates various degrees of the


A Review of Digital Learning Environments for Teaching NLP in K-12 11

confidence level in natural language such as “not sure” or “probably”; if the user is in the advanced mode, the system
will indicate the confidence score as numerical probability. One system (Build-a-bot) output text-only response and.
the remaining systems (eCraft2Learn, Zhorai, ConvoBlocks, IWE, Convo) include additional output modalities such as

voice, action and graphic visualizations.

Data Output Modalities

classification label with confidence score
text or audio

textual response

word visualization

voice, text and graphic visualizations
textual response/action

classification label

i) 1 Z 3 4
Quantity of tools

Fig. 6. Data output modalities supported by the learning tools

One important task for pedagogical systems is scaffolding what happens “under-the-hood”. This involves helping
learners to understand the NLP tasks and processes, explaining how the model works, why it works or does not
work, and what leads the system or model to perform a certain way. Of the tools we reviewed, some systems provide
“output explanation”, often achieved by showing the evaluation metric along with some explanation, like confidence
level. The most popular evaluation metric provided by these systems is model accuracy. Two (LearningNL, NLP4All)
systems display a confusion matrix. Six systems do not provide specific evaluation or “output explanation” for their
NLP tasks. Some systems offer “model explanation”, which allows the users to tinker the training models by specifying
training features (e.g., NLP4All), customizing learning parameters (e.g., epochs in TM). Some systems provide additional
scaffolding on the NLP process itself. For instance, TM has an option for the users to explore what’s happening “under
the hood” by showing accuracy and loss per learning epoch (Figure 7). Similarly, in ML4Kids, there is a “describe your
model” module that shows the animated scaffolding of neural network concepts including input layer, encoding, bag of
words, feature extraction, word embedding, hidden layer, biases, weight, loss function, back propagation (Figure 8).
ML4Kids also encourages learners to input more training data (“The more you can get, the better it should learn, but

you need at least five examples of each as an absolute minimum’).

3.3. How have researchers evaluated these tools in educational contexts?

In this subsection, we will summarize the evaluation results of the learning systems (Table 5). Among the 21 publications
collected for this review, a total of fifteen reported on learning system evaluations, so this subsection is limited to the
discussion of this subset. We will begin with a summary of the learners’ characteristics, then move on to the types of

research design and study contexts, and will finally cover the types of assessments and findings of the studies.


12 Xiaoyi Tian and Kristy Elizabeth Boyer
Under the hood f&] x Accuracy per epoch (2)
1.05 ace
Here are a few graphs that can help you understand “= test ace
how well your model is working. 0.85
Don’t worry if this doesn’t make sense at first—you i
don’t need to use any of this to use Teachable 8 C6
Machine and, in fact, most people don’t :) 2
20.44
Vocab 0.2-
0.0 T T T T 1
Accuracy per class (Q) 0 10 20 30 40 50
Epochs
CLASS ACCURACY # SAMPLES
Class 1 1.00 5
Class2 1.00 5 Loss per epoch (2)
sal — loss
— test loss
Confusion Matrix (2)
scaleCouni
Class 1 4
3
E
3
Class "| 7
0.15
+ nu oO
3 3 0.0 T T T T 1
ol 0 10 20 30 40 50
Prediction Epochs
Fig. 7. The “under the hood” pages in Teachable Machine explaining the accuracy and loss per learning epoch
Understand your machine learning model ep
< Back to model
There are other things you can count. Things that are helpful to count depend on the project you are doing, and what you want your model to learn to . ;
ee aaa eae Ctl 8 serie
feature
Counting things is quick and simple, but not as good for projects where the order of words is important.
extraction
< Back
Q
() ¢ tt + >
) Use these controls to zoom in on the
eylabion SS \ 1 ~ diagrams
\. p >
YN
Fig. 8. The “describe your model” module in ML4Kids that provides animated scaffolding of neural network concepts
Table 5. Evaluation studies of NLP learning tools
Tool Ref. | Outcomes Research | Study Sample size Application
design location and context
participant
age
NLP4ALL [21] | learning experience Qual Denmark 24 high school | formal (social
students studies classroom)
Continued on next page



A Review of Digital Learning Environments for Teaching NLP in K-12 13
Table 5 — continued from previous page
Tool Ref. | Assessment type Research | Study Sample size Application
design location and context
participant
age
[4] | learning experience Case Denmark 20 masters formal,
study students introductory
computational
linguistics class
Cognimates | [9] | Al knowledge, Mixed Multiple (US, 104, age 6-14 informal
Perception of the agents German,
Denmark,
Sweden)
eCraft2Learn | [23] | learning experience Case Sri Lanka and 25 informal
study Singapore undergraduate, | (afterschool
18 children age | programs)
7-13
[1] | learning experience Qual Greece 48, age 13-17 informal
[24] | AI knowledge (written Mixed Indonesia 40, age 16-17 formal classroom
essay); classroom
experience
ML4Kids [26] | no evaluation - - - -
Teachable [45] | workshop experience Qual Finland 34 elementary | formal
Machine students, age
12-13
LearningML | [13] | AI knowledge Mixed Spain (college) | 14 colleges informal
students (workshops)
[41] | Al and ML knowledge Mixed Online 135, age 10-16 | informal (online
(tested reliability and webinar and
fidelity) self-paced learning)
Convo [60, | Al literacy, attitudes, Mixed Online 12, age 11-14 informal
61] | perceptions about CA
persona
Zhorai [31] | AI/ML knowledge; Mixed US 14, age 8-11 informal
attitudes;
interest/motivation in
learning; engagement
Continued on next page



14 Xiaoyi Tian and Kristy Elizabeth Boyer
Table 5 — continued from previous page
Tool Ref. | Assessment type Research | Study Sample size Application
design location and context
participant
age
ConvoBlocks | [49, | AI/ML Mixed Online 9 teachers and | informal
50] | knowledge/competency, 47 students age
student project topics; 11-18
engagement and interest
[51] | perceptions of Alexa Mixed same as [50] same as [50] same as [50]
persona; AI conception
[52] | perception and trust of Mixed Online (US, 27 children informal, Zoom
CAs Singapore, (age M = 13.96, | workshop
Canada, New SD = 1.829) and
Zealand, 19 parents (age
Indonesia, Iran, | M = 46.35, SD =
Japan, and 11.07)
India)
Interactive [5] | experience on the system | Case unknown two high informal
Word study school students,
Embeddings one
(IWE) undergraduate
Build-a-Bot | [38] | no evaluation - - - -
3.3.1 Learners. The majority of learning activities target middle and high school learners, who are approximately

11-18 years old. Only three studies involve children under the age of 11, which is roughly equivalent to 6th grade in the
United States education system. Some studies also included adult learners, such as parents and undergraduate students,
as part of their pilot study. Learners in the studies come from a variety of locations, including Denmark, the United
States, Germany, Sweden, Sri Lanka, Singapore, Greece, Indonesia, Finland, Spain, and Canada. Some studies were
conducted online with participants mainly from the US, and also from countries like Singapore, Canada, New Zealand,

Indonesia, Iran, Japan, and India.

3.3.2. Evaluation Methodologies and Study Contexts. Next, we report the research design and data analysis type for the
learning tools that reported evaluations. As shown in Figure 9, the majority (9/15) of studies reported their evaluation
using a mixed methods approach. Other evaluations have been conducted as qualitative (3) or case studies (3). Sample
sizes are moderate, ranging from 3 to 135 participants. The median sample size is 29.5 with a standard deviation of 36.9.
Studies with relatively large sample sizes include Rodriguez-Garcia et al. [41], presenting a study with 135 students,
Druga [9], presenting a multi-site international study including 104 children, as well as Van Brummelen et al. [52], with
49 participants including 27 children and 19 parents.

The research study contexts are categorized into informal (10) and formal learning environments (4), as shown in

Figure 10. Formal learning environment means study that is conducted in a classroom in a school, during the normal



A Review of Digital Learning Environments for Teaching NLP in K-12 15

Research design and data analysis type for evaluation studies Research study context (formal vs informal)

Mixed informal

No evaluation

Case study

Qualitative

o—

i} x 2 = 4 5 6 7 8 s i) 1 2 4 4 5 6 7
Quantity of studies Quantity of studies

8 9 10

Fig. 9. Research design and data analysis type Fig. 10. Research study context (formal vs. informal)

class time. In formal learning environments, the topics are integrated into relevant subjects (e.g., social studies). Students
are expected to participate in the learning activity as they would in their normal class activities. In contrast, in informal
learning environments such as after-school programs, online workshops and summer camps, students have much more
freedom to choose whether to attend the activity or not. Because of the voluntary and low-pressure nature of the
informal learning environments, the learning activities are typically designed with special consideration for keeping
participants interested and engaged, especially for multiple-sessions or longitudinal studies [9].

The duration of the studies varied widely, with the shortest lasting only for 2 hours and the longest extending to
30 hours spanning over a month. The majority of the studies conducted their research over multiple sessions or days,
often in the format of workshops or class meetings, with the duration of individual sessions ranging from 1.5 hours to
3.5 hours.

Assessment types used to evaluate the learning tools

Al knowledge

Experience (system or learning activity)
No evaluation

Engagement

Interest / motivation

Perception of agents

Attitudes towards Al

Al literacy

Researcher observations

i) 1 2 3 4 5 6 7
Quantity of assessment types applied

Fig. 11. Assessment types used to evaluate the learning tools, one tool might use multiple assessment types

3.3.3 Learning Tools Evaluation Outcomes. The outcomes in evaluating learning tools primarily encompass the acquisi-

tion of AI knowledge, learning experiences, levels of engagement, and perceptions and attitudes towards AI (Figure 11).


16 Xiaoyi Tian and Kristy Elizabeth Boyer

Among these, the most prevalent form of assessment is AI knowledge acquisition, as measured in seven tools. Next, we
present the findings of these evaluative studies organized by the aforementioned outcomes.

AI knowledge acquisition. Studies consistently report a positive shift in learners’ AI knowledge. For instance, Lin
et al. [31] evaluated children’s understanding of the concepts introduced by Zhorai, concluding that a conversational
interface coupled with visualization of the training process was effective in teaching children about machine learning
concepts. In a similar vein, a study involving youth aged 10-16 years using LearningML [41] reported significant
improvement in participants’ understanding of AI through comparison of pre- and post-tests (p < .001, effect size 0.486).
Interestingly, this intervention’s impact was particularly higher for learners with less prior AI knowledge (p < .001,
effect size 1.007). In a study involving 40 students from Indonesia aged 16-17 using AI blocks in Snap! [24], the analysis
of students’ written essays found that 77.5% of the essays demonstrated understanding of AI. From the essays, students
also identified both benefits and dangers of speech synthesis. Similarly, in a series of user studies using Cognimates,
Druga [9] reported that children developed a strong understanding of AI concepts, but their ability to collaborate and
communicate played a significant role in their learning, and their collaboration skills and understanding of AI concepts
differed by the SES background of their community.

Learning experience. This phenomenon is another focal point of evaluation, five systems report the experience of
system use and overall learning activity, assessed through system log analysis, reflective questionnaires and user feedback.
This construct, inclusive of engagement as a measurable variable, revealed enriching experiences across diverse tools. For
instance, the application of NLP4All in classrooms fostered meaningful participation and constructive discussions [21],
while the use of Teachable Machine (TM) in mobile environments demonstrated feasibility and accessibility for novice
learners, albeit with limitations in predictive accuracy and sound recognition [45]. The integration of physical devices
like Amazon Echo was found engaging by Van Brummelen et al. [50], highlighting their role in making AI accessible to
everyone, even though they are not strictly necessary. A usability study involving Interactive Word Embeddings (I[WE)
pointed out that while 3D scatter plots and word analogy were intuitive and understandable, user-defined semantic
dimensions required more explanations for proper interpretation [5]. Another study using Teachable Machine (TM) in
a primary school in Finland revealed the tool’s feasibility for novice learners exploring machine learning, especially in
mobile environments due to the low computational cost [45]. However, limitations were noted in prediction accuracy
due to small training data and in sound recognition models, which were partially impaired by background noise.
Furthermore, the gamification and competitive elements integrated into AI tools can significantly improve students’
understanding and engagement with AI, contributing to a richer and more immersive learning experience [21].

Perceptions, attitudes, and interests. The cultivation of interest and the shaping of perceptions and attitudes
towards AI are crucial outcomes explored in several studies. These studies highlight the importance of designing agent
persona and providing transparency and explanation to build trust in conversational agents [52]. The findings regarding
perceptions of AI are mixed. In a study in which students learned about and utilized Convo, their perceptions of the
intelligence of conversational AI agents decreased post-workshop, while their confidence in their abilities to construct
these agents increased [61]. In contrast, Van Brummelen et al. [51] report students felt Alexa to be more intelligent
and felt closer to Alexa after programming the conversational AI. Another study raised concerns about over-trusting
technology. Van Brummelen et al. [52] observe that children with no programming experience reported higher trust in

Alexa’s correctness after a programming activity.


A Review of Digital Learning Environments for Teaching NLP in K-12 LZ

4 CASE STUDY: CONVOBLOCKS

In this section, we describe a well-studied system, ConvoBlocks on MIT App inventor, as a concrete example of how a

specific NLP teaching tool is developed and evaluated.

4.1. Overview of ConvoBlocks

ConvoBlocks is a block-based programming environment for 5-12th grade students to create conversational agents.
ConvoBlocks is primarily designed for students aged 11 to 18 to create their own conversational agents for deployment
on any Alexa-enabled device or within the MIT App Inventor interface. The programming interface includes a range of
code block types: “Voice? which triggers Alexa to execute a task such as speaking a phrase; “Control,” which modifies
the program’s flow based on constructs such as “if”, “while”; and “Text,” which allows textual data manipulation, such

as combining strings.

4.1.1 Pedagogical Approach. ConvoBlocks offers a list of built-in tutorials for beginner, intermediate and advanced.
learners. For beginners, the system provides “Alexa Hello World” and “Alexa Calculator” tutorials. The system provides
an “Alexa number guessing game” tutorial for intermediate learners and an “Alexa Messenger” tutorial for advanced
learners. Figure 12 and 13 show the interface for the intermediate “number guessing game” tutorial, in which the target
grade level is 6-12th grade. This tutorial guides the developer to create an Alexa Skill that allows the player to try
guessing a random number while receiving feedback on whether the guessed number is too big, too small or just right.
Within the development environment, the tutorial is positioned at the left, and the user has the option to toggle the
display of the tutorial by clicking the “Toggle Tutorial” button. The tutorial is divided into several tabs, and each tab
represents a module (e.g., setup, programming, integration), toggled vertically. There are multiple pages in each tab,

toggled horizontally; each contains a fine-grained sequence of actions and screenshots to complete the task.

APP INVENTOR.
Challenge
Sotup
The Number Guessing Game Tutorial
Previous 130120 Next
Define a Slot and Please create your skill and send
A updates to Amazon before testing
Add a List of a userGuess Manage Slot your skill

Utterances tm guessing aa

Now that we have our slot det

em B

Properties box.

Within Properties, we first have a textbox.

$4)

Logout of Amazon

Send Updates

Non-visible components

QuizMe Rename Delete

Programmed Utterances

Upload File
"Alexa, tell quessing game host to...”

Y intent: QuizMe

start a game

myGuessSlot

start the quiz

begin the guessing game

Add Remove

Fig. 12. ConvoBlocks interface, “Designer” view


18 Xiaoyi Tian and Kristy Elizabeth Boyer

MIT
FB apr wiventor

Challenge

Setup

‘The Number Guessing Game Tutorial
© Built-in

Previous 150120 Next conti

Define the Intent

1 Sending code to Amazon.

Handler for initialize global NN 100)

Update manifest
makeGuessintent intialize global © O
Now we want our makeGuessintent to Update skill
myGuessSlot value. IF our guess is a ,
greater than our secret variable, THEN Bevocedures ae
Alexa will ask: “{myGuessSlot) is too big 8 Ocawosnctos
another opportunty to guess, which wil outze
correctly. IF our guess is less than our Se
secret variable, THEN Alexa will ask: WiserGuess

again’, which will once again create a
loop. IF our guess is accurate, THEN
Alexa will say: “{myGuessSlot} is correct.
Congratulations!"

4

Because we want a different response [opeAnaan
based on our myGuessSlot value, we es mae

need to use conditionals. Conditionals

provide us with the ability to choose

different responses based on different Media

parameters using iffelse iffthen

statements. Let's try it out! Upload File © Programmed Utterances
To start open the Control drawer and “ ‘na ean sue
drag out the [Geena ncaa block. (6) ‘Alexa, tell guessing game host t..
and place it into the Rugs
vakeGuessintent.spoken [ag (6) ¥ Intent: QuizMe
teas oe x starta game
=— | bo Be —
. sed
C [Hide Warnings | begin the guessing game
2
s
. —

Fig. 13. ConvoBlocks interface, “Blocks” page

4.1.2 System Interface Design. For the project working page, the programming environment includes two views, a
“Designer” view (Figure 12) and a “Blocks” page (Figure 13). In the Designer view, developers set high-level project
types, function names, intents and entities; in the Blocks page, developers program the specific actions the artifact will
perform (e.g., say “Hello world”) by dragging and dropping block-based code elements. The user can test the project on

the right side of the screen in both views.

4.2 Evaluation and Critical Analysis

4.2.1 Study Implementation and Outcomes. Besides the system’s built-in tutorial and its own block-based programming
interface, the research team of ConvoBlocks also developed a conversational AI curriculum that was implemented as a
5-day, 2.5-hour-per-day workshop. On a high level, the workshop covers important topics in conversational AI and
NLP, including training, transfer learning, large language models, intents, speech recognition and speech synthesis.
The workshop schedule features a series of interactive lectures and group discussions on the “Big 5 AI Ideas Touretzky
et al. [46]? comparing differences between rule-based and ML-based architectures, building conversational AI projects,
and AT ethics.

The main conversational agent concepts taught in the ConvoBlocks curriculum include events (dialogue management),
conditions (also part of dialogue management), and event-driven program representations (conversation representation).
Students define events and responses to manage agent dialogue, add conditions to their event handlers to define whether
or not particular pieces of dialogue or events occur, and represent conversations through event-driven block-based
programming.

The ConvoBlocks environment was implemented and evaluated in informal learning environments. Preliminary
results were published in 2021 [50], in which the authors organized a five-day workshop with 47 students ranging in
age from 11 to 18. The workshop achieved the desired outcome: significant learning gains were reported in general

Al and conversational AI concepts, though machine learning and ethics were identified as more challenging areas. A


A Review of Digital Learning Environments for Teaching NLP in K-12 19

total of 70 projects were created by the students. Programming conversational AI also seemed to make a difference in
students’ perception of AI. Another evaluation study reported that after programming the conversational AI, students
felt Alexa to be more intelligent and felt closer to Alexa [51]. This finding contrasts with [60]’s study where they found

participants’ perceptions of the intelligence of conversational AI agents decreased after the programming workshop.

4.2.2 Comparative Analysis. While Convo allows for easy device integration and features a variety of scaffolding
integrated in the environment, there are several disparities when compared to other systems in the landscape of NLP
learning tools. To elaborate, one notable absence in ConvoBlocks is the ability to define conditional relationships between
different intents, which is a feature often included in other conversational AI platforms such as Dialogflow Google
[17]. In human-human conversations, certain dialogues are often contextually dependent on preceding interactions.
For instance, a user might express the intent of request movie recommendation, followed by a request more information
intent. However, ConvoBlocks treats all intents with uniform priority, not allowing for such contextual dependencies
and consequently, lacks conversational flow—a visual representation that is especially pivotal in K-12 contexts [27, 37].
This may make it challenging to create coherent and engaging conversational experiences, particularly for beginners.
Balancing visual appeal with functional robustness is crucial, and understanding these disparities can guide future

developments in creating more refined and inclusive learning tools.

5 DISCUSSION

In this section, we will first summarize the key findings from the literature review and the case study. Then, we will

identify gaps in the current state of research and design and discuss implications for NLP education in K-12.

5.1 Key Findings

This paper identified 11 digital learning environments for NLP learning in K-12 education, with most being accessible
as online web apps. However, some tools have limitations such as restricted access or language support, which may
affect their usability for beginners.

The 11 digital learning environments for NLP in K-12 education primarily support text classification, speech
recognition, and intent recognition tasks, with limited support for other popular NLP tasks. These tools offer varying
capabilities for training and deploying NLP models and provide different data input modalities, such as keyboard and
speech. However, the majority of the systems lack in-depth scaffolding and explanations for NLP processes, which
could be improved for better learner understanding.

A majority of the studies employed mixed methods for evaluating their tools, with moderate sample sizes ranging
from 3 to 135 (median = 29.5). Research studies were more often deployed in informal learning contexts than formal
contexts.

Most NLP learning activities target middle and high school students, with evaluations focusing on AI knowledge
assessment and learning experiences. These tools prove effective for teaching NLP and AI concepts, fostering interest,
and improving students’ understanding and engagement. However, learning challenges persist in machine learning and

ethics concepts, and there is more work to be done in addressing issues of over-trusting technology.

5.2 Gaps and Implications

In light of these findings, we identified several research and design gaps that need to be addressed:


20 Xiaoyi Tian and Kristy Elizabeth Boyer

5.2.1 Gap 1: Limited NLP task variety. Most of the NLP tasks supported by these tools are natural language understanding
(NLU) related (e.g., intent recognition), with other NLP tasks such as speech synthesis, semantic parsing, and question-
answering only introduced by a few tools. Given the breadth of NLP tasks, researchers and educators have to make
choices about where to begin to teach children. NLU is a popular starting point because its applications (e.g., voice
assistants) are ubiquitous in students’ daily life, and thus can serve as a more tangible and relatable introduction to NLP.
However, this review revealed a potential research gap in exploring learning tools to support NLP tasks other than
NLU. While in-depth mastery of each NLP task is not the primary goal of K-12 NLP education, students should gain a
foundational understanding of the range of the problems NLP can address, and be able to explore the more complex
applications if they wish. There is a clear opportunity to develop and investigate new tools that support other important
NLP tasks, such as text generation, information retrieval, and topic modeling, in order to expose younger students to a

wider range of NLP tasks.

5.2.2 Gap 2: Challenges in comprehensive evaluation methods. Developing effective evaluation for NLP learning tools,
especially for younger students, is fraught with challenges. Because many of the NLP learning tools are emergent
and still being refined, they have not been extensively evaluated. Among the tools that have received evaluation, this
evaluation predominantly leans towards AI knowledge assessment, with specific emphasis or qualitative reports on
student learning experiences. The learning objectives of the activities aim to foster students’ general AI knowledge. As
a result, there is limited or no emphasis on evaluating students’ understanding of NLP concepts in the assessments.

Evaluating educational tools in general, particularly in K-12 environments, presents many challenges [12]. One of the
primary obstacles is student recruitment. Gaining access to, and permissions for, student participation can be a logistical
challenge. Other considerations range from aligning with school calendars, managing sample sizes, and deciding
on the format and duration of the program. Integrating these learning tools into the existing school ecosystem also
poses dilemmas: How does one best incorporate them? Is it more effective to introduce them as voluntary add-ons, or
replace current curriculum components? Should the evaluation prioritize consistent administration by having dedicated
researchers present, or should it prioritize authenticity and work toward scalability by training teachers to manage
these new educational modules themselves?

Given these challenges, it is unsurprising that a robust, NLP-specific evaluation mechanism is not commonplace
yet. As the field progresses, refining evaluation methods to holistically assess NLP-specific learning outcomes will be
crucial. This will not only ensure the effectiveness of the learning tools but also enhance our understanding of their

impact on students’ learning trajectories.

5.2.3 Gap 3: Insufficient pedagogical explanation. This review reveals that while many NLP and ML pedagogical
systems provide some level of explanation to support learner understanding of tasks and processes, there is still room
for improvement. Model accuracy is the most popular evaluation metric presented to users, but not all systems offer
detailed evaluation outputs or interactive explanations. The question of how to distill an abstract evaluation metric into
intuitive insights, without oversimplifying the underlying mechanism, remains a challenge. Researchers might draw on
work in non-NLP applications, where using explainability techniques and interpretable ML models has been shown to
enhance children’s understanding of AI [35]. To better support users in developing a comprehensive understanding of
NLP and ML techniques, pedagogical systems should refine their evaluation metrics, adopt interpretable models, offer
more in-depth explanations of model workings, and incorporate engaging educational methods, such as animations and

visual aids [3, 25].


A Review of Digital Learning Environments for Teaching NLP in K-12 21

5.2.4 Gap 4: Limited focus on younger children. The majority of NLP and ML learning activities target middle and high
school students, with only a few studies focusing on children under the age of 11. Given the complexity of NLP, which
requires knowledge in programming, linguistics, and mathematics, it seems reasonable to introduce NLP concepts to
students with a stronger foundation in these areas. Besides the complexity of the content, gaining consistent access to
younger age groups can be challenging due to protective educational environments and parental concerns. Additionally,
these children exhibit more considerable variation in developmental readiness, which further challenges the design
and evaluation of suitable learning activities. However, this should not exclude younger students from learning about
NLP. There is a gap in learning activities specifically designed for younger children. In future studies, researchers and
educators should develop tools and activities adaptable to younger age groups and their level of understanding in order

to cultivate early interest in NLP and ML.

5.2.5 Gap 5: Insufficient personalized support for diverse learners. The current learning tools rarely deliver personalized
and adaptive learning experiences to cater to the unique needs of individual learners. Given the significant variation in
cognitive load and learning requirements between beginner and advanced activities [7], a one-size-fits-all approach can
lead to less effective learning outcomes. This marks a critical gap in the field, underscoring the need for tools that offer
adaptivity, accommodating learners’ skill levels through personalized levels of difficulty, scaffolding, and explanation
[30]. Future research should prioritize the development of systems that can dynamically adjust to individual learners,

thereby fostering a more inclusive and effective learning environment.

5.2.6 Gap 6: Lack of recommendations for effective teaching strategies. While some studies offer insights into successful
teaching methods, there is a gap in the literature regarding concrete recommendations for effectively incorporating
NLP education in K-12 classrooms. Strategies such as gamification, embodiment, iteration, and instant feedback have
been shown to enhance learning experiences [6, 18], but these need to be further explored and integrated into NLP
educational tools. A complementary strategy, using relevant data that relates to students’ lives or real-world problems,
can create more engaging and meaningful learning experiences [13, 40]. For example, training speech recognition
systems with user-defined speech data can empower students to develop socially relevant agents that are sensitive to
gender, voice, and accent variations. By providing educators and developers with clear recommendations for effective

teaching strategies, NLP learning tools can be designed to better engage and educate students in this complex field.

6 CONCLUSION

NLP education in K-12 presents a unique opportunity to engage students in the interdisciplinary field of language and
technology. In this paper, we present a comprehensive literature review of the state-of-the-art learning environments
that support teaching of NLP concepts. We summarized key findings from the development of these learning tools and
their evaluations. We also highlight the current gaps in the field of NLP learning tools and activities, and we discuss
practical implications and future directions. Using these design implications, we hope that educators and researchers
can develop more effective and engaging educational experiences for a broader range of students. Diversifying NLP
tasks, refining evaluation methods, enhancing explanations and scaffolding, and creating age-appropriate learning
activities will not only improve students’ understanding of NLP concepts but also foster early interest in this rapidly
evolving field. As technology continues to advance, it is crucial to equip the next generation with the knowledge and

skills necessary to navigate and contribute to the world of NLP and AI.


22

Xiaoyi Tian and Kristy Elizabeth Boyer

ACKNOWLEDGMENTS

I would like to thank Amogh Mannekote and Lydia Pezzullo for their thorough copy-editing of this paper, and to

everyone in the LearnDialogue group for their companionship and encouragement.

REFERENCES

1

2]

3]

4]

5]

6]

7)

[8]

9]
10]

(11)

12]

[13]

14]

15]

[16]

(17]
[18]

19]

[20]

21]

[22]
[23]

24]

Dimitris Alimisis, Rene Alimisi, Dimitrios Loukatos, and Emmanouil Zoulias. 2019. Kids make their own robots: good practices from the eCraft2Learn
project. Form@ re-Open Journal per la formazione in rete 19, 1 (2019), 12-29.

Dimitris Alimisis and Dimitrios Loukatos. 2018. STEM education post-graduate students’ training in the eCraft2Learn ecosystem. In Proceedings of
the 2nd International Conference on Innovating STEM Education, Athen, Greece. 22-24.

Cecilia O Alm and Alex Hedges. 2021. Visualizing NLP in Undergraduate Students’ Learning about Natural Language. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 35. 15480-15488.

Rebekah Baglini and Hermes Hjorth. 2021. Natural Language Processing 4 All (NLP4AIl): A New Online Platform for Teaching and Learning NLP
Concepts. In Proceedings of the Fifth Workshop on Teaching NLP. Association for Computational Linguistics, Online, 28-33.

Saptarashmi Bandyopadhyay, Jason Xu, Neel Pawar, and David Touretzky. 2022. Interactive Visualizations of Word Embeddings for K-12 Students.
In EAAI-22: The 12th Symposium on Educational Advances in Artificial Intelligence.

Amy L Baylor and Jeeheon Ryu. 2003. The effects of image and animation in enhancing pedagogical agent persona. Journal of Educational Computing
Research 28, 4 (2003), 373-394.

Dolly Bounajim, Arif Rachmatullah, Madeline Hinckle, Bradford Mott, James Lester, Andy Smith, Andrew Emerson, Fahmid Morshed Fahid, Xiaoyi
Tian, Joseph B Wiggins, et al. 2021. Applying Cognitive Load Theory to Examine STEM Undergraduate Students’ Experiences in An Adaptive
Learning Environment: A Mixed-Methods Study. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, Vol. 65. SAGE
Publications Sage CA: Los Angeles, CA, 556-560.

Michelle Carney, Barron Webster, Irene Alvarado, Kyle Phillips, Noura Howell, Jordan Griffith, Jonas Jongejan, Amit Pitaru, and Alexander Chen.
2020. Teachable machine: Approachable Web-based tool for exploring machine learning classification. In Extended abstracts of the 2020 CHI conference
on human factors in computing systems. 1-8.

Stefania Druga. 2018. Growing up with AI: Cognimates: from coding to teaching machines. Master’s thesis. Massachusetts Institute of Technology.
Stefania Druga and Amy J Ko. 2021. How do children’s perceptions of machine intelligence change when training and coding smart programs?. In
Interaction design and children. 49-61.

Amy Eguchi, Hiroyuki Okada, and Yumiko Muto. 2021. Contextualizing AI education for K-12 students to enhance their learning of AI literacy
through culturally responsive approaches. KI-Kiinstliche Intelligenz 35, 2 (2021), 153-161.

Garry Falloon, Maria Hatzigianni, Matt Bower, Anne Forbes, and Michael Stevenson. 2020. Understanding K-12 STEM education: A framework for
developing STEM literacy. Journal of Science Education and Technology 29 (2020), 369-385.

Juan David Rodriguez Garcia, Jestis Moreno-Leén, Marcos Roman-Gonzalez, and Gregorio Robles. 2020. LearningML: a tool to foster computational
thinking skills through practical artificial intelligence projects. Revista de Educacién a Distancia (RED) 20, 63 (2020).

Radhika Garg, Hua Cui, Spencer Seligson, Bo Zhang, Martin Porcheron, Leigh Clark, Benjamin R Cowan, and Erin Beneteau. 2022. The Last Decade
of HCI Research on Children and Voice-based Conversational Agents. In Proceedings of the 2022 CHI Conference on Human Factors in Computing
Systems. 1-19.

Michail Giannakos, Iro Voulgari, Sofia Papavlasopoulou, Zacharoula Papamitsiou, and Georgios Yannakakis. 2020. Games for artificial intelligence
and machine learning education: Review and perspectives. Non-formal and informal science learning in the ICT era (2020), 117-133.

David Antonio Gomez Jauregui, Léonor Philip, Céline Clavel, Stéphane Padovani, Mahin Bailly, and Jean-Claude Martin. 2013. Video analysis of
approach-avoidance behaviors of teenagers speaking with virtual agents. In Proceedings of the 15th ACM International Conference on Multimodal
Interaction. 189-196.

Google. 2023. Dialogflow. https://dialogflow.cloud.google.com/cx. Accessed: May 2023.

Christiane Gresse von Wangenheim, Jean CR Hauck, Fernando S Pacheco, and Matheus F Bertonceli Bueno. 2021. Visual tools for teaching machine
learning in K-12: A ten-year systematic mapping. Education and Information Technologies 26, 5 (2021), 5733-5778.

Matthias Hagen, Martin Potthast, Michel Biichner, and Benno Stein. 2015. Twitter Sentiment Detection via Ensemble Classification Using Averaged
Confidence Scores.. In ECIR. 741-754.

Tom Hitron, Yoav Orlev, Iddo Wald, Ariel Shamir, Hadas Erel, and Oren Zuckerman. 2019. Can children understand machine learning concepts? The
effect of uncovering black boxes. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1-11.

Arthur Hjorth. 2021. NaturalLanguageProcesing4All: -A Constructionist NLP tool for Scaffolding Students’ Exploration of Text. In Proceedings of the
17th ACM Conference on International Computing Education Research. 347-354.

Dan Jurafsky and James H Martin. 2023. Speech and language processing (3rd (draft) ed.)._ https://web.stanford.edu/~jurafsky/slp3/

KM Kahn and N Winters. 2018. AI programming by children. Constructionism 2018, 322-331.

K Megasari Kahn, Rani Megasari, Erna Piantari, and Enjun Junaeti. 2018. AI programming by children using snap! block programming in a

developing country. (2018).


A Review of Digital Learning Environments for Teaching NLP in K-12 23

[25]
[26]
[27]
[28]
[29]
[30]
(31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]

40]

[41]

42]

43]

44]

45]

46]

47]
48]

[49]

[50]

[51]

[52]

[53]

Minsuk Kahng, Nikhil Thorat, Duen Horng Chau, Fernanda B Viégas, and Martin Wattenberg. 2018. GAN lab: Understanding complex deep
generative models using interactive visual experimentation. [EEE transactions on visualization and computer graphics 25, 1 (2018), 310-320.

Dale Lane. 2018. Machine learning for kids. https://machinelearningforkids.co.uk/

JA Large and Jamshid Beheshti. 2005. Interface design, web portals, and children. Library Trends 54, 2 (2005), 318-342.

Irene Lee, Safinah Ali, Helen Zhang, Daniella DiPaola, and Cynthia Breazeal. 2021. Developing middle school students’ AI literacy. Proceedings of
the 52nd ACM technical symposium on computer science education, 191-197.

Yo-Seob Lee and Phil-Joo Moon. 2020. Analysis of machine learning education tool for kids. International Journal of Advanced Culture Technology 8,
4 (2020), 235-241.

Kam Cheong Li and Billy Tak-Ming Wong. 2021. Features and trends of personalised learning: A review of journal publications from 2001 to 2018.
Interactive Learning Environments 29, 2 (2021), 182-195.

Phoebe Lin, Jessica Van Brummelen, Galit Lukin, Randi Williams, and Cynthia Breazeal. 2020. Zhorai: Designing a Conversational Agent for
Children to Explore Machine Learning Concepts. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 13381-13388.

Diane Litman. 2016. Natural language processing for enhancing teaching and learning. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 30.

James Lockwood and Aidan Mooney. 2017. Computational thinking in education: Where does it fit? A systematic literary review. arXiv preprint
arXiv:1703.07659 (2017).

Duri Long and Brian Magerko. 2020. What is AI Literacy? Competencies and Design Considerations. In Proceedings of the 2020 CHI Conference on
Human Factors in Computing Systems. 1-16.

Gaspar Isaac Melsion, Ilaria Torre, Eva Vidal, and Jolanda Leite. 2021. Using Explainability to Help Children UnderstandGender Bias in AI. In
Interaction design and children. 87-99.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in pre-training distributed word
representations. Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018) (2018).

Jaehyun Park, Sung H Han, Hyun K Kim, Seunghwan Oh, and Heekyung Moon. 2013. Modeling user experience: A case study on a mobile device.
International Journal of Industrial Ergonomics 43, 2 (2013), 187-196.

Kate Pearce, Sharifa Alghowinem, and Cynthia Breazeal. 2022. Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent
Recognition and Question Answering Architecture. arXiv preprint arXiv:2212.07542 (2022).

Jan Piasecki, Marcin Waligora, and Vilius Dranseika. 2018. Google search as an additional source in systematic reviews. Science and engineering
ethics 24 (2018), 809-810.

Yim Register and Amy J Ko. 2020. Learning machine learning with personal data helps stakeholders ground advocacy arguments in model mechanics.
In Proceedings of the 2020 ACM Conference on International Computing Education Research. 67-78.

Juan David Rodriguez-Garcia, Jesus Moreno-Leén, Marcos Roman-Gonzalez, and Gregorio Robles. 2021. Evaluation of an online intervention to
teach artificial intelligence with learningml to 10-16-year-old students. In Proceedings of the 52nd ACM technical symposium on computer science
education. 177-183.

David Williamson Shaffer and Mitchel Resnick. 1999. “Thick” authenticity: New media and authentic learning. Journal of interactive learning
research 10, 2 (1999), 195-216.

Elif Taslibeyaz, Engin Kursun, and Selcuk Karaman. 2020. How to Develop Computational Thinking: A Systematic Review of Empirical Studies.
Informatics in Education 19, 4 (2020), 701-719.

Cansu Tatar and Deniz Eseryel. 2019. A literature review: Fostering computational thinking through game-based learning in K-12. In The 42nd
Annual Convention of The Association for the Educational Communications and Technology. 288-297.

Tapani Toivonen, Ilkka Jormanainen, Juho Kahila, Matti Tedre, Teemu Valtonen, and Henriikka Vartiainen. 2020. Co-designing machine learning
apps in K-12 with primary school children. In 2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT). IEEE, 308-310.
David Touretzky, Christina Gardner-McCune, Fred Martin, and Deborah Seehorn. 2019. Envisioning AI for K-12: What should every child know
about AI?. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 9795-9799.

David S Touretzky and Christina Gardner-McCune. [n. d.]. Artificial intelligence thinking in k-12. ([n.d.]), 153-180.

Nataliia V Valko, Tatiana L Goncharenko, Nataliya O Kushnir, and Viacheslav V Osadchyi. 2022. Cloud technologies for basics of artificial intelligence
study in school. In CTE Workshop Proceedings, Vol. 9. 170-183.

Jessica Van Brummelen. 2019. Tools to create and democratize conversational artificial intelligence. Master’s thesis. Massachusetts Institute of
Technology.

Jessica Van Brummelen, Tommy Heng, and Viktoriya Tabunshchyk. 2021. Teaching Tech to Talk: K-12 Conversational Artificial Intelligence Literacy
Curriculum and Development Tools. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.

Jessica Van Brummelen, Viktoriya Tabunshchyk, and Tommy Heng. 2021. “Alexa, Can I Program You?”: Student Perceptions of Conversational
Artificial Intelligence Before and After Programming Alexa. In Interaction Design and Children. 305-313.

Jessica Van Brummelen, Mingyan Claire Tian, Maura Kelleher, and Nghi Hoang Nguyen. 2022. Learning Affects Trust: Design Recommendations
and Concepts for Teaching Children—and Nearly Anyone-about Conversational Agents. arXiv preprint arXiv:2209.05063 (2022).

Xiaoyu Wan, Xiaofei Zhou, Zaiqiao Ye, Chase K Mortensen, and Zhen Bai. 2020. SmileyCluster: supporting accessible machine learning in K-12
scientific discovery. In proceedings of the Interaction Design and Children Conference. 23-35.


24

54]

55]

[56]

57]

[58]

59]

[60]

61]

62]

Xiaoyi Tian and Kristy Elizabeth Boyer

Gary KW Wong, Xiaojuan Ma, Pierre Dillenbourg, and John Huan. 2020. Broadening artificial intelligence education in K-12: where to start? ACM
Inroads 11, 1 (2020), 20-29.

Ying Xu, Joseph Aubele, Valery Vigil, Andres S Bustamante, Young-Suk Kim, and Mark Warschauer. 2022. Dialogue with a conversational agent
promotes children’s story comprehension via enhancing engagement. Child Development 93, 2 (2022), e149-e167.

Ying Xu, Stacy Branham, Xinwei Deng, Penelope Collins, and Mark Warschauer. 2021. Are Current Voice Interfaces Designed to Support Children’s
Language Development?. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-12.

Ying Xu and Mark Warschauer. 2020. Exploring young children’s engagement in joint reading with a conversational agent. In Interaction Design and
Children. 216-228.

Chenliang Zhou, Dominic Kuang, Jingru Liu, Hanbo Yang, Zijia Zhang, Alan Mackworth, and David Poole. 2020. AISpace2: an interactive visualization
tool for learning and teaching artificial intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 13436-13443.

Xiaofei Zhou, Jessica Van Brummelen, and Phoebe Lin. 2020. Designing AI learning experiences for K-12: emerging works, future opportunities and
a design framework. arXiv preprint arXiv:2009.10228 (2020).

Jessica Zhu. 2021. Creating Your Own Conversational Artificial Intelligence Agents Using Convo, a Conversational Programming System. Master’s
thesis. Massachusetts Institute of Technology.

Jessica Zhu and Jessica Van Brummelen. 2021. Teaching Students About Conversational AI Using Convo, a Conversational Programming Agent. In
2021 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). 1-5.

Abigail Zimmermann-Niefield, Makenna Turner, Bridget Murphy, Shaun K Kane, and R Benjamin Shapiro. 2019. Youth learning machine learning
through building models of athletic moves. In Proceedings of the 18th ACM international conference on interaction design and children. 121-132.
