arX1v:2510.10806v1 [cs.CL] 12 Oct 2025

Is IMPLICIT KNOWLEDGE ENOUGH FOR LLMS? A RAG
APPROACH FOR TREE-BASED STRUCTURES

A PREPRINT
Mihir Gupte* Paolo Giusto Ramesh S.
General Motors General Motors General Motors
mihir.gupte@gm. com paolo. giusto@gm.com ramesh.s@gm.com

October 14, 2025

ABSTRACT

Large Language Models (LLMs) are adept at generating responses based on information within
their context. While this ability is useful for interacting with structured data like code files, another
popular method, Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model’s in-context learning. However, it is not well-explored how to best represent this retrieved
knowledge for generating responses on structured data, particularly hierarchical structures like trees.
In this work, we propose a novel bottom-up method to linearize knowledge from tree-like structures
(like a GitHub repository) by generating implicit, aggregated summaries at each hierarchical level.
This approach enables the knowledge to be stored in a knowledge base and used directly with RAG.
We then compare our method to using RAG on raw, unstructured code, evaluating the accuracy and
quality of the generated responses. Our results show that while response quality is comparable across
both methods, our approach generates over 68% fewer documents in the retriever, a significant gain
in efficiency. This finding suggests that leveraging implicit, linearized knowledge may be a highly
effective and scalable strategy for handling complex, hierarchical data structures.

Keywords Retrieval-Augmented Generation - Large Language Models - Implicit Knowledge - Hierarchical Data -
Information Retrieval

1 Introduction

Large Language Models (LLMs) have become the cornerstone of many applications, from chatbots to advanced
analytics. This widespread adoption is fueled by their remarkable ability to learn from in-context information, a property
often referred to as "in-context learning" ({17]). Furthermore, their proficiency in learning from just a few examples, a
concept known as few-shot learning, makes them highly effective for tasks like information retrieval and analysis ({18]).

A popular and robust method for enhancing LLMs is Retrieval-Augmented Generation (RAG) ((19]). This technique
works by first retrieving a set of relevant documents from a knowledge base in response to a user query. The LLM is
then prompted to generate an answer based on both the query and the retrieved knowledge. A significant advantage
of RAG is its structure-agnostic nature, allowing it to operate effectively on raw, unstructured data stored as vector
embeddings, such as plain text.

While RAG has proven effective, its application to diverse data structures is an active area of research. For instance,
studies have explored its use with graph-based data, which is common in social networks and other complex systems. A
breadth of recent work ((3}{6)[7]]), propose methods for performing RAG over knowledge graphs, explicitly capturing
details like adjacency and centrality. While this approach is well-suited for applications where information is naturally
represented as a graph, it can be computationally intensive and may be considered overkill for other hierarchical
structures, which are often a subset of graphs (e.g., tree-like folder structures).

“Work done during rotation in Connected Vehicle Experience Research, General Motors


Technical Report A PREPRINT

This brings to light a critical question in RAG: what is the most efficient way to capture knowledge? While LLMs can
generate high-quality responses from raw, explicit knowledge, it remains to be seen whether the same quality can be
achieved by implicitly capturing that knowledge.

A critical technical step in applying RAG to such structures is linearization, which converts the multi-dimensional,
nested hierarchy (like a file tree) into a one-dimensional sequence of discrete documents. These documents, which
serve as the chunks of information stored in the vector database, are constrained by a set limit of tokens. Thus, when we
propose linearizing the tree by generating implicit knowledge, we are employing a method to create these optimized,
token-limited documents for the tree’s components by distilling their core content and context, rather than using raw
data. An example of generated implicit knowledge can be found in Appendix[C]

Generate Implicit

\
i}
Knowledge
i
\

(custom defined templates /
attributes)

File / Folder
Content

Associated
Metadata

Retrieve Relevant Content
of the Node's Children

Figure 1: Workflow for generating Implicit Knowledge from a given set of hierarchical files

This paper addresses these intersecting challenges within the context of a common problem at GM: enabling an
application to "talk" to unstructured code repositories. Not only are code repositories naturally organized in a
hierarchical, tree-like format, but they are also often raw, unstructured, and not explicitly documented.

Our work makes the following contributions:

1. We propose a novel method for traversing hierarchical structures like trees to generate implicit knowledge
tailored for RAG-type applications.

2. We demonstrate that our implicit knowledge generation method leads to significantly more efficient RAG
usage. Our findings show that our approach yields a comparable quality of responses while generating almost
four times less data in the vector database. This suggests that implicit knowledge may be sufficient and more
efficient for managing complex, structured information.

2 Related Work

Existing graph-based LLM applications can be broadly categorized into two types. The first category focuses on solving
fundamental graph problems, such as those related to node properties (e.g., degree, centrality) and relational properties
(e.g., shortest path between two nodes) [3] [6] [7]. The second category, which is more relevant to our work, evaluates the
ability of LLMs to "talk to a graph" by answering natural language queries about its content and structure ((8} {4]).

Our work shares a conceptual similarity with the latter category, particularly the bottom-up knowledge aggregation
approach proposed by [4]. However, our primary objective is to distill knowledge from a hierarchical structure for a
more efficient Retrieval-Augmented Generation (RAG) pipeline. In contrast, their method focuses on retrieving similar
embedding vectors and summarizing clusters to provide answers, which represents a different approach to a similar
problem.

Another line of work centers on the construction of Knowledge Graphs (KGs) from text and using LLMs for subsequent
reasoning over these KGs ({9]{10}). These methods primarily address the challenge of structuring unstructured text into


Technical Report A PREPRINT

a graph format, whereas our work begins with an existing hierarchical structure and focuses on creating an optimal
representation of its knowledge for efficient retrieval.

Another line of work relevant to our research is knowledge distillation, which focuses on compressing knowledge from
a large model into a smaller one. Our concept of generating implicit knowledge shares commonality with this field.
For instance, some methods create explicit frameworks, such as using "teacher-models" to improve Chain-of-Thought
reasoning ({14]}) or to distill knowledge for unlabeled data ({15]). Other works use customized loss functions to extract
subtler forms of implicit knowledge from a model’s activations or internal states ({16]). While these methods primarily
focus on model-level distillation, they underscore the broader principle that valuable knowledge can be distilled into a
more compact and efficient form. Our work applies this fundamental idea to the realm of RAG, demonstrating how
knowledge can be distilled from a data source itself, rather than from a model.

Finally, it has been demonstrated that the performance of RAG-based approaches can be significantly affected by the
number of documents stored in vector databases, often leading to performance degradation as the context size increases
((11) [12]). This has spurred research into methods that optimize the retrieval process and manage large contexts more
effectively, such as the work on long-context RAG by Jiang et al. ((13]). This body of literature underscores the
critical need for developing efficient and scalable methods to manage the knowledge used in RAG frameworks. Our
work on generating implicit knowledge addresses this need directly by enabling more concise and effective storage of
information, thus mitigating the very performance issues identified in these studies.

3 Generating Implicit Knowledge for Trees

A key challenge with using Retrieval-Augmented Generation (RAG) on hierarchical structures is that simply linearizing
a tree and capturing raw data does not account for the holistic semantic information contained within the entire structure.
A practical example is a "folder-level" query, where naively storing raw information can lead to a loss of contextual
semantics. Additionally, raw data, especially code, can be extremely long-winded and token-heavy, making it inefficient
to capture in its entirety.

To mitigate these problems, we propose a novel method to iterate through hierarchical structures to generate implicit
knowledge on trees. We first traverse to the leaf-level of a given tree and capture the "templated" knowledge using a
large language model (LLM). We generate implicit knowledge for all leaf nodes in this manner. Subsequently, we
traverse to the parent node of each leaf and generate a higher-level summary by using all the implicit knowledge
previously generated for its children. This ensures that the parent node has a holistic understanding of its entire sub-tree.
The algorithm for traversing and generating implicit knowledge is given by Algorithm|[I]

Algorithm 1: Bottom-Up Knowledge Aggregation

Input

Tree structure T’ representing the file hierarchy

Large language model L

Prompt template for leaf nodes Prag

Prompt template for parent nodes Pparent

Output : Knowledge base kK B

begin

foreach leaf node i € T do

Generate implicit knowledge JmpK nowledge; using Preap and LD;

Store ImpK nowledge; in K B;

end

while not reached root of T do

foreach parent node N at current level do
Collect {ImpK nowledge;} from all child nodes i of N;
Generate implicit knowledge ImpK nowledgey using Pyaren and L;
Store ImpKnowledgeyn in KB;

end

Move one level up in T;

end

end

The specific content of this template can vary depending on the use case for a given hierarchical structure; the template
we utilized is provided in Appendix[A]


Technical Report A PREPRINT

“Folder” Root

mM, -par,

Merge
—————>
"Folder" oe Traverse
Sa

Create (LLM

Add Context

mM, .par, m, .par,

Figure 2: Workflow for generating Implicit Knowledge from a given set of hierarchical files

4 Experiment Setup

4.1 Data Setup

For our experiments, we use a proprietary, unstructured code repository from GM. The repository holds various files,
predominantly MATLAB Simulink scripts designed for vehicle simulation in different types of environments. The data
is particularly challenging because it does not represent a single, organized software package. Instead, it comprises a
diverse set of utilities and simulation files collected by a team over several years, often stored in a decentralized and
ad-hoc manner. While the files themselves are arranged in a hierarchical (tree-like) structure, the repository as a whole
lacks a cohesive, logical organization, making it an ideal real-world test case for our method.

4.2 Setup of Methods

We compare two distinct methods. Our baseline method establishes a conventional RAG pipeline by directly indexing
and storing the raw content of the code files and their associated GitHub metadata (e.g., file paths, names) in a vector
database.

For our proposed method, we first generate implicit knowledge by programmatically traversing the repository using the
novel approach detailed in Section 4.2. This generated knowledge, rather than the raw files, is then stored and indexed
in the vector database.

4.3 Qualitative Evaluation

We qualitatively constructed a small yet representative test dataset consisting of 10 questions with corresponding
ground-truth answers. To establish the fidelity of the evaluation standard, the ground-truth answers were rigorously
defined and validated by Subject Matter Experts (SMEs). The SMEs ensured that each answer was factually correct,
complete, and accurately represented the information contained within the code repository. The questions were designed
to probe the system’s ability to retrieve and synthesize information at both the file-level and folder-level of the hierarchy,


Technical Report A PREPRINT

and the final results were then quantitatively evaluated using the set of metrics detailed in Section [4.4] A sample
question-answer pair is presented below:

Question: Is there a file that performs Aerodynamic simulation over Track and Coast data?

Answer: Yes, the file AeroMapCompare.m plots "Track" and "Coast" data, representing different
driving scenarios. It specifically visualizes various ride height metrics in millimeters using subplots.

For a complete list of questions, please refer to Appendix |B]

4.4 Evaluation Metrics

We evaluate both methods using three popular string-matching metrics to measure how closely the LLM-generated
answers align with the ground-truth answers. These metrics are commonly used in natural language generation and
summarization tasks ({19]).

4.4.1 BLEU-1

The BLEU-/ (Bilingual Evaluation Understudy) score is a metric used to evaluate the quality of text generated by an
LLM by comparing it to one or more human-written reference texts. The score ranges from 0 to 1, with a higher score
indicating a greater degree of similarity between the generated and reference texts.

The BLEU-1 score is a precision-based metric that measures the overlap of unigrams between a candidate (generated)
text and a set of reference texts. Its formula is a modified form of unigram precision.

The core of the BLEU-1 score is calculated as follows:

il,
BLEU-1 = BP - exp (>: Wp log m)

n=1

Where:

¢ BP is the brevity penalty, a factor that penalizes short generated sentences. It’s calculated as:

1 ife>r
BP=
{eer ife<r

where c is the length of the candidate sentence and r is the effective reference corpus length.

* wr, is the weight for each n-gram. For BLEU-1, we only consider unigrams, so w, = 1 and all other weights
are 0.

* pr is the modified n-gram precision. For BLEU-1, this is p;, the modified unigram precision.

4.4.2 E-F1 Score

The E-F/ (Extraction F1) score is a metric that evaluates the token-level overlap between a model’s generated answer
and a reference answer. It’s a more granular measure than BLEU-1, focusing on how well the model extracts the key
entities or phrases from the source text.

* Precision (P): This metric answers the question, "Of all the tokens the model generated, how many were
correct?" It is calculated as the number of correct tokens divided by the total number of tokens in the generated

answer.
Number of Correct Tokens

~ Total Tokens in Generated Answer
* Recall (f): This metric answers the question, "Of all the tokens in the reference answer, how many did the
model correctly generate?" It’s the number of correct tokens divided by the total number of tokens in the

reference answer.
Number of Correct Tokens

~~ Total Tokens in Reference Answer

The E-F1 score combines precision and recall into a single metric, providing a balanced measure of both correctness
and completeness. The formula is:


Technical Report A PREPRINT

The score works by counting the number of common tokens between the generated and reference answers. It then
calculates precision and recall based on this count, and finally, it computes the harmonic mean. A higher F1 score
indicates a better balance between not generating irrelevant words (high precision) and generating all the necessary
words (high recall) from the reference.

4.4.3. EM Formula

The formula for token-level EM is based on a simple ratio of matching tokens. It is calculated by dividing the number
of tokens in the reference that are found in the prediction by the total number of tokens in the reference.

Number of reference tokens found in prediction
EMtoken =

Total number of tokens in reference
5 Results

The results of our comparative analysis are presented in Table 6.1. The table shows the mean scores for three evaluation
metrics—BLEU-1, E-F1l, and EM—across file-level, folder-level, and overall evaluation tasks for both the baseline and
our proposed method.

Table 6.1 summarizes the results for the various metrics described in the previous section. We also calculate the amount
of documents generated in the vector database in Table 6.2.

File Folder Overall
Bleu-l E-Fl EM | Bleu-l E-Fl EM | Bleu-1 E-Fl EM
Baseline 0.21 0.33 0.61 | 0.13 0.23 0.54 | 0.17 0.28 0.58
Proposed | 0.21 0.33. 0.60 | 0.15 0.28 0.81 | 0.18 0.30 0.71
Table 1: Results categorized across File, Folder & Overall questions across our metrics

As shown in Table 6.1, the proposed method consistently performs similar to or outperforms the baseline on the
E-F1 and EM metrics across all evaluation levels. This finding indicates that our approach, by generating concise and
semantically focused implicit knowledge, is highly effective at extracting the key entities and phrases required for
accurate question answering.

In addition to its superior performance on key token-level metrics, our proposed method also demonstrates substantial
gains in efficiency. By generating implicit knowledge, our approach requires over 3.5x fewer documents in the vector
database compared to the baseline method. This result indicates that implicit knowledge may be a more resource-efficient
and scalable solution for managing complex, hierarchical structures.

# of documents generated |

Baseline 490

Proposed 156

Table 2: Number of documents generated for the vector database by each approach

6 Discussion and Limitations

Our experiments provide compelling evidence for the efficacy of using a bottom-up knowledge aggregation approach
on hierarchical structures. Our findings demonstrate that generating and utilizing implicit knowledge for RAG tasks not
only yields comparable or superior performance but also offers significant gains in efficiency.

6.1 Implicit Knowledge vs. Raw Data

As our experiments on code repositories show, models operating on implicit knowledge, which is distilled from raw
data, achieve a higher quality of response on key token-level metrics. Specifically, our method demonstrates superior
performance for "folder-level" questions, which require a holistic understanding of information across multiple files.
This suggests that the LLM-generated summaries effectively capture the contextual and semantic relationships that are
often lost when raw, verbose files are simply linearized.


Technical Report A PREPRINT

Performance Comparison (File vs Folder)

File Folder

Mmm Baseline
lm Proposed

0.814

0.614 osog

0.332 0.328

0.210 0.208

BLEU-1 EFL —EM

BLEU-1 EF1 EM

Figure 3: Performance of baseline & proposed methods on File-level and Folder-level questions

Furthermore, this process proved to be substantially more efficient. Our method generates almost four times fewer
documents in the vector database while maintaining performance parity. This result directly addresses a major limitation
of RAG pipelines, where an increasing number of documents can lead to performance degradation. By providing the
model with distilled, relevant context, our approach optimizes RAG usage and leads to a more scalable solution.

However, these findings also raise several important open questions for future research:

1. Can this implicit knowledge generation method be generalized to other non-hierarchical structures, such as
knowledge graphs or relational databases?

2. Is there an optimal representation for implicit knowledge that balances efficiency and performance for a given
LLM? In this paper, we only explored a simple Markdown-based structure.

3. Is linearization, even with our novel aggregation method, the most effective way to capture information from
hierarchical structures?

6.2 Generalizing to Graph-Based Tasks

A significant body of literature evaluates Graph-based RAG tasks on structural questions pertaining to relationships,
such as degree and centrality ((3][6]{7]). As a future direction, we propose exploring whether our implicit knowledge
generation approach can be adapted to these more complex data structures ({[20]). While a direct hierarchical traversal
may not be applicable to arbitrary graphs, the underlying idea of creating holistic, aggregated knowledge from
interconnected nodes could still be effective. We would ideally explore benchmark datasets used in graph-based RAG
to evaluate if generating implicit knowledge improves performance on these tasks.

6.3 Implicit Knowledge as a General Paradigm

Our work provides a proof-of-concept that LLMs do not require raw data; they simply require high-quality implicit
knowledge. We have demonstrated that this leads to equal, if not superior, performance while utilizing a significantly
smaller amount of data. This finding helps address the challenge of performance degradation that LLMs often face
when provided with larger amounts of context ({[1][21]). Our experiments, however, are limited to a specific code
repository scenario.

A key question for future work is to investigate the generalizability of this paradigm: will generating implicit knowledge
lead to similar performance for a broader range of tasks and domains? We also aim to formally quantify the trade-off
between the level of implicit knowledge and the final performance of the RAG pipeline.

7 Conclusion

In this work, we introduced a novel approach to optimizing Retrieval-Augmented Generation (RAG) pipelines for
hierarchical data, with a focus on unstructured code repositories that naturally form tree-based structures. Instead of


Technical Report A PREPRINT

flattening the hierarchy, our method applies a bottom-up traversal and aggregates implicit knowledge from the tree,
providing a compelling alternative to traditional RAG. Our results highlight two major advantages over the baseline:
reduced semantic loss and greater efficiency. First, our implicit knowledge approach produced responses of comparable
or superior quality on key token-level metrics, showing that high-level knowledge can be sufficient for both file-level
and folder-level queries. Second, our method was significantly more efficient, requiring over 3.5x fewer documents in
the vector database. This study opens several promising avenues for future research, including extending the implicit
knowledge paradigm to other structured domains such as graphs, and further exploring the balance between knowledge
distillation and model performance. Ultimately, our findings suggest that structure-aware RAG methods can provide a
path toward more accurate, efficient, and scalable retrieval systems.


Technical Report A PREPRINT

A Prompt Template

A.1 Template for Generating Root-level (file-level) Knowledge

## Task Overview

You task is to generate a .md file from a single file (e.g., .m, .par, .py, etc.)
that includes a script that

may include functions defined with related I/O variables and invoked in the script
» parameters,

and classes defined with methods used to create objects of the class, and perform
other operation of objects of the class.

Generate metadata for the asset using the following information that the asset
contains:
“¢“fcontext}‘ ‘*

Use the below information to generate the metadata:

If the file appear to be a script or a function (e.g., .m, .py, etc..), consider
the following classification of a script’s maturity level when providing your
answer:

1. Ad-hoc/Initial Stage:
a. Description:
These are scripts created for immediate, specific tasks. They often
lack formal structure, documentation, and error handling.
They might be "one-off" solutions.
Very similar to a prototype stage.
b. Characteristics: Minimal or no comments.
Limited error checking.
Often not version-controlled.
Highly dependent on the creator’s knowledge.
2. Proof of Concept/Experimental Stage:
a. Description:
Scripts developed to validate a specific idea or approach.
They demonstrate functionality but may not be optimized for
performance or reliability.
This is where the script is being tested for viability.
b. Characteristics:
Basic documentation.
Some level of error handling.
May be subject to frequent changes.
Used for testing and experimentation.
3. Development/Refinement Stage:
a. Description:
Scripts that are actively being improved and refined.
They start to incorporate better coding practices, error handling, and
documentation.
This is where the scripts start to become more stable.
b. Characteristics:
Improved documentation and comments.
Robust error handling and logging.
Version control implementation.
Increased code modularity and reusability.
4. Stable/Production Stage:
a. Description:
Scripts that are considered reliable and ready for regular use.
They have undergone thorough testing and are well-documented.
They are integrated into established workflows.
b. Characteristics:
Comprehensive documentation.
Extensive error handling and logging.
Rigorous testing and validation.
Integration with other systems and processes.
5. Optimized/Mature Stage:
a. Description:



Technical Report A PREPRINT

Scripts that are continuously monitored and optimized for performance,
efficiency, and maintainability.
They are subject to ongoing improvements and updates.
Similar to the CMMI optimizing level.
b. Characteristics:
Performance monitoring and optimization.
Automated testing and deployment.
Continuous integration and continuous delivery (CI/CD).
Proactive maintenance and updates.
It’s important to note that the maturity of a script or a function invoked ina
script is often context-dependent.
So, if you can try to infer the context of the script. For a simple script used
for personal tasks may never need to reach the "optimized" stage,
while a script used in a critical production environment will require a high level
of maturity.

A.2 Template for Generating Parent-level (folder-level) Knowledge

## Task Overview

You task is to generate a .md file for the folder, by merging several .md files,
each representing an item in the folder.

It is possible that some .md files included in the folder are describing .m, .par,

-py files while other .md files are describing folders included

in this folder.

When you generate the single .md file please consider if the contents describe in
the .md files (one per item in the folder) are somehow

interrelated or just a loose collection of things with a similar them.

Generate metadata using the following information:
“«<f{context}‘ ‘*

## Input

A set of files with .md extension each file summarizing an item in the folder

## Output Requirements

Generate a structured meta-description with the following sections:

1. Folder Name

2. Folder Location in the repo (tree-structure)

3. An aggregated summary of the folder contents

Here is an example of the summary:

"This folder contains data and scripts related to the analysis of a Porsche Taycan

Turbo S’s lap record at the Nurburgring. The primary focus is on extracting and
analyzing

vehicle speed and lap time data from video footage of the lap.

The analysis involves several steps, including video pre-processing,

frame extraction, optical character recognition (OCR) for data extraction,

and data plotting for insights into vehicle performance."

4. An list of the folder types of contents (e.g., data, scripts, functions,
classes, models, etc..)

5. A more detailed description of the folder contents. If the folder has scripts,
describe the scripts one by one,

if the folder has data describe the data, etc...

6. A set of references (e.g., from tools documents that are used to run the script
)

7. The folder creator name

8. The folder creation date

10



Technical Report A PREPRINT

B List of Questions

File-level Questions

What does the file Lap_Video_to_Speed do?
Ts there a file that performs Aerodynamic simulation over Track and Coast data?
What is the name of the file that measures Brake Rotor Temperature as a function of time?

What parameters are used to plot the Motor Torque in MotorCurvePlotTool.m?

woe OM me

What symbols are used to calculate the peak tire slip based on MF-Tire?
Folder-level Questions

1. What information does the folder CarSim CompTire contain?

2. Are there any scripts to evaluate Suspension Tools?

3. Do any scripts make use of the file Read_TIR_2CompTire_Func.m?

4. What is the folder LapTimeBenchmarkPlotter folder plotting?

5. What is the difference between the folder for SuspentionDoeCalculator and RollCenterModifier?

C Example of Generated Implicit Knowledge

C.1 Raw Node Information

For demonstration purposes only the first 100 lines are added. The file has 1200+ lines total.

classdef Core < handle

% Core gmLTS object that groups other gmLTS classes into a single
4 object for ease of use. This class is designed to handle the

% interactions between the other gmLTS classes to minimize their
% interdendency where possible.

hh

h

properties
OptSettings (1,1) gmLTS.OptimizationSettings % Structure of optimizer
settings
SimInterfaceObj (1,1) % Simulation execution interface

IterationTable table = table % collection of trial parameters and
results
ResultsDirectory (1,:) char = pwd 4 where to save session and

timeseries data

PlotProgress (1,1) logical = false % flag to plot iteration progress
during optimization

SaveDataWithSession (1,1) logical = false % flag to save or not save
timeseries data when session is saved

MuAdjustVector (:,1) double % staging area for distance-based mu
vector

ObjSaveName char % Name used for object file when calling the
SaveSession method

end

properties (SetObservable, AbortSet) % enables listeners for changes
DriverObj (1,1) gmLTS.DriverModel = gmLTS.DriverModel % Calibrations
and settings for driver model
VehicleObj (1,1) gmLTS.Vehicle = gmLTS.Vehicle % Contains properties
required by SpeedPlan
PathObj (1,1) gmLTS.DriverPath = gmLTS.DriverPath("") % Target path
object required by SpeedPlan
SpeedPlanObj (1,1) gmLTS.SpeedPlan = gmLTS.SpeedPlan % Speed plan
object
end

11



Technical Report A PREPRINT

properties (SetAccess = private)
VerConstants (1,1) gmLTS.VerConstants % gmLTS version constants used
for compatibility checking
EnvInfo (1,1) gmLTS.EnvInfo % Collector for machine and environment
info
ToolDir char % Location of gmLTS tool
end

properties (Dependent)
BestIteration (1,1) uinti6 % fastest run from .IterationTable
Ver (1,1) string 4% created from version numbers

end

properties (Access=private)
CarSimInputPar=’ Import_9b5ab993 -2ddf -4397-a57b-cScb4bae380b. par’
CarSimOutputPar=’ Export_327261fc -589c-4785 -98a9 -32d51f64b3a3.par’
CarSimMiscPar=’ GenVS_3e2f6d6c -c91le -4ea4-a295 -6bf7f£db25d45. par’
CarSimSpdCtrlPar=’SpdPath_cc9b5b3a -18f7 -4847 -9f7a-9a6789a3d7b5. par’
end

C.2 Generated Implicit Knowledge

# Metadata for File: +gmLTS/@Core/Core.m
QO. **Name of File:** Core.m

1. **Creator or Author Name and Information**
- **xAuthor Name:** XXX
- **xAuthor Email:** XXX

2. **Explanation:** The metadata generated is based on the content and structure
of the provided MATLAB class file, ‘Core.m‘. The analysis focuses on the class
definition, properties, methods, and the context provided by the latest commit
details. The classification of the script’s maturity level is inferred from the

coding practices, documentation, error handling, and the overall structure
observed in the script.

3. **Type of Asset:** This is a class.

4. **A summary of I/Os required to run**
- **Required Parameters/Objects to Instantiate the Class: **
- Optional arguments for configuration settings such as ’driverSettingsFile’,
*targetPathObjectFile’, ’vehicleSettingsFile’, ’optSettingsFile’, °
resultsInterface’, ’simInterface’, ’rootModel’.
- **Example of Instantiation:**
‘¢¢matlab
g = gmLTS.Core(’targetPathObjectFile’,’MRC-Jake’,...
>vehicleSettingsFile’,’vehicleZYA’,...
>driverSettingsFile’,’driverZYA’,...
7optSettingsFile’,’optsettingsZYA’,...
>rootModel’,’GMLTS_ZERV ’) ;

5. **Compatibility:** The class is designed to work within the MATLAB environment,
specifically tailored for lap time simulation and optimization tasks. It
likely interfaces with other MATLAB scripts, functions, and toolboxes relevant
to vehicle dynamics simulation.

6. **Category:** Vehicle Dynamics Simulation and Optimization

7. **Description and Features: **

12



Technical Report A PREPRINT

Sr

oF

- The ‘Core‘ class is part of the gmLTS (General Motors Lap Time Simulation)
framework, designed to group other gmLTS classes into a single object for
ease of use. It handles interactions between different gmLTS classes to
minimize their interdependency. The class provides functionalities for
loading configuration settings, resetting iterations, cleaning paths,
creating new iterations, creating speed plans, finding the best candidate,
loading candidates, running iterations, and optimizing lap times. It also
includes private methods for internal operations such as calculating tire
force margins, finding failed segment indices, and creating composite best
plans.

**Keywords:** Lap Time Simulation, Vehicle Dynamics, Optimization, MATLAB,
gmLTS, Simulation Interface

*xxMaturity Level:** Development/Refinement Stage
- **Explanation:** The ‘Core‘ class demonstrates characteristics of being in
the development/refinement stage. It includes improved documentation and
comments throughout the class, robust error handling, and a structured
approach to defining properties and methods. The class is designed with
modularity in mind, allowing for easy integration and reuse within the gmLTS
framework. The presence of input parsing, default settings, and the ability
to handle various configuration options further indicate a move towards a
more stable and refined codebase. However, the class may still be subject to
changes and improvements as it is actively developed and refined for better
performance, reliability, and integration with other systems or processes.

13



Technical Report A PREPRINT

References

[1] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: A
comprehensive survey. IEEE Transactions on Knowledge and Data Engineering, 2024.

[2] Christos Xypolopoulos, Guokan Shang, Xiao Fei, Giannis Nikolentzos, Hadi Abdine, Iakovos Evdaimon, Michail
Chatzianastasis, Giorgos Stamou, and Michalis Vazirgiannis. Graph linearization methods for reasoning on graphs
with large language models. arXiv preprint arXiv:2410.19494, 2024.

[3] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language
models solve graph problems in natural language? In Advances in Neural Information Processing Systems,
volume 36, pages 30840-30861, 2023.

[4] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. Raptor:
Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning
Representations, 2024.

[5] Wei Liu, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang, Haiyan Zhao, Zhi Jin, and Qianxiang Wang. Graphcoder:
Enhancing repository-level code completion via code context graph-based retrieval and language model. arXiv
preprint arXiv:2406.07003, 2024.

[6] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large language
models. arXiv preprint arXiv:2310.04560, 2023.

[7] Xin Li, Weize Chen, Qizhi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Chen Qian, Yiwei Wei, Chuan Shi, Zhiyuan
Liu, et al. Can large language models analyze graphs like professionals? a benchmark, datasets and models. In
Advances in Neural Information Processing Systems, volume 37, pages 141045—141070, 2024.

[8] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan
Hooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. In
Advances in Neural Information Processing Systems, volume 37, pages 132876—132907, 2024.

[9] Mufei Li, Siqi Miao, and Pan Li. Simple is effective: The roles of graphs and large language models in
knowledge-graph-based retrieval-augmented generation. arXiv preprint arXiv:2410.20724, 2024.

[10] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha
Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach to
query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.

[11] Shahar Levy, Nir Mazor, Lihi Shalmon, Michael Hassid, and Gabriel Stanovsky. More documents, same length:
Isolating the challenge of multiple documents in rag. arXiv preprint arXiv:2503.04388, 2025.

[12] Daniel Warfield and Benjamin Fletcher. Do vector databases lose accuracy at scale? EyeLevel.ai Blog, October
2024. URL: https://www.eyelevel.ai/post/do-vector-databases-lose-accuracy-at-scale

[13] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-
context Ilms. arXiv preprint arXiv:2406.15319, 2024.

[14] Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit
chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.

[15] Yuzheng Wang, Zuhao Ge, Zhaoyu Chen, Xian Liu, Chuangjia Ma, Yunquan Sun, and Lizhe Qi. Explicit and
implicit knowledge distillation via unlabeled data. In ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.

[16] Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, and Furu Wei. Direct preference knowledge distillation
for large language models. arXiv preprint arXiv:2406.19774, 2024.

[17] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu,
Tianyu Liu, et al. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.

[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind

Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In
Advances in Neural Information Processing Systems, volume 33, pages 1877-1901, 2020.

[19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, and Haofen Wang.
Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.

[20] Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, and Chao Huang. A survey of large language models for
graphs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages
6616-6626, 2024.

14


Technical Report A PREPRINT

[21] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Scharli, and Denny
Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on
Machine Learning, pages 31210-31227. PMLR, 2023.

15
