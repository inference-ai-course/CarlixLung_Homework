2510.10062v1 [cs.CL] 11 Oct 2025

e
e

arXiv

HUME: MEASURING THE HUMAN-MODEL
PERFORMANCE GAP IN TEXT EMBEDDING TASKS

Adnan E] Assadi Isaac Chung Roman Solomatin
Carleton University Zendesk SberAI
Niklas Muennighoff Kenneth Enevoldsen
Stanford University Aarhus University
ABSTRACT

Comparing human and model performance offers a valuable perspective for under-
standing the strengths and limitations of embedding models, highlighting where
they succeed and where they fail to capture meaning and nuance. However, such
comparisons are rarely made, as human performance on embedding tasks is difficult
to measure. To fill this gap, we introduce HUME: Human Evaluation Framework
for Text Embeddings. While frameworks like MTEB provide broad model evalua-
tion, they lack reliable estimates of human performance, limiting the interpretability
of model scores. We measure human performance across 16 MTEB datasets span-
ning reranking, classification, clustering, and semantic textual similarity across
linguistically diverse high- and low-resource languages. Humans achieve an av-
erage performance of 77.6% compared to 80.1% for the best embedding model,
although variation is substantial: models reach near-ceiling performance on some
datasets while struggling on others, suggesting dataset issues and revealing short-
comings in low-resource languages. We provide human performance baselines,
insight into task difficulty patterns, and an extensible evaluation framework that
enables a more meaningful interpretation of the model and informs the development
of both models and benchmarks. Our code, dataset, and leaderboard are publicly

available atthttps://github.com/embeddings-—benchmark/mteb

S> |_| fe . me att all (<300M)
ai i $8 Qwen FE 9% Qwen I Medium G00M-1B)
m — 78.3 78.2 77.6 715 769 783 Be a =. Sa
A 61.
& 60
Z
is]
Oo
a
vo
2 40
)
5
<
20
0
» ¥ 5 LY x ‘\ y 1
2 © S < x eo J L
Rae oe se we oe ove e é © roe eon we - we we
On tot ~ e wy RG 2 NS oe oe SS
fh P_® S Be 4 Rs oe gs
ro F ow S ow we 4 a go” a x Rd on »
go x“ o Bs x

Figure 1: Overall ranking of human performance versus 13 embedding models across 16 tasks.
Human annotators achieve 4th place with a score of 77.6, demonstrating competitive but not domi-
nant performance. The ranking reveals significant variation in model performance across different
parameter scales and architectures. Darker shades of blue means a larger model.


1 INTRODUCTION

Embedding models are central to modern NLP systems, powering applications such as search, rec-
ommendation, semantic analysis, and information retrieval. Many benchmarks test the performance
of embedding models, with the most comprehensive offering a diverse suite of tasks that test their
Despite these advances, the interpretation and quality of these scores are often unclear as it is absent
of human performance references. Current metrics define performance in terms of theoretical maxima
(e.g., MAP = 1.0) that assume perfect consensus on task outcomes. However, many NLP tasks
inherently involve ambiguity and disagreement (Plank| {2022), making a model’s score difficult to
meaningfully interpret without reasonable references. For instance, if a model achieves 0.85 MAP
in reranking, it is unclear whether this should be considered strong, mediocre, or beyond what
annotators typically achieve. This disconnect highlights the need for human-centered evaluation that
contextualizes benchmark results.

To address this, we introduce HUME: a Human Evaluation framework for text embedding tasks.
HUME evaluates annotator performance across four task categories: reranking, classification, clus-
tering, and semantic textual similarity (STS), using 16 diverse datasets from the Massive Text
Embedding Benchmark (MTEB) (Muennighoff et al.||2022), adapted for human annotation feasibility.
Through multi-annotator experiments, we analyze task difficulty, quantify variation across humans,
and compare results directly against state-of-the-art embedding models.

Our contributions are threefold: (1) a generalizable framework for human evaluation of embedding
tasks, (2) empirical evidence of how humans perform across diverse datasets and task types, and
(3) comparative analysis of models and humans that highlights strengths, weaknesses, and ambiguities
in both benchmarks and models that yield actionable insights. Together, these contributions establish
a foundation for human-aligned evaluation of embedding models and guide future benchmark design.

2 RELATED WORK

2.1 TEXT EMBEDDING MODELS AND MTEB

Text embedding models map natural language into dense vectors that capture semantic information.

They have progressed from static embeddings (Mikolov et al] to
contextual encoders Liu et al.[/2019) and more recently to models optimized for
embeddings like Sentence-BERT (Reimers & Gurevych||2019), E5 (Wang et al.|/2022), and GTE
let al.|[2023), powering applications such as search, classification, and clustering.

To evaluate these models across its diverse use-cases, MTEB (Muennighoff et al.|/2022

et al.| |2025) has risen as the de facto benchmark framework for embeddings and consolidates
evaluation across diverse tasks and datasets. Despite its breadth and community-driven extensions —
spanning multilingual, multimodal, and domain-specific variants (Xiao et al.

2025}|Tang & Yang}|2025 2024 2024} |Enevoldsen et al.||2024
etal. 2024 -MTEB Tacks human

performance baselines, making it difficult to contextualize model achievements.

2.2 HUMAN EVALUATION IN NLP

Human evaluation is well established in NLP, especially for generative tasks like machine translation
(Graham et al.} [2013), summarization (Fabbri et al. 2021), and dialogue (Gupta et al.| [2019). In
contrast, embedding-based tasks have relied almost exclusively on automated metrics, with little
attention to human baselines.

In information retrieval, initiatives such as TREC (Voorhees & Tice} |2000) collect human relevance

judgments, but these serve as gold standards rather than benchmarks of human performance under
model metrics (e.g., DCG, MRR). Similarly, GLUE, SuperGLUE (Wang et al.| {2018} /2019), and
MERA (Fenogenova et al.| — a Russian GLUE-like benchmark — report human baselines, but
mainly for classification and reasoning tasks. For embeddings, works like STS
report inter-annotator agreement, yet these are not converted into model-comparable performance


scores. This leaves a gap: human performance on embedding benchmarks such as MTEB remains
largely unquantified.

3 METHODOLOGY

3.1 FRAMEWORK DESIGN

Our framework builds on MTEB by establishing reproducible human evaluation protocols that align
directly with model evaluation. It consists of task-specific annotation interfaces, principled dataset
sampling, a standardized results format, and the use of aligned metrics. This design reveals where
evaluation practices introduce ambiguity or inconsistency.

3.2. TASKS, DATASETS, AND METRICS

Our selection criteria ensures comprehensive coverage across multiple dimensions: (1) linguistic
diversity: including both high-resource languages (English, Arabic, Russian) and lower-resource
languages (Norwegian Bokmal, Danish)|']to test cross-lingual generalization, (2) domain variety:
spanning news, social media, encyclopedic content, scientific literature, and forum discussions
to capture real-world application diversity, (3) construction methods: including both curated
human annotations and synthetic dataset creation to understand how dataset origin affects human-
model alignment, (4) task relevance: using tasks from established benchmarks widely adopted
in the embedding evaluation community, and (5) task complexity variation: ranging from binary
classification to fine-grained similarity judgments. This systematic selection ensures our findings
generalize across the diverse landscape of embedding applications while maintaining direct relevance
to existing evaluation frameworks.

Each task category uses a primary evaluation metric to enable consistent human—model comparisons.
‘Table 1}summarizes the datasets, their domains, and the primary metrics applied. Detailed task

examples are provided in|Appendix B
3.3. ANNOTATION PROCEDURE

All annotations are conducted in Argilla (Argilla Project Contributors} |2025) using task-specific inter-

faces: binary relevance judgments for reranking, categorical labels for classification, free assignment
of cluster IDs for clustering, and 0-5 similarity scores for STS. Sample sizes are balanced by task
complexity: reranking uses 20-49 queries per dataset, classification 40-48 examples, clustering 30
items across clusters, and STS 30-50 sentence pairs.

All annotators were male, aged 20-35, from culturally diverse backgrounds, and experienced NLP
practitioners with native or near-native proficiency in the evaluated languages. They followed
structured guidelines and completed all annotations independently, without access to ground truth or
model predictions. The downsampled task subsets used for comparisons are included in the MTEB

package, with detailed task examples provided in|Appendix B

English tasks were annotated by two annotators to enable agreement analysis. Multilingual tasks were
annotated by a single annotator with corresponding language expertise. Inter-annotator agreement
was assessed with task-appropriate metrics: Fleiss’ kappa 1971) for classification, pairwise

ARI (Strehl & Ghosh} |2003) for clustering, pairwise Spearman correlation (Agirre et al.||2012a) for

STS, and mean Spearman/Kendall’s tau 2008) for reranking. A detailed agreement
analysis is provided in}|Appendix D} This controlled evaluation setup minimizes potential confounds

from dataset variation and enables direct performance comparisons on identical evaluation instances.

3.4 MODEL SELECTION AND EVALUATION

We evaluate 13 embedding models chosen to cover multiple dimensions: (1) parameter scale
(22M-7B), (2) architecture (encoder- and decoder-based), (3) instruction tuning (instruction-tuned
and standard), and (4) multilingual capability (English and multilingual). This selection spans

‘With 0 being "The Left-Behinds" and 5 being "The Winners", we cover eng: 5, ara: 5, rus: 4, dan: 3, nob: 1

according to the 0-5 scale by (Joshi et al.}/2021).


Datasets Description Metrics

Core17, News21, Information retrieval benchmarks (news, documents) MAP,
Robust04 (Weller et al.||2024) MRR@10,
WikiMulti (Enevoldsen et al.} Wikipedia article reranking (eng, dan, nob) nDCG@ 10
2025

Emotion (Saravia et al. 2018) Emotion classification from social media text
Tweet Senti (Barbieri et al.| Sentiment analysis of tweets Accuracy, F1,
2022 Weighted F1

oxicity (cjadams et al.||2019) Toxic content detection
gual Sentimen

Multilin Sentiment classification (ara, eng, nob, rus)

(Mollanorozy et al.|/2023

Classification | Reranking

2p a Entity clustering from Wikipedia
‘5 |ArXiv (arXiv.org submitters] | Academic paper topic clustering (derived labels) V-Measure,
@ (024 ARI, AMI
D [Reddit Forum discussion topic clustering
SIB200 (Adelant et al. 2023) Multilingual sentence clustering (ara, dan, eng, rus)
STSBenchmark (May 2021) General semantic similarity benchmark
~% |SICK-R Marelli et al.|/2014) Semantic relatedness and entailment Spearman,
~ |STS12 (Ag Shared task semantic similarity Pearson

[2022

Multilingual semantic similarity (ara, eng, rus)

diverse computational budgets and training paradigms, capturing the current embedding landscape.

All evaluated models are provided in/Appendix F

All models are evaluated on the downsampled instances annotated by humans, using identical metrics,
protocols, and computational settings. Human performance is computed using the metrics in
mirroring MTEB protocols. For primary analyses, we report MAP for reranking, Accuracy for
classification, V-Measure for clustering, and Spearman correlation for STS.

4 RESULTS AND ANALYSIS

Figure ||provides an overview of human performance relative to 13 state-of-the-art embedding models
across 16 tasks. Human annotators rank 4th overall with a score of 0.776, trailing 3 large models
but outperforming 10 others. This positioning reveals that humans neither represent a performance
ceiling nor a lower bound, but rather occupy a middle ground that varies significantly by task category
and language.

Table 2|presents the aggregated human results compared to selected models in various sizes. Below

we analyze each category and language group in the following sections and refer to}|Appendix Ajfor
the full results on specific task categories.

4.1 PERFORMANCE PATTERNS BY TASK CATEGORY

[Figure 2|shows human performance relative to model performance ranges across all 26 task-language
pairs. Each task shows human performance (point) positioned within the full spectrum from worst to
best model performance (range bars). Humans consistently perform in the upper portion of model
ranges, typically exceeding median model performance (61.5% of tasks) while rarely matching the
best models (15.4% of tasks). Classification tasks show the strongest human performance, with
humans outperforming all models in 3 of 7 tasks, while clustering and reranking reveal consistent
gaps where humans fall short of top-performing models. Detailed gap analysis can be found in

Appendix E


Classification Clustering Reranking STS Overall

Model ara eng nob rus ara dan eng rus dan eng nob ara eng rus

Number of datasets qd) (4) dd) d) d) d) 4 ad) d) 4 dad) d) 4 ad) (26)
all-MiniLM-L6-v2 57.2 58.8 51.7 55.5 35.2 24.5 55.1 31.4 78.4 93.7 71.2 6.2 83.5 33.1 61.9
all-mpnet-base-v2 53.5 62.0 47.0 60.5 21.4 22.9 59.7 36.9 79.0 93.3 80.5 13.2 83.0 42.2 63.4
e5-mistral-7b-instruct 74.5 70.0 70.5 70.0 68.5 76.0 82.7 77.7 90.6 96.4 86.1 16.0 85.9 63.0 78.2
embeddinggemma-300m 71.0 58.6 54.0 73.5 19.2 43.7 65.1 36.9 74.3 86.9 71.4 36.7 69.9 66.2 64.2

gte-Qwen2-1.5B-instruct 75.2 76.5 70.8 74.5 73.7 67.1 75.9 72.2 84.3 95.3 87.8 28.8 84.0 54.2 77.5
jasper_en_vision_language_v1 63.5 87.1 70.5 79.8 64.3 54.7 83.2 43.7 90.1 95.8 90.0 40.9 88.1 69.5 80.1

multilingual-e5-base 75.8 64.7 73.8 77.2 35.9 40.6 45.6 36.2 92.2 94.4 87.5 31.0 85.2 62.7 68.2
multilingual-e5-large 77.0 64.9 75.0 80.0 34.6 31.0 52.5 46.9 95.0 95.3 92.2 33.8 86.3 68.8 70.4
multilingual-e5-small 72.2 62.2 69.2 81.2 35.5 38.0 51.7 59.1 88.6 94.2 88.3 28.8 85.2 60.3 69.0
mxbai-embed-large-v 1 57.2 66.4 52.2 59.0 26.5 34.2 61.9 30.5 90.8 94.5 82.0 12.7 87.6 43.7 66.6
Qwen3-Embedding-0.6B 77.2 74.7 59.8 74.8 78.8 58.5 68.4 68.3 90.0 95.5 83.6 38.0 88.5 60.3 76.8
SFR-Embedding-Mistral 77.5 69.8 68.8 72.5 73.1 71.2 85.1 68.9 89.2 96.3 86.1 15.3 86.4 64.0 78.3
stella_en_1.5B_v5 65.8 84.0 67.0 79.2 36.8 42.6 78.6 46.7 91.7 96.0 88.6 37.2 86.7 62.1 76.9
Human 95.0 70.3 85.0 92.5 76.0 62.7 67.4 68.0 91.4 87.2 89.8 67.5 83.1 58.7 77.6

Table 2: Human performance compared to 13 embedding models across task categories and languages.
Bold indicates highest performance (human or model), underline indicates best model performance.
Humans achieve top performance in 5 of 14 aggregated task-language pairs, particularly excelling in
non-English sentiment analysis and Arabic semantic similarity. Overall results are aggregated over
the 26 task-language pairs.

Classification: Human performance averages 70.3 across classification tasks, with substantial
variation reflecting task-specific challenges. Performance ranges from 45.8 on emotion classification
(& = 0.39, fair agreement) to 95.0 on Arabic sentiment analysis. Models generally exceed human
performance, with the best achieving 87.1 on English tasks. However, humans outperform models on
non-English sentiment analysis, particularly in Arabic (95.0 vs. 77.5) and Russian (92.5 vs. 81.2),
likely benefiting from native cultural and linguistic understanding that current models fail to capture.

Clustering Clustering presents the greatest challenge for humans, averaging 67.4 V-measure with
extreme variation. Near-perfect performance on WikiCities (97.6, ARI = 0.91) contrasts sharply
with poor performance on ArXiv papers (49.2, ARI = —0.001), which uses derived labels from
ArXiv categories. Models consistently outperform humans, with the best reaching 85.1%. The poor
inter-annotator agreement on academic clustering indicates fundamental task ambiguity rather than
human limitation.

Reranking: Humans achieve strong performance in reranking (87.2 average MAP), demonstrating
intuitive document relevance understanding. Models exceed human performance (best: 96.4), but
humans maintain competitive scores with high inter-annotator agreement (p = 0.64-0.85), suggesting
these tasks align well with human judgment.

STS: Human performance averages 83.2 Spearman correlation, with notable dataset-specific variation.
STS12 achieves 91.2 while STS22-Russian drops to 58.7, likely reflecting dataset quality issues
discussed in Models achieve comparable performance (best: 88.5), with moderate inter-
annotator agreement (9 = 0.58-0.77).

These results reveal a critical insight that challenges conventional evaluation paradigms: human
performance variation often reflects task quality rather than human limitations. Tasks with high human
performance and agreement (reranking, toxicity classification) represent well-specified problems
with clear ground truth, while tasks with low human agreement (emotion classification, academic
clustering) may suffer from ambiguous annotation guidelines or inherently subjective judgments.
Models achieve “superhuman” performance by reproducing consistent label patterns from training
data, but this consistency may mask fundamental issues with task specification. Rather than treating
low human performance as a ceiling to surpass, our findings suggest that it often signals the need for
improved task design and clearer annotation frameworks.


STS12 lo
SICK-R @ |

sTSBenchmark e.|

STS

STS22 (eng) fe
STS22 (ara) if @
STS22 (rus) el
News21 e
WikiMulti (dan) le
WikiMulti (nob) |e

Robust04 e |

Reranking

Corel7 e I
WikiMulti (eng) el
WikiCities I e
SIB200 (ara) I @
Reddit @ I
SIB200 (rus) | e
SIB200 (dan) I @

Clustering

SIB200 (eng) | @
ArXiv e |
Multilingual Sentiment (ara) | e

Multilingual Sentiment (rus) l e
Model Range (Worst..Best)

Multilingual Sentiment (nob) Model Median I e

Human (Classification) t ©

Human (STS)

Human (Clustering)

Human (Reranking) | e

Human (Other) be

Tweet Senti
Multilingual Sentiment (eng)

Toxicity

Classification

Emotion

0.2 0.4 0.6 0.8 1.0
Performance score

Figure 2: Comprehensive view of human performance relative to all model performance ranges across
16 tasks by language.

4.2 CROSS-LINGUAL PERFORMANCE ANALYSIS

Arabic exhibits the strongest human advantage: a 67% win rate against the best models and 100%
against the mean, with the largest gap in semantic similarity (67.5% vs. 40.9%, a 26.6-point margin).

Russian and Norwegian also show consistent human superiority in sentiment analysis, where humans
achieve 92.5% and 85.0%, respectively, substantially outperforming the best models. These advan-
tages likely stem from cultural and contextual knowledge that models fail to capture, especially in
lower-resource languages.

English tasks are more balanced, with models generally matching or exceeding humans, reflecting
the language’s dominance in training data. Danish shows mixed outcomes, possibly due to stronger
multilingual coverage for Germanic languages.

4.3, DATASET QUALITY AND EVALUATION CHALLENGES

Our analysis reveals systematic quality issues in several MTEB datasets that fundamentally com-
promise their reliability as evaluation benchmarks. Human performance variation often correlates
with underlying ambiguity rather than genuine human limitations, providing a diagnostic tool for
identifying problematic evaluation targets. Below we highlight two such examples:

Emotion Classification Ambiguity: The emotion classification dataset exemplifies inherent labeling
ambiguity, achieving only fair inter-annotator agreement (« = 0.39) with 52.1% consensus. Real
examples demonstrate the fundamental challenges: “I feel like such a noob when the customers make
really dull and stupid jokes that im supposed to find funny” could reasonably be labeled as sadness (0),
anger (3), or even surprise (5) depending on interpretation. Similarly, “I am feeling very indecisive
and spontaneous” contains mixed emotional states that resist single-label categorization. Sarcastic
expressions like “I got paid too much because I get so many deliveries at work Im feeling a bit shamed”
present surface emotions that differ from intended meaning. When human experts fundamentally
disagree on correct answers for such inherently ambiguous cases, the apparent “superhuman” model


100

80

a
°

Win rate (%)

s
lo}
Win rate (%)

20

English Multilingual

100% 100% 100%

80

Win rate (%)
oa
oOo

Win rate (%)

cS
o

20

vs Best vs Median vs Mean

Figure 3: Human win rates across task categories and languages. Top left: By task category shows
humans perform moderately in classification but struggle in clustering, reranking, and STS against best
models. Top right: English-only vs multilingual tasks reveals humans perform better on multilingual
tasks (29% vs 0% against best models). Bottom left: Performance varies dramatically by baseline
comparison (15% vs best, 62% vs mean models). Bottom right: Language-specific breakdown shows
varying performance across different language codes.

performance (87.1% vs. 45.8% human) likely reflects consistent reproduction of arbitrary majority
label patterns rather than superior emotional understanding.

ArXiv Clustering Breakdown: Academic paper clustering shows complete breakdown of human
agreement (ARI=-0.001), indicating fundamental disagreement about how to categorize academic
papers. Real examples illustrate the core ambiguity: papers like “Self-Supervised Audio-Visual
Representation Learning with Relaxed Cross-Modal Synchronicity“ could legitimately cluster with
computer vision, machine learning, or audio processing groups depending on the annotator’s perspec-
tive on primary methodology versus application domain. “The architecture of innovation: Tracking
face-to-face interactions with ubicomp technologies” spans social science, computer science, and
architecture domains. Such interdisciplinary papers create fundamental disagreement about correct
clustering approaches, with no objectively correct answer. The task uses derived labels from ArXiv
categories, but the core issue is that academic papers often span multiple domains, making any
single clustering scheme inherently ambiguous. The high model performance (84.6% vs. 49.2%
human) suggests that the models are reproducing consistent labeling patterns rather than solving the
fundamental categorization challenge.

High-Quality Benchmark Identification: Conversely, tasks with high human agreement provide
reliable evaluation targets. Reranking tasks achieve strong inter-annotator agreement (p = 0.64—0.85)
with clear performance targets, while toxicity classification shows moderate agreement (« = 0.55)
with 77.8% annotator consensus. These represent genuine evaluation challenges where model
improvements likely reflect meaningful progress rather than pattern matching to flawed labels.

These patterns suggest that apparent “superhuman” model performance often occurs precisely where
human agreement is lowest, indicating that models excel not through superior understanding but
through consistent reproduction of systematic labeling patterns. This raises concerns about the label


quality in embeddings benchmarks, and we encourage future benchmark developers to critically
examine the datasets before including them in a benchmark, potentially using human annotations
framework like HUME. Detailed analysis of specific quality issues is provided in|Appendix C

5 DISCUSSION

5.1 TASK QUALITY AND EMBEDDING EVALUATION RELIABILITY

Models achieve their highest relative performance precisely where human experts show the least agree-
ment. This striking pattern raises fundamental questions about what constitutes reliable evaluation in
embedding benchmarks.

Tasks with high human agreement, such as reranking (9 = 0.64—0.85) and toxicity classification
(«& = 0.55), provide reliable evaluation targets, while low-agreement tasks like emotion classification
(& = 0.39) and ArXiv clustering (ARI = —0.001) suffer from ambiguous guidelines or subjective
judgments. While some cases involve clear quality issues, the mechanisms behind this inverse
relationship remain unclear. Models may excel through superior semantic understanding or by
exploiting systematic annotation patterns.

Cultural factors further complicate evaluation. Humans retain substantial advantages in Arabic
semantic similarity (67.5% vs. 40.9% best model) and multilingual sentiment analysis, revealing
genuine model limitations in cross-cultural understanding.

These findings suggest reliable evaluation depends as much on task quality as model capability.
Rather than automatically treating high model performance as progress, we recommend prioritizing
high-agreement tasks for model and benchmark development, addressing cultural competence gaps,
and critically examining whether apparent model superiority on ambiguous tasks reflects genuine
capabilities or systematic biases in evaluation datasets.

5.2 IMPLICATIONS FOR MODEL DEVELOPMENT AND EVALUATION PRACTICES

Our findings reveal concrete directions for both embedding model development and evaluation
methodology that address the fundamental quality issues we’ve identified.

Prioritize High-Agreement Tasks for Development: Development efforts should focus on tasks
where humans demonstrate both high performance and agreement, as these provide the most reliable
benchmarks for measuring genuine progress. Reranking tasks, with their clear performance targets
and strong agreement (p = 0.64 — 0.85), offer dependable evaluation where the persistent model-
human gap (96.4% vs. 87.2%) represents meaningful challenges requiring better modeling of
relevance relationships. Toxicity classification, despite moderate agreement (K = 0.55), provides
another reliable target with 77.8% human consensus. In contrast, optimizing for tasks with poor
human agreement (emotion classification k = 0.39, ArXiv clustering ARI = —0.001) may lead
models to excel at reproducing arbitrary labeling patterns rather than developing genuine semantic
capabilities.

Address Cultural and Linguistic Competence Gaps: The substantial human advantages in non-
English tasks reveal critical model limitations that scaling training data alone cannot address. Arabic
semantic similarity shows the largest human advantage (67.5% vs. 40.9% best model), while
multilingual sentiment analysis demonstrates consistent human superiority in non-English languages
(95.0% Arabic, 92.5% Russian). These gaps suggest that current models lack the cultural and
contextual knowledge necessary for cross-lingual understanding, requiring architectural innovations
or training approaches that go beyond simple data scaling to capture cultural nuances and contextual
understanding.

Replace Problematic Benchmark Datasets: Our analysis identifies specific datasets that compro-
mise benchmark reliability and should be replaced in future MTEB iterations: emotion classification
(kK = 0.39), ArXiv clustering (ARI = —0.001), and STS22-Russian (systematic parsing artifacts).
These tasks provide unreliable evaluation targets that may mislead model development efforts by
rewarding pattern matching to flawed gold standards. Replacement datasets should demonstrate
reasonable human agreement and clear task specifications, validated through human evaluation studies
before inclusion in benchmark suites.


Develop Culturally-Aware Evaluation Frameworks: Current benchmarks may underestimate
cross-cultural understanding challenges by focusing on tasks where models can leverage English-
centric training advantages. The pronounced human advantages in Arabic and other non-English
tasks demonstrate the need for evaluation frameworks that properly assess cultural and contextual
understanding. Future benchmarks should include more diverse cultural contexts and ensure that high
model performance reflects genuine cross-cultural competence rather than exploitation of training
data biases.

Consider Agreement-Weighted Evaluation: Model performance should be interpreted in light
of human agreement levels, with evaluation practices adapted accordingly. We propose reporting
model performance alongside human agreement metrics to provide proper interpretive context. A
model achieving 85% accuracy on emotion classification (185% of human performance, & = 0.39)
represents a fundamentally different achievement than 85% on reranking (97% of human performance,
p = 0.75). High model performance on low-agreement tasks should be viewed skeptically as potential
artifacts of flawed evaluation targets rather than genuine capability improvements.

5.3. LIMITATIONS AND FUTURE DIRECTIONS

Our study has several limitations. First, while three annotators participated overall, only two
annotations were collected for most tasks, constraining our ability to fully characterize human
judgment distributions. Sample sizes (20-50 instances per task) balance feasibility and reliability
but may not capture the full complexity of human performance variation. Additionally, annotators
were “average” or above-average raters without specialized training; expert annotators could perform
better, particularly on technical tasks like academic clustering. However, we believe that the samples
provided clearly show that decisions on evaluation datasets can be made from even a few samples.

Future work should expand annotator pools to better capture human judgment variability and extend
evaluations to additional MTEB task categories. Crucially, the field needs rigorous research on
designing tasks that balance clear specification with meaningful challenge, avoiding both ambiguity,
which depresses human agreement, and oversimplification, which diminishes the evaluation’s ability
to discriminate model capabilities.

Finally, embedding evaluation should aim not for model “superhuman” performance but for frame-
works that reliably distinguish genuine semantic understanding from sophisticated pattern matching.
Achieving this requires careful attention to task quality, human agreement, and the cultural contexts
shaping language understanding.

6 CONCLUSION

We introduce HUME, a comprehensive human evaluation framework for MTEB, addressing a critical
gap in understanding empirical performance bounds for embedding models. By measuring human
performance across 16 datasets spanning reranking, classification, clustering, and STS, we establish
baselines that reframe how model achievements should be interpreted.

Our findings show that human performance varies substantially by task categories. Tasks with high
agreement provide reliable benchmarks, while low-agreement tasks often indicate design issues
rather than human limitations. Apparent model “superhuman” performance tends to coincide with
low human agreement, suggesting models exploit patterns rather than achieving true semantic
understanding.

HUME provides both a methodology and baseline measurements for evaluating embedding models.
By releasing our resources and protocols, we aim to support the community in adopting human-
grounded evaluation practices and developing models better aligned with human judgment and
real-world language understanding.

REFERENCES

David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O Alabi, Yanke Mao,
Haonan Gao, and Annie En-Shiun Lee. Sib-200: A simple, inclusive, and big evaluation dataset
for topic classification in 200+ languages and dialects. arXiv preprint arXiv:2309.07445, 2023.


Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot
on semantic textual similarity. In Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar,
Yuval Marton, and Deniz Yuret (eds.), *SEM 2012: The First Joint Conference on Lexical and
Computational Semantics — Volume 1: Proceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval
2012), pp. 385-393, Montréal, Canada, 7-8 June 2012a. Association for Computational Linguistics.

URL https://aclanthology.org/S12-1051/

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: a pilot
on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task,
and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval
°12, pp. 385-393, USA, 2012b. Association for Computational Linguistics.

Argilla Project Contributors. Argilla: A collaboration tool for building high-quality ai datasets.

2025. Version as of Sep. 2025; open-source data labeling / curation

platform.

arXiv.org submitters. arxiv dataset, 2024. URL https: //www.kaggle.com/dsv/7548853

Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. XLM-T: Multilingual language
models in Twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language
Resources and Evaluation Conference, pp. 258-266, Marseille, France, June 2022. European

Language Resources Association. URL|https://aclanthology.org/2022.1lrec-1

Daniel Cer, Mona Diab, Eneko Agirre, Ifigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1:
Semantic textual similarity multilingual and crosslingual focused evaluation. In Steven Bethard,
Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens
(eds.), Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),
pp. 1-14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi:

10.18653/v1/S17-2001. URL|https://aclanthology.org/S17-2001/

Xi Chen, Ali Zeynali, Chico Camargo, Fabian Fléck, Devin Gaffney, Przemyslaw Grabowicz,
Scott Hale, David Jurgens, and Mattia Samory. SemEval-2022 task 8: Multilingual news article
similarity. In Guy Emerson, Natalie Schluter, Gabriel Stanovsky, Ritesh Kumar, Alexis Palmer,
Nathan Schneider, Siddharth Singh, and Shyam Ratan (eds.), Proceedings of the 16th International
Workshop on Semantic Evaluation (SemEval-2022), pp. 1094-1106, Seattle, United States, July
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.semeval- 1.155. URL
https://aclanthology.org/2022.semeval-1.155

Mathieu Ciancone, Imene Kerboua, Marion Schaeffer, and Wissam Siblini. Mteb-french: Resources

for french sentence embedding evaluation and analysis, 2024. URL|https://arxiv.org/
abs/2405.20468

cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and nithum.
Jigsaw unintended bias in toxicity classification, 2019. URL |https://kaggle.com/

competitions/jigsaw-unintended-bias-in-toxicity-classification

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In North American Chapter of the Associa-

tion for Computational Linguistics, 2019. URL|https://api.semanticscholar.org/

Corpus1ID:52967399

Kenneth Enevoldsen, Marton Kardos, Niklas Muennighoff, and Kristoffer Nielbo. The scan-
dinavian embedding benchmarks: Comprehensive assessment of multilingual and monolin-
gual text embedding. In Advances in Neural Information Processing Systems, 2024. URL

Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Marton Kardos, Ashwin Mathur, David Stap,
Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala,
Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan

10


Rystrgm, Roman Solomatin, Omer Cagatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita
Sukhlecha, Bhavish Pahwa, Rafat PoSwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras,
Bjorn Pliister, Jan Philipp Harries, Loic Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu,
Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek
Suppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang,
Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale,
Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Giinther, Mengzhou Xia, Weijia Shi,
Xing Han Lt, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria
Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide,
Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia
Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker,
Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. MMTEB:
Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. doi:

10.48550/arXiv.2502.13595. URL https://arxiv.org/abs/2502.13595

Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and
Dragomir Radev. SummEval: Re-evaluating summarization evaluation. Transactions of the
Association for Computational Linguistics, 9:391—409, 2021. doi: 10.1162/tacl_a_00373. URL

Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova,
Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana
Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin,
Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, and Sergei Markov. Mera: A compre-

hensive Ilm evaluation in russian, 2024. URL https://arxiv.org/abs/2401.04531

Joseph L. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76
(5):378-382, 1971. doi: 10.1037/h0031619.

Wikimedia Foundation. Wikimedia downloads. URL/https://dumps.wikimedia.org

Gregor Geigle, Nils Reimers, Andreas Riicklé, and Iryna Gurevych. Tweac: Transformer with

extendable qa agent classifiers. arXiv preprint, abs/2104.07081, 2021. URL/http://arxiv}
org/abs/2104.07081

Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. Continuous measurement
scales in human evaluation of machine translation. In LAW@ACL, 2013. URLihttps://api
semanticscholar.org/CorpusID:1128384

Prakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy Pavel, Maxine Eskenazi, and Jeffrey P. Bigham.
Investigating evaluation of open-domain dialogue systems with human generated multiple refer-

ences, 2019. URL https: //arxiv.org/abs/1907.10568

Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2(1):193-218,
Dec 1985. doi: 10.1007/BF01908075. URL/https://doi.org/10.1007/BF01908075

Kalervo Jarvelin and Jaana Kekalainen. Cumulated gain-based evaluation of ir techniques. ACM
Trans. Inf. Syst., 20(4):422—446, October 2002. ISSN 1046-8188. doi: 10.1145/582415.582418.

URL https://doi.org/10.1145/582415.582418

Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and

fate of linguistic diversity and inclusion in the nlp world, 2021. URL|https://arxiv.org/
abs/2004.09095

Ali Shiraee Kasmaee, Mohammad Khodadad, Mohammad Arshi Saloot, Nicholas Sherck, Stephen
Dokas, Hamidreza Mahyar, and Soheila Samiee. Chemteb: Chemical text embedding benchmark,
an overview of embedding models performance & efficiency on a specific domain, 2025. URL

https://arxiv.org/abs/2412.00532

Sean Lee, Aamir Shakir, Darius Koenig, and Julius Lipp. Open source strikes bread -

new fluffy embeddings model, 2024. URL |/https://www.mixedbread.ai/blog/
mxbai-embed-large-vl

11


Xianming Li and Jing Li. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871, 2023.

Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards

general text embeddings with multi-stage contrastive learning, 2023. URL
org/abs/2308.03281

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.

ArXiv, abs/1907.11692, 2019. URL https: //api.semanticscholar.org/CorpusID:
198953378

Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schiitze. Introduction to Information
Retrieval. Cambridge University Press, 2008.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto
Zamparelli. A SICK cure for the evaluation of compositional distributional semantic models.
In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard,
Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the
Ninth International Conference on Language Resources and Evaluation (LREC’14), pp. 216—
223, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL
http://www. lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf

Philip May. Machine translated multilingual sts benchmark dataset. 2021. URL https://github

Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-
embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog,

2024. URLihttps://www.salesforce.com/blog/sfr-—embedding/

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume 2, NIPS’13, pp. 3111-3119, Red
Hook, NY, USA, 2013. Curran Associates Inc.

Sepideh Mollanorozy, Marc Tanti, and Malvina Nissim. Cross-lingual transfer learning with {P }ersian.
In Lisa Beinborn, Koustava Goswami, Saliha Murado
uglu, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Edoardo M. Ponti, Ryan Cot-
terell, and Ekaterina Vylomova (eds.), Proceedings of the 5th Workshop on Research in Com-
putational Linguistic Typology and Multilingual NLP, pp. 89-95, Dubrovnik, Croatia, May
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.sigtyp-1.9. URL
https://aclanthology.org/2023.sigtyp-1.9

Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding
benchmark. arXiv preprint arXiv:2210.07316, 2022.

Karl Pearson. Note on regression and inheritance in the case of two parents. Proceedings of the
Royal Society of London, 58:240-—242, 1895. doi: 10.1098/rsp1.1895.0041.

Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word
representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543,
Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162.

URLihttps://aclanthology.org/D14-1162/

Barbara Plank. The “problem” of human label variation: On ground truth in data, modeling and
evaluation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing, pp. 10671-10682,
Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-

tics. doi: 10.18653/v1/2022.emnlp-main.731. URL https: //aclanthology.org/2022

Rafat Poswiata, Stawomir Dadas, and Michal Peretkiewicz. Pl-mteb: Polish massive text embedding
benchmark. arXiv preprint arXiv:2405. 10138, 2024.

12


Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Conference on Empirical Methods in Natural Language Processing, 2019. URL {https}

api.semanticscholar.org/CorpusID:201646309

Andrew Rosenberg and Julia Hirschberg. V-measure: A conditional entropy-based external cluster
evaluation measure. In Jason Eisner (ed.), Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP-COoNLL), pp. 410-420, Prague, Czech Republic, June 2007. Association for Computa-

tional Linguistics. URL/https://aclanthology.org/D07-1043/

Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER:
Contextualized affect representations for emotion recognition. In Ellen Riloff, David Chiang,
Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pp. 3687-3697, Brussels, Belgium, 2018. Association
for Computational Linguistics. doi: 10.18653/v1/D18-1404. URL https: //aclanthology}

Artem Snegirev, Maria Tikhonova, Anna Maksimova, Alena Fenogenova, and Alexander Abramov.
The russian-focused embedders’ exploration: rumteb benchmark and russian embedding model

design, 2024. URL https: //arxiv.org/abs/2408.12503

Marina Sokolova and Guy Lapalme. A systematic analysis of performance measures for classifi-
cation tasks. Information Processing & Management, 45(4):427—437, 2009. ISSN 0306-4573.
doi: https://doi.org/10.1016/j.ipm.2009.03.002. URL|https://www.sciencedirect.com/

C Spearman. The proof and measurement of association between two things. International Journal
of Epidemiology, 39(5):1137-1150, 10 2010. ISSN 0300-5771. doi: 10.1093/ije/dyq191. URL
https://doi.org/10.1093/ije/dyql91

Alexander Strehl and Joydeep Ghosh. Cluster ensembles — a knowledge reuse framework for
combining multiple partitions. J. Mach. Learn. Res., 3(null):583-617, March 2003. ISSN
1532-4435. doi: 10.1162/153244303321897735. URL https://doi.org/10.1162/

15324430332189 7/39

Yixuan Tang and Yi Yang. Finmteb: Finance massive text embedding benchmark, 2025. URL
https://arxiv.org/abs/2502.10990

Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram
Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi,
Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen,
Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao
Zheng, Jyotinder Singh, Abheesht Sharma, Divya Sreepat, Aashi Jain, Adham Elarabawy, AJ Co,
Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca,
Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hernandez Abrego, Hesen Zhang, Hui
Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul
Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie
Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram
Rao, Waleed Khawaja, Wenlei Zhou, Xiaogi Ren, Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong,
Zhongli Ding, Francesco Visin, Gaél Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon,
Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts,
Yunhsuan Sung, Raphael Hoffmann, Tris Warkentin, Armand Joulin, Tom Duerig, and Mojtaba
Seyedhosseini. Embeddinggemma: Powerful and lightweight text representations, 2025. URL

https://arxiv.org/abs/2509.20354

Nguyen Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and correction for chance. Journal of Machine
Learning Research, 11:2837-2854, 10 2010.

Ellen Voorhees and D Tice. Building a question answering test collection. (34), 2000-07-01 2000.

13


Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen,
Grzegorz Chrupata, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop Black-
boxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, Brussels, Belgium,
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL
https://aclanthology.org/W18-5446

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R. Bowman. SuperGLUE: a stickier benchmark for general-purpose language
understanding systems. Curran Associates Inc., Red Hook, NY, USA, 2019.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan
Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.

ArXiv, abs/2212.03533, 2022. URL|at tps ://api. semant icscholar.org/Corpus!Dj
254366618

Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving
text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.

Silvan Wehrli, Bert Arnrich, and Christopher Irrgang. German text embedding clustering benchmark,

2024. URL/https://arxiv.org/abs/2401.02709

Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme,
Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models
to follow instructions, 2024.

Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, Marton Kardos, Roman
Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: Massive

image embedding benchmark, 2025. URLhttps://arxiv.org/abs/2504.10471

Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:

Packed resources for general chinese embeddings, 2024. URL|https://arxiv.org/abs/

2309.07597
Dun Zhang, Jiacheng Li, Ziyang Zeng, and Fulong Wang. Jasper and stella: distillation of sota

embedding models, 2025a. URL|https://arxiv.org/abs/2412.19048

Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun
Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embed-
ding: Advancing text embedding and reranking through foundation models, 2025b. URL

https://arxiv.org/abs/2506.05176

Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi,
and Arash Amini. Famteb: Massive text embedding benchmark in persian language. arXiv preprint
arXiv:2502.11571, 2025.

A DETAILED RESULTS BY TASK CATEGORY

This section provides comprehensive results for all tasks, organized by category. Each table includes
human performance alongside all 13 evaluated models, with inter-annotator agreement metrics where
available.

Table 3|presents full results of the clustering tasks. |Table 4/presents full results of the classification
tasks. |Table 5|presents full results of the reranking tasks. |Table 6|presents full results of the STS
tasks.

B TASK EXAMPLES

This section provides screenshots of the actual Argilla annotation interfaces used in our study,
illustrating the annotation challenges and interface design that human annotators encountered.

14


Emotion *

© Submitted =

=) Discara cul S. Save as draft © Submit

i guess it doesn t help that i got sick on black friday and was forced against my will to maintain my promise to stay in but being
back in the city feels amazing

Figure 4: Emotion Classification annotation interface showing the 6-category emotion labeling task.
This task achieved fair inter-annotator agreement (& = 0.39) due to ambiguous emotional states and
mixed emotions in social media text. Human performance: 45.8%, Best model: 87.1%.

Submitted -] = Fiters s! Sort
Sentiment *
£9
2) negative (7 neu GE
© Discard enl|S. Save as dratt + Submit

Good morning to you, however it's night time for me, so | am off to bed *hugs* Have a great day

Figure 5: Tweet Sentiment Classification annotation interface demonstrating sentiment polarity
annotation. This task achieved moderate inter-annotator agreement (« = 0.48) with reasonable
consensus on positive/negative sentiment. Human performance: 84.4%, Best model: 90.9%.

Q [Submitea-] = Fiters Sort
Cluster 1D *

2per Abstract @) Discard aul 5 Save as draft © Submit
Accessibility percolation in random fitness landscapes. The fitness landscape encodes the mapping of genotypes to fitness and

provides a succinct representation of possible trajectories followed by an

evolving population. Evolutionary accessibility is quantified by the existence

of fitness-monotonic paths connecting far away genotypes. Studies of

accessibility percolation use probabilistic fitness landscape models to explore

the emergence of such paths as a function of the initial fitness, the

parameters of the landscape or the structure of the genotype graph. This

chapter reviews these studies and discusses their implications for the

predictability of evolutionary processes.

Figure 6: ArXiv Clustering annotation interface showing academic papers that caused complete
annotator disagreement (ARI = —0.001) due to interdisciplinary research overlap. Papers could be
categorized by methodology, application domain, or research community, leading to fundamental
disagreement. Human performance: 49.2%, Best model: 84.6%.

Submitted ~| = Filters 1 Sort
Cluster ID *

© Submited i

Discard eit) 5, Save as draft ~ Submit

originally featured amateur voice actors, local to East Texas.

Figure 7: Reddit Clustering annotation interface demonstrating thematic grouping of discussion
topics. This task achieved fair agreement (ARI = 0.34) due to overlapping themes across different
discussion topics. Human performance: 68.8%, Best model: 100%.

2, [Submited ~] = Finers # Sort ~
Cluster ID *

5

© Discard cit S) Save as draft = Submit
The show originally featured amateur voice actors, local to East Texas.

Figure 8: SIB200 Clustering annotation interface showing multilingual sentence clustering task. This
task achieved moderate inter-annotator agreement (ARI = 0.42) with variation across languages
depending on cultural context and sentence complexity.

15


a [Suvmitea~) = Fines 1 son
Document 1 Relevance *

_ PSEA GB Not Relevant

GED 2) Not Relevant

Document 4 Relevance *

the mid-1970's. The population had declined to an estimated 880 Individuals in 1880, and possibly declined to around 785 individuals in 1993,
based upon BPA's (1993) recent estimates. The population may be reaching the age of reproductive senescence, since for most sturgeon
species females reproduce between the ages of 16 to 25 years (Doroshov 1993). Although the continuing lack of natural flows affecting
sturgeon juvenile rerutment i considered the primary threat ‘ots continue existence, other factors are also contributing tothe ws CHEED = wrrcicven
populations’ decline. See the “Summary of Factors Affecting the Species" section for a more complete discussion on the factors affecting the
white sturgeon's decline. Consequently, the Service has determined that this distinct population of white sturgeon is in danger af extinction
throughout its range and therefore fits the Act's definition of an endangered species. Issue 10: In comments on the proposed rule, BPA stated

‘rato Lib Dam oper 5 lve

Document § Relevance *

Document 6 Relevance *

currently occupied. The presence or absence of tortolses will not factor Into the determination of actions that trigger section 7. Any action that

may affect critical habitat will trigger section 7 consultation. The requirement to consider adverse modification of critical habitat is an Document 7 Relevance *
incremental section 7 consideration above and beyond section 7 review necessary to evaluate Jeopardy and incidental take. As required by 50 AGSER
CFR 402.14, a Federal agency must consult with the Service iit determines an action may affect a listed species or Its crtical habitat. Federal

agencies are responsible for determining whether or not to consult with the Service and should consider a number of factors when
determining if a proposed action may affect critical habitat. To the extent possible, agencies should consult on a programmatic basis. The

© Diseara ans) Saveas draft + Submit
Service will co

Figure 9: Robust04 Reranking annotation interface showing document relevance assessment for
information retrieval queries. This task achieved strong inter-annotator agreement (p = 0.72). Human
performance: 88.5%, Best model: 98.8%.

a [Suomited ~] = Filters son

Document 2 Relevance *

What are the critical reviews and ratings for the film Existenz?
Document 3 Relevance”

2 relevent CRED
Document 4 Relevance *

Document § Relevance *

Conversely, James Berardinelli gave the film a two out of four star rating in his review. He cites that the film had a "disjointed feel", and called
ita *missed opportunity” that suffered from being released near The Matrix and Open Your Eyes, which he states aid similar things that were
accomplished better in those films.

The film received generally positive reviews, with a 74% approval rating at Rotten Tomatoes based on 74 reviews, with an average rating of 4) Rolevant
6.60/10. The site's critical consensus reads, "Gooey, slimy, grotesque fun." Metacritic assigned a score of 68 out of 100, based on 29 critics,
indicating "generally favorable reviews”.

Document 6 Relevance *

EASE €) Not Relevant

Roger Ebert gave the film three stars aut of four in his review of the film, noting its release after fallow science-fiction film The Matrix. He Document 7 Relevance *
compared the two films, stating that while both have special effects, Cronenberg's fm was stranger along with having his best effects involve

) Discard «tn|5) Save as drat Submit

Nominated, Best Sound Editing in a Foreign Feature: David Evans, Wayne Griffin, Mark Gingras, John Laing, Tom Bjelic, and Paul Shikata

Figure 10: Wikipedia Multilingual Reranking annotation interface demonstrating cross-lingual
relevance judgment. This task achieved moderate agreement (9 = 0.64) due to cross-lingual
complexity.

& [Submited -] = Fiters 1 Sort
Similarity Score *

© Submites :

© Discard n|§) Save.as draft + | Submit

twice the quantity or amount of something

‘a quantity that is twice as great as another .

Figure 11: STS12 annotation interface showing semantic similarity assessment using a 0-5 scale.
This well-curated dataset achieved strong inter-annotator agreement (9 = 0.77). Human performance:
91.2%, Best model: 92.0%.

a [Submited =] = Fiters 1 Sort
—— Similarity Score *

ze @QPOoe

© Discars cu 5) Saveas craft + Submit
Sentence 1

‘A woman is putting on eyeshadow

‘The woman is removing make-up

Figure 12: SICK-R annotation interface showing semantic relatedness and entailment task. This task
achieved moderate agreement (p = 0.68) due to task complexity. Human performance: 82.6%, Best
model: 94.1%.

16


Model Arxiv Reddit SIB200 WikiCities

all-MiniLM-L6-v2 61.8 56.5 33.2 73.8
all-mpnet-base-v2 56.0 63.0 332 86.7
e5-mistral-7b-instruct TI2 74.9 73.8 100.0
embeddinggemma-300m 73.0 72.8 36.0 70.3
gte-Qwen2-1.5B-instruct 70.8 88.4 eee 73.6
jasper_en_vision_language_vl 83.9 95.1 59.0 95.1
multilingual-e5-base 31.9 60.0 34.4 66.2
multilingual-e5-large 51.1 55.4 37.8 69.2
multilingual-e5-small 30.5 76.9 38.7 72.6
mxbai-embed-large-v1 48.3 72.1 33.0 90.8
Qwen3-Embedding-0.6B 69.1 76.3 65.5 74.8
SFR-Embedding-Mistral 75.5 81.4 74.8 100.0
stella_en_1.5B_v5 84.6 100.0 44.2 86.4
Human 49.2 68.8 65.2 97.6

Table 3: Full clustering results.

Model Emotion MultilSenti ToxicConvo TweetSenti
all-MiniLM-L6-v2 42.1 57.9 66.4 59.6
all-mpnet-base-v2 44.0 61.5 60.4 58.4
e5-mistral-7b-instruct 47.3 75.9 68.0 75.8
embeddinggemma-300m 40.4 64.6 66.4 67.8
gte-Qwen2-1.5B-instruct 56.2 78.1 78.7 79.3
jasper_en_vision_language_v1 75.4 77.3 86.7 90.9
multilingual-e5-base 36.2 78.6 66.0 69.1
multilingual-e5-large 38.5 81.1 63.3 65.3
multilingual-e5-small 33.8 75.8 65.1 69.6
mxbai-embed-large-v1 42.1 64.4 68.7 65.8
Qwen3-Embedding-0.6B 51.9 74.1 74.4 87.8
SFR-Embedding-Mistral 46.7 771A 67.3 75.6
stella_en_1.5B_v5 71.9 76.1 82.9 89.1
Human 45.8 87.5 73.3 84.4

Table 4: Full Classification results.

C TASK QUALITY ANALYSIS

C.1 DATASET QUALITY ISSUES
Our analysis revealed quality issues across multiple datasets that significantly impact human-model

performance comparisons. These issues fall into several categories that help explain performance
patterns observed in our study.

C.1.1 STS22-RUSSIAN

The Russian subset of “STS22” dataset shows patterns that may help explain the comparatively low
human agreement we observed.

17


Model Corel7 News21 Robust04 Wikipedia

all-MiniLM-L6-v2 95.6 98.8 96.3 77.8
all-mpnet-base-v2 98.6 98.6 97.8 79.2
e5-mistral-7b-instruct 98.8 99.5 98.8 88.4
embeddinggemma-300m 84.2 91.4 89.8 76.0
gte-Qwen2-1.5B-instruct 97.5 992 98.5 86.1
jasper_en_vision_language_v] 98.2 100.0 98.7 88.8
multilingual-e5-base 96.2 98.6 96.9 88.5
multilingual-e5-large 95.7 97.8 ye 92.6
multilingual-e5-small 95.6 98.1 OT a 87.6
mxbai-embed-large-v 1 O72 98.0 98.6 85.6
Qwen3-Embedding-0.6B 97.0 100.0 98.5 86.8
SFR-Embedding-Mistral 97.9 99.7 98.8 87.9
stella_en_1.5B_v5 98.6 100.0 98.3 89.2
Human S5.2 O27 88.5 87.9

Table 5: Full Reranking results.

Model SICK-R STS12 STS22 STSB
all-MiniLM-L6-v2 915 85.7 48.4 81.8
all-mpnet-base-v2 89.8 83.7 54.3 78.4
e5-mistral-7b-instruct 93.2 89.1 58.5 85.9
embeddinggemma-300m 72.6 77.9 la 58.3
gte-Qwen2-1.5B-instruct 93.4 86.9 60.3 80.0
jasper_en_vision_language_v1 93.8 92.0 67.2 88.7
multilingual-e5-base 91.5 86.5 63.3 82.9
multilingual-e5-large 89.4 89.9 65.9 83.6
multilingual-e5-small 88.6 87.6 63.3 81.9
mxbai-embed-large-v1 93.4 91.1 53.9 88.9
Qwen3-Embedding-0.6B 93.3 91.8 63.6 90.9
SFR-Embedding-Mistral 94.1 89.2 ect 86.4
stella_en_1.5B_v5 92.3 89.1 64.3 87.1

Human 82.6 91.2 68.2 80.4

Table 6: Full STS results.

Context Expansion Issues:

¢ Sentence pairs labeled as “4” (identical meaning) where one sentence contains basic infor-
mation and the paired sentence includes additional backstory or context

¢ Translated example pattern: “Company reports earnings” vs. “Company reports earnings of
$X million, exceeding expectations due to strong performance in sector Y”

¢ Human annotators correctly identify these as semantically different (similarity 2-3), while
gold labels mark them as identical

18


¢ This explains the low human performance on STS22-Russian (58.5%) compared to models
(69.5%)

Parsing and Processing Errors:

¢ Incomplete sentence parsing affecting semantic interpretation

¢ Parsing artifacts from web pages (e.g., page numbers, lists of automatically generated related
news, ads)

C.1.2 MULTILINGUAL SENTIMENT CLASSIFICATION-RUSSIAN

The Russian subset of “MultilingualSentimentClassification” consists of news articles from different
news sites. The task is to classify each text as “positive” or “negative”. However, the dataset presents
several challenges:

Neutral and Ambiguous Content:
¢ Many samples are based on press releases from companies or government departments,
which are often neutral in tone and difficult to categorize as positive or negative.

¢ Translated example: “The total amount of pension savings accumulated in JSC ’Unified
Accumulative Pension Fund’ (UAPF) as of September 1, 2016, amounted to about 6.41
trillion tenge, the press center of the pension fund said, KazTAG reports. ...”

¢ Such sentences are more informative than sentiment-bearing.
Parsing and Processing Errors:

¢ Similar to the issues described in|§ C.1.1} the dataset contains parsing artifacts from web
pages.

C.1.3. EMOTION CLASSIFICATION
The emotion classification dataset suffers from inherent label ambiguity that explains the low human
agreement (K = 0.39):
Mixed Emotional States:
e Texts expressing multiple emotions simultaneously: “i am feeling very indecisive and
spontaneous” (labeled as fear but could be surprise)

¢ “i was feeling pretty anxious all day but my first day at work was a very good day and that
helped a lot” (contains both fear and joy)

¢ “i am feeling crampy and cranky” (physical discomfort mixed with anger)
Sarcastic and Ironic Expressions:
¢ “7 got paid too much because i get so many deliveries at work im feeling a bit shamed so

will curb the spending for a bit” (sarcasm about being “overpaid’’)

¢ “7 feel like such a noob when the customers make really dull and stupid jokes that im
supposed to find funny” (surface sadness but underlying anger/frustration)

¢ “i feel very cheated since i am supporting the family and doing all the other stuff while he
spends hours a day gaming” (labeled as joy but clearly expressing anger)

Contextual and Subjective Interpretation:

¢ “7 feel shame in a strange way” (ambiguous emotional context, labeled as surprise)
¢ “7 feel all funny sometimes” (vague emotional description that could be multiple categories)

¢ “i feel underappreciated and under valued” (could be sadness, anger, or fear depending on
interpretation)

19


C.1.4 ARXIV CLUSTERING CHALLENGES

Academic paper clustering presents fundamental categorization difficulties that explain the complete
breakdown of human agreement (ARI = —0.001). This task uses derived labels from ArXiv paper
categories:

Interdisciplinary Research Papers:

¢ “Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Syn-
chronicity” (could cluster with computer vision, audio processing, or self-supervised learn-
ing)

e “The architecture of innovation: Tracking face-to-face interactions with ubicomp technolo-
gies” (spans social science, computer science, and architecture)

¢ “PIINET: A 360-degree Panoramic Image Inpainting Network Using a Cube Map” (computer
vision, graphics, or deep learning focus)

Methodological vs. Application Domain Confusion:

¢ “Convergent Actor-Critic Algorithms Under Off-Policy Training and Function Approxima-
tion” (reinforcement learning methodology vs. control theory application)

¢ “Learning-Based Adaptive IRS Control with Limited Feedback Codebooks” (machine
learning method vs. wireless communications application)

¢ “Structure-preserving numerical methods for stochastic Poisson systems” (numerical meth-
ods vs. mathematical physics)

Emerging and Cross-Domain Research:

e “The modularity of action and perception revisited using control theory and active inference”
(cognitive science, control theory, or neuroscience)

¢ “Food-chain competition influences gene’s size” (evolutionary biology, computational biol-
ogy, or mathematical modeling)

¢ “Wavelet Analysis of Dengue Incidence and its Correlation with Weather and Vegetation
Variables in Costa Rica” (epidemiology, signal processing, or environmental science)

C.2 IMPACT ON EVALUATION
These quality issues have several critical implications for embedding evaluation:

1. Artificial Model Advantages: Models may achieve “superhuman” performance by consis-
tently reproducing systematic labeling patterns rather than demonstrating superior semantic
understanding. This is particularly evident in tasks with low human agreement where models
can exploit consistent but incorrect labeling patterns.

2. Misleading Benchmarks: Tasks with fundamental quality issues provide unreliable targets
for model development. High model performance on such tasks may not indicate genuine
capability improvements but rather successful pattern matching to flawed gold standards.

3. Cultural and Linguistic Bias: Quality issues disproportionately affect non-English tasks,
potentially masking genuine model limitations in cross-cultural understanding while artifi-
cially inflating performance on problematic English datasets.

4. Evaluation Validity: The validity of using these datasets as benchmarks is questionable
when human experts cannot agree on correct answers, suggesting fundamental issues with
task specification rather than human limitations.

D_ INTER-ANNOTATOR AGREEMENT ANALYSIS

D.1 AGREEMENT METRICS BY TASK CATEGORY

This section provides detailed inter-annotator agreement analysis using task-appropriate metrics.
Agreement levels follow standard guidelines: « > 0.8 (excellent), 0.6 < «& < 0.8 (substantial),

20


0.4 < K < 0.6 (moderate), 0.2 < K < 0.4 (fair), & < 0.2 (poor). For correlations: p > 0.7 (strong),
0.4 < p < 0.7 (moderate), p < 0.4 (weak).

D.1.1 CLASSIFICATION TASKS

¢ Emotion Classification: « = 0.39 (fair agreement)
— 2 annotators, 48 items, 96 total annotations

— Mean percentage agreement: 52.1%
— Performance: Human 45.8%, Best model 87.1%

* Toxicity Classification: « = 0.55 (moderate agreement)
— 2 annotators, 45 items, 90 total annotations

— Mean percentage agreement: 77.8%
— Performance: Human 73.3%, Best model 86.7%

¢ Tweet Sentiment Classification: « = 0.41 (moderate agreement)

— 2 annotators, 45 items, 90 total annotations
— Mean percentage agreement: 62.2%
— Performance: Human 84.4%, Best model 90.9%

¢ Multilingual Sentiment Classification: Agreement only for English

— English: « = 0.24 (fair agreement), 2 annotators, 40 items, 62.5% agreement
— Arabic, Norwegian, Russian: Single annotator (no agreement metrics)
— Performance: Human advantages in non-English variants

D.1.2 CLUSTERING TASKS

¢ ArXiv Clustering: ARI = —0.001 (no agreement)
— 2 annotators, 30 items, 60 total annotations

— Complete breakdown of consensus on academic paper categories
— Performance: Human 49.2%, Best model 84.6%

* Reddit Clustering: ARI = 0.42 (moderate agreement)

— 2 annotators, 30 items, 60 total annotations
— Moderate consensus on discussion topic groupings
— Performance: Human 68.8%, Best model 100%
* WikiCities Clustering: ARI = 0.91 (excellent agreement)
— 2 annotators, 30 items, 60 total annotations

— High consensus on geographical entity groupings
— Performance: Human 97.6%, Best model 100%

¢ SIB200 Clustering: Agreement only for English

— English: ARI = 0.15 (weak agreement), 2 annotators, 30 items
— Arabic, Danish, Russian: Single annotator (no agreement metrics)
— Performance varies significantly across languages

D.1.3. RERANKING TASKS

* News21: p = 0.85 (strong agreement)

— 2 annotators, 31 items, 62 total annotations
— Mean Kendall tau: 0.85, Binary kappa: 0.83
— Performance: Human 92.7%, Best model 100%

* Corel7: p = 0.80 (strong agreement)

— 2 annotators, 20 items, 40 total annotations
— Mean Kendall tau: 0.80, Binary kappa: 0.78

21


— Performance: Human 85.2%, Best model 98.8%
* Robust04: = 0.75 (strong agreement)

— 2 annotators, 49 items, 98 total annotations
— Mean Kendall tau: 0.75, Binary kappa: 0.72
— Performance: Human 88.5%, Best model 98.8%

¢ Wikipedia Multilingual Reranking: Agreement only for English

— English: p = 0.64 (moderate agreement), 2 annotators, 30 items
— Mean Kendall tau: 0.64, Binary kappa: 0.60

— Danish, Norwegian: Single annotator (no agreement metrics)

— Performance varies across languages

D.1.4 STS TASKS

¢ STS12: p = 0.77 (strong agreement)

— 2 annotators, 50 items, 100 total annotations
— Performance: Human 91.2%, Best model 92.0%

¢ STSBenchmark: p = 0.58 (moderate agreement)

— 2 annotators, 50 items, 100 total annotations
— Performance: Human 80.4%, Best model 90.9%

¢ SICK-R: p = 0.63 (moderate agreement)

— 2 annotators, 40 items, 80 total annotations
— Performance: Human 82.6%, Best model 94.1%

¢ STS22: Agreement only for English

— English: p = 0.75 (strong agreement), 2 annotators, 30 items
— Arabic, Russian: Single annotator (no agreement metrics)
— Performance varies significantly by language

D.2. AGREEMENT PATTERNS AND TASK RELIABILITY
D.2.1 HIGH-AGREEMENT TASKS (RELIABLE BENCHMARKS)
Tasks with high human agreement (« > 0.6 or p > 0.7) consistently demonstrate:

* Clear, objective task specifications with minimal ambiguity
¢ Adequate context for making informed judgments

¢ Cultural and linguistic familiarity for annotators

¢ Well-defined evaluation criteria with concrete examples

¢ Minimal dataset quality issues or processing artifacts

¢ Consistent performance patterns across annotators

Examples: WikiCities clustering, News21/Core17/Robust04 reranking, STS12, STSBenchmark

D.2.2. LOW-AGREEMENT TASKS (PROBLEMATIC BENCHMARKS)
Tasks with low agreement (« < 0.4 or p < 0.6) often exhibit:

¢ Ambiguous annotation guidelines or subjective judgment requirements
¢ Cross-cultural interpretation challenges

¢ Insufficient context for accurate assessment

¢ Systematic dataset quality issues or processing artifacts

¢ Inherently subjective or multi-faceted concepts

¢ Inconsistent or contradictory gold standard labels

Examples: Emotion classification, ArXiv clustering, STS22-Russian

22


E ADDITIONAL HUMAN VS MODEL ANALYSIS

STS - STS22 (ara)

Classification - Multilingual Sentiment (ara)
Classification - Multilingual Sentiment (rus)
Classification - Multilingual Sentiment (nob)
STS - STS12

Clustering - WikiCities

Reranking - WikiMulti (nob)

Clustering - SIB200 (ara)

Reranking - WikiMulti (dan)

STS - STS22 (eng)

Classification - Tweet Senti

Reranking - News21

Reranking - WikiMulti (eng)

Clustering - SIB200 (rus)

Reranking - Robust04

STS - STSBenchmark

STS - STS22 (rus)

STS - SICK-R

Clustering - SIB200 (dan)

Classification - Toxicity

Reranking - Core17

Classification - Multilingual Sentiment (eng)
Clustering - SIB200 (eng)

Classification - Emotion

-0.8

-2.4
-2.5
-2.8
-3.6
-4.5
-6.4
-7.3
-8.2
-9.7
-10.3
-10.6
-11.1
-11.5
-13.3
-13.3
-13.5
-18.0
-29.3
-29.6

+26.7
+17.5
+11.3
+10.0

Clustering - Reddit -31.2

Clustering - ArXiv |-35.4

-40 —30 —20 -10 0 10 20 30
Gap vs Best model (percentage points)

Figure 13: Human performance gaps versus best-performing models across 26 task-language pairs.
Humans outperform the best models on only 4 tasks (15.4%), with largest advantages in Arabic
semantic similarity and sentiment analysis. The analysis reveals systematic model advantages in
technical domains (clustering, reranking) versus human advantages in culturally-informed tasks.

This section contains additional analysis on human vs model. [Figure 14|shows the human performance
gaps versus median-performing models over all tasks by language. shows the task
difficulty categorization based on human performance levels. [Figure 16]shows the model consistency
analysis showing performance ranges across tasks. [Figure 2|shows a comprehensive view of human
performance relative to all model performance ranges across 16 tasks by language.

F MODELS EVALUATED

‘Table 7|shows information about each evaluated model.

23


Clustering - SIB200 (ara) +40.1

Classification - Multilingual Sentiment (ara) | +22.7
Clustering - WikiCities | +22.7
Clustering - SIB200 (rus) as S+ 22-4
Clustering - SIB200 (dan) Re 20.0
Classification - Multilingual Sentiment (rus) | +18.0
Classification - Multilingual Sentiment (nob) ee) +16.2
Classification - Tweet Senti a +14.9
Clustering - SIB200 (eng) — +10.5
Classification - Toxicity — +6.0
Reranking - WikiMulti (nob) a +3.7
STS - STS12 +22
Classification - Emotion ol +1.9
Reranking - WikiMulti (dan) i) +2.4
STS - STS22 (eng) | +0.5
STS - STSBenchmark
STS - STS22 (rus)
Reranking - WikiMulti (eng)
Clustering - Reddit
Reranking - News21
STS - SICK-R
Reranking - Robust04
Classification - Multilingual Sentiment (eng)
Reranking - Core17
Clustering - ArXiv |-19.8
—20 -10 0 10 20 30 40

Gap vs Median model (percentage points)

Figure 14: Human performance gaps versus median-performing models across 26 tasks by language.
Humans achieve 61.5% win rate against median models, demonstrating competitive performance
when compared to typical rather than best-performing models. This analysis reveals that human
performance is much more competitive when compared against representative model performance
rather than cherry-picked best results.

24


WikiCities

Multilingual Sentiment (ara)
News21

Multilingual Sentiment (rus)
WikiMulti (dan)

STS12

WikiMulti (nob)

Robust04

Core17

Multilingual Sentiment (nob)
Tweet Senti

SICK-R

WikiMulti (eng)
STSBenchmark

STS22 (eng)

Multilingual Sentiment (eng)
SIB200 (ara)

Toxicity

Reddit

SIB200 (rus)

STS22 (ara)

SIB200 (dan)

STS22 (rus)

SIB200 (eng)

ArXiv

Emotion

20

97.6
95.0
92.7
92.5
91.4
91.2
89.8
88.5
85.2
85.0
84.4
82.6
82.4
80.4
78.4
77.5
76.0
73.3
68.8
68.1
67.5
62.7
58.5
54.0
49.2
45.8

40 60
Human performance (%)

Figure 15: Task difficulty categorization based on human performance levels. The majority of tasks
(69%) fall into the “easy” category (human performance > 0.7), shown in green. Only two tasks fall
below 0.5 (shown in red), both with notably low inter-annotator agreement, suggesting fundamental
task ambiguity rather than limitations of human ability.

25


News21

Robust04

STS22 (eng)

WikiMulti (eng)

STS12

Core17

WikiMulti (dan)

WikiMulti (nob)

SICK-R

Multilingual Sentiment (ara)
Multilingual Sentiment (rus)
Toxicity

Multilingual Sentiment (nob)
Tweet Senti

STSBenchmark

WikiCities

STS22 (ara)

Multilingual Sentiment (eng)
STS22 (rus)

Emotion

Reddit

SIB200 (rus)

SIB200 (dan)

ArXiv

SIB200 (eng)

SIB200 (ara)

7.3

9.0
12.1
12.5
14.1
14.5
20.7
21.1
21.5
24.0
25.7
26.2
28.0
32.4
32.6
33.8
34.7
35.7
36.5
41.7
44.6
47.2
53.1
54.1
59.1
59.6
10 20 30 40 50 60

Model performance range (pp)

Figure 16: Model consistency analysis showing performance ranges across tasks. Higher value
indicates greater variability across models (lower consistency). Tasks with small ranges (high con-
sistency) often align with high human agreement, whereas tasks with large ranges (low consistency)
typically correspond to tasks where humans also struggle. This pattern suggests that both human and
model performance reflect underlying task quality and clarity of task specification.

26


Parameters

Model (Millions)
Alibaba-NLP/gte-Qwen2-1.5B-instruct|Li et al.|(2023) 1780
google/embeddinggemma-300m|Vera et al.|(2025) 300
intfloat/e5-mistral-7b-instruct g (2023} 2022) 7111
intfloat/multilingual-e5-large Wang at} 560
intfloat/multilingual-e5-base|Wang (2022) 278
intfloat/multilingual-e5-small/Wang et al.|(2022) 118
mixedbread-ai/mxbai-embed-large-v ) 335
NovaSearch/jasper_en_vision_lan g 1999
Qwen/Qwen3-Embedding-0.6B Zhang et t al.|(2025b) 596
Salesforce/SFR-Embedding-Mistral (Meng et al.|/2024 7110
sentence-transformers/all-MiniLM-L6-v2|R & 22.7
sentence-transformers/all-mpnet-base-v2|R 109

Table 7: List of all evaluated models. Model sizes are in millions of parameters

G LLM USAGE STATEMENT

Large language models were used to assist with formatting, citation integration, and writing polish
during the preparation of this manuscript. Specifically, we used LLMs for:

¢ Formatting assistance for LaTeX tables and mathematical notation
¢ Integration and standardization of citation formats
¢ Minor writing improvements for clarity and flow

* Code documentation and data processing script organization

All substantive content, including research design, data analysis, interpretation of results, and
scientific conclusions, was developed entirely by the authors. The core contributions, methodology,
and findings presented in this work are the original intellectual contribution of the research team.
LLM assistance was limited to technical formatting and presentation improvements that did not
influence the scientific content or conclusions of the study.

27
