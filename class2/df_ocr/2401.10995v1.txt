arX1v:2401.10995v1 [cs.CL] 19 Jan 2024

The Radiation Oncology NLP Database

Zhengliang Liu, Jason Holmes, Wenxiong Liao, Chenbin Liu,
Lian Zhang, Hongying Feng, Peilong Wang, Muhammad Ali Elahi,
Hongmin Cai, Lichao Sun, Quanzheng Li, Xiang Li,
Tianming Liu, Jiajian Shen, Wei Liu

Abstract

We present the Radiation Oncology NLP
Database (ROND), the first dedicated Natural
Language Processing (NLP) dataset for radia-
tion oncology, an important medical specialty
that has received limited attention from the
NLP community in the past. With the advent
of Artificial General Intelligence (AGI), there
is an increasing need for specialized datasets
and benchmarks to facilitate research and devel-
opment. ROND is specifically designed to ad-
dress this gap in the domain of radiation oncol-
ogy, a field that offers many opportunities for
NLP exploration. It encompasses various NLP
tasks including Logic Reasoning, Text Clas-
sification, Named Entity Recognition (NER),
Question Answering (QA), Text Summariza-
tion, and Patient-Clinician Conversations, each
with a distinct focus on radiation oncology con-
cepts and application cases. In addition, we
have developed an instruction-tuning dataset
consisting of over 20k instruction pairs (based
on ROND) and trained a large language model,
CancerChat. This serves to demonstrate the
potential of instruction-tuning large language
models within a highly-specialized medical
domain. The evaluation results in this study
could serve as baseline results for future re-
search. ROND aims to stimulate advancements
in radiation oncology and clinical NLP by of-
fering a platform for testing and improving
algorithms and models in a domain-specific
context. The ROND dataset is a joint effort
of multiple U.S. health institutions. The data
is available at https: //github.com/zl-liu/
Radiation-Oncology-NLP-Database.

1 Introduction

Radiation oncology is a critical medical specialty
that employs high-energy radiation to treat and
manage cancer and other diseases (Bernier et al.,
2004; Unkelbach et al., 2018). Indeed, like many
medical domains, there is much potential to inte-
grate natural language processing (NLP) into ra-
diotherapy research and practice (Bitterman et al.,

2021; Rezayi et al., 2022). However, there is lim-
ited development and evaluation of NLP models in
this domain due to the lack of dedicated datasets
(Rezayi et al., 2022). In response to this need,
we present the Radiation Oncology NLP Database
(ROND).

ROND is the world’s first NLP dataset specif-
ically created for radiation oncology. It aims to
provide a comprehensive platform for researchers
to develop, test, and improve NLP models and
methods within this domain. This dataset covers
a wide spectrum of NLP tasks, including Logic
Reasoning, Clinical Text Classification, Named En-
tity Recognition (NER), Question Answering (QA),
and Text Summarization. Each of these tasks is
centered around distinct aspects of radiation oncol-
ogy, offering researchers a rich and varied dataset
for exploration and model training. In addition,
ROND contains a Patient-Clinician conversation
dataset, which provides valuable insights into pa-
tient interactions, symptom descriptions, and treat-
ment discussions, enhancing our understanding and
modeling of complex medical dialogues. Figure 1
presents an overview of ROND.

The unique structure of ROND facilitates the
development of models capable of reasoning logi-
cally about complex radiation oncology concepts,
classifying domain-specific text data, recognizing
and categorizing specialized entities, accurately an-
swering radiation oncology-related questions, and
summarizing lengthy documents and research pa-
pers in the field. We aim to establish a benchmark
for future studies that stimulates innovation in ra-
diation oncology research and ultimately improves
patient care through the power of NLP.

We believe this database is of particular impor-
tance in the age of Artificial General Intelligence
(AGD (Bubeck et al., 2023; Zhao et al., 2023; Liu
et al., 2023a). Successful large language models
(LLM) such as ChatGPT, GPT-4, LLAMA (Tou-
vron et al., 2023) and PaLM (Chowdhery et al.,


Text Classification

Logic Reasoning

Text
Summarization

Patient-Clinician
Conversations

Radiation Oncology NLP
Database

Named Entity
Recognition

Question &
Answering

Figure 1: Overview of the Radiation Oncology NLP Database.

2022) are trained on vast amounts of public domain
data. Some LLMs such as Med-Palm 2 (Singhal
et al., 2022) are trained on both public biomedical
data sources and private hospital (e.g., through the
Google-Mayo Clinic partnership) data, and conse-
quently are highly capable of processing medical
text (Singhal et al., 2022). However, there is no
existing dataset that specifically supports NLP in ra-
diation oncology. The ROND dataset complements
recent LLM advancements and offers a platform to
better integrate LLMs into healthcare.

2 The Radiation Oncology NLP Database

2.1 Logic Reasoning

The Logic Reasoning subset of the Radiation On-
cology NLP Database (ROND) presents questions
designed to assess the logical reasoning capabili-
ties of NLP models within the context of radiation
oncology. The questions are geared towards the
understanding of fundamental concepts and princi-
ples in radiation oncology, such as the properties
of radioactive elements, atomic structure, electron
orbits, X-ray emission, penumbra effects, and inter-

action of different particles with matter. We man-
ually created and annotated 100 logic reasoning
questions for this dataset.

Each question in this subset is structured as a
yes/no question, designed to elicit a binary re-
sponse. The questions range from basic atomic
structure, such as "Does an atom consist of a pos-
itively charged nucleus surrounded by a cloud of
negatively charged electrons?" to more specific
queries about X-ray production and penumbra ef-
fect, such as "In X-ray production, does the effi-
ciency of x-ray production depend on the size of
the target?" or "Is physical penumbra influenced by
geometric penumbra, beam energy, and the lateral
transport of electrons in the tissues?"

This dataset provides an avenue to evaluate the
ability of NLP models to apply logical reasoning
within the domain-specific context of radiation on-
cology, emphasizing both the understanding of fun-
damental radiation oncology concepts and the abil-
ity to apply this knowledge to specific scenarios.


E=~ )o
Eo

C1) Tumor location & size
2) Comorbidities
3] Cost

&

1. Head and neck cancers of tumors located at the base of the
skull where nerves come out.

Label: Proton therapy.

2. Lung cancers in the middle of chest or near the esophagus.
G Label: Proton therapy

3. Lower-cost option for prostate cancers.

Label: Photon therapy

Proton therapy?

Photon therapy?

Figure 2: Illustration of the Clinical Text Classification dataset.

2.2 Clinical Text Classification

The Clinical Text Classification subset of ROND
is designed to test the capability of NLP models
in categorizing text inputs related to radiation on-
cology into predefined labels. This specific task
focuses on determining the appropriate type of ther-
apy ("Proton therapy" or "Photon therapy") based
on descriptions of different cancer scenarios. We
manually created and annotated 100 cases for this
dataset. Please see Figure 2 for an illustration of
this dataset.

The dataset presents a variety of clinical sce-
narios and characteristics of cancers, such as the
location and sensitivity of the tumor, cost consid-
erations, patient demographics, and potential risks.
These descriptions are then categorized into two
major classes: "Proton therapy" and "Photon ther-

"

apy”.

Examples include categorizing "Head and neck
cancers of tumors located at the base of the skull
where nerves come out" and "Cancers in children"
under the label ’Proton therapy’. On the other hand,
"Lower-cost option for prostate cancers" and "Bet-
ter protection of skin" are classified under ’ Photon
therapy’.

We aim to facilitate the training and evaluation
of NLP models capable of accurately classifying
radiation oncology cases into relevant treatment
venues, thereby potentially aiding decision-making

processes in clinical settings.

2.3. Named Entity Recognition (NER)

The Named Entity Recognition (NER) subset of
ROND is designed to annotate entities in the text
that pertain to the field of radiation oncology. This
task is crucial for understanding specific details
within the text, such as identifying the names of
doctors and patients (PERSON), types of diseases
(DISEASE), types of treatment (TREATMENT),
anatomical structures (ANATOMY), numeric val-
ues (NUMBER), symptoms (SYMPTOM), and out-
comes (OUTCOME). We asked GPT-4 to generate
sample sentences, and manually reviewed and an-
notated 20 sets of NER samples that are factually
correct. While not all sentences necessarily contain
all seven NER tags, these are the maximum pos-
sible tags that any sentence from our sample sets
might include. Figure 3 presents a sample from the
NER dataset.

For instance, in the sentence "Dr. Jenkins, a radi-
ation oncologist, treated patient Sarah Williams for
breast cancer, utilizing intensity-modulated radia-
tion therapy (IMRT) with a total dose of 50 Gy in
25 fractions", the model is expected to identify "Dr.
Jenkins" as a PERSON, "radiation" and "intensity-
modulated radiation therapy" as a TREATMENT,
"breast" as an ANATOMY, "50" and "25" as NUM-
BERs, and "skin irritation" as a SYMPTOM.

In the context of another sentence, "In a study


led by Dr.

Sentence: Dr. Patel reported that the 30 patients with stage IIIB non-small
cell lung cancer who underwent concurrent chemoradiation therapy
showed a significant decrease in dyspnea and cough symptoms compared
to those receiving radiotherapy alone.

Tokens: [Dr., Patel, reported, that, the, 30, patients, with, stage, IIIB, non-
small, cell, lung, cancer, who, underwent, concurrent, chemoradiation,
therapy, showed, a, significant, decrease, in, dyspnea, and, cough,
symptoms, compared, to, those, receiving, radiotherapy, alone]

NER_tags: [Dr. (B-PERSON), Patel (I-PERSON), reported (O), that (O), the (O),
30 (B-NUMBER), patients (O), with (O), stage (B-DISEASE), IIIB (B-DISEASE),
non-small (I-DISEASE), cell (I-DISEASE), lung (I-DISEASE), cancer (I-DISEASE),
who (0), underwent (O), concurrent (B-TREATMENT), chemoradiation (I-
TREATMENT), therapy (I-TREATMENT), showed (O), a (O), significant (O),
decrease (O), in (O), dyspnea (B-SYMPTOM), and (O), cough (B-SYMPTOM),
symptoms (I-SYMPTOM), compared (O), to (O), those (O), receiving (O),
radiotherapy (B- TREATMENT), alone (O)]

tit

Figure 3: A sample of the NER dataset.

Jackson, 60 patients with glioblas-

Person

Other

Number

Disease

Treatment

Symptom

liographic information such as the title of the re-

toma were treated using hypofractionated radiation
therapy, administering 40 Gy in 15 fractions", the
model should detect "Dr. Jackson" as a PERSON,
"glioblastoma" as a SYMPTOM, "hypofraction-
ated radiation therapy" as a TREATMENT, and
"40" and "15" as NUMBERs.

The objective of this subset is to evaluate a
model’s ability to identify these entities in radia-
tion oncology text, which is fundamental for struc-
tured information extraction and other downstream
tasks such as de-identification of sensitive patient
information (e.g., names and addresses) (Liu et al.,
2023b).

2.4 Text Summarization

The Text Summarization dataset within the Radia-
tion Oncology NLP Database offers a unique set of
resources for the exploration of text summarization
methods in a highly specialized medical context.
This dataset comprises a variety of research ab-
stracts from arXiv that are categorized under the
Medical Physics class. First, we collected all such
papers published since 2022. We then program-
matically extracted the abstracts from these papers
and asked GPT-4 to produce summaries. Finally,
we manually selected 200 paper abstracts that are
correctly and meaningfully summarized by GPT-4
to form this dataset.

Each record within the dataset includes key bib-

search, the authors involved, the submission date,
the arXiv identifier, the DOI, and the BibTeX entry.
The categorization information is also provided in
the form of classifications.

The core components of each record are the
abstract and its corresponding summary. The ab-
stracts provide a brief, yet comprehensive overview
of the research conducted, including its objectives,
methodology, results, and conclusions. Corre-
spondingly, the summaries distill the critical el-
ements of these abstracts into a concise form, de-
signed to swiftly provide the reader with the key
takeaways of the study.

These pairs of abstracts and their summaries con-
stitute a valuable resource for supervised learning
tasks. They can facilitate the development and
fine-tuning of models focused on abstract summa-
rization within the domain of radiation oncology
and medical physics, a critical area in the broader
field of cancer treatment.

2.5 Question and Answering (QA)

The QA subset of the Radiation Oncology NLP
Database stands as a rigorous and comprehensive
collection of multiple-choice questions encompass-
ing a vast array of topics within the field of ra-
diation physics. We in-house designed 100 ques-
tions comparable to those in the RAPHEX exam
(Hendee et al., 2007), which is a radiation oncology


The binding energies for tungsten’s K, L, and M shells are 70, 11, and 2 keV, respectively. A
50 keV photon interacts with tungsten through the photoelectric effect. What are the possible

energies of the emitted photoelectron?

(a) a continuous spectrum from 2 keV to 70 keV

(b) 39 keV and 48 keV
(c) 9 keV, 59 keV and 68 keV
(d) 9 keV

Figure 4: A sample multiple-choice question from the QA dataset.

physics test-preparation exam for medical physi-
cists and radiation oncologists. We created these
questions from scratch, as we are not legally per-
mitted to reproduce or redistribute materials from
the RAPHEX exam. The questions cover eight cat-
egories: "math-based questions", "basic physics”,
"radiation measurements", "treatment planning”,
"{maging modalities and applications", "brachyther-
apy", "advanced treatment planning", and "safety,
QA, and protection". Figure 4 contains an example
from the QA dataset.

Each question in this dataset demands a deep
understanding of core radiation physics principles.
Topics range from particle acceleration and atomic
mass structures to photon interactions, x-ray spec-
tra, radiation attenuation, and the functionalities of
a linear accelerator. The multiple-choice format
adds a layer of complexity to the challenge, neces-
sitating that AI models not only comprehend the
underlying physics concepts but also discern the
most accurate answer from a selection of closely
related options.

This dataset is designed to mimic the stringent
conditions of academic assessments in the field.
This design aids in creating a realistic test of an
AI model’s abilities in knowledge comprehension,
reasoning, and mathematical computation under
conditions mirroring those in a professional or edu-
cational context.

3 Clinical significance

Logic Reasoning: The development of models
capable of logic reasoning in radiation oncology
holds the promise of aiding professionals in com-
plex decision-making processes. Such models can
enhance the comprehension of intricate clinical
scenarios, support decision-making, and optimize
individualized treatment planning. This would be
particularly useful in areas such as proton therapy

where detailed reasoning often guides the selection
between different treatment strategies (Liu et al.,
2012; Unkelbach et al., 2018; Schild et al., 2014).
Clinical Text Classification: By training models
to classify clinical cases, we are opening avenues
to more personalized and efficient patient care. For
instance, classifying patient cases into categories
such as eligibility for photon versus proton ther-
apy based on patient information and clinical notes
can expedite decision making and improve treat-
ment outcomes (Bitterman et al., 2021; Taylor et al.,
2023).

Named Entity Recognition (NER): NER tasks,
which involve identifying and classifying key in-
formation in text, provide a structured way to ex-
tract critical data points from unstructured clinical
notes. This functionality is crucial in radiation on-
cology where specific entities, such as tumor types,
anatomical locations, or dosimetric parameters, are
paramount for the creation of optimal treatment
plans (Unkelbach et al., 2018; Schild et al., 2014;
Liu et al., 2018).

Text Summarization: The ability to extract the
most salient information from large volumes of text
is valuable in any field (El-Kassas et al., 2021), but
in radiation oncology, it can directly contribute to
improved patient care. For example, summarizing
the key findings of the latest research in radiation
oncology could help clinicians stay updated with
current knowledge without having to go through
lengthy papers, enabling them to swiftly apply
these findings in their practice. In addition, text
summarization can produce succinct descriptions
from lengthy clinical reports and notes (Feblowitz
et al., 2011; Cai et al., 2021; Liu et al., 2023b),
which can significantly save time and facilitate clin-
ical communication.

Question and Answering (QA): QA systems in
the domain of radiation oncology could revolution-


ChatGPT (GPT3.5)
ChatGPT (GPT4)

Med. physicists (Human)
Non-experts (Human)

Average score (%)

g% & % “9,
My 6, YB %
%, 4q os. Ks
2p ¢ a) Qe,
Xe, ° Xp
“ey Gy
%s %,

‘ny Se to, oe
SY, % i, &
@ ty, Oe S
% ®, %, Y Q
“% %, ce % q
%, or % (S) "a
% “G5, a cm 7
2g
io % %,.
“%, * %

Figure 5: A detailed analysis of LLMs’ performance on the medical physics board exam (RAPHEX) level questions.

Bard ChatGPT GPT-4
0.600 0.456 0.656

Table 1: Accuracy of state-of-the-art LLMs on the Logic
Reasoning dataset.

Bard ChatGPT GPT-4
0.770 0.740 0.840

Table 2: Accuracy of state-of-the-art LLMs on the Text
Classification dataset.

ize the way practitioners access and analyze rele-
vant information. Being able to ask specific ques-
tions and receive accurate answers quickly, whether
in terms of patient history, or intricate radiobiologi-
cal effects (Wang et al., 2018; Omer, 2021), would
immensely improve the efficiency of the oncology
practice, saving practitioners time and possibly en-
hancing patient outcomes. The QA dataset in this
database delineates the knowledge needed for ef-
fective QA in this highly specialized domain.

4 A Conversational Instruction Tuning
Dataset based on ROND

To facilitate future development of radiation on-
cology focused language models, we employed
a data generation method to create synthetic data
based on expert annotated data from the six key
components within ROND: Logic Reasoning, Clin-

Bard ChatGPT GPT-4
0.667 0.642 0.785

Table 3: Accuracy of state-of-the-art LLMs on the NER
dataset.

Bard ChatGPT GPT-4
0.139 0.270 0.317

Table 4: BLEU4 scores of state-of-the-art LLMs on the
Text Summarization dataset.

ical Text Classification, Named Entity Recognition
(NER), Text Summarization, Question and Answer-
ing (QA), and Conversational data.

This synthetic data generation process was car-
ried out to build a large dataset suitable for "in-
struction tuning". Instruction tuning is an effec-
tive approach aimed at enhancing language models’
ability to comprehend and follow natural language
inputs based on training on pairs of instruction-
input-outputs (Wei et al., 2021; Peng et al., 2023).
This strategy facilitates multi-task learning and en-
hances generalization for unseen tasks (Ouyang
et al., 2022).

We processed the diverse data formats within
ROND into a unified instruction tuning structure,
where each data entry consists of three components:
instruction, input, and output (please refer to Figure
7 for an example). This uniform structure stream-


Prompt for data
augmentation

demonstration

Radiation Oncology NLP
base

Augmented
data

Instruction

tunnig

Figure 6: Data augmentation and instruction tunning.

r

"instruction": "Please classify the following text about radiation oncology as “Proton therapy” or “Photon therapy” ",
"input": "Treatments for prostate cancer with reduced risk of bowel dysfunction.",

"output": "Proton therapy"
}s

Figure 7: An example from the instruction tuning dataset.

Bard ChatGPT GPT-4
0.410 0.530 0.760

Table 5: Accuracy of state-of-the-art LLMs on the QA
dataset.

ChatGPT CancerChat
26 24

Table 6: Preference evaluation between ChatGPT and
CancerChat.

lines the training and tuning process, contributing
to more efficient learning and inference.

For the data augmentation/synthetic data gener-
ation, we utilized the APIs of ChatGPT and GPT-
4. Specifically, we employed the ChatGPT API
to generate synthetic samples for the Conversa-
tional data, Text Summarization, and Named Entity
Recognition. These tasks require somewhat less
domain-specific understanding of radiation oncol-
ogy, making ChatGPT a fitting choice. On the other
hand, for tasks such as Logic Reasoning, Clinical
Text Classification, and Question Answering that
require more depth of understanding and reason-
ing ability, we used the GPT-4 API. This decision
leverages the strength of each AI model, ensuring
optimal synthetic data generation across different
tasks within the ROND database.

We scaled ROND from a few hundred data sam-
ples to 20,160 instruction tuning pairs. The ma-
jority of these samples, over 17,000, are conversa-

tional, simulating interactions between patients and
healthcare providers. This is supplemented with
500 samples each for Logic Reasoning and Clin-
ical Text Classification, and 1,000 samples each
for Named Entity Recognition and Text Summa-
rization. This ensures a comprehensive dataset that
reflects the multifaceted nature of language process-
ing in the context of radiation oncology, thereby
providing a robust base for developing and refining
future NLP models in this domain. Furthermore,
the emphasis on conversational data equips mod-
els trained on this dataset provides the potential
to create efficient chatbots specialized in radiation
oncology.

4.1 CancerChat

To evaluate the effectiveness and utility of the in-
struction tuning dataset, we trained a demo lo-
cal LLM, CancerChat, based on Falcon-7B (AI-
mazrouei et al., 2023) (a model that recently tops
the Open LLM Leaderboard (Face)). Figure 6 il-
lustrates the process of generating the instructiom-
tuning dataset and the training of CancerChat.
The training was conducted on a server with 1
A100 80GB GPU. We utilize LoRA (Low Rank
Adaptation) (Hu et al., 2021) since LoRA weights
facilitate model sharing and deployment. Our train-
ing parameters were: a batch size of 128, a fixed
learning rate at 3e-4, a lora_r (the rank of the low-
rank factorization) set to 8, a lora_alpha (the scaling
factor for the rank) set to 16, and a dropout rate of


0.05 to mitigate overfitting.

To preliminarily assess the performance of Can-
cerChat, we conducted a blind comparison using
50 queries (see Table 6). A medical physicist was
asked to evaluate and express their preference for
the responses generated by ChatGPT and Cancer-
Chat. The comparison yielded close results, with
26 of the 50 responses more favorable for ChatGPT,
while the remaining 24 preferred CancerChat.

These preliminary results underscore the poten-
tial of CancerChat, which, despite being a smaller
local model specifically tailored for the domain
of radiation oncology, demonstrated competitive
performance when compared to ChatGPT. This
suggests the feasibility and promise of developing
domain-specific language models leveraging our
instruction-tuning dataset, offering a pathway for
future advancements in NLP for medical special-
ties.

5 Benchmarking Large Language Models

We evaluate three state-of-the-art language mod-
els, namely Bard (powered by Google PaLM 2
(Anil et al., 2023)), ChatGPT (OpenAJ), and GPT-
4 (OpenAI, 2023), against the proposed radiation
oncology NLP database and obtained insightful
results.

On the Logic Reasoning dataset, GPT-4 achieved
the highest accuracy of 0.656, followed by Bard
with an accuracy of 0.600, while ChatGPT scored
the lowest with an accuracy of 0.456 (Table 1).

On the Clinical Text Classification dataset, GPT-
4 again outperformed the other two models with an
accuracy of 0.840. However, the performance gap
was narrower with Bard and ChatGPT registering
accuracies of 0.770 and 0.740, respectively (Table
2). Similar trends were observed in the Named
Entity Recognition (NER) dataset, where GPT-4
led with an accuracy of 0.758. Bard scored 0.667,
and ChatGPT trailed with an accuracy of 0.646
(Table 3).

For text summarization (evaluated using the
BLEU score (Papineni et al., 2002)), GPT-4
recorded the highest score of 0.317, with ChatGPT
performing markedly better than Bard, scoring
0.270 compared to Bard’s 0.139 (Table 4). Finally,
on the Question Answering (QA) dataset, GPT-
4 once again demonstrated superior performance
with an accuracy of 0.760. ChatGPT achieved an
accuracy of 0.530, while Bard scored the lowest
with 0.410 (Table 5). We also conducted a detailed

analysis on eight categories of the QA dataset. Gen-
erally, GPT-4 outperformed both trained medical
physicists and non-experts who participated in this
study. The medical physicists averaged an accuracy
of 0.76, while the non-experts averaged 0.28.
These results underscore the effectiveness of
GPT-4 across various NLP tasks. However, while
GPT-4 demonstrated superior performance across
the evaluated NLP tasks in ROND, we hold these
results as a baseline for future research. As state-
of-the-art as these models may be, we envision the
development of more refined models specifically
tailored to the domain of radiation oncology. These
specialized models will likely exhibit improved per-
formance and understanding of the nuances of this
unique field and deliver significant potential for
advancing radiation oncology and clinical NLP.

Limitations

The primary limitation of this study is inherent to
its novelty. ROND represents the first dedicated
NLP dataset in the field of radiation oncology. As
such, it is anticipated that there will be inherent
challenges and unforeseen issues associated with
this dataset that will only become apparent when
the database is utilized practically by the commu-
nity. The efficacy and practical utility of language
models trained or evaluated on this dataset will
need to be validated through real-world applica-
tion and extensive feedback from clinicians. This
iterative process of application, feedback, and re-
finement is essential to not only identify potential
problems but also to improve the robustness and ap-
plicability of the models derived from this dataset.

Ethics Statement

The authors of this study conducted the research
in strict adherence to ethical guidelines and there
is no involvement of Protected Health Information
(PHI). We acknowledge that while this study has
the potential to greatly improve radiation oncology
NLP, it is necessary to consider the need for respon-
sible use and development of AI. Any potential
bias or errors within the dataset can have signifi-
cant implications on model training and subsequent
applications. Thus, we recommend rigorous valida-
tion, including feedback from clinical practitioners,
before deploying models trained on this dataset in
a practical setting. Future research should also be
conducted with the broader social and ethical im-
plications in mind, always prioritizing the safety


and best interests of patients.

References

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Merouane Debbah, Etienne Goffinet, Daniel Hes-
low, Julien Launay, Quentin Malartic, Badreddine
Noune, Baptiste Pannier, and Guilherme Penedo.
2023. Falcon-40B: an open large language model
with state-of-the-art performance.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305. 10403.

Jacques Bernier, Eric J Hall, and Amato Giaccia. 2004.
Radiation oncology: a century of achievements. Na-
ture Reviews Cancer, 4(9):737-747.

Danielle S Bitterman, Timothy A Miller, Raymond H
Mak, and Guergana K Savova. 2021. Clinical natural
language processing for radiation oncology: a review
and practical primer. International Journal of Radia-
tion Oncology* Biology* Physics, 110(3):641-655.

Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg, et al. 2023. Sparks of artificial general intelli-
gence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712.

Xiaoyan Cai, Sen Liu, Junwei Han, Libin Yang, Zhen-
guo Liu, and Tianming Liu. 2021. Chestxraybert: A
pretrained language model for chest radiology report
summarization. [EEE Transactions on Multimedia.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.

Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea,
and Hoda K Mohamed. 2021. Automatic text sum-
marization: A comprehensive survey. Expert systems
with applications, 165:113679.

Hugging Face. Open LLM Leaderboard - a Hug-
ging Face Space by HuggingFaceH4 — hug-
gingface.co. https://huggingface.co/spaces/
Hugg ingFaceH4/open_1l1m_leaderboard.

Joshua C Feblowitz, Adam Wright, Hardeep Singh,
Lipika Samal, and Dean F Sittig. 2011. Summa-
rization of clinical information: a conceptual model.
Journal of biomedical informatics, 44(4):688-699.

William R Hendee, Howard I Amols, and Colin G Or-
ton. 2007. The abr written and oral examinations

in medical physics as currently conducted are suf-
ficiently comprehensive and demanding to ensure
that successful candidates have adequate knowledge
and experience to practice in the designated specialty
field. Medical physics, 34(9):3417-3419.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685.

Chenbin Liu, Terence T Sio, Wei Deng, Jie Shan,
Thomas B Daniels, William G Rule, Pedro R Lara,
Shawn M Korte, Jiajian Shen, Xiaoning Ding, et al.
2018. Small-spot intensity-modulated proton therapy
and volumetric-modulated arc therapies for patients
with locally advanced non-small-cell lung cancer: a
dosimetric comparative study. Journal of applied
clinical medical physics, 19(6):140-148.

Wei Liu, Xiaodong Zhang, Yupeng Li, and Radhe Mo-
han. 2012. Robust optimization of intensity modu-
lated proton therapy. Medical physics, 39(2):1079-
1091.

Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,
Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,
Mengshen He, Zhengliang Liu, et al. 2023a. Sum-
mary of chatgpt/gpt-4 research and perspective to-
wards the future of large language models. arXiv
preprint arXiv:2304.01852.

Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu,
Chao Cao, Haixing Dai, Lin Zhao, Wei Liu, Ding-
gang Shen, Quanzheng Li, et al. 2023b. Deid-gpt:
Zero-shot medical text de-identification by gpt-4.
arXiv preprint arXiv:2303.11032.

Hiba Omer. 2021. Radiobiological effects and medical
applications of non-ionizing radiation. Saudi Journal
of Biological Sciences, 28(10):5585-5592.

OpenAI. Introducing ChatGPT — openai.com. https:
//openai.com/blog/chatgpt.

OpenAI. 2023. Gpt-4 technical report. arXiv.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. ‘Training language models to follow in-
structions with human feedback. arXiv preprint
arXiv:2203.02155.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311-318.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277.


Saed Rezayi, Haixing Dai, Zhengliang Liu, Zihao Wu,
Akarsh Hebbar, Andrew H Burns, Lin Zhao, Dajiang
Zhu, Quanzheng Li, Wei Liu, et al. 2022. Clinical-
radiobert: Knowledge-infused few shot learning for
clinical notes named entity recognition. In Machine
Learning in Medical Imaging: 13th International
Workshop, MLMI 2022, Held in Conjunction with
MICCAI 2022, Singapore, September 18, 2022, Pro-
ceedings, pages 269-278. Springer.

Steven E Schild, William G Rule, Jonathan B Ashman,
Sujay A Vora, Sameer Keole, Aman Anand, Wei
Liu, and Martin Bues. 2014. Proton beam therapy
for locally advanced lung cancer: A review. World
journal of clinical oncology, 5(4):568.

Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-
davi, Jason Wei, Hyung Won Chung, Nathan Scales,
Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,
et al. 2022. Large language models encode clinical
knowledge. arXiv preprint arXiv:2212.13138.

Paige A Taylor, Elizabeth Miles, Lone Hoffmann,
Sarah M Kelly, Stephen F Kry, Ditte Sloth Mgller,
Hugo Palmans, Kamal Akbarov, Marianne C Aznar,
Enrico Clementel, et al. 2023. Prioritizing clinical
trial quality assurance for photons and protons: A
failure modes and effects analysis (fmea) compari-
son. Radiotherapy and Oncology, 182:109494.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Roziére, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.

Jan Unkelbach, Markus Alber, Mark Bangert, Rasmus
Bokrantz, Timothy CY Chan, Joseph O Deasy, Albin
Fredriksson, Bram L Gorissen, Marcel Van Herk,
Wei Liu, et al. 2018. Robust radiotherapy planning.
Physics in Medicine & Biology, 63(22):22TRO2.

Rong Wang, Tingyang Zhou, Wei Liu, and Li Zuo.
2018. Molecular mechanism of bystander effects
and related abscopal/cohort effects in cancer therapy.
Oncotarget, 9(26):18637.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.

Lin Zhao, Lu Zhang, Zihao Wu, Yuzhong Chen, Haixing
Dai, Xiaowei Yu, Zhengliang Liu, Tuo Zhang, Xintao
Hu, Xi Jiang, et al. 2023. When brain-inspired ai
meets agi. arXiv preprint arXiv:2303.15935.

10
