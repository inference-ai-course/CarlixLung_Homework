arX1v:2510.09790v1 [cs.CL] 10 Oct 2025

Preprint under review

STEERING EMBEDDING MODELS WITH GEOMETRIC
ROTATION: MAPPING SEMANTIC RELATIONSHIPS
ACROSS LANGUAGES AND MODELS

Michael Freenor & Lauren Alvarez
Fuel ix, TELUS Digital
{michael.freenor, lauren.alvarez}@fuelix.ai

ABSTRACT

Understanding how language and embedding models encode semantic relation-
ships is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (’king” - ’man” + ”woman” =
*queen’), modern high-dimensional text representations lack straightforward in-
terpretable geometric properties. We introduce Rotor-Invariant Shift Estimation
(RISE), a geometric approach that represents semantic transformations as consis-
tent rotational operations in embedding space, leveraging the manifold structure
of modern language representations. RISE operations have the ability to operate
across both languages and models with high transfer of performance, suggesting
the existence of analogous cross-lingual geometric structure. We evaluate RISE
across three embedding models, three datasets, and seven morphologically di-
verse languages in five major language groups. Our results demonstrate that RISE
consistently maps discourse-level semantic transformations with distinct gram-
matical features (e.g., negation and conditionality) across languages and models.
This work provides the first systematic demonstration that discourse-level seman-
tic transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

1 INTRODUCTION

Understanding how contemporary language models encode and manipulate semantic knowledge has
become a central challenge in deep learning interpretability. The ability to interpret (probe) and con-
trol (steer) these internal representations is fundamental to developing trustworthy, safe AI systems.
In word2vec and similar models, semantic relationships could be captured
with simple vector arithmetic in the embedding space (i.e. the famous “king” - “man” + “woman”
= “queen” analogy). This geometric transparency offered both interpretability and controllability,
enabling researchers to navigate semantic space through intuitive mathematical operations.

However, this clarity has largely disappeared in modern transformer-based language models. While
large language models (LLMs) have achieved remarkable performance across diverse language tasks
(Achiam et al.) (2023), their internal workings remain largely opaque
2021), limiting our ability to understand, predict, and control their behavior
in critical applications. Unlike the interpretable directions found in static word embeddings, the
geometry of modern text representations lacks the same straightforward correspondence to semantic
operations. This opacity poses significant challenges for understanding how these models organize
linguistic knowledge and limits our ability to control their behavior in principled ways.

The central challenge lies in identifying which geometric operations correspond to meaningful se-
mantic transformations in these complex representation spaces. Current approaches often rely on

task-specific probes (Rogers et al. Hewitt & Manning ;|Alain & Bengio ) or steer-

ing vectors (Zou et al. Merullo et al.
2023), but lack generalizable frameworks for systematically mapping semantic relationships

to geometric structure. Without such principled methods, we cannot determine whether the geo-



Preprint under review

metric regularities that made static word embeddings interpretable persist in modern language or
embedding models, albeit in more complex forms.

We address this gap by introducing Rotor-Invariant Shift Estimation (RISE), a geometric approach
that represents semantic transformations as consistent rotational operations in embedding space,
leveraging the manifold structure of modern language representations. RISE is a rotor-based align-
ment method that identifies cross-lingual and cross-model geometric transformations. Specifically,
we demonstrate how RISE identifies and generalizes three discourse-level semantic changes (nega-
tion, conditionality, and politeness) across seven morphologically distinct languages. The goal of
this study is to develop a framework for identifying discourse-level semantic changes that cor-
respond to consistent geometric transformations, and determine how well these transforma-
tions can be mapped across model architectures and typologically diverse languages. Our ap-
proach treats semantic transformations as rotations on the unit hypersphere where sentence embed-
dings reside, enabling us to align different linguistic contexts into a common geometric framework.
This paper presents evidence that certain semantic transformations exhibit generalizable geomet-
ric structure while others vary based on context-dependence, extending the linear representation
hypothesis to cross-lingual discourse. We demonstrate this through systematic experiments across
three models and seven languages, revealing that negation, conditionality, and politeness transfor-
mations can be captured as consistent rotational operations.

2 RELATED WORK

2.1 LINEAR REPRESENTATION HYPOTHESIS

The linear representation hypothesis (LRH), or linear subspace hypothesis, has emerged as a promis-

ing theory for bridging the interpretability gap for embeddings (Mikolov et al.} }2013b
Goldberg} |2014}|Bolukbasi et al.}/2016}|Ethayarajh)|2019 2024} |2025). The LRH posits

that semantic concepts are encoded as linear structures within embedding spaces, meaning linear
algebraic operations can be used for interpretation and control (e.g., ’king” - *man” + ”’woman” =

*queen” presented by |Mikolov et al.|(2013b)). (2024) formalized the LRH by unifying

three distinct notions of linearity that had developed independently across the literature:

Mimno & Thompson} |2017
Ethayarajh et al.||2018 2019} |Li et al. Hewitt & Manning
Chang et al.

2. logistic probing (Alain & Bengio} |2017 2018 2020}
2022 2022 2022 2023), and

3. steering vectors (Wang et al.}{2023 2023} [Merullo et al.|/2023
2023).

(2024) theoretical framework addresses a critical gap by synthesizing the first formaliza-
tion of what “linear representation” means. However, while the LRH has been validated primarily

within individual languages, there remains a significant gap in understanding how semantic transfor-
mations generalize across linguistic contexts. Most existing work examines static concept encodings
rather than dynamic semantic transformations that reflect real-world lan-
guage use. Our work is the first to extend the LRH to multilingual contexts and embedding models,
though the linear representations we consider are not Euclidean lines but geodesic flows.

1. word2vec-like embedding differences (

2.2 LINEAR & GEOMETRIC REPRESENTATION TECHNIQUES

The geometric foundations established by [Park et al.](2024) are crucial for understanding when and
why linear algebraic operations succeed in capturing semantic relationships. With traditional Eu-
clidean geometry, it is hard to accept that arbitrary dot products or cosine similarities have semantic
meaning. Moreover, (2024) demonstrated that the choice of inner product fundamen-
tally determines the interpretability of geometric operations, providing principled foundations for
representation analysis. Our work builds directly on recent advances in understanding linear rep-

resentations in language models (Park et al.| (2024 2023). RISE implements a technique
that respects semantic structure, similar to the geometric framework developed by (2024).


Preprint under review

While previous work focused primarily on categorical concepts and word-level transformations,
RISE extends our understanding to sentence-level, discourse-level transformations and cross-lingual
analysis with seven morphologically diverse languages.

2.2.1 STEERING VECTORS & EMBEDDING MODELS

The practical applications of linear representation theory have been explored through steering vector

techniques. {Turner et al.|(2023), [Liu et al.](2023), and|Zou et al.|(2023) demonstrated that targeted
modifications to internal, latent space representations can systematically alter model behavior with-
out parameter updates. The majority of steering vector research
is connected to activation steering, only investigating the im-

pact of steering vectors in the activation, hidden, and/or latent layer of an LLM. Recently, |Pham &
introduced Householder Pseudo-Rotation (HPR), which addresses activation norm
consistency issues in LLM behavioral modification through direction-magnitude decomposition and
pseudo-rotational transformations. Building on the insight that geometric approaches outperform
additive methods, our work extends geometric reasoning to semantic transformations in embedding
space through Riemannian operations. To our knowledge, there is no work investigating the appli-
cation of steering vectors to embedding models — only completion models. This study contributes
theoretical insights to the usefulness of steering vectors for embedding model interpretation and
control.

2.3. CHALLENGES IN GENERALIZATION AND RELIABILITY

Current knowledge about the generalization properties of linear representations reveals significant
limitations. The taxonomy of generalization research in natural language processing (NLP)
provides a framework for evaluating robustness, but systematic applications to
representation-based techniques (i.e., steering, probing, or embedding manipulation) have been lim-
ited. Recent empirical studies have revealed that steering vector effectiveness varies substantially
across different inputs and contexts (2024). Secondly, the relationship between local
and global linearity represents a particularly critical gap in current understanding. There have been
numerous demonstrations of local linear behavior within specific domains or prompt formats, but
achieving global linearity (generalizable to multiple model architectures with different pre-training)
as required by strong versions of the LRH, remains challenging. While many studies demonstrate
impressive results in controlled settings, they often fail to address the robustness needed in practical
applications. This study contributes to the literature gap by presenting a robust framework for geo-
metrically identifying discourse-level semantic changes across typologically diverse languages and
model architectures.

3. THEORETICAL MOTIVATION

The limitations identified above point toward a fundamental theoretical challenge: existing ap-
proaches operate in Euclidean space while modern embeddings live on curved manifolds. This
geometric mismatch may explain why steering vectors show inconsistent cross-context performance
and why linear methods struggle with robust generalization. We hypothesize that discourse-level
semantic transformations correspond to intrinsic geometric operations on the embedding manifold,
rather than fixed directions derived from Euclidean computations. If semantic transformations can be
characterized as consistent rotational operations on the unit hypersphere where embeddings reside,
this would provide both theoretical grounding for the Linear Representation Hypothesis in
curved spaces (through geodesics) and practical tools for cross-lingual semantic control. Test-
ing this hypothesis requires robust evaluation across diverse languages and embedding architectures
to determine whether geometric consistency reflects universal semantic properties or model-specific
artifacts.

4 ROTOR-INVARIANT SHIFT ESTIMATION (RISE)

Modern sentence embeddings from multilingual encoders reside approximately on a unit hyper-
sphere in high-dimensional space when the training objective enforces or fixes the £2-norm con-

straints 2020), the embeddings are normalized to unit length (Reimers & Gurevych


Preprint under review

2019), or the model is designed to produce isotropic embeddings 2020} |Ethayarajh| |2019).

Local semantic transformations (e.g., negation, politeness, conditionality) can be understood as ro-
tational displacements on this sphere. The key insight is that these displacements can be interpreted
and controlled by aligning different contexts to a common geometric frame.

For any neutral sentence embedding n € S¢~! and its semantically transformed variant v € S¢~?,
we can compute an orthogonal transformation (Clifford-algebraic rotor) R(n) that aligns n to a
canonical reference direction e;. By applying this same transformation to v, we express the semantic
change in a standardized coordinate system:

§ = R(n) log, (v), (1)

where log,,(v) denotes the Riemannian logarithm that computes the tangent vector from n to v on
the hypersphere, and R(n) aligns the tangent vector to the canonical reference direction. Normalized
embeddings reside on a unit hypersphere, where geodesics define the shortest paths between points,
preserving the manifold’s intrinsic geometry rather than imposing Euclidean distance measures.
These geodesic paths represent the natural notion of “line” in the embedding space, as they define
the shortest distance between two points on the surface. By working with geodesics, we ensure our
semantic transformations are consistent with the manifold structure. To “flatten” out the curved arc
to a straight vector, the Riemannian logarithmic map log,,(v) produces the vector from n to v on a
tangent plane at n. By operating within the tangent space at n, geodesic differences can be treated
as ordinary vectors.

4.1 THE ROTOR ALIGNMENT ALGORITHM

RISE proceeds in three steps:

Canonicalization. For each neutral—transformed sentence pair (n;, v;), compute a rotor R(n,) that
maps n, to the reference direction e;. We interpret canonicalization as controlling for the semantics
present in the first elements of our pairs. By applying the canonical rotation to the second of the
two the idea is that we have isolated the key differences between the elements in a fixed frame of
reference.

Prototype Learning. Canonicalize all semantic changes into the reference frame and average all

the tangent vectors to calculate one Prototype p, where / is the total amount of sentence pairg'

This is a similar technique to mean-centering (Jorgensen et al.||2024):

1 M
P= yD R(ni) log, (vi). (2)

Prediction. To predict the semantic transformation for an unseen neutral embedding n*, the proto-
type p can be used to predict the transformation embedding v* by converting the prototype p with
the Riemannian exponential map and an application of the transpose of n*’s canonicalizing rotor:

uv" = exp,«(R(n*)Tp). (3)

R(n*)T 7p rotates p into the tangent space at n* . Then the Riemannian exponential exp,,.() takes
the tangent vector p and moves along the geodesic starting at n*. The vector direction is which
geodesic to follow and the length is how far along that arc to go (in radians).

4.2 DIFFERENTIATION FROM RELATED WORK

Our approach is related to recent advances in understanding linear representations in language mod-
els. As discussed in Section 2.2, [Park et al.|(2025) use a “causal inner product” that respects semantic
structure in a function space using the Riesz isomorphism. However, RISE uses Riemannian geom-
etry to operate consistently on the curved manifolds. Both methods take advantage of geometric
properties, but the methods are distinctly different.

‘For small angular differences, first-order equivalent to simply averaging the points and re-normalizing after
the fact.


Preprint under review

Crucially, RISE transformations exhibit commutativity: applying multiple semantic transformations
yields consistent results regardless of order (see Appendix[Ap. This commutativity property provides
strong evidence for the LRH, as it demonstrates that semantic transformations behave like vector
additions in the tangent space—geodesics serve as the curved-space generalization of straight lines.
The preservation of additive structure across semantic operations suggests that the geometric frame-
work captures fundamental algebraic properties of meaning composition. We discuss more about
the commutativity properties in Appendix [A]

Furthermore, the analysis in |Park et al.| (2025) focused on categorical relationships in the unem-
bedding space of language models; our work examines discourse-level transformations in sentence

embeddings across multiple languages. RISE effectively implements a non-Euclidean transforma-
tion that aligns with the natural curved manifold structure of the embedding space. This connection
to high-dimensional geometry provides theoretical grounding for why rotational operations can cap-
ture semantic transformations more effectively than simple vector additions, and extends the linear
subspace hypothesis to curved/geodesic subspaces.

5 EXPERIMENTAL DESIGN

5.1 DISCOURSE-LEVEL SEMANTIC CHANGES & LANGUAGE SELECTION

We focus on three discourse-level semantic transformations that vary in their context-dependence:

Negation: The logical reversal of the propositional content of a statement; where the proposition is
*P” we take the negation to be ’not-P.’ Moreso, we are negating the predicate. This transformation is
semantically precise and should exhibit high geometric consistency across contexts and languages.

Conditionality: Converting declarative statements into conditional constructions (“P” > “If P”).
This introduces modal semantics that may interact with contextual factors.

Politeness: Increasing the social formality or deference level of utterances. This is highly context-
dependent and culturally variable, making it a challenging test case for geometric consistency.

We selected seven morphologically diverse languages to ensure broad coverage of morphological,
syntactic phenomena, and resource levels: English, Spanish, Japanese, Tamil, Thai, Arabic, and
Zulu. This selection spans multiple language families (Indo-European, Sino-Tibetan, Dravidian,
Afroasiatic, Niger-Congo) and different morphological types (analytic, agglutinative, fusional). The
languages also represent different levels of language model availability and resources. The diversity
is crucial because different languages realize semantic transformations through distinct linguistic
mechanisms. For instance, negation might be expressed through: (1) Particles (i.e. English “not’);
(2) Affixes (i.e. Tamil verb-internal negation, Japanese “nai”); and (3) Auxiliary constructions (var-
ious languages). By testing across this range, we can determine whether geometric consistency
reflects universal semantic properties or is merely an artifact of particular linguistic structures.

5.2 DATASETS & EMBEDDING MODELS

We use three datasets and three models for evaluation. We synthetically generated one dataset, re-
ferred to as the Synthetic Multilingual dataset, and used two open-source datasets: The Bench-
mark of Linguistic Minimal Pairs (BLiMP) (Warstadt et al.) and Sentences Involving
Compositional Knowledge (SICK) (Marelli et al.|/2014). For each language-transformation com-
bination in the Synthetic Multilingual dataset, we generated 1,000 neutral-transformed sentence

pairs using GPT-4.5 with carefully controlled prompts (see Appendix |C). To ensure robust analysis,
we implemented several diversity controls (see Appendix |D).

We compare three multilingual embedding models: OpenAI’s text-embedding-3-large (OpenAI
2024), Beijing Academy of AI’s bge-m3 (Chen et al.|/2024), and Google’s mBERT (Devlin et al.

2019). The text-embedding-3-large model produces 3072-dimensional vectors, bge-m3 produces
1024-dimensional vectors, and mBERT produces 768-dimensional vectors. All selected models
produce constant-length embeddings that reside on a hypersphere making them suitable for our
geometric analysis. This dimensional diversity allows us to test whether RISE effectiveness depends
on embedding dimensionality. We calculate a rotor alignment score where the scores represent mean


Preprint under review

cosine similarity between predicted embedding vectors and the semantically transformed pair on
held-out test sets, with higher values indicating more consistent geometric structure.

6 RESULTS

6.1 CROSS-LANGUAGE TRANSFER COMPARISON

This section discusses the comparison of models trained in one of the seven languages and tested
on all seven. The results of this section demonstrate RISE multilingual performance computed by
three embedding models. See Appendix |B] for comprehensive results across all phenomena for each
model.

Negation emerges as the most robust discourse-level, semantic transformation, achieving the high-
est mean rotor alignment score (0.788) across all model-language combinations with performance
ranging from 0.686 to 0.918. Figure [I]demonstrates RISE performance on negation for each model.
RISE transformations for negation are most geometrically consistent in text-embedding-3-large.
Negation’s strong performance indicates that generalizable discourse-level, semantic changes are
captured by RISE and best applied cross-lingually in text-embedding-3-large.

RISE Cross-Language Transfer Comparison - Negation
mBERT

text-embedding-3-large

ES EN AR

Source Language
JA

«

JA TA TH ZU
Target Language

Figure 1: Embedding model heatmap cross-lingual transfer comparison on negation.

AR J ES JA tT OTH
Target Language Target Language

Conditionality demonstrates the highest stability and consistency across cross-language transfers,
with the lowest performance variability (0.038) and most stable individual measurements (see Ap-
pendix[B). With the second highest, mean performance(0.780), conditionality is particularly consis-
tent results across all combinations. The strong transfer seen in bge-m3 and text-embedding-3-large
suggests that conditional semantics are captured by stable geometric structure despite their modal
complexity.

Politeness exhibits the most variable geometric structure, ranking third in performance (0.762
mean) with the highest performance variability (0.060) across combinations. This variability aligns
with expectations, as politeness realizations depend heavily on cultural context and linguistic con-
ventions, making cross-language transfer inherently more challenging.

The contrast across phenomena performance reflects an interesting insight. In the results, negation
appears more robust, politeness is most variable, and conditionality sits between. This suggests em-
beddings encode logical semantic operators (negation and conditionality) with strong cross-lingual
consistency. However, pragmatic operators (politeness) are less reliable due to inherent language-
specific indicators and cultural conventions. Additionally, cross-language analysis revealed dimen-
sionality does not directly predict cross-lingual performance. Despite having lower dimensionality,
bge-m3 (1024-dim) demonstrated the least variance in cross-language performance for all phenom-
ena and languages. While text-embedding-3-large (3072-dim) showed highest cross-language per-
formance (Figure |2), mBERT (768-dim) showed strong monolingual performance, but exhibited
high variability, particularly for politeness in cross-language settings. These results highlight that
training methodology and architectural choices matter more than raw embedding dimensionality for
cross-language semantic transfer.

The cross-language analysis presented in Appendix [B] supports our hypothesis that discourse-level
semantic transformations correspond to intrinsic geometric operations on the embedding manifold,
rather than fixed directions derived from Euclidean computations. The variation across models,


Preprint under review

preservation of linguistic relationships across languages, and transformation patterns indicate that
RISE successfully identifies sematic transformation on the embedding manifold. The limitations
and future work are scared further on.

RISE Cross-Language Transt tionality
text-e1

RISE Cross-Language Transfer: Negatior RISE Cross-Lang nsfer: Polite
text-embedding-3-large text-embeddin ma oe rge

s
* aang Oanuige ‘arg

ge

Figure 2: Cross-language transfer heatmaps for vext-embedding- -3-large model showing RISE per-
formance across all language pairs for conditionality, negation, and politeness transformations.
Darker colors indicate higher cosine similarity between predicted and target embeddings.

6.2 CROSS-MODEL TRANSFER COMPARISON

To evaluate RISE prototypes’ robustness to transfer across different embedding architectures, we
conducted cross-model mapping experiments using the method developed by [Morris et al.| (2020).
This approach learns statistical mappings between embedding spaces through principal component
analysis and distributional alignment, enabling transfer of learned RISE prototypes from one model
to another. We specifically examined transfer from text-embedding-3-large (3072-dimensional) to
bge-m3 (1024-dimensional), demonstrating cross-model semantic transfer across different dimen-
sionalities and training objectives. For each language pair and phenomenon, we learn RISE pro-
totypes in text-embedding-3-large using 80% of the data, map these prototypes and e; to bge-m3
space, and evaluate performance on native bge-m3 embeddings using the remaining 20%. Figure 3]
demonstrates comprehensive cross-model and cross-language transfer results.

Cross-model transfer from text-embedding-3-large to bge-m3 reveals strong language-dependent
performance. English achieves 0.80-0.82 similarity across all transformations, while other lan-
guages cluster around 0.70-0.75, and Zulu consistently scores 0.63-0.66. This 20% performance
gap persists across conditionality, negation, and politeness transformations. These results suggest
rotations can transfer between architecturally different models, but their effectiveness depends crit-
ically on source language, indicating that learned transformations are not architecture-independent.
The consistent English advantage across models suggests these embedding spaces share more robust
geometric structures for English, likely reflecting training data imbalances (Anglo-centric bias in the
composition of the model’s training data). The consistent language ranking across different seman-
tic transformations (conditionality, negation, politeness) suggests the bias is structural rather than
semantic. In conclusion, RISE successfully captures semantic patterns that perform consistently in
a cross-model comparison.

Figure 3: Cross-Model Semantic Transfer: textembedding- -3-large > bee. -m3. Each cell shows
transfer performance from source language prototype (text-embedding-3-large) to target language

test set (bge-m3). Diagonal elements represent pure cross-model transfer, while off-diagonal el-
ements show combined cross-model and cross-language transfer using Morris statistical mapping

(Moms et al, 2020).


Preprint under review

6.3. ENGLISH TASK-BASED COMPARISON

Our main investigation is how well RISE peforms in in multi-lingual settings. However there are
limited external datasets for evaluating the performance discourse-level, semantics transformation
tasks. Due to the limited resources, we had to select the most related datasets, BLiMP and SICK.
BLiMP is LLM evaluation paired sentence dataset for major grammatical phenomena in English,
and SICK is a dataset with paired sentences with entailment, contradiction, and neutral labels.

Table[I]summarizes RISE performance across the three datasets. The results confirm that all models
achieve strong performance, with particular strengths varying by dataset: mBERT excels on gram-
matical tasks (BLiMP) and contradiction detection (SICK), while bge-m3 shows the most consistent
performance across synthetic multilingual data. The dramatic performance gap between BLIMP
(>0.92) and SICK (0.62-0.74) suggests that RISE rotations might be capturing something more
specific than general semantic transformations.

The high BLIMP performance indicates RISE excels at preserving grammatical/syntactic structure,
while the moderate SICK performance suggests these same rotations don’t preserve semantic re-
latedness as well. These results show that benchmark choice dramatically affects relative model
ranking. Instead, robustness depends on whether the task prioritizes cross-lingual consistency (fa-
voring bge-m3) or raw performance on specific phenomena (favoring text-embedding-3-large for
negation, mMBERT for grammatical tasks).

Table 1: RISE Validation: Performance Across Three Validation Datasets. The performance is
measured with the rotor alignment score between RISE-steered embeddings and target embeddings
where bold values indicate best performance per dataset.

Model Synthetic Multilingual BLiMP Benchmark SICK Dataset
OpenAI (3072d) 0.771 0.929 0.623
BGE-M3 (1024d) 0.782 0.956 0.631
mBERT (768d) 0.709 0.961 0.736
Average 0.754 0.949 0.663

7 DISCUSSION

Our findings demonstrate that meaningful semantic operations can be recovered as geometric trans-
formations in modern language model representations. RISE successfully identifies consistent ge-
ometric structure for discourse-level semantic changes, primarily for text-embedding-3-large and
negation in multilingual settings. These findings provide positive results for extending the inter-
pretable geometry of early word embeddings to contemporary embeddings in high dimensional
spaces.

Evaluation benchmarks (Table [I) reveal task-dependent effectiveness. RISE achieves near-perfect
performance on syntactic acceptability (BLIMP: 0.93-0.96) but only moderate performance on se-
mantic similarity (SICK: 0.62-0.74), suggesting better alignment with grammatical rather than se-
mantic transformations. Section 6.1 shows that negation and conditionality are the most gener-
alizable discourse-level, semantic changes captured by RISE and best applied cross-lingually in
text-embedding-3-large. Together these results support that RISE is most successful at identify-
ing semantic transformation with distinct grammatical factors, but more work is needed to justify
semantic transformations in multilingual models are universal geometric operations.

8 LIMITATIONS AND FUTURE WORK

First, our analysis focuses on three specific linguistic transformation types. Future work should ex-
pand to additional semantic and pragmatic phenomena to test the generality of geometric consistency
principles. Second, while our experiments used three diverse embedding models (text-embedding-
3-large, bge-m3, and mBERT), validation across additional architectures would strengthen claims
about the universality of geometric semantic structure. Third, while our language selection spans
diverse typological features, the reliance on GPT-4.5 for data generation may introduce subtle biases


Preprint under review

toward English-centric conceptualizations of semantic phenomena. Future work should incorporate
more diverse data sources and validation by native speakers.

9 BROADER IMPLICATIONS FOR MULTILINGUAL MODEL
INTERPRETABILITY

The cross-linguistic transfer results connect to broader research on multilingual representation learn-
ing (2022). Our findings provide support that modern multilingual
embedding models share similar geometric structures for semantic operations with distinct grammat-
ical factors (e.g. negation and conditionality), but more work is needed to understand how complex
semantic transformation (e.g. politeness) are represented by theories of universal semantic repre-
sentation. These results have implications for language model interpretability. The demonstrated
ability to learn geometric transformations for semantic operations suggests that: (1) Despite their
complexity, modern language models maintain interpretable geometric structure for certain types of
semantic relationships. (2) The cross-linguistic consistency of geometric transformations suggests
that multilingual models learn universal semantic representations that transcend language-specific
surface features. (3) The ability to predict semantic transformations geometrically opens possibili-
ties for more principled control of model behavior through targeted geometric interventions.

10 CONCLUSION

The ability to learn geometric transformations for discourse changes relates to work on controllable
text generation and steering vectors (Turner et al.}/2023}|Li et al.|{2023). Our rotor-based approach,
RISE, provides a geometric framework for understanding and implementing semantic control in lan-
guage models. This work investigated whether discourse-level semantic transformations in multilin-
gual embedding spaces correspond to intrinsic geometric operations, specifically rotations identified
through the RISE method. Our comprehensive evaluation across multiple models, languages, and
validation tasks reveals a more complex reality than initially hypothesized. This work demonstrates
that modern language model representations maintain interpretable geometric structure for some se-
mantic transformations, extending the promise of geometric semantics from early word embeddings
to contemporary transformer models. We show that:

1. Semantic transformations exhibit consistent geometric structure, with consistency varying
systematically based on context-independence.

2. Cross-linguistic transfer of geometric transformations achieves consistent performance,
supporting theories of universal semantic representation in multilingual models.

3. The RISE method provides a framework for identifying semantically meaningful geometric
structure in high-dimensional representation spaces.

As language models continue to evolve, understanding these geometric foundations will be crucial
for developing more interpretable and controllable AI systems. By revealing transferable geometric
structure in semantic transformations (e.g. negation and conditionality), this work opens new possi-
bilities for understanding and controlling language model behavior through geometric interventions.
Our work establishes geometric steering as a principled approach to cross-lingual semantic control,
achieving 77%-95% cross-language transfer effectiveness across typologically diverse languages.
By developing RISE, we demonstrate that interpretable structure exists for some grammatically
distinct semantic transformations, providing a tools for understanding and controlling how these
systems encode semantic knowledge. While RISE remains valuable for analyzing model-specific
semantic structures, claims about universal geometric operations require substantial qualification.
Our cross-model transfer experiments expose an English-centric bias, with English achieving 20%
higher transfer scores than languages like Zulu. This English-centric bias persists across all semantic
transformations, indicating that current multilingual models encode geometric structures that prior-
itize English. Future work should focus on developing more equitable multilingual representations
and investigating which language-specific geometric structures are an inherent feature of the models.


Preprint under review

REFERENCES

P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton
University Press, 2008.

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. arXiv preprint arXiv:2303.08774, 2023.

Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classi-
fier probes. In International Conference on Learning Representations, 2017. URL [https :|
//openreview.net/forum?id=ryF7rTgqgl

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions of the Association for Computational
Linguistics, 4:385-399, 2016.

Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational
Linguistics, 48(1):207-219, 2022.

Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man is to
computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in
Neural Information Processing Systems, 2016.

Tyler Chang, Zhuowen Tu, and Benjamin Bergen. The geometry of multilingual language model
representations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 119-136, 2022.

Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing. Prob-
ing bert in hyperbolic spaces. arXiv preprint arXiv:2104.03869, 2021.

Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding:
Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge dis-

tillation. arXiv preprint arXiv:2402.03216, 2024. URL https: //arxiv.org/abs/2402.

0321.6

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers), pp. 4171-4186. Association for Computational
Linguistics, 2019.

Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposi-
tion. arXiv preprint arXiv:2209. 10652, 2022.

Kawin Ethayarajh. How contextual are contextualized word representations? comparing the ge-
ometry of bert, elmo, and gpt-2 embeddings. In Proceedings of EMNLP-IJCNLP, pp. 55-65,
2019.

Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. Towards understanding linear word analo-
gies. arXiv preprint arXiv: 1810.04882, 2018.

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-
agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 878-891, 2022.

Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary space. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pp. 30-45, 2022.

John Hewitt and Christopher D Manning. A structural probe for finding syntax in word representa-
tions. In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume I (Long and Short Pa-
pers), pp. 4129-4138, 2019.

10


Preprint under review

Wataru Hirota, Masahiro Tanaka, Sho Takase, Naoaki Okazaki, and Kentaro Inui. Emu: Enhancing
multilingual sentence embeddings with 12 constrained softmax loss. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pp. 7904-7911, 2020. doi: 10.1609/aaai.v34i05.
6301.

Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel,
Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, et al. A taxonomy
and review of generalization research in nlp. Nature Machine Intelligence, 5(10):1161-1174,
2023.

Shawn Im and Yixuan Li. A unified understanding and evaluation of steering methods. arXiv
preprint arXiv:2502.02716, 2025.

Rishi Jha, Collin Zhang, Vitaly Shmatikov, and John X. Morris. Harnessing the universal geometry

of embeddings. arXiv preprint arXiv:2505.12540, 2025. URL|https://arxiv.org/abs/

Yibo Jiang, Bryon Aragam, and Victor Veitch. Uncovering meanings of embeddings via partial
orthogonality. arXiv preprint arXiv:2310.17611, 2023.

Ole Jorgensen, Dylan Cope, Nandi Schoots, and Murray Shanahan. Improving activation steering
in language models with mean-centring. In Responsible Language Models Workshop at AAAI-24,
2024.

Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors
(tcav). In International Conference on Machine Learning, pp. 2668-2677. PMLR, 2018.

Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.
In Proceedings of CoNLL, pp. 171-180, 2014.

Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence em-
beddings from pre-trained language models. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 9119-9130, 2020.

Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Watten-
berg. Emergent world representations: Exploring a sequence model trained on a synthetic task.
arXiv preprint arXiv:2210.13382, 2022.

Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time
intervention: Eliciting truthful answers from a language model. Advances in Neural Information
Processing Systems, 36:4145 1-41530, 2023.

Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning
more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668,
2023.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto
Zamparelli. A sick cure for the evaluation of compositional distributional semantic models.
In Proceedings of the Ninth International Conference on Language Resources and Evaluation
(LREC’ 14), pp. 216-223, Reykjavik, Iceland, 2014. European Language Resources Association
(ELRA).

Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-
style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop at ICLR, 2013a.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pp. 746-751,
2013b.

11


Preprint under review

David Mimno and Laure Thompson. The strange geometry of skip-gram with negative sampling. In
Conference on Empirical Methods in Natural Language Processing, 2017.

John X Morris, Rishi Bommasani, Aakanksha Naik, and Alexander M Rush. The linearity of cross-
lingual word embeddings: A geometric analysis. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pp. 7955-7964. Association for
Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.641.

Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models
of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023.

nostalgebraist. Interpreting gpt: the logit lens, 2020. URL https://www.alignment forum.
org/posts/AcKRB8wDpdaNévoru/interpreting-gpt-—the-logit-—lens

OpenAI. text-embedding-3-large. OpenAI API models announcement, 2024. announced January 25,
2024; 3072 dimensions, improved performance on MIRACL and MTEB benchmarks. Available
from OpenAI API documentation.

Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry
of large language models. In International Conference on Machine Learning, 2024.

Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical and hi-
erarchical concepts in large language models. In The Thirteenth International Conference on
Learning Representations, 2025.

Van-Cuong Pham and Thien Nguyen. Householder pseudo-rotation: A novel approach to activation
editing in Ilms with direction-magnitude perspective. In Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing, pp. 13737-13751, 2024.

Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and
Been Kim. Visualizing and measuring the geometry of bert. In Advances in Neural Information
Processing Systems, volume 32, 2019.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 3980-3990, 2019. doi: 10.18653/v1/D19- 1410.

Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner.
Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023.

Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about
how bert works. Transactions of the association for computational linguistics, 8:842—866, 2021.

Daniel Tan, David Chanin, Aengus Lynch, Brooks Paige, Dimitrios Kanoulas, Adria Garriga-
Alonso, and Robert Kirk. Analysing the generalisation and reliability of steering vectors. Ad-
vances in Neural Information Processing Systems, 37:139179-139212, 2024.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, and Ste-
fano Soatto. Linear spaces of meanings: compositional structures in vision-language models. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15395-15404,
2023.

Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte Mac-
Diarmid. Activation addition: Steering language models without optimization. arXiv preprint
arXiv:2308.10248, 2023.

Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for (score-based) text-
controlled generative models. In Advances in Neural Information Processing Systems, volume 36,
pp. 35331-35349, 2023.

12


Preprint under review

Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and
Samuel R. Bowman. Blimp: The benchmark of linguistic minimal pairs for english. Transactions
of the Association for Computational Linguistics, 8:377-392, 2020. doi: 10.1162/tacl_a_00321.

URL https://doi.org/10.1162/tacl_a_00321

Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan,
Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A
top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023.

A MATHEMATICAL PROPERTIES OF RISE

Roadmap. This appendix has two parts. First, we state geometry preliminaries on the unit sphere,
including explicit exponential and logarithmic map formulas (Lemma (Ip. Second, we analyze se-
quential RISE edits: Theorem proves that RISE transformations commute up to second order
in prototype magnitudes, and Proposition [A-1] shows that each RISE update scales linearly in the
embedding dimension d. Together these results provide a rigorous foundation for RISE’s geometric
behavior and computational efficiency.

Relevance. These mathematical results support our main claims in the paper. Lemma|I] provides
the explicit exponential and logarithmic map formulas that underlie RISE’s use of geodesics on the
unit hypersphere. Theorem|A. I]formalizes that sequential RISE edits commute up to second order,
showing that different discourse-level transformations can be applied in any order without signif-
icant distortion. This result highlights the local geometric consistency of RISE transformations,
rather than implying global additive steering. Proposition [A.1] shows that each RISE transforma-
tion can be applied in O(d) time and memory, demonstrating the method’s scalability to modern
high-dimensional embeddings. Together, these results provide theoretical grounding for both the
geometric consistency and the practical efficiency reported in the main text.

A.1 GEOMETRY PRELIMINARIES ON THE SPHERE

We work on the unit sphere S¢~! C R¢ with the standard round metric. For n € S¢~1, the tangent
space is T;,S¢-! = {a € R@: (x,n) = 0}. The exponential map exp,, : T;,S¢~! + S41 is defined
for all tangent vectors, while the logarithmic map log,, is well-defined for all v € S?~! except
the antipode v = —n. For each n, fix an orthogonal map R(n) € O(d) such that R(n)n = e1,
where e; = (1,0,...,0)'. When analyzing local behavior (e.g., Theorem [A.1}, we take R(-) to
be any C* (continuously differentiable) choice on a neighborhood of the geodesic segment(s) under
consideration; such a local choice always exists.

Lemma 1 (Exponential and logarithmic maps on the unit sphere). For n € Sel tangent vector
€ € T,S*!, and point v € St! \ {—n},

expn(€) = cos(l|é]|) m + sim((1é1]) = log, (v) = arecos((n, v))

u—(n,v)n

Iv — (nr, v)nIl

Proof. These formulas follow from the fact that geodesics on S?~? are great circles in R@ (unit-

radius sphere). See, e.g.,|Absil et al.|(2008} Sec. 5.4).

A.2. ROTOR CONSTRUCTION AND IMPLEMENTATION

In Clifford algebra terms, a rotor is an element of Spin(d) that rotates vectors by the sandwich
product x ++ rar, where 7 denotes reversion. For our purposes, we only require an orthogonal
operator R(n) € O(d) with R(n)n = e; that depends smoothly on n. One closed-form rotor
mapping n ++ e; (valid when n 4 —e}) is

r(n) = _ item
/2(1 + (e1,n))’

In practice we realize this as a standard linear operator without explicit Clifford algebra structures.
Two efficient O(d) realizations are:

r(n)nF(n) = e4.

13


Preprint under review

* Householder reflection: H(n) = [—2@5 with w = n—ey, which satisfies H(n)n = e1

ell
(determinant —1).

* Givens rotation: a 2 x 2 rotation in the plane spanned by {n, e; }, extended by the identity
elsewhere, with determinant +1.

Both satisfy the required conditions R(n)n = e, and local C! smoothness, and are numerically
stable away from n ~ —e,. In the antipodal case (n + —e1) we use a two-step construction: map
n to an auxiliary orthogonal vector u _L e1, then wu to 1. In all cases, applying R(n) or R(n)' toa
vector costs O(d) operations.

A.3. COMMUTATIVITY PROPERTIES OF SEQUENTIAL RISE OPERATIONS

A.3.1 THE RISE SEQUENTIAL PROCEDURE

Given no € S?~! and prototypes p'4, Pp € Te,S*!:

Apply A: £4 = R(no)' Pa, mi = exp, (Ea), Apply B: £3 = R(n1)' pp, nz = exp, (Ep).
A.3.2 FIRST-ORDER COMMUTATIVITY ANALYSIS

Theorem A.1 (RISE commutativity to first order). For small prototype magnitudes ||p.4\\, ||DB|| <

1,
d(result of Ao B, result of Bo A) =O

—

[Pall - Pall).

Proof. Using Lemma]l| expand exp,,, (€4) = no+€a+O((|€all?). Let na = €4. Canonicalization
at ny = 9 + 74 + O(|\74||?) differs from that at no by O(||74||).

Let Pring : Ty, 8? 4 > Trg * denote parallel transport along the short geodesic from n 1 to
no. On the unit sphere, ||P,, +n. — I|| = O(||/n1 — nol]) = O(||74]|), where I denotes the identity
operator on the tangent space. With a C! choice of R(-), || R(m1)' — R(no) | || = O(||n1 — noll) =
O(||n4|l). Therefore,

Pryono R(m)' Pe = R(no)' Pp + O(|lnall lee).
Now expand the second step:

ng = no + R(no)' (Ba + PB) + O((PallllPall) + O(|lPall? + lial”).

Swapping roles of A and B gives the same expansion with p'4, zp reversed. Subtracting yields a
difference of order ||p.4||||p’3||-

Geometric interpretation. Re-canonicalization is equivalent (to first order) to parallel-
transporting the next step’s vector back to the initial tangent space. On S¢~! with constant curvature,
order effects are second order.

A.4. COMPUTATIONAL COMPLEXITY

Proposition A.1 (Per-transformation complexity). Each RISE transformation can be implemented
in O(d) time and O(d) memory:

1. Canonicalization: applying R(n) or R(n)" costs O(d).
2. Logarithmic map log,,(v): O(d

as

using Lemma{i|

nN

3. Exponential map exp,,(&): O(d) using Lemma{i|
4. Storage: prototype p € T.,S*-! costs O(d).

Comparison with matrix methods. Dense d x d rotations require O(d”) time and memory. RISE
achieves equivalent updates in O(d).

14


Preprint under review

Implementation note (Householder). A practical canonicalization is the Householder reflection

=
ww
A(n) =I -2—{, w=n-e,
I|w|?
which maps n ++ e; in O(d). Since H(n) isa reflection (det = —1), it suffices for canonicalization.

Near n * e1, one may switch to a numerically stable alternative.

B  CROSS-LANGUAGE TRANSFER ANALYSIS AND RESULTS

To test whether geometric transformations generalize across languages, we conducted comprehen-
sive cross-language transfer experiments. This section reports detailed results across 3 models and 3
semantic phenomena, analyzing both quantitative performance and geometric properties of learned
transformations.

RISE Cross-Language Transfer: Conditionality

BGE-M3
AR ey TH zu

fs A A
‘Target Language

RISE Cross-Language Transfer: Negation
BGE-M3

RISE Cross-Language Transfer: Polite

BGE-M3
09
os
o7
06
os
os

aR ey H Ea

Es a TA
Target Language

ean Cosine Similarity

mH Ea

Figure 4: Cross-language transfer heatmaps for bge-m3 model showing RISE performance across all
language pairs for conditionality, negation, and politeness transformations. Darker colors indicate
higher cosine similarity between predicted and target embeddings.

RISE Cross-Language Transfer: Polite
text-embedding-3-large

09
os
o7
5 06
os
oa
aR ey ™H zw

Ss a8 TA
Target Language

RISE Cross-Language Transfer: Conditionality
text: i

RISE Cross-Language Transfer: Negation
-embedding-3-large 3-1

text-embedding-3-large

Mean Cosine Similarity

TH Ea ™ mw

Es i a” es a ’
Target Language Target Language

Figure 5: Cross-language transfer heatmaps for text-embedding-3-large model showing RISE per-
formance across all language pairs for conditionality, negation, and politeness transformations.
Darker colors indicate higher cosine similarity between predicted and target embeddings.

RISE Cross-Language Transfer: Conditionality RISE Cross-Language Transfer: Negation
‘mBERT mBERT

os
os
ore

=

é
ose
05
04

ae ey ES y " ™ mw

A
Target Language

RISE Cross-Language Transfer: Polite
mBERT

TH zw TH zu

ES wa 7A Bs wa TA
Target Language ‘Target Language

Figure 6: Cross-language transfer heatmaps for mBERT model showing RISE performance across
all language pairs for conditionality, negation, and politeness transformations. Darker colors indicate
higher cosine similarity between predicted and target embeddings.

15


Preprint under review

B.1 CROSS-LANGUAGE TRANSFER PERFORMANCE

The above heatmaps demonstrate comprehensive cross-language transfer results across our three
models. Training rotor prototypes on one language and evaluating on others reveals remarkable
cross-linguistic performance, particularly for negation and conditionality. Most language pairs show
transfer scores above 0.70, with negation achieving particularly strong off-diagonal performance
(most scores > 0.80).

Negation emerges as the most performant transformation, achieving the highest mean cross-
language transfer scores (0.788 across all model-language combinations) with performance ranging
from 0.686 to 0.918.

Conditionality demonstrates the highest stability and consistency across cross-language transfers,
with the lowest performance variability (0.038) and most stable individual measurements (0.056
average std deviation). Mean performance of 0.780 places it second overall.

Politeness shows more variation but still achieves substantial cross-linguistic success (most scores
> 0.70).

B.2 GEOMETRIC ANALYSIS OF CROSS-LANGUAGE CENTROIDS

Analysis of the learned centroids reveals additional insights into the geometric structure of semantic
transformations. For each phenomenon, we computed ”ideal” transformation vectors by averaging
canonicalized transformed embeddings across languages.

For negation, the centroids show high similarity across languages (pairwise cosines > 0.95).

Conditionality centroids maintain high geometric consistency, supporting the observed stability in
transfer performance across all model-language combinations.

Politeness centroids cluster more loosely but still maintain substantial similarity (pairwise cosines
> 0.87).

B.3 QUANTITATIVE CROSS-LANGUAGE ANALYSIS

Table 2: Complete Cross-Language Transfer Matrix: Statistical Summary

Model Phenomenon All Transfers Monolingual Cross-Lang Ratio
Conditionality 13.6x + 0.7 14.5x 13.5x 0.93

OpenAI (3072d) Negation 19.7x + 1.2 20.6x 19.6x 0.95
Politeness 23.1x + 1.9 25.3x 22.8x 0.90

Conditionality 13.9x+0.6 14.5x 13.8x 0.95

BGE-M3 (1024d) Negation 18.5x + 0.8 19.3x 18.4x« 0.95
Politeness 25.2x + 1.2 26.4x 25.1x 0.95

Conditionality  12.8x + 1.3 15.0x 12.5x 0.83

mBERT (768d) Negation 18.0x + 1.8 20.9x 17.5x 0.84
Politeness 20.8x + 3.9 26.1x 20.0x 0.77

Statistics computed across complete 7x7 language transfer matrix (49 language pairs per phenomenon).
Values show advantage ratios + standard deviation across all language pairs.

Ratio indicates relative cross-language transfer effectiveness (Cross-Lang/Monolingual).

All models maintain strong cross-language performance (77%-95% of monolingual performance).

Tables[2]and[3|provide comprehensive quantitative analysis of cross-language transfer performance.
Notably, all models maintain strong cross-language performance (77%-—95% of monolingual perfor-
mance), with bge-m3 showing the most consistent cross-language effectiveness across all phenom-
ena.

16


Preprint under review

Table 3: Model Architecture and Overall RISE Performance Summary

Model Dims Validation Avg Cross-Lang Avg Random Adv
OpenAI text-embedding-3-large 3072 0.774 19.0x 6.3x
BGE-M3 1024 0.790 19.8x 11.7x
mBERT 768 0.802 16.9x 11.9x

Validation Avg: Mean performance across Synthetic Multilingual, BLIMP, and SICK datasets.
Cross-Lang Avg: Mean advantage ratio across English—>Spanish and Japanese English transfers.
Random Adv: Mean advantage ratio over random baselines in monolingual English scenarios.
Bold values indicate best performance in each category.

C PROMPT TEMPLATES

We provide the exact prompt templates used to generate neutral sentences and their semantic vari-
ants. Each template is shown in monospace using the 1st listing environment for clarity and
reproducibility.

C.1 NEUTRAL SENTENCE GENERATION

You are a linguistics assistant. Generate ONE terse, blunt English
sentence that is politeness-neutral: it must be neither explicitly
polite nor impolite. Keep it concise (8 to 12 words), direct, and
free of polite markers such as "please", honorifics, hedging,

or apologies, yet ensure it is not rude. If the situation contains
a placeholder (e.g., "a favor", "a cultural practice"), replace

it with a concrete, plausible example.

Context category: {category}
Detailed situation: {example}

Respond with ONLY the single sentence (no explanations, no quotation
marks).

C.2 POLITENESS REPHRASING

You are an expert translator and pragmatics specialist. Rewrite the
following sentence in {language_name} to make it more POLITE while
preserving its original meaning. Incorporate the given politeness

features.

Sentence: "{sentence}"
Politeness features (JSON): {features_json}
Respond ONLY with a JSON object in the exact format:

{"polite": "<rewritten sentence>"}
Do NOT add any other keys, explanations, or markdown.

C.3. NEGATION

You are an expert translator and semantics specialist. Rewrite the
following sentence in {language_name} so that it expresses the
NEGATION of its original meaning while remaining natural and fluent.
Incorporate the given negation features.

Sentence: "{sentence}"

Negation features (JSON): {features_json}

Respond ONLY with a JSON object in the exact format:

17


Preprint under review

{"negation": "<rewritten sentence>"}
Do NOT add any other keys, explanations, or markdown.

C.4  CONDITIONALITY

You are an expert translator and syntax/pragmatics expert. Rewrit
the following sentence in {language_name} so that the statement
becomes CONDITIONAL (i.e., it only holds under a certain condition)
while preserving overall meaning and sounding natural. Incorporate
the provided conditionality features.

Sentence: "{sentence}"
Conditionality features (JSON): {features_json}
Respond ONLY with a JSON object in the exact format:

{"conditionality": "<rewritten sentence>"}
Do NOT add any other keys, explanations, or markdown.

D_ DATA GENERATION METHODOLOGY

D.1 DIVERSITY CONTROLS

To guard against artefacts that might arise from narrow lexical or topical coverage we apply sev-
eral sampling diversifiers. (i) Each neutral sentence prompt draws its situation description from a
randomly chosen context category and exemplar, yielding a wide topical spread before any trans-
formation is applied. (ii) Within every language we shuffle sentence—feature assignments so that no
specific lexical field correlates with a particular transformation subtype. (iii) For each transformation
we uniformly sample property values (e.g., negation particle, politeness strategy) per language and
sentence, guaranteeing that every combination of language and subtype appears the same number
of times. (iv) After generation we remove near-duplicates and enforce a 5—25 token length window,
which empirically yields a near-uniform length distribution. Together these steps ensure that our
corpus varies in topic, syntax, and lexical choice while remaining balanced across languages and
transformation subtypes. These controls ensure that observed geometric patterns reflect semantic
properties rather than artifacts of lexical choice or sentence structure.

1. Topical Diversity: Neutral sentences were drawn from varied context categories (social
interactions, factual statements, requests, etc.)

2. Feature Balance: Transformation features (e.g., negation particles, politeness strategies)
were uniformly sampled to prevent correlation with specific lexical fields.

3. Length Normalization: Sentences were filtered to 5-25 tokens to ensure comparable em-
bedding properties.

4. Deduplication: Near-duplicate outputs were removed to prevent repeated data.

D.2 FEATURE-BASED TRANSFORMATION METHODOLOGY

We generated sentence pairs systematically by first sampling neutral sentences in seven typologically
diverse languages (English, Spanish, Tamil, Thai, Arabic, Japanese, and Zulu), and subsequently
transforming each sentence using feature-controlled prompts. Each transformation was guided by
uniformly sampling linguistic features from a predefined typological metadata set (illustrated be-
low).

The full inventories of typological properties for politeness, negation, and conditionality are pro-
vided in Tables [4}{6]

D.2.1 TRANSFORMATION PROCEDURE

For each neutral sentence, we uniformly sampled exactly one set of feature values from the typologi-
cal metadata and prompted the language model (GPT-4.5) to generate the transformed variant adher-

18


Preprint under review

Language Strategy Type Grammatical/Lexical Devices
English Negative politeness Modal conditional, hedging,
idiomatic/proverbial, taboo
avoidance
Spanish Positive politeness Modal conditional, morpholog-
ical politeness, hedging, id-
iomatic/proverbial
Tamil Relational/Kinship po- Morphological politeness
liteness
Thai Positive politeness; Re- | Morphological politeness,
lational/Kinship modal conditional
Arabic Positive politeness; Re- Modal conditional, morpho-
lational/Kinship logical politeness, idiomat-
ic/proverbial
Japanese Relational/Kinship po- Morphological politeness,
liteness modal conditional, hedging
Zulu Relational/Kinship po- Morphological politeness
liteness

Table 4: Typological features sampled uniformly for politeness transformations.

Language Marker Position Morphological Realization
English Clause-medial Negative particle; negative
aux/modal; negative affix
Spanish Clause-medial; Negative particle
concord
Tamil Clause-final Negative particle; verb-internal
negation
Thai Clause-medial Negative particle
Arabic Clause-initial / Negative particle; negative affix
medial
Japanese Clause-final Verb-internal negation
Zulu Clause-medial Negative particle

Table 5: Typological features sampled uniformly for negation transformations.

ing to these specifications. By uniformly sampling across multiple typological dimensions—strategy
types, morphological realizations, and pragmatic contexts—we ensured comprehensive coverage of
each language’s linguistic variability. This methodology supports cross-linguistic embedding anal-
ysis and ensures that observed embedding-space transformations reflect typological distinctions ac-
curately.

D.3 > FEATURE-CONTROLLED PROMPTING

To generate each transformation in a systematic and reproducible manner, we employ a feature-
controlled prompting strategy with a large language model (LLM). Each prompt is carefully tem-
plated to specify the source language, the desired transformation type, and a set of fine-grained
feature tags that guide the model’s output. For example, a prompt might indicate the language code
(’[TA]” for Tamil), the transformation (Politeness Rephrase’’), and a particular strategy or keyword
(such as ’add honorific’) relevant to that transformation. By explicitly encoding these features, we
ensure that the LLM produces the intended variation—whether a more polite rephrasing, a negated
statement, or a conditional construction—in a consistent and transparent way.

To further guarantee balanced coverage, we maintain a metadata table that enumerates all possible
sub-types or strategies for each transformation. This enables us to stratify the sampling of transfor-
mation features across languages and sentences, ensuring that every variant type is equally repre-
sented. For instance, multiple politeness strategies (e.g., adding honorifics, using indirect language)
or different negation words (no” vs. not’) are distributed uniformly across the dataset. This con-
trolled coverage is critical for fair comparisons: it prevents any language from being overrepresented
by a particular style of rephrasing or negation, and minimizes inadvertent correlations between lan-
guage and transformation realization. Our stratified sampling approach follows established princi-

19


Preprint under review

Language Clause Struc- Morphological Marking

ture
English Initial; final; em- Explicit marker; conditional
bedded tense/aspect
Spanish Initial; final; em- Conditional mood; explicit
bedded marker
Tamil Final Explicit marker; conditional
mood
Thai Initial Explicit marker
Arabic Initial; final Conditional mood; explicit
marker
Japanese Final; embedded Conditional mood; explicit
marker
Zulu Initial Conditional mood; _ explicit
marker

Table 6: Typological features sampled uniformly for conditionality transformations.

ples of controlled experimental design, providing a robust foundation for cross-lingual embedding
analysis.

All transformed sentences are generated using a single, consistent LLM—specifically, GPT-
4.5—with a temperature of 1.0 and a maximum token limit of 128 per prompt. The relatively high
temperature encourages diversity in phrasing, while the one-shot generation policy (taking the first
model output without retries or manual curation) avoids selection bias. With carefully constructed
prompts, the model reliably produces valid transformations on the first attempt, and all outputs re-
main in the target language specified by the prompt. This procedure ensures that our dataset is both
systematically varied and reproducible, supporting rigorous downstream analysis.

D.4. QUALITY CONTROL AND DEDUPLICATION

To ensure the integrity and uniqueness of our dataset, we implemented a rigorous two-level dedu-
plication process. At the first level, we removed any transformed sentence that was exactly identical
to another within the same category and language. This step addresses the possibility that the LLM
might produce identical outputs for different inputs, especially for short or formulaic sentences. At
the second level, we ensured that each (neutral, variant) pair was unique across the entire dataset. In
rare cases where two different source sentences yielded the same transformed output, we treated this
as a collision and regenerated a new variant using a slightly altered prompt. Through this process,
every neutral sentence in our dataset is paired one-to-one with three distinct transformed sentences
(one per transformation type), with no overlaps. The result is a clean set of sentence pairs, each
exhibiting a unique, transformation-driven difference.

Beyond deduplication, we applied a suite of diversity controls to guard against artefacts arising from
narrow lexical or topical coverage. Each neutral sentence prompt was drawn from a wide range of
context categories and exemplars, ensuring topical breadth before any transformation was applied.
Within each language, we shuffled sentence—feature assignments so that no specific lexical field cor-
related with a particular transformation subtype. For each transformation, we uniformly sampled
property values (such as negation particles or politeness strategies) per language and sentence, guar-
anteeing that every combination of language and subtype appeared the same number of times. After
generation, we removed near-duplicates and enforced a 5—25 token length window, which empiri-
cally yielded a near-uniform length distribution. Together, these steps ensure that our corpus varies
in topic, syntax, and lexical choice while remaining balanced across languages and transformation
subtypes, providing a robust foundation for subsequent embedding analysis.

D.5 EMBEDDING GENERATION

With our dataset of neutral and transformed sentences in hand, we next obtain high-dimensional vec-
tor representations using a state-of-the-art multilingual sentence encoder. Specifically, we employ
OpenAI’s text -embedding-—3-large model, which produces 3072-dimensional embeddings

20


Preprint under review

aligned semantically across more than 90 languagesP| All embeddings are generated in a frozen
(non-fine-tuned) setting, with a single API call per sentence. According to the model card, each
sentence embedding is computed by mean-pooling the token-level hidden states, followed by layer
normalization. This means that every token—including short functional items like negation parti-
cles—contributes proportionally to the final vector.

Our approach assumes that all sentence embeddings reside in a shared semantic space where linear
structure is meaningful. We adopt the perspective that this space forms a latent manifold encoding
universal semantic features, as hypothesized by|Jha et al.| (2025). In this framework, certain direc-
tions in the embedding space correspond to specific attributes, such as politeness or negation. If
sentence transformations truly correspond to adding or subtracting a semantic attribute, we expect
the difference vector (variant minus source) to be relatively consistent across examples. This aligns
with the ’universal geometry for embeddings” framework, in which multilingual embeddings from
different models or languages can be brought to a common representation where semantic differ-
ences are captured by geometric translations. While our work stays within a single encoder’s space,
we leverage a similar idea: analyzing whether the transformation rotors” (difference vectors) clus-
ter for similar transformations across languages. This methodology sets the stage for validating
whether these quasi-linear transformations indeed behave like translations in a Riemannian seman-

tic space (Jha et al.||2025), which we explore in the next section via rotor-based analysis of the
embedding differences.

It is important to note that applying a single global rotation or principal component analysis (PCA)
can distort other dimensions and is not adaptive to individual vectors. Because the base embedding
is already a mean across tokens, edits that insert or replace a handful of tokens translate to small but
coherent rotations of the global vector—precisely the kind of local, content-independent shift that
our rotor method is designed to capture.

D.6 FINAL DATASET STATISTICS

The resulting corpus comprises 1,000 neutral sentences in each of the seven languages, totaling
7,000 examples. For English neutral sentences, the mean token length is 9.1 tokens (with a median
of 9.0 tokens), with token counts ranging from 3 to 12 tokens and an average character length of
54.4 characters. This distribution confirms that our generation process produced concise, natural
sentences suitable for semantic transformation analysis across languages and transformation types.

To further validate the diversity and balance of our dataset, we analyzed the distribution of sentence
lengths per language, which reveals broadly similar profiles with a peak around 10-15 tokens. Addi-
tionally, we examined the distribution of word frequencies, confirming a typical long-tail distribution
in each language. These statistics affirm that our corpus is both balanced and rich in content, pro-
viding a solid empirical foundation for the cross-lingual transformation analysis in the subsequent
sections.

E LLM USAGE DISCLOSURE

Large language models (LLMs) were used to assist with multiple aspects of this research, includ-
ing: ideation, writing, programming, and implementation of experimental code, and identification
of related work and literature. All LLM-generated content, code, and references were subject to
human review, testing, and verification to ensure accuracy, functionality, and relevance. Any claims,
results, experimental implementations, and citations presented in this work have been reviewed by
the authors. The authors take responsibility for all content, including any errors or inaccuracies that
may remain despite our review process.

F RISE VS BASELINE COMPARISONS

This section presents comprehensive comparisons between RISE and random baseline prototypes to
validate that RISE learns meaningful semantic directions rather than benefiting from arbitrary vector
orientations.

“‘https://platform.openai.com/docs/guides/embeddings

21



Preprint under review

F.1 CROSS-LANGUAGE TRANSFER VS RANDOM BASELINES

The following figures show detailed heatmaps comparing RISE performance against random pro-
totypes of equivalent magnitude across all language pairs and phenomena. Each comparison uses
10,000 random trials to ensure statistical robustness.

F.2 RISE BASELINE COMPARISON

Figure [7] demonstrates the baseline validity of RISE by comparing it against random prototypes
across multiple language transfer scenarios. The consistent and substantial advantages (ranging
from 5.1x to 26.2x) across all models and phenomena provide crucial validation that RISE learns
meaningful semantic directions rather than exploiting statistical artifacts. Notably, cross-language
transfers often maintain or even exceed monolingual performance relative to random baselines, con-
firming that RISE captures universal semantic patterns that generalize across language boundaries.
Overall, RISE analyses show that embedding models encode some transformations as universal
operators, but others remain highly culture- and resource-dependent. Future research should re-
fine evaluation benchmarks to account for phenomenon-specific variability and investigate training
regimes that promote balanced universality without sacrificing discriminative capacity.

22


Preprint under review

Figure 7: RISE vs Random Baseline Comparisons across Language Transfer Scenarios.
Top: English monolingual analysis showing RISE performance vs random prototypes for all three
models and phenomena.
Middle: English prototype — Spanish target cross-language transfer demonstrating maintained ad-
vantages over random baselines.
Bottom: Japanese prototype — English target transfer confirming universal semantic patterns across
diverse language pairs. All comparisons use 10,000 random trials for statistical robustness.

RISE vs Random Prototype Comparison: English Prototype — English Target

(10,000 Random Trials per Condition)
Conditionality Negation Polite

a0. Method
mm Openal (3072d)

0.863 0.869 0.887 0.865 ame GEMS (10240)

aes = mBERT (768d)
08 0.763
06 06
0.4 0.4
0.2 0.2
|| 0.041 0.032
00 me, =

RISE vs Random Prototype Comparison: English Prototype > Spanish Target
(10,000 Random Trials per Condition)
Conditionality Negation Polite

RISE Performance (Cosine Similarity)

English Prototype > English Target
2 2 2 2 2
6 & FS a ©

Method
-— a (30720)
lim BGE-M3 (10240)

0.816 0.807
eA mBERT a

0.784 0.8 o.7e1 08 -
mmm_Random
0.673
. 0.6 0.6
. 0.4 0.4
. 0.2 0.2
ar eet bad ss 032

RISE vs Random Prototype Comparison: Japanese Prototype ~ English Target
(10,000 Random Trials per Condition)
Conditionality Negation Polite

2
&

Normalized Improvement
((sim_BC - sim_AB) / (1 - sim_AB))
English Prototype > Spanish Target
° °
ES a

°

= ay (30720)

0.832 0.8 0.780 0.825

= ay
0.779 0.726 o8 wm Random
07 eae 0.740
0.664
0.6
. 0.6 ses
05
0.4
04
0.3
: 0.2 02
o1
0.057
0.041
ss

23

2
&

) 7 (1 = sim_AB))
°
a

Normalized Improvement

im_BC - sim_AB!
Japanese Prototype English Target
°
FS

(sit

°



Preprint under review

Figure 8: RISE vs Random Baseline Comparison for text-embedding-3-large. Top row shows RISE
performance, bottom row shows random baseline performance (averaged over 10,000 trials). The
dramatic performance gap demonstrates that RISE learns meaningful semantic directions rather than
benefiting from arbitrary vector orientations.

RISE vs Random Advantage Ratios: OpenAl (3072d)
(All Cross-Language Transfer Pairs)
Conditionality

Negation Polite

weabie : I 2 34x -13Mx13.2K PONE 95x 17.7% 195k | 206x 20.6x —-20.4x Arabic MER ox | 25ax 207% \/gsae) 20.7% 27

English ie eeaame ces 3 3.2 3 3.x English (ype 85x 19.6x | 207x  20.6x 2 English eres 5.2 25.2x 205x | 29x  20.4x 26
§
ae
© Spanish RE ax 3.1% 3.3x 3.0x Spanish fees 7.8x 10.0x 20.4x 20. 9.5x Spanish e
& ~
2 uw
2 24H
SS Japanese 3.2 r x 3.2 x EEa japanese -Bunas K 9.2x 0.9 0.5% x ral Japanese 3
2 2
B Tamil Tamil rs 9.0x 0.6 6 Tamil e
28
ra
8
2
5]
at

20

a es g¢ € ¢€ x s ££ S$ S&S & yp xs ¢s § E& §& &
Ro & &§ gg eo RG RS s es we ” & & § gg “ 4
€é & & € es Rs ¥ <é & &

Target Language

Figure 9: RISE vs Random Baseline Comparison for bge-m3. Top row shows RISE performance,
bottom row shows random baseline performance (averaged over 10,000 trials). bge-m3 shows re-
markably consistent RISE performance across all phenomena and language pairs, with random base-
lines consistently near zero.

RISE vs Random Advantage Ratios: BGE-M3 (1024d)
(All Cross-Language Transfer Pairs)
Conditionality

Negation Polite

Arabic ax ax 14.2 > 1 3.2x Arabic ee SOx 80x 18% 17.2 Arabic
27
English “ibe 14.2x 1 3.8 8 2.7 English uae 90x 180x187 | 8 6. English =
3
ts
& Spanish Been 1 4x 13x ix 12x [OME ts2x | ieax | nee oo ese: ER spaniel) 26§
& Spanis pani panis! &
s ~
3.
2 F
'S Japanese ments 4.0) 1 ERa Japanese -Mecnyd 18.9% 8.8x y 6 Japanese os
Ay 2875
& 4
2 4
RMT 133% | Gx © idx 143) 4 14.3 3.2K Tamil ees 8.5x 6x | 190% 8 Tamil %
£
ra
&
$
S
<

» ¢
» &

6;
%
%y
Se
i
ey,

oe oe ae
& gf Se as
Fon de

Target Language

Figure 10: RISE vs Random Baseline Comparison for mBERT. Top row shows RISE performance,
bottom row shows random baseline performance (averaged over 10,000 trials). mBERT demon-
strates strong RISE performance for specific phenomena with clear superiority over random base-
lines across all conditions.

RISE vs Random Advantage Ratios: mBERT (768d)
(All Cross-Language Transfer Pairs)
Conditionality

Negation Polite

Arabic sx | 946 1.7% 2.5x ox Arabic 5,6x Sie) 15.5x 9.7 —18.9x 8 Arabic

26
English (pene 2.7% » ox 2.1% English -Jeueas 2.0 7.6x 2x 6.3x 6.8x English <
24 id
8
2
8 Spanish Be 123 2.3) ax SUM Spanish 5 6.6x 164x [RO 16.2% Spanish axe
FA a
S Japanese x lx  123x 2.3: 0.7; CRP Japanese -MecaE 6.7 7.5x Wax 16.4x 5. Japanese eur 20%
o 2
8 2
3 a oe ws es = 18a
G Tamil ae 1 ox 3x ax Bele 195k 15.6x | 189K 15.9x 8.6 Tamil Ey
g
16 e
4

4

12

a : @ N ES > ve © N > o RG . s > RS
& § § ££ & € es ££ § € & © ®® e ££ «§ € & ©
SS e £ oe €

Target Language

24


Preprint under review

F.3. PHENOMENON-SPECIFIC PERFORMANCE VS RANDOM BASELINES

Figure 11: Phenomenon-specific RISE performance vs random baselines across all three models.
Shows mean normalized improvement scores for conditionality, negation, and politeness compared
to random prototype baselines. Error bars represent standard error of random baseline (10,000
trials). All RISE performance significantly exceeds random baselines, with advantage ratios ranging
from 5.1x to 15.2x.

RISE Performance by Linguistic Phenomenon
English Monolingual: Learned Prototypes vs Random Baselines

Performance Comparison
EE Conditionality (RISE)
(E_Negation (RISE)

I Politeness (RISE)

22. Conditionality (Random)

(222 Negation (Random)

{223 Politeness (Random)

Mean Cosine Similarity

OpenAl (3072d) BGE-M3 (1024d) mBERT (768d)

Embedding Model

These baseline comparisons provide crucial validation that RISE’s strong performance stems from
learning meaningful semantic transformations rather than exploiting statistical artifacts or benefiting
from arbitrary vector orientations in high-dimensional spaces.

F.4 DETAILED BASELINE COMPARISON ANALYSIS

Table 7: RISE vs Random Prototype Performance: English Monolingual Analysis

Model Phenomenon RISE Perf Random Baseline Adv Ratio
Conditionality 0.463 0.057 + 0.0003 8.1x
OpenAI (3072d) Negation 0.210 0.041 + 0.0002 5.1x
Politeness 0.181 0.031 + 0.0002 5.8x
Conditionality 0.610 0.057 + 0.0003 10.7x
BGE-M3 (1024d) Negation 0.391 0.041 + 0.0002 9.5x
Politeness 0.461 0.031 + 0.0002 14.9x
Conditionality 0.625 0.057 + 0.0003 11.0x
mBERT (768d) Negation 0.624 0.041 + 0.0002 15.2x
Politeness 0.294 0.031 + 0.0002 9.5x

Random baseline computed from 10,000 random prototypes of equivalent magnitude.
Standard errors shown for random baselines (tSEM).

Adv Ratio = RISE Performance / Random Baseline.

All models show significant advantages over random baselines (5.1x—15.2x).

Tables [7}{10] demonstrate the statistical robustness of our findings. All RISE advantages are sta-
tistically significant (p < 0.001) with ultra-precise standard errors from 10,000 independent trials.
Cross-language transfer often outperforms monolingual scenarios, demonstrating universal semantic
patterns learned by RISE across language boundaries.

25


Preprint under review

Table 8: Cross-Language Transfer Performance: RISE vs Random Baselines

Transfer Scenario OpenAI (3072d) BGE-M3 (1024d) mBERT (768d)
English Prototype — Spanish Target

Conditionality 14.4x 14.2x 13.8x

Negation 19.6x 19.0x 19.0x

Politeness 25.2x 25.5x 21.6x
Japanese Prototype — English Target

Conditionality 13.7x 14.7x 11.7x

Negation 17.6x 18.9x 16.7x

Politeness 23.5x 26.2x 18.6x

Cross-Language Average 19.0x 19.8x 16.9x

Monolingual Average 6.3x 11.7x 11.9x

Values show advantage ratios (RISE Performance / Random Baseline).

Cross-language transfer often outperforms monolingual scenarios.

Demonstrates universal semantic patterns learned by RISE across language boundaries.
Random baselines consistent across all language pairs (language-agnostic).

Table 9: Statistical Robustness: Random Baseline Validation

Phenomenon Random Mean _ Standard Error 95% Confidence Interval

Conditionality 0.0567 0.000276 [0.0562, 0.0572]
Negation 0.0412 0.000200 [0.0408, 0.0416]
Politeness 0.0315 0.000154 [0.0312, 0.0318]

Random baselines computed from 10,000 independent trials per phenomenon.
Ultra-precise standard errors (4—6 decimal places) ensure statistical robustness.
Confidence intervals demonstrate consistent, language-agnostic random performance.
All RISE advantages are statistically significant (p < 0.001).

Table 10: Phenomenon-Specific RISE Performance Analysis

Phenomenon Complexity Avg Performance Consistency
Politeness High 0.312 High (o = 0.134)
Conditionality Medium 0.566 Very High (o = 0.081)
Negation Low 0.408 High (0 = 0.207)

Complexity based on linguistic theory and cross-language variation.

Avg Performance computed across all models and language pairs.

Consistency measured by standard deviation across models (lower = more consistent).
Conditionality shows highest consistency, suggesting universal semantic patterns.

26
