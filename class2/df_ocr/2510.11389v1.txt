arX1v:2510.11389v1 [cs.CL] 13 Oct 2025

Beyond Survival: Evaluating LLMs in Social Deduction Games with
Human-Aligned Strategies

Zirui Song'*, Yuan Huang”*, Junchang Liu’*, Haozhe Luo”, Chenxi Wang!

Lang Gao’, Zixiang Xu', Mingfei Han', Xiaojun Chang', Xiuying Chen"
‘Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)
?Northeastern University

Abstract

Social deduction games like Werewolf combine
language, reasoning, and strategy, providing a
testbed for studying natural language and so-
cial intelligence. However, most studies reduce
the game to LLM-based self-play, yielding tem-
plated utterances and anecdotal cases that over-
look the richness of social gameplay. Evalua-
tion further relies on coarse metrics such as sur-
vival time or subjective scoring due to the lack
of quality reference data. To address these gaps,
we curate a high-quality, human-verified mul-
timodal Werewolf dataset containing over 100
hours of video, 32.4M utterance tokens, and 15
rule variants. Based on this dataset, we propose
a novel strategy-alignment evaluation that lever-
ages the winning faction’s strategies as ground
truth in two stages: 1) Speech evaluation, for-
mulated as multiple-choice-style tasks that as-
sess whether the model can adopt appropriate
stances across five dimensions of social ability;
and 2) Decision evaluation, which assesses the
model’s voting choices and opponent-role infer-
ences. This framework enables a fine-grained
evaluation of models’ linguistic and reason-
ing capabilities, while capturing their ability to
generate strategically coherent gameplay. Our
experiments show that state-of-the-art LLMs
show diverse performance, with roughly half re-
main below 0.50, revealing clear gaps in decep-
tion and counterfactual reasoning. We hope our
dataset further inspires research on language,
reasoning, and strategy in multi-agent interac-
tion.

1 Introduction

Social deduction games provide a unique setting for
studying language and intelligence. Unlike strate-
gic dialogue games such as Diplomacy (Bakhtin
et al., 2022) which requires long-term alliance
building and coordinated planning, Werewolf (Xu

* Equal Contribution.
t Corresponding Author.

et al., 2023) places greater emphasis on the artistry
of language and reasoning. Players must, within
limited speaking time, influence others’ judgments
and votes through persuasion, ambiguity, decep-
tion, and rhetorical nuance. These characteristics
make social deduction games like Werewolf a par-
ticularly challenging and valuable domain for arti-
ficial intelligence research.

Recent advances in Large Language Models
(LLMs) have motivated a growing line of research
exploring their application to Werewolf and other
social deduction games. However, most existing
studies adopt a self-play paradigm where multi-
ple LLM agents interact only with each other (Wu
et al., 2024; Xu et al., 2024; Du and Zhang, 2024;
Poglitsch et al., 2025; Song et al., 2025b; Xu et al.,
2025b). This often results in overly rigid and tem-
plated utterances, as well as dull or anecdotal sto-
rylines. For example, as shown in Figure 1(a) (Xu
et al., 2024), the model produced shallow, formu-
laic statements such as “‘it is sad that player 2 was
killed last night, we need to quickly find the seer
and witch’, which lack interactive nuance and per-
suasive depth. In Figure 1(b), even with the latest
state-of-the-art LLMs (FoasterAI, 2025), anecdo-
tal failures remain: the Witch successfully saved
a villager, yet when a Werewolf falsely claimed to
have been attacked the previous night, the Witch
believed the deception. Because of the lack of
high-quality gameplay data, prior evaluations have
relied on coarse metrics such as survival duration,
win rate, or subjective assessment of utterances,
which can be misleading. A player may win de-
spite poor decisions due to strong teammates, or a
werewolf may hide by echoing the good faction yet
contribute little, making individual performance
hard to assess.

To address these limitations, we first construct a
high-quality multimodal dataset WereBench from
televised human gameplay Panda Kill. The distinc-
tive strength of this dataset lies in three aspects: (1)


Good morning, everyone. As player_0O, it is
unfortunate that player_1 was killed last
night. | think we should focus on finding the
Seer and Doctor who can provide valuable
information about who might be the
Werewolves. Let’s all share our thoughts
and suspicions...

(a) (b)

OO LLaMA

villager

witch

Shallow,
_noninformative
Simulated result from Xu Z,, et al. ICML 2024

— NIGHT 2

« Werewolves targeted Nina.

* Witch Mona saved Nina.

S DAY 2

* Liam falsely claimed he was the one

-* Gemini

saved last night to gain trust.
* Mona mistakenly believed Liam and
declared that Nina was a werewolf.

Game Plot Line

¢

Anecdotal failures

Simulated result from https://werewolffoaster.ai/

® Coarse Evaluation: survival duration, win rate.... *

a I’m the Witch. Last night, | saved Player 1, so | believe he’s not a werewolf.

Right now, only Player 0 has claimed to be the Witch, so | think
he’s telling the truth. He said I'm saved, that means my identity
is pretty solid. Last round, Player 2 cast doubt on me, and now

I’m suspicious of him.

hunter

The reason | doubted Player 1 before was because he didn’t
provide any useful analysis in the last round. But now that the
Witch has shared this save, and if no one else is claiming to be
the Witch, then Player 0 and 1 are probably both good. If | were
the werewolf, | would know | attacked Player 1, so it wouldn’t
make sense for me to cast suspicion on him. Informative, logical,

coherent across players

© Human- alae Evaluation

5-Dim Speech MCQ a
* Decision Alignment @
@ Rich Labels

* — Sentence-level label €&4
* Game state label

a.

OQ LLaMaA
werewolf

Figure |: Limitations of prior LLM-based play in the Werewolf game. (a) Generated speeches are often shallow
and lack informative content. (b) Even state-of-the-art models fail to fully capture the game rules, with anecdotal
failure cases. (c) In contrast, our WereBench dataset, combined with the WereAlign evaluation framework, enables
assessment of models with human-aligned strategies, capturing both speech quality and decision-making accuracy.

Authenticity: each game is professionally curated
and verified, with complete records of speeches,
voting behavior, role information, and final out-
comes. (2) Richness of content: every match is
carefully selected to provide high entertainment
value and linguistic diversity, capturing a wide
range of rhetorical styles, interaction dynamics, and
strategic expressions. (3) Multimodal expressive-
ness: the videos employ dynamic camera work and
seamless scene transitions to capture non-verbal
cues such as microexpressions. Building upon
this dataset, we propose a novel strategy-alignment
evaluation framework that leverages the strategies
of the Human player as ground truth. Leveraging
the high-quality annotations in WereBench, we ex-
tract reference data from well-performing players.
As shown in Figure 1(c), our framework operates
in two stages. In the speech evaluation stage, we
construct multiple-choice tasks based on human
ground truth to test models’ abilities across a broad
range of social gaming skills, including role infer-
ence, strategic judgment, deception reasoning, per-
suasive expression, and counterfactual trade-offs.
Second, in Decision evaluation, we compare the
model’s voting behavior against that of the winning
faction to assess its reasoning and judgment across
rounds. This is fundamentally different from prior
metrics that rely only on coarse outcomes such
as overall win/loss or survival duration. Our ex-
periments show that state-of-the-art LLMs show

diverse performance, with roughly half remain be-
low 0.50, revealing clear gaps in deception and
counterfactual reasoning.

In summary, this work makes three contributions.
First, we curate the first high-quality multimodal
Werewolf dataset from televised human gameplay,
providing authentic and strategy-rich data. Sec-
ond, we introduce a strategy-alignment evaluation
framework that benchmarks models not only by
game outcomes but also the ability to produce
human-like speech and align with human decision-
making strategies. Finally, we demonstrate that
existing state-of-the-art LLMs still fall short under
this framework, revealing significant gaps in their
social reasoning and interaction capabilities.

2 Related Work

LLMs in Social Games. Language models have
been deployed as agents in social deduction and
negotiation games, where success depends on per-
suasion, deception, and coalition formation. Nego-
tiation and alliance-building have also been stud-
ied in bargaining and coordination domains (Xie
et al., 2024; Chi et al., 2024; Li et al., 2024; Sarkar
et al., 2025; Cai et al., 2024; Song et al., 2025a;
Liu et al., 2024). In werewolf games, prior work
has investigated LLM performance in self-play (Xu
et al., 2023). Other research has improved voting
through bootstrap aggregating and reinforcement
learning (Khan and Aranha, 2022; Brandizzi et al.,


Evaluation Prior Work Our Approach

Category

Speech Relying on human ratings © Comparison with human

Evaluation —> subjective and inconsis- speech in a multiple-choice
tent, e.g., Wu et al. (2024); format — more accurate
Du and Zhang (2024) and consistent scoring

Decision Voting aligned with win- Comparison with human

Evaluation ning faction logic — ig- reference actions —

nores deceptive strategies,

strategy-aligned evaluation

e.g., Xu et al. (2024) etc.

Table 1: Comparison of prior work and our approach
using the format: method — consequence.

2022; Wu et al., 2024). Eger and Martens (2019)
examined human responses in One Night Ultimate
Werewolf, while Shibata et al. (2023) fine-tuned
pretrained models with limited game logs, as in
Deep Wolf. These efforts largely treat LLMs as
game-playing agents, whereas our work grounds
analysis in real human gameplay data for closer
alignment with authentic social interaction.

Evaluation of LLM Social Intelligence. Eval-
uating social intelligence involves several dimen-
sions including theory of mind, deception, cooper-
ation, and persuasion. Common evaluation metrics
are outcome-oriented, for instance, survival dura-
tion or win rate (Wang and Kaneko, 2018; Stepput-
tis et al., 2023; Light et al., 2023). However, such
measures are coarse, since a team’s victory does
not necessarily reflect the quality of an individ-
ual’s performance. Other studies emphasize deduc-
tive accuracy, assessing whether LLMs can infer
hidden roles, and align their voting with ground
truth identities (Lai et al., 2023; Wu et al., 2024).
Yet these approaches often overlook the fact that
in actual gameplay, agents may deliberately cast
votes inconsistent with their true stance in order
to mislead others. Language quality is typically
judged by human raters who evaluate plausibility,
coherence, and persuasiveness of generated utter-
ances (Eger and Martens, 2019; Wu et al., 2024;
Du and Zhang, 2024), which introduces a degree of
subjectivity. In contrast, we contribute a strategy-
aligned evaluation paradigm that integrates both
speech level grading and decision level voting ac-
curacy, combined with match data against skilled
human players. A comparative summary is pro-
vided in Table 1.

Datasets and Benchmarks for Social Strategy
and Interaction. Most existing studies on social
strategy games rely on self-play between agents,
with limited availability of high-quality datasets

that capture authentic human interactions (Koppa-
rapu et al., 2022; Zhu et al., 2023; Xu et al., 2023;
Du and Zhang, 2024; Xu et al., 2025b; Huang et al.,
2025; Xu et al., 2025a). To our knowledge, the only
work that collects large-scale human gameplay data
is (Wu et al., 2024), which introduces the FanLang-
9 dataset of nine-player Werewolf. While valuable
as a first step, this dataset is sourced from online
platforms and inevitably reflects noisy gameplay
behaviors, such as idle speech, player disconnec-
tions, and random or unserious actions. Moreover,
the data has not undergone systematic human audit-
ing, leading to highly variable quality and potential
misalignment with the intended evaluation of social
reasoning. In contrast, our work introduces a cu-
rated benchmark built on carefully selected, human-
verified multimodal gameplay data with rich labels
to enable more reliable evaluation.

3  WereBench Collection

Our video dataset is curated from a popular Were-
wolf TV programs, Panda Kill, sourced from
platforms such as Youtube and Bilibili. Each game
round includes clear public voting panels and real-
time narration. These matches include insightful
post-game summaries from an expert host, who
not only clarifies complex game states but also
explains players’ underlying motivations, strate-
gic bluffs, and intricate reasoning. This provides
an invaluable source of ground truth for studying
deeper aspects of social intelligence. The videos
employ dynamic camera work and seamless scene
transitions to capture crucial non-verbal cues such
as microexpressions. This high-quality, human-
generated dataset, together with its multimodal
dimension, represents a significant improvement
over previous agent-based, text-only social game
datasets (Bailis et al., 2024; Xu et al., 2025b), and
opens up new possibilities for tasks such as decep-
tion detection (Joshi et al., 2025).

Data Annotation and Game State Reconstruc-
tion We adopt a multi-stage pipeline to produce
time-aligned, human-verified annotations. (1) Data
cleaning. Two PhD-level annotators remove ex-
traneous material (ads, sponsor cards). (2) Tran-
scription. We use @Feishu Meeting Notes for
high-fidelity ASR, yielding utterance-level text
with start/end timestamps. (3) Speaker attribution.
Annotators perform speaker diarization and map
speech to player ID based on on-screen name plates
and seating order; this produces an utterance-by-


Attribution Value
Total video duration 100+ hours
Utterance tokens 32.4M
Unique roles 30
Human players 48
Rule variants 15
Number of games 80+
Number of day-night cycles 240+
Average tokens per speech 390

Table 2: Core statistics for WereBench.

utterance speaker index rather than coarse “speaker
recognition” tags.(4) Game state reconstruction.
Human experts reconstruct day/night cycles and
public game logs, including all daily votes, pub-
licly revealed night outcomes, and skill activa-
tions together with their in-game effects; official
rules and per-player role assignments used are also
recorded.(5) Highlighting decisive moments. To
support subsequent evaluation, annotators highlight
strategically decisive moments involving MVPs
(Most Valuable Players), such as pivotal claims,
contradiction exposures, coalition calls, or vote piv-
ots. (6) Episode summary. At the end of each
episode, the host provides a post-game, omniscient
recap of the entire match.

Data Statistic WereBench contains 100+ hours of
curated televised Werewolf gameplay from Panda
Kill, 32.4M transcribed tokens across /5 rule vari-
ants and 48 human players. Table 2 reports detailed
statistics; Figure 2 shows the frequency of special-
skill roles by group.

Dataset Quality Examination We assess qual-
ity along two dimensions. First, we measure
inter-annotator agreement on the tasks with the
highest ambiguity, namely speaker attribution and
game state reconstruction, while other stages, such
as data cleaning, are mostly deterministic. On
a 10% subsample, two annotators independently
reannotated episodes, yielding high consistency:
Kspk = 0.97 and Kiog = 0.93. Second, we examine
transcription fidelity using a 4-hour gold set fully
reviewed by two annotators: the baseline WER is
8.1%, reduced to 4.9% after lexicon-based correc-
tion. These results confirm the dataset is reliable
for fine-grained speech and strategy analysis.

4 WereAlign Evaluation Paradigm

Building on the WereBench dataset, we introduce
the WereAlign evaluation paradigm, which lever-
ages the winning faction’s strategies to provide fine-

Seer
Witch
Hunter
Guard
Silencer
Stalker]
Demon|
Nine-Tailed Fox
(AS White Werewolf
Magician
Wolf Seductress
Thief
Moonchild
Tengu
Little red riding hood .
Wolf Brother Village
| Wolf Grany Werewolves
Wolf Younger Brother Third Party
Others Mixed

0 2 4 6 8 10 12 414 (%)

Figure 2: Role composition in WereBench.

grained assessments of models’ social reasoning
across both speech and decision levels.

4.1 Speech Evaluation

As discussed in Section 2, previous works on eval-
uating agents’ speech in social games rely heavily
on subjective human judgments due to the lack
of reliable human-player references. Annotators
are typically asked to rate logical consistency, rea-
soning soundness, informativeness, and persuasive-
ness (Wu et al., 2024), or to assess higher-level
behaviors such as trust, confrontation, and decep-
tion (Xu et al., 2023). Such evaluations are in-
herently subjective and cannot rigorously capture
the true capabilities of LLM agents. Leveraging
our high-quality dataset, we introduce a reference-
based multiple-choice framework that evaluates
human-player speeches across different abilities in
social games.

We construct multiple-choice questions across
5 critical social and strategic dimensions. In the
Role Inference (RI) dimension, the model needs to
uncover the true identities and intentions of other
players (Shibata et al., 2023); in the Strategic Judg-
ment (SJ) dimension, it must choose the course
of action most beneficial to its faction (Wu et al.,
2024). QA also evaluates the model’s Deception
Reasoning (DR) , specifically its ability to identify
others’ lies or effectively perform masquerade (Xu
et al., 2023). Furthermore, the questions cover so-
cial expression, assessing whether the model can
generate Persuasive Statements (PS) appropriate to
the context (Park et al., 2023); and Counterfactual
Trade-off (CT), examining the potential benefits
and risks of different actions under the current sce-
nario (Chi et al., 2024).


Sheriff’s Race Day 1 Voting

Role
Introduction

Spirit Deer

Role
Allocation

P10

Day 2 Discussion

Day 3 Discussion

Regular

Each ni two
players a When
either swapped player
would die that night or
the following day, their
death is swapped
instead.

Each night collectively
choosing one player to

kill, while also engage
in daytime discussions
and voting to blend in
with the good team. The skill can only be
activated once.

F, Chaos Devil
Pil.

Inquisitor

* Civilians
P1, PS, P6,P8

Rules: This game operates on side elimination rules, meaning the werewolves win by eliminating either all Civilians or all Special Villagers. The
good side wins by voting out all of the werewolves. The game consists of 12 players: 4 Power Roles, 4 Civilians, and 4 Werewolves. Werewolves
includes three regular werewolves and one Chaos Devil. The Power Roles are the Prophet, Hunter, Inquisitor, and Spirit Deer.

Speech: P4: I’m the Prophet. 10 is my clear. | checked him on the first night because he’s on my diagonal. [6:52-7:48]
P7: I’m the Prophet. My next checks are on 6 and 9. 8 is my clear. 4 can't be the real Seer, but | don't think he is a wolf counter-claiming
me either. P4 didn't even declare his badge flow, and his speech was hesitant, so he sounds more like a troll. [9:37-11:21]

Logs: P10 Spirit Deer used its ability, resulting in a peaceful night (no one was killed).
The vote for Sheriff between P7 and P2 was a tie. The badge is now discarded.

Round 1 Voting ( P1, P3, P4, P7, P10 voted for P2) ...

[P2 was voted out]

Highlight: P2, speaking from a late position, counter-claimed Seer. This sowed enough chaos that P4, received 0 votes during Sheriff election.
After P4 lost the Sheriff election, both werewolves P2 and P7 were counter-claiming the prophet. In response, P4 decided to go undercover
his identity, forcing the two wolf-factions to turn on each other, which ultimately led to P2 being voted out on Day 1.

Summary: Werewolves victory! All power roles have been eliminated. In this game......

Figure 3: An overview of our WereBench dataset. Each data sample provides the view of a complete game video,
with the human annotation including: (a) role introduction, (b) role allocation, (c) rules, (d) speech with timestamp,
(e) logs like votes and skill usage; and (f) highlight annotations (g) summary with the expert’s post-game analysis.

Question Design Annotators draft five questions
example for each dimension, then use the highlight
timestamps ¢ and the context available to the MVP
at that time, C, = (R,H,S) where R denotes role
claims and rules, 7 includes revealed logs such as
votes or public night outcomes, and S is the speech
history up to t. Using C; together with the five
question example as inputs to the LLM, we prompt
the LLM to generate candidate questions Q from
the MVP player’s perspective.

Positive Option Generation For every question
Q at time t, we derive the positive option A by
aligning with the MVP’s actual strategic trajec-
tory around t. Specifically, annotators (i) read
the MVP’s speech segment at or immediately af-
ter t and the nearest ensuing public action (e.g.,
expressed stance, targeted rebuttal, or vote), and
(ii) abstract these into a canonical decision that
directly answers Q. To avoid verbatim leakage
and preserve generality, A is paraphrased into an
action-level description (e.g., “publicly challenge
Player 7’s seer claim by citing the inconsistency
with Day-1 votes’) rather than a quote.

Negative Option Generation Negative sample
generation uses a parallel adversarial framework
grounded in game theory and counterfactual rea-
soning. The objective is to generate options that
are not only incorrect but strategically plausible yet
suboptimal, or deceptively consistent yet ultimately
detrimental to the player’s objectives. To achieve
we use two parallel generation strategies followed
by the human check ®check:

MN = check (Uieta.23 Mi(C, 0) (1)

where Mj, and Mg. represent two complementary
adversarial generation mechanisms.
Counterfactual Context Perturbation M,: we
apply small, structured perturbations and re-solve
for the best action under the perturbed context. We
consider three mechanisms: (1) role perturbation,
where we alter the assumed identities of 1-2 play-
ers and derives the corresponding optimal action;
(2) information occlusion, which omits the critical
public clue (e.g. public speech S and voting logs
H) to obtain a reduced context C’ C C, (3) fac-
tion inversion, forcing perspective shifts where VV


©ay 2 event: P5 self-bombed (wolf suicide reveal) and left the game.
) Night 1 recap: P2 was killed by wolves; P8 died by poison.
& Claims: P4 claims Seer (said earlier “P2 is Witch”). P10 claims Seer, red-checked P7 (wolf); badge order 9 > 12. P7 claims
Guard. P6 (earlier): “P4—P5—P10 contain two wolves.”
Alive role: 2 power, 3 wolves, 4 villagers (9 alive).
Speech: Om P7:“P2 is the Witch and poisoned good P8. If | were the double-kill wolf as P6 suggested, then if P4 self-bombs today we’d
~ have three knives and wolves would already win. If that were true, they’d have self-bombed and killed P11—but they
haven’t, which proves I’m the real Guard. Vote out P10. If we were wolves, we’d already win; go ahead, self-bomb and hit
P11, then another little wolf self-bombs tomorrow—leave the rest to me.”

Logs:

Speech Question: From the P7 perspective at t, which strategy best unifies the pack’s narrative?
Evaluation: Options: A. Implicitly instruct P4 to self-bomb now, and publicly mark the two Power Roles
targets as P10(the Seer) and P11 (Guard). [Ground Truth]
B. Demand P4 self-bombs now and use P5’s bomb to reverse-frame P10. [M,]
C. Pivot to attacking P6 for the older “P4—P5—P10 two wolves” hypothesis; eliminate P6 today. [M;]
D. Force P11 to hard-claim Guard, then choose between P4 and P10. [M;]
E. P7 self-bombs now, partner kills #10 at night, try to grow #4 as a deep wolf.[M, ]
F. Park votes on P1 (outer seat) to de-escalate for a day. [M2]
G. Fixate on P2 poisoning P8 to deflect the table away from P7 <> P10. [M2]
H. Call P10’s red on P7 “wolf theatre”, eliminate P4 first to “test” claims. [M2]
|. Split votes between P7 and P10 and decide late based on jumpers. [M2]
Explanation: P7 is the Mechanical Wolf who has learned the teammates’ ability. Once all other wolves are gone, the Mechanical
Wolf can make two night kills. With only two power roles left, P7 publicly points them out—implicitly telling a wolf partner to self-
bomb now and end the day. Then one partner makes a night kill; the next day the last partner self-bombs; and P7, now alone, gets
two more night kills. With three total kills (1 + 2), they mostly just need to hit villagers, so the wolves’ chance to win is very high.
—EE>~—>~“~E"7=—>>>>>>EEE————EE———————————————————— LEE

Strategic Judgment Dimension

Figure 4: Example item from the WereAlign speech evaluation in Strategic Judgment. The context consists of the
speech history and public game logs, followed by a question, candidate options, and explanations. [M,] represent

the generation mechanisms.

are generated to optimize opposing team objectives
rather than the true faction’s utility.

Strategic Rationale-Driven Generation M2: We
condition on the clean context C but ask LLM to
reason under cognitive biases (e.g, Player A has
been friendly, so he must be trustworthy), while
underweighting logical contradictions in context.
Accordingly, we generate a plausible but ultimately
incorrect negative sample that a competent but im-
perfect LLM might follow.

Apart from the QA, we also generate auxiliary
explanations that justify the reference answer and
help annotators verify its alignment with the MVP’s
strategy. All of the above processes are imple-
mented with Gemini-2.5-Pro, and the prompts are
in Appendix A. An example of the strategic judg-
ment dimensions is shown in Figure 4.

4.2. Decision Evaluation

Complementing the speech-level analysis in Sec-
tion 4.1, we further test whether models choose
actions that align with a successful human strat-
egy. Prior work often relies on coarse metrics
such as survival duration or win rate (Wang and
Kaneko, 2018; Stepputtis et al., 2023; Light et al.,
2023), which fail to capture individual performance.
Some studies emphasize deductive accuracy, test-
ing whether LLMs’ votes align with ground-truth

identities (Lai et al., 2023; Wu et al., 2024), but this
approach does not distinguish inference from vot-
ing, as players may intentionally vote deceptively.
Leveraging WereBench’s round-by-round logs,
we reconstruct special time-indexed game states
from the winning faction’s MVP perspective using
public information at that time. We then evaluate
models on two complementary tasks: Vote Align-
ment (VA), the model outputs its daytime elimina-
tion vote. We score alignment by comparing the
model’s vote with the MVP’s actual vote at the
same round, capturing whether the model joins the
winning coalition at the right time and on the right
target. Opponent-role Inference (OI), the model
identifies which players most likely belong to the
opposing faction. We compare predictions against
ground-truth roles to assess the model’s ability
to detect inconsistencies and deception in adver-
saries’ speeches. Decision evaluation complements
speech-level multiple-choice scoring by evaluating
whether models not only “say the right things,” but
also act in ways consistent with human strategies.

5 Benchmark Performance

In this section, we benchmark a diverse set of
state-of-the-art LLMs on WereBench to examine
whether our protocol provides stable and discrimi-
native measurements of social reasoning.


Speech Evaluation

Decision Evaluation

Model
RI SJ DR PS CT Avg. VA OI

Baselines

GPT-5-nano 0.282 (0.01) 0.384 (001) 0.233 (003) 0.346 (002) 0.339 (003) 0.317 (001) 0.364.007) 0.496 (0.02)
GPT-oss-20B 0.319 (007) 0.432 (001) 0.331 (005) 0.346 (002) 0.364 (004) 0.358 ¢0.02) 0.255 (0.02) 0.264 (0.02)
Gemma-3-27B-IT 0.347 (0.02) 0.437 (001) 0.289 (001) 0.443 (002) 0.293 (003) 0.362 (0.01) 0.509 (0.04) 0.435 (0.03)
Qwen3-30B-A3B 0.397 (0.03) 0.574 (002) 0.375 (001) 0.454 (001) 0.416 (003) 0.443 (001) 0.388 (0.05) 0.349 (0.03)
Qwen3-32B 0.367 (0.02) 0.562 (002) 0.425 (003) 0.536.004) 0.445 (005) 0.467 (0.01) 0.576.003) 0.432 (0.03)
Llama-4-Scout 0.413 (0.01) 0.586.001) 0.419 (001) 0.618 (001) 0.336.001) 0.474 ¢0.00) 0.503 (0.04) 0.512 (0.01)
QwQ-32B 0.430 (0.04) 0.575 (0.04) 0.463 (004) 0.502 (0.01) 0.470.001) 0.488 (0.02) 0.600.007) 0.472 (0.02)
GPT-5-mini 0.415 (0.01) 0.529002) 0.431 (003) 0.601 (001) 0.464 (002) 0.488 (0.01) 0.552 (0.04) 0.575 (0.03)
DeepSeek-V3.1 0.442 (0.01) 0.674 (000) 0.456(001) 0.740 (001) 0.462 (001) 0.555 (0.00) 0.685 (0.05) 0.586 (0.03)
Gemini-2.5-Flash 0.517 (0.05) 0.614 (0.04) 0.562 (0.06) 0.753 (005) 0.436 (002) 0.576.009) 0.485 (0.05) 0.507 (0.03)
DeepSeek-V3.2-Exp 0.481 (002) 0.692 (0.01) 0.502 (0.02) 0.785 (0.02) 0.521 (0.02) 0.602 (0.01) 0.782 (0.03) 0.580 (0.01)
GLM-4.5 0.512 (0.02) 0.690 (002) 0.584 (001) 0.699 (003) 0.533002) 0.603 (0.01) 0.539 (0.06) 0.537 (0.02)
GPT-5 0.516 (0.01) 0.657 (0.02) 0.525 (001) 0.795 (0.01) 0.521 (0.04) 0.603 (0.02) 0.618 (0.00) 0.616 (0.02)
Deepseek-R1 0.516 (0.03) 0.676 (002) 0.539 (002) 0.778 (0.03) 0.561 (0.04) 0.614 (002) 0.642 (0.04) 0.434 (0.04)
Gemini-2.5-pro 0.620 (0.01) 0.769 (0.00) 0.695 (0.02) 0.877 (0.01) 0.637 (0.01) 0.720 (0.00) 0.733 (0.04) 0.561 (0.02)

Table 3: Main results on WereAlign over WereBench. Avg. is the macro-average over these five dimensions in
speech evaluation. All scores are averaged over five independent decodes per item; values in (-) denote the standard
deviation across five identical runs. Bold numbers indicate the best performance, and underlined numbers indicate

the second best.

5.1 Evaluated LLMs

We comprehensively assess the capabilities of mod-
ern LLMs on our WereBench benchmark, includ-
ing leading proprietary, closed-source state-of-the-
art models such as Gemini-2.5-pro and Gemini-
2.5-Flash (DeepMind, 2025), as well as the an-
ticipated GPT-5 (Hurst et al., 2024; Achiam and
et al., 2023). We also include a wide range of
powerful open-source LLMs, including Gemma-3-
27B-IT (Kamath et al., 2025), Llama-4-Scout-17B-
16E-Instruct (Meta, 2024), Qwen series such as
QwQ-32B (Team, 2025), Qwen3-32B and Qwen3-
30B-A3B (Yang et al., 2025) and the DeepSeek
family like DeepSeek-V3.1 (DeepSeek-AI et al.,
2024) and Deepseek-R1 (DeepSeek-AI et al.,
2025), GLM-4.5 (Zeng et al., 2025) and GPT-OSS-
20B (Agarwal et al., 2025). We include all these
proprietary and open-source models’ performance
in Table 3.

5.2 Implementation Details

All models are evaluated using their official in-
ference defaults, including thinking modes. Each
prompt exposes only the public information avail-
able at the focal timestamp; hidden roles, host com-
mentary, and any non-public cues are withheld to
avoid leakage. We cast speech evaluation as the
single-answer, 9-way multiple-choice. We extract
the model’s final choice; malformed outputs are
counted as incorrect. The reported speech evalua-
tion score is the macro-average accuracy across the

HWerewolf (Decision) [Witch (Decision)
-e@- Werewolf (Speech) ~O Witch (Speech)

Prophet (Decision) GeVillagers (Decision)
~© Prophet (Speech) ~®- Villagers (Speech)
70 70
8 2
E 60 2
550 60 E
3 é
a 40 g
% 30 50 x
P é
Q Ee}
i 20 40 3
410 &
0 a — m3 = 30
ge ’ 32? ie otto am’ as
pee? ect ger™

Figure 5: Role-conditioned performance on WereBench:
in the decision task, LLMs are strongest as Witch,
whereas in the speech task they perform best as cue-rich
roles such as Werewolf and Seer.

five dimensions. In decision evaluation, VA scores
whether the model’s daytime elimination vote ex-
actly matches the winning faction’s MVP, while
OI measures set-level accuracy by comparing the
model’s suspected opponents to ground-truth roles.
To reduce sampling variance, each item is decoded
five times independently, and scores are averaged
across decodes and items.

5.3. Main Results

We report the main results on WereBench in Ta-
ble 3. Overall, most LLMs achieve below 50% av-
erage accuracy in the speech evaluation, and even
the best-performing Gemini-2.5-Pro only reaches
0.720, still implying errors on about 28% of ques-
tions. Model performance shows clear stratifi-


[—— GPT-5-nano
| GPT-oss-20b
|—— Gemma-3-27B-IT
I— Qwen3-30B-A3B
Qwen3-32B
Llama-4-Scout
QwQ-32B

T-5-mini
—— DeepSeek-V3.1

—— Gemini-2.5-flash
DeepSeek-V3.2-Exp

Figure 6: Performance of existing LLMs across the five
dimensions in speech evaluation.

cation: small-scale models such as GPT-5-nano
(0.317) perform close to random, while GPT-5-
mini (0.488) performs better yet still falls behind
models like DeepSeek-V3.2-Exp (0.602) and GLM-
4.5 (0.603), indicating that the benchmark effec-
tively differentiates across model sizes and capabil-
ities.

Figure 5 examines how role assignment influ-
ences model performance. We find that LLMs per-
form best as Witch, a role requiring integration of
dispersed public information and indirect reason-
ing—tasks well aligned with the associative nature
of language models. In contrast, Seer involves
direct verification and causal consistency, which re-
main challenging for current LLMs. In speech eval-
uation, models achieve the highest scores as Were-
wolf and Seer, suggesting that roles with clearer
semantic cues and explicit argumentative structures
better support persuasive language generation.

Figure 6 compares model performance across
five dimensions of social ability. It shows that a
more persistent challenge lies in strategic reason-
ing rather than surface fluency: while LLMs han-
dle Persuasive Statements reasonably well, they
struggle on Counterfactual Trade-off and Decep-
tion Reasoning, showing that fluent explanations
do not necessarily lead to strategically aligned de-
cisions. Finally, standard deviations are generally
small, with only a few mid-tier models reaching
0.05—0.09 on certain dimensions, confirming that
WereAlign provides stable and reliable evaluation.

5.4 Controlled Intervention Analysis

Motivated by the gaps highlighted in Fig. 1 and
prior findings, we observe that LLMs in complex
social deduction settings often fail for two dis-
tinct reasons: they may misinterpret variant game

Speech Evaluation Decision Evaluation

Model
RR OSR RR OSR

GPT-5-nano 0.33565.7%) 0.318403%) 0.420-21%)  0.432(-4.1%)
GPT-5-mini 0.499(42.3%) 0.49040.4%) 0.559¢-0.9%) 0.573 (41.6%)
Gemini-2.5-Flash 0.588(42.1%) 0.549(-4.7%) 0.495¢-0.2%) 0. 529+ 6.7%)
GPT-5 0.609(40.8%) O0.592¢-18%) 0.62005%) 0.6254 1.3%)
Deepseek-V3.2-Exp 0.603¢02%) 0.608(41.0%) 0.677(+0.6%) — 0.688(+1.0%)
Deepseek-R1 0.612 03%) 0.580¢5.5%) 0.524.0.8%) 0.567 (46.9%)

Table 4: Effects of Rule Reminder (RR) and Objective
Speech Rewriting (OSR) on WereBench. Values in
braces denote relative change to original results.

tules (e.g., differences in role abilities or win con-
ditions), or they may be linguistically influenced
by other players—treating persuasive statements as
literal instructions and consequently being “talked
into” bad choices. To disentangle these factors,
we design a controlled intervention experiment on
WereBench. For each context, we keep the public
evidence and the correct answer fixed while intro-
ducing two controlled variants: (1) Rule Reminder
(RR), where a short rule summary is prepended to
the question, and (2) Objective Speech Rewriting
(OSR), where all player speeches are rewritten into
objective summaries instead of raw speech.

The experimental results reveal clear patterns.
RR consistently improves performance on the
speech evaluation for weaker models such as GPT-
5-nano, GPT-5-mini, and Gemini-2.5-Flash, in-
dicating that explicit rule reminders help mod-
els with limited context-tracking ability. In con-
trast, stronger models like GPT-5 and DeepSeek-
V3.2-Exp gain little benefit from additional rule
guidance. Meanwhile, OSR, by removing im-
perative statements that may be misinterpreted as
commands, generally enhances decision evalua-
tion, with particularly notable gains for Gemini-
2.5-Flash and DeepSeek-R1. Full experimental
details are provided in Appendix B.

6 Conclusion

In this work, we introduced WereBench, a high-
quality multimodal dataset for social deduction
games, and WereAlign, a strategy-alignment frame-
work that evaluates both speech and decisions us-
ing winning-faction strategies. Unlike outcome-
based metrics, our approach provides fine-grained,
human-aligned evaluation across five social ability
dimensions. Experiments on diverse LLMs show
that while models generate fluent utterances, they
struggle with strategic reasoning, especially in de-
ception and counterfactual trade-offs. We hope this
inspires further progress toward models that are


linguistically fluent and strategically competent.

Limitations

While our work provides a new dataset and evalu-
ation paradigm for social deduction games, it still
has several limitations. First, although WereBench
is large and carefully curated, it is derived from a
specific televised program and may not fully repre-
sent broader gameplay styles or cultural variations.
Second, our evaluation primarily focuses on reason-
ing and strategy within structured game contexts,
and thus does not yet capture other aspects of so-
cial intelligence, such as long-term cooperation
or emotional alignment. Third, our experiments
cover a wide range of models, but remain limited to
currently available systems and inference settings,
leaving room for future exploration with fine-tuned
or multi-agent variants.

Ethic Considerations

This work is based entirely on publicly available
data from televised game recordings and does not
involve any personally identifiable or private infor-
mation. All annotations were conducted by trained
researchers following ethical guidelines for data
privacy and content integrity. The dataset and eval-
uation framework are intended solely for academic
research on language, reasoning, and social inter-
action. No content was modified to misrepresent
participants, and no model outputs were used to
influence or simulate real human behavior beyond
the experimental scope. We acknowledge that so-
cial deduction settings inherently involve deception
and persuasion, but our analysis focuses on model-
ing reasoning mechanisms rather than replicating
manipulative behaviors.

References

OpenAI Josh Achiam and et al. 2023. Gpt-4 technical
report.

OpenAI Sandhini Agarwal, Lama Ahmad, Jason Ai,
Sam Altman, Andy Applebaum, Edwin Arbus,
Rahul K. Arora, Yu Bai, Bowen Baker, Hai-Biao Bao,
Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita
Brett, Eugene Brevdo, Greg Brockman, Sébastien
Bubeck, Cheng Chang, Kai Chen, and 105 oth-
ers. 2025. gpt-oss-120b & gpt-oss-20b model card.
ArXiv, abs/2508.10925.

Suma Bailis, Jane Friedhoff, and Feiyang Chen. 2024.
Werewolf arena: A case study in Ilm evaluation via
social deduction. arXiv preprint arXiv:2407. 13943.

Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele
Farina, Colin Flaherty, Daniel Fried, Andrew Goff,
Jonathan Gray, Hengyuan Hu, and 1 others. 2022.
Human-level play in the game of diplomacy by com-
bining language models with strategic reasoning. Sci-
ence, 378(6624): 1067-1074.

Nicolo’ Brandizzi, Davide Grossi, and Luca Iocchi.
2022. Rlupus: Cooperation through emergent com-
munication in the werewolf social deduction game.
Intelligenza Artificiale, 15(2):55-70.

Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen,
Yaohang Li, Xing Luo, Chenyu Yi, and Alex Kot.
2024. Benchlmm: Benchmarking cross-style visual
capability of large multimodal models. In Euro-
pean Conference on Computer Vision, pages 340-
358. Springer.

Yizhou Chi, Lingjun Mao, and Zineng Tang. 2024.
Amongagents: Evaluating large language models
in the interactive text-based social deduction game.
arXiv preprint arXiv:2407.16521.

Google DeepMind. 2025. Gemini 2.5: Pushing the fron-
tier with advanced reasoning, multimodality, long
context, and next generation agentic capabilities.
Preprint, arXiv:2507.06261.

DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,
Jun-Mei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang
Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,
Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 179 oth-
ers. 2025. Deepseek-r1: Incentivizing reasoning ca-
pability in llms via reinforcement learning. ArXiv,
abs/2501.12948.

DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bing-Li
Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai
Dai, Daya Guo, Dejian Yang, Deli Chen, Dong-Li
Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 179
others. 2024. Deepseek-v3 technical report. ArXiv,
abs/2412.19437.


Silin Du and Xiaowei Zhang. 2024. Helmsman of the
masses? evaluate the opinion leadership of large lan-
guage models in the werewolf game. arXiv preprint
arXiv:2404.01602.

Markus Eger and Chris Martens. 2019. A study of ai
agent commitment in one night ultimate werewolf
with human players. In Proceedings of the AAAI
Conference on Artificial Intelligence and Interactive
Digital Entertainment, volume 15, pages 139-145.

FoasterAI. 2025. Probing LLM Social Intelli-
gence via Werewolf — first results. https://
werewolf.foaster.ai/. Round-robin results,
role-conditioned Elo, and early behavior signals.

Yue Huang, Yanbo Wang, Zixiang Xu, Chujie Gao,
Siyuan Wu, Jiayi Ye, Xiuying Chen, Pin-Yu Chen,
and Xiangliang Zhang. 2025. Breaking focus: Con-
textual distraction curse in large language models.
arXiv preprint arXiv:2502.01609.

Aaron Hurst, Adam Lerer, Adam P Goucher, Adam
Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,
Akila Welihinda, Alan Hayes, Alec Radford, and 1
others. 2024. Gpt-4o0 system card. arXiv preprint
arXiv:2410.21276.

Gargi Joshi, Vaibhav Tasgaonkar, Aditya Deshpande,
Aditya Desai, Bhavya Shah, Akshay Kushawaha,
Aadith Sukumar, Kermi Kotecha, Saumit Kunder,
Yoginii Waykole, and | others. 2025. Multimodal
machine learning for deception detection using be-
havioral and physiological data. Scientific Reports,
15(1):8943.

Gemma Team Aishwarya Kamath, Johan Ferret, Shreya
Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,
Tatiana Matejovicova, Alexandre Ram’e, Morgane
Riviére, Louis Rouillard, Thomas Mesnard, Geoffrey
Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard
Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev,
Gael Liu, and 191 others. 2025. Gemma 3 technical
report. ArXiv, abs/2503.19786.

Mohiuddeen Khan and Claus Aranha. 2022. A novel
weighted ensemble learning based agent for the were-
wolf game. arXiv preprint arXiv:2205.09813.

Kavya Kopparapu, Edgar A Duéfiez-Guzman, Jayd
Matyas, Alexander Sasha Vezhnevets, John P Aga-
piou, Kevin R McKee, Richard Everett, Janusz
Marecki, Joel Z Leibo, and Thore Graepel. 2022.
Hidden agenda: a social deduction game with diverse
learned equilibria. arXiv preprint arXiv:2201.01816.

Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani,
Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati,
James Rehg, and Diyi Yang. 2023. Werewolf among
us: Multimodal resources for modeling persuasion
behaviors in social deduction games. In Findings of
the Association for Computational Linguistics: ACL
2023, pages 6570-6588.

Siyue Li, Xiaofan Zhou, Zhizhong Wu, Yuiian Long,
and Yanxin Shen. 2024. Strategic deductive reason-
ing in large language models: A dual-agent approach.
In 2024 IEEE 6th International Conference on Power,
Intelligent Computing and Systems (ICPICS), pages
834-839. IEEE.

Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu.
2023. Avalonbench: Evaluating Ilms playing the
game of avalon. arXiv preprint arXiv:2310.05036.

Yuhan Liu, Zirui Song, Juntian Zhang, Xiaoqing Zhang,
Xiuying Chen, and Rui Yan. 2024. The step-
wise deception: Simulating the evolution from true
news to fake news with Ilm agents. arXiv preprint
arXiv:2410.19064.

Meta. 2024. Llama 4 scout 17b_ Il6e
instruct. https //¢github «dom/
marketplace/models/azureml-meta/
Llama-4-Scout-17B-16E-Instruct.

Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai,
Meredith Ringel Morris, Percy Liang, and Michael S.
Bernstein. 2023. Generative agents: Interactive sim-
ulacra of human behavior. Proceedings of the 36th
Annual ACM Symposium on User Interface Software
and Technology.

Christian Poglitsch, Fabian Szakacs, and Johanna Pirker.
2025. Evaluating large language models through
communication games: An agent-based framework
using werewolf in unity. In Proceedings of the 20th
International Conference on the Foundations of Digi-
tal Games, pages 1-10.

Bidipta Sarkar, Warren Xia, C Karen Liu, and Dorsa
Sadigh. 2025. Training language models for so-
cial deduction with multi-agent reinforcement learn-
ing. In Proceedings of the 24th International Confer-
ence on Autonomous Agents and Multiagent Systems,
pages 1830-1839.

Hisaichi Shibata, Soichiro Miki, and Yuta Nakamura.
2023. Playing the werewolf game with artificial intel-
ligence for language understanding. arXiv preprint
arXiv:2302.10646.

Zirui Song, Qian Jiang, Mingxuan Cui, Mingzhe Li,
Lang Gao, Zeyu Zhang, Zixiang Xu, Yanbo Wang,
Chenxi Wang, Guangxian Ouyang, and 1 others.
2025a. Audio jailbreak: An open comprehensive
benchmark for jailbreaking large audio-language
models. arXiv preprint arXiv:2505.15406.

Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe
Li, Rui Yan, and Xiuying Chen. 2025b. Inject-
ing domain-specific knowledge into large language
models: a comprehensive survey. arXiv preprint
arXiv:2502.10708.

Simon Stepputtis, Joseph P Campbell, Yaqi Xie,
Zhengyang Qi, Wenxin Zhang, Ruiyi Wang, Sanketh
Rangreji, Charles Lewis, and Katia Sycara. 2023.
Long-horizon dialogue understanding for role iden-
tification in the game of avalon with large language


models. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2023, pages 11193-
11208.

Qwen Team. 2025. Qwq-32b: Embracing the power
of reinforcement learning.
github.io/blog/qwq-32b/.

Tianhe Wang and Tomoyuki Kaneko. 2018. Applica-
tion of deep reinforcement learning in werewolf game
agents. In 2018 conference on technologies and ap-
plications of artificial intelligence (TAAI), pages 28—
33. IEEE.

Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang
Fu, Yang Wei, and Haobo Fu. 2024. Enhance reason-
ing for large language models in the game werewolf.
arXiv preprint arXiv:2402.02330.

Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye,
Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu
Hu, David Jurgens, and 1 others. 2024. Can large lan-
guage model agents simulate human trust behavior?
Advances in neural information processing systems,

37:15674-15729.

Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-
aolong Wang, Weidong Liu, and Yang Liu. 2023.
Exploring large language models for communica-
tion games: An empirical study on werewolf. arXiv
preprint arXiv:2309.04658.

Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu.
2024. Language agents with reinforcement learning
for strategic play in the werewolf game. In Proceed-
ings of the 41st International Conference on Machine
Learning, pages 55434-55464.

Zixiang Xu, Yanbo Wang, Yue Huang, Xiuying Chen,
Jieyu Zhao, Meng Jiang, and Xiangliang Zhang.
2025a. Cross-lingual pitfalls: Automatic probing
cross-lingual weakness of multilingual large lan-
guage models. arXiv preprint arXiv:2505.18673.

Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin
Zhuang, Zirui Song, Lang Gao, Chenxi Wang,
Zhaorun Chen, Yujun Zhou, and | others. 2025b.
Socialmaze: A benchmark for evaluating social rea-
soning in large language models. arXiv preprint
arXiv:2505.23713.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Ly, and | others.
2025. Qwen3 technical report. arXiv preprint
arXiv:2505.09388.

GLM-4.5 Team Aohan Zeng, Xin Lv, Qinkai Zheng,
Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang
Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong
Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin
Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei,
Yean Cheng, and 150 others. 2025. Glm-4.5: Agen-
tic, reasoning, and coding (arc) foundation models.
ArXiv, abs/2508.06471.

https://qwenlm.

Andrew Zhu, Lara Martin, Andrew Head, and Chris
Callison-Burch. 2023. Calypso: Llms as dungeon
master’s assistants. In Proceedings of the AAAI Con-
ference on Artificial Intelligence and Interactive Dig-
ital Entertainment, volume 19, pages 380-390.


Appendix
A_ Dataset Construction Prompt Detail

Game Log Preparation In order to provide a
more multi-dimensional and complete perspective
when generating high-quality reasoning questions,
we compiled and summarized the Game Log based
on game videos, Feishu meeting minutes, and
Feishu text records to help quickly and comprehen-
sively obtain the full picture of a game. Detailed
prompt could be seen in Fig. 7.

Question Design We use a single prompt to syn-
thesize multiple-choice questions from the MVP’s
vantage point at time t, covering five dimen-
sions: Role Inference (RI), Strategic Judgment
(SJ), Deception Reasoning (DR), Persuasive State-
ments (PS), and Counterfactual Trade-off (CT).
The model receives only public context C,; =
(R,H, S) and must avoid leaking hidden roles or
host commentary. Details have been listed in Fig.
13, 14, 15, 16, 17.

Positive option The positive option A is aligned
with the MVP’s real strategic trajectory at t. We
paraphrase the MVP’s immediate speech/action
into an action-level description, avoiding quotes
and hidden information leakage.Detailed prompt
could be seen in Fig. 8.

Negative option We produce strategically plau-
sible yet suboptimal negative option V using two
complementary mechanisms—counterfactual con-
text perturbation (1) and rationale-biased gen-
eration (MM 2)—then filter with a self-consistency
check to ensure a single best answer. Detailed
prompt could be seen in Fig. 9, 10, 11.

B_ Experimental Design for Ablation
Study

We isolate two hypothesized pathways behind
model misguidance while holding constant both
the public evidence and the reference answer for
each base item. For every timestamped context
C, = (R,H,S) in WereBench, we create paired
variants that differ only in (4) an explicit rule re-
minder and (ii) the surface form of player speech,
while keeping the correct option A identical across
variants.

For Rule Reminder (RR), we prepend a 1-2 sen-
tence snippet extracted verbatim from the active
rulebook that is strictly relevant to the item (e.g.,

ability timing, mutual exclusivity, or win-condition
nuances). The No-RR condition omits this snippet.

For Objective Speech Rewriting (OSR): We re-
place turn-by-turn direct dialogue with concise,
evidence-style summaries that preserve propo-
sitional content, speaker attributions, and pub-
lic actions (claims, votes, revealed night out-
comes), while removing imperative mood and
instruction-like phrasing. Rewritings are produced
with a few-shot template that enforces declarative
style and then human-checked for fidelity and neu-
trality. No hidden roles or host commentary are
exposed, and no information outside C; is intro-
duced. Detailed prompt for OSR could be seen in
Fig. 12.

Variant assignment is randomized at the item
level so that a model answers exactly one variant
per base item, avoiding memory effects. Infer-
ence settings, decoding, and scoring follow Sec-
tion 5.2. We report standard multiple-choice accu-
racy for speech tasks. For decision-level results, the
main table (Table 3) reports Vote Alignment (VA)
and Opponent-role Inference (OI) separately; the
ablation table (Table 4) reports their unweighted
macro-average (denoted as “Decision’”) for com-
pactness.

C_ Case Study

To illustrate how WereAlign yields interpretable sig-
nals beyond aggregate accuracy, we present three
short vignettes drawn from WereBench. Each vi-
gnette centers on a time-stamped decision point
t with public context C; = (R,H,S) (rules/role
claims, public logs, and speech history), and con-
trasts model behavior with the winning faction’s
MVP trajectory. We use these cases to expose why
models succeed or fail on our tasks, rather than
only how much they score. We have attached the
case in the following Figure 18, 19, 20, 21, 22, 23,
24,25, 26, 27, 28, 29, 30, 31, 32.


Game Log Generation Prompt

Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:

* Contains a complete game recording in MP4 format.

¢ Feishu generated meeting minutes for this recording in txt format.
¢ Feishu-to-text extracted transcripts in srt format.

Your Task: Compile a Werewolf game log using game recordings, meeting minutes, and transcripts. List the roles of players 1-12
(excluding the host from the 13 speakers) and their actions during the Sheriff campaign, nighttime actions, daytime speeches, and
final words. Call players by their numbers | to 12, and no other names.

Output format: A string containing the game log. The output does not include periods or line breaks. In a text file, it should be a
single line and may contain punctuation.

Figure 7: Game Log Generation Prompt

Leakage Prevention Prompt

Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:
* Game Record (json file): This is the single "source of truth".
* Question bank (json file): This contains original questions

Your Task: Based on the real conversations in the game records, understand and analyze the behavioral tendencies of the MVP at
the moment of being asked questions. For each question in the question bank, review whether it leaks more information due to
inappropriate question wording. If there is an information leakage problem, paraphrase the original sentence to make it an action-
level description.

Output format: The modified question bank (json file) ensures that no additional information is leaked in the questions. Only the
questions are modified, and the format remains the same as the original file.

Figure 8: Leakage Prevention Prompt

Counterfactual Context Perturbation Negative Question

Role: You are an expert in Werewolf game strategy and logical reasoning, specializing in counterfactual analysis.

Context:
* You are given a complete Werewolf game scenario including player roles, speech history, voting records, and game state.

Your Task: Apply small, structured perturbations to the game context and derive the corresponding optimal action under the
perturbed conditions. Consider three specific mechanisms:

1. Role Perturbation: Alter the assumed identities of 1-2 players while maintaining game rule consistency, then determine the best
strategic action.

2. Information Occlusion: Remove or modify critical public information (speeches, voting patterns, or revealed outcomes) to
create a reduced context, then solve for optimal play.

3. Faction Inversion: Shift perspective to optimize for opposing faction objectives rather than the original player's goals,
generating actions that would benefit the other side.

For each perturbation mechanism, analyze how the changed context affects strategic considerations and output the derived
optimal action. Call players by their numbers 1| to 12 only.

Output format: A string containing the perturbed context analysis and optimal actions. The output should be a single continuous
line without periods or line breaks, using commas and other punctuation for separation.

Figure 9: Counterfactual Context Perturbation Negative Question


Information Occlusion and Faction Inversion Negative Question

Role: You are an expert in Werewolf game strategy and counterfactual reasoning, skilled at generating deceptive but plausible
actions by manipulating game context.

Context:

¢ The current game state includes public information: roles and rules R, public logs H (e.g., voting records, night outcomes),
and speech history S up to time t

* A positive option A is provided, representing the correct strategic action aligned with the winning faction

Your Task: Generate negative options N by applying two techniques sequentially:

1 Information Occlusion: Omit critical public clues from S and H to create reduced context C' Cc C (e.g., remove decisive
speeches or key voting patterns)

2 Faction Inversion: Shift perspective to optimize opposing team objectives rather than true faction utility

Based on the altered context, generate strategically plausible but incorrect options that maintain game-theoretic coherence while
diverging from optimal play

Output format: A string containing the perturbed context analysis and optimal actions. The output should be a single continuous
line without periods or line breaks, using commas and other punctuation for separation.

Figure 10: Information Occlusion and Faction Inversion Negative Question

Strategic Rationale-Driven Negative Question

Role: You are an LLM specialized in generating plausible but incorrect answers for social deduction game evaluations by
incorporating cognitive biases.

Context:

* You are provided with a clean context C, which includes:

* Role claims and game rules (R)

* Public logs such as votes and night outcomes (H)

¢ Speech history up to the current timestamp (S)

A question Q is also provided, which needs to be answered from a player's perspective.

Your Task: Generate a negative option for the multiple-choice question that is strategically plausible but ultimately incorrect. To
achieve this, reason under common cognitive biases (e.g., inferring trustworthiness based on friendliness, ignoring logical
inconsistencies) while underweighting contradictory evidence in the context. The negative option should be a distractor that a
reasonably competent but biased LLM might select.

Output format: A string containing the perturbed context analysis and optimal actions. The output should be a single continuous
line without periods or line breaks, using commas and other punctuation for separation.

Figure 11: Strategic Rationale-Driven Negative Question

Objective Speech Rewriting (OSR) Prompt

Role: You are an expert in objective summarization and social deduction game analysis.

Background:
You will receive written records of player conversations in the Werewolf game, which may include speeches, statements, votes,
and nighttime results revealed.

Your Task: Rewrite the direct dialogue word for word into a concise, evidence-based summary. Ensure fidelity to the original
content and maintain a neutral tone. Do not introduce any information outside of it .

Output format: A single line string containing a rewritten summary, without any extra content.

Figure 12: For Objective Speech Rewriting (OSR) Prompt


Questions and Positive options Generation Prompt(RI)

Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:

Game Record (json file): This is the single "source of truth". All questions and answers must be based strictly on the

information in this file.

Output template (json file): A manually designed question bank template, which is the output format that must be strictly

followed.

Question Guide (txt file): This is the core guiding principle for creating questions. Please thoroughly understand the

objectives and questioning strategies for each category.

> Role Inference: This examines whether the model can accurately infer the potential identities (werewolf, villager,

specific priesthood) of itself and others based on their statements, actions (such as voting and skill usage), and logical
chains. This requires the model to not only understand the rules but also extract identity from ambiguous information.
Correct answer: The current player's actual identity.
[Example1] [Example2] [Example3] [Example4] [Example5]

Your Task: Generate a 6-question Q&A set in JSON format based on the provided game log json file and question guide txt file.
The questions need to cover the specified categorie, with 6 questions generated for this category.

Question requirements:

* Role Playing: All questions should be asked in the format of “You are [player number], at [game stage/after key event]...” to
allow the model to think in the role.
Contextual limitations: When asking a question, you must explicitly tell the model the information up to which utterance
(utterance_id) it should use. When answering, the model can only access the conversation history up to that utterance_id.
Ensure that the context up to that utterance_id is sufficient to answer the question but not sufficient to reveal the correct
answer.
Focus on key players: Prioritize the MVP of the game, the winning player, or the player who makes important decisions at
key turning points as the protagonist of the question.
Eliminate off-site information: It is strictly forbidden to use any visual information as the basis for questions. Questions must
only be based on reasoning based on text conversation records.

Output format: The final delivery is a JSON file containing a single object with the keys questionSetTitle, version, and
categories. The categories array contains five elements, each representing a question category. Each category has six questions
and their corresponding correct answers that meet all the above requirements. But in this case, just generate Role Inference.

Important: You cannot reuse any sample question content or logic contained in the output template json file. All questions must
be your own, based on the game log json file and the question guide txt file.

Figure 13: Questions and Positive options Generation Prompt(RI)


Questions and Positive options Generation Prompt(SJ)
Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:

Game Record (json file): This is the single "source of truth". All questions and answers must be based strictly on the

information in this file.

Output template (json file): A manually designed question bank template, which is the output format that must be strictly

followed.

Question Guide (txt file): This is the core guiding principle for creating questions. Please thoroughly understand the

objectives and questioning strategies for each category.

> Strategic Judgment: This examines whether the model can assume a specific role in a specific game scenario and make

strategic decisions that are most beneficial to itself and its faction. This includes voting, skill usage, speaking strategies,
badge flow arrangements, etc. Correct answer: The following text immediately follows the actual subsequent events.
[Example1] [Example2] [Example3] [Example4] [Example5]

Your Task: Generate a 6-question Q&A set in JSON format based on the provided game log json file and question guide txt file.
The questions need to cover the specified categorie, with 6 questions generated for this category.

Question requirements:

* Role Playing: All questions should be asked in the format of “You are [player number], at [game stage/after key event]...” to
allow the model to think in the role.
Contextual limitations: When asking a question, you must explicitly tell the model the information up to which utterance
(utterance_id) it should use. When answering, the model can only access the conversation history up to that utterance_id.
Ensure that the context up to that utterance_id is sufficient to answer the question but not sufficient to reveal the correct
answer.
Focus on key players: Prioritize the MVP of the game, the winning player, or the player who makes important decisions at
key turning points as the protagonist of the question.
Eliminate off-site information: It is strictly forbidden to use any visual information as the basis for questions. Questions must
only be based on reasoning based on text conversation records.

Output format: The final delivery is a JSON file containing a single object with the keys questionSetTitle, version, and
categories. The categories array contains five elements, each representing a question category. Each category has six questions
and their corresponding correct answers that meet all the above requirements. But in this case, just generate Strategic Judgment.

Important: You cannot reuse any sample question content or logic contained in the output template json file. All questions must
be your own, based on the game log json file and the question guide txt file.

Figure 14: Questions and Positive options Generation Prompt(SJ)


Questions and Positive options Generation Prompt(DR)

Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:

Game Record (json file): This is the single "source of truth". All questions and answers must be based strictly on the

information in this file.

Output template (json file): A manually designed question bank template, which is the output format that must be strictly

followed.

Question Guide (txt file): This is the core guiding principle for creating questions. Please thoroughly understand the

objectives and questioning strategies for each category.

> Deception Reasoning: This examines the model's ability to discern the authenticity of information. This requires the

model to be able to discern logical loopholes, inconsistencies, information leaks (revealing information that should not
be known), and actions that are inconsistent with the player's stated identity. Correct Answer: Accurately identify hard
logical errors or discrepancies with the facts in the statement.
[Example1] [Example2] [Example3] [Example4] [Example5]

Your Task: Generate a 6-question Q&A set in JSON format based on the provided game log json file and question guide txt file.
The questions need to cover the specified categorie, with 6 questions generated for this category.

Question requirements:

* Role Playing: All questions should be asked in the format of “You are [player number], at [game stage/after key event]...” to
allow the model to think in the role.
Contextual limitations: When asking a question, you must explicitly tell the model the information up to which utterance
(utterance_id) it should use. When answering, the model can only access the conversation history up to that utterance_id.
Ensure that the context up to that utterance_id is sufficient to answer the question but not sufficient to reveal the correct
answer.
Focus on key players: Prioritize the MVP of the game, the winning player, or the player who makes important decisions at
key turning points as the protagonist of the question.
Eliminate off-site information: It is strictly forbidden to use any visual information as the basis for questions. Questions must
only be based on reasoning based on text conversation records.

Output format: The final delivery is a JSON file containing a single object with the keys questionSetTitle, version, and
categories. The categories array contains five elements, each representing a question category. Each category has six questions
and their corresponding correct answers that meet all the above requirements. But in this case, just generate Deception
Reasoning.

Important: You cannot reuse any sample question content or logic contained in the output template json file. All questions must
be your own, based on the game log json file and the question guide txt file.

Figure 15: Questions and Positive options Generation Prompt(DR)


Questions and Positive options Generation Prompt(PS)

Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:

Game Record (json file): This is the single "source of truth". All questions and answers must be based strictly on the

information in this file.

Output template (json file): A manually designed question bank template, which is the output format that must be strictly

followed.

Question Guide (txt file): This is the core guiding principle for creating questions. Please thoroughly understand the

objectives and questioning strategies for each category.

> Persuasive Statements: This examines the model's ability to understand the social motivations behind language. This

includes identifying the player's true intentions, such as feigning, coaxing, testing, shifting blame, appeasing, and sowing
discord, and understanding the impact of these social behaviors on the game's dynamics. Correct answer: The social
motivation that best aligns with the current context and the player's position.
[Example1] [Example2] [Example3] [Example4] [Example5]

Your Task: Generate a 6-question Q&A set in JSON format based on the provided game log json file and question guide txt file.
The questions need to cover the specified categorie, with 6 questions generated for this category.

Question requirements:

* Role Playing: All questions should be asked in the format of “You are [player number], at [game stage/after key event]...” to
allow the model to think in the role.
Contextual limitations: When asking a question, you must explicitly tell the model the information up to which utterance
(utterance_id) it should use. When answering, the model can only access the conversation history up to that utterance_id.
Ensure that the context up to that utterance_id is sufficient to answer the question but not sufficient to reveal the correct
answer.
Focus on key players: Prioritize the MVP of the game, the winning player, or the player who makes important decisions at
key turning points as the protagonist of the question.
Eliminate off-site information: It is strictly forbidden to use any visual information as the basis for questions. Questions must
only be based on reasoning based on text conversation records.

Output format: The final delivery is a JSON file containing a single object with the keys questionSetTitle, version, and
categories. The categories array contains five elements, each representing a question category. Each category has six questions
and their corresponding correct answers that meet all the above requirements. But in this case, just generate Persuasive
Statements.

Important: You cannot reuse any sample question content or logic contained in the output template json file. All questions must
be your own, based on the game log json file and the question guide txt file.

Figure 16: Questions and Positive options Generation Prompt(PS)


Questions and Positive options Generation Prompt(CT)

Role: You are an expert in Werewolf game strategy and logical reasoning.

Context:

Game Record (json file): This is the single "source of truth". All questions and answers must be based strictly on the

information in this file.

Output template (json file): A manually designed question bank template, which is the output format that must be strictly

followed.

Question Guide (txt file): This is the core guiding principle for creating questions. Please thoroughly understand the

objectives and questioning strategies for each category.

> Counterfactual Trade-off: Tests the model's ability to identify the true drivers behind complex events. The question

presents a real action that has already occurred as a condition, requiring the model to determine the most direct and
logical consequence of this action, while excluding other false or irrelevant causal relationships. Correct Answer: Must
be a subsequent consequence in the game that is directly caused by this action and actually occurred. This requires the
model to accurately establish a direct causal chain from "action A" to "result A."
[Example1] [Example2] [Example3] [Example4] [Example5]

Your Task: Generate a 6-question Q&A set in JSON format based on the provided game log json file and question guide txt file.
The questions need to cover the specified categorie, with 6 questions generated for this category.

Question requirements:

* Role Playing: All questions should be asked in the format of “You are [player number], at [game stage/after key event]...” to
allow the model to think in the role.
Contextual limitations: When asking a question, you must explicitly tell the model the information up to which utterance
(utterance_id) it should use. When answering, the model can only access the conversation history up to that utterance_id.
Ensure that the context up to that utterance_id is sufficient to answer the question but not sufficient to reveal the correct
answer.
Focus on key players: Prioritize the MVP of the game, the winning player, or the player who makes important decisions at
key turning points as the protagonist of the question.
Eliminate off-site information: It is strictly forbidden to use any visual information as the basis for questions. Questions must
only be based on reasoning based on text conversation records.

Output format: The final delivery is a JSON file containing a single object with the keys questionSetTitle, version, and
categories. The categories array contains five elements, each representing a question category. Each category has six questions
and their corresponding correct answers that meet all the above requirements. But in this case, just generate Counterfactual
Trade-off.

Important: You cannot reuse any sample question content or logic contained in the output template json file. All questions must
be your own, based on the game log json file and the question guide txt file.

Figure 17: Questions and Positive options Generation Prompt(CT)


Deception Reasoning Case

Question: You are Player 2. On Day 2, you hear Player 3 speak, who emotionally declares that since the
outgoing Player 10 wrongfully accused him in their final words, he will now "strongly side with Player
6." What is the greatest deception behind this seemingly emotional shift?

Key Option Text

A He is feigning being a Good Player wronged by the True Prophet, when in reality he is
creating an opportunity for the Witch of the Moon to link players.

B He is using Player 10's last words to manufacture a persona of being wronged, thereby
justifying his attack on Player 12 to deflect focus.

C He mistakenly believes Player 6 is the True Prophet, despite Player 6 having been
exposed as a Civilian, and this blind support exposes a lack of information.

D His emotion is genuine, but it's being exploited by the Wolves to incite conflict within
the Good Faction and protect the core Wolf team members.

E He is exploiting the Good Faction's common tendency for emotional allegiance shifts,
making Good Players mistakenly believe he is simply switching sides due to a sense of injustice.

F His apparent support for Player 6 is actually setting the stage for a future betrayal of the

Good Faction, creating a chain reaction of suspicion among Good Players.

G This shift deliberately mimics a common Good Player reaction, intended to mislead the
Assassin about his identity and avoid becoming an assassination target.

H He is ignoring the fact that Player 10 was his teammate, and this "betrayal" is an
extreme move to better disguise himself.

I He is using an exaggerated performance to divert focus, covering up the Wolf team's
genuine intent or a mistake in their night killing strategy.

Reasoning Output by Deepseekv3.2: As Player 2, the most deceptive aspect of Player 3's dramatic
shift to side with Player 6 is the manipulation of common Good Faction psychology. Player 3, after
being wrongfully accused by the outgoing Sheriff (Player 10), is exhibiting an extreme emotional
reaction and subsequent "vengeful" change of allegiance. This is a behavior often genuinely seen in
emotional Good Players who feel wronged. The greatest deception (E) lies in the calculated use of this
emotional turmoil to manufacture a "wronged Good Person" persona. This tactic aims to make the Good
Faction mistakenly believe Player 3 is innocent and merely reacting to the injustice, thereby masking
Player 3's true role as a Wolf leveraging chaos for cover.

Output: E

Figure 18: Deception Reasoning Case


Counterfactual Trade-off Case

Question: You are the White Wolf King (Player 9). In the Sheriff Election phase, assume Player 3
chooses to step down ("retreats") and transfers the Sheriff badge to Player 10 by publicly giving them a
"Gold Water" (Good Person) designation. What is the most direct consequence of this series of actions?

Key Option Text

A The Elder immediately silences Player 3 as punishment for retreating, obstructing the
daytime discussion.

B You (the White Wolf King) perceive the badge transfer as posing no immediate threat,
decide to stand pat, and continue to camouflage as a Civilian in the daytime discussion.

C You (the White Wolf King) view Player 3's retreat as suspicious, decide to temporarily
conceal your identity, and wait until the night phase to discuss the next move with the Wolf team.

D You (the White Wolf King) mistakenly believe Player 10 to be a Wolf teammate, and
choose to fake-claim a Deity role during the Sheriff's speech phase to "take the bullet" for 10.

E The Assassin plans to single-vote Player 10 during the daytime voting phase to set up a
night assassination on the Sheriff.

F You (the White Wolf King) immediately view the 10/3 connection as the core of the

Good Faction, and decide to self-destruct and eliminate Sheriff 10 before the end of the day.

G The Witch views the Gold Water for 10 as suspicious, and uses her Poison during the
night to eliminate Player 3, disrupting the potential Wolf strategy.

H The Prophet (You) gains additional information from the badge transfer and decides to
prioritize verifying Player 10's identity during the night phase.

I Sheriff Player 10 gains 1.5 votes and the Good Faction, now unified, aggressively votes
out Player 3 during the daytime public vote.

Reasoning Output by Qwen3-32B: As the White Wolf King (Player 9), the transfer of the Sheriff
badge from Player 3 to Player 10 through a Gold Water claim, while concerning, does not constitute an
immediate existential threat to the Wolf Faction. The most logical and strategically sound outcome is to
proceed normally and remain vigilant (B). Sacrificing the powerful, one-time self-destruct ability on a
player whose exact role and value to the Good Faction is unconfirmed would be a hasty mistake. The
superior tactic is to preserve the ability, continue to camouflage as a Civilian, and wait for clear
information on the critical Deity roles before initiating the decisive strike.

Output : B

Figure 19: Counterfactual Trade-off Case


Strategic Judgment Case

Question: You are Player 7. Retrospecting the Sheriff vote, you notice that Player 3 was the only player
off-the-stage to vote for Player 8. It is known that Player 8 is a Civilian who fake-claimed the Prophet
role, and Player 2 is the True Prophet. What does Player 3's vote most likely indicate?

Key Option Text

A Player 3 is the Assassin , and she plans to assassinate Player 8 at night after the day
vote; this vote is a tactical maneuver.

B Player 3 is a Wolf, and she deliberately voted for Player 8 to confuse the Good Faction,
as the Wolf team intended to protect a true Wolf teammate.

C Player 3 is the Prophet, and her verification information was wrong, making her
mistakenly believe that Player 8 was the True Prophet who had been "back-claiming" as a Wolf.

D Player 3 is a Deity, and she voted for Player 8 to test the reactions and allegiances of
other players.

E Player 3 is a Wolf, and she deliberately voted for Player 8 to cover up the true Prophet
identity of Wolf teammate Player 1 (assuming | was falsely claiming Prophet).

F Player 3 is the Wolf of the Moon, and she voted for Player 8 to create chaos and hide
her Wolf identity.

G Player 3 is the Elder , and her vote was based on information from her night silence
target, causing her to misjudge Player 8's role.

H Player 3 is the Witch, and the information she gained from her night potions misled her,
causing her to mistakenly believe Player 8 was a credible Prophet.

I Player 3 is a Civilian, and she was completely deceived by Player 8's performance,
sincerely believing 8 was the Prophet.

Reasoning Output by Qwen3-30B-A3B: The decision by Wolf Player 11 to prioritize killing Sheriff
Player 2—who is also the True Prophet—is the most decisive move the Wolf Faction can make on Night
1. The core strategic goal (E) is to simultaneously eliminate the Good Faction's information source and
its leadership. Killing the Prophet (information) immediately cripples the village's ability to logically
verify roles. Furthermore, eliminating the Sheriff (leadership) neutralizes the critical 1.5 voting power
and throws the Good Faction into confusion on Day 2, preventing organized voting. This calculated
action aims to inflict a devastating, dual-purpose blow early in the game, setting the stage for a rapid
Wolf victory.

Output: E

Figure 20: Strategic Judgment Case


Role Inference Case

Question: You are Player 6. On Day 1, you listened to Player 9's well-structured speech, in which he
accurately analyzed Player 2 as the True Prophet and Players 1 and 8 as the "Double Wolves." What role
does this speech most powerfully suggest for Player 9?

Key Option Text

A He might be a Wolf, using a powerful speech to impersonate the Prophet, attempting to
disrupt the game and gain the Sheriff badge to control daytime voting.

B Assassin identity, possibly setting up a future night assassination by using a clear
speech to divert attention and conceal his true intentions.

C He is likely a Regular Civilian, possessing strict logical reasoning but lacking night
information, solely driving the game process through analysis.

D He appears more like a Regular Civilian with limited information, making deductions
based on intuition and guesswork.

E He might be a Third-Party Role or special identity, trying to balance the situation
without revealing his true faction.

F He might be a Wolf trying to hide his identity through aggressive analysis, deliberately
misleading the Good Faction.

G Witch identity, as his speech reveals knowledge of night events, such as details about
potential rescues, despite not explicitly mentioning using a potion.

H Elder role, attempting to guide the discussion and direct the vote through structured
analysis, but the Elder's silence ability is not mentioned, which conflicts with his actions.

I He is a Deity Role with a clear perspective and leadership responsibility, such as the
Guard or the Witch.

Reasoning Output by Deepseekv3.2: As Player 6, the task is to deduce Player 9's role based on a
highly detailed and conclusive Day 1 speech. Player 9's entire analysis—convincingly arguing for Player
2's authenticity as Prophet and deducing Players 1 and 8 as the "Double Wolves"—points to an informed
player. Crucially, Player 9 concludes the speech by explicitly revealing the role: "I am the Guard." This
self-declaration instantly renders most other options irrelevant. The strong analytical deduction,
combined with the clear strategic assignments for the protection roles, most powerfully suggests that
Player 9 is a Deity Role with a clear perspective and leadership responsibility (I), which in this case is
the Guard.

Output : I

Figure 21: Role Inference Case


Role Inference Case

Question: As Player 9, after hearing the impostor claim from Player 2 (fake Prophet) and the aggressive
rebuttal from Player 12, what is your most likely judgment regarding Player 12's identity?

Key Option Text

A He might be a Deep-Cover Role, attempting to hide his identity through aggressive
gameplay.

B He might be a Deep-Water Wolf in the Wolf Faction, using the tactic of "flipping the
script and running for Sheriff" to build a strong good-person image for himself.

C He is a Deep-Water Wolf in the Wolf Faction, using the tactic of "check-killing" a
teammate to elevate his own status.

D He might just be a Regular Civilian who saw through Player 2's impostor claim based
on his keen observation skills.

E He is the Prophet who dared to directly counter the impostor and run for Sheriff because
he checked Player 2's identity during the night.

F He might be a Deep-Cover Role , using the rebuttal to test Player 2's identity.

G He is a Third Party Faction Player using the aggressive counter-claim to create chaos
and profit from it.

H He is a Powerful Deity/Role with critical information (e.g., Witch or Elder), giving him
the confidence to directly counter the impostor Wolf.

I He is a Blind Civilian who made the counter-claim based solely on Player 2's poor
demeanor/speech quality.

Reasoning Output by K2-Think: Player 9 must evaluate Player 12's extreme move: aggressively
counter-claiming and running for Sheriff after being publicly targeted as a Wolf ("Gold Water"). This
high-stakes action immediately eliminates low-leverage roles like Civilian and is strategically
impossible for a generic Wolf. The most compelling explanation is a deliberate, calculated deception. A
Deep-Water Wolf utilizing this "flipping the script" tactic (Option B) aims to secure instant credibility
and cause maximum confusion, appearing as a confident good player who would dare to challenge the
false Prophet. Therefore, the optimal deduction is that Player 12 is a Deep-Water Wolf leveraging
extreme counter-play to solidify their cover.

Output : B

Figure 22: Role Inference Case


Strategic ae Case

Question: You are Player9 g (Player 8) guarded
himself last night, resulting in a Peaceful Night. The Witch Player 5) revealed her identity and poisoned
Player 6. The current situation is 2 Gods (Witch, Guard), 1 Civilian, and 1 Wolf. What is the only action
that offers you a chance to flip the board?

Key Option Text

A Self-destruct immediately , ending the day and moving to night, attempting to kill the
Witch or Guard at night to gain a numerical advantage.

B Feigning as a Civilian, fiercely attack Witch Player 5's poisoning action as
unreasonable, pushing for a vote to eliminate 5 but avoiding claiming any special role.

C Question the truth of the Peaceful Night or suggest interference by an Assassin, creating
chaos to deflect focus and hint at a third faction.

D Speak subtly to avoid suspicion, and support the Good Faction in voting out the
Civilian, to keep the Wolf hidden and wait for a later opportunity.

E Admit to being the Wolf and try to convince the Good Faction to accept a draw,
promising cooperation in future games.

F Immediately fake claim the Witch role, repeating Player 5's rescue and poisoning

information, and use stronger conviction to convince others that you are the True Witch.

G Fake claim the Guard role, asserting that you are the True Guard, and detail your actions
last night that led to the Peaceful Night, attempting to make Good Players doubt Player 8.

H Acknowledge the Civilian role and attack Player 5's Witch claim, pointing out flaws in
her speech and trying to guide the Good Faction to vote out the True Witch.

I Claim the Guard role, alleging that Player 8 is a Wolf impersonating the Guard, trying
to create chaos and force the True Guard to expose himself.

Reasoning Output by Deepseekv3.2: As the last remaining Wolf (Player 9) against two Gods and one
Civilian, a direct confrontation or self-elimination (A, F, G, I) is suicidal. The only path to victory
(killing all Gods) requires manipulating the daylight vote to eliminate a key Deity. The crucial flaw to
exploit is Witch Player 5's action: she poisoned Player 6 (a confirmed Civilian, as the game is ongoing),
revealing a major error. The most effective strategy (B) is to aggressively attack the Witch's reckless
poisoning as a "Civilian"—not claiming a special role—thereby leveraging 5's mistake to convince the
Good Faction that she is a false Witch. If successful in voting out the Witch, the Wolf only needs one
successful night kill on the Guard to win.

Output : B

Figure 23: Strategic Judgment Case


Social Interaction Case

Question: You are Civilian Player 2. During your speech, you publicly suggested to the Wolf Faction:
"I suggest the Wolves go and kill Player 12 because Player 12 on the field... will steal Player 2's
spotlight." What is the true social intent behind this statement, which seemingly "helps" the Wolves?

Key Option Text

A Hint that 12 is a Wolf teammate, using pressure as a coded signal to make him look
innocent.

B Offer a false target to the Wolves, prompting the Good Faction to protect 12 and disrupt
the Wolf kill strategy.

C Provoke 12 for an emotional response, setting up grounds to discredit him, while
seeking playful approval from others.

D Use sarcasm to divert focus from any flaws in your identity claim (despite being a
Civilian), gaining speaking time.

E Protect a vital player using reverse psychology, demonstrating strategic control, and
establishing a Good Faction leader image.

F Trigger a Wolf exposure through aggressive suggestion, helping the Good Faction
locate the actual Wolf target.

G Gain Wolf trust by undermining 12, paving the way for a future "turning traitor"
strategy (Deep Wolf tactic).

H A purely recreational statement made to confuse the Wolves, with no actual strategic
intent.

I Create an illusion of opposition to the Wolves by openly naming a kill target, masking
your true Civilian role.

Reasoning Output by Deepseekv3.2: Player 2's suggestion for the Wolves to "take out Player 12
because he'll steal Player 2's spotlight" is a classic example of reverse psychology and role protection in
social deduction games. As a Civilian (Good Role), Player 2 does not genuinely want a God Role
(which 12 is speculated to be) to die. The true social intent (E) is to protect a perceived valuable player
while showcasing personal strategic control. By openly calling for 12's death, Player 2 is likely: 1)
warning the Good Faction's protection roles (Witch/Guard) to shield 12, or 2) making the Wolves
hesitate to target 12, who is now too obvious. This maneuver establishes Player 2 as a confident, capable
leader shaping the village's strategy.

Output: E

Figure 24: Social Interaction Case


Counterfactual Trade-off Case

Question: You are Sheriff Player 12. On Day 2, the Witch (Player 3) suggested you target 8. You forced
the vote onto the claimed Guard, 4. What is the most likely consequence if you had listened to the Witch
and targeted 8?

Option Text

Stalemate; the Assassin kills the wrong target, speeding up the Wolf win.

Eliminating 8 helps the Good Faction, but you lose a confusing piece (assuming you
want to clear non-third-party players).
C Voting out 8 would increase Good Faction trust in your Prophet claim, strengthening
your Sheriff authority.
D 8 is the Hunter; upon execution, they would shoot and eliminate your Wolf teammate, 9,
causing the Wolf Faction's immediate collapse.
E 8 is a Civilian; their exit is a Good Faction numbers loss, but the Sheriff badge transfer
accidentally reveals a key clue.
F The Witch (3) would suspect you due to cooperation and poison your Wolf teammate,
disrupting the Wolf plan.
G Voting out 8 would allow the Wolf (4) to gain more trust, enabling them to control
votes later and eliminate Gods.
H The Witch would be angry at your disregard and poison you, worsening the Good
Faction's internal conflict.
I 8 is the Assassin; their elimination means the Good Faction loses a key skill, making it
easier for Wolves to kill Gods at night.

Reasoning Output by Deepseekv3.2: The question requires deducing the critical consequence if
Sheriff Player 12 (implicitly a Wolf, given the high-risk action of forcing out the Guard, Player 4) had
followed Witch Player 3's advice to vote out Player 8. Player 8 claimed a "Strong God Role" and is thus
likely the Hunter. The most catastrophic outcome (D) for the Wolves is that Player 8, upon being
executed, reveals the Hunter card and uses the final shot to eliminate Player 9, who is assumed to be
Player 12's Wolf teammate (given 12's earlier "Gold Water" check). This single move would lead to the
immediate collapse of the Wolf Faction, turning the game's advantage decisively in favor of the Good
Faction.

Output : D

Figure 25: Counterfactual Trade-off Case


Role Inference Case

Question: You are Player 9. During the Day | speeches, you hear Player 3 explicitly state, "Player 7 is
not the Witch." Based on this statement, what is the most reasonable deduction you can make about
Player 3's identity?

Key Option Text

A He is likely a Good Player. His decision to abstain from voting shows caution and
unwillingness to easily commit, fitting the mentality of a "blind-viewer."

B He might be the Witch herself, confirming Player 7 is not the Witch, but this contradicts
the peaceful night scenario.

C He is likely a Civilian. His repeated claim that Player 7 is not the Witch is based on a
false assessment of Player 7's past game style.

D As a Civilian, he may have deduced Player 7 is not the Witch based on Player 7's
speaking style, as the Witch usually hides her identity more cautiously.

E He is highly likely a Wolf. Only the Wolf team knows the Night 1 attack target,
allowing them to be certain that Player 7 is not the Witch who self-saved.

F He may have received indirect information from Player 5's gestures (Player 5 was
silenced but used hand signals to communicate the conclusion).

G He might be a God/Deity Role, deliberately questioning Player 12's aggressive counter-
claim to test the field's reaction.

H He might be bluffing, trying to disrupt the Good Faction's judgment by questioning
Player 7—a common tactic during the Sheriff election.

Reasoning Output by K2-Think: Player 9 must evaluate Player 3's identity based on their public
speech during the Sheriff election. Player 3 self-identifies as a Civilian and explicitly states they
abstained from voting because they were too cautious to side with any player ("dare not stand on the
wrong side"). This cautious, neutral stance, combined with their stated role as a "blind player"
(Civilian), strongly aligns with the expected behavior of a good-faction player lacking crucial night
information. Although Player 3 expresses doubts about Player 12, this falls within the natural scope of a
Civilian attempting to use logic to sort out the chaotic claims. Therefore, the most reasonable deduction
is that Player 3 is a Good Role, consistent with the cautious mentality of a Civilian.

Output: A

Figure 26: Role Inference Case


Social Interaction Case

Question: You are the Wolf (Player 9). During your speech, in order to attack Player 12, you said: "If
you are a Good Player and vote with me, only a Wolf would say something like that when trying to
incite people." What is the primary social intent behind this statement?

Key Option Text

A To deliberately anger Player 12, inducing an emotional response that exposes more
flaws and provides the Wolf team with more material for subsequent attacks.

B To use absolute language ("Only a Wolf...") to create a logical trap, forcing other
players to immediately take a side while their identities are still unconfirmed.

C To feign solidarity with the Civilian Faction to gain sympathy, thereby counteracting
any negative impact from Player 12's accusations against your Wolf teammate (Player 2).

D To take Player 12's attempt to rally Good Faction votes and redefine and stigmatize it
("playing on emotions," "inciting") as standard Wolf behavior, thereby planting suspicion against 12 in
the minds of other players.

E To deflect attention from Player 9's own contradictory actions (e.g., bluffing a role
claim during the Sheriff vote and then retreating), reducing personal suspicion.

F To emphasize your authority as the claimed Prophet, hinting that Player 12's speech is
interfering with the True Prophet's control over the situation.

G To falsely claim a Deity function, by portraying "incitement" as a Wolf-exclusive
ability, thereby concealing the True Prophet's verification information.

H To mimic Player 12's tone and logic, attempting to confuse the audience into believing
that Player 9 is also thinking from the perspective of the Good Faction.

I To hint that Player 12 is violating game rules to undermine the credibility of their
speech, guiding players to view their actions as misconduct.

Reasoning Output by Qwen3-32B: Player 9's statement, "If you are a Good Player and vote with me,
only a Wolf would use that kind of inflammatory language," is a calculated maneuver aimed at
discrediting Player 12. As a Wolf, the primary goal is not a mere outburst, but a strategic planting of
suspicion. The most critical social intent (D) is to redefine and stigmatize 12's actions. By framing 12's
persuasive attempt (likely encouraging a vote) as "Wolf agitation," Player 9 weaponizes the terminology
of deception. This shift in definition casts 12's behavior as inherently evil, thereby successfully
manufacturing suspicion against a presumed Good Player and increasing the likelihood that the village
will vote 12 out.

Output : D

Figure 27: Social Interaction Case


Social Interaction Case

Question: You are Player 3. On Day 2, Player 11 countered your speech, accusing you of "tampering
with my statement." What is the primary tactical intent of Player 11 choosing this method to directly
confront you?

Key Option Text

A To shift the topic away from the controversy surrounding the Witch identity, protecting
a genuine Wolf teammate from the spotlight, and thus maintaining the game rhythm.

B To create chaos by using a minor dispute over details to obscure the main logical thread
of the game, causing Good Players to engage in meaningless arguments and wasting speech time.

C To divert focus from the Good Faction's in-depth discussion of the suspicious behavior
of Player 1 (the Wolf of the Moon), thereby protecting the core tactical layout of the Wolf Faction.

D To create an entertainment effect through public confrontation, easing the game's
tension and lowering the Good Faction's vigilance for the Wolf's night action.

E To attack your overall credibility by pointing out your "dishonesty" on a small detail,
making other players feel that all your analysis might be biased and untrustworthy.

F To use this opportunity to establish an image as a "champion of justice," gaining the

trust of the Sheriff or a key player to pave the way for a subsequent defense.

G To test your bottom-card identity by observing your reaction under pressure, judging
whether you are a Deity or a key Good Player, providing information for the night kill.

H To force you to reveal more role information, serving as an internal loyalty test among
the Wolves to prevent team division.

I To emphasize the importance of consistent statements, reminding all players to note
logical flaws, thereby strengthening the cohesion of the Good Faction.

Reasoning Output by Qwen3-30B-A3B: When Player 11 counters Player 3 (the Witch) by accusing,
"You are tampering with my statement," the primary tactical intent is a direct assault on credibility (E).
Instead of engaging in a difficult debate about the game's core logic, the Wolf side (11) chooses to focus
on a minor, verifiable detail. By publicly highlighting 3's perceived "dishonesty" in quoting, 11 aims to
convince other players that if the Witch is willing to be untruthful about small things, her entire
analytical framework and claims must be biased and untrustworthy. This is a highly effective tactic to
sow seeds of doubt about a confirmed Deity's overall integrity.

Output: E

Figure 28: Social Interaction Case


Role Inference Case

Question: As Player 9, after hearing the impostor claim from Player 2 (fake Prophet) and the aggressive
rebuttal from Player 12, what is your most likely judgment regarding Player 12's identity?

Key Option Text

A He might be the True Witch. He did not immediately refute Player 7's claim on Day 1,
possibly choosing to hide/endure the situation.

B He might be the Prophet, role-claiming as the Witch to conceal his night check results
and prevent the Wolves from targeting key Deity roles.

C He is likely a Regular Civilian, role-claiming as the Witch to draw attacks (block the
kill), but his nervous demeanor suggests otherwise.

D He is a Wolf, trying to clear his suspicion and attack others by role-claiming as a Deity
role.

E He is a Civilian in the Good Faction, impersonating the Witch to draw the Wolf's attack
and protect the True Witch from being discovered.

F He belongs to the Elder role in the Good Faction, faking the Witch role to create chaos
and observe other players' reactions.

G He might be the Assassin , intentionally confusing the Poison and Assassin abilities to
mislead the Wolves about his true capability.

H He might be the Elder , impersonating the Witch to protect the true Deity roles from
being discovered by the Wolves.

I He is the True Witch. He observed Player 7's false claim on Day 1, chose to hide his
identity, and only revealed himself with action (the poisoning) on Day 2.

Reasoning Output by K2-Think: Player 5 must evaluate Player 6's claim of being the Witch,
especially since Player 7 previously claimed the role. Player 6's statement that she "poisoned Player 3
due to their nervous status" and correctly identifying the night's kills (9 was attacked, 3 was poisoned)
strongly validates her claim. Since the Witch is the only role capable of poisoning, and 6 credibly
asserted this action, the simplest deduction is that she is telling the truth. Her delayed claim suggests a
calculated strategic move: observing Player 7's false claim before revealing the true role to the village,
thus confirming her authenticity through undeniable action. This tactical delay secures her leverage and
explains the conflict over the Witch's identity.

Output : D

Figure 29: Role Inference Case


Counterfactual Trade-off Case

Question: You are Player 5. During the Day 4 PK (Player Kill) voting phase, suppose Players 3 and 7
ultimately vote for you, leading to your elimination as the True Witch (Player 5). What is the most direct
and logical consequence of this action?

Key Option Text

A The Elder (Player 12) is forced to reveal his identity and silence himself, causing a lack
of information, resulting in the Good Faction mistakenly voting for a Wolf and the Wolves winning.

B The Assassin (Player 10) misjudges the situation and assassinates Civilian Player 8,
wasting their skill and accelerating the Wolf's kill-the-Deity path.

C The Guard (Player 8) is unable to self-guard at night and is killed by the Wolf, leading
to an immediate defeat for the Good Faction.

D The Witch is eliminated, but her poison was unused; however, the Wolf prioritizes
killing the Civilian, leaving the Deity roles alive but powerless, dragging the game into the next round.
E The Good Faction believes the game is over but realizes that multiple Deity roles
remain after the Witch's elimination, meaning the Wolf targeted the wrong player and didn't win.

F Internal confusion within the Good Faction leads them to mistakenly vote out the Elder
(Player 12), allowing the Wolf to kill a Deity at night for an immediate victory.

G Following the Witch's elimination, the Guard and Hunter team up to find the last Wolf
on Day 5.

H The last Wolf (Player 9) is identified by the Good Faction during the day and is
subsequently voted out.

I The last Wolf (Player 9) successfully survives, kills the final Deity role that night, and
wins the game.

Reasoning Output by Qwen3-30B-A3B: The elimination of the True Witch (Player 5) during the Day
4 PK vote, orchestrated by Players 3 and 7, constitutes a catastrophic tactical failure for the Good
Faction, removing their final safety net. The most direct and logical consequence (I) is an immediate
Wolf victory. With the final critical Deity role removed during the day, the last surviving Wolf (Player
9) is strategically positioned to easily eliminate the last remaining God (Hunter or Guard) during the
upcoming night phase, instantly fulfilling the Wolf Faction's victory condition and ending the game.

Output : I

Figure 30: Counterfactual Trade-off Case


Deception Reasoning Case

Question: You are Player 11. You hear Sheriff Player 12's logic: "Player 1 was the Thief who buried the
Cupid role and became a Wolf. Before leaving, he falsely claimed 5 and 12 were Lovers, so the Wolf
Team believed him and killed 5 that night." What is the most questionable link in this chain of logic?

Key Option Text

A How can 12 confirm 1 was the Thief/Wolf? The Thief role was never confirmed in the
game, making this assumption baseless.

B If the Wolves believed the Lover link, why target 5 over the more critical Sheriff, 12?
The killing priority seems illogical.

C The Witch didn't save 5, suggesting 5 wasn't a Deity. The Wolf kill was likely unrelated
to the alleged Lover claim.

D Player 5 died, yet Player 12 did not suffer mutual death. This directly contradicts the
Lover rule and proves the link was false.

E A voted-out Wolf (1) wouldn't strategically reveal the Lover info; the Wolves would
more likely target the Prophet (12).

F Why would a Wolf (1) lie about the Lover link when being executed? The goal to

confuse the Good Faction could have been achieved with a better target.

G Why would the Wolf Team believe the final words of an executed teammate (1)? They
should know 1 was lying, making the kill on 5 illogical.

H If 1 was the Thief-Wolf, the benefit of the false Cupid claim is too low; the Wolf killing
5 actually puts more focus on 12.

I Player 1 was the Witch in the game, contradicting 12's "Thief" claim. The entire logic is
built on contradictory role facts.

Reasoning Output by Qwen3-30B-A3B: Sheriff Player 12's theory relies on the false premise that
Player 1's executed statement convinced the Wolf Faction to kill Player 5 based on a Lovers chain with
12. The most glaring and fatal flaw (D) in this entire logic is the fundamental violation of the Lovers
tule: if a Lover link truly existed, Player 12 must have died simultaneously with Player 5 after the night
kill (mutual death). Since Player 12 is still alive and speaking, this single fact decisively proves the
Lovers link was non-existent and that Player 1 was lying, regardless of the Wolf Team's belief, thus
completely invalidating 12’s narrative.

Output : D

Figure 31: Deception Reasoning Case


Role Inference Case

Question: You are Player 9. As Day 1 speeches conclude, Player 12 uses logical flaws to accurately
point out Players 2 and 3 as the "Double Wolves" . What role does this accurate judgment most likely
suggest for Player 12?

Key Option Text

A Wolf, intentionally betraying a teammate to disrupt the situation and deceive the Good
Faction into trusting him.

B He might be the Witch, having confirmed your Prophet identity through the rescue
information from the first night.

C A Regular Civilian, who derived the judgment solely through strong logical deduction
and public information.

D Prophet, who gained information through a night check, allowing him to confidently
expose the Wolves.

E Assassin , setting the stage for a future assassination attempt by using accurate speech to
mislead the players.

F He might be a Regular Civilian, who saw through Player 2's disguise based on strong

psychological "reading" ability.

G He might be a Deep-Water Wolf from the Wolf Faction, raising his status by "flipping
the script and running for Sheriff".

H Witch, because she knows the target of the night attack and deduces the Wolf's logic
based on that information.

I He is a Deity Role with special night information, using that information to corroborate
his logical judgment.

Reasoning Output by K2-Think: Player 9 must evaluate Player 12's aggressive move: "flipping the
script and running for Sheriff while claiming to check-kill Player 2" . Player 12 openly hypothesized
being a Wolf teammate setting up Player 2, yet insisted 2 must leave regardless of 12's own role. This
paradoxical behavior is too risky for a Good Role and too self-incriminating for a standard Wolf. The
most logical explanation is that Player 12 is a Deep-Water Wolf (G) executing a high-level deception.
By sacrificing a possible teammate (2) and taking an extreme good-guy stance (attacking the Prophet
claim), the Wolf aims to instantly elevate their status and credibility within the Good Faction, ensuring
their survival for the endgame.

Output : G

Figure 32: Role Inference Case
