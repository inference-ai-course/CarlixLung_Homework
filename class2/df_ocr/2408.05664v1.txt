arXiv:2408.05664v1 [cs.CL] 11 Aug 2024

Training an NLP Scholar at a Small Liberal Arts College: A Backwards
Designed Course Proposal

Grusha Prasad*
Department of Computer Science
Colgate University
gprasad@colgate.edu

Abstract

The rapid growth in natural language process-
ing (NLP) over the last couple years has gen-
erated student interest and excitement in learn-
ing more about the field. In this paper, we
present two types of students that NLP courses
might want to train. First, an “NLP engineer”
who is able to flexibly design, build and apply
new technologies in NLP for a wide range of
tasks. Second, an “NLP scholar” who is able to
pose, refine and answer questions in NLP and
how it relates to the society, while also learning
to effectively communicate these answers to
a broader audience. While these two types of
skills are not mutually exclusive — NLP en-
gineers should be able to think critically, and
NLP scholars should be able to build systems —
we think that courses can differ in the balance
of these skills. As educators at Small Liberal
Arts Colleges, the strengths of our students and
our institution favors an approach that is better
suited to train NLP scholars. In this paper we
articulate what kinds of skills an NLP scholar
should have, and then adopt a backwards de-
sign to propose course components that can aid
the acquisition of these skills.

1 Introduction

Natural language processing (NLP) as an aca-
demic discipline has undergone a rapid expansion
in the last decade. Moreover, the feverishness
around emerging technologies from NLP influ-
ences what students want to learn in their CS educa-
tion. They want courses that train them in the tools
and techniques of both NLP and Machine Learning
more broadly. As educators, we are faced with an
exciting challenge — how do we effectively train
students to engage productively with our field?

In this paper, we differentiate between two kinds
of desired learning outcomes for an NLP course:
students should be able to (1) build and use new

“Equal contribution.

Forrest Davis”
Department of Computer Science
Colgate University
fdavis@colgate.edu

technologies, and (2) pose, refine and answer ques-
tions in NLP. While NLP courses might seek to
achieve both of these outcomes, the relative empha-
sis placed on each of them can differ across courses.
Some courses might place a larger emphasis on (1),
and present students with many opportunities to
program and develop substantial projects, often
with a focus on working with state-of-the-art ap-
proaches. In contrast, some courses might place a
larger empahsis on (2) and present students with
opportunities to engage critically with NLP tools
and techniques, often in the service of answering
questions and facing challenges that bridge disci-
plinary boundaries. We will refer to the type of
student the former course is designed to train as
an “NLP engineer’, and the student for the latter
course as an “NLP scholar’.

As educators in a Computer Science Department
at Colgate University (a Small Liberal Arts Col-
lege; SLAC), where the curriculum structure re-
quires students to take a wider range of non-CS
classes with the goal of cultivating critical thinking,
we argue that we are better placed to train NLP
scholars, than we are to train NLP engineers. In
this paper we present a design for an upper level
NLP course intended to train NLP scholars.

Using a backward design, we start by articulat-
ing what are the skills that an NLP scholar should
have, and propose a set of course principles/tenets
that can facilitate the acquisition of these skills.
Then, drawing on our previous experience teaching
Applied Machine Learning and Natural Langauge
Processing at our institution, we propose a course
that is structured around a capstone project. Con-
cretely, our paper makes three contributions:

1. We make explicit the link between the kinds
of students we want to train, the desired skills
we want the students to have, and the course
structure and content.

2. We propose a course structure designed


around a capstone experience which has six
different course components and relate each
of these components to concrete concepts and
skills they are designed to help with.

3. We present preliminary materials and evalua-
tion rubrics for several of these components.

The paper is structured as follows: First, in §2,
we take critical stock of our students, their experi-
ences, and their relation to computer science, ask-
ing ourselves how can we build on the strengths of
our students and our institution. Then, in §3, we
present skills we think an NLP scholar should have,
and the principles we think a course designed to
train NLP should adopt. Finally, in §4, we detail
our different course components and discuss how
they relate to our overall learning objectives.!

2 Teaching NLP as an upper level course
at a Small Liberal Arts College

Small Liberal Arts College (SLACs) are undergrad-
uate focused institutions that emphasize, among
other things, drawing connections between differ-
ent fields (King et al., 2007). Therefore, in these
institutions there is more emphasis on students tak-
ing classes in different disciplines. A consequence
of this is that CS departments in liberal arts colleges
tend to have fewer required courses and curricula
with flatter prerequisite structures (Teresco et al.,
2022). In this section we introduce the CS curricu-
lum structure and class room dynamics at Colgate
University, a SLAC in the US that we teach at.
Then, we outline how this has influenced our expe-
rience of teaching upper level elective courses in
Natural Language Processing and Machine Learn-
ing in previous semesters, and the changes we want
to make in our proposed course as a result of this
experience.

2.1 Required and elective courses at Colgate

Our curriculum has four required classes: one at
the 100 level (Introduction to Computing) and three
at the 200 level (Data Structures and Algorithms,
Introduction to Computer Systems, Discrete Struc-
tures). In addition students have to take four elec-
tives, with at least two classes at the 300-level or
above, and at least one 400-level course. All of

‘A github repository with links to final project templates
and our toolkit can be found here: https: //github.com/
forrestdavis/NLPScholar.

our 400-level courses require students to work on
“capstone” final projects.

Most electives have one or two of the required
200-level courses as prerequisites, with almost no
elective requiring another elective as a prerequisite.
As a consequence of this flat prerequisite struc-
ture, students can come into a 400-level elective
like NLP with more limited programming experi-
ence than their peers at institutions with a more
hierarchical prerequisite structure, and with likely
little to no prior background in relevant classes like
Machine Learning or Artificial Intelligence. The
challenge, therefore, is to design a class that sets up
our students up our students, who have relatively
limited programming experience and relevant con-
ceptual background, to work on meaningful cap-
stone projects.

2.2. Take-aways from previous iterations of
teaching NLP and Applied ML

In the past three semesters we’ve taught two sec-
tions each of NLP and Applied ML. In all of these
four classes, while all students were able to success-
fully produce capstone projects, we thought most
of these projects could have been more ambitious.
We’ve identified two factors that likely contributed
to the smaller scope of the projects: the order in
which earlier non-neural approaches vs. more con-
temporary neural network content was presented,
and students’ prior programming experience.

Order of introducing non-neural vs. neural
approaches In both Applied ML and NLP we
spent the first half of the semester working through
non-neural network approaches. For example, in
Applied ML we spent the first four weeks of the
semester working through evaluation metrics, gra-
dient descent, regression, SVM, decision trees and
random forests before introducing neural networks.
Similarly, in NLP we spent the first six to seven
weeks working through FSAs, context free gram-
mars and parsing, n-gram language models and
Bayesian classification before introducing neural
language models and transfer learning.

We adopted this approach because starting with
the non-neural approaches makes it easier to
ground the computational task we are trying to
solve with our ML and NLP models, and the met-
rics we use to evaluate these models. For example,
starting with FSAs and CFGs before n-gram mod-
els highlights the complexity of modeling structure
in language and how co-occurrence statistics can


capture some of this complexity. Then, introducing
classification tasks and evaluation metrics in the
context of n-gram models makes it possible to rea-
son about the relation between the tasks we want
to solve and the metrics we use to evaluate perfor-
mance on these tasks before trying to understand
the contemporary approaches that have been found
to be successful.

A consequence of this approach, however, was
that by the time we taught contemporary ap-
proaches applicable to a wider range of tasks, it
was too late for students to effectively incorporate
them into their capstone projects. Therefore, many
students chose to use non-neural approaches in
their final project which meant that they worked on
simpler tasks and/or were met with limited success
on their tasks. We propose to address this issue by
using a layered approach to introducing concepts
(§ 4.1) and designing labs early in the semester that
scaffold some of the aspects common to all projects
such as data pre-processing and generating plots

(§ 4.2).

Prior programming experience As discussed
earlier, since students can take NLP after having
taken as few as three CS courses, they can have
more limited programming experience than stu-
dents at other institutions with a more hierarchical
prerequisite structure that requires students to take
more CS classes before taking NLP. As a conse-
quence, we found that students struggled with im-
plementing more complex NLP or ML pipelines
using sparse starter code or links to existing code-
bases. We propose to address this by designing
a modular toolkit with the core components re-
quired for any NLP pipeline. The toolkit will be
designed such that students can use it off the shelf
from the beginning of class. Crucially, as the class
progresses and we want them to engage with im-
plementational details of different components, the
modular design makes it easy to ask them swap
out different components that they implement from
scratch (§ 4.3). We also plan to include a midterm
project that requires students to replicate previ-
ous work, which will give them practice working
with existing code bases and integrating it with the
toolkit(§ 4.4).

3 How to train an NLP Scholar

Given the liberal arts context that we are in, and the
background that students come in with, we think
that we are not very well placed to train NLP en-

gineers who can leave the class knowing how to
design and build new tools to tackle a wide range
of NLP tasks. Rather, we are better placed to train
NLP scholars who, when given a task or challenge,
can identify and apply existing tools to the chal-
lenge or task, while thinking critically about the
limitations of these tools, our methods to evaluate
them, and the societal context in which they are
used. Concretely, there are three skills we want the
NLP scholars we train to have.

S1: Describe language processing NLP schol-
ars should be able articulate clearly what compu-
tations underlie language processing and how the
systems we are building are approximating differ-
ent aspects of this at a computational and/or algo-
rithmic level. That is, they will develop an under-
standing for language as a computational system,
while setting aside concerns from psycho- or neuro-
linguistics of how exactly linguistic processing is
realized.

S2: Effectively using existing NLP tools NLP
scholars should recognize what existing NLP tools
are appropriate for solving different tasks or an-
swering different questions, and be able use these
tools to solve the tasks and answer the questions.

S3: Evaluate claims about NLP systems NLP
scholars should be able to identify the rhetorical
tools used to make arguments about the potential
and limitations of NLP systems, both in academic
papers and public media, and use evidence based
approaches to evaluate these arguments.

3.1 Tenets of our proposed course

Given these high level skills, we now describe some
of the tenets we are adopting in this course to facil-
itate the acquistion of these skills.

T1: Appreciate the complexity of language
Students should recognize the complexity involved
in language processing. We hope to accomplish this
by having students create and evaluate symbolic
computational models of language processing.

T2: Focus on multilingualism Students should
recognize the role that linguistic diversity and mul-
tilingualism plays in our understanding of language
as a phenomenon, and describe the scientific and
societal benefits of modeling languages other than
English.


Course Design Decision Scholar goal Tenet Capstone skill
Layered approach $1, $2 T1,T3,T4 -

Lab vs. Lecture Integration S1 T1,T2,T4 C2

Toolkit S2 T4 C2

Midterm replication 82 T3,T4,T5 Cl1,C2
Capstone project S1, 82, S3 T4 Cl, C2, C3
Society and Science Comm  S3 T5,T6 -

Table 1: Mapping our course design decisions to the tenets and desired skills in our proposed course.

T3: Recognize how NLP tasks abstract away
from the complexity Students should describe
how different NLP tasks (e.g., language modeling)
abstract away from the complexity of language
processing. They should also be able explain the
practical importance of this abstraction while artic-
ulating the limits that this abstraction poses on the
conclusions we can draw about language process-
ing from building and studying these models.

T4: Build NLP systems using existing tools
Students should be able to describe all of the com-
ponents of building NLP systems. They should
also be able to use existing code-bases and tools to
design systems that solve specific tasks or answer
specific questions.

TS5: Critically examine the role of benchmarks
Students should be able to articulate how and why
benchmarks shape NLP research and product de-
velopment, while reasoning about the limitations
of benchmarks.

T6: Critically examine the impact of hype cul-
ture on science and society Critically reason
about what kinds of results about NLP systems’
capabilities (or their lack thereof) get hyped and
why, while describing the impact that this hype can
have on society.

3.2 Capstone specific skills

We think that an effective way to achieve the skills
is to learn by doing, and we think that engaging in
a capstone project that requires students to answer
a concrete question or solve concrete task is an
ideal way to learn NLP. We describe the different
components of our proposed capstone project in
§ 4.5, but overall, we want students to be able to
do the following.

C1: Reading Scientific Papers Students should
be able to distill the hypotheses, methods, results

and conclusions from a scientific paper while crit-
ically evaluating whether the conclusions follow
from the results.

C2: Replicate prior work Students should be
able to follow the procedures described in a pa-
per to replicate prior work, and reason about what
counts as a successful replication.

C3: Engage in peer-review Students should be
able to give constructive criticism in a peer-review
setup, as well as incorporate constructive feedback
to improve their work.

4 Course Components and Assessments

Following our backwards design, we intentionally
related course components and assessments to the
skills we want our students to acquire, the tenets
underlying our course principles, and the skills we
want demonstrated in a capstone project (see § 3).
We summarize these connections in Table 1.

4.1 Layered approach to introducing concepts

In the previous iterations, we adopted a largely
sequential approach to introducing concepts. We
found that this resulted in a bit of a fragmented
course structure. In this iteration we want to adopt
a layered approach that introduces core concepts at
multiple levels of abstraction at different points in
the course. To accomplish this, we plan to start the
class by introducing the NLP pipeline at a very high
level, and then un-blackboxing different parts as
the semester progresses. This un-blackboxing pro-
cess applies to both computational concepts (e.g.,
when students are asked to write context free gram-
mars to engage with the complexity of language) as
well as to practical implementational level details
(e.g., when students are asked to implement tok-
enization). We describe below how we plan to use
the lab-course integration and the toolkit as tools
for this layered approach.


4.2 Labs-Lecture intergration

Following other natural sciences, many courses in
our department have an accompanying lab. These
labs serve to provide supplemental practice with
concepts covered in the course and concrete hands-
on time for programming. Within the context of our
NLP course, we intentionally designed labs for two
broad purposes: preparing students for midterm
projects (see § 4.4 for more details on this project)
and to more deeply explore content not fully cov-
ered in lecture.

In designing lab content, we are motivated by
a tension we observed in earlier versions of this
course and related courses (e.g., Machine Learn-
ing): student final projects tend to draw on current
trends in the field, but knowledge of the field’s foun-
dations help contextualize modern techniques. As
a result, while earlier material is critical, time spent
on developing intuitions for traditional concepts
(e.g., context-free grammars) reduces the scope of
what students are capable of accomplishing in their
final projects. For example, in earlier versions of
NLP, there were only around 2 weeks between the
introduction of transformers and the completion of
earlier phases of the final project.

Labs resolve this tension by providing time out-
side of lecture for developing skills critical to suc-
cess in their final projects. This includes, a re-
fresher on Python (the middle of our curriculum
is taught in Java and C) and labs targeting data
processing, hypothesis generation, conducting ex-
periments, plotting, and interpreting results.

In their additional role as an opportunity for
deeper engagement with lecture content, labs take
the form of hands-on exploration, similar to the
“scaffolded discovery’ advocated in Schofield et al.
(2021). Concretely, consider context-free gram-
mars. In lecture, they will be discussed at a con-
ceptual level (what are they, how do they relate to
language, what are ways we may use them, etc.). In
lab, students will be asked to write actual context-
free grammars for fragments of a language, follow-
ing in the style of (Eisner and Smith, 2008). In this
capacity, labs allow us to decide when to abstract
and when to go a bit deeper, without relying solely
on lecture time.

4.3 Toolkit

In developing the course, we had to decide what to
blackbox and when. A variety of factors influenced
our thinking on this. We want students to develop

Gather
Probabilities/Surprisals

Data Pre-
processing

Representational Probing

Finetuning

Precision
Recall

BLEU
Generation “

Figure 1: Sketch of the core components of the toolkit

a high-level understanding of the core parts of an
NLP pipeline and how they relate to one another,
while also leaving space for deeper dives into spe-
cific aspects in the form of student implementations.
We believe, following our course principles, that
students learn best by doing. In this case, students
gain a fuller understanding of the NLP pipeline if
they can use and explore a working implementa-
tion. Additionally, they benefit from implementing
parts of this pipeline. Our toolkit seeks to balance
these two needs by being modular.

As sketched in Figure 1, there are 4 basic com-
ponents to our toolkit: data pre-processing, model,
experiments, and handling the output (plotting and
evaluation metrics). Data pre-processing includes
both basic text processing (e.g., reading different
file types, text normalization) and tokenization. Fol-
lowing an earlier implementation of this toolkit, the
model components is comprised of a parent class
with basic methods (e.g., getting the logits from
a model, aligning logits to words, getting inter-
mediate output) that allow for a shared structure
across disparate models (including transformers
and RNNs/LSTMs). The experiment portion of the
toolkit facilitates basic approaches in the field (e.g.,
probing, targeted syntactic evaluations). Finally,
the output of experiments interacts with both met-
rics (e.g., Fl) and also a plotting interface. Each of
these components can be supplemented with stu-
dent implementations. For example, a lab can fo-
cus on building a different type of tokenizer, which
students can add to the pipeline to extend the capa-
bilities of the existing toolkit.

In designing the toolkit we are mindful of two
things: we want the code to be readable and ap-
proachable for students to dig into while also build-
ing in a level of abstraction that facilitates using the
toolkit without fully understanding the components
early in the semester. Like in earlier versions of


the toolkit we plan to abstract from the implemen-
tation via basic plain text files specifying exper-
imental material (e.g., grammatical and ungram-
matical sentences) and a config file that specified
pre-processing details (e.g., including puncutation)
and the desired models to run the experiment on.
Additionally, we will build on top of existing, and
widely adopted NLP toolkits like NLTK and Hug-
gingFace (especially for models), so that students
gain some familiarity with existing industry tools.

4.4 Midterm

To scaffold the final project, the middle of the
semester culminates in a midterm project, rather
than a midterm exam. The intention of this project
is to provide students an opportunity to concretely
apply skills relevant to the final paper, but in a more
structured format. Concretely, we identify 4 skills
that we want to ensure students can apply going
into the final project:

1. Working with existing code and/or libraries

2. Hypothesis generation and operationalizing a
question

3. Interpreting and synthesizing results

4. Science communication

Development of these skills are facilitated in
lab and in the production of their midterm paper.
We believe it is challenging, if at the end of the
semester, students are faced with two tasks, i) ap-
ply new knowledge in a new format (e.g., many
other computer science courses in our department
lack a final paper), and ii) develop a novel idea
that excites them. The midterm project seeks to di-
vorce these (to some extent). Rather than producing
novel work, students are tasked with replicating ex-
isting work. This pre-existing structure helps guide
them in the development and application of course
concepts. In closely studying an existing paper,
they gain familiarity with how researchers frame
questions, how to synthesize results in a way that
is digestible to the reader, and how to work with
existing code and data.

In the following sections, we discuss how we
approached selecting papers to serve as the base of
student replications, and how we scaffold the skills
necessary to complete it.

Types of papers and why In choosing papers,
we focused on the core themes of the course and

looked for papers that were (often) short, con-
tained existing code, exposed a key methodology,
and would facilitate good discussions. We settled
on 5 themes: Basic Methodology, Interpretability,
Experimental Design, Cross-Linguistic or Multi-
linguality, What’s in the Data. Example papers
are provided in Table 2. The Basic Methodology
theme serves a particular function in scaffolding the
midterm project, which we return to below. For the
others, we wanted to highlight different approaches
for evaluating models (Interpretability), how care-
fully created experiments can expose flaws in sci-
entific reasoning (Experimental Design), how we
should think broadly about language (cf. the “Ben-
der Rule’, Bender 2019; Multilinguality), and how
we should think critically about what is in our train-
ing data (What’s in the Data). These categories are
not exclusive — papers can fall in more than one
category. For example, Deas et al. (2023) invites
discussion of both the contents of our training and
evaluation data, but also, highlights the diversity of
what is meant by English.

Scaffolds for writing the paper In writing their
midterm papers, students will demonstrate their
knowledge of creating research: going from data to
question to experiments to results, and finally a con-
clusion. To scaffold the acquisition of these skills,
we are relying on early labs. Students will have labs
that guide them throw basic research pipelines like
formatting data to work with a model. Concretely,
we draw on papers from the Basic Methodology
as an in-lab replication assignment. Additionally,
students will present their midterm projects to us
for feedback, facilitating practice with scientific
communication.

4.5 Final

The course culminates in a capstone project in the
form of a final project, done in small groups with in-
dividual final papers. We aim through the semester
to have assessments that help ensure the students
remain on track. Concretely, over the course of a
few semesters of piloting this, we have settled on 5
phases:

. Individual Ideation and Group Formation

. Feedback Discussion

1

2

3. Pilot Presentation
4. Poster Presentation
5

. Final Paper


Theme Example 1

Example 2

Basic Methodology

Newman et al. (2021)

Warstadt and Bowman (2020)

Interpretability

Clark et al. (2019)

Hewitt and Manning (2019)

Experimental Design

McCoy et al. (2020)

Hewitt and Liang (2019)

Cross-Linguistic

Ravfogel et al. (2019)

Mueller et al. (2020)

What’s in the Data

Yedetore et al. (2023)

Deas et al. (2023)

Table 2: Paper themes and examples. For the midterm paper, students will replicate papers from any category but

Basic Methodology, which are used in lab.

We discuss the motivation and content of each
phase below.

Ideation to Group Project In driving towards a
group project, we want an opportunity for students
to externalize their own interests and map it to the
course content. We have an explicit assessment to
target this. Students submit an individual project
proposal that outlines their idea, a concrete link
to a relevant dataset or a description of how they
would make one, a question operationalized with an
experiment or task, and their availability to work in
the semester (so that groups can take into account
working style/time). See Appendix A for more
details. Students are asked to read all the individual
project ideas and to submit a ranking of projects
they would like to work on. Rather than just having
students form groups immediately, we believe this
facilitates better groups by allowing students to
find overlapping interest with people they might
not already know.

Feedback Discussion After forming groups, stu-
dents are asked to put together a project proposal
(similar to their individual project proposals). We
review these and then have meetings with each
group to help flesh out any limitations in their cur-
rent proposal. This often takes the form of helping
them make their project either more ambitious, or
conversely, scaled down to something manageable
in the remaining time of the semester. As opposed
to written feedback, we find these conversations
productive and useful in helping students articu-
late a project that is appropriate and exciting for
them. In the end of the discussion, we establish
what should comprise their pilot presentation.

Pilot As the class shifts to focusing on final
projects, we ensure that there is time in labs for
groups to make progress. We formalize this desire
for making progress prior to the end of the semester

with a pilot presentation. Groups are meant to
implement the core part of their final project and
present it to the class in the form of a short presenta-
tion. By demonstrating that their project works, at
least in a limited capacity, groups can expose any is-
sues that might arise in conducting their work (e.g.,
the data are not good, the model they are drawing
on performs more poorly than expected). Addi-
tionally, students get practice communicating their
results to a more general audience (each other). For
each presentation, students submit feedback forms
(see Appendix B) and drive the questions in the
discussion period after the presentation.

Poster Presentation In the final week of the
semester, students create and present posters about
their final project. This ensures that they have a
nearly complete final project prior to working on
the final paper during the examination period. In
the past, we have had students give feedback for
each poster using a form. We have decided to have
students write anonymized reviews of a subset of
posters. This, we hope, encourages them to engage
more substantially with the poster and helps teach
them the process of science (namely, peer-review).
We plan to use a reviewing form similar to that
of used by ARR (e.g., highlighting the strengths,
weaknesses, and key takeaway). A copy of our
poster template, rubric, and prior feedback form is
given in Appendix B.

Final paper Finally, each student is asked to in-
dividually write a final paper, following a template
included in Appendix C. The paper mimics a typi-
cal research paper in NLP (drawing on ACL’s style
guide). Given the group nature of the other parts of
the final project, we want to retain some way to as-
sess the individual contributions of group members.
We have found that final papers in the past have
exposed both exciting differences in interpreting
results but also highlight the varied contributions


of group members.

4.6 Society Reflection

A core aim of this course is not only to introduce
students to NLP but also produce critically engaged
practitioners. The final component of the course is
meant to scaffold this. In their paper on incorporat-
ing reflection on social issues in CS classes, Davis
and Walker (2011) highlighted, among other things,
the following two challenges: First, students are
good at repeating platitudes (e.g., “it is important to
address biases in models’), but find it challenging
to critically think about the issues in practice. Sec-
ond, formally assessing students’ ability to engage
with these issues can be challenging.

In our course, we hope to address these chal-
lenges by requiring students to write a 1-2 page
“Society reflection” paper. For this paper, students
will be asked to find and read a recent news article
that discusses advances in NLP and asked to evalu-
ate the arguments the article makes (implicitly or
explicitly) about the societal implications of these
advances. Since students are asked to evaluate spe-
cific arguments, merely repeating platitudes will
not be sufficient; they will need to draw on their
knowledge of the tools and techniques in NLP to
examine the central claims, while also reflecting
on the rhetorical reasons for making those claims.
Students’ ability to engage with the societal issues
as NLP practitioners will be assessed based on their
ability to identify the core claims and the soundness
of their arguments when evaluating these claims.

5 Discussion

In this paper we presented two types of students
NLP courses might want to train — NLP engineers
and NLP scholars — and used a backwards design
approach to design a capstone focused upper-level
course to train the latter type of student. We started
by outlining three skills we would like the NLP
scholars we train to have. Then, we described six
tenets and three capstone specific skills that shaped
the content, organization and assessments in the
course. Finally we described these different com-
ponents of the course. Having taught two sections
of NLP and two sections of Machine Learning in
the last two years, our proposed course represents
the synthesis of many conversations, both among
ourselves, but also with other educators and with
our students.

In designing the upperlevel course, we had to

resolve a tension between the knowledge and criti-
cal thinking skills we wanted our students, as NLP
scholars, to have and our desire to develop a course
that fostered substantial and exciting undergradu-
ate NLP projects. To foster a deeper understanding
and appreciation of what NLP is about, we thought
it was important to spend time on older material
(especially since we could not assume any prior
background in AI or NLP). However, to prepare
our students for successful and exciting capstone
projects we thought it was important to introduce
them to newer tools and techniques early on.

We resolved this tension in our proposed course
in the following ways.

1. We adopted a layered approach to introduc-
ing concepts where we introduce the NLP
pipeline at an abstract level in the beginning,
and peel away the layers as the course pro-
gresses. To accomplish this, we proposed a
modular toolkit and and intentional course-lab
integration.

2. We proposed a midterm replication project
that provides students with an opportunity to
work on larger scale projects with existing
code-bases, while also giving them practice
with critical thinking (e.g., reasoning about
what counts as a successful replication) and
science communication.

3. We proposed a highly scaffolded approach
to their final capstone project involving five
stages. We piloted this five-stage process in
two sections of Applied Machine Learning
this semester, and found that on average the
student projects were more ambitious and suc-
cessful. We include course materials from this
pilot in the appendix.

NLP Scholar in Different Institutions and Pro-
grams While the course components we pro-
posed are specifically designed with the constraints
of our institution and program in mind, we believe
aspects are applicable to training NLP Scholars in
other contexts. Specifically, there are two educa-
tional environments that we would like to highlight:
a computer science department at a research heavy
(i.e., R1) university and other departments like lin-
guistics or psychology where a computational lin-
guistics course would be relevant. For both, the
toolkit and societal reflection seem appropriate and
useful.


The final project, as we have constructed it, re-
quires ample one-on-one time with students, both
for formal components of the grade (e.g., the
project feedback and in-class presentations), but
also, for helping student groups progress in their
project (e.g., debugging issues). We believe these
would be difficult to scale in R1 settings without
relying heavily on TAs and restructuring the pro-
gression of the final projects. However, we believe
the final project structure, and in particular, the
replication component, would be applicable across
disciplines. Having linguistics students replicate
work on targeted syntactic evaluations, for exam-
ple, is a good way of taking a common practice
in linguistics (the construction of minimal pairs)
and applying it to a computational domain. On
the other hand, we think the lab/lecture structure is
well suited to R1 computer science students, while
it may pose challenges in being ported to other
disciplinary backgrounds. We are assuming that
students in our class need more time to the concepts
than on the aspects of programming.

Usefulness of the NLP Engineer vs. NLP Scholar
distinction Even if the goal of many NLP courses
might be to train students who are a combination
of an NLP Engineer and Scholar, we think the dis-
tinction can be useful when faced with conflicting
goals for the course. For example, Schofield et al.
(2021) advocated for a discovery based approach
to teaching NLP, but concluded that students might
need more guidance on/practice with aspects like
File I/O, loading and saving data, manipulating
numpy and spaCy objects etc. Here, the NLP En-
gineer vs. Scholar distinction can help instructors
decide what components should students produc-
tively struggle with, and which components they
are better of getting more structured guidance on.
If the goal of the course or the specific assign-
ment is to equip students with the ability to build
something like an NLP Engineer, then it might be
helpful for students to productively struggle with
components like File/IO with minimal guidance. If
on the other hand, the goal is to have students use
a tool to answer a question or explore a topic like
an NLP Scholar, then students might be better off
having more guidance on components of the assign-
ment like File I/O, so they can devote more time to
the exploration and analysis. This line of reasoning
is applicable even for graduate level NLP courses
or undergraduate courses where students come in
with much stronger programming experience, and

are unlikely to be challenged by programming as-
pects like File I/O — in these courses, instructors
will need to decide the extent to which other im-
plementational details such as GPU parallelization
should be scaffolded.

Conclusion We differentiated between two types
of students — NLP scholar and NLP engineer —
and proposed an upper level course, with a cap-
stone experience, designed to train the latter type
of student. Ultimately, we hope that articulating
the goals for an NLP scholar, and tying them back
to specific components in our proposed course can
facilitate broader discussions about what kinds of
NLP practitioners we want to train, why, and how
best to train them.

Acknowledgments

We would like to thank our colleagues in computer
science and our students over the years for help-
ing shape the course. In particular the students in
NLP at Colgate in Spring 2023 and Fall 2023, the
students in Applied Machine Learning at Colgate
in Spring 2024, and the students in the Methods
in Computational Linguistics seminar at MIT in
Fall 2022. Additionally, we would like to thank our
anonymous reviewers for their helpful comments
and suggestions.

References

Emily M Bender. 2019. The# benderrule: On naming
the languages we study and why it matters.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D. Manning. 2019. What does BERT
look at? an analysis of BERT’s attention. In Pro-
ceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP,
pages 276-286, Florence, Italy. Association for Com-
putational Linguistics.

Janet Davis and Henry M Walker. 2011. Incorporating
social issues of computing in a small, liberal arts col-
lege: a case study. In Proceedings of the 42nd ACM
technical symposium on Computer science education,

pages 69-74.

Nicholas Deas, Jessica Grieser, Shana Kleiner,
Desmond Patton, Elsbeth Turcan, and Kathleen McK-
eown. 2023. Evaluation of African American lan-
guage bias in natural language generation. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing, pages 6805-
6824, Singapore. Association for Computational Lin-
guistics.


Jason Eisner and Noah A. Smith. 2008. Competitive
grammar writing. In Proceedings of the Third Work-
shop on Issues in Teaching Computational Linguis-
tics, pages 97-105, Columbus, Ohio. Association for
Computational Linguistics.

John Hewitt and Percy Liang. 2019. Designing and in-
terpreting probes with control tasks. In Proceedings
of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 2733-2743, Hong Kong,
China. Association for Computational Linguistics.

John Hewitt and Christopher D. Manning. 2019. A
structural probe for finding syntax in word represen-
tations. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume I (Long and Short Papers), pages
4129-4138, Minneapolis, Minnesota. Association for
Computational Linguistics.

Patricia M King, Marie Kendall Brown, Nathan K Lind-
say, and Jones R VanHecke. 2007. Liberal arts stu-
dent learning outcomes: An integrated approach.
About Campus, 12(4):2-9.

R. Thomas McCoy, Junghyun Min, and Tal Linzen.
2020. BERTs of a feather do not generalize together:
Large variability in generalization across models with
similar test set performance. In Proceedings of the
Third BlackboxNLP Workshop on Analyzing and In-
terpreting Neural Networks for NLP, pages 217-227,
Online. Association for Computational Linguistics.

Aaron Mueller, Garrett Nicolai, Panayiota Petrou-
Zeniou, Natalia Talmina, and Tal Linzen. 2020.
Cross-linguistic syntactic evaluation of word predic-
tion models. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 5523-5539, Online. Association for Computa-
tional Linguistics.

Benjamin Newman, Kai-Siang Ang, Julia Gong, and
John Hewitt. 2021. Refining targeted syntactic evalu-
ation of language models. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 3710-3723, Online.
Association for Computational Linguistics.

Shauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.
Studying the inductive biases of RNNs with synthetic
variations of natural languages. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume I (Long and
Short Papers), pages 3532-3542, Minneapolis, Min-
nesota. Association for Computational Linguistics.

Alexandra Schofield, Richard Wicentowski, and Julie
Medero. 2021. Learning how to learn NLP: Devel-
oping introductory concepts through scaffolded dis-
covery. In Proceedings of the Fifth Workshop on

Teaching NLP, pages 131-137, Online. Association
for Computational Linguistics.

James D Teresco, Andrea Tartaro, Amanda Holland-
Minkley, Grant Braught, Jakob Barnard, and Douglas
Baldwin. 2022. Cs curricular innovations with a
liberal arts philosophy. In Proceedings of the 53rd
ACM Technical Symposium on Computer Science
Education-Volume 1, pages 537-543.

Alex Warstadt and Samuel R. Bowman. 2020. Lin-
guistic analysis of pretrained sentence encoders with
acceptability judgments.

Aditya Yedetore, Tal Linzen, Robert Frank, and
R. Thomas McCoy. 2023. How poor is the stim-
ulus? evaluating hierarchical generalization in neu-
ral networks trained on child-directed speech. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 9370-9393, Toronto, Canada.
Association for Computational Linguistics.


A Project Proposal and Pilot Presentation Templates

Students are asked to make two project proposals. One that represents their individual idea and interest,
and then later, a proposal for a group project. A verison of the proposal template is copied below.

A.1 Proposal Template

A.1.1  Introduction/ Motivation

¢ What is the big picture question you are trying to answer/ problem you are trying to solve? Why is
this an interesting and worthwhile question to answer/ problem to work on?

¢ What is the specific question you will pursue? Why? (Note: you will need to pick something that is
feasible to answer in 3-4 weeks)

¢ What are all the possible outcomes of your project? Do you think one (or few) of these outcomes are
more likely than others? Why?

A.1.2. Background/Literature review

Find at least three papers related to your project. For each project write a paragraph or two summarizing:
¢ What were the goals of that paper? How is it related to your project?
¢ What methods did the paper use?
¢ What were the conclusions?

Google scholar (or other comparable database search) is a better place to look than standard search: you
are less likely to find blogposts with unverified content on Google scholar. Note, on Google Scholar you
might see a lot of preprints from arxiv, even if they were also published elsewhere. It is good practice to
try and find the most recent version of a paper. The general rule of thumb you should use: peer reviewed
published papers are more credible than preprints.

A.1.3 Planned methods

Describe what methods you plan to use to address your question and describe how your methods compare
to prior work you describe in the background section.

¢ What dataset will you use? Is it already available or do you have to create it?
¢ What model(s)/approaches will you use?

¢ How will you evaluate your models? What counts as success? What conclusions can you draw (if
any) if you get negative results?

A.1.4 Proposed timeline/division of labor
Breakdown your project into separate tasks. For each task, list the expected amount of time, who plans to
work on it and when they expect to complete it by. For this part, it might be most straightforward to fill
out a table following the format below.

One of your tasks should be “Prepare for pilot result presentation” and your timeline should
highlight what work you hope to accomplish before the pilot results.

Task Time required Expected date of completion Person



A.2 Pilot

After discussions between us and each group, students work towards a final project pilot presentation.
This presentation demonstrates early progress on their project and showcases their question and opera-
tionalization. It takes the form of a 5-8 minute presentation with time for questions from the other students.
During the presentations, students are asked to provide:

¢ Feedback about the content (question, methods, result interpretation, conclusion etc)

¢ Feedback about the presentation (framing, visuals, oral presentation etc)

B_ Poster Presentation Templates

At the conclusion of the semesters, groups create and present a final poster. Details on the poster template,
grading rubric, and student feedback from are provided below.
B.1_ Poster Template

We provide students with a final poster template (given in Figure 2) in the form of a google slide and are
asked to make use of the on-campus printing services to gather their physical poster.

Title of your project goes here

thor 1, Author 2, and Author 3

Experimental setup Summary + Conclusion

What is the big picture question you are trying to answer? Important methodological details. What are the main things you want the audience to take away?
How does this relate to the specific question you are trying to Rae * how theyweresspllt Did thingstolasexpected?
answer? i ‘ sae

Evaluation metrics What are some limitations?

Can you make these questions clearer with one or two simple

examples? Make sure to cite prior work! (e.g. Don’t just say BERT, cite the paper

that introduced BERT).
What are your hypotheses?

Why is answering this question interesting?

Is there a useful graphic that can help?

What background information is crucial to understanding this Main results in the paper. List two or three concrete next steps and why they are interesting!
project?
Ideally you should not be talking about more than two or three
If you are introducing a new model type, dataset, methodology etc, main results. If you have more than that, prioritize!
this might be a good place to introduce the main idea/ logic.
This section should have the least amount of text. Use tables or
Use graphics and visualization to the extent possible! graphs wherever possible!

References

Use APA format. But if you need to shorten some journal or
conference names, thats fine in a poster!

Feel free to also reduce the font size if required.

Figure 2: Final poster template

B.2. Poster Rubric

In an effort to balance listening to and grading the presentations, we settled on a brief rubric in Table 3.


Extractable from poster? (0: missing 1: yes 2: clear)

Introduction 0 1 2
Background 0 1 2
Experimental Setup 0 1 2
Results 0 1 2
Summary + Conclusion 0 1 2
Future work 0 1 2
Clarity of presentation (0: bad 1: ok 2: exceeds expectations)

Division of labor (ok: majority of team contributes) 0 1 2
Oral pitch (ok: leave with question, operationalization, takeaways) 0 1 2
Timing (ok: <= 5 mins) 0 1 2
Visuals (ok: informative, legible) 0 1 2
Answering Questions (ok: understand the question ask) 0 1 2

Table 3: Final poster rubric

B.3 Student Feedback on Posters

When not presenting, students are asked to engage with the other presentations and provide feedback.
In prior courses, this feedback too the form of filling out a paper form. Concretely, they were asked to
provide (at least) one bit of constructive feedback along each of these dimensions:

¢ Content

¢ Visualizations

¢ Argument presentation
¢ Future direction

C_ Final Paper Templates

Using a latex template styled on ACL’s submission template, students are asked to write an individual
final paper. Some guidelines we gave to students are provided below.

C.1 Final Paper Guidelines

Your final paper should have the following sections.

C.1.1 Introduction/Motivation

¢ What is the big picture question you are trying to answer/ problem you are trying to solve? Why is
this interesting?

¢ What is the specific question you are pursuing? Why?

C.1.2. Background/Literature review

Find at least three papers related to your project. For each paper write a paragraph or two summarizing:
¢ What were the goals of that paper? How is it related to your project?
¢ What methods did the paper use?
¢ What were the conclusions?

Note, on Google Scholar you might see a lot of preprints from arxiv, even if they were also published
elsewhere. It is good practice to try and find the most recent published version (i.e. conference version) of
a paper.


C.1.3. Methods

Describe what methods you used to address your question and describe how your methods compare to
prior work you describe in the background section.

¢ What dataset did you use? Was it already available or did you have to create it?
¢ What model(s)/approaches did you use?

¢ How did you evaluate your models? What counted as success? What conclusions can you draw (if
any) if you got negative results?

¢ Include a link to a repository with your code (or similar) in the paper

C.1.4_ Results

Describe what you found in your work. Put your results in a figure or a table that helps the reader
synthesize what you’ve done.

C.1.5 Discussion
How does your work answer your question? What are the implications of your results? What are ways the
work could be extended? What are the limitations of your work?
