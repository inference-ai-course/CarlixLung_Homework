2306.16367v1 [cs.LG] 28 Jun 2023

arXiv

Multi-Site Clinical Federated Learning using
Recursive and Attentive Models and NVFlare

Won Joon Yun!, Samuel Kim?, and Joongheon Kim!

! School of Electrical Engineering, Korea University, Seoul, Republic of Korea
2 Cipherome Inc., San Jose, California, USA

Abstract—The prodigious growth of digital health data has
precipitated a mounting interest in harnessing machine learning
methodologies, such as natural language processing (NLP), to
scrutinize medical records, clinical notes, and other text-based
health information. Although NLP techniques have exhibited
substantial potential in augmenting patient care and informing
clinical decision-making, data privacy and adherence to regula-
tions persist as critical concerns. Federated learning (FL) emerges
as a viable solution, empowering multiple organizations to train
machine learning models collaboratively without disseminating
raw data. This paper proffers a pragmatic approach to medical
NLP by amalgamating FL, NLP models, and the NVFlare
framework, developed by NVIDIA. We introduce two exemplary
NLP models, the Long-Short Term Memory (LSTM)-based model
and Bidirectional Encoder Representations from Transformers
(BERT), which have demonstrated exceptional performance in
comprehending context and semantics within medical data. This
paper encompasses the development of an integrated framework
that addresses data privacy and regulatory compliance challenges
while maintaining elevated accuracy and performance, incorpo-
rating BERT pretraining, and comprehensively substantiating the
efficacy of the proposed approach.

Index Terms—Federated Learning, Clinical Data, Language
Model

I. INTRODUCTION

The burgeoning expansion of digital health data has in-
cited a mounting interest in employing machine learning
methodologies, such as natural language processing (NLP),
to scrutinize medical records, clinical notes, and other text-
based health information. NLP techniques have exhibited con-
siderable potential in augmenting patient care and informing
clinical decision-making [[]]. However, the sensitive nature of
health data and the imperative for adherence to regulations,
such as the Health Insurance Portability and Accountability
Act (HIPAA), pose considerable challenges in terms of data
privacy and security. Federated learning (FL) constitutes a
distributed machine learning paradigm that empowers mul-
tiple organizations to collaboratively train a model without
sharing raw data, thereby ensuring data privacy and legal
compliance [2], [3], [4]. FL facilitates the cooperative training
of a shared machine learning model across multiple clinics
while safeguarding patient data privacy and complying with
regulations. Furthermore, the collaborative training using data
from diverse clinics promotes the development of more ro-
bust and accurate models, which may potentially generalize
better to unseen data, culminating in enhanced diagnostics.

Additionally, FL obviates data silos that frequently arise in
multi-site clinics, as it allows institutions to learn from one
another’s data without infringing upon privacy regulations.
This collaborative approach can foster increased knowledge
sharing and improved patient outcomes. As a result, FL can
address real-world data discrepancies, such as varying data
quality, data distribution, and data labeling practices across
different clinics, rendering the shared model more applicable
to heterogeneous clinical settings and populations.

Motivated by the indispensability of FL, this paper proffers
a pragmatic approach to medical NLP by amalgamating FL,
NLP models, and NVFlare. We introduce two exemplary NLP
models: the Long-Short Term Memory (LSTM)-based model
and Bidirectional Encoder Representations from Transformers
(BERT) [5]. NVFlare, devised by NVIDIA, is a versatile and
scalable framework for FL that delivers system reliability,
privacy preservation, and optimal resource allocation [6]. By
integrating these components, the proposed framework tackles
the challenges of data privacy and regulatory compliance
whilst maintaining elevated accuracy and performance.

The salient contributions are tri-folded: First, this paper
integrates the FL framework that addresses the challenges of
data privacy and regulatory compliance while maintaining high
accuracy and performance in medical NLP; Second, this paper
incorporates not only a general training method but also BERT
pretraining, which broadens the applicability of the proposed
framework; Lastly, this paper comprehensively demonstrates
the efficacy of the proposed approach, which transitions from
traditional LSTM-based models to BERT, thereby establishing
the practicality of the reference framework.

This paper is structured as follows: Sec. |II] delineates the
related work encompassing FL, NLP models, and NVFlare;
Sec. [IIT] expounds upon the methodology for integrating these
technologies; Sec. [IV] deliberates the results and performance
of the proposed approach; and Sec. |V| concludes the paper
with a discourse on future research directions.

II. RELATED WORK
A. Federated Learning with Medical NLP Models

FL is a distributed machine learning paradigm that allows
multiple organizations to collaboratively train a model without
sharing raw data [2]. In this approach, each organization trains
a local model on its data and submits the model updates
to a central server. The server aggregates these updates to


produce a global model, which is then disseminated back to the
participating organizations for further local updates. This iter-
ative process continues until the desired level of accuracy and
convergence is achieved. FL’s key advantage lies in preserving
data privacy by retaining sensitive information within the
boundaries of each participating organization. This approach
has garnered significant attention in healthcare, where data
privacy concerns and regulatory compliance are of paramount
importance [7].

In the context of medical NLP, FL enables the development
of robust models that leverage the expertise of multiple health-
care institutions while adhering to privacy regulations. By
combining FL with advanced NLP models, such as LSTM and
BERT, it is possible to create powerful solutions for tasks like
medical entity extraction, relation extraction, clinical docu-
ment classification, generating medical reports, and predicting
patient outcomes from clinical notes, all while preserving data
privacy.

LSTM-based models are often preferred in medical NLP
tasks due to their capacity to handle sequential data and
capture long-range dependencies [8]. On the other hand, BERT
is an advanced language model that has demonstrated excep-
tional performance in various NLP tasks, surpassing traditional
LSTM-based models [5]. BERT is pre-trained on large text
corpora and fine-tuned on specific tasks, enabling it to discern
intricate relationships and contextual information within the
text. BERT’s bidirectional nature permits it to apprehend the
context of words from both left and right directions, which is
particularly advantageous for analyzing complex and domain-
specific language found in medical data.

B. Related Federated Learning Frameworks

This section presents an overview of the prominent FL
frameworks extensively employed in both industrial and aca-
demic settings. These frameworks include:

e TFF (9): TFF, an open-source framework developed by
Google, facilitates machine learning and other computations
on decentralized data. It enables developers to implement
federated learning algorithms using the high-level APIs of
TensorFlow.

e PySyft {10}: The OpenMined community developed PySyft,
a Python library providing federated learning, secure multi-
party computation, and differential privacy capabilities. PySyft
extends widely-used deep learning libraries such as PyTorch
and TensorFlow to support secure and privacy-preserving
machine learning.

e FATE [il]: WeBank’s open-source FL framework, FATE,
is designed to facilitate secure computations on distributed
data. It encompasses various federated learning algorithms,
including vertical and horizontal federated learning.

e LEAF [12]: LEAF, an open-source benchmarking frame-
work for federated learning, was developed by researchers at
Carnegie Mellon University. It offers a collection of datasets,
pre-processing tools, and evaluation metrics to enable fair
comparisons among different federated learning algorithms.

System Pipeline

1. Command tasks 2. Execute NVFlare 3. Obtain results

COMMAND:
NVFlare - Global model

DER IENETETAE DIN 1 ee - Performance metric
/bert_finetuning.sh N - Open protoc. H
v - FL started I

NVFlare provision for pretraining/fine tuning

Client 1

1. Preparation: Public & secure keys
© 2. Method: Secure communication
3. Message: Model parameters

Secure .
Comm. .

“as EES

Client N

. Local client: Training & validate

. Clients > Server: Parameter upload

. Server: Parameter aggregation & validate
. Server > Client: Parameter distribute

. Local: parameter Update

Server . Repeat 1-5 for E communication rounds.

Fig. 1. System pipeline.

In comparison to these frameworks, NVFlare delivers su-
perior system reliability, privacy preservation, and optimal
resource allocation for distributed machine learning [6]. The
framework is designed to be flexible and scalable, promoting
collaboration among multiple healthcare institutions in train-
ing deep learning models without sharing raw data. While
BERT has not been previously implemented on NVFlare, this
paper demonstrates the integration of medical NLP models
and NVFlare, thereby rendering it an ideal platform for the
development of privacy-preserving medical NLP solutions.

II. METHODOLOGY
A. NVFlare integration

Fig. [1] represents the referencing system pipeline. The sys-
tem pipeline comprises three main stages: i) tasks allocation,
ii) NVFlare execution, and iii) obtaining results. The task
allocation stage involves the allocation of pretraining and
finetuning processes. Upon executing NVFlare execution, the
system generates NVFlare provision, which establishes server-
client protocols, followed by federated learning. The NVFlare
provision involves secure communication between the server
and clients, involving a process that includes the preparation of
public and secure keys, secure communication methods, and
transmission of model parameters. The procedure of NVFlare
operations encompasses local client training and validation,
client-to-server parameter upload, server parameter aggrega-
tion and validation, server-to-client parameter distribution, and
local parameter update, which repeats for & communication
rounds. Finally, the system obtains optimal global models and
performance metrics.

B. Model training

The weights of NLP models are updated to detect patient
diagnosis. To detect the patient diagnosis, this paper adopts bi-
nary classification as a task. The task is to detect patients with
adverse drug reactions (ADR). We have collected electronic
health records of 8,638 patients with clopidogrel prescriptions
(1,824 patients were identified as treatment failure cases) [13].

We also adopt a widely-used pre-training objective for trans-
formers, namely the masked-language-model (MLM) tech-
nique, as introduced in the BERT model [5]. The MLM


TABLE I
PARAMETERS USED IN THIS PAPER.

Description Values
Number of clients 8
e Machine 1 (Local server)
* OS: Ubuntu 20.04 LTS,
+ CPU: Intel Xeon E5-2638 (2ea),
* GPU: NVIDIA RTX 2080 Ti (4ea),
* RAM: 128 GB
e Machine 2 (AWS server, p3.8xlarge)
PyTorch v11.3, CUDA v11.7, NVFlare v2.2,
MLM-Pytorch , X-Transformers
e # of train data (pretraining): 453,377,
e # of valid. data (pretraining): 8,683,
e # of train data (validation): 6,927,
e # of valid. data (validation): 1,732,
e Optimizer/learning rate: Adam, 10-7

Hardware spec.

Software info.

Data info.

Learning info.

TABLE II
MEDICAL NLP MODELS USED IN THIS PAPER.
Specification/Model | BERT | BERT-mini | LSTM
Hidden dimension 128 50 128
# of attention heads 6 2 -
# of hidden layers 12 6 3

objective aims to predict the original tokens of a sentence
from their masked versions to enable the model to learn to
comprehend the context and semantics of the language. In this
study, a masking probability of p = 0.15 is utilized for MLM,
which involves masking 15% of the tokens in each sequence.
To regulate the BERT model, 10% of the tokens were not
masked but were included in the loss calculation. During the
MLM training process, the model generates probabilities for
each token in the vocabulary at the masked positions, which
are compared with the ground truth token for the masked
positions using the cross-entropy loss. Through minimizing
the loss function during training, the model learns to produce
contextually accurate token predictions, thereby improving its
understanding of the semantics and structure of the language.

IV. DEMONSTRATION
A. Experiment Setup

To investigate the performance of the proposed framework,
a feasibility testing on BERT pretraining and a comparison of
three models (e.g., LSTM, BERT, BERT-mini) in centralized,
FL, and standalone training modes are studied. These studies
are conducted with eight clients and two Linux machines. The
rest of the simulation parameters are listed in Tables [I] and [IT]

B. Results

1) Feasibility study on BERT pretraining: In this feasibility
study on BERT pretraining, the paper aims to investigate four
distinct training schemes: /) BERT using centralized data, 2)
BERT utilizing a small dataset, 3) BERT trained on imbal-
anced data, and 4) BERT using balanced data. It is noteworthy
that BERT employing centralized data is considered the upper
bound of performance metric, while BERT utilizing a small
dataset is regarded as the lower bound of performance metric.
On the other hand, BERT trained on imbalanced data and
balanced data represent the main focus of the FL schemes.
The data imbalance is implemented by splitting the data into
ratios of {0.29, 0.22, 0.17, 0.14, 0.09, 0.04, 0.03, 0.02}, with

TABLE II
TOP-1 ACCURACY [%] OF VARIOUS NLP MODELS.
Schemes/Model BERT BERT-mini LSTM
Centralized 80.1 Pe 87.9
Standalone 72:2 68.5 67.3
FL 80.1 72.3 87.5

the number of data points identical for each client in the BERT
using balanced data scheme. The MLM loss is found to be
comparatively lower in BERT using centralized data, BERT
using balanced data, and BERT trained on imbalanced data,
as well as in BERT employing a small dataset. The MLM loss
begins at 10.7 and ultimately reaches 3.5 for BERT utilizing
centralized data, BERT trained on imbalanced data, and BERT
using balanced data. In contrast, BERT utilizing a small dataset
attains only an MLM loss of 4.4, indicating that BERT with
a decentralized approach is insufficient in generating medical
knowledge effectively.

2) Feasibility study on recursive model in FL: This study
investigates the feasibility of FL through the utilization
of three models: LSTM, BERT, and BERT-mini. Table |III
presents the performance of these models in centralized,
FL, and standalone approaches, respectively. The LSTM-
based model exhibits the highest performance, with val-
ues of 87.9%, 87.5%, 67.3% for centralized, FL, and stan-
dalone approaches, respectively. Similarly, BERT and BERT-
mini demonstrate similar performance tendencies across the
schemes. However, the LSTM-based model outperforms both
BERT and BERT-mini.

3) Limitation on BERT: Although BERT is generally rec-
ognized for its superior performance in comparison to LSTM
across a wide range of natural language processing tasks, there
are certain circumstances in which LSTM may outperform
BERT. These situations can be attributed to the following
factors: Firstly, task characteristics play a significant role, as
LSTM may possess a more appropriate structure for spe-
cific tasks relative to BERT. For example, LSTM may excel
in sequence modeling or time series data processing tasks.
Secondly, dataset size is a crucial determinant, since LSTM
can be effectively trained with relatively smaller amounts of
data. Consequently, in scenarios with limited datasets, LSTM

11) T ]
—— FedAvg (Balance)
FedAvg (Imbalance) | +

—— Centralized
9 —— Standalone

MLM Loss

a

200 400 600 800 1000 1200

Train iterations

Fig. 2. MLM loss.


Initalize server and client

SimulatorRunner -— INFO - Create the simulate clients.

ClientManager -— INFO - Client: New client site-1@127.0.0.1 joined. Sent
client:site-1 for project si

FederatedClient - INFO - Successfully registe

-d850f27ac@52. Total clients: 1

4
Token & SSH Protocols

4

toke

FederatedClient - INFO - Successfully registered client:site-8 for project simulator_server.

SimulatorRunner — INFO — Set the client status ready.
SimulatorRunner — INFO — Deploy and start the Server App

Local train
2023-04-0
2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07

iBertLearner
CiBertLearner
CiBertLearner
CiBertLearner

B/, 312
89,917 — CiBertLearner
®,242 -— CiBertLearner

- CiBertLearner

- CibertLearner — INFO:
— INFO:
- INFO:
— INFO:

2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07
2023-04-07

ScatterAndGather End aggregation.
ScatterAndGather Start
ScatterAndGather End-pe 3
ScatterAndGathef Round 9 nished.
ScatterAndGathéx Round 10 started.

ScatterAndGather schéduted-task train

valid_acc=0.
valid_acc=0.
valid_acc=0.
valid_acc=0.
valid_acc=0.
valid_acc=0.
valid_acc=0.

train_loss=0.
train_loss=0.
train_loss=1.
train_loss=1.
train_loss=1.
train_loss=0.
train_loss=1.

Server received site-4’s model parameters

DXO0Aggregator - aggregating 8 update(s) at round 9

rsist model on server.
+—~madel on server.

Ready for the next round

Fig. 3. Demonstraion example of BERT fine-tuning.

may surpass BERT’s performance. Furthermore, overfitting
is an issue that plagues large models like BERT, making
them susceptible to poor generalization when applied to small
datasets. In contrast, LSTM can achieve superior generaliza-
tion due to its fewer parameters. Finally, the performance dis-
parity may emerge from differences in optimization methods
employed during model training, including hyperparameter
settings, learning rate, loss functions, and others. Under certain
conditions, LSTM may learn more efficiently than BERT.

4) Demonstrations: Fig. |3] illustrates the implementation
of the NVFlare integrated framework, showcasing a BERT
fine-tuning example. Initially, the server and clients are es-
tablished, with token and SSH-based protocols employed to
ensure secure communication channels. Subsequently, each
client proceeds to train its model locally, taking an average
of 12.7 seconds per local epoch to complete the process.
Upon completion, the aggregator gathers the local model
parameters from each client. Once all model parameters have
been collected, the subsequent federated round commences.

V. CONCLUSION AND FUTURE RESEARCH DIRECTIONS

This paper proposes a practical approach to medical NLP
by integrating FL with NVFlare. The proposed framework
addresses the challenges of data privacy and regulatory com-
pliance while maintaining high accuracy and performance.
Furthermore, this study compares the performance of LSTM-
based models to BERT and BERT-mini across various training
settings: centralized, FL, and standalone. While LSTM outper-
forms BERT in the experiments, this outcome may be due to
task characteristics, dataset size, overfitting, or optimization-
related factors.

Future research directions includes investigating the impact
of different tasks and dataset sizes on the performance of
LSTM and BERT in medical NLP applications.

REFERENCES

[1] R. Xu, N. Baracaldo, Y. Zhou, A. Anwar, and H. Ludwig, “Hybridalpha:
An efficient approach for privacy-preserving federated learning,” in Proc.
of ACM Workshops on Artificial Intelligence and Security, 2019, pp. 13-
23,

[2] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning:
Concept and applications,” ACM Transactions on Intelligent Systems and
Technology (TIST), vol. 10, no. 2, pp. 1-19, 2019.

J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis, S.-L. Kim,
and M. Debbah, “Communication-efficient and distributed learning over
wireless networks: Principles and applications,’ Proceedings of the
IEEE, vol. 109, no. 5, pp. 796-819, 2021.

H. Safri, M. M. Kandi, Y. Miloudi, C. Bortolaso, D. Trystram, and
F. Desprez, “Towards developing a global federated learning platform for
loT,” in Proc. IEEE International Conference on Distributed Computing
Systems (ICDCS), Bologna, Italy, July 2022, pp. 1312-1315.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv: 1810.04805, 2018.

NVIDIA, “NvFlare: A framework for federated learning,”

developer.nvidia.com/nvflare, 2021.
[7] N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni,

and S. Bakas, “The future of digital health with federated learning,” npj
Digital Medicine, vol. 3, no. 1, pp. 1-7, 2020.

Z. C. Lipton, D. C. Kale, C. Elkan, and R. C. Wetzel, “Learning to di-
agnose with LSTM recurrent neural networks,” in Proc. of International
Conference on Learning Representations (ICLR), San Juan, Puerto Rico,
May 2016.

[9] “TensorFlow federated tutorial,” https://www.tensorflow.org/federated/

2023.
OpenMined, “PySyft,” https://github.com/OpenMined/PySyft, 2023.
https://github.com/FederatedAI/FATE

] FederatedAIL, “FATE,” 2023.

S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konecny, H. B. McMa-
han, V. Smith, and A. Talwalkar, “LEAF: A benchmark for federated
settings,” arXiv preprint arXiv:1812.01097, 2019.

I. G. Lee, S. Kim, M. Ban, M. Kim, and J. Chiang, “Predictive models
for clopidogrel outcome using prescription records and diagnosis codes,”
in Machine Learning for Health Care (MLHC), August 2022.

