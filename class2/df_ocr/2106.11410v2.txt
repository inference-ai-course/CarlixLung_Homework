arXiv:2106.11410v2 [cs.CL] 15 Jul 2021

A Survey of Race, Racism, and Anti-Racism in NLP

Anjalie Field
Carnegie Mellon University
anjalief@cs.cmu.edu

Zeerak Waseem
University of Sheffield
z.w.butt@sheffield.ac.uk

Abstract

Despite inextricable ties between race and lan-
guage, little work has considered race in NLP
research and development. In this work, we
survey 79 papers from the ACL anthology that
mention race. These papers reveal various
types of race-related bias in all stages of NLP
model development, highlighting the need for
proactive consideration of how NLP systems
can uphold racial hierarchies. However, per-
sistent gaps in research on race and NLP re-
main: race has been siloed as a niche topic
and remains ignored in many NLP tasks; most
work operationalizes race as a fixed single-
dimensional variable with a ground-truth label,
which risks reinforcing differences produced
by historical racism; and the voices of histor-
ically marginalized people are nearly absent in
NLP literature. By identifying where and how
NLP literature has and has not considered race,
especially in comparison to related fields, our
work calls for inclusion and racial justice in
NLP research practices.

1 Introduction

Race and language are tied in complicated ways.
Raciolinguistics scholars have studied how they are
mutually constructed: historically, colonial pow-
ers construct linguistic and racial hierarchies to
justify violence, and currently, beliefs about the
inferiority of racialized people’s language practices
continue to justify social and economic exclusion
(Rosa and Flores, 2017).! Furthermore, language
is the primary means through which stereotypes
and prejudices are communicated and perpetuated
(Hamilton and Trolier, 1986; Bar-Tal et al., 2013).

However, questions of race and racial bias
have been minimally explored in NLP literature.

'We use racialization to refer the process of “ascribing and
prescribing a racial category or classification to an individual
or group of people ... based on racial attributes including but

not limited to cultural and social history, physical features,
and skin color” (Charity Hudley, 2017).

Su Lin Blodgett
Microsoft Research

sulin. blodgett@microsoft.com

Yulia Tsvetkov
University of Washington
yuliats@cs.washington.edu

While researchers and activists have increasingly
drawn attention to racism in computer science and
academia, frequently-cited examples of racial bias
in AI are often drawn from disciplines other than
NLP, such as computer vision (facial recognition)
(Buolamwini and Gebru, 2018) or machine learn-
ing (recidivism risk prediction) (Angwin et al.,
2016). Even the presence of racial biases in search
engines like Google (Sweeney, 2013; Noble, 2018)
has prompted little investigation in the ACL com-
munity. Work on NLP and race remains sparse,
particularly in contrast to concerns about gender
bias, which have led to surveys, workshops, and
shared tasks (Sun et al., 2019; Webster et al., 2019).

In this work, we conduct a comprehensive sur-
vey of how NLP literature and research practices
engage with race. We first examine 79 papers from
the ACL Anthology that mention the words ‘race’,

‘racial’, or ‘racism’ and highlight examples of how

racial biases manifest at all stages of NLP model
pipelines ($3). We then describe some of the limi-
tations of current work (§4), specifically showing
that NLP research has only examined race in a nar-
row range of tasks with limited or no social context.
Finally, in §5, we revisit the NLP pipeline with a fo-
cus on how people generate data, build models, and
are affected by deployed systems, and we highlight
current failures to engage with people traditionally
underrepresented in STEM and academia.

While little work has examined the role of race
in NLP specifically, prior work has discussed race
in related fields, including human-computer in-
teraction (HCI) (Ogbonnaya-Ogburu et al., 2020;
Rankin and Thomas, 2019; Schlesinger et al.,
2017), fairness in machine learning (Hanna et al.,
2020), and linguistics (Charity Hudley et al., 2020;
Motha, 2020). We draw comparisons and guid-
ance from this work and show its relevance to NLP
research. Our work differs from NLP-focused re-
lated work on gender bias (Sun et al., 2019), ‘bias’


generally (Blodgett et al., 2020), and the adverse
impacts of language models (Bender et al., 2021)
in its explicit focus on race and racism.

In surveying research in NLP and related fields,
we ultimately find that NLP systems and research
practices produce differences along racialized lines.
Our work calls for NLP researchers to consider
the social hierarchies upheld and exacerbated by
NLP research and to shift the field toward “greater
inclusion and racial justice” (Charity Hudley et al.,
2020).

2 What is race?

It has been widely accepted by social scientists that
race is a social construct, meaning it “was brought
into existence or shaped by historical events, social
forces, political power, and/or colonial conquest”
rather than reflecting biological or ‘natural’ differ-
ences (Hanna et al., 2020). More recent work has
criticized the “social construction” theory as circu-
lar and rooted in academic discourse, and instead
referred to race as “colonial constituted practices”,
including “an inherited western, modern-colonial
practice of violence, assemblage, superordination,
exploitation and segregation” (Saucier et al., 2016).

The term race is also multi-dimensional and
can refer to a variety of different perspectives, in-
cluding racial identity (how you self-identify), ob-
served race (the race others perceive you to be),
and reflected race (the race you believe others per-
ceive you to be) (Roth, 2016; Hanna et al., 2020;
Ogbonnaya-Ogbutu et al., 2020). Racial catego-
rizations often differ across dimensions and depend
on the defined categorization schema. For exam-
ple, the United States census considers Hispanic
an ethnicity, not a race, but surveys suggest that
2/3 of people who identify as Hispanic consider
it a part of their racial background.” Similarly,
the census does not consider ‘Jewish’ a race, but
some NLP work considers anti-Semitism a form
of racism (Hasanuzzaman et al., 2017). Race de-
pends on historical and social context—there are
no ‘ground truth’ labels or categories (Roth, 2016).

As the work we survey primarily focuses on the
United States, our analysis similarly focuses on the

*https://www.census.gov/mso/
www/training/pdf/race-ethnicity-
onepager.pdf/, https: //www.census.gov/
topics/population/race/about.html,
https://www.pewresearch.org/fact-tank/
2015/06/15/is-being-hispanic-a-matter-
of-race-ethnicity-—or-both/

U.S. However, as race and racism are global con-
structs, some aspects of our analysis are applicable
to other contexts. We suggest that future studies
on racialization in NLP ground their analysis in the
appropriate geo-cultural context, which may result
in findings or analyses that differ from our work.

3 Survey of NLP literature on race

3.1 ACL Anthology papers about race

In this section, we introduce our primary survey
data—papers from the ACL Anthology*>—and we
describe some of their major findings to empha-
size that NLP systems encode racial biases. We
searched the anthology for papers containing the
terms ‘racial’, ‘racism’, or ‘race’, discarding ones
that only mentioned race in the references section
or in data examples and adding related papers cited
by the initial set if they were also in the ACL An-
thology. In using keyword searches, we focus on
papers that explicitly mention race and consider
papers that use euphemistic terms to not have sub-
stantial engagement on this topic. As our focus
is on NLP and the ACL community, we do not in-
clude NLP-related papers published in other venues
in the reported metrics (e.g. Table 1), but we do
draw from them throughout our analysis.

Our initial search identified 165 papers. How-
ever, reviewing all of them revealed that many do
not deeply engage on the topic. For example, 37
papers mention ‘racism’ as a form of abusive lan-
guage or use ‘racist’ as an offensive/hate speech
label without further engagement. 30 papers only
mention race as future work, related work, or mo-
tivation, e.g. in a survey about gender bias, “Non-
binary genders as well as racial biases have largely
been ignored in NLP” (Sun et al., 2019). After
discarding these types of papers, our final analysis
set consists of 79 papers.*

Table | provides an overview of the 79 papers,
manually coded for each paper’s primary NLP task
and its focal goal or contribution. We determined
task/application labels through an iterative process:
listing the main focus of each paper and then col-
lapsing similar categories. In cases where papers

>The ACL Anthology includes papers from all official
ACL venues and some non-ACL events listed in Appendix A,
as of December 2020 it included 6, 200 papers

4We do not discard all papers about abusive language, only
ones that exclusively use racism/racist as a classification label.
We retain papers with further engagement, e.g. discussions
of how to define racism or identification of racial bias in hate
speech classifiers.


n 3 "aS =

5 ie) = & o)

= o low mQ Q&

3 N 2) es 2 B

i) > SC 8 «Ss sr

= € 6 8 8 §/8

Oo «££ Q2 A A B/E
Abusive Language 6 4 2 5 2 2) 21
Social Science/Social Media 2 10 6 1 - 1 | 20
Text Representations (LMs, embeddings) - 2 - 9 2 - | 13
Text Generation (dialogue, image captions, story gen.) —- - 1 5 1 1/8
Sector-specific NLP applications (edu., law, health) 1 2 - - 1 3] 7
Ethics/Task-independent Bias 1 1 1 2/6
Core NLP Applications (parsing, NLI, IE) 1 - 1 1 1 -4|4
Total 11 18 11 22 8 94) 79

Table 1: 79 papers on race or racism from the ACL anthology, categorized by NLP application and focal task.

could rightfully be included in multiple categories,
we assign them to the best-matching one based on
stated contributions and the percentage of the paper
devoted to each possible category. In the Appendix
we provide additional categorizations of the papers
according to publication year, venue, and racial
categories used, as well as the full list of 79 papers.

3.2 NLP systems encode racial bias

Next, we present examples that identify racial bias
in NLP models, focusing on 5 parts of a standard
NLP pipeline: data, data labels, models, model out-
puts, and social analyses of outputs. We include
papers described in Table 1 and also relevant liter-
ature beyond the ACL Anthology (e.g. NeurIPS,
PNAS, Science). These examples are not intended
to be exhaustive, and in §4 we describe some of the
ways that NLP literature has failed to engage with
race, but nevertheless, we present them as evidence
that NLP systems perpetuate harmful biases along
racialized lines.

Data A substantial amount of prior work has al-
ready shown how NLP systems, especially word
embeddings and language models, can absorb and
amplify social biases in data sets (Bolukbasi et al.,
2016; Zhao et al., 2017). While most work focuses
on gender bias, some work has made similar ob-
servations about racial bias (Rudinger et al., 2017;
Garg et al., 2018; Kurita et al., 2019). These studies
focus on how training data might describe racial
minorities in biased ways, for example, by exam-
ining words associated with terms like ‘black’ or
traditionally European/African American names
(Caliskan et al., 2017; Manzini et al., 2019). Some
studies additionally capture who is described, re-

vealing under-representation in training data, some-
times tangentially to primary research questions:
Rudinger et al. (2017) suggest that gender bias may
be easier to identify than racial or ethnic bias in
Natural Language Inference data sets because of
data sparsity, and Caliskan et al. (2017) alter the
Implicit Association Test stimuli that they use to
measure biases in word embeddings because some
African American names were not frequent enough
in their corpora.

An equally important consideration, in addition
to whom the data describes is who authored the
data. For example, Blodgett et al. (2018) show
that parsing systems trained on White Mainstream
American English perform poorly on African
American English (AAE).° In a more general exam-
ple, Wikipedia has become a popular data source
for many NLP tasks. However, surveys suggest
that Wikipedia editors are primarily from white-
majority countries,° and several initiatives have
pointed out systemic racial biases in Wikipedia
coverage (Adams et al., 2019; Field et al., 2021).’
Models trained on these data only learn to process
the type of text generated by these users, and fur-
ther, only learn information about the topics these
users are interested in. The representativeness of
data sets is a well-discussed issue in social-oriented
tasks, like inferring public opinion (Olteanu et al.,
2019), but this issue is also an important considera-

>We note that conceptualizations of AAE and the accom-
panying terminology for the variety have shifted considerably
in the last half century; see King (2020) for an overview.

®https://meta.wikimedia.org/wiki/

Research:Wikipedia_Editors_Survey_2011 April

Thttps://en.wikipedia.org/wiki/
Racial_bias_on_Wikipedia


tion in ‘neutral’ tasks like parsing (Waseem et al.,
2021). The type of data that researchers choose
to train their models on does not just affect what
data the models perform well for, it affects what
people the models work for. NLP researchers can-
not assume models will be useful or function for
marginalized people unless they are trained on data
generated by them.

Data Labels Although model biases are often
blamed on raw data, several of the papers we survey
identify biases in the way researchers categorize or
obtain data annotations. For example:

¢ Annotation schema Returning to Blodgett
et al. (2018), this work defines new parsing
standards for formalisms common in AAE,
demonstrating how parsing labels themselves
were not designed for racialized language va-
rieties.
Annotation instructions Sap et al. (2019)
show that annotators are less likely to label
tweets using AAE as offensive if they are
told the likely language varieties of the tweets.
Thus, how annotation schemes are designed
(e.g. what contextual information is provided)
can impact annotators’ decisions, and fail-
ing to provide sufficient context can result
in racial biases.
Annotator selection Waseem (2016) show
that feminist/anti-racist activists assign differ-
ent offensive language labels to tweets than
figure-eight workers, demonstrating that an-
notators’ lived experiences affect data annota-
tions.

Models Some papers have found evidence that
model instances or architectures can change the
racial biases of outputs produced by the model.
Sommerauer and Fokkens (2019) find that the word
embedding associations around words like ‘race’
and ‘racial’ change not only depending on the
model architecture used to train embeddings, but
also on the specific model instance used to extract
them, perhaps because of differing random seeds.
Kiritchenko and Mohammad (2018) examine gen-
der and race biases in 200 sentiment analysis sys-
tems submitted to a shared task and find different
levels of bias in different systems. As the train-
ing data for the shared task was standardized, all
models were trained on the same data. However,
participants could have used external training data
or pre-trained embeddings, so a more detailed in-

vestigation of results is needed to ascertain which
factors most contribute to disparate performance.

Model Outputs Several papers focus on model
outcomes, and how NLP systems could perpetuate
and amplify bias if they are deployed:

¢ Classifiers trained on common abusive lan-
guage data sets are more likely to label tweets
containing characteristics of AAE as offensive
(Davidson et al., 2019; Sap et al., 2019).

¢ Classifiers for abusive language are more
likely to label text containing identity terms
like ‘black’ as offensive (Dixon et al., 2018).

¢ GPT outputs text with more negative senti-
ment when prompted with AAE -like inputs
(Groenwold et al., 2020).

Social Analyses of Outputs While the examples
in this section primarily focus on racial biases in
trained NLP systems, other work (e.g. included
in ‘Social Science/Social Media’ in Table 1) uses
NLP tools to analyze race in society. Examples in-
clude examining how commentators describe foot-
ball players of different races (Merullo et al., 2019)
or how words like ‘prejudice’ have changed mean-
ing over time (Vylomova et al., 2019).

While differing in goals, this work is often sus-
ceptible to the same pitfalls as other NLP tasks.
One area requiring particular caution is in the in-
terpretation of results produced by analysis models.
For example, while word embeddings have become
a common way to measure semantic change or es-
timate word meanings (Garg et al., 2018), Joseph
and Morgan (2020) show that embedding associ-
ations do not always correlate with human opin-
ions; in particular, correlations are stronger for be-
liefs about gender than race. Relatedly, in HCI,
the recognition that authors’ own biases can affect
their interpretations of results has caused some au-
thors to provide self-disclosures (Schlesinger et al.,
2017), but this practice is uncommon in NLP.

We conclude this section by observing that when
researchers have looked for racial biases in NLP
systems, they have usually found them. This litera-
ture calls for proactive approaches in considering
how data is collected, annotated, used, and inter-
preted to prevent NLP systems from exacerbating
historical racial hierarchies.


4 Limitations in where and how NLP
operationalizes race

While §3 demonstrates ways that NLP systems
encode racial biases, we next identify gaps and lim-
itations in how these works have examined racism,
focusing on how and in what tasks researchers have
considered race. We ultimately conclude that prior
NLP literature has marginalized research on race
and encourage deeper engagement with other fields,
critical views of simplified classification schema,
and broader application scope in future work (Blod-
gett et al., 2020; Hanna et al., 2020).

4.1 Common data sets are narrow in scope

The papers we surveyed suggest that research on
race in NLP has used a very limited range of
data sets, which fails to account for the multi-
dimensionality of race and simplifications inher-
ent in classification. We identified 3 common data
sources:®

¢ 9 papers use a set of tweets with inferred prob-
abilistic topic labels based on alignment with
U.S. census race/ethnicity groups (or the pro-
vided inference model) (Blodgett et al., 2016).
11 papers use lists of names drawn from
Sweeney (2013), Caliskan et al. (2017), or
Garg et al. (2018). Most commonly, 6 pa-
pers use African/European American names
from the Word Embedding Association Test
(WEAT) (Caliskan et al., 2017), which in turn
draws data from Greenwald et al. (1998) and
Bertrand and Mullainathan (2004).
10 papers use explicit keywords like ‘Black
woman’, often placed in templates like “Iam a
_____—” to test if model performance remains
the same for different identity terms.

While these commonly-used data sets can iden-
tify performance disparities, they only capture a
narrow subset of the multiple dimensions of race
($2). For example, none of them capture self-
identified race. While observed race is often appro-
priate for examining discrimination and some types
of disparities, it is impossible to assess potential
harms and benefits of NLP systems without assess-
ing their performance over text generated by and
directed to people of different races. The corpus
from Blodgett et al. (2016) does serve as a start-
ing point and forms the basis of most current work
assessing performance gaps in NLP models (Sap

5 We provide further counts of what racial categories papers
use and how they operationalize them in Appendix B.

et al., 2019; Blodgett et al., 2018; Xia et al., 2020;
Xu et al., 2019; Groenwold et al., 2020), but even
this corpus is explicitly not intended to infer race.

Furthermore, names and hand-selected iden-
tity terms are not sufficient for uncovering model
bias. De-Arteaga et al. (2019) show this in ex-
amining gender bias in occupation classification:
when overt indicators like names and pronouns are
scrubbed from the data, performance gaps and po-
tential allocational harms still remain. Names also
generalize poorly. While identity terms can be ex-
amined across languages (van Miltenburg et al.,
2017), differences in naming conventions often do
not translate, leading some studies to omit examin-
ing racial bias in non-English languages (Lauscher
and Glavas, 2019). Even within English, names of-
ten fail to generalize across domains, geographies,
and time. For example, names drawn from the
U.S. census generalize poorly to Twitter (Wood-
Doughty et al., 2018), and names common among
Black and white children were not distinctly differ-
ent prior to the 1970s (Fryer Jr and Levitt, 2004;
Sweeney, 2013).

We focus on these 3 data sets as they were
most common in the papers we surveyed, but
we note that others exist. Preotiuc-Pietro and
Ungar (2018) provide a data set of tweets with
self-identified race of their authors, though it is
little used in subsequent work and focused on
demographic prediction, rather than evaluating
model performance gaps. Two recently-released
data sets (Nadeem et al., 2020; Nangia et al.,
2020) provide crowd-sourced pairs of more- and
less-stereotypical text. More work is needed to
understand any privacy concerns and the strengths
and limitations of these data (Blodgett et al., 2021).
Additionally, some papers collect domain-specific
data, such as self-reported race in an online com-
munity (Loveys et al., 2018), or crowd-sourced
annotations of perceived race of football players
(Merullo et al., 2019). While these works offer
clear contextualization, it is difficult to use these
data sets to address other research questions.

4.2 Classification schemes operationalize
race as a fixed, single-dimensional
U.S.-census label

Work that uses the same few data sets inevitably
also uses the same few classification schemes, often
without justification. The most common explicitly
stated source of racial categories is the U.S. census,


which reflects the general trend of U.S.-centrism
in NLP research (the vast majority of work we sur-
veyed also focused on English). While census cate-
gories are sometimes appropriate, repeated use of
classification schemes and accompanying data sets
without considering who defined these schemes
and whether or not they are appropriate for the cur-
rent context risks perpetuating the misconception
that race is ‘natural’ across geo-cultural contexts.
We refer to Hanna et al. (2020) for a more thorough
overview of the harms of “widespread uncritical
adoption of racial categories,” which “can in turn
re-entrench systems of racial stratification which
give rise to real health and social inequalities.” At
best, the way race has been operationalized in NLP
research is only capable of examining a narrow sub-
set of potential harms. At worst, it risks reinforcing
racism by presenting racial divisions as natural,
rather than the product of social and historical con-
text (Bowker and Star, 2000).

As an example of questioning who devised racial
categories and for what purpose, we consider the
pattern of re-using names from Greenwald et al.
(1998), who describe their data as sets of names
“judged by introductory psychology students to be
more likely to belong to White Americans than to
Black Americans” or vice versa. When incorpo-
rating this data into WEAT, Caliskan et al. (2017)
discard some judged African American names as
too infrequent in their embedding data. Work sub-
sequently drawing from WEAT makes no mention
of the discarded names nor contains much discus-
sion of how the data was generated and whether or
not names judged to be white or Black by introduc-
tory psychology students in 1998 are an appropriate
benchmark for the studied task. While gathering
data to examine race in NLP is challenging, and in
this work we ourselves draw from examples that
use Greenwald et al. (1998), it is difficult to inter-
pret what implications arise when models exhibit
disparities over this data and to what extent models
without disparities can be considered “debiased’.

Finally, almost all of the work we examined con-
ducts single-dimensional analyses, e.g. focus on
race or gender but not both simultaneously. This
focus contrasts with the concept of intersection-
ality, which has shown that examining discrim-
ination along a single axis fails to capture the
experiences of people who face marginalization
along multiple axes. For example, consideration
of race often emphasizes the experience of gender-

privileged people (e.g. Black men), while consid-
eration of gender emphasizes the experience of
race-privileged people (e.g. white women). Nei-
ther reflect the experience of people who face dis-
crimination along both axes (e.g. Black women)
(Crenshaw, 1989). A small selection of papers have
examined intersectional biases in embeddings or
word co-occurrences (Herbelot et al., 2012; May
et al., 2019; Tan and Celis, 2019; Lepori, 2020), but
we did not identify mentions of intersectionality in
any other NLP research areas. Further, several of
these papers use NLP technology to examine or val-
idate theories on intersectionality; they do not draw
from theory on intersectionality to critically exam-
ine NLP models. These omissions can mask harms:
Jiang and Fellbaum (2020) provide an example us-
ing word embeddings of how failing to consider in-
tersectionality can render invisible people marginal-
ized in multiple ways. Numerous directions remain
for exploration, such as how ‘debiasing’ models
along one social dimension affects other dimen-
sions. Surveys in HCI offer further frameworks
on how to incorporate identity and intersectional-
ity into computational research (Schlesinger et al.,
2017; Rankin and Thomas, 2019).

4.3 NLP research on race is restricted to
specific tasks and applications

Finally, Table 1 reveals many common NLP appli-
cations where race has not been examined, such as
machine translation, summarization, or question an-
swering.” While some tasks seem inherently more
relevant to social context than others (a claim we
dispute in this work, particularly in §5), research on
race is compartmentalized to limited areas of NLP
even in comparison with work on ‘bias’. For exam-
ple, Blodgett et al. (2020) identify 20 papers that
examine bias in co-reference resolution systems
and 8 in machine translation, whereas we identify
0 papers in either that consider race. Instead, race
is most often mentioned in NLP papers in the con-
text of abusive language, and work on detecting or
removing bias in NLP models has focused on word
embeddings.

Overall, our survey identifies a need for the ex-
amination of race in a broader range of NLP tasks,
the development of multi-dimensional data sets,
and careful consideration of context and appropri-
ateness of racial categories. In general, race is

We identified only 8 relevant papers on Text Generation,

which focus on other areas including chat bots, GPT-2/3, hu-
mor generation, and story generation.


difficult to operationalize, but NLP researchers do
not need to start from scratch, and can instead draw
from relevant work in other fields.

5 NLP propagates marginalization of
racialized people

While in §4 we primarily discuss race as a topic or
a construct, in this section, we consider the role, or
more pointedly, the absence, of traditionally under-
represented people in NLP research.

5.1 People create data

As discussed in §3.2, data and annotations are gen-
erated by people, and failure to consider who cre-
ated data can lead to harms. In §3.2 we identify
a need for diverse training data in order to ensure
models work for a diverse set of people, and in §4
we describe a similar need for diversity in data that
is used to assess algorithmic fairness. However,
gathering this type of data without consideration of
the people who generated it can introduce privacy
violations and risks of demographic profiling.

As an example, in 2019, partially in response
to research showing that facial recognition al-
gorithms perform worse on darker-skinned than
lighter-skinned people (Buolamwini and Gebru,
2018; Raji and Buolamwini, 2019), researchers
at IBM created the “Diversity in Faces” data set,
which consists of 1 million photos sampled from
the the publicly available YFCC-100M data set and
annotated with “craniofacial distances, areas and
ratios, facial symmetry and contrast, skin color,
age and gender predictions” (Merler et al., 2019).
While this data set aimed to improve the fairness
of facial recognition technology, it included pho-
tos collected from a Flickr, a photo-sharing web-
site whose users did not explicitly consent for this
use of their photos. Some of these users filed a
lawsuit against IBM, in part for “subjecting them
to increased surveillance, stalking, identity theft,
and other invasions of privacy and fraud.”!° NLP

“nttps://www.classaction.org/news/
class-action-accuses-—ibm-of-flagrant—
violdtions-or=i11 linois-biometric=
privacy-law-to-develop-facial
recognition-tech#embedded-document
https://www.nbcnews.com/tech/internet/
facial—recognition—s-dirty-1i bbl
secret-millions-—online-photos-—scraped
n981921 IBM has since removed the “Diversity in Faces”
data set as well as their “Detect Faces” public API and
stopped their use of and research on facial recognition.
https://qz.com/1866848/why-ibm-abandoned-
its-facial-recognition-program/

researchers could easily repeat this incident, for
example, by using demographic profiling of social
media users to create more diverse data sets. While
obtaining diverse, representative, real-world data
sets is important for building models, data must
be collected with consideration for the people who
generated it, such as obtaining informed consent,
setting limits of uses, and preserving privacy, as
well as recognizing that some communities may
not want their data used for NLP at all (Paullada,
2020).

5.2 People build models

Research is additionally carried out by people who
determine what projects to pursue and how to
approach them. While statistics on ACL confer-
ences and publications have focused on geographic
representation rather than race, they do highlight
under-representation. Out of 2,695 author affili-
ations associated with papers in the ACL Anthol-
ogy for 5 major conferences held in 2018, only 5
(0.2%) were from Africa, compared with 1, 114
from North America (41.3%).!! Statistics pub-
lished for 2017 conference attendees and ACL fel-
lows similarly reveal a much higher percentage
of people from “North, Central and South Amer-
ica” (55% attendees / 74% fellows) than from “Eu-
rope, Middle East and Africa” (19%/13%) or “Asia-
Pacific” (23%/13%).'? These broad regional cate-
gories likely mask further under-representation, e.g.
percentage of attendees and fellows from Africa
as compared to Europe. According to an NSF re-
port that includes racial statistics rather than na-
tionality, 14% of doctorate degrees in Computer
Science awarded by U.S. institutions to U.S. cit-
izens and permanent residents were awarded to
Asian students, < 4% to Black or African Ameri-
can students, and 0% to American Indian or Alaska
Native students (National Center for Science and
Engineering Statistics, 2019).!>

It is difficult to envision reducing or eliminating
racial differences in NLP systems without changes
in the researchers building these systems. One
theory that exemplifies this challenge is interest
convergence, which suggests that people in posi-
tions of power only take action against systematic

"nttp://www.marekrei.com/blog/
geographic-diversity-of-—nlp-conferences/

°nttps://www.aclweb.org/portal/content /
acl-diversity-—statistics

Results exclude respondents who did not report race or
ethnicity or were Native Hawaiian or Other Pacific Islander.



problems like racism when it also advances their
own interests (Bell Jr, 1980). Ogbonnaya-Ogburu
et al. (2020) identify instances of interest conver-
gence in the HCI community, primarily in diversity
initiatives that benefit institutions’ images rather
than underrepresented people. In a research setting,
interest convergence can encourage studies of incre-
mental and surface-level biases while discouraging
research that might be perceived as controversial
and force fundamental changes in the field.

Demographic statistics are not sufficient for
avoiding pitfalls like interest convergence, as they
fail to capture the lived experiences of researchers.
Ogbonnaya-Ogbutu et al. (2020) provide several
examples of challenges that non-white HCI re-
searchers have faced, including the invisible labor
of representing ‘diversity’, everyday microaggres-
sions, and altering their research directions in ac-
cordance with their advisors’ interests. Rankin and
Thomas (2019) further discuss how research con-
ducted by people of different races is perceived dif-
ferently: “Black women in academia who conduct
research about the intersections of race, gender,
class, and so on are perceived as ‘doing service,’
whereas white colleagues who conduct the same re-
search are perceived as doing cutting-edge research
that demands attention and recognition.” While we
draw examples about race from HCI in the absence
of published work on these topics in NLP, the lack
of linguistic diversity in NLP research similarly
demonstrates how representation does not neces-
sarily imply inclusion. Although researchers from
various parts of the world (Asia, in particular) do
have some numerical representation among ACL
authors, attendees, and fellows, NLP research over-
whelmingly favors a small set of languages, with
a heavy skew towards European languages (Joshi
et al., 2020) and ‘standard’ language varieties (Ku-
mar et al., 2021).

5.3. People use models

Finally, NLP research produces technology that is
used by people, and even work without direct ap-
plications is typically intended for incorporation
into application-based systems. With the recogni-
tion that technology ultimately affects people, re-
searchers on ethics in NLP have increasingly called
for considerations of whom technology might harm
and suggested that there are some NLP technolo-
gies that should not be built at all. In the context of
perpetuating racism, examples include criticism of

tools for predicting demographic information (Tat-
man, 2020) and automatic prison term prediction
(Leins et al., 2020), motivated by the history of
using technology to police racial minorities and re-
lated criticism in other fields (Browne, 2015; Buo-
lamwini and Gebru, 2018; McIlwain, 2019). In
cases where potential harms are less direct, they
are often unaddressed entirely. For example, while
low-resource NLP is a large area of research, a
paper on machine translation of white American
and European languages is unlikely to discuss how
continual model improvements in these settings in-
crease technological inequality. Little work on low-
resource NLP has focused on the realities of struc-
tural racism or differences in lived experience and
how they might affect the way technology should
be designed.

Detection of abusive language offers an infor-
mative case study on the danger of failing to con-
sider people affected by technology. Work on abu-
sive language often aims to detect racism for con-
tent moderation (Waseem and Hovy, 2016). How-
ever, more recent work has show that existing hate
speech classifiers are likely to falsely label text con-
taining identity terms like “black’ or text containing
linguistic markers of AAE as toxic (Dixon et al.,
2018; Sap et al., 2019; Davidson et al., 2019; Xia
et al., 2020). Deploying these models could censor
the posts of the very people they purport to help.

In other areas of statistics and machine learning,
focus on participatory design has sought to am-
plify the voices of people affected by technology
and its development. An ICML 2020 workshop
titled “Participatory Approaches to Machine Learn-
ing” highlights a number of papers in this area
(Kulynych et al., 2020; Brown et al., 2019). A
few related examples exist in NLP, e.g. Gupta et al.
(2020) gather data for an interactive dialogue agent
intended to provide more accessible information
about heart failure to Hispanic/Latinx and African
American patients. The authors engage with health-
care providers and doctors, though they leave focal
groups with patients for future work. While NLP
researchers may not be best situated to examine
how people interact with deployed technology, they
could instead draw motivation from fields that have
stronger histories of participatory design, such as
HCI. However, we did not identify citing participa-
tory design studies conducted by others as common
practice in the work we surveyed. As in the case
of researcher demographics, participatory design is


not an end-all solution. Sloane et al. (2020) provide
a discussion of how participatory design can col-
lapse to ‘participation-washing’ and how such work
must be context-specific, long-term, and genuine.

6 Discussion

We conclude by synthesizing some of the obser-
vations made in the preceding sections into more
actionable items. First, NLP research needs to
explicitly incorporate race. We quote Benjamin
(2019): “[technical systems and social codes] op-
erate within powerful systems of meaning that ren-
der some things visible, others invisible, and create
a vast array of distortions and dangers.”

In the context of NLP research, this philosophy
implies that all technology we build works in ser-
vice of some ideas or relations, either by upholding
them or dismantling them. Any research that is
not actively combating prevalent social systems
like racism risks perpetuating or exacerbating them.
Our work identifies several ways in which NLP
research upholds racism:

¢ Systems contain representational harms and

performance gaps throughout NLP pipelines
¢ Research on race is restricted to a narrow sub-
set of tasks and definitions of race, which can
mask harms and falsely reify race as ‘natural’
¢ Traditionally underrepresented people are ex-
cluded from the research process, both as con-
sumers and producers of technology

Furthermore, while we focus on race, which
we note has received substantially less attention
than gender, many of the observations in this work
hold for social characteristics that have received
even less attention in NLP research, such as so-
cioeconomic class, disability, or sexual orientation
(Mendelsohn et al., 2020; Hutchinson et al., 2020).

Nevertheless, none of these challenges can be ad-
dressed without direct engagement with marginal-
ized communities of color. NLP researchers can
draw on precedents for this type of engagement
from other fields, such as participatory design and
value sensitive design models (Friedman et al.,
2013). Additionally, numerous organizations al-
ready exist that serve as starting points for partner-
ships, such as Black in AI, Masakhane, Data for
Black Lives, and the Algorithmic Justice League.

Finally, race and language are complicated, and
while readers may look for clearer recommenda-
tions, no one data set, model, or set of guidelines
can ‘solve’ racism in NLP. For instance, while we

draw from linguistics, Charity Hudley et al. (2020)
in turn call on linguists to draw models of racial
justice from anthropology, sociology, and psychol-
ogy. Relatedly, there are numerous racialized ef-
fects that NLP research can have that we do not
address in this work; for example, Bender et al.
(2021) and Strubell et al. (2019) discuss the envi-
ronmental costs of training large language models,
and how global warming disproportionately affects
marginalized communities. We suggest that read-
ers use our work as one starting point for bringing
inclusion and racial justice into NLP.

Acknowledgements

We gratefully thank Hanna Kim, Kartik Goyal, Ar-
tidoro Pagnoni, Qinlan Shen, and Michael Miller
Yoder for their feedback on this work. Z.W. has
been supported in part by the Canada 150 Research
Chair program and the UK-Canada Artificial Intel-
ligence Initiative. A.F. has been supported in part
by a Google PhD Fellowship and a GRFP under
Grant No. DGE1745016. This material is based
upon work supported in part by the National Sci-
ence Foundation under Grants No. I[S2040926 and
IIS2007960. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the NSF.

7 Ethical Considerations

We, the authors of this work, are situated in the
cultural contexts of the United States of America
and the United Kingdom/Europe, and some of us
identify as people of color. We all identify as NLP
researchers, and we acknowledge that we are situ-
ated within the traditionally exclusionary practices
of academic research. These perspectives have im-
pacted our work, and there are viewpoints outside
of our institutions and experiences that our work
may not fully represent.

References

Julia Adams, Hannah Briickner, and Cambria Naslund.
2019. Who counts as a notable sociologist on
Wikipedia? gender, race, and the “professor test”.
Socius, 5.

Silvio Amir, Mark Dredze, and John W. Ayers. 2019.
Mental health surveillance over social media with
digital cohorts. In Proceedings of the Sixth Work-
shop on Computational Linguistics and Clinical Psy-


chology, pages 114-120, Minneapolis, Minnesota.
Association for Computational Linguistics.

Julia Angwin, Jeff Larson, Surya Mattu, and Lauren
Kirchner. 2016. Machine bias: There’s software
used across the country to predict future criminals
and it’s biased against blacks. ProPublica.

Stavros Assimakopoulos, Rebecca Vella Muskat, Lon-
neke van der Plas, and Albert Gatt. 2020. Annotat-
ing for hate speech: The MaNeCo corpus and some
input from critical discourse analysis. In Proceed-
ings of the 12th Language Resources and Evaluation
Conference, pages 5088-5097, Marseille, France.
European Language Resources Association.

Daniel Bar-Tal, Carl F Graumann, Arie W Kruglanski,
and Wolfgang Stroebe. 2013. Stereotyping and prej-
udice: Changing conceptions. Springer Science &
Business Media.

Francesco Barbieri and Jose Camacho-Collados. 2018.
How gender and skin tone modifiers affect emoji se-
mantics in Twitter. In Proceedings of the Seventh
Joint Conference on Lexical and Computational Se-
mantics, pages 101-106, New Orleans, Louisiana.
Association for Computational Linguistics.

Derrick A Bell Jr. 1980. Brown v. board of education
and the interest-convergence dilemma. Harvard law
review, pages 518-533.

Emily Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? & . In Proceedings of the 2021 Confer-
ence on Fairness, Accountability, and Transparency,
page 610-623, New York, NY, USA. Association for
Computing Machinery.

Ruha Benjamin. 2019. Race After Technology: Aboli-
tionist Tools for the New Jim Code. Wiley.

Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
termg on Twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1010-1019, Atlanta,
Georgia. Association for Computational Linguistics.

Marianne Bertrand and Sendhil Mullainathan. 2004.
Are Emily and Greg more employable than Lak-
isha and Jamal? A field experiment on labor mar-
ket discrimination. American Economic Review,

94(4):991-1013.

Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5454—
5476, Online. Association for Computational Lin-
guistics.

Su Lin Blodgett, Lisa Green, and Brendan O’ Connor.

2016. Demographic dialectal variation in social
media: A case study of African-American English.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1119-1130, Austin, Texas. Association for Compu-
tational Linguistics.

Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,

Robert Sim, and Hanna Wallach. 2021. Stereotyp-
ing Norwegian Salmon: An Inventory of Pitfalls in
Fairness Benchmark Datasets. In Proceedings of the
Joint Conference of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing, Online. Association for Compu-
tational Linguistics.

Su Lin Blodgett, Johnny Wei, and Brendan O’Connor.

2018. Twitter Universal Dependency parsing for
African-American and mainstream American En-
glish. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume I; Long Papers), pages 1415-1425, Melbourne,
Australia. Association for Computational Linguis-
tics.

Tolga Bolukbasi, Kai-Wei Chang, James Zou,

Venkatesh Saligrama, and Adam Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? Debiasing word embeddings. In
Proceedings of the 30th International Confer-
ence on Neural Information Processing Systems,
page 4356-4364, Red Hook, NY, USA. Curran
Associates Inc.

Rishi Bommasani, Kelly Davis, and Claire Cardie.

2020. Interpreting Pretrained Contextualized Repre-
sentations via Reductions to Static Embeddings. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4758—
4781, Online. Association for Computational Lin-
guistics.

Geoffrey C. Bowker and Susan Leigh Star. 2000.

Sorting Things Out: Classification and Its Conse-
quences. Inside Technology. MIT Press.

Anna Brown, Alexandra Chouldechova, Emily Putnam-

Hornstein, Andrew Tobin, and Rhema Vaithianathan.
2019. Toward algorithmic accountability in pub-
lic services: A qualitative study of affected commu-
nity perspectives on algorithmic decision-making in
child welfare services. In Proceedings of the 2019
CHI Conference on Human Factors in Computing
Systems, CHI’ 19, page 1-12, New York, NY, USA.
Association for Computing Machinery.

Simone Browne. 2015. Dark Matters: On the Surveil-

lance of Blackness. Duke University Press.

Joy Buolamwini and Timnit Gebru. 2018. Gender

shades: Intersectional accuracy disparities in com-
mercial gender classification. In Proceedings of
the lst Conference on Fairness, Accountability and


Transparency, pages 77-91, New York, NY, USA.
PMLR.

Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334): 183-186.

Michael Castelle. 2018. The linguistic ideologies of
deep abusive language classification. In Proceed-
ings of the 2nd Workshop on Abusive Language On-
line (ALW2), pages 160-170, Brussels, Belgium. As-
sociation for Computational Linguistics.

Bharathi Raja Chakravarthi. 2020. HopeEDI: A mul-
tilingual hope speech detection dataset for equality,
diversity, and inclusion. In Proceedings of the Third
Workshop on Computational Modeling of People’s
Opinions, Personality, and Emotion’s in Social Me-
dia, pages 41-53, Barcelona, Spain (Online). Asso-
ciation for Computational Linguistics.

Anne H. Charity Hudley. 2017. Language and Racial-
ization. In Ofelia Garcia, Nelson Flores, and Mas-
similiano Spotti, editors, The Oxford Handbook of
Language and Society, pages 381-402. Oxford Uni-
versity Press.

Anne H. Charity Hudley, Christine Mallinson, and
Mary Bucholtz. 2020. Toward racial justice in lin-
guistics: Interdisciplinary insights into theorizing
race in the discipline and diversifying the profession.
Language, 96(4):e200-235.

Isobelle Clarke and Jack Grieve. 2017. Dimensions of
abusive language on Twitter. In Proceedings of the
First Workshop on Abusive Language Online, pages
1-10, Vancouver, BC, Canada. Association for Com-
putational Linguistics.

Kimberlé Crenshaw. 1989. Demarginalizing the inter-
section of race and sex: A black feminist critique of
antidiscrimination doctrine, feminist theory and an-
tiracist politics. University of Chicago Legal Forum,
1989(8).

Thomas Davidson, Debasmita Bhattacharya, and Ing-
mar Weber. 2019. Racial bias in hate speech and
abusive language detection datasets. In Proceedings
of the Third Workshop on Abusive Language Online,
pages 25-35, Florence, Italy. Association for Com-
putational Linguistics.

Maria De-Arteaga, Alexey Romanov, Hanna Wal-
lach, Jennifer Chayes, Christian Borgs, Alexandra
Chouldechova, Sahin Geyik, Krishnaram Kentha-
padi, and Adam Tauman Kalai. 2019. Bias in bios:
A case study of semantic representation bias in a
high-stakes setting. In Proceedings of the Confer-
ence on Fairness, Accountability, and Transparency,
page 120-128, New York, NY, USA. Association for
Computing Machinery.

Dorottya Demszky, Nikhil Garg, Rob Voigt, James
Zou, Jesse Shapiro, Matthew Gentzkow, and Dan Ju-
rafsky. 2019. Analyzing polarization in social me-
dia: Method and application to tweets on 21 mass

shootings. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 2970-3005, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,
and Lucy Vasserman. 2018. Measuring and mitigat-
ing unintended bias in text classification. In Pro-
ceedings of the 2018 AAAI/ACM Conference on Al,
Ethics, and Society, page 67-73, New York, NY,
USA. Association for Computing Machinery.

Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1365-1374, Portland, Oregon, USA. Association for
Computational Linguistics.

Yanai Elazar and Yoav Goldberg. 2018. Adversarial
removal of demographic attributes from text data.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
11-21, Brussels, Belgium. Association for Computa-
tional Linguistics.

Anjalie Field, Chan Young Park, and Yulia Tsvetkov.
2021. Controlled analyses of social biases in
Wikipedia bios. Computing Research Repository,
arXiv:2101.00078. Version 1.

Batya Friedman, Peter Kahn, Alan Borning, and Alina
Huldtgren. 2013. Value sensitive design and infor-
mation systems. In Neelke Doorn, Daan Schuur-
biers, Ibo van de Poel, and Michael Gorman, editors,
Early engagement and new technologies: Opening
up the laboratory, volume 16. Springer, Dordrecht.

Roland G Fryer Jr and Steven D Levitt. 2004. The
causes and consequences of distinctively black
names. The Quarterly Journal of Economics,
119(3):767-805.

Ryan J. Gallagher, Kyle Reing, David Kale, and Greg
Ver Steeg. 2017. Anchored correlation explanation:
Topic modeling with minimal domain knowledge.
Transactions of the Association for Computational
Linguistics, 5:529-542.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2018. Word embeddings quantify
100 years of gender and ethnic stereotypes. Pro-

ceedings of the National Academy of Sciences,
115(16):E3635-E3644.

Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and
Montse Cuadros. 2018. Hate speech dataset from
a white supremacy forum. In Proceedings of the
2nd Workshop on Abusive Language Online (ALW2),
pages 11-20, Brussels, Belgium. Association for
Computational Linguistics.


Nabeel Gillani and Roger Levy. 2019. Simple dynamic
word embeddings for mapping perceptions in the
public sphere. In Proceedings of the Third Work-
shop on Natural Language Processing and Compu-
tational Social Science, pages 94-99, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.

Anthony G Greenwald, Debbie E McGhee, and Jor-
dan LK Schwartz. 1998. Measuring individual dif-
ferences in implicit cognition: the implicit associa-
tion test. Journal of personality and social psychol-
ogy, 74(6):1464.

Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita
Honnavalli, Sharon Levy, Diba Mirza, and
William Yang Wang. 2020. Investigating African-
American Vernacular English in transformer-based
text generation. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 5877-5883, Online.
Association for Computational Linguistics.

Itika Gupta, Barbara Di Eugenio, Devika Salunke, An-
drew Boyd, Paula Allen-Meares, Carolyn Dickens,
and Olga Garcia. 2020. Heart failure education
of African American and Hispanic/Latino patients:
Data collection and analysis. In Proceedings of the
First Workshop on Natural Language Processing for
Medical Conversations, pages 41-46, Online. Asso-
ciation for Computational Linguistics.

David L Hamilton and Tina K Trolier. 1986. Stereo-
types and stereotyping: An overview of the cogni-
tive approach. In J. F. Dovidiom and S. L. Gaert-
ner, editors, Prejudice, discrimination, and racism,
pages 127—163. Academic Press.

Alex Hanna, Emily Denton, Andrew Smart, and Jamila
Smith-Loud. 2020. Towards a critical race method-
ology in algorithmic fairness. In Proceedings of the
2020 Conference on Fairness, Accountability, and
Transparency, page 501-512, New York, NY, USA.
Association for Computing Machinery.

Mohammed Hasanuzzaman, Gaél Dias, and Andy
Way. 2017. Demographic word embeddings for
racism detection on Twitter. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume I: Long Papers),
pages 926-936, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Aurélie Herbelot, Eva von Redecker, and Johanna
Miiller. 2012. Distributional techniques for philo-
sophical enquiry. In Proceedings of the 6th Work-
shop on Language Technology for Cultural Heritage,
Social Sciences, and Humanities, pages 45-54, Avi-
gnon, France. Association for Computational Lin-
guistics.

Xiaolei Huang, Linzi Xing, Franck Dernoncourt, and
Michael J. Paul. 2020. Multilingual Twitter cor-
pus and baselines for evaluating demographic bias
in hate speech recognition. In Proceedings of the

12th Language Resources and Evaluation Confer-
ence, pages 1440-1448, Marseille, France. Euro-
pean Language Resources Association.

Ben Hutchinson, Vinodkumar Prabhakaran, Emily
Denton, Kellie Webster, Yu Zhong, and Stephen De-
nuyl. 2020. Social biases in NLP models as barriers
for persons with disabilities. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 5491-5501, Online. As-
sociation for Computational Linguistics.

May Jiang and Christiane Fellbaum. 2020. Interdepen-
dencies of gender and race in contextualized word
embeddings. In Proceedings of the Second Work-
shop on Gender Bias in Natural Language Process-
ing, pages 17-25, Barcelona, Spain (Online). Asso-
ciation for Computational Linguistics.

Kenneth Joseph and Jonathan Morgan. 2020. When do
word embeddings accurately reflect surveys on our
beliefs about people? In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 4392-4415, Online. Association
for Computational Linguistics.

Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 6282-6293, Online. Association for Computa-
tional Linguistics.

David Jurgens, Libby Hemphill, and Eshwar Chan-
drasekharan. 2019. A just and comprehensive strat-
egy for using NLP to address online abuse. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 3658—
3666, Florence, Italy. Association for Computa-
tional Linguistics.

Saket Karve, Lyle Ungar, and Joao Sedoc. 2019. Con-
ceptor debiasing of word representations evaluated
on WEAT. In Proceedings of the First Workshop
on Gender Bias in Natural Language Processing,
pages 40-48, Florence, Italy. Association for Com-
putational Linguistics.

Anna Kasunic and Geoff Kaufman. 2018. Learning to
listen: Critically considering the role of AI in human
storytelling and character creation. In Proceedings
of the First Workshop on Storytelling, pages 1-13,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.

Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Da-
vani, Morteza Dehghani, and Xiang Ren. 2020. Con-
textualizing hate speech classifiers with post-hoc ex-
planation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 5435-5442, Online. Association for Computa-
tional Linguistics.


Sharese King. 2020. From African American Vernac-
ular English to African American Language: Re-
thinking the study of race and language in African
Americans’ speech. Annual Review of Linguistics,
6(1):285-300.

Svetlana Kiritchenko and Saif Mohammad. 2018. Ex-
amining gender and race bias in two hundred sen-
timent analysis systems. In Proceedings of the
Seventh Joint Conference on Lexical and Compu-
tational Semantics, pages 43-53, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Bogdan Kulynych, David Madras, Smitha Milli, In-
ioluwa Deborah Raji, Angela Zhou, and Richard
Zemel. 2020. Participatory approaches to machine
learning. International Conference on Machine
Learning Workshop.

Sachin Kumar, Antonios Anastasopoulos, Shuly Wint-
ner, and Yulia Tsvetkov. 2021. Machine translation
into low-resource language varieties. In Proceed-
ings of the 59th Annual Meeting of the Association
for Computational Linguistics. Association for Com-
putational Linguistics.

Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,
and Yulia Tsvetkov. 2019. Measuring bias in contex-
tualized word representations. In Proceedings of the
First Workshop on Gender Bias in Natural Language
Processing, pages 166-172, Florence, Italy. Associ-
ation for Computational Linguistics.

Jana Kurrek, Haji Mohammad Saleem, and Derek
Ruths. 2020. Towards a comprehensive taxonomy
and large-scale annotated corpus for online slur us-
age. In Proceedings of the Fourth Workshop on On-
line Abuse and Harms, pages 138-149, Online. As-
sociation for Computational Linguistics.

Anne Lauscher and Goran Glavas. 2019. Are we con-
sistently biased? multidimensional analysis of bi-
ases in distributional word vectors. In Proceedings
of the Eighth Joint Conference on Lexical and Com-
putational Semantics (*SEM 2019), pages 85-91,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.

Nayeon Lee, Andrea Madotto, and Pascale Fung. 2019.
Exploring social bias in chatbots using stereotype
knowledge. In Proceedings of the 2019 Workshop
on Widening NLP, pages 177-180, Florence, Italy.
Association for Computational Linguistics.

Kobi Leins, Jey Han Lau, and Timothy Baldwin. 2020.
Give me convenience and give her death: Who
should decide what uses of NLP are appropriate, and
on what basis? In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 2908-2913, Online. Association for
Computational Linguistics.

Michael Lepori. 2020. Unequal representations: Ana-
lyzing intersectional biases in word embeddings us-

ing representational similarity analysis. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics, pages 1720-1728, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.

Haochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zitao
Liu, and Jiliang Tang. 2020. Does gender matter?
towards fairness in dialogue systems. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics, pages 4403-4416, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.

Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and
Derry Tanti Wijaya. 2019. Detecting frames in news
headlines and its application to analyzing news fram-
ing trends surrounding U.S. gun violence. In Pro-
ceedings of the 23rd Conference on Computational
Natural Language Learning (CoNLL), pages 504—
514, Hong Kong, China. Association for Computa-
tional Linguistics.

Kate Loveys, Jonathan Torrez, Alex Fine, Glen Mori-
arty, and Glen Coppersmith. 2018. Cross-cultural
differences in language markers of depression on-
line. In Proceedings of the Fifth Workshop on
Computational Linguistics and Clinical Psychology:
From Keyboard to Clinic, pages 78-87, New Or-
leans, LA. Association for Computational Linguis-
tics.

Thomas Manzini, Lim Yao Chong, Alan W Black,
and Yulia Tsvetkov. 2019. Black is to criminal
as caucasian is to police: Detecting and removing
multiclass bias in word embeddings. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 615-621, Minneapo-
lis, Minnesota. Association for Computational Lin-
guistics.

Chandler May, Alex Wang, Shikha Bordia, Samuel R.
Bowman, and Rachel Rudinger. 2019. On measur-
ing social biases in sentence encoders. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 622-628, Minneapo-
lis, Minnesota. Association for Computational Lin-
guistics.

Elijah Mayfield, Michael Madaio, Shrimai Prab-
humoye, David Gerritsen, Brittany McLaughlin,
Ezekiel Dixon-Roman, and Alan W Black. 2019.
Equity beyond bias in language technologies for ed-
ucation. In Proceedings of the Fourteenth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 444-460, Florence, Italy. Asso-
ciation for Computational Linguistics.

Charlton D. McIlwain. 2019. Black Software: The In-
ternet and Racial Justice, from the AfroNet to Black
Lives Matter. Oxford University Press, Incorpo-
rated.


J. A. Meaney. 2020. Crossing the line: Where do de-
mographic variables fit into humor detection? In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics: Student Re-
search Workshop, pages 176-181, Online. Associa-
tion for Computational Linguistics.

Julia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky.
2020. A framework for the computational linguistic
analysis of dehumanization. Frontiers in Artificial
Intelligence, 3:55.

Michele Merler, Nalini Ratha, Rogerio S Feris, and
John R Smith. 2019. Diversity in faces. Computing
Research Repository, arXiv:1901.10436. Version 6.

Jack Merullo, Luke Yeh, Abram Handler, Alvin Gris-
som II, Brendan O’Connor, and Mohit Iyyer. 2019.
Investigating sports commentator bias within a large
corpus of American football broadcasts. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 6355-6361, Hong
Kong, China. Association for Computational Lin-
guistics.

Emiel van Miltenburg, Desmond Elliott, and Piek
Vossen. 2017. Cross-linguistic differences and simi-
larities in image descriptions. In Proceedings of the
10th International Conference on Natural Language
Generation, pages 21-30, Santiago de Compostela,
Spain. Association for Computational Linguistics.

Ehsan Mohammady and Aron Culotta. 2014. Using
county demographics to infer attributes of Twitter
users. In Proceedings of the Joint Workshop on So-
cial Dynamics and Personal Attributes in Social Me-
dia, pages 7-16, Baltimore, Maryland. Association
for Computational Linguistics.

Aida Mostafazadeh Davani, Leigh Yeh, Mohammad
Atari, Brendan Kennedy, Gwenyth Portillo Wight-
man, Elaine Gonzalez, Natalie Delong, Rhea Bha-
tia, Arineh Mirinjian, Xiang Ren, and Morteza De-
hghani. 2019. Reporting the unreported: Event ex-
traction for analyzing the local representation of hate
crimes. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
5753-5757, Hong Kong, China. Association for
Computational Linguistics.

Suhanthie Motha. 2020. Is an antiracist and decoloniz-
ing applied linguistics possible? Annual Review of
Applied Linguistics, 40:128-133.

Moin Nadeem, Anna Bethke, and Siva Reddy. 2020.
Stereoset: Measuring stereotypical bias in pre-
trained language models. Computing Research
Repository, arXiv:2004.09456. Version 1.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked

language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1953-1967, Online. As-
sociation for Computational Linguistics.

National Center for Science and Engineering Statistics.
2019. Doctorate recipients from U.S. universities.
National Science Foundation.

Safiya U. Noble. 2018. Algorithms of Oppression:
How Search Engines Reinforce Racism. NYU Press.

Thudiya Finda Ogbonnaya-Ogburu, Angela D.R. Smith,
Alexandra To, and Kentaro Toyama. 2020. Critical
race theory for HCI. In Proceedings of the 2020
CHI Conference on Human Factors in Computing
Systems, CHI ’20, page 1-16, New York, NY, USA.
Association for Computing Machinery.

Alexandra Olteanu, Carlos Castillo, Fernando Diaz,
and Emre Kiciman. 2019. Social data:  Bi-
ases, methodological pitfalls, and ethical boundaries.
Frontiers in Big Data, 2:13.

Julia Parish-Morris. 2019. Computational linguistics
for enhancing scientific reproducibility and reducing
healthcare inequities. In Proceedings of the Sixth
Workshop on Computational Linguistics and Clini-
cal Psychology, pages 94-102, Minneapolis, Min-
nesota. Association for Computational Linguistics.

Amandalynne Paullada. 2020. How Does Machine
Translation Shift Power? In Proceedings of the First
Workshop on Resistance AI.

Ellie Pavlick, Heng Ji, Xiaoman Pan, and Chris
Callison-Burch. 2016. The gun violence database:
A new task and data set for NLP. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1018-1024, Austin,
Texas. Association for Computational Linguistics.

Daniel Preotiuc-Pietro and Lyle Ungar. 2018. User-
level race and ethnicity predictors from Twitter text.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 1534-1545,
Santa Fe, New Mexico, USA. Association for Com-
putational Linguistics.

Inioluwa Deborah Raji and Joy Buolamwini. 2019. Ac-
tionable auditing: Investigating the impact of pub-
licly naming biased performance results of com-
mercial AI products. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society,
pages 429-435.

Anil Ramakrishna, Victor R. Martinez, Nikolaos Ma-
landrakis, Karan Singla, and Shrikanth Narayanan.
2017. Linguistic analysis of differences in portrayal
of movie characters. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1669—
1678, Vancouver, Canada. Association for Computa-
tional Linguistics.


Yolanda A. Rankin and Jakita O. Thomas. 2019.
Straighten up and fly right: Rethinking intersection-
ality in HCI research. Interactions, 26(6):64—-68.

Alexey Romanov, Maria De-Arteaga, Hanna Wal-
lach, Jennifer Chayes, Christian Borgs, Alexan-
dra Chouldechova, Sahin Geyik, Krishnaram Ken-
thapadi, Anna Rumshisky, and Adam Kalai. 2019.
What’s in a name? Reducing bias in bios without
access to protected attributes. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers), pages 4187-4195, Minneapolis, Min-
nesota. Association for Computational Linguistics.

Jonathan Rosa and Nelson Flores. 2017. Unsettling
race and language: Toward a raciolinguistic perspec-
tive. Language in Society, 46(5):62 1-647.

Wendy D Roth. 2016. The multiple dimensions of race.
Ethnic and Racial Studies, 39(8):1310-1338.

Shamik Roy and Dan Goldwasser. 2020. Weakly su-
pervised learning of nuanced frames for analyzing
polarization in news media. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 7698-7716,
Online. Association for Computational Linguistics.

Rachel Rudinger, Chandler May, and Benjamin
Van Durme. 2017. Social bias in elicited natural lan-
guage inferences. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Process-
ing, pages 74-79, Valencia, Spain. Association for
Computational Linguistics.

Wesley Santos and Ivandré Paraboni. 2019. Moral
stance recognition and polarity classification from
Twitter and elicited text. In Proceedings of the Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP 2019), pages 1069-
1075, Varna, Bulgaria. INCOMA Ltd.

Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,
and Noah A. Smith. 2019. The risk of racial bias
in hate speech detection. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 1668-1678, Florence,
Italy. Association for Computational Linguistics.

Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Ju-
rafsky, Noah A. Smith, and Yejin Choi. 2020. So-
cial bias frames: Reasoning about social and power
implications of language. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 5477-5490, Online. As-
sociation for Computational Linguistics.

P.K. Saucier, T.P. Woods, P. Douglass, B. Hesse, T.K.
Nopper, G. Thomas, and C. Wun. 2016. Concep-
tual Aphasia in Black: Displacing Racial Formation.
Critical Africana Studies. Lexington Books.

Ari Schlesinger, W. Keith Edwards, and Rebecca E.
Grinter. 2017. Intersectional hci: Engaging iden-
tity through gender, race, and class. In Proceedings
of the 2017 CHI Conference on Human Factors in
Computing Systems, CHI’ 17, page 5412-5427, New
York, NY, USA. Association for Computing Machin-
ery.

Tyler Schnoebelen. 2017. Goal-oriented design for eth-
ical machine learning and NLP. In Proceedings of
the First ACL Workshop on Ethics in Natural Lan-
guage Processing, pages 88—93, Valencia, Spain. As-
sociation for Computational Linguistics.

Deven Santosh Shah, H. Andrew Schwartz, and Dirk
Hovy. 2020. Predictive biases in natural language
processing models: A conceptual framework and
overview. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 5248-5264, Online. Association for Computa-
tional Linguistics.

Usman Shahid, Barbara Di Eugenio, Andrew Rojecki,
and Elena Zheleva. 2020. Detecting and understand-
ing moral biases in news. In Proceedings of the First
Joint Workshop on Narrative Understanding, Story-
lines, and Events, pages 120-125, Online. Associa-
tion for Computational Linguistics.

Sima Sharifirad and Stan Matwin. 2019. Using
attention-based bidirectional LSTM to identify dif-
ferent categories of offensive language directed to-
ward female celebrities. In Proceedings of the 2019
Workshop on Widening NLP, pages 46-48, Florence,
Italy. Association for Computational Linguistics.

Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,
and Nanyun Peng. 2019. The woman worked as
a babysitter: On biases in language generation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 3407-
3412, Hong Kong, China. Association for Computa-
tional Linguistics.

M Sloane, E Moss, O Awomolo, and L Forlano. 2020.
Participation is not a design fix for machine learning.
Computing Research Repository, arXiv:2007.02423.
Version 3.

Harold Somers. 2006. Language engineering and the
pathway to healthcare: A user-oriented view. In
Proceedings of the First International Workshop
on Medical Speech Translation, pages 28-35, New
York, New York. Association for Computational Lin-
guistics.

Pia Sommerauer and Antske Fokkens. 2019. Concep-
tual change and distributional semantic models: an
exploratory study on pitfalls and possibilities. In
Proceedings of the Ist International Workshop on
Computational Approaches to Historical Language
Change, pages 223-233, Florence, Italy. Associa-
tion for Computational Linguistics.


Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for
deep learning in NLP. In Proceedings of the 57th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 3645-3650, Florence, Italy.
Association for Computational Linguistics.

Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth
Belding, Kai-Wei Chang, and William Yang Wang.
2019. Mitigating gender bias in natural language
processing: Literature review. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 1630-1640, Florence,
Italy. Association for Computational Linguistics.

Latanya Sweeney. 2013. Discrimination in online ad
delivery: Google ads, black names and white names,
racial discrimination, and click advertising. Queue,
11(3):10-29.

Samson Tan, Shafiq Joty, Min-Yen Kan, and Richard
Socher. 2020. It’s morphin’ time! Combating
linguistic discrimination with inflectional perturba-
tions. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 2920-2935, Online. Association for Computa-
tional Linguistics.

Yi Chern Tan and L Elisa Celis. 2019. Assessing social
and intersectional biases in contextualized word rep-
resentations. In Proceedings of the 2019 Conference
on Advances in Neural Information Processing Sys-
tems, volume 32, pages 13230-13241. Curran Asso-
ciates, Inc.

Rachael Tatman. 2020. What I Won’t Build. Workshop
on Widening NLP.

Rocco Tripodi, Massimo Warglien, Simon Levis Sul-
lam, and Deborah Paci. 2019. Tracing antisemitic
language through diachronic embedding projections:
France 1789-1914. In Proceedings of the Ist Inter-
national Workshop on Computational Approaches to
Historical Language Change, pages 115-125, Flo-
rence, Italy. Association for Computational Linguis-
tics.

Ekaterina Vylomova, Sean Murphy, and Nicholas
Haslam. 2019. Evaluation of semantic change of
harm-related concepts in psychology. In Proceed-
ings of the Ist International Workshop on Computa-
tional Approaches to Historical Language Change,
pages 29-34, Florence, Italy. Association for Com-
putational Linguistics.

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,
and Sameer Singh. 2019. Universal adversarial trig-
gers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 2153-2162, Hong
Kong, China. Association for Computational Lin-
guistics.

William Warner and Julia Hirschberg. 2012. Detecting
hate speech on the world wide web. In Proceedings
of the Second Workshop on Language in Social Me-
dia, pages 19-26, Montréal, Canada. Association for
Computational Linguistics.

Zeerak Waseem. 2016. Are you a racist or am I seeing
things? annotator influence on hate speech detection
on Twitter. In Proceedings of the First Workshop on
NLP and Computational Social Science, pages 138—
142, Austin, Texas. Association for Computational
Linguistics.

Zeerak Waseem, Thomas Davidson, Dana Warmsley,
and Ingmar Weber. 2017. Understanding abuse: A
typology of abusive language detection subtasks. In
Proceedings of the First Workshop on Abusive Lan-
guage Online, pages 78-84, Vancouver, BC, Canada.
Association for Computational Linguistics.

Zeerak Waseem and Dirk Hovy. 2016. Hateful sym-
bols or hateful people? predictive features for hate
speech detection on Twitter. In Proceedings of the
NAACL Student Research Workshop, pages 88-93,
San Diego, California. Association for Computa-
tional Linguistics.

Zeerak Waseem, Smarika Lulz, and Isabelle Bingel,
Joachim Augenstein. 2021. Disembodied machine
learning: On the illusion of objectivity in NLP.
Computing Research Repository, arXiv:2101.11974.
Version |.

Kellie Webster, Marta R. Costa-jussa, Christian Hard-
meier, and Will Radford. 2019. Gendered ambigu-
ous pronoun (GAP) shared task at the gender bias
in NLP workshop 2019. In Proceedings of the First
Workshop on Gender Bias in Natural Language Pro-
cessing, pages 1-7, Florence, Italy. Association for
Computational Linguistics.

Michael Wojatzki, Saif Mohammad, Torsten Zesch,
and Svetlana Kiritchenko. 2018. Quantifying qual-
itative data for understanding controversial issues.
In Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), Miyazaki, Japan. European Language Re-
sources Association (ELRA).

Zach Wood-Doughty, Nicholas Andrews, Rebecca
Marvin, and Mark Dredze. 2018. Predicting Twit-
ter user demographics from names alone. In Pro-
ceedings of the Second Workshop on Computational
Modeling of People’s Opinions, Personality, and
Emotions in Social Media, pages 105-111, New Or-
leans, Louisiana, USA. Association for Computa-
tional Linguistics.

Zach Wood-Doughty, Michael Smith, David Bronia-
towski, and Mark Dredze. 2017. How does Twitter
user behavior vary across demographic groups? In
Proceedings of the Second Workshop on NLP. and
Computational Social Science, pages 83-89, Van-
couver, Canada. Association for Computational Lin-
guistics.


Lucas Wright, Derek Ruths, Kelly P Dillon, Haji Mo-
hammad Saleem, and Susan Benesch. 2017. Vec-
tors for counterspeech on Twitter. In Proceedings
of the First Workshop on Abusive Language Online,
pages 57-62, Vancouver, BC, Canada. Association
for Computational Linguistics.

Mengzhou Xia, Anjalie Field, and Yulia Tsvetkov.
2020. Demoting racial bias in hate speech detection.
In Proceedings of the Eighth International Work-
shop on Natural Language Processing for Social Me-
dia, pages 7-14, Online. Association for Computa-
tional Linguistics.

Qiongkai Xu, Lizhen Qu, Chenchen Xu, and Ran Cui.
2019. Privacy-aware text rewriting. In Proceed-
ings of the 12th International Conference on Nat-
ural Language Generation, pages 247-257, Tokyo,
Japan. Association for Computational Linguistics.

Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Con-
ghui Zhu, and Tiejun Zhao. 2020. Demographics
should not be the reason of toxicity: Mitigating
discrimination in text classifications with instance
weighting. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 4134-4145, Online. Association for Computa-
tional Linguistics.

Jieyu Zhao and Kai-Wei Chang. 2020. LOGAN: Lo-
cal group bias detection by clustering. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1968-1977, Online. Association for Computational
Linguistics.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2979-2989, Copenhagen,
Denmark. Association for Computational Linguis-
tics.


25

20
15
10 ;

HWS WV WP A WG XL OS 92

S
SPL LL LL LS Lf

oa

Figure 1: Year of publication of 79 papers that mention
“racial” or “racism”. More papers have been published
in recent years (2019-2020).

40

30

20

10 ia

0 Ree = = — — — —
ota fH oO Ioouw
S2292un32222
g eaSPes6oager
= ae) oS
s © ~

Figure 2: Venue of publication of 79 papers that men-
tion “racial” or “racism”. About half (46.8%) were pub-
lished in workshops.

A ACL Anthology Venues

ACL events: AACL, ACL, ANLP, CL, CoNLL,
EACL, EMNLP, Findings, NAACL, SemEval,
*SEM, TACL, WMT, Workshops, Special Interest
Groups

Non-ACL events: ALTA, AMTA, CCL, COL-
ING, EAMT, HLT, ISCNLP, JEP/TALN/RECITAL,
LILT, LREC, MUC, PACLIC, RANLP, RO-
CLING/IJCLCLP, TINLAP, TIPSTER

B_ Additional Survey Metrics

We show three additional breakdowns of the data
set: Figure 1 shows the number of papers published
each year, Figure 2 shows the number of papers
published in each venue, and Table 2 shows how
papers have operationalized race. As expected,
given the growth of NLP research in general and
the increasing focus on social issues (e.g. “Ethics
and NLP” track was added to ACL in 2020) more
work has been published on race in more recent
years (2019, 2020). In Figure 2, we consider if
work on race has been siloed into or out of specific

3 oo z 9
oO oO a
& ees 3
¢ &@ 4 8 2 8
ah wa # S » & &
FE z 3 &€& © B& 2
eb e826 38s
OUR RAZ & MN Total
5 2 1 5 13
7 2 1 8 1 1 20
BWAH 1 3 4
{BWAH} 1 1 3 1 2 8
W/non-W 1 1 2

Total 9 2 10 4 11 5 6 47

Table 2: Racial categories used by ACL Anthology
papers. BWAH stand for Black, White, Asian, and
Hispanic. {BWAH} denotes any incomplete subset of
BWAH other than BW (e.g. Black and Hispanic). 4+
denotes that the paper used > 4 racial categories, often
including “other’’, “mixed”, or an open-ended text box.
Papers with multiple schema are counted as separate
data points.

venues. The majority of papers were published in
workshops, which is consist with the large num-
ber of workshop papers. In 2019, approximately
2,038 papers were published in workshops'* and
1,680 papers were published in conferences (ACL,
EMNLP, NAACL, CONLL, CICLing), meaning
54.8% were published in workshops. In our data
set, 46.8% of papers surveyed were published in
workshops. The most number of papers were pub-
lished in the largest conferences: ACL and EMNLP.
Thus, while Table 1 suggests that discussions of
race have been siloed to particular NLP applica-
tions, Figure 2 does not show evidence that they
have been siloed to particular venues.

In Table 2, for all papers that use categorization
schema to classify race, we show what racial cate-
gories they use. If a paper uses multiple schemes
(e.g. collects crowd-sourced annotations of stereo-
types associated with different races and also asks
annotators to self-report their race), we report each
scheme as a separate data point. This table does not
include papers that do not specify racial categories
(e.g. examine “racist language” without specifying
targeted people or analyze semantic change of top-
ics like “racism” and “prejudice’). Finally, we map
terms used by papers to the ones in Table 2, e.g. pa-
pers examining African American vs. European
American names are included in BW.

The majority of papers focus on_ binary

4https://www.aclweb.org/anthology/
venues/ws/


Black/white racial categories. While many papers
draw definitions from the U.S. census, very few pa-
pers consider less-commonly-selected census cat-
egories like Native American or Pacific Islander.
The most common method for identifying people’s
race uses first or last names (10 papers) or explicit
keywords like “black” and “white” (10 papers).


C_ Full List of Surveyed Papers

Year Venue NLP Task Task Type
Assimakopoulos et al. (2020) 2020 LREC Abusive Language Collect Corpus
Bommasani et al. (2020) 2020 ACL Text Representations Detect Bias
Chakravarthi (2020) 2020 Workshop Abusive Language Collect Corpus
Groenwold et al. (2020) 2020 EMNLP Text Generation Detect Bias
Gupta et al. (2020) 2020 Workshop  Sector-spec. NLP apps. Collect Corpus
Huang et al. (2020) 2020 LREC Abusive Language Detect Bias
Jiang and Fellbaum (2020) 2020 Workshop Text Representations Detect Bias
Joseph and Morgan (2020) 2020 ACL Text Representations Detect Bias
Kennedy et al. (2020) 2020 ACL Abusive Language Debias
Kurrek et al. (2020) 2020 Workshop Abusive Language Collect Corpus
Lepori (2020) 2020 COLING Text Representations Detect Bias
Liu et al. (2020) 2020 COLING Text Generation Debias
Meaney (2020) 2020 Workshop — Social Science/Media = Survey/Position
Nangia et al. (2020) 2020 EMNLP Text Representations Detect Bias
Roy and Goldwasser (2020) 2020 EMNLP Social Science/Media Analyze Corpus
Sap et al. (2020) 2020 ACL Abusive Language Collect Corpus
Shah et al. (2020) 2020 ACL Ethics/Task-indep. Bias Survey/Position
Shahid et al. (2020) 2020 Workshop — Social Science/Media Analyze Corpus
Tan et al. (2020) 2020 ACL Ethics/Task-indep. Bias Develop Model
Xia et al. (2020) 2020 Workshop Abusive Language Debias
Zhang et al. (2020) 2020 ACL Abusive Language Detect Bias
Zhao and Chang (2020) 2020 EMNLP _ Ethics/Task-indep. Bias Detect Bias
Amir et al. (2019) 2019 Workshop  Sector-spec. NLP apps. Analyze Corpus
Davidson et al. (2019) 2019 Workshop Abusive Language Detect Bias
Demszky et al. (2019) 2019 NAACL Social Science/Media Analyze Corpus
Gillani and Levy (2019) 2019 Workshop — Text Representations Analyze Corpus
Jurgens et al. (2019) 2019 ACL Abusive Language Survey/Position
Karve et al. (2019) 2019 Workshop — Text Representations Debias
Kurita et al. (2019) 2019 Workshop — Text Representations Detect Bias
Lauscher and Glava¥ (2019) 2019 Workshop _ Text Representations Detect Bias
Lee et al. (2019) 2019 Workshop Text Generation Detect Bias
Liu et al. (2019) 2019 CoNLL Social Science/Media Develop Model
Manzini et al. (2019) 2019 NAACL Text Representations Debias
May et al. (2019) 2019 ACL Text Representations Detect Bias
Mayfield et al. (2019) 2019 Workshop  Sector-spec. NLP apps. Survey/Position
Merullo et al. (2019) 2019 EMNLP Social Science/Media Analyze Corpus
Mostafazadeh Davani et al. (2019) 2019 EMNLP Core NLP Applications Develop Model
Parish-Morris (2019) 2019 Workshop Sector-spec. NLP apps. Survey/Position
Romanov et al. (2019) 2019 NAACL _ Sector-spec. NLP apps. Debias
Santos and Paraboni (2019) 2019 RANLP Social Science/Media Collect Corpus
Sap et al. (2019) 2019 ACL Abusive Language Detect Bias
Sharifirad and Matwin (2019) 2019 Workshop Abusive Language Analyze Corpus
Sommerauer and Fokkens (2019) 2019 Workshop _ Text Representations Detect Bias
Tripodi et al. (2019) 2019 Workshop — Text Representations Analyze Corpus
Vylomova et al. (2019) 2019 Workshop — Social Science/Media Analyze Corpus
Wallace et al. (2019) 2019 EMNLP Text Generation Detect Bias
Xu et al. (2019) 2019 INLG Text Generation Develop Model
Barbieri and Camacho-Collados (2018) 2018 *SEM Social Science/Media Analyze Corpus


Blodgett et al. (2018)

Castelle (2018)

de Gibert et al. (2018)

Elazar and Goldberg (2018)
Kasunic and Kaufman (2018)
Kiritchenko and Mohammad (2018)
Loveys et al. (2018)
Preotiuc-Pietro and Ungar (2018)
Sheng et al. (2019)

Wojatzki et al. (2018)
Wood-Doughty et al. (2018)
Clarke and Grieve (2017)
Gallagher et al. (2017)
Hasanuzzaman et al. (2017)
Ramakrishna et al. (2017)
Rudinger et al. (2017)
Schnoebelen (2017)

van Miltenburg et al. (2017)
Waseem et al. (2017)
Wood-Doughty et al. (2017)
Wright et al. (2017)

Blodgett et al. (2016)

Pavlick et al. (2016)

Waseem (2016)

Waseem and Hovy (2016)
Mohammady and Culotta (2014)
Bergsma et al. (2013)
Herbelot et al. (2012)

Warner and Hirschberg (2012)
Eisenstein et al. (2011)
Somers (2006)

2018
2018
2018
2018
2018
2018
2018
2018
2018
2018
2018
2017
2017
2017
2017
2017
2017
2017
2017
2017
2017
2016
2016
2016
2016
2014
2013
2012
2012
2011
2006

ACL
Workshop
Workshop

EMNLP
Workshop
*SEM
Workshop
COLING
EMNLP
LREC
Workshop
Workshop
TACL
IJCNLP

ACL
Workshop
Workshop

INLG
Workshop
Workshop
Workshop

EMNLP

EMNLP
Workshop
Workshop
Workshop

NAACL
Workshop
Workshop

ACL
Workshop

Core NLP Applications
Abusive Language
Abusive Language

Ethics/Task-indep. Bias

Text Generation
Social Science/Media
Sector-spec. NLP apps.
Social Science/Media
Text Generation
Social Science/Media
Social Science/Media
Abusive Language
Social Science/Media
Abusive Language
Social Science/Media

Core NLP Applications

Ethics/Task-indep. Bias
Image Processing
Abusive Language

Social Science/Media
Abusive Language

Ethics/Task-indep. Bias

Core NLP Applications
Abusive Language
Abusive Language

Social Science/Media
Social Science/Media
Social Science/Media
Abusive Language
Social Science/Media
Sector-spec. NLP apps.

Debias
Analyze Corpus
Collect Corpus

Debias
Survey/Position

Detect Bias
Analyze Corpus
Develop Model

Detect Bias
Collect Corpus
Develop Model
Analyze Corpus
Develop Model
Develop Model
Analyze Corpus

Detect Bias
Survey/Position

Detect Bias
Survey/Position
Analyze Corpus
Analyze Corpus
Collect Corpus
Collect Corpus

Detect Bias
Collect Corpus
Develop Model
Develop Model
Analyze Corpus
Develop Model
Analyze Corpus
Survey/Position
